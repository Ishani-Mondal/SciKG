{"title": [{"text": "Natural Language Models for Predicting Programming Comments", "labels": [], "entities": [{"text": "Predicting Programming Comments", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.9349807898203532}]}], "abstractContent": [{"text": "Statistical language models have successfully been used to describe and analyze natural language documents.", "labels": [], "entities": []}, {"text": "Recent work applying language models to programming languages is focused on the task of predicting code, while mainly ignoring the prediction of programmer comments.", "labels": [], "entities": [{"text": "predicting code", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.8821747601032257}]}, {"text": "In this work, we predict comments from JAVA source files of open source projects, using topic models and n-grams, and we analyze the performance of the models given varying amounts of background data on the project being predicted.", "labels": [], "entities": [{"text": "JAVA source files", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.889213502407074}]}, {"text": "We evaluate models on their comment-completion capability in a setting similar to code-completion tools built into standard code editors, and show that using a comment completion tool can save up to 47% of the comment typing.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Since our models are trained using various data sources the vocabularies used by each of them are different, making the comment likelihood given by each model incomparable due to different sets of out-of-vocabulary tokens.", "labels": [], "entities": []}, {"text": "We thus evaluate models using a character saving metric which aims at quantifying the percentage of characters that can be saved by using the model in a word-completion settings, similar to standard code completion tools built into code editors.", "labels": [], "entities": [{"text": "character saving", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.743156224489212}]}, {"text": "For a comment word with n characters, w = w 1 , . .", "labels": [], "entities": []}, {"text": ", w n , we predict the two most likely words given each model filtered by the first 0, . .", "labels": [], "entities": []}, {"text": ", n characters of w.", "labels": [], "entities": []}, {"text": "Let k be the minimal k i for which w is in the top two predicted word tokens where tokens are filtered by the first k i characters.", "labels": [], "entities": []}, {"text": "Then, the number of saved characters for w is n \u2212 k.", "labels": [], "entities": []}, {"text": "In we report the average percentage of saved characters per comment using each of the above models.", "labels": [], "entities": []}, {"text": "The final results are also averaged over the nine input projects.", "labels": [], "entities": []}, {"text": "As an example, in the predicted comment shown in, taken from the project Minor-Third, the token entity is the most likely token according to the model SO trigram, out of tokens starting with the prefix 'en'.", "labels": [], "entities": []}, {"text": "The saved characters in this case are 'tity'.", "labels": [], "entities": []}, {"text": "displays the average percentage of characters saved per class comment using each of the models.", "labels": [], "entities": []}, {"text": "Models trained on in-project data (IN) perform significantly better than those trained on another data source, regardless of the model type, with an average saving of 47.1% characters using a trigram model.", "labels": [], "entities": []}, {"text": "This is expected, as files from the same project are likely to contain similar comments, and identifier names that appear in the comment of one class may appear in the code of another class in the same project.", "labels": [], "entities": []}, {"text": "Clearly, in-project data should be used when available as it improves comment prediction leading to an average increase of between 6% for the worst model: Average percentage of characters saved per comment using n-gram, LDA and link-LDA models trained on three training sets: IN, OUT, and SO.", "labels": [], "entities": [{"text": "comment prediction", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7114682048559189}, {"text": "IN", "start_pos": 276, "end_pos": 278, "type": "METRIC", "confidence": 0.9755942225456238}, {"text": "OUT", "start_pos": 280, "end_pos": 283, "type": "METRIC", "confidence": 0.9083084464073181}]}, {"text": "The results are averaged over nine JAVA projects (with standard deviations in parenthesis).", "labels": [], "entities": [{"text": "JAVA", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.8834912180900574}]}], "tableCaptions": [{"text": " Table 1: Average percentage of characters saved per comment using n-gram, LDA and link-LDA models  trained on three training sets: IN, OUT, and SO. The results are averaged over nine JAVA projects (with  standard deviations in parenthesis).", "labels": [], "entities": [{"text": "IN", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.9621754288673401}, {"text": "OUT", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.9274830222129822}, {"text": "JAVA projects", "start_pos": 184, "end_pos": 197, "type": "DATASET", "confidence": 0.9033601582050323}]}, {"text": " Table 3: Average words per project for which each  tested model completes the word better than the  other. This indicates that each of the models is bet- ter at predicting a different set of comment words.", "labels": [], "entities": []}]}