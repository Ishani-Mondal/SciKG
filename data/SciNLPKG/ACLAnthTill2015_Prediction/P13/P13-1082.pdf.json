{"title": [{"text": "A Multi-Domain Translation Model Framework for Statistical Machine Translation", "labels": [], "entities": [{"text": "Multi-Domain Translation Model", "start_pos": 2, "end_pos": 32, "type": "TASK", "confidence": 0.7543885409832001}, {"text": "Statistical Machine Translation", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.8076582352320353}]}], "abstractContent": [{"text": "While domain adaptation techniques for SMT have proven to be effective at improving translation quality, their practical-ity fora multi-domain environment is often limited because of the computational and human costs of developing and maintaining multiple systems adapted to different domains.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9929478168487549}]}, {"text": "We present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time.", "labels": [], "entities": []}, {"text": "We also describe a method for unsupervised adaptation with development and test data from multiple domains.", "labels": [], "entities": []}, {"text": "Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1 BLEU over unadapted systems and single-domain adaptation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9980424642562866}]}], "introductionContent": [{"text": "The effectiveness of domain adaptation approaches such as mixture-modeling has been established, and has led to research on a wide array of adaptation techniques in SMT, for instance (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 165, "end_pos": 168, "type": "TASK", "confidence": 0.993177056312561}]}, {"text": "In all these approaches, adaptation is performed during model training, with respect to a representative development corpus, and the models are kept unchanged when then system is deployed.", "labels": [], "entities": []}, {"text": "Therefore, when working with multiple and/or unlabelled domains, domain adaptation is often impractical fora number of reasons.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.743941456079483}]}, {"text": "Firstly, maintaining multiple systems for each language pair, each adapted to a different domain, is costly in terms of computational and human resources: the full system development pipeline needs to be performed for all identified domains, all the models are separately stored and need to be switched at runtime.", "labels": [], "entities": []}, {"text": "This is impractical in many real applications, in particular a web translation service which is faced with texts coming from many different domains.", "labels": [], "entities": [{"text": "web translation", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7253409028053284}]}, {"text": "Secondly, domain adaptation bears a risk of performance loss.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7878385782241821}]}, {"text": "If there is a mismatch between the domain of the development set and the test set, domain adaptation can potentially harm performance compared to an unadapted baseline.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7280099838972092}]}, {"text": "We introduce a translation model architecture that delays the computation of features to the decoding phase.", "labels": [], "entities": []}, {"text": "The calculation is based on a vector of component models, with each component providing the sufficient statistics necessary for the computation of the features.", "labels": [], "entities": []}, {"text": "With this framework, adaptation to anew domain simply consists of updating a weight vector, and multiple domains can be supported by the same system.", "labels": [], "entities": []}, {"text": "We also present a clustering approach for unsupervised adaptation in a multi-domain environment.", "labels": [], "entities": []}, {"text": "In the development phase, a set of development data is clustered, and the models are adapted to each cluster.", "labels": [], "entities": []}, {"text": "For each sentence that is being decoded, we choose the weight vector that is optimized on the closest cluster, allowing for adaptation even with unlabelled and heterogeneous test data.) delay the computation of translation model features for the purpose of interactive machine translation with online training.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 269, "end_pos": 288, "type": "TASK", "confidence": 0.7014627754688263}]}, {"text": "The main difference to our approach is that we store sufficient statistics not fora single model, but a vector of models, which allows us to weight the contribution of each component model to the feature calculation.", "labels": [], "entities": []}, {"text": "The similarity suggests that our framework could also be used for interactive learning, with the ability to learn a model incrementally from user feedback, and weight it differently than the static models, opening new research opportunities.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Illustration of instance weighting with  weight vectors for two corpora.", "labels": [], "entities": []}, {"text": " Table 2: Parallel data sets English-German.", "labels": [], "entities": []}, {"text": " Table 3: Parallel data sets Czech-English.", "labels": [], "entities": []}, {"text": " Table 4: Translation experiments EN-DE. BLEU scores reported.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9622727632522583}, {"text": "EN-DE", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.864954948425293}, {"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.999276340007782}]}, {"text": " Table 6: Translation experiments CZ-EN. BLEU scores reported.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9658902287483215}, {"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9994350075721741}]}]}