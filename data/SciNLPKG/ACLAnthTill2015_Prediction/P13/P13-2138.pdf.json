{"title": [{"text": "Generalizing Image Captions for Image-Text Parallel Corpus", "labels": [], "entities": [{"text": "Generalizing Image Captions", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8995294769605001}]}], "abstractContent": [{"text": "The ever growing amount of web images and their associated texts offers new opportunities for integrative models bridging natural language processing and computer vision.", "labels": [], "entities": []}, {"text": "However, the potential benefits of such data are yet to be fully realized due to the complexity and noise in the alignment between image content and text.", "labels": [], "entities": []}, {"text": "We address this challenge with contributions in two folds: first, we introduce the new task of image caption generalization, formulated as visually-guided sentence compression , and present an efficient algorithm based on dynamic beam search with dependency-based constraints.", "labels": [], "entities": [{"text": "image caption generalization", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.8419051170349121}, {"text": "sentence compression", "start_pos": 155, "end_pos": 175, "type": "TASK", "confidence": 0.7177107185125351}]}, {"text": "Second, we release anew large-scale corpus with 1 million image-caption pairs achieving tighter content alignment between images and text.", "labels": [], "entities": []}, {"text": "Evaluation results show the intrinsic quality of the generalized captions and the extrinsic utility of the new image-text parallel corpus with respect to a concrete application of image caption transfer.", "labels": [], "entities": [{"text": "image caption transfer", "start_pos": 180, "end_pos": 202, "type": "TASK", "confidence": 0.7456325689951578}]}], "introductionContent": [{"text": "The vast number of online images with accompanying text raises hope for drawing synergistic connections between human language technologies and computer vision.", "labels": [], "entities": []}, {"text": "However, subtleties and complexity in the relationship between image content and text make exploiting paired visual-textual data an open and interesting problem.", "labels": [], "entities": []}, {"text": "Some recent work has approached the problem of composing natural language descriptions for images by using computer vision to retrieve images with similar content and then transferring  text from the retrieved samples to the query image (e.g.,,).", "labels": [], "entities": []}, {"text": "Other work (e.g.,) uses computer vision to bias summarization of text associated with images to produce descriptions.", "labels": [], "entities": [{"text": "summarization of text associated with images", "start_pos": 48, "end_pos": 92, "type": "TASK", "confidence": 0.8258331815401713}]}, {"text": "All of these approaches rely on existing text that describes visual content, but many times existing image descriptions contain significant amounts of extraneous, non-visual, or otherwise non-desirable content.", "labels": [], "entities": []}, {"text": "The goal of this paper is to develop techniques to automatically cleanup visually descriptive text to make it more directly usable for applications exploiting the connection between images and language.", "labels": [], "entities": []}, {"text": "As a concrete example, consider the first image in.", "labels": [], "entities": []}, {"text": "This caption was written by the photo owner and therefore contains information related to the context of when and where the photo was taken.", "labels": [], "entities": []}, {"text": "Objects such as \"lamp\", \"door\", \"camera\" are not visually present in the photo.", "labels": [], "entities": []}, {"text": "The second image shows a similar but somewhat different issue.", "labels": [], "entities": []}, {"text": "Its caption describes visible objects such as \"bridge\" and \"yard\", but \"Cabelas Driver\" are overly specific and not visually detectable.", "labels": [], "entities": []}, {"text": "The text of the third image, \"A house being pulled by a boat\", pertains directly to the visual content of the image, but is unlikely to be useful for tasks such as caption transfer because the depiction is unusual.", "labels": [], "entities": [{"text": "caption transfer", "start_pos": 164, "end_pos": 180, "type": "TASK", "confidence": 0.9792169630527496}]}, {"text": "This phenomenon of information gap between the visual content of the images and their corresponding narratives has been studied closely by.", "labels": [], "entities": []}, {"text": "The content misalignment between images and text limits the extent to which visual detectors can learn meaningful mappings between images and text.", "labels": [], "entities": []}, {"text": "To tackle this challenge, we introduce the new task of image caption generalization that rewrites captions to be more visually relevant and more readily applicable to other visually similar images.", "labels": [], "entities": [{"text": "image caption generalization", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.8424559632937113}]}, {"text": "Our end goal is to convert noisy imagetext pairs in the wild () into pairs with tighter content alignment, resulting in new simplified captions over 1 million images.", "labels": [], "entities": []}, {"text": "Evaluation results show both the intrinsic quality of the generalized captions and the extrinsic utility of the new image-text parallel corpus.", "labels": [], "entities": []}, {"text": "The new parallel corpus will be made publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since there is no existing benchmark data for image caption generalization, we crowdsource evaluation using Amazon Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "image caption generalization", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.8727434277534485}, {"text": "Amazon Mechanical Turk (AMT)", "start_pos": 108, "end_pos": 136, "type": "DATASET", "confidence": 0.84897713859876}]}, {"text": "We empirically compare the following options: \uf0e0 A tree in bloom . Pillbox in field behind a pub car park.", "labels": [], "entities": []}, {"text": "Flowering tree in mixed forest at Wakehurst.", "labels": [], "entities": [{"text": "Wakehurst", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.9604476094245911}]}, {"text": "\uf0e0 Flowering tree in forest.", "labels": [], "entities": []}, {"text": "The insulbrick matches the yard.", "labels": [], "entities": []}, {"text": "This is outside of medina ohio near the tonka truck house.", "labels": [], "entities": []}, {"text": "This is outside the house.", "labels": [], "entities": []}, {"text": "Turkers are provided with an image and two captions (produced by different methods) and are asked to select a better one, i.e., the most relevant and plausible caption that contains the least extraneous information.", "labels": [], "entities": []}, {"text": "We observe that VISUAL (full model with visually guided content selection) performs the best, being selected over SALIENCY (content-selection without visual information) in 72.48% cases, and even over the original image caption in 81.75% cases.", "labels": [], "entities": [{"text": "VISUAL", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.6656419634819031}, {"text": "SALIENCY", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9929752349853516}]}, {"text": "This forced-selection experiment between VI-SUAL and ORIG demonstrates the degree of noise prevalent in the image captions in the wild.", "labels": [], "entities": [{"text": "ORIG", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.7047166228294373}]}, {"text": "Of course, if compared against human-compressed captions, the automatic captions are preferred much less frequently -in 19% of the cases.", "labels": [], "entities": []}, {"text": "In those 19% cases when automatic captions are preferred over human-compressed ones, it is sometimes that humans did not fully remove information that is not visually present or verifiable, and other times humans overly compressed.", "labels": [], "entities": []}, {"text": "To verify the utility of dependency-based constraints, we also compare two variations of VISUAL, with and without dependency-based constraints.", "labels": [], "entities": [{"text": "VISUAL", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.5753054022789001}]}, {"text": "As expected, the algorithm with constraints is preferred in the majority of cases.", "labels": [], "entities": []}, {"text": "We evaluate the usefulness of our new image-text parallel corpus for automatic generation of image descriptions.", "labels": [], "entities": [{"text": "automatic generation of image descriptions", "start_pos": 69, "end_pos": 111, "type": "TASK", "confidence": 0.7875178813934326}]}, {"text": "Here the task is to produce, fora query image, a relevant description, i.e., a visually descriptive caption.", "labels": [], "entities": []}, {"text": "Following, we produce a caption fora query image by finding top k most similar images within the 1M image-text corpus () and then transferring their captions to the query image.", "labels": [], "entities": []}, {"text": "To compute evaluation measures, we take the average scores of BLEU(1) and F-score (unigrambased with respect to content-words) over k = 5 candidate captions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.998878538608551}, {"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9959143996238708}]}, {"text": "Image similarity is computed using two global (whole) image descriptors.", "labels": [], "entities": [{"text": "Image similarity", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7858482897281647}]}, {"text": "The first is the GIST feature), an image descriptor related to perceptual characteristics of scenes -naturalness, roughness, openness, etc.", "labels": [], "entities": []}, {"text": "The second descriptor is also a global image descriptor, computed by resizing the image into a \"tiny image\" (, which is effective in matching the structure and overall color of images.", "labels": [], "entities": []}, {"text": "To find visually relevant images, we compute the similarity of the query image to im-: Good (left three, in blue) and bad examples (right three, in red) of generalized captions ages in the whole dataset using an unweighted sum of gist similarity and tiny image similarity.", "labels": [], "entities": []}, {"text": "Gold standard (human compressed) captions are obtained using AMT for 1K images.", "labels": [], "entities": [{"text": "AMT", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.6571118831634521}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Strict matching gives credit only to identical words between the gold-standard caption and the automatically produced caption.", "labels": [], "entities": []}, {"text": "However, words in the original caption of the query image (and its compressed caption) do not overlap exactly with words in the retrieved captions, even when they are semantically very close, which makes it hard to see improvements even when the captions of the new corpus are more general and transferable over other images.", "labels": [], "entities": []}, {"text": "Therefore, we also report scores based on semantic matching, which gives partial credits to word pairs based on their lexical similarity.", "labels": [], "entities": [{"text": "semantic matching", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.6955996155738831}]}, {"text": "The best performing approach with semantic matching is VISUAL (with LM = Image corpus), improving BLEU, Precision, F-score substantially over those of ORIG, demonstrating the extrinsic utility of our newly generated image-text parallel corpus in comparison to the original database.", "labels": [], "entities": [{"text": "semantic matching", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7076754122972488}, {"text": "VISUAL", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.5977516174316406}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9994864463806152}, {"text": "Precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9921686053276062}, {"text": "F-score", "start_pos": 115, "end_pos": 122, "type": "METRIC", "confidence": 0.963240385055542}]}, {"text": "shows an example of caption transfer.", "labels": [], "entities": [{"text": "caption transfer", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.9728104174137115}]}], "tableCaptions": [{"text": " Table 3: Image Description Transfer: performance in BLEU and F1 with strict & semantic matching.", "labels": [], "entities": [{"text": "Image Description Transfer", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.8721198042233785}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9896871447563171}, {"text": "F1", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.959170937538147}]}]}