{"title": [{"text": "Measuring semantic content in distributional vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "Some words are more contentful than others: for instance, make is intuitively more general than produce and fifteen is more 'precise' than a group.", "labels": [], "entities": []}, {"text": "In this paper, we propose to measure the 'semantic con-tent' of lexical items, as modelled by distributional representations.", "labels": [], "entities": []}, {"text": "We investigate the hypothesis that semantic content can be computed using the Kullback-Leibler (KL) divergence, an information-theoretic measure of the relative entropy of two distributions.", "labels": [], "entities": []}, {"text": "Ina task focus-ing on retrieving the correct ordering of hyponym-hypernym pairs, the KL divergence achieves close to 80% precision but does not outperform a simpler (linguis-tically unmotivated) frequency measure.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.5300494432449341}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9992184638977051}]}, {"text": "We suggest that this result illustrates the rather 'intensional' aspect of distributions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional semantics is a representation of lexical meaning that relies on a statistical analysis of the way words are used in corpora.", "labels": [], "entities": []}, {"text": "In this framework, the semantics of a lexical item is accounted for by modelling its co-occurrence with other words (or any larger lexical context).", "labels": [], "entities": []}, {"text": "The representation of a target word is thus a vector in a space where each dimension corresponds to a possible context.", "labels": [], "entities": []}, {"text": "The weights of the vector components can take various forms, ranging from simple co-occurrence frequencies to functions such as Pointwise Mutual Information (for an overview, see).", "labels": [], "entities": []}, {"text": "This paper investigates the issue of computing the semantic content of distributional vectors.", "labels": [], "entities": [{"text": "computing the semantic content of distributional vectors", "start_pos": 37, "end_pos": 93, "type": "TASK", "confidence": 0.8006420135498047}]}, {"text": "That is, we look at the ways we can distributionally express that make is a more general verb than produce, which is itself more general than, for instance, weave.", "labels": [], "entities": []}, {"text": "Although the task is related to the identification of hyponymy relations, it aims to reflect a more encompassing phenomenon: we wish to be able to compare the semantic content of words within parts-of-speech where the standard notion of hyponymy does not apply (e.g. prepositions: see with vs. next to or of vs. concerning) and across parts-of-speech (e.g. fifteen vs. group).", "labels": [], "entities": []}, {"text": "The hypothesis we will put forward is that semantic content is related to notions of relative entropy found in information theory.", "labels": [], "entities": []}, {"text": "More specifically, we hypothesise that the more specific a word is, the more the distribution of the words co-occurring with it will differ from the baseline distribution of those words in the language as a whole.", "labels": [], "entities": []}, {"text": "(A more intuitive way to phrase this is that the more specific a word is, the more information it gives us about which other words are likely to occur near it.)", "labels": [], "entities": []}, {"text": "The specific measure of difference that we will use is the Kullback-Leibler divergence of the distribution of words co-ocurring with the target word against the distribution of those words in the language as a whole.", "labels": [], "entities": []}, {"text": "We evaluate our hypothesis against a subset of the WordNet hierarchy (given by), relying on the intuition that in a hyponym-hypernym pair, the hyponym should have higher semantic content than its hypernym.", "labels": [], "entities": [{"text": "WordNet hierarchy", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.9116913378238678}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first define our notion of semantic content and motivate the need for measuring semantic content in distributional setups.", "labels": [], "entities": []}, {"text": "We then describe the implementation of the distributional system we use in this paper, emphasising our choice of weighting measure.", "labels": [], "entities": []}, {"text": "We show that, using the compo-nents of the described weighting measure, which are both probability distributions, we can calculate the relative entropy of a distribution by inserting those probability distributions in the equation for the Kullback-Leibler (KL) divergence.", "labels": [], "entities": []}, {"text": "We finally evaluate the KL measure against a basic notion of frequency and conclude with some error analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "In Section 2, we defined semantic content as a notion encompassing various referential properties, including a basic concept of extension in cases where it is applicable.", "labels": [], "entities": []}, {"text": "However, we do not know of a dataset providing human judgements over the general informativeness of lexical items.", "labels": [], "entities": []}, {"text": "So in order to evaluate our proposed measure, we investigate its ability to retrieve the right ordering of hyponym pairs, which can be considered a subset of the issue at hand.", "labels": [], "entities": []}, {"text": "Our assumption is that if X is a hypernym of Y , then the information content in X will be lower than in Y (because it has a more 'general' meaning).", "labels": [], "entities": []}, {"text": "So, given a pair of words {w 1 , w 2 } in a known hyponymy relation, we should be able to tell which of w 1 or w 2 is the hypernym by computing the respective KL divergences.", "labels": [], "entities": []}, {"text": "We use the hypernym data provided by (Baroni et al, 2012) as testbed for our experiment.", "labels": [], "entities": []}, {"text": "This set of hyponym-hypernym pairs contains 1385 instances retrieved from the WordNet hierarchy.", "labels": [], "entities": [{"text": "WordNet hierarchy", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.9369819462299347}]}, {"text": "Before running our system on the data, we make slight modifications to it.", "labels": [], "entities": []}, {"text": "First, as our distributions are created over the British National Corpus, some spellings must be converted to British English: for instance, color is replaced by colour.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 49, "end_pos": 72, "type": "DATASET", "confidence": 0.9409237504005432}]}, {"text": "Second, five of the nouns included in the test set are not in the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.8074238896369934}]}, {"text": "Those nouns are brethren, intranet, iPod, webcam and IX.", "labels": [], "entities": []}, {"text": "We remove the pairs containing those words from the data.", "labels": [], "entities": []}, {"text": "Third, numbers such as eleven or sixty are present in the Baroni et al set as nouns, but not in the BNC.", "labels": [], "entities": [{"text": "Baroni et al set", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.7177190706133842}, {"text": "BNC", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.9578237533569336}]}, {"text": "Pairs containing seven such numbers are therefore also removed from the data.", "labels": [], "entities": []}, {"text": "Finally, we encounter tagging issues with three words, which we match to their BNC equivalents: acoustics and annals are matched to acoustic and annal, and trouser to trousers.", "labels": [], "entities": [{"text": "BNC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8272731304168701}]}, {"text": "These modifications result in a test set of 1279 remaining pairs.", "labels": [], "entities": []}, {"text": "We then calculate both the self-information measure and the KL divergence of all terms in-1 Note that KL divergence is not symmetric: The latter is inferior as a few very small values of p(ci|t) can have an inappropriately large effect on it.", "labels": [], "entities": []}, {"text": "The data is available at http://clic.cimec.", "labels": [], "entities": []}, {"text": "unitn.it/Files/PublicData/eacl2012-data. zip.", "labels": [], "entities": []}, {"text": "cluded in our test set.", "labels": [], "entities": []}, {"text": "In order to evaluate the system, we record whether the calculated entropies match the order of each hypernym-hyponym pair.", "labels": [], "entities": []}, {"text": "That is, we count a pair as correctly represented by our system if w 1 is a hypernym of w 2 and KL(w 1 ) < KL(w 2 ) (or, in the case of the baseline, SI(w 1 ) < SI(w 2 ) where SI is selfinformation).", "labels": [], "entities": []}, {"text": "Self-information obtains 80.8% precision on the task, with the KL divergence lagging a little behind with 79.4% precision (the difference is not significant).", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9991288781166077}, {"text": "KL divergence", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.8097639381885529}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9976420402526855}]}, {"text": "In other terms, both measures perform comparably.", "labels": [], "entities": []}, {"text": "We analyse potential reasons for this disappointing result in the next section.", "labels": [], "entities": []}], "tableCaptions": []}