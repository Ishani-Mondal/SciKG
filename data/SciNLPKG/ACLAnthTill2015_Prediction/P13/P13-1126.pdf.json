{"title": [{"text": "Vector Space Model for Adaptation in Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.744543214639028}]}], "abstractContent": [{"text": "This paper proposes anew approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7482379972934723}, {"text": "statistical machine translation (SMT)", "start_pos": 58, "end_pos": 95, "type": "TASK", "confidence": 0.7792513916889826}]}, {"text": "The general idea is first to create a vector profile for the in-domain development (\"dev\") set.", "labels": [], "entities": []}, {"text": "This profile might, for instance, be a vector with a di-mensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular sub-corpus to all the phrase pairs that can be extracted from the dev set.", "labels": [], "entities": []}, {"text": "Then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 137, "end_pos": 153, "type": "METRIC", "confidence": 0.9458690285682678}]}, {"text": "Thus, we obtain a decoding feature whose value represents the phrase pair's closeness to the dev.", "labels": [], "entities": []}, {"text": "This is a simple, computationally cheap form of instance weighting for phrase pairs.", "labels": [], "entities": []}, {"text": "Experiments on large scale NIST evaluation data show improvements over strong base-lines: +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline, and significant improvements inmost circumstances over baselines with linear mixture model adaptation.", "labels": [], "entities": [{"text": "NIST evaluation data", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.7249528169631958}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9980792999267578}, {"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9904097318649292}]}, {"text": "An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre.", "labels": [], "entities": [{"text": "VSM adaptation", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.9769702553749084}]}], "introductionContent": [{"text": "The translation models of a statistical machine translation (SMT) system are trained on parallel data.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 28, "end_pos": 65, "type": "TASK", "confidence": 0.7760706841945648}]}, {"text": "Usage of language and therefore the best translation practice differs widely across genres, topics, and dialects, and even depends on a particular author's or publication's style; the word \"domain\" is often used to indicate a particular combination of all these factors.", "labels": [], "entities": []}, {"text": "Unless there is a perfect match between the training data domain and the (test) domain in which the SMT system will be used, one can often get better performance by adapting the system to the test domain.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9827541708946228}]}, {"text": "Domain adaptation is an active topic in the natural language processing (NLP) research community.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.824457198381424}, {"text": "natural language processing (NLP)", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.736308773358663}]}, {"text": "Its application to SMT systems has recently received considerable attention.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9965885877609253}]}, {"text": "Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc.", "labels": [], "entities": [{"text": "SMT model adaptation", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.9406779408454895}, {"text": "data selection", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.7427316606044769}, {"text": "phrase sense disambiguation", "start_pos": 144, "end_pos": 171, "type": "TASK", "confidence": 0.7108794649442037}]}, {"text": "Research on mixture models has considered both linear and log-linear mixtures.", "labels": [], "entities": []}, {"text": "Both were studied in, which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly.", "labels": [], "entities": []}, {"text": "(, instead, opted for combining the sub-models directly in the SMT log-linear framework.", "labels": [], "entities": [{"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9719781875610352}]}, {"text": "In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9617272019386292}]}, {"text": "The resulting bilingual sentence pairs are then used as additional training data (.", "labels": [], "entities": []}, {"text": "Data selection approaches () search for bilingual sentence pairs that are similar to the indomain \"dev\" data, then add them to the training data.", "labels": [], "entities": [{"text": "Data selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6741950064897537}]}, {"text": "Instance weighting approaches () 1285 typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level.", "labels": [], "entities": []}, {"text": "For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights.", "labels": [], "entities": []}, {"text": "The 2012 JHU workshop on Domain Adaptation for MT 1 proposed phrase sense disambiguation (PSD) for translation model adaptation.", "labels": [], "entities": [{"text": "JHU workshop on Domain Adaptation for MT 1", "start_pos": 9, "end_pos": 51, "type": "TASK", "confidence": 0.7340604811906815}, {"text": "phrase sense disambiguation (PSD)", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.8108290632565817}, {"text": "translation model adaptation", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.9222729603449503}]}, {"text": "In this approach, the context of a phrase helps the system to find the appropriate translation.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew instance weighting approach to domain adaptation based on a vector space model (VSM).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7400813102722168}]}, {"text": "As in, this approach works at the level of phrase pairs.", "labels": [], "entities": []}, {"text": "However, the VSM approach is simpler and more straightforward.", "labels": [], "entities": [{"text": "VSM", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.83018559217453}]}, {"text": "Instead of using word-based features and a computationally expensive training procedure, we capture the distributional properties of each phrase pair directly, representing it as a vector in a space which also contains a representation of the dev set.", "labels": [], "entities": []}, {"text": "The similarity between a given phrase pair's vector and the dev set vector becomes a feature for the decoder.", "labels": [], "entities": []}, {"text": "It rewards phrase pairs that are in some sense closer to those found in the dev set, and punishes the rest.", "labels": [], "entities": []}, {"text": "In initial experiments, we tried three different similarity functions: Bhattacharyya coefficient, Jensen-Shannon divergency, and cosine measure.", "labels": [], "entities": [{"text": "Bhattacharyya coefficient", "start_pos": 71, "end_pos": 96, "type": "METRIC", "confidence": 0.9621232450008392}, {"text": "divergency", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9217332601547241}]}, {"text": "They all enabled VSM adaptation to beat the non-adaptive baseline, but Bhattacharyya similarity worked best, so we adopted it for the remaining experiments.", "labels": [], "entities": [{"text": "VSM adaptation", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.9792381823062897}, {"text": "Bhattacharyya similarity", "start_pos": 71, "end_pos": 95, "type": "METRIC", "confidence": 0.9101858139038086}]}, {"text": "The vector space used by VSM adaptation can be defined in various ways.", "labels": [], "entities": [{"text": "VSM adaptation", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.944216251373291}]}, {"text": "In the experiments described below, we chose a definition that measures the contribution (to counts of a given phrase pair, or to counts of all phrase pairs in the dev set) of each training subcorpus.", "labels": [], "entities": []}, {"text": "Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in, in that both approaches rely on information about the subcorpora from which the data originate.", "labels": [], "entities": [{"text": "VSM adaptation", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.9769107103347778}]}, {"text": "However, a key difference is that in this paper we explicitly capture each phrase pair's distribution across subcorpora, and compare it to the aggregated distribution of phrase pairs in the dev set.", "labels": [], "entities": []}, {"text": "In mixture models, a phrase pair's distribu-tion across subcorpora is captured only implicitly, by probabilities that reflect the prevalence of the pair within each subcorpus.", "labels": [], "entities": []}, {"text": "Thus, VSM adaptation occurs at a much finer granularity than mixture model adaptation.", "labels": [], "entities": [{"text": "VSM adaptation", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.9451594948768616}]}, {"text": "More fundamentally, there is nothing about the VSM idea that obliges us to define the vector space in terms of subcorpora.", "labels": [], "entities": []}, {"text": "For instance, we could cluster the words in the source language into S clusters, and the words in the target language into T clusters.", "labels": [], "entities": []}, {"text": "Then, treating the dev set and each phrase pair as a pair of bags of words (a source bag and a target bag) one could represent each as a vector of dimension S + T, with entries calculated from the counts associated with the S + T clusters (in away similar to that described for phrase pairs below).", "labels": [], "entities": []}, {"text": "The (dev, phrase pair) similarity would then be independent of the subcorpora.", "labels": [], "entities": []}, {"text": "One can think of several other ways of defining the vector space that might yield even better results than those reported here.", "labels": [], "entities": []}, {"text": "Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments.", "labels": [], "entities": [{"text": "VSM adaptation", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.9692577123641968}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: NIST Chinese-English data. In the  genres column: nw=newswire, bc=broadcast  conversation, bn=broadcast news, wl=weblog,  ng=newsgroup, un=UN proc., bng = bn & ng.", "labels": [], "entities": [{"text": "NIST Chinese-English data", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.9386603434880575}]}, {"text": " Table 3: Comparison of different similarity func- tions. * and ** denote significant gains over the  baseline at p < 0.05 and p < 0.01 levels, respec- tively.", "labels": [], "entities": []}, {"text": " Table 4: Results for variants of adaptation.", "labels": [], "entities": []}, {"text": " Table 5: Results for adaptation based on joint or  maginal counts.", "labels": [], "entities": [{"text": "adaptation", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.9710853099822998}]}, {"text": " Table 5. How do these new re- sults affect the VSM vs. linear mixture compari- son? Naturally, the conclusions for Arabic don't  change. For Chinese, 3-feature VSM is now su- perior to linear mixture at p < 0.01 on NIST06  test set, but 3-feature VSM still doesn't have a sta- tistically significant edge over linear mixture on  NIST08 test set. A fair summary would be that 3- feature VSM adaptation is decisively superior to  linear mixture adaptation for Arabic to English,  and highly competitive with linear mixture adap-", "labels": [], "entities": [{"text": "NIST06  test set", "start_pos": 216, "end_pos": 232, "type": "DATASET", "confidence": 0.9892771442731222}, {"text": "NIST08 test set", "start_pos": 330, "end_pos": 345, "type": "DATASET", "confidence": 0.9895544250806173}]}, {"text": " Table 6: Results of combining VSM and linear mixture adaptation. \"lin-lm\" is linear language model  adaptation, \"lin-tm\" is linear translation model adaptation. * and ** denote significant gains over the row  \"no vsm\" at p < 0.05 and p < 0.01 levels, respectively.", "labels": [], "entities": [{"text": "linear mixture adaptation", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6444418827692667}]}]}