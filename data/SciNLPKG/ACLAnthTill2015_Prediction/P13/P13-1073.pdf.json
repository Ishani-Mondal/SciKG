{"title": [{"text": "Generating Synthetic Comparable Questions for News Articles", "labels": [], "entities": [{"text": "Generating Synthetic Comparable Questions", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7380579560995102}]}], "abstractContent": [{"text": "We introduce the novel task of automatically generating questions that are relevant to a text but do not appear in it.", "labels": [], "entities": []}, {"text": "One motivating example of its application is for increasing user engagement around news articles by suggesting relevant comparable questions, such as \"is Beyonce a better singer than Madonna?\", for the user to answer.", "labels": [], "entities": []}, {"text": "We present the first algorithm for the task, which consists of: (a) of-fline construction of a comparable question template database; (b) ranking of relevant templates to a given article; and (c) instantiation of templates only with entities in the article whose comparison under the template's relation makes sense.", "labels": [], "entities": []}, {"text": "We tested the suggestions generated by our algorithm via a Mechanical Turk experiment , which showed a significant improvement over the strongest baseline of more than 45% in all metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "For companies whose revenues are mainly adbased, e.g. Facebook, Google and Yahoo, increasing user engagement is an important goal, leading to more time spent onsite and consequently to increased exposure to ads.", "labels": [], "entities": []}, {"text": "Examples for typical engaging content include other articles for the user to read, updates from the user's social neighborhood and votes or comments on videos, blogs etc.", "labels": [], "entities": []}, {"text": "In this paper we propose anew way to increase user engagement around news articles, namely suggesting questions for the user to answer, which are related to the viewed article.", "labels": [], "entities": []}, {"text": "Our motivation is that there are questions that are \"irresistible\" because they are fun, involve emotional reaction and expect simple answers.", "labels": [], "entities": []}, {"text": "These are comparative questions, such as \"is Beyonce a better singer than Madonna?\", \"who is better looking, Brad Pitt or George Clooney?\", \"who is faster: Superman or Flash?\" and \"which camera brand do you prefer: Canon or Nikon?\"", "labels": [], "entities": []}, {"text": "Furthermore, such questions are social in nature since users would be interested in reading the opinions of other users, similar to viewing other comments (.", "labels": [], "entities": []}, {"text": "Hence, a user that provided an answer may return to view other answers, further increasing her engagement with the site.", "labels": [], "entities": []}, {"text": "One approach for generating comparable questions would be to employ traditional question generation, which syntactically transform assertions in a given text into questions (.", "labels": [], "entities": [{"text": "question generation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.734898254275322}]}, {"text": "Sadly, fun and engaging comparative questions are typically not found within the text of news articles.", "labels": [], "entities": []}, {"text": "A different approach would be to find concrete relevant questions within external collections of manually generated comparable questions.", "labels": [], "entities": []}, {"text": "Such collections include Community-based Question Answering (CQA) sites such as Yahoo!", "labels": [], "entities": [{"text": "Question Answering (CQA)", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.8072849690914154}]}, {"text": "Answers and Baidu Zhidao and sites that are specialized in polls, such as Toluna.", "labels": [], "entities": [{"text": "Answers", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8468269109725952}, {"text": "Baidu Zhidao", "start_pos": 12, "end_pos": 24, "type": "DATASET", "confidence": 0.8570780754089355}, {"text": "Toluna", "start_pos": 74, "end_pos": 80, "type": "DATASET", "confidence": 0.9433214068412781}]}, {"text": "However, it is highly unlikely that such sources will contain enough relevant questions for any news article due to typical sparseness issues as well as differences in interests between askers in CQA sites and news reporters.", "labels": [], "entities": []}, {"text": "To better address the motivating application above, we propose the novel task of automatically suggesting comparative questions that are relevant to a given input news article but do not appear in it.", "labels": [], "entities": []}, {"text": "To achieve broad coverage for our task, we present an algorithm that generates synthetic concrete questions from question templates, such as \"Who is a better actor: #1 or #2?\".", "labels": [], "entities": []}, {"text": "Our algorithm consists of two parts.", "labels": [], "entities": []}, {"text": "An offline part constructs a database of comparative question templates that appear in a large question corpus.", "labels": [], "entities": []}, {"text": "For a given news article, an online part chooses relevant tem-: An example news article from OMG!", "labels": [], "entities": [{"text": "OMG!", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.9329519271850586}]}, {"text": "plates for the article by matching between the article content and typical template contexts.", "labels": [], "entities": []}, {"text": "The algorithm then instantiates each relevant template with two entities that appear in the article.", "labels": [], "entities": []}, {"text": "Yet, fora given template, only some of the entities are plausible slot fillers.", "labels": [], "entities": [{"text": "slot fillers", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.8485099375247955}]}, {"text": "For example, 'Madonna' is not a reasonable filler for \"Who is a better dad, #1 or #2?\".", "labels": [], "entities": []}, {"text": "Thus, our algorithm employs entity filtering to exclude candidate instantiations that do not make sense.", "labels": [], "entities": [{"text": "entity filtering", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7442038059234619}]}, {"text": "To test the performance of our algorithm, we conducted a Mechanical Turk experiment that assessed the quality of suggested questions for news articles on celebrities.", "labels": [], "entities": []}, {"text": "We compared our algorithm to a random baseline and to a partial version of our algorithm that includes a template relevance component but lacks filtering of candidate instantiations.", "labels": [], "entities": []}, {"text": "The results show that the full algorithm provided 45% more correct instantiations, but surprisingly also 46% more relevant suggestions compared to the stronger baseline.", "labels": [], "entities": []}, {"text": "These results point at the importance of both picking relevant templates and smart instantiation selection to the quality of generated questions.", "labels": [], "entities": []}, {"text": "In addition, they indicate that user perception of relevance is affected by the correctness of the question.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our algorithm's performance, we designed a Mechanical Turk (MTurk) experiment in which human annotators assess the quality of the questions that our algorithm generates fora sample of news articles.", "labels": [], "entities": []}, {"text": "As the source of test articles, we chose the OMG!", "labels": [], "entities": [{"text": "OMG!", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.9179584681987762}]}, {"text": "website , which contains news articles on celebrities.", "labels": [], "entities": []}, {"text": "Test articles were selected by first randomly sampling 5,000 news article from those that were posted on OMG!", "labels": [], "entities": [{"text": "OMG!", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.9532202780246735}]}, {"text": "We then filtered out articles that are longer than 4,000 characters, which were found to be tiresome for annotators to read, and those that are shorter than 300 characters, which consist mainly of video and photos.", "labels": [], "entities": []}, {"text": "We were left with a pool of 1,016 articles from which we randomly sampled 100 as the test set.", "labels": [], "entities": []}, {"text": "For each test article our algorithm obtained the top three relevant comparable relations, and for each relation selected the best instantiation (if exists).", "labels": [], "entities": []}, {"text": "We used two baselines for performance comparison.", "labels": [], "entities": []}, {"text": "The first random baseline chooses a relation randomly out of all possible relations in the database and then instantiates it with a random pair of entities that appear in the article.", "labels": [], "entities": []}, {"text": "The second relevance baseline chooses the most relevant Relevance Correctness Random baseline 29% 43% Relevance baseline 37% 53% Full algorithm 54% 77%: Relevance and correctness percentage by tested algorithm relation to the article based on our algorithm, but still instantiates it with a random pair.", "labels": [], "entities": [{"text": "Relevance", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9716268181800842}]}, {"text": "For each test article, we presented to the evaluators the questions generated by the three tested algorithms in a random order to avoid any bias.", "labels": [], "entities": []}, {"text": "We note that our second baseline enabled us to measure the stand-alone contribution of the LDA-based relevance model.", "labels": [], "entities": [{"text": "LDA-based relevance", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.6238337755203247}]}, {"text": "In addition, it enabled us to measure the relative contribution of the instantiation models on top of relevance model.", "labels": [], "entities": []}, {"text": "Each article was evaluated by 10 MTurk workers, which were asked to mark for each displayed question whether it is relevant and whether it is correct (see Section 2 for relevance and correctness definitions).", "labels": [], "entities": []}, {"text": "The workers were given precise instructions along with examples before they started the test.", "labels": [], "entities": []}, {"text": "A control story was used to filter out dishonest or incapable workers 5 .", "labels": [], "entities": []}], "tableCaptions": []}