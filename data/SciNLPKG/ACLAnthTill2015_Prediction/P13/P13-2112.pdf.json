{"title": [{"text": "Simpler unsupervised POS tagging with bilingual projections", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.805315226316452}]}], "abstractContent": [{"text": "We present an unsupervised approach to part-of-speech tagging based on projections of tags in a word-aligned bilingual parallel corpus.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6697383671998978}]}, {"text": "In contrast to the existing state-of-the-art approach of Das and Petrov, we have developed a substantially simpler method by automatically identifying \"good\" training sentences from the parallel corpus and applying self-training.", "labels": [], "entities": []}, {"text": "In experimental results on eight languages, our method achieves state-of-the-art results .", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Using parallel data from Europarl ( we apply our method to build taggers for the same eight target languages as -Danish, Dutch, German, Greek, Italian, Portuguese, Spanish and Swedish -with English as the source language.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.9801778197288513}]}, {"text": "Our training data (Europarl) is a subset of the training data of Das and Petrov (who also used the ODS United Nations dataset which we were unable to obtain).", "labels": [], "entities": [{"text": "training data (Europarl)", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.635471397638321}, {"text": "ODS United Nations dataset", "start_pos": 99, "end_pos": 125, "type": "DATASET", "confidence": 0.9107962101697922}]}, {"text": "The evaluation metric and test data are the same as that used by Das and Petrov.", "labels": [], "entities": []}, {"text": "Our results are comparable to theirs, although our system is penalized by having less training data.", "labels": [], "entities": []}, {"text": "We tag the source language with the Stanford POS tagger (: Overall accuracy, accuracy on known tokens, accuracy on unknown tokens, and proportion of known tokens for Italian (left) and Dutch (right).", "labels": [], "entities": [{"text": "Stanford POS tagger", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.845300535360972}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9962373971939087}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9987382292747498}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9987995624542236}]}, {"text": "shows results for our seed model, self training and revision, and the results reported by Das and Petrov.", "labels": [], "entities": []}, {"text": "Self training and revision improve the accuracy for every language over the seed model, and gives an average improvement of roughly two percentage points.", "labels": [], "entities": [{"text": "revision", "start_pos": 18, "end_pos": 26, "type": "TASK", "confidence": 0.8385885953903198}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9993706345558167}]}, {"text": "The average accuracy of self training and revision is on par with that reported by Das and Petrov.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9998030066490173}, {"text": "revision", "start_pos": 42, "end_pos": 50, "type": "TASK", "confidence": 0.8791627883911133}]}, {"text": "On individual languages, self training and revision and the method of Das and Petrov are split -each performs better on half of the cases.", "labels": [], "entities": []}, {"text": "Interestingly, our method achieves higher accuracies on Germanic languages -the family of our source language, English -while Das and Petrov perform better on Romance languages.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9932277798652649}]}, {"text": "This might be because our model relies on alignments, which might be more accurate for more-related languages, whereas Das and Petrov additionally rely on label propagation.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 155, "end_pos": 172, "type": "TASK", "confidence": 0.7485919892787933}]}, {"text": "Compared to Das and Petrov, our model performs poorest on Italian, in terms of percentage point difference inaccuracy.", "labels": [], "entities": []}, {"text": "shows accuracy, accuracy on known words, accuracy on unknown words, and proportion of known tokens for each iteration of our model for Italian; iteration 0 is the seed model, and iteration 31 is the final model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9993165731430054}, {"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9985036849975586}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9985746145248413}]}, {"text": "Our model performs poorly on unknown words as indicated by the low accuracy on unknown words, and high accuracy on known words compared to the overall accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9978345036506653}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9981732368469238}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9963954091072083}]}, {"text": "The poor performance on unknown words is expected because we do not use any language-specific rules to handle this case.", "labels": [], "entities": []}, {"text": "Moreover, on average for the final model, approximately 10% of the test data tokens are unknown.", "labels": [], "entities": []}, {"text": "One way to improve the performance of our tagger might be to reduce the proportion of unknown words by using a larger training corpus, as Das and Petrov did.", "labels": [], "entities": []}, {"text": "We examine the impact of self-training and revision over training iterations.", "labels": [], "entities": []}, {"text": "We find that for all languages, accuracy rises quickly in the first 5-6 iterations, and then subsequently improves only slightly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9994415640830994}]}, {"text": "We exemplify this in (right panel) for Dutch.", "labels": [], "entities": []}, {"text": "(Findings are similar for other languages.)", "labels": [], "entities": []}, {"text": "Although accuracy does not increase much in later iterations, they may still have some benefit as the vocabulary size continues to grow.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9990905523300171}]}], "tableCaptions": [{"text": " Table 2: Token-level POS tagging accuracy for our seed model, self training and revision, and the method  of Das and Petrov (2011). The best results on each language, and on average, are shown in bold.", "labels": [], "entities": [{"text": "Token-level POS tagging", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.6954562465349833}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.977009654045105}]}]}