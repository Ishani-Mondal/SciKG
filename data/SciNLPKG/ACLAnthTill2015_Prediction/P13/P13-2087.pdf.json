{"title": [], "abstractContent": [{"text": "We present a fast method for re-purposing existing semantic word vectors to improve performance in a supervised task.", "labels": [], "entities": []}, {"text": "Recently , with an increase in computing resources , it became possible to learn rich word embeddings from massive amounts of unlabeled data.", "labels": [], "entities": []}, {"text": "However, some methods take days or weeks to learn good em-beddings, and some are notoriously difficult to train.", "labels": [], "entities": []}, {"text": "We propose a method that takes as input an existing embedding, some labeled data, and produces an embedding in the same space, but with a better predictive performance in the supervised task.", "labels": [], "entities": []}, {"text": "We show improvement on the task of sentiment classification with respect to several baselines, and observe that the approach is most useful when the training set is sufficiently small.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.96297287940979}]}], "introductionContent": [{"text": "Incorporating the vector representation of a word as a feature, has recently been shown to benefit performance in several standard NLP tasks such as language modeling (, POS-tagging and NER), parsing, as well as in sentiment and subjectivity analysis tasks).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 149, "end_pos": 166, "type": "TASK", "confidence": 0.7366762161254883}, {"text": "parsing", "start_pos": 192, "end_pos": 199, "type": "TASK", "confidence": 0.9743220806121826}, {"text": "sentiment and subjectivity analysis tasks", "start_pos": 215, "end_pos": 256, "type": "TASK", "confidence": 0.8279879450798034}]}, {"text": "Real-valued word vectors mitigate sparsity by \"smoothing\" relevant semantic insight gained during the unsupervised training over the rare and unseen terms in the training data.", "labels": [], "entities": []}, {"text": "To be effective, these word-representations -and the process by which they are assigned to the words (i.e. embedding) -should capture the semantics relevant to the task.", "labels": [], "entities": []}, {"text": "We might, for example, consider dramatic (term X) and pleasant (term Y) to correlate with a review of a good movie (task A), while finding them of opposite polarity in the context of a dating profile (task B).", "labels": [], "entities": []}, {"text": "Consequently, good vectors for X and Y should yield an inner product close to 1 in the context of task A, and \u22121 in the context of task B.", "labels": [], "entities": []}, {"text": "Moreover, we may already have on our hands embeddings for X and Y obtained from yet another (possibly unsupervised) task (C), in which X and Y are, for example, orthogonal.", "labels": [], "entities": []}, {"text": "If the embeddings for task C happen to be learned from a much larger dataset, it would make sense to reuse task C embeddings, but adapt them for task A and/or task B.", "labels": [], "entities": []}, {"text": "We will refer to task C and its embeddings as the source task and the source embeddings, and task A/B, and its embeddings as the target task and the target embeddings.", "labels": [], "entities": []}, {"text": "Traditionally, we would learn the embeddings for the target task jointly with whatever unlabeled data we may have, in an instance of semisupervised learning, and/or we may leverage labels from multiple other related tasks in a multitask approach.", "labels": [], "entities": []}, {"text": "Both methods have been applied successfully to learn task-specific embeddings.", "labels": [], "entities": []}, {"text": "But while joint training is highly effective, a downside is that a large amount of data (and processing time) is required a-priori.", "labels": [], "entities": []}, {"text": "In the case of deep neural embeddings, for example, training time can number in days.", "labels": [], "entities": []}, {"text": "On the other hand, learned embeddings are becoming more abundant, as much research and computing effort is being invested in learning word representations using large-scale deep architectures trained on web-scale corpora.", "labels": [], "entities": []}, {"text": "Many of said embeddings are published and can be harnessed in their raw form as additional features in a number of supervised tasks.", "labels": [], "entities": []}, {"text": "It would, thus, be advantageous to learn a task-specific embedding directly from another (source) embedding.", "labels": [], "entities": []}, {"text": "In this paper we propose a fast method for reembedding words from a source embedding S to a target embedding T by performing unconstrained optimization of a convex objective.", "labels": [], "entities": []}, {"text": "Our objective is a linear combination of the dataset's log-likelihood under the target embedding and the Frobenius norm of the distortion matrix -a matrix of component-wise differences between the target and the source embeddings.", "labels": [], "entities": []}, {"text": "The latter acts as a regularizer that penalizes the Euclidean distance between the source and target embeddings.", "labels": [], "entities": []}, {"text": "The method is much faster than joint training and yields competitive results with several baselines.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.7926793098449707}, {"text": "IMDB  movie review dataset", "start_pos": 58, "end_pos": 84, "type": "DATASET", "confidence": 0.9307859838008881}]}]}