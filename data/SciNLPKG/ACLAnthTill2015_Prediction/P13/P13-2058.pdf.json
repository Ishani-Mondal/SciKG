{"title": [{"text": "Task Alternation in Parallel Sentence Retrieval for Twitter Translation", "labels": [], "entities": [{"text": "Parallel Sentence Retrieval", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.6083859900633494}, {"text": "Twitter Translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.6682222932577133}]}], "abstractContent": [{"text": "We present an approach to mine comparable data for parallel sentences using translation-based cross-lingual information retrieval (CLIR).", "labels": [], "entities": []}, {"text": "By iteratively alternating between the tasks of retrieval and translation, an initial general-domain model is allowed to adapt to in-domain data.", "labels": [], "entities": [{"text": "translation", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.9293528199195862}]}, {"text": "Adaptation is done by training the translation system on a few thousand sentences retrieved in the step before.", "labels": [], "entities": [{"text": "Adaptation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9801487326622009}]}, {"text": "Our setup is time-and memory-efficient and of similar quality as CLIR-based adaptation on millions of parallel sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) crucially relies on large amounts of bilingual data.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8434899151325226}]}, {"text": "Unfortunately sentence-parallel bilingual data are not always available.", "labels": [], "entities": []}, {"text": "Various approaches have been presented to remedy this problem by mining parallel sentences from comparable data, for example by using cross-lingual information retrieval (CLIR) techniques to retrieve a target language sentence fora source language sentence treated as a query.", "labels": [], "entities": [{"text": "cross-lingual information retrieval (CLIR)", "start_pos": 134, "end_pos": 176, "type": "TASK", "confidence": 0.7443598906199137}]}, {"text": "Most such approaches try to overcome the noise inherent in automatically extracted parallel data by sheer size.", "labels": [], "entities": []}, {"text": "However, finding good quality parallel data from noisy resources like Twitter requires sophisticated retrieval methods.", "labels": [], "entities": []}, {"text": "Running these methods on millions of queries and documents can take weeks.", "labels": [], "entities": []}, {"text": "Our method aims to achieve improvements similar to large-scale parallel sentence extraction approaches, while requiring only a fraction of the extracted data and considerably less computing resources.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7051088660955429}]}, {"text": "Our key idea is to extend a straightforward application of translation-based CLIR to an iterative method: Instead of attempting to retrieve in one step as many parallel sentences as possible, we allow the retrieval model to gradually adapt to new data by using an SMT model trained on the freshly retrieved sentence pairs in the translationbased retrieval step.", "labels": [], "entities": [{"text": "SMT", "start_pos": 264, "end_pos": 267, "type": "TASK", "confidence": 0.9508123993873596}]}, {"text": "We alternate between the tasks of translation-based retrieval of target sentences, and the task of SMT, by re-training the SMT model on the data that were retrieved in the previous step.", "labels": [], "entities": [{"text": "translation-based retrieval of target sentences", "start_pos": 34, "end_pos": 81, "type": "TASK", "confidence": 0.9291134476661682}, {"text": "SMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9947197437286377}, {"text": "SMT", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.9566365480422974}]}, {"text": "This task alternation is done iteratively until the number of newly added pairs stabilizes at a relatively small value.", "labels": [], "entities": []}, {"text": "In our experiments on Arabic-English Twitter translation, we achieved improvements of over 1 BLEU point over a strong baseline that uses indomain data for language modeling and parameter tuning.", "labels": [], "entities": [{"text": "Arabic-English Twitter translation", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.6298640072345734}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9994138479232788}]}, {"text": "Compared to a CLIR-approach which extracts more than 3 million parallel sentences from a noisy comparable corpus, our system produces similar results in terms of BLEU using only about 40 thousand sentences for training in each of a few iterations, thus being much more time-and resource-efficient.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9971393346786499}]}], "datasetContent": [{"text": "We simulated the full-scale retrieval approach by with the CLIR model described in section 3.", "labels": [], "entities": []}, {"text": "It took 14 days to run 5.5M Arabic queries on 3.7M English documents.", "labels": [], "entities": []}, {"text": "In contrast, our iterative approach completed a single iteration in less than 24 hours.", "labels": [], "entities": []}, {"text": "In the absence of a Twitter data set for retrieval, we selected the parameters \u03bb = 0.6 (eq.1), L = 0.005 and C = 0.95 in a mate-finding task on Wikipedia data.", "labels": [], "entities": [{"text": "Twitter data set", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.8108110825220743}, {"text": "Wikipedia data", "start_pos": 144, "end_pos": 158, "type": "DATASET", "confidence": 0.9367188513278961}]}, {"text": "The n-best list size for P nbest (t|q) was 1000.", "labels": [], "entities": []}, {"text": "All SMT models included a 5-gram language model built from the English side of the NIST data plus the English side of the Twitter corpus D trg . Word alignments were created using GIZA++.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9886536002159119}, {"text": "NIST data", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.992591142654419}, {"text": "Word alignments", "start_pos": 145, "end_pos": 160, "type": "TASK", "confidence": 0.6933773458003998}]}, {"text": "Rule extraction and parameter tuning (MERT) was carried outwith cdec, using standard features.", "labels": [], "entities": [{"text": "Rule extraction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8133759200572968}, {"text": "parameter tuning (MERT)", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.6042185723781586}]}, {"text": "We ran MERT 5 times per iteration, carrying over the weights which achieved median performance on the development set to the next iteration.", "labels": [], "entities": []}, {"text": "reports median BLEU scores on test of our standard adaptation baseline, the full-scale retrieval approach and the best result from our task alternation systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9994679093360901}]}, {"text": "Approximate randomization tests showed that improvements of full-scale retrieval and task alternation over the baseline were statis-tically significant.", "labels": [], "entities": []}, {"text": "Differences between full-scale retrieval and task alternation were not significant.", "labels": [], "entities": []}, {"text": "2 illustrates the impact of \u03b8, which controls the importance of the previous model compared to the current one, on median BLEU (a) and change of S in (b) over iterations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9397769570350647}]}, {"text": "For all \u03b8, few iterations suffice to reach or surpass full-scale retrieval performance.", "labels": [], "entities": []}, {"text": "Yet, no run achieved good performance after one iteration, showing that the transductive setup must be combined with task alternation to be effective.", "labels": [], "entities": []}, {"text": "While we see fluctuations in BLEU for all \u03b8-values, \u03b8 = 0.1 achieves high scores faster and more consistently, pointing towards selecting a bolder updating strategy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9989345669746399}]}, {"text": "This is also supported by plot (b), which indicates that choosing \u03b8 = 0.1 leads to faster stabilization in the pairs added per iteration (S in ).", "labels": [], "entities": [{"text": "plot (b)", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9365627020597458}]}, {"text": "We used this stabilization as a stopping criterion.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Standard Domain Adaptation with in-domain LM  and tuning; Full-scale CLIR yielding over 3M in-domain par- allel sentences; Task alternation (\u03b8 = 0.1, iteration 7) using  \u223c40k parallel sentences per iteration.", "labels": [], "entities": []}]}