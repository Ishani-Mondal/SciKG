{"title": [], "abstractContent": [{"text": "A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not.", "labels": [], "entities": []}, {"text": "We propose anew concept, sentiment relevance , to make this distinction and argue that it better reflects the requirements of sentiment analysis systems.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.824932187795639}]}, {"text": "We demonstrate experimentally that sentiment relevance and subjectivity are related, but different.", "labels": [], "entities": [{"text": "sentiment relevance", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8834228813648224}]}, {"text": "Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer.", "labels": [], "entities": [{"text": "sentiment relevance", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.803161233663559}, {"text": "knowledge transfer", "start_pos": 384, "end_pos": 402, "type": "TASK", "confidence": 0.7028649002313614}]}, {"text": "We show that both methods learn sentiment relevance classifiers that perform well.", "labels": [], "entities": [{"text": "sentiment relevance classifiers", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.8145232001940409}]}], "introductionContent": [{"text": "It is generally recognized in sentiment analysis that only a subset of the content of a document contributes to the sentiment it conveys.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9664584994316101}]}, {"text": "For this reason, some authors distinguish the categories subjective and objective.", "labels": [], "entities": []}, {"text": "Subjective statements refer to the internal state of mind of a person, which cannot be observed.", "labels": [], "entities": []}, {"text": "In contrast, objective statements can be verified by observing and checking reality.", "labels": [], "entities": []}, {"text": "Some sentiment analysis systems filter out objective language and predict sentiment based on subjective language only because objective statements do not directly reveal sentiment.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.8820644915103912}]}, {"text": "Even though the categories subjective/objective are well-established in philosophy, we argue that they are not optimal for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.969690591096878}]}, {"text": "We instead introduce the notion of sentiment relevance (S-relevance or SR for short).", "labels": [], "entities": []}, {"text": "A sentence or linguistic expression is S-relevant if it contains information about the sentiment the document conveys; it is S-nonrelevant (SNR) otherwise.", "labels": [], "entities": []}, {"text": "Ideally, we would like to have at our disposal a large annotated training set for our new concept of sentiment relevance.", "labels": [], "entities": [{"text": "sentiment relevance", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.9158088862895966}]}, {"text": "However, such a resource does not yet exist.", "labels": [], "entities": []}, {"text": "For this reason, we investigate two semi-supervised approaches to S-relevance classification that do not require Srelevance-labeled data.", "labels": [], "entities": [{"text": "S-relevance classification", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.946171373128891}]}, {"text": "The first approach is distant supervision (DS).", "labels": [], "entities": [{"text": "distant supervision (DS)", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.6329625487327576}]}, {"text": "We create an initial labeling based on domain-specific metadata that we extract from a public database and show that this improves performance by 5.8% F 1 compared to a baseline.", "labels": [], "entities": [{"text": "F 1", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9926665425300598}]}, {"text": "The second approach is transfer learning (TL).", "labels": [], "entities": [{"text": "transfer learning (TL)", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.9293776631355286}]}, {"text": "We show that TL improves F 1 by 12.6% for sentiment relevance classification when we use a feature representation based on lexical taxonomies that supports knowledge transfer.", "labels": [], "entities": [{"text": "TL", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.6905391216278076}, {"text": "F 1", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.992468386888504}, {"text": "sentiment relevance classification", "start_pos": 42, "end_pos": 76, "type": "TASK", "confidence": 0.9255737860997518}, {"text": "knowledge transfer", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.7541391551494598}]}, {"text": "In our approach, we classify sentences as S-(non)relevant because this is the most fine-grained level at which S-relevance manifests itself; at the word or phrase level, S-relevance classification is not possible because of scope and context effects.", "labels": [], "entities": [{"text": "S-relevance classification", "start_pos": 170, "end_pos": 196, "type": "TASK", "confidence": 0.8439335525035858}]}, {"text": "However, S-relevance is also a discourse phenomenon: authors tend to structure documents into S-relevant passages and S-nonrelevant passages.", "labels": [], "entities": []}, {"text": "To impose this discourse constraint, we employ a sequence model.", "labels": [], "entities": []}, {"text": "We represent each document as a graph of sentences and apply a minimum cut method.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the concept of sentiment relevance and relates it to subjectivity.", "labels": [], "entities": [{"text": "sentiment relevance", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8874398171901703}]}, {"text": "In Section 3, we review previous work related to sentiment relevance.", "labels": [], "entities": [{"text": "sentiment relevance", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.9509195983409882}]}, {"text": "Next, we describe the methods applied in this paper (Section 4) and the features we extract (Section 5).", "labels": [], "entities": []}, {"text": "Finally, we turn to the description and results of our experiments on distant supervision (Section 6) and transfer learning (Section 7).", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.9034863710403442}]}, {"text": "We end with a conclusion in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We will now examine the similarities of Srelevance and an existing subjectivity dataset.", "labels": [], "entities": [{"text": "Srelevance", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8288611769676208}]}, {"text": "introduced subjectivity data (henceforth P&L corpus) that consists of 5000 highly subjective (quote) review snippets from rottentomatoes.com and 5000 objective (plot) sentences from IMDb plot descriptions.", "labels": [], "entities": [{"text": "P&L corpus", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.7923732548952103}, {"text": "rottentomatoes.com", "start_pos": 122, "end_pos": 140, "type": "DATASET", "confidence": 0.9581469297409058}]}, {"text": "We now show that although the P&L selection criteria (quotes, plot) bear resemblance to the definition of S-relevance, the two concepts are different.", "labels": [], "entities": []}, {"text": "We use quote as S-relevant and plot as Snonrelevant data in TL.", "labels": [], "entities": []}, {"text": "We divide both the SR and P&L corpora into training (50%) and test sets (50%) and train a Maximum Entropy (MaxEnt) classifier) with bag-ofword features.", "labels": [], "entities": []}, {"text": "Macro-averaged F 1 for the four possible training-test combinations is shown in Table 1.", "labels": [], "entities": [{"text": "F 1", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9846975803375244}]}, {"text": "The results clearly show that the classes defined by the two labeled sets are different.", "labels": [], "entities": []}, {"text": "A classifier trained on P&L performs worse by about 8% on SR than a classifier trained on SR (68.5 vs. 76.4).", "labels": [], "entities": []}, {"text": "A classifier trained on SR performs worse by more than 20% on P&L than a classifier trained on P&L (67.4 vs. 89.7).", "labels": [], "entities": [{"text": "P&L", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.8528748552004496}, {"text": "P&L", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.8533558249473572}]}, {"text": "Note that the classes are not balanced in the S-relevance data while they are balanced in the subjectivity data.", "labels": [], "entities": []}, {"text": "This can cause a misestimation O SNR Braxton is a gambling addict in deep to Mook (Ellen Burstyn), a local bookie.", "labels": [], "entities": []}, {"text": "S SNR Kennesaw is bitter about his marriage to a socialite (Rosanna Arquette), believing his wife to be unfaithful.", "labels": [], "entities": []}, {"text": "As we would expect, the baseline performance of the supervised classifier on SR is low: 69.9% (Table 4, line 1).", "labels": [], "entities": []}, {"text": "MinCut significantly boosts the performance by 7.9% to 77.5% (line 1), a result similar to ().", "labels": [], "entities": [{"text": "MinCut", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9532251358032227}]}, {"text": "Adding semantic features improves supervised classification significantly by 5.7% (75.6% online 4).", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.745731920003891}]}, {"text": "When MinCut and both types of semantic features are used together, these improvements are partially cumula- tive: an improvement over the baseline by 12.6% to 82.5% (line 4).", "labels": [], "entities": []}, {"text": "We also experiment with a training set where an artificial class imbalance is introduced, matching the 80:20 imbalance of SR:SNR in the S-relevance corpus.", "labels": [], "entities": []}, {"text": "After applying MinCut, we find that while the results for BL with and without imbalances does not differ significantly.", "labels": [], "entities": [{"text": "BL", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.7466593980789185}]}, {"text": "However, models using CX and VN features and imbalances are actually significantly inferior to the respective balanced versions.", "labels": [], "entities": []}, {"text": "This result suggests that MinCut is more effective at coping with class imbalances than artificial balancing.", "labels": [], "entities": []}, {"text": "MinCut and semantic features are successful for TL because both impose constraints that are more useful in a setup where noise is a major problem.", "labels": [], "entities": [{"text": "MinCut", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8325508236885071}, {"text": "TL", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9319061040878296}]}, {"text": "MinCut can exploit test set information without supervision as the MinCut graph is built directly on each test set review.", "labels": [], "entities": [{"text": "MinCut", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9405000805854797}]}, {"text": "If high-confidence information is \"seeded\" within a document and then spread to neighbors, mistakes with low confidence are corrected.", "labels": [], "entities": []}, {"text": "This way, MinCut also leads to a compensation of different class imbalances.", "labels": [], "entities": [{"text": "MinCut", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.91048264503479}]}, {"text": "The results are evidence that semantic features are robust to the differences between subjectivity and S-relevance (cf. Section 2).", "labels": [], "entities": []}, {"text": "In the CX+VN model, meaningful feature classes receive high weights, e.g., the human class from CoreLex which contains professions that are frequently associated with non-relevant plot descriptions.", "labels": [], "entities": []}, {"text": "To illustrate the run-based parameter optimization criterion, we show F 1 and median/mean run lengths for different values of c for the best TL: Classification results: F SR (S-relevant F 1 ), F SNR (S-nonrelevant F 1 ), and Fm (macro-averaged F 1 ).", "labels": [], "entities": [{"text": "run-based parameter optimization", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.6844419042269388}, {"text": "F 1", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9824102818965912}, {"text": "Fm", "start_pos": 225, "end_pos": 227, "type": "METRIC", "confidence": 0.940265953540802}]}, {"text": "B indicates a significant improvement over the BL base classifier (69.9), M over BL MinCut (77.5).", "labels": [], "entities": [{"text": "M", "start_pos": 74, "end_pos": 75, "type": "METRIC", "confidence": 0.9975881576538086}, {"text": "BL MinCut", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.7273268103599548}]}, {"text": "setting (line 4) in.", "labels": [], "entities": []}, {"text": "Due to differences in the base classifier, the optimum of c may vary between the experiments.", "labels": [], "entities": []}, {"text": "A weaker base classifier may yield a higher weight on the sequence model, resulting in a larger c.", "labels": [], "entities": []}, {"text": "The circled point shows the data point selected through optimization.", "labels": [], "entities": []}, {"text": "The optimization criterion does not always correlate perfectly with F 1 . However, we find no statistically significant difference between the selected result and the highest F 1 value.", "labels": [], "entities": [{"text": "F 1", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9852945506572723}, {"text": "F 1 value", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.9442700743675232}]}, {"text": "These experiments demonstrate that Srelevance classification improves considerably through TL if semantic feature generalization and unsupervised sequence classification through MinCut are applied.", "labels": [], "entities": [{"text": "Srelevance classification", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.8180730938911438}, {"text": "semantic feature generalization", "start_pos": 97, "end_pos": 128, "type": "TASK", "confidence": 0.6110939184824625}]}], "tableCaptions": [{"text": " Table 1: TL/in-task F 1 for P&L and SR corpora", "labels": [], "entities": [{"text": "TL/in-task F 1", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.7963146328926086}]}, {"text": " Table 2: % incorrect sentences containing specific  words", "labels": [], "entities": []}, {"text": " Table 3: Classification results: F SR (S-relevant F 1 ), F SNR (S-nonrelevant F 1 ), and F m (macro-averaged  F 1 ). Superscript numbers indicate a significant improvement over the corresponding line.", "labels": [], "entities": [{"text": "F SR", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9078483879566193}]}, {"text": " Table 4: Classification results: F SR (S-relevant F 1 ), F SNR (S-nonrelevant F 1 ), and F m (macro-averaged  F 1 ). B indicates a significant improvement over the BL base classifier (69.9), M over BL MinCut (77.5).", "labels": [], "entities": [{"text": "F SR", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.8866345584392548}, {"text": "M", "start_pos": 192, "end_pos": 193, "type": "METRIC", "confidence": 0.9948285222053528}, {"text": "BL MinCut", "start_pos": 199, "end_pos": 208, "type": "METRIC", "confidence": 0.6100759208202362}]}]}