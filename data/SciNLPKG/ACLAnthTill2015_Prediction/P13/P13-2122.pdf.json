{"title": [{"text": "Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation", "labels": [], "entities": [{"text": "Incremental Topic-Based Translation Model Adaptation", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7799806118011474}, {"text": "Conversational Spoken Language Translation", "start_pos": 57, "end_pos": 99, "type": "TASK", "confidence": 0.6585529670119286}]}], "abstractContent": [{"text": "We describe a translation model adaptation approach for conversational spoken language translation (CSLT), which encourages the use of contextually appropriate translation options from relevant training conversations.", "labels": [], "entities": [{"text": "translation model adaptation", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.9000451366106669}, {"text": "conversational spoken language translation (CSLT)", "start_pos": 56, "end_pos": 105, "type": "TASK", "confidence": 0.7420392504760197}]}, {"text": "Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations , which is used to bias translation choices towards the current context.", "labels": [], "entities": []}, {"text": "A significant novelty of our adaptation technique is its incremental nature; we continuously update the topic distribution on the evolving test conversation as new utterances become available.", "labels": [], "entities": []}, {"text": "Thus, our approach is well-suited to the causal constraint of spoken conversations.", "labels": [], "entities": []}, {"text": "On an English-to-Iraqi CSLT task, the proposed approach gives significant improvements over a baseline system as measured by BLEU, TER, and NIST.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9966678023338318}, {"text": "TER", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9653794765472412}, {"text": "NIST", "start_pos": 140, "end_pos": 144, "type": "DATASET", "confidence": 0.9658864736557007}]}, {"text": "Interestingly, the incremental approach outperforms a non-incremental oracle that has up-front knowledge of the whole conversation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conversational spoken language translation (CSLT) systems facilitate communication between subjects who do not speak the same language.", "labels": [], "entities": [{"text": "Conversational spoken language translation (CSLT)", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7391957214900425}]}, {"text": "Current systems are typically used to achieve a specific task (e.g. vehicle checkpoint search, medical diagnosis, etc.).", "labels": [], "entities": [{"text": "vehicle checkpoint search, medical diagnosis", "start_pos": 68, "end_pos": 112, "type": "TASK", "confidence": 0.6550707717736562}]}, {"text": "These task-driven conversations typically revolve around a set of central topics, which may not be evident at the beginning of the interaction.", "labels": [], "entities": []}, {"text": "As the conversation progresses, however, the gradual accumulation of contextual information can be used to infer the topic(s) of discussion, and to deploy contextually appropriate translation phrase pairs.", "labels": [], "entities": []}, {"text": "For example, the word 'drugs' will predominantly translate into Spanish as 'medicamentos' (medicines) in a medical scenario, whereas the translation 'drogas' (illegal drugs) will predominate in a law enforcement scenario.", "labels": [], "entities": []}, {"text": "Most CSLT systems do not take high-level global context into account, and instead translate each utterance in isolation.", "labels": [], "entities": []}, {"text": "This often results in contextually inappropriate translations, and is particularly problematic in conversational speech, which usually exhibits short, spontaneous, and often ambiguous utterances.", "labels": [], "entities": []}, {"text": "In this paper, we describe a novel topic-based adaptation technique for phrase-based statistical machine translation (SMT) of spoken conversations.", "labels": [], "entities": [{"text": "topic-based adaptation", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.702214390039444}, {"text": "phrase-based statistical machine translation (SMT) of spoken conversations", "start_pos": 72, "end_pos": 146, "type": "TASK", "confidence": 0.7853926092386245}]}, {"text": "We begin by building a monolingual latent Dirichlet allocation (LDA) topic model on the training conversations (each conversation corresponds to a \"document\" in the LDA paradigm).", "labels": [], "entities": []}, {"text": "At run-time, this model is used to infer a topic distribution over the evolving test conversation up to and including the current utterance.", "labels": [], "entities": []}, {"text": "Translation phrase pairs that originate in training conversations whose topic distribution is similar to that of the current conversation are given preference through a single similarity feature, which augments the standard phrase-based SMT log-linear model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 237, "end_pos": 240, "type": "TASK", "confidence": 0.9444616436958313}]}, {"text": "The topic distribution for the test conversation is updated incrementally for each new utterance as the available history grows.", "labels": [], "entities": []}, {"text": "With this approach, we demonstrate significant improvements over a baseline phrase-based SMT system as measured by BLEU, TER and NIST scores on an English-to-Iraqi CSLT task.", "labels": [], "entities": [{"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9056088924407959}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9990922212600708}, {"text": "TER", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9945175051689148}]}], "datasetContent": [{"text": "The baseline English-to-Iraqi phrase-based SMT system was built as described in Section 3.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.8289793729782104}]}, {"text": "This system translated each utterance independently, ignoring higher-level conversational context.", "labels": [], "entities": []}, {"text": "For the topic-adapted system, we compared translation performance with a varying number of LDA topics.", "labels": [], "entities": [{"text": "translation", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.969534695148468}]}, {"text": "In intuitive agreement with the approximate number of scenario types known to be covered by our data set, a range of 20-40 topics yielded the best results.", "labels": [], "entities": []}, {"text": "We compared the proposed incremental topic tracking approach to a non-causal oracle approach that had up-front access to the entire source conversations at run-time.", "labels": [], "entities": [{"text": "topic tracking", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7407259792089462}]}, {"text": "In all cases, we compared translation performance on both clean-text and automatic speech recognition (ASR) transcriptions of the source utterances.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9565119743347168}, {"text": "automatic speech recognition (ASR) transcriptions of the source utterances", "start_pos": 73, "end_pos": 147, "type": "TASK", "confidence": 0.8204602870074186}]}, {"text": "ASR transcriptions were generated using a high-performance two-pass HMM-based system, which delivered a word error rate (WER) of 10.6% on the test set utterances.", "labels": [], "entities": [{"text": "ASR transcriptions", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9604356586933136}, {"text": "word error rate (WER)", "start_pos": 104, "end_pos": 125, "type": "METRIC", "confidence": 0.8927476008733114}]}, {"text": "summarizes test set performance in BLEU (), NIST) and TER ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.996906578540802}, {"text": "NIST", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.867786169052124}, {"text": "TER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9853157997131348}]}, {"text": "Given the morphological complexity of Iraqi Arabic, computing string-based metrics on raw output can be misleadingly low and does not always reflect whether the core message was conveyed.", "labels": [], "entities": []}, {"text": "Since the primary goal of CSLT is information transfer, we present automatic results that are computed after stemming with an Iraqi Arabic stemmer.", "labels": [], "entities": [{"text": "information transfer", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.8321687877178192}]}, {"text": "We note that in all settings (incremental and non-causal oracle) our adaptation approach matches or significantly outperforms the baseline across multiple evaluation metrics.", "labels": [], "entities": []}, {"text": "In particular, the incremental LDA system with 40 topics is the top-scoring system in both clean-text and ASR settings.", "labels": [], "entities": []}, {"text": "In the ASR setting, which simulates a realworld deployment scenario, this system achieves improvements of 0.39 (BLEU), -0.6 (TER) and 0.08 (NIST).", "labels": [], "entities": [{"text": "ASR", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9634256958961487}, {"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9989801049232483}, {"text": "TER", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9952527284622192}, {"text": "NIST", "start_pos": 140, "end_pos": 144, "type": "DATASET", "confidence": 0.8103764057159424}]}], "tableCaptions": [{"text": " Table 1: Stemmed results on 3,138-utterance test  set. Asterisked results are significantly better than  the baseline (p \u2264 0.05) using 1,000 iterations  of paired bootstrap re-sampling", "labels": [], "entities": [{"text": "3,138-utterance test  set", "start_pos": 29, "end_pos": 54, "type": "DATASET", "confidence": 0.6916451652844747}, {"text": "Asterisked", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9573134183883667}]}]}