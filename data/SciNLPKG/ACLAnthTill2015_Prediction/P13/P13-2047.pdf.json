{"title": [{"text": "English\u2192Russian MT evaluation campaign", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9032122194766998}]}], "abstractContent": [], "introductionContent": [{"text": "Machine Translation (MT) between English and Russian was one of the first translation directions tested at the dawn of MT research in the 1950s).", "labels": [], "entities": [{"text": "Machine Translation (MT) between English and Russian", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8774050739076402}, {"text": "MT", "start_pos": 119, "end_pos": 121, "type": "TASK", "confidence": 0.9894722104072571}]}, {"text": "Since then the MT paradigms changed many times, many systems for this language pair appeared (and disappeared), but as far as we know there was no systematic quantitative evaluation of a range of systems, analogous to DARPA'94 ( and later evaluation campaigns.", "labels": [], "entities": [{"text": "MT paradigms", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.9096696972846985}]}, {"text": "The Workshop on Statistical MT (WMT) in 2013 has announced a Russian evaluation track for the first time.", "labels": [], "entities": [{"text": "Statistical MT (WMT)", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.8681013464927674}]}, {"text": "However, this evaluation is currently ongoing, it should include new methods for building statistical MT (SMT) systems for Russian from the data provided in this track, but it will not cover the performance of existing systems, especially rule-based (RBMT) or hybrid ones.", "labels": [], "entities": [{"text": "statistical MT (SMT)", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7194737434387207}]}, {"text": "Evaluation campaigns play an important role in promotion of the progress for MT technologies.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9942952990531921}]}, {"text": "Recently, there have been a number of MT shared tasks for combinations of several European, Asian and Semitic languages), which we took into account in designing the campaign for the English-Russian direction.", "labels": [], "entities": [{"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.965235710144043}]}, {"text": "The evaluation has been held in the http://www.statmt.org/wmt13/ context of ROMIP, 2 which stands for Russian Information Retrieval Evaluation Seminar and is a TREC-like Russian initiative started in 2002.", "labels": [], "entities": [{"text": "Russian Information Retrieval Evaluation Seminar", "start_pos": 102, "end_pos": 150, "type": "TASK", "confidence": 0.6261920630931854}]}, {"text": "One of the main challenges in developing MT systems for Russian and for evaluating them is the need to deal with its free word order and complex morphology.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9921514987945557}]}, {"text": "Long-distance dependencies are common, and this creates problems for both RBMT and SMT systems (especially for phrasebased ones).", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9649090766906738}]}, {"text": "Complex morphology also leads to considerable sparseness for word alignment in SMT.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.7490428388118744}, {"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9564011693000793}]}, {"text": "The language direction was chosen to be English\u2192Russian, first because of the availability of native speakers for evaluation, second because the systems taking part in this evaluation are mostly used in translation of English texts for the Russian readers.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main idea of manual evaluation was (1) to make the assessment as simple as possible fora human judge and to make the results of evaluation unambiguous.", "labels": [], "entities": []}, {"text": "We opted for pairwise comparison of MT outputs.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9699704647064209}]}, {"text": "This is different from simultaneous ranking of several MT outputs, as commonly used in WMT evaluation campaigns.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 55, "end_pos": 65, "type": "TASK", "confidence": 0.8709162175655365}, {"text": "WMT evaluation", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.8712805211544037}]}, {"text": "In case of a large number of participating systems each assessor ranks only a subset of MT outputs.", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9630885124206543}]}, {"text": "However, a fair overall ranking cannot be always derived from such partial rankings).", "labels": [], "entities": []}, {"text": "The pairwise comparisons we used can be directly converted into unambiguous overall rankings.", "labels": [], "entities": []}, {"text": "This task is also much simpler for human judges to complete.", "labels": [], "entities": []}, {"text": "On the other hand, pairwise comparisons require a larger number of evaluation decisions, which is feasible only for few participants (and we indeed had relatively few submissions in this campaign).", "labels": [], "entities": []}, {"text": "Below we also discuss how to reduce the amount of human efforts for evaluation.", "labels": [], "entities": []}, {"text": "In our case the assessors were asked to make a pairwise comparison of two sentences translated by two different MT systems against a gold standard translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.9094708561897278}]}, {"text": "The question for them was to judge translation adequacy, i.e., which MT output conveys information from the reference translation better.", "labels": [], "entities": [{"text": "MT output conveys information", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.8243983685970306}]}, {"text": "The source English sentence was not presented to the assessors, because we think that we can have more trust in understanding of the source text by a professional translator.", "labels": [], "entities": []}, {"text": "The translator also had access to the entire text, while the assessors could only see a single sentence.", "labels": [], "entities": []}, {"text": "For human evaluation we employed the multifunctional TAUS DQF tool 7 in the 'Quick Comparison' mode.", "labels": [], "entities": [{"text": "TAUS DQF tool 7", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.729037806391716}]}, {"text": "Assessors' judgements resulted in rankings for each sentence in the test set.", "labels": [], "entities": []}, {"text": "In case of ties the ranks were averaged, e.g. when the ranks of the systems in positions 2-4 and 7-8 were tied, their ranks became: 1 3 3 3 5 6 7.5 7.5.", "labels": [], "entities": []}, {"text": "To produce the final ranking, the sentence-level ranks were averaged overall sentences.", "labels": [], "entities": []}, {"text": "Pairwise comparisons are time-consuming: n pairwise decisions.", "labels": [], "entities": []}, {"text": "In this study we also simulated a 'human-assisted' insertion sort algorithm and its variant with binary search.", "labels": [], "entities": [{"text": "human-assisted' insertion sort algorithm", "start_pos": 35, "end_pos": 75, "type": "TASK", "confidence": 0.7316677391529083}]}, {"text": "The idea is to run a standard sort algorithm and ask a human judge each time a comparison operation is required.", "labels": [], "entities": []}, {"text": "This assumes that human perception of quality is transitive: if we know that A < B and B < C, we can spare evaluation of A and C.", "labels": [], "entities": []}, {"text": "This approach also implies that sentence pairs to judge are generated and presented to assessors on the fly; each decision contributes to selection of the pairs to be judged in the next step.", "labels": [], "entities": []}, {"text": "If the systems are pre-sorted in a reasonable way (e.g. by an MT metric, under assumption that automatic pre-ranking is closer to the 'ideal' ranking than a random one), then we can potentially save even more pairwise comparison operations.", "labels": [], "entities": []}, {"text": "Presorting makes ranking somewhat biased in favour of the order established by an MT metric.", "labels": [], "entities": [{"text": "Presorting", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9891997575759888}, {"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.6607752442359924}]}, {"text": "For example, if it favours one system against another, while inhuman judgement they are equal, the final ranking will preserve the initial order.", "labels": [], "entities": []}, {"text": "Insertion sort of n sentences requires n \u2212 1 comparisons in the best case of already sorted data and n(n\u22121) 2 in the worst case (reversely ordered data).", "labels": [], "entities": [{"text": "Insertion sort of n sentences", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8383348941802978}]}, {"text": "Insertion sort with binary search requires \u223c n log n comparisons regardless of the initial order.", "labels": [], "entities": []}, {"text": "For this study we ran exhaustive pairwise evaluation and used its results to simulate human-assisted sorting.", "labels": [], "entities": [{"text": "human-assisted sorting", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.5801486670970917}]}, {"text": "In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (), NIST), METEOR (Banerjee and), TER (, and GTM ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9978753328323364}, {"text": "METEOR", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9782664179801941}, {"text": "TER", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9129894375801086}]}, {"text": "We also wanted to estimate the correlations of these metrics with human judgements for the English\u2192Russian pair on the corpus level and on the level of individual sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Automatic evaluation results", "labels": [], "entities": []}, {"text": " Table 2: Human evaluation results", "labels": [], "entities": []}, {"text": " Table 3: Correlation to human judgements", "labels": [], "entities": [{"text": "Correlation to human judgements", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.7916859239339828}]}]}