{"title": [{"text": "Learning to Prune: Context-Sensitive Pruning for Syntactic MT", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.8394632935523987}]}], "abstractContent": [{"text": "We present a context-sensitive chart pruning method for CKY-style MT decoding.", "labels": [], "entities": [{"text": "MT decoding", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.8357811868190765}]}, {"text": "Source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells.", "labels": [], "entities": []}, {"text": "The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power.", "labels": [], "entities": []}, {"text": "On a full-scale English-to-German experiment with a string-to-tree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU.", "labels": [], "entities": [{"text": "speed-up", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9836902022361755}, {"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.999168872833252}]}], "introductionContent": [{"text": "Syntactic MT models suffer from decoding efficiency bottlenecks introduced by online n-gram language model integration and high grammar complexity.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.6453000903129578}, {"text": "n-gram language model integration", "start_pos": 85, "end_pos": 118, "type": "TASK", "confidence": 0.6429834961891174}]}, {"text": "Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring, coarse-to-fine processing ( and grammar transformations ().", "labels": [], "entities": []}, {"text": "For more expressive, linguistically-motivated syntactic MT models (), the grammar complexity has grown considerably over hierarchical phrase-based models, and decoding still suffers from efficiency issues.", "labels": [], "entities": []}, {"text": "In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to cube pruning) and additive to its pruning power.", "labels": [], "entities": [{"text": "MT decoding", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.8098797500133514}]}, {"text": "The main intuition of our method is to find those source phrases (i.e. any sequence of consecutive words) that are unlikely to have any consistently aligned target counterparts according to the source context and grammar constraints.", "labels": [], "entities": []}, {"text": "We show that by using highly-efficient sequence labelling models learned from the bitext used for translation model training, such phrases can be effectively identified prior to MT decoding, and corresponding chart cells can be excluded for decoding without affecting translation quality.", "labels": [], "entities": [{"text": "translation model training", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.9029315908749899}, {"text": "MT decoding", "start_pos": 178, "end_pos": 189, "type": "TASK", "confidence": 0.9011780321598053}]}, {"text": "We call our method context-sensitive pruning (CSP); it can be viewed as a bilingual adaptation of similar methods in monolingual parsing) which improve parsing efficiency by \"closing\" chart cells using binary classifiers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 152, "end_pos": 159, "type": "TASK", "confidence": 0.9691749811172485}]}, {"text": "Our contribution is that we demonstrate such methods can be applied to synchronous-grammar parsing by labelling the source-side alone.", "labels": [], "entities": [{"text": "synchronous-grammar parsing", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.6918334513902664}]}, {"text": "This is achieved through a novel training scheme where the labelling models are trained over the word-aligned bitext and gold-standard pruning labels are obtained by projecting target-side constituents to the source words.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first work to apply this technique to MT decoding.", "labels": [], "entities": [{"text": "MT decoding", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.9564316272735596}]}, {"text": "The proposed method is easy to implement and effective in practice.", "labels": [], "entities": []}, {"text": "Results on a full-scale English-to-German experiment show that it gives more than 60% speed-up over a strong cube pruning baseline, with no loss in BLEU.", "labels": [], "entities": [{"text": "speed-up", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9905546307563782}, {"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.999275267124176}]}, {"text": "While we use a string-to-tree model in this paper, the approach can be adapted to other syntax-based models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}