{"title": [{"text": "Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information", "labels": [], "entities": [{"text": "Latent Semantic Matching", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6610157489776611}, {"text": "Cross-language Text Categorization", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.675710121790568}]}], "abstractContent": [{"text": "Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text cat-egorization, because it does not require labor-intensive linguistic resources.", "labels": [], "entities": [{"text": "Unsupervised object matching (UOM)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7629930774370829}, {"text": "cross-language natural language processing", "start_pos": 62, "end_pos": 104, "type": "TASK", "confidence": 0.6660859286785126}, {"text": "bilingual lexicon acquisition", "start_pos": 113, "end_pos": 142, "type": "TASK", "confidence": 0.6361171702543894}, {"text": "parallel corpus construction", "start_pos": 144, "end_pos": 172, "type": "TASK", "confidence": 0.6853890220324198}]}, {"text": "However , UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks.", "labels": [], "entities": []}, {"text": "To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space.", "labels": [], "entities": [{"text": "latent semantic matching", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.6070574720700582}]}, {"text": "We demonstrate the effectiveness of our method on cross-language text cat-egorization.", "labels": [], "entities": []}, {"text": "The results show that our method outperforms conventional unsu-pervised object matching methods.", "labels": [], "entities": [{"text": "object matching", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.7441097497940063}]}], "introductionContent": [{"text": "Unsupervised object matching is a method for finding one-to-one correspondences between objects across different domains without knowledge about the relation between the domains.", "labels": [], "entities": [{"text": "Unsupervised object matching", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5963157514731089}]}, {"text": "Kernelized sorting () and canonical correlation analysis based methods) are two such examples of unsupervised object matching, which have been shown to be quite useful for cross-language natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "Kernelized sorting", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7030596733093262}, {"text": "object matching", "start_pos": 110, "end_pos": 125, "type": "TASK", "confidence": 0.7474031746387482}, {"text": "cross-language natural language processing (NLP) tasks", "start_pos": 172, "end_pos": 226, "type": "TASK", "confidence": 0.7365451231598854}]}, {"text": "One of the most important properties of the unsupervised object matching is that it does not require any linguistic resources which connects between the languages.", "labels": [], "entities": [{"text": "object matching", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.7649480998516083}]}, {"text": "This distinguishes it from other crosslanguage NLP methods such as machine translation based and projection based approaches (, which we need bilingual dictionaries or parallel sentences.", "labels": [], "entities": []}, {"text": "When we apply unsupervised object matching methods to cross-language NLP tasks, there are two critical problems.", "labels": [], "entities": [{"text": "object matching", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7269488871097565}]}, {"text": "The first is that they only find one-to-one matching.", "labels": [], "entities": []}, {"text": "The second is they require the same size of source-and target-data.", "labels": [], "entities": []}, {"text": "For example, the correct translation of a word is not always unique.", "labels": [], "entities": []}, {"text": "French words 'maison', 'appartment' and 'domicile' can be regarded as translation of an English word 'home'.", "labels": [], "entities": []}, {"text": "In addition, English vocabulary size is not equal to that of French.", "labels": [], "entities": []}, {"text": "These discussions motivate us to introduce a shared space in which both source and target domain objects will reside.", "labels": [], "entities": []}, {"text": "If we can obtain such a shared space, we can match objects within the space, because we can use standard distance metrics on this space.", "labels": [], "entities": []}, {"text": "This will also enable us to use various kinds of non-strict matching.", "labels": [], "entities": []}, {"text": "For example, k-nearest objects in the source domain will be retrieved fora query object in the target domain.", "labels": [], "entities": []}, {"text": "In this paper, we propose a simple but effective method to find the shared space by assuming that two languages have common latent topics, which we call latent semantic matching.", "labels": [], "entities": [{"text": "latent semantic matching", "start_pos": 153, "end_pos": 177, "type": "TASK", "confidence": 0.6479179759820303}]}, {"text": "With latent semantic matching, we first find latent topics in two domains independently.", "labels": [], "entities": [{"text": "latent semantic matching", "start_pos": 5, "end_pos": 29, "type": "TASK", "confidence": 0.7212973634401957}]}, {"text": "Then, the topics in two domains are aligned by kernelized sorting, and objects are embedded in a shared latent topic space.", "labels": [], "entities": []}, {"text": "Latent topic representations are successfully used in a wide range of NLP tasks, such as information retrieval and text classification, because they represent intrinsic information of documents.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.8006689548492432}, {"text": "text classification", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7841902673244476}]}, {"text": "By matching latent topics, we can find relation between source and target domains, and additionally we can handle different numbers of objects in two domains.", "labels": [], "entities": []}, {"text": "We compared latent semantic matching with conventional unsupervised object matching meth-ods on the task of cross-language text categorization, i.e. classifying target side unlabeled documents by label information obtained from source side documents.", "labels": [], "entities": [{"text": "latent semantic matching", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.6485708157221476}, {"text": "cross-language text categorization", "start_pos": 108, "end_pos": 142, "type": "TASK", "confidence": 0.6721798578898112}]}, {"text": "The results show that, with more source side documents, our method achieved the highest classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.8373956680297852}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9256014823913574}]}], "datasetContent": [{"text": "We compared our method, latent semantic matching (LSM), with three unsupervised object matching methods: Kernelized Sorting (KS), Convex Kernelized Sorting (CKS), Least-Squares Object Matching (LSOM).", "labels": [], "entities": [{"text": "latent semantic matching (LSM)", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.7899183432261149}, {"text": "Least-Squares Object Matching (LSOM", "start_pos": 163, "end_pos": 198, "type": "TASK", "confidence": 0.7020136117935181}]}, {"text": "We set the number of the latent topics K to 100 and employed the k-nearest neighbor method (k=10) as the classifier.", "labels": [], "entities": []}, {"text": "For, KS, CKS and LSOM, we find the oneto-one correspondence between documents in the source language and documents in the target language.", "labels": [], "entities": []}, {"text": "Then, we assign class labels of the target documents according to the correspondence.", "labels": [], "entities": []}, {"text": "In order to build a corpus with various language pairs for evaluation, we crawled product reviews from Amazon U.S., German, France and Japan with five categories: 'Books', 'Electronics', 'Music', 'Kitchen', 'Watch'.", "labels": [], "entities": []}, {"text": "The corpus is neither sentence level parallel nor comparable.", "labels": [], "entities": []}, {"text": "For each category, we randomly select 60 documents as the test data (M =300) for all methods and 60 documents as the training data (N =300) for KS, CKS, LSOM and LSM(300).", "labels": [], "entities": []}, {"text": "We also compared latent semantic matching with 120 training documents for each category (N =600), and called this method LSM(600).", "labels": [], "entities": [{"text": "latent semantic matching", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.6110998292764028}]}, {"text": "Note that since KS, CKS and LSOM require that the data sizes are the same for source and target domains, they cannot use training data more than test data.", "labels": [], "entities": []}, {"text": "To avoid local optimum solutions of NMF, we executed our methods with 100 different initialization values and chose the solution that achieved the best objective function of KS.", "labels": [], "entities": []}, {"text": "shows average accuracies with standard division overall language pairs.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9813821315765381}]}, {"text": "From the table, classification accuracy of all methods significantly outperformed random classifier (accuracy=0.2).", "labels": [], "entities": [{"text": "classification", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.9384060502052307}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9595677256584167}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9987711310386658}]}, {"text": "The results showed the effectiveness of both unsupervised object matching and latent semantic matching.", "labels": [], "entities": [{"text": "object matching", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.7122501730918884}, {"text": "latent semantic matching", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.6148086488246918}]}, {"text": "When comparing LSM(300) with KS, CKS and LSOM, LSM(300) obtained better results than these unsupervised object matching methods.", "labels": [], "entities": [{"text": "object matching", "start_pos": 104, "end_pos": 119, "type": "TASK", "confidence": 0.7363573312759399}]}, {"text": "The result supports the effectiveness of the latent topic matching.", "labels": [], "entities": [{"text": "latent topic matching", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.5742924312750498}]}, {"text": "Moreover, LSM(600) achieved the highest accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9989215135574341}]}, {"text": "There are large differences between LSM(600) and the others.", "labels": [], "entities": []}, {"text": "This result implies not only the effectiveness of the latent topic matching but also increasing the number of source side documents (labeled training data) contributes to improving classification accuracy.", "labels": [], "entities": [{"text": "topic matching", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.6767843961715698}, {"text": "classification", "start_pos": 181, "end_pos": 195, "type": "TASK", "confidence": 0.9561477899551392}, {"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.8556665182113647}]}, {"text": "This is natural in terms of supervised learning but only our method can deal with source side documents that are larger in number.", "labels": [], "entities": []}, {"text": "shows examples of latent topics in English and German extracted and aligned by LSM(600).", "labels": [], "entities": [{"text": "LSM(600)", "start_pos": 79, "end_pos": 87, "type": "DATASET", "confidence": 0.948172003030777}]}, {"text": "We can see that some author names, words related to camera, and cooking equipment appear in 'Books', 'Electronics' and 'Kitchen' topics, respectively.", "labels": [], "entities": []}, {"text": "Similarity, there are some artists' names in 'Music' and watch brands in 'Watch'.", "labels": [], "entities": [{"text": "Watch", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9313859939575195}]}], "tableCaptions": []}