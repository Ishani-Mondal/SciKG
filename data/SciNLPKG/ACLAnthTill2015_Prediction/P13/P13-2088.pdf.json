{"title": [{"text": "LABR: A Large Scale Arabic Book Reviews Dataset", "labels": [], "entities": [{"text": "LABR: A Large Scale Arabic Book Reviews Dataset", "start_pos": 0, "end_pos": 47, "type": "DATASET", "confidence": 0.6227905915843116}]}], "abstractContent": [{"text": "We introduce LABR, the largest sentiment analysis dataset to-date for the Arabic language.", "labels": [], "entities": [{"text": "LABR", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.8207176923751831}, {"text": "sentiment analysis", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9217461347579956}]}, {"text": "It consists of over 63,000 book reviews, each rated on a scale of 1 to 5 stars.", "labels": [], "entities": []}, {"text": "We investigate the properties of the the dataset, and present its statistics.", "labels": [], "entities": []}, {"text": "We explore using the dataset for two tasks: sentiment polarity classification and rating classification.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.8025123476982117}, {"text": "rating classification", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.7179726809263229}]}, {"text": "We provide standard splits of the dataset into training and testing , for both polarity and rating classification , in both balanced and unbalanced settings.", "labels": [], "entities": [{"text": "rating classification", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.6398347020149231}]}, {"text": "We run baseline experiments on the dataset to establish a benchmark.", "labels": [], "entities": []}], "introductionContent": [{"text": "The internet is full of platforms where users can express their opinions about different subjects, from movies and commercial products to books and restaurants.", "labels": [], "entities": []}, {"text": "With the explosion of social media, this has become easier and more prevalent than ever.", "labels": [], "entities": []}, {"text": "Mining these troves of unstructured text has become a very active area of research with lots of applications.", "labels": [], "entities": []}, {"text": "Sentiment Classification is among the most studied tasks for processing opinions (.", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9347166121006012}]}, {"text": "In its basic form, it involves classifying apiece of opinion, e.g. a movie or book review, into either having a positive or negative sentiment.", "labels": [], "entities": [{"text": "classifying apiece of opinion, e.g. a movie or book review", "start_pos": 31, "end_pos": 89, "type": "TASK", "confidence": 0.7411875995722684}]}, {"text": "Another form involves predicting the actual rating of a review, e.g. predicting the number of stars on a scale from 1 to 5 stars.", "labels": [], "entities": []}, {"text": "Most of the current research has focused on building sentiment analysis applications for the English language, with much less work on other languages.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.9313640296459198}]}, {"text": "In particular, there has been little work on sentiment analysis in Arabic (, and very few, considerably small-sized, datasets to work with (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.9523273408412933}]}, {"text": "In this work, we try to address the lack of large-scale Arabic sentiment analysis datasets in this field, in the hope of sparking more interest in research in Arabic sentiment analysis and related tasks.", "labels": [], "entities": [{"text": "Arabic sentiment analysis", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.6569647391637167}, {"text": "Arabic sentiment analysis", "start_pos": 159, "end_pos": 184, "type": "TASK", "confidence": 0.7306710084279379}]}, {"text": "Towards this end, we introduce LABR, the Large-scale Arabic Book Review dataset.", "labels": [], "entities": [{"text": "LABR", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9090533256530762}, {"text": "Large-scale Arabic Book Review dataset", "start_pos": 41, "end_pos": 79, "type": "DATASET", "confidence": 0.7161966502666474}]}, {"text": "It is a set of over 63K book reviews, each with a rating of 1 to 5 stars.", "labels": [], "entities": []}, {"text": "We make the following contributions: (1) We present the largest Arabic sentiment analysis dataset to-date (up to our knowledge); (2) We provide standard splits for the dataset into training and testing sets.", "labels": [], "entities": [{"text": "Arabic sentiment analysis", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.623073806365331}]}, {"text": "This will make comparing different results much easier.", "labels": [], "entities": []}, {"text": "The dataset and the splits are publicly available at www.mohamedaly.info/datasets; (3) We explore the structure and properties of the dataset, and perform baseline experiments for two tasks: sentiment polarity classification and rating classification.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 191, "end_pos": 224, "type": "TASK", "confidence": 0.8327513734499613}, {"text": "rating classification", "start_pos": 229, "end_pos": 250, "type": "TASK", "confidence": 0.8244024217128754}]}], "datasetContent": [{"text": "We downloaded over 220,000 reviews from the book readers social network www.goodreads.com during the month of March 2013.", "labels": [], "entities": []}, {"text": "These reviews were from the first 2143 books in the list of Best Arabic Books.", "labels": [], "entities": []}, {"text": "After harvesting the reviews, we found out that over 70% of them were not in Arabic, either because some non-Arabic books exist in the list, or because of existing translations of some of the books in other languages.", "labels": [], "entities": []}, {"text": "After filtering out the non-Arabic reviews, and performing several pre-processing steps to cleanup HTML tags and other unwanted content, we ended up with 63,257 Arabic reviews.", "labels": [], "entities": []}, {"text": "The shows the number of reviews for each rating.", "labels": [], "entities": []}, {"text": "We consider as positive reviews those with ratings 4 or 5, and negative reviews those with ratings 1 or 2.", "labels": [], "entities": []}, {"text": "Reviews with rating 3 are considered neutral and not included in the polarity classification.", "labels": [], "entities": []}, {"text": "The number of positive reviews is much larger than that of negative reviews.", "labels": [], "entities": []}, {"text": "We believe this is because the books we got reviews for were the most popular books, and the top rated ones had many more reviews than the the least popular books.", "labels": [], "entities": []}, {"text": "The average user provided 3.84 reviews with the median being 2.", "labels": [], "entities": []}, {"text": "The average book got almost 30 reviews with the median being 6.", "labels": [], "entities": []}, {"text": "shows the number of reviews per user and book.", "labels": [], "entities": []}, {"text": "As shown in the, most books and users have few reviews, and vice versa.", "labels": [], "entities": []}, {"text": "show a box plot of the number of reviews per user and book.", "labels": [], "entities": []}, {"text": "We notice that books (and users) tend to have (give) positive reviews than negative reviews, where the median number of positive reviews per book is 5 while that for negative reviews is only 2 (and similarly for reviews per user).", "labels": [], "entities": []}, {"text": "shows the statistics of tokens and sentences.", "labels": [], "entities": []}, {"text": "The reviews were tokenized and \"rough\" sentence counts were computed (by looking for punctuation characters).", "labels": [], "entities": []}, {"text": "The average number of tokens per review is 65.4, the average number of sentences per review is 5.4, and the average number of tokens per sentence is 12.", "labels": [], "entities": []}, {"text": "show that the distribution is similar for positive and negative reviews.", "labels": [], "entities": []}, {"text": "shows a plot of the frequency of the tokens in the vocabulary in a loglog scale, which conforms to Zipf's law).", "labels": [], "entities": []}, {"text": "We explored using the dataset for two tasks: (a) Sentiment polarity classification: where the goal is to predict if the review is positive i.e. with rating 4 or 5, or is negative i.e. with rating 1 or 2; and (b)   Rating classification: where the goal is to predict the rating of the review on a scale of 1 to 5.", "labels": [], "entities": [{"text": "Sentiment polarity classification", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.848460833231608}]}, {"text": "To this end, we divided the dataset into separate training and test sets, with a ratio of 8:2.", "labels": [], "entities": []}, {"text": "We do this because we already have enough training data, so there is no need to resort to cross-validation ().", "labels": [], "entities": []}, {"text": "To avoid the bias of having more positive than negative reviews, we explored two settings: (a) a balanced split where the number of reviews from every class is the same, and is taken to be the size of the smallest class (where larger classes are down-sampled); (b) an unbalanced split where the number of reviews from every class is unrestricted, and follows the distribution shown in. shows the number of reviews in the training and test sets for each of the two tasks for the balanced and unbalanced splits, while shows the breakdown of these numbers per class.", "labels": [], "entities": []}, {"text": "show results of the experiments for both tasks in both balanced/unbalanced settings.", "labels": [], "entities": []}, {"text": "We tried different features: unigrams, bigrams, and trigrams with/without tf-idf weighting.", "labels": [], "entities": []}, {"text": "For classifiers, we used Multinomial Naive Bayes, Bernoulli Naive Bayes (for binary counts), and Support Vector Machines.", "labels": [], "entities": []}, {"text": "We report two measures: the total classification accuracy (percentage of correctly classified test examples) and weighted F1 measure).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.8781270384788513}, {"text": "F1 measure", "start_pos": 122, "end_pos": 132, "type": "METRIC", "confidence": 0.9742915332317352}]}, {"text": "All experiments were implemented in Python using scikit-learn (Pedregosa et al., 2011) and Qalsadi (available at pypi.python.org/pypi/qalsadi).", "labels": [], "entities": []}, {"text": "We notice that: (a) The total accuracy and weighted F1 are quite correlated and go hand-inhand.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9825258255004883}, {"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9387341141700745}]}, {"text": "(b) Task 1 is much easier than task 2, which is expected.", "labels": [], "entities": []}, {"text": "(c) The unbalanced setting seems eas-     The same for the rating classification task.", "labels": [], "entities": [{"text": "rating classification task", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.8527728319168091}]}, {"text": "In the balanced set, all classes have the same number of reviews as the smallest class, which is done by down-sampling the larger classes.", "labels": [], "entities": []}, {"text": "ier than the balanced one.", "labels": [], "entities": []}, {"text": "This might be because the unbalanced sets contain more training examples to make use of.", "labels": [], "entities": []}, {"text": "(d) SVM does much better in the unbalanced setting, while MNB is slightly better than SVM in the balanced setting.", "labels": [], "entities": [{"text": "MNB", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.6239174604415894}]}, {"text": "(e) Using more ngrams helps, and especially combined with tf-idf weighting, as all the best scores are with tfidf.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Important Dataset Statistics.", "labels": [], "entities": [{"text": "Important Dataset Statistics", "start_pos": 10, "end_pos": 38, "type": "DATASET", "confidence": 0.8979078133900961}]}, {"text": " Table 2: Training and Test sets. B stands for bal- anced, and U stands for Unbalanced.", "labels": [], "entities": [{"text": "bal", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9580209851264954}]}, {"text": " Table 3: Task 1: Polarity Classification Experimental Results. 1g means using the unigram model,  1g+2g is using unigrams + bigrams, and 1g+2g+3g is using trigrams. Tf-Idf indicates whether tf-idf  weighting was used or not. MNB is Multinomial Naive Bayes, BNB is Bernoulli Naive Bayes, and SVM  is the Support Vector Machine. The numbers represent total accuracy / weighted F1 measure. See Sec.  5.", "labels": [], "entities": [{"text": "Polarity Classification Experimental", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.8010594050089518}, {"text": "BNB", "start_pos": 258, "end_pos": 261, "type": "METRIC", "confidence": 0.9691893458366394}, {"text": "accuracy / weighted F1 measure", "start_pos": 356, "end_pos": 386, "type": "METRIC", "confidence": 0.7434449851512909}]}, {"text": " Table 4: Task 2: Rating Classification Experimental Results. See Table 3 and Sec. 5.", "labels": [], "entities": [{"text": "Rating Classification Experimental", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.910737136999766}]}]}