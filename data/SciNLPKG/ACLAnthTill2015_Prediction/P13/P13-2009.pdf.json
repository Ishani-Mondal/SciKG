{"title": [], "abstractContent": [{"text": "Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8388991057872772}]}, {"text": "Here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.7845888435840607}]}, {"text": "In experiments on the multilingual GeoQuery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9971504807472229}]}, {"text": "These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations , and suggest that research in semantic parsing could benefit from advances in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7167591005563736}, {"text": "semantic parsing evaluations", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.8859872023264567}, {"text": "semantic parsing", "start_pos": 151, "end_pos": 167, "type": "TASK", "confidence": 0.8130702674388885}, {"text": "machine translation", "start_pos": 199, "end_pos": 218, "type": "TASK", "confidence": 0.7599941492080688}]}], "introductionContent": [{"text": "Semantic parsing (SP) is the problem of transforming a natural language (NL) utterance into a machine-interpretable meaning representation (MR).", "labels": [], "entities": [{"text": "Semantic parsing (SP)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.902465569972992}]}, {"text": "It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (, supervised, unsupervised, and response-based (.", "labels": [], "entities": []}, {"text": "At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL).", "labels": [], "entities": [{"text": "SP", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.984948456287384}, {"text": "machine translation (MT)", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.8599435448646545}]}, {"text": "Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction ( and the use of automata such as tree transducers) to encode the relationship between NL and MRL.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9617043137550354}, {"text": "word alignment", "start_pos": 114, "end_pos": 128, "type": "TASK", "confidence": 0.6995410919189453}, {"text": "rule extraction", "start_pos": 160, "end_pos": 175, "type": "TASK", "confidence": 0.7777638733386993}]}, {"text": "The key difference between the two tasks is that in SP, the target language (the MRL) has very different properties to an NL.", "labels": [], "entities": [{"text": "SP", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9698087573051453}]}, {"text": "In particular, MRs must conform strictly to a particular structure so that they are machine-interpretable.", "labels": [], "entities": [{"text": "MRs", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.981132447719574}]}, {"text": "Contrast this with ordinary MT, where varying degrees of wrongness are tolerated by human readers (and evaluation metrics).", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9906880855560303}]}, {"text": "To avoid producing malformed MRs, almost all of the existing research on SP has focused on developing models with richer structure than those commonly used for MT.", "labels": [], "entities": [{"text": "MRs", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8362342715263367}, {"text": "SP", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9734691977500916}, {"text": "MT", "start_pos": 160, "end_pos": 162, "type": "TASK", "confidence": 0.9904982447624207}]}, {"text": "In this work we attempt to determine how accurate a semantic parser we can build by treating SP as a pure MT task, and describe pre-and postprocessing steps which allow structure to be preserved in the MT process.", "labels": [], "entities": [{"text": "MT task", "start_pos": 106, "end_pos": 113, "type": "TASK", "confidence": 0.9018171429634094}, {"text": "MT process", "start_pos": 202, "end_pos": 212, "type": "TASK", "confidence": 0.879525899887085}]}, {"text": "Our contributions are as follows: We develop a semantic parser using off-the-shelf MT components, exploring phrase-based as well as hierarchical models.", "labels": [], "entities": []}, {"text": "Experiments with four languages on the popular GeoQuery corpus show that our parser is competitve with the state-ofthe-art, in some cases achieving higher accuracy than recently introduced purpose-built semantic parsers.", "labels": [], "entities": [{"text": "GeoQuery corpus", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9059157371520996}, {"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9942981600761414}]}, {"text": "Our approach also appears to require substantially less time to train than the two bestperforming semantic parsers.", "labels": [], "entities": []}, {"text": "These results support the use of MT methods as an informative baseline in SP evaluations and show that research in SP could benefit from research advances in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9722732305526733}, {"text": "MT", "start_pos": 158, "end_pos": 160, "type": "TASK", "confidence": 0.9824944138526917}]}], "datasetContent": [{"text": "Dataset We conduct experiments on the GeoQuery data set.", "labels": [], "entities": [{"text": "GeoQuery data set", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.972852865854899}]}, {"text": "The corpus consists of a set of 880 natural-language questions about U.S. geography in four languages (English, German, Greek and Thai), and their representations in a variablefree MRL that can be executed against a Prolog database interface.", "labels": [], "entities": []}, {"text": "Initial experimentation was done using 10 fold cross-validation on the 600-sentence development set and the final evaluation on a held-out test set of 280 sentences.", "labels": [], "entities": []}, {"text": "All semantic parsers for GeoQuery we compare against also makes use of NP lists (Jones et al., 2012), which contain MRs for every noun phrase that appears in the NL utterances of each language.", "labels": [], "entities": []}, {"text": "In our experiments, the NP list was included by appending all entries as extra training sentences to the end of the training corpus of each language with 50 times the weight of regular training examples, to ensure that they are learned as translation rules.", "labels": [], "entities": []}, {"text": "Evaluation for each utterance is performed by executing both the predicted and the gold standard MRs against the database and obtaining their respective answers.", "labels": [], "entities": [{"text": "MRs", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.5321170091629028}]}, {"text": "An MR is correct if it obtains the same answer as the gold standard MR, allowing fora fair comparison between systems using different learning paradigms.", "labels": [], "entities": []}, {"text": "Following we report accuracy, i.e. the percentage of NL questions with correct answers, and F 1 , i.e. the harmonic mean of precision (percentage of correct answers obtained).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9996053576469421}, {"text": "F 1", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9941058158874512}, {"text": "harmonic mean of", "start_pos": 107, "end_pos": 123, "type": "METRIC", "confidence": 0.7927876512209574}, {"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.6442825198173523}]}, {"text": "Implementation In all experiments, we use the IBM Model 4 implementation from the GIZA++ toolkit) for alignment, and the phrase-based and hierarchical models implemented in the Moses toolkit ( for rule extraction.", "labels": [], "entities": [{"text": "alignment", "start_pos": 102, "end_pos": 111, "type": "TASK", "confidence": 0.9636377096176147}, {"text": "rule extraction", "start_pos": 197, "end_pos": 212, "type": "TASK", "confidence": 0.7720070779323578}]}, {"text": "The best symmetrization algorithm, translation and language model weights for each language are selected using cross-validation on the development set.", "labels": [], "entities": [{"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9569507241249084}]}, {"text": "In the case of English and German, we also found that stemming ( was hepful in reducing data sparsity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: GeoQuery accuracies with and without  NPs. Rows with (-NP) did not use the NP list.", "labels": [], "entities": [{"text": "NP list", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.8707811832427979}]}, {"text": " Table 1: Accuracy and F 1 scores for the multilingual GeoQuery test set. Results for other systems as  reported by Jones et al. (2012).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991682767868042}, {"text": "F 1 scores", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9873235026995341}, {"text": "GeoQuery test set", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.801218161980311}]}]}