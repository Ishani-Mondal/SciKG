{"title": [{"text": "Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression", "labels": [], "entities": [{"text": "Semantic Tagging of Conversational Understanding", "start_pos": 16, "end_pos": 64, "type": "TASK", "confidence": 0.8148800253868103}]}], "abstractContent": [{"text": "Finding concepts in natural language utterances is a challenging task, especially given the scarcity of labeled data for learning semantic ambiguity.", "labels": [], "entities": [{"text": "Finding concepts in natural language utterances", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7153557042280833}]}, {"text": "Furthermore, data mismatch issues, which arise when the expected test (target) data does not exactly match the training data, aggravate this scarcity problem.", "labels": [], "entities": []}, {"text": "To deal with these issues, we describe an efficient semi-supervised learning (SSL) approach which has two components: (i) Markov Topic Regression is anew probabilistic model to cluster words into semantic tags (con-cepts).", "labels": [], "entities": [{"text": "semi-supervised learning (SSL)", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.7318319082260132}]}, {"text": "It can efficiently handle semantic ambiguity by extending standard topic models with two new features.", "labels": [], "entities": []}, {"text": "First, it encodes word n-gram features from labeled source and unlabeled target data.", "labels": [], "entities": []}, {"text": "Second , by going beyond a bag-of-words approach , it takes into account the inherent sequential nature of utterances to learn semantic classes based on context.", "labels": [], "entities": []}, {"text": "(ii) Retrospective Learner is anew learning technique that adapts to the unlabeled target data.", "labels": [], "entities": []}, {"text": "Our new SSL approach improves semantic tagging performance by 3% absolute over the baseline models, and also compares favorably on semi-supervised syntactic tagging.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.775187611579895}]}], "introductionContent": [{"text": "Semantic tagging is used in natural language understanding (NLU) to recognize words of semantic importance in an utterance, such as entities.", "labels": [], "entities": [{"text": "Semantic tagging", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8111636936664581}, {"text": "natural language understanding (NLU)", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.8369600375493368}]}, {"text": "Typically, a semantic tagging model require large amount of domain specific data to achieve good performance ().", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7166810929775238}]}, {"text": "This requires a tedious and time intensive data collection and labeling process.", "labels": [], "entities": []}, {"text": "In the absence of large labeled training data, the tagging model can behave poorly on test data (target domain).", "labels": [], "entities": []}, {"text": "This is usually caused by data mismatch issues and lack of coverage that arise when the target data does not match the training data.", "labels": [], "entities": [{"text": "coverage", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9835329055786133}]}, {"text": "To deal with these issues, we present anew semi-supervised learning (SSL) approach, which mainly has two components.", "labels": [], "entities": [{"text": "semi-supervised learning (SSL)", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.6877493381500244}]}, {"text": "It initially starts with training supervised Conditional Random Fields (CRF) () on the source training data which has been semantically tagged.", "labels": [], "entities": []}, {"text": "Using the trained model, it decodes unlabeled dataset from the target domain.", "labels": [], "entities": []}, {"text": "With the data mismatch issues in mind, to correct errors that the supervised model make on the target data, the SSL model leverages the additional information byway of anew clustering method.", "labels": [], "entities": []}, {"text": "Our first contribution is anew probabilistic topic model, Markov Topic Regression (MTR), which uses rich features to capture the degree of association between words and semantic tags.", "labels": [], "entities": []}, {"text": "First, it encodes the n-gram context features from the labeled source data and the unlabeled target data as prior information to learn semantic classes based on context.", "labels": [], "entities": []}, {"text": "Thus, each latent semantic class corresponds to one of the semantic tags found in labeled data.", "labels": [], "entities": []}, {"text": "MTR is not invariant to reshuffling of words due to its Markovian property; hence, word-topic assignments are also affected by the topics of the surrounding words.", "labels": [], "entities": [{"text": "MTR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7303026914596558}]}, {"text": "Because of these properties, MTR is less sensitive to the errors caused by the semantic ambiguities.", "labels": [], "entities": [{"text": "MTR", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8978878855705261}]}, {"text": "Our SSL uses MTR to smooth the semantic tag posteriors on the unlabeled target data (decoded using the CRF model) and later obtains the best tag sequences.", "labels": [], "entities": []}, {"text": "Using the labeled source and automati-cally labeled target data, it re-trains anew CRFmodel.", "labels": [], "entities": []}, {"text": "Although our iterative SSL learning model can deal with the training and test data mismatch, it neglects the performance effects caused by adapting the source domain to the target domain.", "labels": [], "entities": [{"text": "SSL learning", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.9227609634399414}]}, {"text": "In fact, most SSL methods used for adaptation, e.g.,,,, etc., do not emphasize this issue.", "labels": [], "entities": []}, {"text": "With this in mind, we introduce anew iterative training algorithm, Retrospective Learning, as our second contribution.", "labels": [], "entities": []}, {"text": "While retrospective learning iteratively trains CRF models with the automatically annotated target data (explained above), it keeps track of the errors of the previous iterations so as to carry the properties of both the source and target domains.", "labels": [], "entities": []}, {"text": "In short, through a series of experiments we show how MTR clustering provides additional information to SSL on the target domain utterances, and greatly impacts semantic tagging performance.", "labels": [], "entities": [{"text": "MTR clustering", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.967318207025528}, {"text": "semantic tagging", "start_pos": 161, "end_pos": 177, "type": "TASK", "confidence": 0.744562417268753}]}, {"text": "Specifically, we analyze MTR's performance on two different types of semantic tags: named-entities and descriptive tags as shown in.", "labels": [], "entities": [{"text": "MTR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9611304402351379}]}, {"text": "Our experiments show that it is much harder to detect descriptive tags compared to named-entities.", "labels": [], "entities": []}, {"text": "Our SSL approach uses probabilistic clustering method tailored for tagging natural language utterances.", "labels": [], "entities": [{"text": "tagging natural language utterances", "start_pos": 67, "end_pos": 102, "type": "TASK", "confidence": 0.8579762876033783}]}, {"text": "To the best of our knowledge, our work is the first to explore the unlabeled data to iteratively adapt the semantic tagging models for target domains, preserving information from the previous iterations.", "labels": [], "entities": []}, {"text": "With the hope of spurring related work in domains such as entity detection, syntactic tagging, etc., we extend the earlier work on SSL partof-speech (POS) tagging and show in the experiments that our approach is not only useful for semantic tagging but also syntactic tagging.", "labels": [], "entities": [{"text": "entity detection", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.8105215728282928}, {"text": "syntactic tagging", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7330573052167892}, {"text": "SSL partof-speech (POS) tagging", "start_pos": 131, "end_pos": 162, "type": "TASK", "confidence": 0.6477415362993876}, {"text": "semantic tagging", "start_pos": 232, "end_pos": 248, "type": "TASK", "confidence": 0.7092071175575256}, {"text": "syntactic tagging", "start_pos": 258, "end_pos": 275, "type": "TASK", "confidence": 0.719148799777031}]}, {"text": "The remainder of this paper is divided as follows: \u00a72 gives background on SSL and semantic clustering methods, \u00a73 describes our new clustering approach, \u00a74 presents the new iterative learning, \u00a75 presents our experimental results and \u00a76 concludes our paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We focus hereon audiovisual media in the movie domain.", "labels": [], "entities": []}, {"text": "The user is expected to interact by voice with a system than can perform a variety of tasks such as browsing, searching, querying information, etc.", "labels": [], "entities": []}, {"text": "To build initial NLU models for such a dialog system, we used crowd-sourcing to collect and annotate utterances, which we consider our source domain.", "labels": [], "entities": []}, {"text": "Given movie domain-specific tasks, we asked the crowd about how they would interact with the media system as if they were talking to a person.", "labels": [], "entities": []}, {"text": "Our data from target domain is internally collected from real-use scenarios of our spoken dialog system.", "labels": [], "entities": []}, {"text": "The transcribed text forms of these utterances are obtained from speech recognition engine.", "labels": [], "entities": []}, {"text": "Although the crowd-sourced data is similar to target domain, in terms of pre-defined user intentions, the target domain contains more descriptive vocabulary, which is almost twice as large as the source domain.", "labels": [], "entities": []}, {"text": "This causes data-mismatch issues and hence provides a perfect test-bed fora domain adaptation task.", "labels": [], "entities": []}, {"text": "In total, our corpus has a 40K semantically tagged utterances from each source and target domains.", "labels": [], "entities": []}, {"text": "There are around 15 named-entity and 10 descriptive tags.", "labels": [], "entities": []}, {"text": "We separated 5K utterances to test the performance of the semantic tagging models.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.7117656767368317}]}, {"text": "The most frequent entities are: movie-director ('James Cameron'), movie-title ('Die Hard'), etc.; whereas top descriptive tags are: genre ('feel good'), description ('black and white', 'pg 13'), review-rate ('epic', 'not for me'), theater-location ('near me','city center'), etc.", "labels": [], "entities": []}, {"text": "Unlabeled utterances similar to the movie domain are pulled from a month old web query logs and extracted over 2 million search queries from well-known sites, e.g., IMDB, Netflix, etc.", "labels": [], "entities": []}, {"text": "We filtered queries that are similar to our target set that start with wh-phrases ('what', 'who', etc.) as well as imperatives 'show', 'list', etc.", "labels": [], "entities": []}, {"text": "In addition, we extracted web n-grams and entity lists (see \u00a73) from movie related web sites, and online blogs and reviews.", "labels": [], "entities": []}, {"text": "We collected around 300K movie review and blog entries on the entities observed in our data.", "labels": [], "entities": []}, {"text": "We extract prior distributions for entities and n-grams to calculate entity list \u03b7 and word-tag \u03b2 priors (see \u00a73.1).", "labels": [], "entities": []}, {"text": "We use the Wall Street Journal (WSJ) section of the Penn Treebank as our labeled source data.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) section of the Penn Treebank", "start_pos": 11, "end_pos": 65, "type": "DATASET", "confidence": 0.952708287672563}]}, {"text": "Following previous research, we train on sections 00-18, comprised of 38,219 POS-tagged sentences.", "labels": [], "entities": []}, {"text": "To evaluate the domain adaptation (DA) approach and to compare with results reported by, we use the first and second half of QuestionBank () as our development and test sets (target).", "labels": [], "entities": [{"text": "domain adaptation (DA)", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.8114726662635803}, {"text": "QuestionBank", "start_pos": 125, "end_pos": 137, "type": "DATASET", "confidence": 0.9208410978317261}]}, {"text": "The QuestionBank contains 4000 POS-tagged questions, however it is difficult to tag with WSJ-trained taggers because the word order is different than WSJ and contains a test-set vocabulary that is twice as large as the one in the development set.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 150, "end_pos": 153, "type": "DATASET", "confidence": 0.9063063859939575}]}, {"text": "As for unlabeled data we crawled the web and collected around 100,000 questions that are similar in style and length to the ones in QuestionBank, e.g. \"wh\" questions.", "labels": [], "entities": [{"text": "QuestionBank", "start_pos": 132, "end_pos": 144, "type": "DATASET", "confidence": 0.9400705695152283}]}, {"text": "There are 36 different tag sets in the Penn dataset which includes tag labels for verbs, nouns, adjectives, adverbs, modal, determiners, prepositions, etc.", "labels": [], "entities": [{"text": "Penn dataset", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9866306185722351}]}, {"text": "More information about the Penn Tree-bank tag set can be found here (.", "labels": [], "entities": [{"text": "Penn Tree-bank tag set", "start_pos": 27, "end_pos": 49, "type": "DATASET", "confidence": 0.9958675056695938}]}, {"text": "Here, we want to demonstrate the performance of MTR model for capturing relationships between words and semantic tags against baseline topic models: LDA, HMTM, w-LDA.", "labels": [], "entities": []}, {"text": "We take the semantically labeled utterances from the movie target domain and use the first half for training and the rest for performance testing.", "labels": [], "entities": []}, {"text": "We use all the collected unlabeled web queries from the movie domain.", "labels": [], "entities": []}, {"text": "For fair comparison, each benchmark topic model is provided with prior information on word-semantic tag distributions based on the labeled training data, hence, each K latent topic is assigned to one of K semantic tags at the beginning of Gibbs sampling.", "labels": [], "entities": []}, {"text": "We evaluate the performance separately on descriptive tags, named-entities, and all tags together.", "labels": [], "entities": []}, {"text": "The performance of the four topic models are reported in.", "labels": [], "entities": []}, {"text": "LDA shows the worst performance, even though some supervision is provided byway of labeled semantic tags.", "labels": [], "entities": []}, {"text": "Although w-LDA improves semantic clustering performance over LDA, the fact that it does not have Markov properties makes it fall short behind MTR.", "labels": [], "entities": [{"text": "semantic clustering", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7357991635799408}, {"text": "MTR", "start_pos": 142, "end_pos": 145, "type": "DATASET", "confidence": 0.8014129400253296}]}, {"text": "As for the effect of word features in MTR, we see a 3% absolute performance gain over the second best performing HMTM baseline on named-entity tags, a 1% absolute gain on descriptive tags and a 2% absolute overall gain.", "labels": [], "entities": [{"text": "MTR", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9145928621292114}]}, {"text": "As expected, we see a drop in F-measure on all models on descriptive tags.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9991158843040466}]}, {"text": "We compare the performance of our SSL model to that of state-of-the-art models on semantic and syntactic tagging.", "labels": [], "entities": [{"text": "semantic and syntactic tagging", "start_pos": 82, "end_pos": 112, "type": "TASK", "confidence": 0.6369354724884033}]}, {"text": "Each SSL model is built using labeled training data from the source domain and unlabeled training data from target domain.", "labels": [], "entities": []}, {"text": "In we show the results on Movie and QuestionBank target test datasets.", "labels": [], "entities": [{"text": "Movie and QuestionBank target test datasets", "start_pos": 26, "end_pos": 69, "type": "DATASET", "confidence": 0.820741539200147}]}, {"text": "The results of SSL-Graph on QuestionBank is taken from ().", "labels": [], "entities": [{"text": "QuestionBank", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9596549868583679}]}, {"text": "The selftraining model, Self-CRF adds 3% improvement over supervised CRF models on movie domain, but does not improve syntactic tagging.", "labels": [], "entities": [{"text": "syntactic tagging", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.664995402097702}]}, {"text": "Because it is always inherently biased towards the source domain, self-training tends to reinforce the knowledge that the supervised model already has.", "labels": [], "entities": []}, {"text": "SSL-Graph works much better for both syntactic and semantic tagging compared to CRF and Self-CRF models.", "labels": [], "entities": []}, {"text": "Our Bayesian MTR efficiently extracts information from the unlabeled data for the target domain.", "labels": [], "entities": [{"text": "MTR", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.8907241821289062}]}, {"text": "Combined with retrospective training, R-SSL-MTR demonstrates noticeable improvements, \u223c2% on descriptive tags, and 1% absolute gains in overall semantic tag-ging performance over SSL-Graph.", "labels": [], "entities": []}, {"text": "On syntactic tagging, the two retrospective learning models is comparable, close to 1% improvement over the SSL-Graph and SSL-MTR.", "labels": [], "entities": [{"text": "syntactic tagging", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.6783497929573059}]}, {"text": "Here we focus on the accuracy of our models in tagging semantically ambiguous words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9969038367271423}, {"text": "tagging semantically ambiguous words", "start_pos": 47, "end_pos": 83, "type": "TASK", "confidence": 0.8763028383255005}]}, {"text": "We investigate words that have more than one observed semantic tag in training data, such as \"are there any genre movies available.\", \"remove all movies about [war] description .\").", "labels": [], "entities": []}, {"text": "Our corpus contained 30,000 unique vocabulary, 55% of which are contained in one or more semantic categories.", "labels": [], "entities": []}, {"text": "Only 6.5% of those are tagged as multiple categories (polysemous), which are the sources of semantic ambiguity.", "labels": [], "entities": []}, {"text": "shows the precision of two best models for most confused words.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9993329644203186}]}, {"text": "We compare our two best SSL models with different smoothing regularizes: R-SSL-MTR (MTR) and R-SSL-Graph (GRAPH).", "labels": [], "entities": []}, {"text": "We use precision and recall criterion on semantically confused words.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9995231628417969}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9976741671562195}]}, {"text": "In we show two most frequent descriptive tags; genre and description, and commonly misclassified words by the two models.", "labels": [], "entities": []}, {"text": "Results indicate that the R-SSL-MTR, performs better than the R-SSL-Graph, in activating the correct meaning of a word.", "labels": [], "entities": []}, {"text": "The results indicate that incorporating context information with MTR is an effective option for identifying semantic ambiguity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2:  Domain Adaptation performance  in F-measure on Semantic Tagging on  Movie Target domain and POS tagging on  QBank:QuestionBank. Best performing models  are bolded.", "labels": [], "entities": [{"text": "Semantic Tagging", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.678442507982254}, {"text": "Movie Target domain", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.9014553427696228}, {"text": "POS tagging", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.6634174138307571}]}, {"text": " Table 3: Classification performance in F-measure  for semantically ambiguous words on the most fre- quently confused descriptive tags in the movie do- main.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.8856490254402161}]}]}