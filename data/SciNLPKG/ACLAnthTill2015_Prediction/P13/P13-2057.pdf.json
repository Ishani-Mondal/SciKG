{"title": [{"text": "Using Context Vectors in Improving a Machine Translation System with Bridge Language", "labels": [], "entities": [{"text": "Improving a Machine Translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6441136077046394}]}], "abstractContent": [{"text": "Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong.", "labels": [], "entities": []}, {"text": "Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way.", "labels": [], "entities": []}, {"text": "We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations.", "labels": [], "entities": []}, {"text": "We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs.", "labels": [], "entities": []}, {"text": "Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9734793603420258}]}], "introductionContent": [{"text": "Parallel corpora as an important component of a statistical machine translation system are unfortunately unavailable for all pairs of languages, particularly in low resource languages and also producing it consumes time and cost.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.6410948435465494}]}, {"text": "So, new ideas have been developed about how to make a MT system which has lower dependency on parallel data like using comparable corpora for improving performance of a MT system with small parallel corpora or making a MT system without parallel corpora.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.978758692741394}]}, {"text": "Comparable corpora have segments with the same translations.", "labels": [], "entities": []}, {"text": "These segments might be in the form of words, phrases or sentences.", "labels": [], "entities": []}, {"text": "So, this extracted information can be added to the parallel corpus or might be used for adaption of the language model or translation model.", "labels": [], "entities": []}, {"text": "Comparable corpora are easily available resources.", "labels": [], "entities": []}, {"text": "All texts that are about the same topic can be considered as comparable corpora.", "labels": [], "entities": []}, {"text": "Another idea for solving the scarce resource problem is to use a high resource language as a pivot to bridge between source and target languages.", "labels": [], "entities": []}, {"text": "In this paper we use the bridge technique to make a source-target system and we will prune the phrase table of this system.", "labels": [], "entities": []}, {"text": "In Section 2, the related works of the bridge approach are considered, in Section 3 the proposed approach will be explained and it will be shown how to prune the phrase table using context vectors, and experiments on GermanEnglish-Farsi systems will be presented in Section 4.", "labels": [], "entities": [{"text": "GermanEnglish-Farsi", "start_pos": 217, "end_pos": 236, "type": "DATASET", "confidence": 0.9365190267562866}]}], "datasetContent": [{"text": "In this work, we try to make a German-Farsi system without using parallel corpora.", "labels": [], "entities": []}, {"text": "We use English language as abridge between German and Farsi languages because English language is a high resource language and parallel corpora of German-English and English-Farsi are available.", "labels": [], "entities": []}, {"text": "We use Moses0 F 1 ( as the MT decoder and IRSTLM1 F 2 tools for making the language model.", "labels": [], "entities": [{"text": "Moses0 F 1", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.5725253423055013}, {"text": "MT decoder", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.7652437686920166}, {"text": "IRSTLM1 F 2", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.8403877019882202}]}, {"text": "shows the statistics of the corpora that we have used in our experiments.", "labels": [], "entities": []}, {"text": "The German-English corpus is from Verbmobil project ().", "labels": [], "entities": [{"text": "Verbmobil project", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.9344376027584076}]}, {"text": "We manually translate 22K English sentences to Farsi to build a small Farsi-English-German corpus.", "labels": [], "entities": []}, {"text": "Therefore, we have a small EnglishGerman corpus as well.", "labels": [], "entities": [{"text": "EnglishGerman corpus", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.9508276879787445}]}, {"text": "With the German-English parallel corpus and an additional German-English dictionary with 118480 entries we have made a German-English (De-En) system and with English-Farsi parallel corpus we have made a German-Farsi (En-Fa) system.", "labels": [], "entities": []}, {"text": "The BLEU score of these systems are shown in.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9638845026493073}]}, {"text": "Now, we create a translation system by combining phrase tables of De-En and En-Fa systems.", "labels": [], "entities": []}, {"text": "Details of creating the source-target system are explained in Section 3.1..", "labels": [], "entities": []}, {"text": "Information of two parallel systems that are used in our experiments.", "labels": [], "entities": []}, {"text": "The size of the phrase table is about 55.7 MB.", "labels": [], "entities": []}, {"text": "Then, we apply the pruning method that we explained in Section 3.2.", "labels": [], "entities": []}, {"text": "With this method only the phrases are kept that their context vectors are similar to each other.", "labels": [], "entities": []}, {"text": "For each source phrase the 35-most similar target translations are kept.", "labels": [], "entities": []}, {"text": "The number of phrases in the phrase table is decreased dramatically while the performance of the system is increased by 2.8 percent BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9992097616195679}]}, {"text": "The results of these experiments are shown in.", "labels": [], "entities": []}, {"text": "The last row in this table is the result of using small parallel corpus to build GermanFarsi system.", "labels": [], "entities": [{"text": "GermanFarsi system", "start_pos": 81, "end_pos": 99, "type": "DATASET", "confidence": 0.9774266183376312}]}, {"text": "We observe that the pruning method has gain better results compared to the system trained on the parallel corpus.", "labels": [], "entities": []}, {"text": "This is maybe because of some translations that are made in the parallel system and do not have enough training data and their probabilities are not precise.", "labels": [], "entities": []}, {"text": "But when we use context vectors to measure the contextual similarity of phrases and their translations, the impact of these training samples are decreased.", "labels": [], "entities": []}, {"text": "Performance results of different ways of bridging Now, we run a series of significance tests to measure the superiority of each method.", "labels": [], "entities": []}, {"text": "In the first significance test, we set the pruned system as our base system and we compare the result of the pseudo parallel corpus system with it, the significance level is 72%.", "labels": [], "entities": [{"text": "significance level", "start_pos": 152, "end_pos": 170, "type": "METRIC", "confidence": 0.9713141024112701}]}, {"text": "For another significance test we set the combined phrase table system without pruning as our base system and we compare the result of the pruned system with it, the significance level is 100%.", "labels": [], "entities": [{"text": "significance level", "start_pos": 165, "end_pos": 183, "type": "METRIC", "confidence": 0.9690693914890289}]}, {"text": "In the last significance test we set the combined phrase table system without pruning as our base system and we compare the result of the pseudo system with it, the significance level is 99%.", "labels": [], "entities": [{"text": "significance level", "start_pos": 165, "end_pos": 183, "type": "METRIC", "confidence": 0.9722448289394379}]}, {"text": "Therefore, we can conclude the proposed method obtains the best results and its difference with pseudo parallel corpus method is not significant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1.  Now, we create a translation system by  combining phrase tables of De-En and En-Fa  systems. Details of creating the source-target  system are explained in Section 3.1.", "labels": [], "entities": []}, {"text": " Table 1. Information of two parallel systems that  are used in our experiments.", "labels": [], "entities": []}, {"text": " Table 2. The last row in this table is the result of  using small parallel corpus to build German- Farsi system. We observe that the pruning  method has gain better results compared to the  system trained on the parallel corpus. This is  maybe because of some translations that are  made in the parallel system and do not have  enough training data and their probabilities are  not precise. But when we use context vectors to  measure the contextual similarity of phrases and  their translations, the impact of these training  samples are decreased. In", "labels": [], "entities": [{"text": "German- Farsi system", "start_pos": 92, "end_pos": 112, "type": "DATASET", "confidence": 0.9194366037845612}]}, {"text": " Table 3, two wrong  phrase pairs that pruning method has removed  them are shown.", "labels": [], "entities": []}, {"text": " Table 3. Sample wrong translations that the  prunning method removed them.", "labels": [], "entities": [{"text": "Sample wrong translations", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8636455138524374}]}, {"text": " Table 4. Performance results of different ways of  bridging", "labels": [], "entities": [{"text": "bridging", "start_pos": 52, "end_pos": 60, "type": "TASK", "confidence": 0.9056625366210938}]}]}