{"title": [{"text": "Mining Informal Language from Chinese Microtext: Joint Word Recognition and Segmentation", "labels": [], "entities": [{"text": "Joint Word Recognition", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.6360158423582712}]}], "abstractContent": [{"text": "We address the problem of informal word recognition in Chinese microblogs.", "labels": [], "entities": [{"text": "informal word recognition", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.6356076697508494}]}, {"text": "A key problem is the lack of word delimiters in Chinese.", "labels": [], "entities": []}, {"text": "We exploit this reliance as an opportunity: recognizing the relation between informal word recognition and Chi-nese word segmentation, we propose to model the two tasks jointly.", "labels": [], "entities": [{"text": "informal word recognition", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.6306537886460623}, {"text": "Chi-nese word segmentation", "start_pos": 107, "end_pos": 133, "type": "TASK", "confidence": 0.6541729867458344}]}, {"text": "Our joint inference method significantly outperforms baseline systems that conduct the tasks individually or sequentially.", "labels": [], "entities": []}], "introductionContent": [{"text": "User generated content (UGC) -including microblogs, comments, SMS, chat and instant messaging -collectively referred to as microtext by or network informal language by, is the hallmark of the participatory Web.", "labels": [], "entities": []}, {"text": "While a rich source that many applications are interested in mining for knowledge, microtext processing is difficult to process.", "labels": [], "entities": [{"text": "microtext processing", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.8144374787807465}]}, {"text": "One key reason for this difficulty is the ubiquitous presence of informal words -anomalous terms that manifest as ad hoc abbreviations, neologisms, unconventional spellings and phonetic substitutions.", "labels": [], "entities": []}, {"text": "Such informality is often present in oral conversation, and user-generated microblogs reflect this informality.", "labels": [], "entities": []}, {"text": "Natural language processing (NLP) tools largely fail to work properly on microtext, as they have largely been trained on formally written text (i.e., newswire).", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7449827591578165}]}, {"text": "Recent work has started to address these shortcomings ().", "labels": [], "entities": []}, {"text": "Informal words and their usage in microtext evolves quickly, following social trends and news events.", "labels": [], "entities": []}, {"text": "These characteristics make it difficult for lexicographers to compile lexica to keep with the pace of language change.", "labels": [], "entities": []}, {"text": "We focus on this problem in the Chinese language.", "labels": [], "entities": []}, {"text": "Through our analysis of a gathered Chinese microblog corpus, we observe that Chinese informal words originate from three primary sources, as given in.", "labels": [], "entities": [{"text": "Chinese microblog corpus", "start_pos": 35, "end_pos": 59, "type": "DATASET", "confidence": 0.7516759832700094}]}, {"text": "But unlike noisy words in English, Chinese informal words are more difficult to mechanically recognize for two critical reasons: first, Chinese does not employ word delimiters; second, Chinese informal words combine numbers, alphabetic letters and Chinese characters.", "labels": [], "entities": []}, {"text": "Techniques for English informal word detection that rely on word boundaries and informal word orthography need to be adapted for Chinese.", "labels": [], "entities": [{"text": "English informal word detection", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.6039928868412971}]}, {"text": "Consider the microtext \"\u2022g \u2020\" (meaning \"Don't tell me the spoilers (to a movie or joke)\", also in.", "labels": [], "entities": []}, {"text": "If \"\u2022\" (\"don't\") and \" \u2020\" (past tense marker) are correctly recognized as two words, we may predict the previously unseen characters \"g\" (\"tell spoilers\") as an informal word, based on the learned Chinese language patterns.", "labels": [], "entities": []}, {"text": "However, state-of-the-art Chinese segmenters 1 incorrectly yield \" \u2022 g \u2020\", preferring to chunk \" \u2020\" (\"thoroughly\") as a word, as they do not consider the possibility that \"g\" (\"spoiler\") could bean informal word.", "labels": [], "entities": []}, {"text": "This example illustrates the mutual dependency between Chinese word segmentation (henceforth, CWS) and informal word recognition (IWR) that should be solved jointly.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.5871926844120026}, {"text": "informal word recognition (IWR)", "start_pos": 103, "end_pos": 134, "type": "TASK", "confidence": 0.7941004186868668}]}, {"text": "Hence, rather than pipeline the two processes serially as previous work, we formulate it as a twolayer sequential labeling problem.", "labels": [], "entities": []}, {"text": "We employ factorial conditional random field (FCRF) to solve both CWS and IWR jointly.", "labels": [], "entities": [{"text": "IWR", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.5317727327346802}]}, {"text": "To our best knowledge, this is the first work that shows how Chinese microtext can be analyzed from raw text to: Our classification of Chinese informal words as originating from three primary sources.", "labels": [], "entities": []}, {"text": "For Phonetic Substitutions, pronunciation is indicated by the phonetic Pinyin transcription system.", "labels": [], "entities": [{"text": "Phonetic Substitutions", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8419841825962067}]}], "datasetContent": [{"text": "We discuss the dataset, baseline systems and experiments results in detail in the following.", "labels": [], "entities": []}, {"text": "We use the standard metrics of precision, recall and F 1 for the IWR task.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.999763548374176}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9995238780975342}, {"text": "F 1", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9888594150543213}, {"text": "IWR task", "start_pos": 65, "end_pos": 73, "type": "TASK", "confidence": 0.8618717193603516}]}, {"text": "Only words that exactly match the manually-annotated labels are considered correct.", "labels": [], "entities": []}, {"text": "For example given the sentence \" H\u00cb \u00cb \u00cbH H H}b\" (\"H\u00d9 \u00d9 \u00d9H H H}b\"; \"How delicious it is\"), if the IWR component identifies \"\u00cb H\" as an informal word, it will be considered correct, whereas both \"\u00cbH}\" and \"\u00cb\" are deemed incorrect.", "labels": [], "entities": [{"text": "IWR", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.7455551624298096}]}, {"text": "For CWS evaluation, we employ the conventional scoring script provided in SIGHAN-5, which also provides out-of-vocabulary recall (OOVR).", "labels": [], "entities": [{"text": "CWS evaluation", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9285725355148315}, {"text": "recall (OOVR)", "start_pos": 122, "end_pos": 135, "type": "METRIC", "confidence": 0.8888534009456635}]}, {"text": "To determine statistical significance of the improvements, we also compute paired, one-tailed t tests.", "labels": [], "entities": []}, {"text": "As pointed out by Yeh and Alexander (2000), the randomization method is more reliable in measuring the significance of F 1 through handling non-linear functions of random variables.", "labels": [], "entities": []}, {"text": "Thus we employ Pad\u00f3 (2006)'s implementation of randomization algorithm to measure the significance of F 1 .  The goal of our experiments is to answer the following research questions: RQ1 Do the two tasks of CWS and IWR benefit from each other?", "labels": [], "entities": [{"text": "significance of F 1", "start_pos": 86, "end_pos": 105, "type": "METRIC", "confidence": 0.8099284470081329}, {"text": "RQ1", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.8277903199195862}]}, {"text": "RQ2 Is jointly modeling both tasks more efficient than conducting each task separately or sequentially?", "labels": [], "entities": []}, {"text": "RQ3 What is the upper bound improvement that can be achieved with perfect support from the opposing task?", "labels": [], "entities": [{"text": "RQ3", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7511691451072693}]}, {"text": "RQ4 Are the features we designed for the joint inference method effective?", "labels": [], "entities": [{"text": "RQ4", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9092054963111877}]}, {"text": "RQ5 Is there a significant difference between the performance of the joint inference of a crossproduct SVM and our proposed FCRF?", "labels": [], "entities": [{"text": "RQ5", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7405605912208557}, {"text": "FCRF", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.8143882751464844}]}, {"text": "In general, our FCRF yields the best performance among all systems (top portion of), answering RQ1.", "labels": [], "entities": [{"text": "FCRF", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.6105985641479492}, {"text": "RQ1", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.36417585611343384}]}, {"text": "Given microblog posts as test data, the F 1 of ICTCLAS drops from 0.985 10 to 0.698, clearly showing the difficulty of processing microtext.", "labels": [], "entities": [{"text": "F 1", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9933706521987915}, {"text": "ICTCLAS", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.6159697771072388}]}, {"text": "The sequential LCRF model and FCRF model both outperform the baselines, which means with the novel features shared by the two tasks, CWS benefits significantly from the results of IWR.", "labels": [], "entities": []}, {"text": "Hence our segmenter outperforms the existing segmenters by tackling one of the bottlenecks of recognizing informal words in Chinese microtext.", "labels": [], "entities": []}, {"text": "For RQ4, to evaluate the effectiveness of our newly-introduced feature sets (those marked with \"*\" in Section 2.3), we also test a FCRF (FCRF \u2212new ) without our new features.", "labels": [], "entities": [{"text": "RQ4", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8100361227989197}, {"text": "FCRF", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.5785436630249023}]}, {"text": "According to, performance drops by a significant amount: 0.088 F 1 on CWS and 0.198 F 1 on IWR.", "labels": [], "entities": [{"text": "F 1", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9894784986972809}, {"text": "CWS", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8838211297988892}, {"text": "F 1", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9831347167491913}, {"text": "IWR", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9349828362464905}]}, {"text": "FCRF \u2212new makes many mistakes identical to the baselines: segmenting informal words into several single-character words and chunking adjacent characters from informal and formal words together., our SVM trained to predict the cross-product CWS/IWR classification (SVM-JC) performs quite well on its own.", "labels": [], "entities": [{"text": "FCRF", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9228798747062683}]}, {"text": "Unsurprisingly, it does not outperform our proposed FCRF, which has access to more structural correlation among the CWS and IWR labels.", "labels": [], "entities": [{"text": "FCRF", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.9228717684745789}]}, {"text": "SVM-JC significantly (p < 0.001) outperforms the baseline SVM system by 0.151 in the IWR task, which we think is partially explained by its good performance (0.761) on the CWS task.", "labels": [], "entities": []}, {"text": "The over-prediction tendency of the individual SVM is largely solved by simultaneously modeling the CWS task, whereas FCRF turns out to be more effective in solving joint inference problem, although in a weaker trend in terms of the statistical significance (p < 0.05).", "labels": [], "entities": [{"text": "FCRF", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.6173220872879028}]}], "tableCaptions": [{"text": " Table 2: Performance comparison on the CWS  task. The two bottom-most rows show upper  bound performance. ' \u2021'('  *  ') in the top four lines  indicates statistical significance at p < 0.001  (0.05) when compared with the previous row.  Symbols in the bottom two lines indicate signifi- cant difference between upper bound systems and  their corresponding counterparts.", "labels": [], "entities": [{"text": "CWS  task", "start_pos": 40, "end_pos": 49, "type": "TASK", "confidence": 0.9062239527702332}]}, {"text": " Table 3: Performance comparison on the IWR  task. ' \u2021' or '  *  ' in the top four rows indicates sta- tistical significance at p < 0.001 or < 0.05 com- pared with the previous row. Symbols in the bot- tom two rows indicate differences between upper  bound systems and their counterparts.", "labels": [], "entities": [{"text": "IWR  task", "start_pos": 40, "end_pos": 49, "type": "TASK", "confidence": 0.7427739202976227}, {"text": "sta- tistical significance", "start_pos": 98, "end_pos": 124, "type": "METRIC", "confidence": 0.8556525558233261}]}, {"text": " Table 4: F 1 comparison between FCRF and  FCRF \u2212new . ('  *  ') indicates statistical significance  at p < 0.05 when compared with the previous row.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9891124367713928}, {"text": "FCRF", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.7517387270927429}, {"text": "FCRF", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8727464079856873}, {"text": "statistical significance", "start_pos": 75, "end_pos": 99, "type": "METRIC", "confidence": 0.9054986834526062}]}, {"text": " Table 5: F 1 comparison between SVM, SVM-JC  and FCRF. ' \u2021'('  *  ') indicates statistical significance  at p < 0.001 (0.05) when compared with the pre- vious row.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9893775582313538}, {"text": "FCRF", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.8367757201194763}, {"text": "statistical significance", "start_pos": 80, "end_pos": 104, "type": "METRIC", "confidence": 0.8503423929214478}]}]}