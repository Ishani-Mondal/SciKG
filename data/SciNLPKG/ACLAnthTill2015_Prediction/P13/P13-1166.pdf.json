{"title": [{"text": "Offspring from Reproduction Problems: What Replication Failure Teaches Us", "labels": [], "entities": []}], "abstractContent": [{"text": "Repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing work.", "labels": [], "entities": [{"text": "Repeating experiments", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9220580458641052}]}, {"text": "We present two concrete use cases involving key techniques in the NLP domain for which we show that reproducing results is still difficult.", "labels": [], "entities": []}, {"text": "We show that the deviation that can be found in reproduction efforts leads to questions about how our results should be interpreted.", "labels": [], "entities": []}, {"text": "Moreover, investigating these deviations provides new insights and a deeper understanding of the examined techniques.", "labels": [], "entities": []}, {"text": "We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers.", "labels": [], "entities": []}, {"text": "Our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should betaken in interpreting our results and more research involving systematic testing of methods is required in our field.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research is a collaborative effort to increase knowledge.", "labels": [], "entities": []}, {"text": "While it includes validating previous approaches, our experience is that most research output in our field focuses on presenting new approaches, and to a somewhat lesser extent building upon existing work.", "labels": [], "entities": []}, {"text": "In this paper, we argue that the value of research that attempts to replicate previous approaches goes beyond simply validating what is already known.", "labels": [], "entities": []}, {"text": "It is also an essential aspect for building upon existing approaches.", "labels": [], "entities": []}, {"text": "Especially when validation fails or variations in results are found, systematic testing helps to obtain a clearer picture of both the approach itself and of the meaning of state-of-theart results leading to a better insight into the quality of new approaches in relation to previous work.", "labels": [], "entities": []}, {"text": "We support our claims by presenting two use cases that aim to reproduce results of previous work in two key NLP technologies: measuring WordNet similarity and Named Entity Recognition (NER).", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 159, "end_pos": 189, "type": "TASK", "confidence": 0.732742190361023}]}, {"text": "Besides highlighting the difficulty of repeating other researchers' work, new insights about the approaches emerged that were not presented in the original papers.", "labels": [], "entities": []}, {"text": "This last point shows that reproducing results is not merely part of good practice in science, but also an essential part in gaining a better understanding of the methods we use.", "labels": [], "entities": []}, {"text": "Likewise, the problems we face in reproducing previous results are not merely frustrating inconveniences, but also pointers to research questions that deserve deeper investigation.", "labels": [], "entities": []}, {"text": "We investigated five aspects that cause experimental variation that are not typically described in publications: preprocessing (e.g. tokenisation), experimental setup (e.g. splitting data for cross-validation), versioning (e.g. which version of WordNet), system output (e.g. the exact features used for individual tokens in NER), and system variation (e.g. treatment of ties).", "labels": [], "entities": []}, {"text": "As such, reproduction provides a platform for systematically testing individual aspects of an approach that contribute to a given result.", "labels": [], "entities": []}, {"text": "What is the influence of the size of the dataset, for example?", "labels": [], "entities": []}, {"text": "How does using a different dataset affect the results?", "labels": [], "entities": []}, {"text": "What is a reasonable divergence between different runs of the same experiment?", "labels": [], "entities": []}, {"text": "Finding answers to these questions enables us to better interpret our state-of-the-art results.", "labels": [], "entities": []}, {"text": "Moreover, the experiments in this paper show that even while strictly trying to replicate a previous experiment, results may vary up to a point where they lead to different answers to the main question addressed by the experiment.", "labels": [], "entities": []}, {"text": "The WordNet similarity experiment use case compares the performance of different similarity measures.", "labels": [], "entities": []}, {"text": "We will show that the answer as to which measure works best changes depending on factors such as the gold standard used, the strategy towards partof-speech or the ranking coefficient, all aspects that are typically not addressed in the literature.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are the following: 1) An in-depth analysis of two reproduction use cases in NLP 2) New insights into the state-of-the-art results for WordNet similarities and NER, found because of problems in reproducing prior research 3) A categorisation of aspects influencing reproduction of experiments and suggestions on testing their influence systematically The code, data and experimental setup for the WordNet experiments are available at http://github.com/antske/ WordNetSimilarity, and for the NER experiments at http://github.com/Mvanerp/ NER.", "labels": [], "entities": [{"text": "WordNetSimilarity", "start_pos": 495, "end_pos": 512, "type": "DATASET", "confidence": 0.9359169006347656}]}, {"text": "The experiments presented in this paper have been repeated by colleagues not involved in the development of the software using the code included in these repositories.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, previous work is discussed.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 describe our real-world use cases.", "labels": [], "entities": []}, {"text": "In Section 5, we present our observations, followed by a more general discussion in Section 6.", "labels": [], "entities": []}, {"text": "In Section 7, we present our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "This experiment can be seen as a real-world case of the sad tale of the Zigglebottom tagger).", "labels": [], "entities": [{"text": "Zigglebottom tagger", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.6629030704498291}]}, {"text": "The (fictional) Zigglebottom tagger is a tagger with spectacular results that looks like it will solve some major problems in your system.", "labels": [], "entities": [{"text": "Zigglebottom tagger", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.6553607881069183}]}, {"text": "However, the code is not available and anew implementation does not yield the same results.", "labels": [], "entities": []}, {"text": "The original authors cannot provide the necessary details to reproduce their results, because most of the work has been done by a PhD student who has finished and moved onto something else.", "labels": [], "entities": []}, {"text": "In the end, the newly implemented Zigglebottom tagger is not used, because it does not lead to the promised better results and all effort went to waste.", "labels": [], "entities": [{"text": "Zigglebottom tagger", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.6541323661804199}]}, {"text": "Van Erp was interested in the NER approach presented in.", "labels": [], "entities": [{"text": "NER", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.927593469619751}]}, {"text": "Unfortunately, the code could not be made available, so she decided to reimplement the approach.", "labels": [], "entities": []}, {"text": "Despite feedback from Freire about particular details of the system, results remained 20 points below those reported in in overall F-score (Van Erp and Van der Meij, 2013).", "labels": [], "entities": [{"text": "F-score", "start_pos": 131, "end_pos": 138, "type": "METRIC", "confidence": 0.9971981048583984}]}, {"text": "The reimplementation process involved choices about seemingly small details such as rounding to how many decimals, how to tokenise or how much data cleanup to perform (normalisation of non-alphanumeric characters for example).", "labels": [], "entities": []}, {"text": "Trying different parameter combinations for feature generation and the algorithm never yielded the exact same results as.", "labels": [], "entities": [{"text": "feature generation", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.6974596828222275}]}, {"text": "The results of the best run in our first reproduction attempt, together with the original results from are presented in.", "labels": [], "entities": []}, {"text": "Van Erp and Van der Meij (2013) provide an overview of the implementation efforts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Variation WordNet measures' results", "labels": [], "entities": []}, {"text": " Table 2: Variations per category", "labels": [], "entities": []}, {"text": " Table 3: Precision, recall and F \u03b2=1 scores for the original experiments from Freire et al.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.999009370803833}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.999082088470459}, {"text": "F \u03b2=1 scores", "start_pos": 32, "end_pos": 44, "type": "METRIC", "confidence": 0.973538076877594}]}]}