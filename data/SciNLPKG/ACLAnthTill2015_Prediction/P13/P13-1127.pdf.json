{"title": [{"text": "From Natural Language Specifications to Program Input Parsers", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a method for automatically generating input parsers from English specifications of input file formats.", "labels": [], "entities": []}, {"text": "We use a Bayesian generative model to capture relevant natural language phenomena and translate the English specification into a specification tree, which is then translated into a C++ input parser.", "labels": [], "entities": []}, {"text": "We model the problem as a joint dependency parsing and semantic role labeling task.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7413616478443146}, {"text": "semantic role labeling task", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.6925941780209541}]}, {"text": "Our method is based on two sources of information: (1) the correlation between the text and the specification tree and (2) noisy supervision as determined by the success of the generated C++ parser in reading input examples.", "labels": [], "entities": []}, {"text": "Our results show that our approach achieves 80.0% F-Score accuracy compared to an F-Score of 66.7% produced by a state-of-the-art semantic parser on a dataset of input format specifications from the ACM International Collegiate Programming Contest (which were written in English for humans with no intention of providing support for automated processing).", "labels": [], "entities": [{"text": "F-Score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9981315732002258}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.8959797024726868}, {"text": "F-Score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9975888729095459}, {"text": "ACM International Collegiate Programming Contest", "start_pos": 199, "end_pos": 247, "type": "TASK", "confidence": 0.43446933627128603}]}], "introductionContent": [{"text": "The general problem of translating natural language specifications into executable code has been around since the field of computer science was founded.", "labels": [], "entities": [{"text": "translating natural language specifications", "start_pos": 23, "end_pos": 66, "type": "TASK", "confidence": 0.8252541422843933}]}, {"text": "Early attempts to solve this problem produced what were essentially verbose, clumsy, and ultimately unsuccessful versions of standard formal programming languages.", "labels": [], "entities": []}, {"text": "In recent years The code, data, and experimental setup for this research are available at http://groups.csail.mit.edu/rbg/code/nl2p: An example of (a) one natural language specification describing program input data; (b) the corresponding specification tree representing the program input structure; and (c) two input examples however, researchers have had success addressing specific aspects of this problem.", "labels": [], "entities": []}, {"text": "Recent advances in this area include the successful translation of natural language commands into database queries) and the successful mapping of natural language instructions into Windows command sequences.", "labels": [], "entities": [{"text": "translation of natural language commands into database queries", "start_pos": 52, "end_pos": 114, "type": "TASK", "confidence": 0.8058522418141365}]}, {"text": "In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates data structures for holding the data.", "labels": [], "entities": [{"text": "translation of natural language input specifications", "start_pos": 73, "end_pos": 125, "type": "TASK", "confidence": 0.7879588504632314}]}, {"text": "The need to automate this task arises because input format specifications are almost always described in natural languages, with these specifications then manually translated by a programmer into the code for reading the program inputs.", "labels": [], "entities": []}, {"text": "Our method highlights potential to automate this translation, thereby eliminating the manual software development overhead.", "labels": [], "entities": [{"text": "translation", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9782774448394775}]}, {"text": "Consider the text specification in.", "labels": [], "entities": []}, {"text": "If the desired parser is implemented in C++, it should create a C++ class whose instance objects hold the different fields of the input.", "labels": [], "entities": []}, {"text": "For example, one of the fields of this class is an integer, i.e., \"a single integer T\" identified in the text specification in.", "labels": [], "entities": []}, {"text": "Instead of directly generating code from the text specification, we first translate the specification into a specification tree (see, then map this tree into parser code (see).", "labels": [], "entities": []}, {"text": "We focus on the translation from the text specification to the specification tree.", "labels": [], "entities": []}, {"text": "We assume that each text specification is accompanied by a set of input examples that the desired input parser is required to successfully read.", "labels": [], "entities": []}, {"text": "In standard software development contexts, such input examples are usually available and are used to test the correctness of the input parser.", "labels": [], "entities": []}, {"text": "Note that this source of supervision is noisy -the generated parser may still be incorrect even when it successfully reads all of the input examples.", "labels": [], "entities": []}, {"text": "Specifically, the parser may interpret the input examples differently from the text specification.", "labels": [], "entities": []}, {"text": "For example, the program input in can be interpreted simply as a list of strings.", "labels": [], "entities": []}, {"text": "The parser may also fail to parse some correctly formatted input files not in the set of input examples.", "labels": [], "entities": []}, {"text": "Therefore, our goal is to design a technique that can effectively learn from this weak supervision.", "labels": [], "entities": []}, {"text": "We model our problem as a joint dependency parsing and role labeling task, assuming a Bayesian generative process.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7416533827781677}, {"text": "role labeling task", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.775746633609136}]}, {"text": "The distribution over the space of specification trees is informed by two sources of information: (1) the correlation between the text and the corresponding specification tree and (2) the success of the generated parser in reading input examples.", "labels": [], "entities": []}, {"text": "Our method uses a joint probability distribution to take both of these sources of information into account, and uses a sampling framework for the inference of specifi-2 During the second step of the process, the specification tree is deterministically translated into code.: Input parser code for reading input files specified in. cation trees given text specifications.", "labels": [], "entities": []}, {"text": "A specification tree is rejected in the sampling framework if the corresponding code fails to successfully read all of the input examples.", "labels": [], "entities": []}, {"text": "The sampling framework also rejects the tree if the text/specification tree pair has low probability.", "labels": [], "entities": []}, {"text": "We evaluate our method on a dataset of input specifications from ACM International Collegiate Programming Contests, along with the corresponding input examples.", "labels": [], "entities": []}, {"text": "These specifications were written for human programmers with no intention of providing support for automated processing.", "labels": [], "entities": []}, {"text": "However, when trained using the noisy supervision, our method achieves substantially more accurate translations than a state-of-the-art semantic parser () (specifically, 80.0% in F-Score compared to an F-Score of 66.7%).", "labels": [], "entities": [{"text": "F-Score", "start_pos": 179, "end_pos": 186, "type": "METRIC", "confidence": 0.9964650869369507}, {"text": "F-Score", "start_pos": 202, "end_pos": 209, "type": "METRIC", "confidence": 0.9725220799446106}]}, {"text": "The strength of our model in the face of such weak supervision is also highlighted by the fact that it retains an F-Score of 77% even when only one input example is provided for each input Your program is supposed to read the input from the standard input and write its output to the standard output.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.9992209672927856}]}, {"text": "The first line of the input contains one integer N.", "labels": [], "entities": []}, {"text": "N lines follow, the i-th of them contains two real numbers Xi, Yi separated by a single space -the coordinates of the i-th house.", "labels": [], "entities": []}, {"text": "Each of the following lines contains four real numbers separated by a single space.", "labels": [], "entities": []}, {"text": "These numbers are the coordinates of two different points (X1, Y1) and (X2, Y2), lying on the highway.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets: Our dataset consists of problem descriptions from ACM International Collegiate Programming Contests.", "labels": [], "entities": [{"text": "ACM International Collegiate Programming Contests", "start_pos": 60, "end_pos": 109, "type": "TASK", "confidence": 0.5632789134979248}]}, {"text": "We collected 106 problems from ACM-ICPC training websites.", "labels": [], "entities": [{"text": "ACM-ICPC training websites", "start_pos": 31, "end_pos": 57, "type": "DATASET", "confidence": 0.8306283752123514}]}, {"text": "From each problem description, we extracted the portion that provides input specifications.", "labels": [], "entities": []}, {"text": "Because the test input examples are not publicly available on the ACM-ICPC training websites, for each specification, we wrote simple programs to generate 100 random input examples.", "labels": [], "entities": []}, {"text": "presents statistics for the text specification set.", "labels": [], "entities": []}, {"text": "The data set consists of 424 sentences, where an average sentence contains 17.3 words.", "labels": [], "entities": []}, {"text": "The data set contains 781 unique words.", "labels": [], "entities": []}, {"text": "The length of each text specification varies from a single sentence to eight sentences.", "labels": [], "entities": []}, {"text": "The difference between the average and median number of trees is large.", "labels": [], "entities": []}, {"text": "This is because half of the specifications are relatively simple and have a small number of possible trees, while a few difficult specifications have over thousands of possible trees (as the number of trees grows exponentially when the text length increases).", "labels": [], "entities": []}, {"text": "Evaluation Metrics: We evaluate the model performance in terms of its success in generating a formal grammar that correctly represents the input format (see).", "labels": [], "entities": []}, {"text": "As a gold annotation, we construct formal grammars for all text specifications.", "labels": [], "entities": []}, {"text": "Our results are generated by automatically comparing the machine-generated grammars with their golden counterparts.", "labels": [], "entities": []}, {"text": "If the formal grammar is correct, then the generated C++ parser will correctly read the input file into corresponding C++ data structures.", "labels": [], "entities": []}, {"text": "We use Recall and Precision as evaluation measures: where the produced structures are the positive structures returned by our framework whose corresponding code successfully reads all input examples (see Algorithm 1 line 18).", "labels": [], "entities": [{"text": "Recall", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.7901462316513062}, {"text": "Precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9274656772613525}]}, {"text": "Note the number of produced structures maybe less than the number of text specifications, because structures that fail the input test are not returned.", "labels": [], "entities": []}, {"text": "Baselines: To evaluate the performance of our model, we compare against four baselines.", "labels": [], "entities": []}, {"text": "The No Learning baseline is a variant of our model that selects a specification tree without learning feature correspondence.", "labels": [], "entities": []}, {"text": "It continues sampling a specification tree for each text specification until it finds one which successfully reads all of the input examples.", "labels": [], "entities": []}, {"text": "The second baseline Aggressive is a state-ofthe-art semantic parsing framework (.", "labels": [], "entities": [{"text": "Aggressive", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.85094153881073}, {"text": "semantic parsing", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7609290480613708}]}, {"text": "The framework repeatedly predicts hidden structures (specification trees in our case) using a structure learner, and trains the structure learner based on the execution feedback of its predictions.", "labels": [], "entities": []}, {"text": "Specifically, at each iteration the structure learner predicts the most plausible specification tree for each text document: Depending on whether the corresponding code reads all input examples successfully or not, the (w i , ti ) pairs are added as an positive or negative sample to populate a training set.", "labels": [], "entities": []}, {"text": "After each iteration the structure learner is re-trained with the training samples to improve the prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9670143723487854}]}, {"text": "In our experiment, we follow (Clarke et al., We take the name Aggressive from this paper.", "labels": [], "entities": [{"text": "Aggressive", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.8733646273612976}]}, {"text": "Comparison with Baselines presents the performance of various models in predicting correct specification trees.", "labels": [], "entities": []}, {"text": "As can be seen, our model achieves an F-Score of 80%.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9996147155761719}]}, {"text": "Our model therefore significantly outperforms the No Learning baseline (by more than 25%).", "labels": [], "entities": []}, {"text": "Note that the No Learning baseline achieves a low Precision of 57.2%.", "labels": [], "entities": [{"text": "Precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9997355341911316}]}, {"text": "This low precision reflects the noisiness of the weak supervision -nearly one half of the parsers produced by No Learning are actually incorrect even though they read all of the input examples without error.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.998035728931427}]}, {"text": "This comparison shows the importance of capturing correlations between the specification trees and their text descriptions.", "labels": [], "entities": []}, {"text": "The input contains several testcases.", "labels": [], "entities": []}, {"text": "Each is specified by two strings S, T of alphanumeric ASCII characters Because our model learns correlations via feature representations, it produces substantially more accurate translations.", "labels": [], "entities": []}, {"text": "While both the Full Model and Aggressive baseline use the same source of feedback, they capitalize on it in a different way.", "labels": [], "entities": []}, {"text": "The baseline uses the noisy feedback to train features capturing the correlation between trees and text.", "labels": [], "entities": []}, {"text": "Our model, in contrast, combines these two sources of information in a complementary fashion.", "labels": [], "entities": []}, {"text": "This combination allows our model to filter false positive feedback and produce 13% more correct translations than the Aggressive baseline.", "labels": [], "entities": [{"text": "Aggressive", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.8841198682785034}]}, {"text": "Clean versus Noisy Supervision To assess the impact of noise on model accuracy, we compare the Full Model against the Full Model (Oracle).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9940180778503418}, {"text": "Full Model (Oracle)", "start_pos": 118, "end_pos": 137, "type": "DATASET", "confidence": 0.8803292274475097}]}, {"text": "The two versions achieve very close performance (80% v.s 84% in F-Score), even though Full Model is trained with noisy feedback.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9951240420341492}]}, {"text": "This demonstrates the strength of our model in learning from such weak supervision.", "labels": [], "entities": []}, {"text": "Interestingly, Aggressive (Oracle) outperforms our oracle model by a 5% margin.", "labels": [], "entities": []}, {"text": "This result shows that when the supervision is reliable, the generative assumption limits our model's ability to gain the same performance improvement as discriminative models.", "labels": [], "entities": []}, {"text": "Impact of Input Examples Our model can also be trained in a fully unsupervised or a semisupervised fashion.", "labels": [], "entities": []}, {"text": "In real cases, it may not be possible to obtain input examples for all text specifications.", "labels": [], "entities": []}, {"text": "We evaluate such cases by varying the amount of supervision, i.e. how many text specifications are paired with input examples.", "labels": [], "entities": []}, {"text": "In each run, we randomly select text specifications and only these selected specifications have access to input examples.", "labels": [], "entities": []}, {"text": "gives the performance of our model with 0% supervision (totally unsupervised) to 100% supervision (our full model).", "labels": [], "entities": []}, {"text": "With much less supervision, our model is still able to achieve performance comparable with the Aggressive baseline.", "labels": [], "entities": [{"text": "Aggressive baseline", "start_pos": 95, "end_pos": 114, "type": "DATASET", "confidence": 0.7081065475940704}]}, {"text": "We also evaluate how the number of provided input examples influences the performance of the model.", "labels": [], "entities": []}, {"text": "indicates that the performance is largely insensitive to the number of input examples -once the model is given even one input example, its performance is close to the best performance it obtains with 100 input examples.", "labels": [], "entities": []}, {"text": "We attribute this phenomenon to the fact that if the generated code is incorrect, it is unlikely to successfully parse any input.", "labels": [], "entities": []}, {"text": "Case Study Finally, we consider some text specifications that our model does not correctly trans-late.", "labels": [], "entities": []}, {"text": "In, the program input is interpreted as a list of character strings, while the correct interpretation is that the input is a list of string pairs.", "labels": [], "entities": []}, {"text": "Note that both interpretations produce C++ input parsers that successfully read all of the input examples.", "labels": [], "entities": []}, {"text": "One possible way to resolve this problem is to add other features such as syntactic dependencies between words to capture more language phenomena.", "labels": [], "entities": []}, {"text": "In, the missing key phrase is not identified because our model is notable to ground the meaning of \"pair of coordinates\" to two integers.", "labels": [], "entities": []}, {"text": "Possible future extensions to our model include using lexicon learning methods for mapping words to C++ primitive types for example \"coordinates\" to int, int.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example of feature types and values. To deal with sparsity, we map variable names such as \"N\"  and \"X\" into a category word \"VAR\" in word features.", "labels": [], "entities": [{"text": "VAR", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9877204298973083}]}, {"text": " Table 2: Statistics for 106 ICPC specifications.", "labels": [], "entities": []}, {"text": " Table 3: Average % Recall and % Precision of our  model and all baselines over 20 independent runs.", "labels": [], "entities": [{"text": "Recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9310685396194458}, {"text": "Precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9984875917434692}]}]}