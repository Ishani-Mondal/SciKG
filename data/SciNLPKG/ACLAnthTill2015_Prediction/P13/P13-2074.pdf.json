{"title": [{"text": "Semantic Roles for String to Tree Machine Translation", "labels": [], "entities": [{"text": "String to Tree Machine Translation", "start_pos": 19, "end_pos": 53, "type": "TASK", "confidence": 0.6196021258831024}]}], "abstractContent": [{"text": "We experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of Galley et al.", "labels": [], "entities": [{"text": "string-to-tree machine translation", "start_pos": 57, "end_pos": 91, "type": "TASK", "confidence": 0.7164882222811381}, {"text": "rule extraction", "start_pos": 112, "end_pos": 127, "type": "TASK", "confidence": 0.712043285369873}]}, {"text": "We compare methods based on augmenting the set of nonterminals by adding semantic role labels, and altering the rule extraction process to produce a separate set of rules for each predicate that encompass its entire predicate-argument structure.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 112, "end_pos": 127, "type": "TASK", "confidence": 0.7174410969018936}]}, {"text": "Our results demonstrate that the second approach is effective in increasing the quality of translations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation (SMT) has made considerable advances in using syntactic properties of languages in both the training and the decoding of translation systems.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8276709616184235}]}, {"text": "Over the past few years, many researchers have started to realize that incorporating semantic features of languages can also be effective in increasing the quality of translations, as they can model relationships that often are not derivable from syntactic structures.", "labels": [], "entities": []}, {"text": "demonstrated the promise of using features based on semantic predicateargument structure in machine translation, using these feature to re-rank machine translation output.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7802338600158691}]}, {"text": "In general, re-ranking approaches are limited by the set of translation hypotheses, leading to a desire to incorporate semantic features into the translation model used during MT decoding.", "labels": [], "entities": [{"text": "MT decoding", "start_pos": 176, "end_pos": 187, "type": "TASK", "confidence": 0.933965653181076}]}, {"text": "Liu and Gildea (2010) introduced two types of semantic features for tree-to-string machine translation.", "labels": [], "entities": [{"text": "tree-to-string machine translation", "start_pos": 68, "end_pos": 102, "type": "TASK", "confidence": 0.7125773032506307}]}, {"text": "These features model the reorderings and deletions of the semantic roles in the source sentence during decoding.", "labels": [], "entities": []}, {"text": "They showed that addition of these semantic features helps improve the quality of translations.", "labels": [], "entities": []}, {"text": "Since tree-to-string systems are trained on parse trees, they are constrained by the tree structures and are generally outperformed by string-to-tree systems.", "labels": [], "entities": []}, {"text": "(2012) integrated two discriminative feature-based models into a phrase-based SMT system, which used the semantic predicateargument structure of the source language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.8749315142631531}]}, {"text": "Their first model defined features based on the context of a verbal predicate, to predict the target translation for that verb.", "labels": [], "entities": []}, {"text": "Their second model predicted the reordering direction between a predicate and its arguments from the source to the target sentence.", "labels": [], "entities": []}, {"text": "use a head-driven phrase structure grammar (HPSG) parser to add semantic representations to their translation rules.", "labels": [], "entities": [{"text": "phrase structure grammar (HPSG) parser", "start_pos": 18, "end_pos": 56, "type": "TASK", "confidence": 0.7995364964008331}]}, {"text": "In this paper, we use semantic role labels to enrich a string-to-tree translation system, and show that this approach can increase the BLEU () score of the translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9993415474891663}]}, {"text": "We extract GHKM-style () translation rules from training data where the target side has been parsed and labeled with semantic roles.", "labels": [], "entities": []}, {"text": "Our general method of adding information to the syntactic tree is similar to the \"tree grafting\" approach of, although we focus on predicate-argument structure, rather than named entity tags and modality.", "labels": [], "entities": []}, {"text": "We modify the rule extraction procedure of to produce rules representing the overall predicateargument structure of each verb, allowing us to model alternations in the mapping from syntax to semantics of the type described by.", "labels": [], "entities": []}], "datasetContent": [{"text": "Semantic role labeling was done using the PropBank standard).", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6463726957639059}, {"text": "PropBank standard", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.9773882925510406}]}, {"text": "Our labeler uses a maximum entropy classifier and for identification and classification of semantic roles, and has a percision of 90% and a recall of 88%.", "labels": [], "entities": [{"text": "identification and classification of semantic roles", "start_pos": 54, "end_pos": 105, "type": "TASK", "confidence": 0.6970968296130499}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9988632202148438}]}, {"text": "The features used for training the labeler area subset of the features used by Gildea and Jurafsky (2000),, and.", "labels": [], "entities": []}, {"text": "The string-to-tree training data that we used is a Chinese to English parallel corpus that contains more than 250K sentence pairs, which consist of 6.3M English words.", "labels": [], "entities": []}, {"text": "The corpus was drawn from the newswire texts available from LDC.", "labels": [], "entities": [{"text": "newswire texts available from LDC", "start_pos": 30, "end_pos": 63, "type": "DATASET", "confidence": 0.8389500856399537}]}, {"text": "We used a 392-sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing.", "labels": [], "entities": []}, {"text": "They are drawn from the newswire portion of NIST evaluation).", "labels": [], "entities": [{"text": "NIST evaluation", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.9068215489387512}]}, {"text": "The development set and the test set only had sentences with less than 30 words for decoding speed.", "labels": [], "entities": []}, {"text": "A set of nine standard features, which include globally normalized count of rules, lexical weighting (, length penalty, and number of rules used, was used for the experiments.", "labels": [], "entities": [{"text": "length penalty", "start_pos": 104, "end_pos": 118, "type": "METRIC", "confidence": 0.9295719265937805}]}, {"text": "In all of our experiments, we used the split-merge parsing method of Petrov et al. on the training corpus, and mapped the semantic roles from the original trees to the result of the split-merge parser.", "labels": [], "entities": [{"text": "split-merge parsing", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.6161504983901978}]}, {"text": "We used a syntax-based decoder with Earley parsing and cube pruning.", "labels": [], "entities": []}, {"text": "We used the Minimum Error Rate Training to tune the decoding parameters for the development set and tested the best weights that were found on the test set.", "labels": [], "entities": [{"text": "Minimum Error Rate Training", "start_pos": 12, "end_pos": 39, "type": "METRIC", "confidence": 0.8022213578224182}]}, {"text": "We ran three sets of experiments: Baseline experiments, where we did not do any semantic role labeling prior to rule extraction and only extracted regular GHKM rules, experiments with our method of Section 2.4 (Method 1), and a set of experiments with our method of Section 2.5 (Method 2).", "labels": [], "entities": [{"text": "semantic role labeling prior to rule extraction", "start_pos": 80, "end_pos": 127, "type": "TASK", "confidence": 0.6621779160840171}]}, {"text": "contains the numbers of the GHKM translation rules used by our three method.", "labels": [], "entities": [{"text": "GHKM translation", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7258390784263611}]}, {"text": "The rules were filtered by the development and the test to increase the decoding speed.", "labels": [], "entities": []}, {"text": "The increases in the number of rules were expected, but they were not big enough to significantly change the performance of the decoder.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of the translation rules used  by the three experimented methods", "labels": [], "entities": []}, {"text": " Table 2: Comparison of example translations from the baseline method and our Method 2.", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores on the test and development  sets, of 8 experiments with random initial feature  weights.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989514350891113}]}]}