{"title": [{"text": "Derivational Smoothing for Syntactic Distributional Semantics", "labels": [], "entities": [{"text": "Derivational Smoothing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.768618255853653}, {"text": "Syntactic Distributional Semantics", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.7497812310854594}]}], "abstractContent": [{"text": "Syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces (Baroni and Lenci, 2010).", "labels": [], "entities": []}, {"text": "However, they are also sparse, with resulting reliability and coverage problems.", "labels": [], "entities": [{"text": "reliability", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9873186945915222}, {"text": "coverage", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.971665620803833}]}, {"text": "We address this problem by derivational smoothing, which uses knowledge about derivationally related words (oldish \u2192 old) to improve semantic similarity estimates.", "labels": [], "entities": [{"text": "derivational smoothing", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.8241609930992126}]}, {"text": "We develop a set of derivational smoothing methods and evaluate them on two lexical semantics tasks in German.", "labels": [], "entities": []}, {"text": "Even for models built from very large corpora, simple derivational smoothing can improve coverage considerably.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional semantics) builds on the assumption that the semantic similarity of words is strongly correlated to the overlap between their linguistic contexts.", "labels": [], "entities": []}, {"text": "This hypothesis can be used to construct context vectors for words directly from large text corpora in an unsupervised manner.", "labels": [], "entities": []}, {"text": "Such vector spaces have been applied successfully to many problems in NLP (see or Erk (2012) for current overviews).", "labels": [], "entities": []}, {"text": "Most distributional models in computational lexical semantics are either (a) bag-of-words models, where the context features are words within a surface window around the target word, or (b) syntactic models, where context features are typically pairs of dependency relations and context words.", "labels": [], "entities": [{"text": "computational lexical semantics", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.6482467850049337}]}, {"text": "The advantage of syntactic models is that they incorporate a richer, structured notion of context.", "labels": [], "entities": []}, {"text": "This makes them more versatile; the Distributional Memory framework by is applicable to a wide range of tasks.", "labels": [], "entities": [{"text": "Distributional Memory", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.8925740122795105}]}, {"text": "It is also ableat least in principle -to capture more fine-grained types of semantic similarity such as predicateargument plausibility.", "labels": [], "entities": []}, {"text": "At the same time, syntactic spaces are much more prone to sparsity problems, as their contexts are sparser.", "labels": [], "entities": []}, {"text": "This leads to reliability and coverage problems.", "labels": [], "entities": [{"text": "reliability", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9841701984405518}, {"text": "coverage", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9296973347663879}]}, {"text": "In this paper, we propose a novel strategy for combating sparsity in syntactic vector spaces, derivational smoothing.", "labels": [], "entities": []}, {"text": "It follows the intuition that derivationally related words (feed -feeder, blocked -blockage) are, as a rule, semantically highly similar.", "labels": [], "entities": []}, {"text": "Consequently, knowledge about derivationally related words can be used as a \"back off\" for sparse vectors in syntactic spaces.", "labels": [], "entities": []}, {"text": "For example, the pair oldish -ancient should receive a high semantic similarity, but in practice, the vector for oldish will be very sparse, which makes this result uncertain.", "labels": [], "entities": []}, {"text": "Knowing that oldish is derivationally related to old allows us to use the much less sparse vector for old as a proxy for oldish.", "labels": [], "entities": []}, {"text": "We present a set of general methods for smoothing vector similarity computations given a resource that groups words into derivational families (equivalence classes) and evaluate these methods on German for two distributional tasks (similarity prediction and synonym choice).", "labels": [], "entities": [{"text": "smoothing vector similarity computations", "start_pos": 40, "end_pos": 80, "type": "TASK", "confidence": 0.7470392882823944}, {"text": "similarity prediction", "start_pos": 232, "end_pos": 253, "type": "TASK", "confidence": 0.7116173952817917}]}, {"text": "We find that even for syntactic models built from very large corpora, a simple derivational resource that groups words on morphological grounds can improve the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The syntactic distributional model that we use represents target words by pairs of dependency relations and context words.", "labels": [], "entities": []}, {"text": "More specifically, we use the W \u00d7 LW matricization of DM.DE, the German version (Pad\u00f3 and Utt, 2012) of Distributional Memory (.", "labels": [], "entities": []}, {"text": "DM.DE was created on the basis of the 884M-token SDEWAC web corpus, lemmatized, tagged, and parsed with the German MATE toolkit.", "labels": [], "entities": [{"text": "DM.DE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9127894639968872}, {"text": "884M-token SDEWAC web corpus", "start_pos": 38, "end_pos": 66, "type": "DATASET", "confidence": 0.8534693717956543}, {"text": "German MATE toolkit", "start_pos": 108, "end_pos": 127, "type": "DATASET", "confidence": 0.6539515256881714}]}, {"text": "We evaluate the impact of smoothing on two standard tasks from lexical semantics.", "labels": [], "entities": []}, {"text": "The first task is predicting semantic similarity.", "labels": [], "entities": [{"text": "predicting semantic similarity", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.9192317724227905}]}, {"text": "We lemmatized and POS-tagged the German GUR350 dataset ( , a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.", "labels": [], "entities": [{"text": "German GUR350 dataset", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.7923407753308614}, {"text": "Goodenough (1965) dataset", "start_pos": 169, "end_pos": 194, "type": "DATASET", "confidence": 0.6583487927913666}]}, {"text": "We predict semantic similarity as cosine similarity.", "labels": [], "entities": []}, {"text": "We make a prediction fora word pair if both words are represented in the semantic space and their vectors have a non-zero similarity.", "labels": [], "entities": []}, {"text": "The second task is synonym choice on the German version of the Reader's Digest WordPower dataset ().", "labels": [], "entities": [{"text": "synonym choice", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.9707062840461731}, {"text": "German version of the Reader's Digest WordPower dataset", "start_pos": 41, "end_pos": 96, "type": "DATASET", "confidence": 0.8555957741207547}]}, {"text": "This dataset, which we also lemmatized and POS-tagged, consists of 984 target words with four synonym candidates each (including phrases), one of which is correct.", "labels": [], "entities": []}, {"text": "Again, we compute semantic similarity as the cosine between target and a candidate vector and pick the highest-similarity candidate as synonym.", "labels": [], "entities": []}, {"text": "For phrases, we compute the maximum pairwise word similarity.", "labels": [], "entities": []}, {"text": "We make a prediction for an item if the target as well as at least one candidate are represented in the semantic space and their vectors have a non-zero similarity.", "labels": [], "entities": []}, {"text": "We expect differences between the two tasks with regard to derivational smoothing, since the words within derivational families are generally related but often not synonymous (cf. the example in Section 3).", "labels": [], "entities": [{"text": "derivational smoothing", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.8730657994747162}]}, {"text": "Thus, semantic similarity judgments should profit more easily from derivational smoothing than synonym choice.", "labels": [], "entities": [{"text": "semantic similarity judgments", "start_pos": 6, "end_pos": 35, "type": "TASK", "confidence": 0.8120929002761841}, {"text": "synonym choice", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.8422386944293976}]}, {"text": "Our baseline is a standard bag-ofwords vector space (BOW), which represents target words by the words occurring in their context.", "labels": [], "entities": []}, {"text": "We use standard parameters (\u00b110 word window, 8.000 most frequent verb, noun, and adjective lemmas).", "labels": [], "entities": []}, {"text": "The model was created from the same corpus as DM.DE.", "labels": [], "entities": [{"text": "DM.DE", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.9588984847068787}]}, {"text": "We also applied derivational smoothing to this model, but did not obtain improvements.", "labels": [], "entities": []}, {"text": "To analyze the impact of smoothing, we evaluate the coverage of models and the quality of their predictions separately.", "labels": [], "entities": []}, {"text": "In both tasks, coverage is the percentage of items for which we make a prediction.", "labels": [], "entities": [{"text": "coverage", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9961144924163818}]}, {"text": "We measure quality of the semantic similarity task as the Pearson correlation between the model predictions and the human judgments for covered items ( ).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 58, "end_pos": 77, "type": "METRIC", "confidence": 0.9804538190364838}]}, {"text": "For synonym choice, we follow the method established by, measuring accuracy over covered items, with partial credit for ties.", "labels": [], "entities": [{"text": "synonym choice", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9505569040775299}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9983596205711365}]}, {"text": "increases correlation somewhat tor = 0.47.", "labels": [], "entities": [{"text": "correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.99920254945755}]}, {"text": "The difference to the unsmoothed model is not significant at p = 0.05 according to method of comparing correlation coefficients.", "labels": [], "entities": []}, {"text": "The bag-of-words baseline (BOW) has a greater coverage than DM.DE models, but at the cost of lower correlation across the board.", "labels": [], "entities": [{"text": "BOW", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.6595804691314697}]}, {"text": "The only DM.DE model that performs worse than the BOW baseline is the non-conservative avgSim (average similarity) scheme.", "labels": [], "entities": [{"text": "BOW baseline", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.7621693909168243}, {"text": "avgSim (average similarity)", "start_pos": 87, "end_pos": 114, "type": "METRIC", "confidence": 0.8409224033355713}]}, {"text": "We attribute this weak performance to the presence of many pairwise zero similarities in the data, which makes the avgSim predictions unreliable.", "labels": [], "entities": []}, {"text": "To our knowledge, there are no previous published papers on distributional approaches to modeling this dataset.", "labels": [], "entities": []}, {"text": "The best previous result is a GermaNet/Wikipedia-based model by . It reports a higher correlation (r = 0.59) but a very low coverage at 33.1%.", "labels": [], "entities": [{"text": "correlation", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.9774216413497925}, {"text": "coverage", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9969947338104248}]}, {"text": "The results for the second task are shown in.", "labels": [], "entities": []}, {"text": "The unsmoothed model achieves an accuracy of 53.7% and a coverage of 80.8%, as reported by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9997100234031677}, {"text": "coverage", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.99871826171875}]}, {"text": "Smoothing increases the coverage by almost 6% to 86.6% (for example, a question item for inferior becomes covered after backing off from the target to Inferiorit\u00e4t (inferiority)).", "labels": [], "entities": [{"text": "coverage", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9922673106193542}]}, {"text": "All smoothed models show a loss inaccuracy, albeit small.", "labels": [], "entities": []}, {"text": "The best model is again a conservative smoothing model (sim = 0) with a loss of 1.1% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9986218214035034}]}, {"text": "Using bootstrap resampling, we established that the difference to the unsmoothed DM.DE model is not significant at p < 0.05.", "labels": [], "entities": []}, {"text": "This time, the avgSim (average similarity) smoothing scheme performs best, with the prototype-based scheme in second place.", "labels": [], "entities": [{"text": "avgSim (average similarity)", "start_pos": 15, "end_pos": 42, "type": "METRIC", "confidence": 0.828621768951416}]}, {"text": "Thus, the results for synonym choice are less clear-cut: derivational smoothing can trade accuracy against  coverage but does not lead to a clear improvement.", "labels": [], "entities": [{"text": "synonym choice", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.9472379684448242}, {"text": "derivational smoothing", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.712198406457901}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9981378316879272}, {"text": "coverage", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9608698487281799}]}, {"text": "What is more, the BOW \"baseline\" significantly outperforms all syntactic models, smoothed and unsmoothed, with an almost perfect coverage combined with a higher accuracy.", "labels": [], "entities": [{"text": "BOW", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.5572630167007446}, {"text": "coverage", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9670953154563904}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9983062744140625}]}], "tableCaptions": [{"text": " Table 1: Results on the semantic similarity task  (r: Pearson correlation, Cov: Coverage)", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 55, "end_pos": 74, "type": "METRIC", "confidence": 0.8780815601348877}]}, {"text": " Table 2: Results on the synonym choice task  (Acc: Accuracy, Cov: Coverage)", "labels": [], "entities": [{"text": "synonym choice task", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.929344117641449}, {"text": "Acc: Accuracy", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.8243424892425537}]}]}