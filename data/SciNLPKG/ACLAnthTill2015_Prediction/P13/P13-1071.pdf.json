{"title": [{"text": "The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia", "labels": [], "entities": [{"text": "Topic Bias", "start_pos": 14, "end_pos": 24, "type": "TASK", "confidence": 0.7111462205648422}, {"text": "Quality Flaw Prediction", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.5540199975172678}]}], "abstractContent": [{"text": "With the increasing amount of user generated reference texts in the web, automatic quality assessment has become a key challenge.", "labels": [], "entities": [{"text": "automatic quality assessment", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.5417514542738596}]}, {"text": "However, only a small amount of annotated data is available for training quality assessment systems.", "labels": [], "entities": []}, {"text": "Wikipedia contains a large amount of texts annotated with cleanup templates which identify quality flaws.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8813343048095703}]}, {"text": "We show that the distribution of these labels is topically biased , since they cannot be applied freely to any arbitrary article.", "labels": [], "entities": []}, {"text": "We argue that it is necessary to consider the topical restrictions of each label in order to avoid a sampling bias that results in a skewed classifier and overly optimistic evaluation results.", "labels": [], "entities": []}, {"text": "We factor out the topic bias by extracting reliable training instances from the revision history which have a topic distribution similar to the labeled articles.", "labels": [], "entities": []}, {"text": "This approach better reflects the situation a classi-fier would face in a real-life application.", "labels": [], "entities": []}], "introductionContent": [{"text": "User generated content is the main driving force of the increasingly social web.", "labels": [], "entities": []}, {"text": "Blogs, wikis and forums makeup a large amount of the daily information consumed by web users.", "labels": [], "entities": []}, {"text": "The main properties of user generated content area low publication threshold and little or no editorial control, which leads to a high variance in quality.", "labels": [], "entities": []}, {"text": "In order to navigate through large repositories of information efficiently and safely, users need away to quickly assess the quality of the content.", "labels": [], "entities": []}, {"text": "Automatic quality assessment has therefore become a key application in today's information society.", "labels": [], "entities": [{"text": "Automatic quality assessment", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6816411713759104}]}, {"text": "However, there is alack of training data annotated with fine-grained quality information.", "labels": [], "entities": []}, {"text": "Wikipedia, the largest encyclopedia on the web, contains so-called cleanup templates, which constitute a sophisticated system of user generated labels that mark quality problems in articles.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9323346018791199}]}, {"text": "Recently, these cleanup templates have been used for automatically identifying articles with particular quality flaws in order to support Wikipedia's quality assurance process in Wikipedia.", "labels": [], "entities": []}, {"text": "Ina shared task, several systems have shown that it is possible to identify the ten most frequent quality flaws with high recall and fair precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9993215799331665}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9896065592765808}]}, {"text": "However, quality flaw detection based on cleanup template recognition suffers from a topic bias that is well known from other text classification applications such as authorship attribution or genre identification.", "labels": [], "entities": [{"text": "quality flaw detection", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.685529907544454}, {"text": "cleanup template recognition", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.814889132976532}, {"text": "text classification", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7095413208007812}, {"text": "authorship attribution or genre identification", "start_pos": 167, "end_pos": 213, "type": "TASK", "confidence": 0.6715494930744171}]}, {"text": "We discovered that cleanup templates have implicit topical restrictions, i.e. they cannot be applied to any arbitrary article.", "labels": [], "entities": []}, {"text": "As a consequence, corpora of flawed articles based on these templates are biased towards particular topics.", "labels": [], "entities": []}, {"text": "We argue that it is therefore not sufficient for evaluating a quality flaw prediction systems to measure how well they can separate (topically restricted) flawed articles from a set of random outliers.", "labels": [], "entities": []}, {"text": "It is rather necessary to determine reliable negative instances with a similar topic distribution as the set of positive instances in order to factor out the sampling bias.", "labels": [], "entities": []}, {"text": "Related studies () have proven that topic bias is a confounding factor that results in misleading crossvalidated performance while allowing only near chance performance in practical applications.", "labels": [], "entities": []}, {"text": "We present an approach for factoring out the bias from quality flaw corpora by mining reliable negative instances for each flaw from the article revision history.", "labels": [], "entities": []}, {"text": "Furthermore, we employ the article revision history to extract reliable positive training instances by using the version of each article at the time it has first been identified as flawed.", "labels": [], "entities": []}, {"text": "This way, we avoid including articles with outdated cleanup templates, a frequent phe-nomenon that can occur when a template is not removed after fixing a problem in an article.", "labels": [], "entities": []}, {"text": "In our experiments, we focus on neutrality and style flaws, since they are of particular high importance within the Wikipedia community () and are recognized beyond Wikipedia in applications such as uncertainty recognition () and hedge detection).", "labels": [], "entities": [{"text": "uncertainty recognition", "start_pos": 199, "end_pos": 222, "type": "TASK", "confidence": 0.7183956205844879}, {"text": "hedge detection", "start_pos": 230, "end_pos": 245, "type": "TASK", "confidence": 0.8535211980342865}]}], "datasetContent": [{"text": "In the following, we describe our system architecture and the setup of our experiments.", "labels": [], "entities": []}, {"text": "Our system for quality flaw detection follows the approach by, since it has been particularly designed as a modular system based on the Unstructured Information Management Architecture , which makes it easy to extend.", "labels": [], "entities": [{"text": "quality flaw detection", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.6395915051301321}]}, {"text": "Instead of using Mallet) as a machine learning toolkit, we employ the Weka Data Mining Software () for classification, since it offers a wider range of state-of-the-art machine learning algorithms.", "labels": [], "entities": [{"text": "Weka Data Mining Software", "start_pos": 70, "end_pos": 95, "type": "DATASET", "confidence": 0.9694776833057404}]}, {"text": "For each of the 12 quality flaws, we employ three different dataset configurations.", "labels": [], "entities": []}, {"text": "The BASE configuration uses the newest version of each flawed article as positive instances and a random set of untagged articles as negative instances.", "labels": [], "entities": [{"text": "BASE", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.80399489402771}]}, {"text": "The RELP configuration uses reliable positives, as described in Section 5.1, in combination with random outliers.", "labels": [], "entities": []}, {"text": "Finally, the RELALL configuration employs reliable positives in combination with the respective reliable negatives as described in Section 5.2.", "labels": [], "entities": [{"text": "RELALL", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.7974767088890076}]}, {"text": "The SVMs achieve a similar cross-validated performance on all feature sets containing ngrams, showing only minor improvements for individual flaws when adding non-lexical features.", "labels": [], "entities": []}, {"text": "This suggests that the classifiers largely depend on the ngrams and that other features do not contribute significantly to the classification performance.", "labels": [], "entities": []}, {"text": "While structural quality flaws can be well captured by special purpose features or intensional modeling, as related work has shown, more subtle content flaws such as the neutrality and style flaws are mainly captured by the wording itself.", "labels": [], "entities": []}, {"text": "Textual features beyond the ngram level, such as syntactic and semantic qualities of the text, could further improve the classification performance of these flaws and should be addressed in future work.", "labels": [], "entities": []}, {"text": "shows the performance of the SVMs with RBF kernel 12 on each dataset using the NGRAM feature set.", "labels": [], "entities": [{"text": "NGRAM feature set", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.9350535670916239}]}, {"text": "The average performance based on NOWIKI is slightly lower while using ALL features results in slightly higher average F 1 -scores.", "labels": [], "entities": [{"text": "NOWIKI", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.8361453413963318}, {"text": "F 1 -scores", "start_pos": 118, "end_pos": 129, "type": "METRIC", "confidence": 0.9819021075963974}]}, {"text": "However, the differences are not statistically significant and thus omitted.", "labels": [], "entities": []}, {"text": "Classifiers using the NONGRAM feature set achieved average F 1 -scores below 0.50 on all datasets.", "labels": [], "entities": [{"text": "NONGRAM feature set", "start_pos": 22, "end_pos": 41, "type": "DATASET", "confidence": 0.904993454615275}, {"text": "F 1 -scores", "start_pos": 59, "end_pos": 70, "type": "METRIC", "confidence": 0.9847450405359268}]}, {"text": "The results have been obtained by 10-fold cross validation on 2,000 documents per flaw.", "labels": [], "entities": []}, {"text": "The classifiers trained on reliable positives and random untagged articles (RELP) outperform the respective classifiers based on the BASE dataset for most flaws.", "labels": [], "entities": [{"text": "RELP", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.6940398812294006}, {"text": "BASE dataset", "start_pos": 133, "end_pos": 145, "type": "DATASET", "confidence": 0.8638816475868225}]}, {"text": "This confirms our original hypothesis that using the appropriate revision of each tagged article is superior to using the latest available version from the dump.", "labels": [], "entities": []}, {"text": "The performance on the RELALL dataset, in which the topic bias has been factored out, yields lower F 1 -scores than the two other approaches.", "labels": [], "entities": [{"text": "RELALL dataset", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.780329704284668}, {"text": "F 1 -scores", "start_pos": 99, "end_pos": 110, "type": "METRIC", "confidence": 0.9896440356969833}]}, {"text": "Flaws that are restricted to a very narrow set of topics (i.e. A topic in is small), such as the in-universe flaw, show the biggest drop in performance.", "labels": [], "entities": []}, {"text": "Since the topic bias plays a major role in the quality flaw detection task, as we have shown earlier, the topiccontrolled classifier cannot take advantage of the topic information, while the classifiers trained on the other corpora can make use of these characteristic as the most discriminative features.", "labels": [], "entities": [{"text": "quality flaw detection task", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.7429295629262924}]}, {"text": "In the RELALL setting, however, the differences between the positive and negative instances are largely determined by the flaws alone.", "labels": [], "entities": []}, {"text": "Classifiers trained on such a dataset therefore come closer to recognizing the actual quality flaws, which makes them more useful in a practical setting despite lower cross-validated scores.", "labels": [], "entities": []}, {"text": "In addition to cross-validation, we performed a cross-corpus evaluation of the classifiers for each flaw.", "labels": [], "entities": []}, {"text": "Therefore, we evaluated the performance of the unbiased classifiers (trained on RELALL) on the biased data (RELP) and vice versa.", "labels": [], "entities": [{"text": "RELALL", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.925432026386261}, {"text": "RELP", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9558864235877991}]}, {"text": "Hereby, the positive training and test instances remain the same in both settings, while the unbiased data contains negative instances sampled from A rel and the unbiased data from A rnd (see).", "labels": [], "entities": []}, {"text": "With the NGRAM feature set, the reliable classifiers outperformed the unreliable classifiers on all flaws that can be well identified with lexical cues, such as Advert or Technical.", "labels": [], "entities": [{"text": "NGRAM feature set", "start_pos": 9, "end_pos": 26, "type": "DATASET", "confidence": 0.905163029829661}]}, {"text": "In the biased case, we found both topic related and flaw specific ngrams among the most highly ranked ngram features.", "labels": [], "entities": []}, {"text": "In the unbiased case, most of the informative ngrams were flaw specific expressions.", "labels": [], "entities": []}, {"text": "Consequently, biased classifiers fail on the unbiased dataset in which the positive and negative class are sampled from the same topics, which renders the highly ranked topic ngrams unusable.", "labels": [], "entities": []}, {"text": "Flaws that do not largely rely on lexical cues, however, cannot be predicted more reliably with the unbiased classifier.", "labels": [], "entities": []}, {"text": "This means that additional features are needed to describe these flaw.", "labels": [], "entities": []}, {"text": "We tested this hypothesis by using the full feature set ALL and saw a substantial improvement on the side of the unbiased classifier, while the performance of the biased classifier remained unchanged.", "labels": [], "entities": []}, {"text": "A direct comparison of our results to related work is difficult, since neutrality and style flaws have not been targeted before in a similar manner.", "labels": [], "entities": []}, {"text": "However, the Advert flaw was also part of the ten flaw types in the PAN Quality Flaw Recognition Task ().", "labels": [], "entities": [{"text": "PAN Quality Flaw Recognition Task", "start_pos": 68, "end_pos": 101, "type": "TASK", "confidence": 0.6744117856025695}]}, {"text": "The best system achieved an F 1 score of 0.839, which is just below the results of our system on the BASE dataset, which is similar to the PAN setup.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9925161600112915}, {"text": "BASE dataset", "start_pos": 101, "end_pos": 113, "type": "DATASET", "confidence": 0.8236202895641327}]}], "tableCaptions": [{"text": " Table 1: Neutrality and style flaw corpora used in this work", "labels": [], "entities": []}, {"text": " Table 2: Agreement of human annotator with gold  standard", "labels": [], "entities": [{"text": "Agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9279274344444275}]}, {"text": " Table 3: Cosine similarity scores between the cat- egory frequency vectors of the flawed article sets  and the respective random or reliable negatives", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.8732021450996399}]}, {"text": " Table 5: Average F 1 -scores over all flaws on RELP  using all features", "labels": [], "entities": [{"text": "Average F 1 -scores", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8974837899208069}, {"text": "RELP", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.6427100896835327}]}, {"text": " Table 6: F 1 scores for the 10-fold cross validation  of the SVMs with RBF kernel on all datasets using  NGRAM features", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9852929413318634}]}]}