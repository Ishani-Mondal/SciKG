{"title": [], "abstractContent": [{"text": "We present a dialectal Egyptian Arabic to English statistical machine translation system that leverages dialectal to Modern Standard Arabic (MSA) adaptation.", "labels": [], "entities": [{"text": "English statistical machine translation", "start_pos": 42, "end_pos": 81, "type": "TASK", "confidence": 0.5211465731263161}, {"text": "Modern Standard Arabic (MSA) adaptation", "start_pos": 117, "end_pos": 156, "type": "DATASET", "confidence": 0.8302400537899562}]}, {"text": "In contrast to previous work, we first narrow down the gap between Egyptian and MSA by applying an automatic character-level transformational model that changes Egyptian to EG , which looks similar to MSA.", "labels": [], "entities": []}, {"text": "The transformations include morphological, phonological and spelling changes.", "labels": [], "entities": []}, {"text": "The transformation reduces the out-of-vocabulary (OOV) words from 5.2% to 2.6% and gives again of 1.87 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9992352724075317}]}, {"text": "Further, adapting large MSA/English parallel data increases the lexical coverage, reduces OOVs to 0.7% and leads to an absolute BLEU improvement of 2.73 points.", "labels": [], "entities": [{"text": "OOVs", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9968283772468567}, {"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9442322850227356}]}], "introductionContent": [{"text": "Modern Standard Arabic (MSA) is the lingua franca for the Arab world.", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.8539886971314748}]}, {"text": "Arabic speakers generally use dialects in daily interactions.", "labels": [], "entities": []}, {"text": "There are 6 dominant dialects, namely Egyptian, Moroccan, Levantine, Iraqi, Gulf, and Yemeni 1 . The dialects may differ in vocabulary, morphology, syntax, and spelling from MSA and most lack spelling conventions.", "labels": [], "entities": []}, {"text": "Different dialects often make different lexical choices to express concepts.", "labels": [], "entities": []}, {"text": "For example, the concept corresponding to \"Oryd\" (\"I want\") is expressed as \"EAwz\" in Egyptian, \"Abgy\" in Gulf, \"Aby\" in Iraqi, and \"bdy\" in Levantine 2 . Often, words have different or opposite meanings in different dialects.", "labels": [], "entities": [{"text": "EAwz", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9629115462303162}]}, {"text": "1 http://en.wikipedia.org/wiki/ Varieties_of_Arabic 2 All transliterations follow the Buckwalter scheme Arabic dialects may differ morphologically from MSA.", "labels": [], "entities": [{"text": "Buckwalter", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.9523648619651794}, {"text": "MSA", "start_pos": 152, "end_pos": 155, "type": "DATASET", "confidence": 0.9163150787353516}]}, {"text": "For example, Egyptian Arabic uses a negation construct similar to the French \"ne pas\" negation construct.", "labels": [], "entities": []}, {"text": "The Egyptian word \"mlEbt$\" (or alternatively spelled ) (\"I did not play\") is composed of \"m+lEbt+$\".", "labels": [], "entities": []}, {"text": "The pronunciations of letters often differ from one dialect to another.", "labels": [], "entities": []}, {"text": "For example, the letter \"q\" is typically pronounced in MSA as an unvoiced uvular stop (as the \"q\" in \"quote\"), but as a glottal stop in Egyptian and Levantine (like \"A\" in \"Alpine\") and a voiced velar stop in the Gulf (like \"g\" in \"gavel\").", "labels": [], "entities": [{"text": "MSA", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.9328824877738953}]}, {"text": "Differing pronunciations often reflect on spelling.", "labels": [], "entities": [{"text": "Differing pronunciations", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9389623701572418}]}, {"text": "Social media platforms allowed people to express themselves more freely in writing.", "labels": [], "entities": []}, {"text": "Although MSA is used informal writing, dialects are increasingly being used on social media sites.", "labels": [], "entities": []}, {"text": "Some notable trends on social platforms include: -Mixed language texts where bilingual (or multilingual) users code switch between Arabic and English (or Arabic and French).", "labels": [], "entities": []}, {"text": "In the example \"wSlny mrsy\" (\"got it thank you\"), \"thank you\" is the transliterated French word \"merci\".", "labels": [], "entities": []}, {"text": "-The use of phonetic transcription to match dialectal pronunciation.", "labels": [], "entities": []}, {"text": "For example, \"Sdq\" (\"truth\") is often written as \"Sj\" in Gulf dialect.", "labels": [], "entities": []}, {"text": "-Creative spellings, spelling mistakes, and word elongations are ubiquitous in social texts.", "labels": [], "entities": []}, {"text": "-The use of new words like \"lol\" (\"LOL\").", "labels": [], "entities": [{"text": "LOL", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9437595009803772}]}, {"text": "-The attachment of new meanings to words such as using \"THn\" to mean \"very\" while it means \"grinding\" in MSA.", "labels": [], "entities": [{"text": "THn", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9384390115737915}, {"text": "MSA", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.8922645449638367}]}, {"text": "The Egyptian dialect has the largest number of speakers and is the most commonly understood dialect in the Arab world.", "labels": [], "entities": []}, {"text": "In this work, we focused on translating dialectal Egyptian to English us-1 ing Egyptian to MSA adaptation.", "labels": [], "entities": [{"text": "MSA adaptation", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.5923324823379517}]}, {"text": "Unlike previous work, we first narrowed the gap between Egyptian and MSA using character-level transformations and word n-gram models that handle spelling mistakes, phonological variations, and morphological transformations.", "labels": [], "entities": []}, {"text": "Later, we applied an adaptation method to incorporate MSA/English parallel data.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows: -We trained an Egyptian/MSA transformation model to make Egyptian look similar to MSA.", "labels": [], "entities": []}, {"text": "We publicly released the training data.", "labels": [], "entities": []}, {"text": "-We built a phrasal Machine Translation (MT) system on adapted Egyptian/English parallel data, which outperformed a non-adapted baseline by 1.87 BLEU points.", "labels": [], "entities": [{"text": "phrasal Machine Translation (MT)", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.8059733013312022}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.997696578502655}]}, {"text": "-We used phrase-table merging to utilize MSA/English parallel data with the available in-domain parallel data.", "labels": [], "entities": [{"text": "phrase-table merging", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.7930533587932587}]}], "datasetContent": [{"text": "We performed the following experiments: -S0 involved translating the EG test using AR.", "labels": [], "entities": [{"text": "EG test", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.6965987086296082}, {"text": "AR", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9748294353485107}]}, {"text": "-S1 and S2 trained on the EG with EG en and both EG en and GW for LM training respectively.", "labels": [], "entities": [{"text": "EG", "start_pos": 26, "end_pos": 28, "type": "DATASET", "confidence": 0.9260193109512329}]}, {"text": "-S * used phrase merging technique.", "labels": [], "entities": [{"text": "phrase merging", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.775469571352005}]}, {"text": "All systems trained on both EG and AR corpora.", "labels": [], "entities": []}, {"text": "We built separate phrase tables from the two corpora and merged them.", "labels": [], "entities": []}, {"text": "When merging, we preferred AR or EG for S AR and S EG respectively.", "labels": [], "entities": [{"text": "AR", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9837256073951721}]}, {"text": "For S ALL , we kept phrases from both phrase tables.", "labels": [], "entities": [{"text": "S ALL", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.5483825504779816}]}, {"text": "summarizes results of using EG and phrase table merging.", "labels": [], "entities": [{"text": "phrase table merging", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.6059919794400533}]}, {"text": "S0 was slightly better than B1, but lagged considerably behind training using EG or EG . S1, which used only EG for training showed an improvement of 1.67 BLEU points from the best baseline system (B4).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9958547949790955}]}, {"text": "Using both language models (S2) led to slight improvement.", "labels": [], "entities": []}, {"text": "Phrase merging that preferred phrases learnt from EG data over AR data performed the best with a BLEU score of 16.96.", "labels": [], "entities": [{"text": "Phrase merging", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8600281178951263}, {"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9996929168701172}]}, {"text": "We analyzed 100 test sentences that led to the greatest absolute change in BLEU score, whether positive or negative, between training with EG and EG . The largest difference in BLEU was 0.69 in favor of EG . Translating the Egyptian sentence \"wbyHtrmwA AlnAs AltAnyp\" produced \" (OOV) the second people\" (BLEU = 0.31).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9820250868797302}, {"text": "BLEU", "start_pos": 177, "end_pos": 181, "type": "METRIC", "confidence": 0.9991641044616699}, {"text": "BLEU", "start_pos": 305, "end_pos": 309, "type": "METRIC", "confidence": 0.9986856579780579}]}, {"text": "Conversion changed \"wbyHtrmwA\" to \"wyHtrmwA\" and \"AltAnyp\" to \"AlvAnyp\" , leading to \"and they respect other people\" (BLEU = 1 In further analysis, we examined 1% of the sentences with the largest difference in BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9985435009002686}, {"text": "BLEU score", "start_pos": 211, "end_pos": 221, "type": "METRIC", "confidence": 0.9828687906265259}]}, {"text": "Out of these, more than 70% were cases where the EG model achieved a higher BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9868350028991699}]}, {"text": "For each observed conversion error, we identified its linguistic character, i.e. whether it is lexical, syntactic, morphological or other.", "labels": [], "entities": []}, {"text": "We found that in more than half of the cases (\u224857%) using morphological information could have improved the conversion.", "labels": [], "entities": []}, {"text": "Consider the following example, where (1) is the original EG sentence and its EG/EN translation, and  In this case, \"rgbtk\" (\"your wish\") was converted to \"rgbth\" (\"his wish\") leading to an unwanted change in the translation.", "labels": [], "entities": []}, {"text": "This could be avoided, for instance, by running a morphological analyzer on the original and converted word, and making sure their morphological features (in this case, the person of the possessive) correspond.", "labels": [], "entities": []}, {"text": "Ina similar case, the phrase \"mEndy$ AEdA\" was converted to \"Endy OEdA'\" , thereby changing the translation from \"I don't have enemies\" to \"I have enemies\".", "labels": [], "entities": [{"text": "AEdA", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.4427163898944855}, {"text": "Endy OEdA", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.7958070337772369}]}, {"text": "Here, again, a morphological analyzer could verify the retaining of negation after conversion.", "labels": [], "entities": []}, {"text": "In another sentence, \"knty\" (\"you (fm.) were\") was correctly converted to the MSA \"knt\" , which is used for feminine and masculine forms.", "labels": [], "entities": []}, {"text": "However, the induced ambiguity ended up hurting translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.9836481809616089}]}, {"text": "Aside from morphological mistakes, conversion often changed words completely.", "labels": [], "entities": [{"text": "conversion", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.9743495583534241}]}, {"text": "In one sentence, the word \"lbAnh\" (\"chewing gum\") was wrongly converted to \"lOnh\" (\"because it\"), resulting in a wrong translation.", "labels": [], "entities": []}, {"text": "Perhaps a morphological analyzer, or just a part-of-speech tagger, could enforce (or probabilistically encourage) a match in parts of speech.", "labels": [], "entities": []}, {"text": "The conversion also faces some other challenges.", "labels": [], "entities": [{"text": "conversion", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9807358980178833}]}, {"text": "Consider the following example: 1.", "labels": [], "entities": []}, {"text": "hwA AHnA EmlnA Ayyyh he is we did we What ? ?", "labels": [], "entities": [{"text": "AHnA EmlnA Ayyyh", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.7398587862650553}]}], "tableCaptions": [{"text": " Table 1: Baseline results using the EG and AR  training sets with GW and EG en corpora for LM  training", "labels": [], "entities": []}, {"text": " Table 2: Summary of results using different com- binations of EG /English and MSA/English train- ing data", "labels": [], "entities": [{"text": "EG /English and MSA/English train- ing data", "start_pos": 63, "end_pos": 106, "type": "DATASET", "confidence": 0.7806386920538816}]}]}