{"title": [{"text": "Discovering User Interactions in Ideological Discussions", "labels": [], "entities": [{"text": "Discovering User Interactions in Ideological Discussions", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.7499250570933024}]}], "abstractContent": [{"text": "Online discussion forums area popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest.", "labels": [], "entities": []}, {"text": "In forums where users discuss social, political, or religious issues, there are often heated debates among users or participants.", "labels": [], "entities": []}, {"text": "Existing research has studied mining of user stances or camps on certain issues, opposing perspectives, and contention points.", "labels": [], "entities": []}, {"text": "In this paper, we focus on identifying the nature of interactions among user pairs.", "labels": [], "entities": []}, {"text": "The central questions are: How does each pair of users interact with each other?", "labels": [], "entities": []}, {"text": "Does the pair of users mostly agree or disagree?", "labels": [], "entities": []}, {"text": "What is the lexicon that people often use to express agreement and disagreement?", "labels": [], "entities": []}, {"text": "We present a topic model based approach to answer these questions.", "labels": [], "entities": []}, {"text": "Since agreement and disagreement expressions are usually multi-word phrases, we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling.", "labels": [], "entities": []}, {"text": "After modeling, we use the modeling results to classify the nature of interaction of each user pair.", "labels": [], "entities": []}, {"text": "Our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "Online discussion/debate forums allow people with common interests to freely ask and answer questions, to express their views and opinions on any subject matter, and to discuss issues of common interest.", "labels": [], "entities": []}, {"text": "A large part of such discussions is about social, political, and religious issues.", "labels": [], "entities": []}, {"text": "On such issues, there are often heated discussions/debates, i.e., people agree or disagree and argue with one another.", "labels": [], "entities": []}, {"text": "Such ideological discussions on a myriad of social and political issues have practical implications in the fields of communication and political science as they give social scientists an opportunity to study real-life discussions/debates of almost any issue and analyze participant behaviors in a large scale.", "labels": [], "entities": []}, {"text": "In this paper, we present such an application, which aims to perform fine-grained analysis of user-interactions in online discussions.", "labels": [], "entities": []}, {"text": "There have been some related works that focus on discovering the general topics and ideological perspectives in online discussions, placing users in support/oppose camps (), and classifying user stances.", "labels": [], "entities": []}, {"text": "However, these works are at a rather coarser level and have not considered more fine-grained characteristics of debates/discussions where users interact with each other by quoting/replying each other to express agreement or disagreement and argue with one another.", "labels": [], "entities": []}, {"text": "In this work, we want to mine the following information:", "labels": [], "entities": []}], "datasetContent": [{"text": "We now evaluate the proposed techniques in the context of the JTE-P model.", "labels": [], "entities": [{"text": "JTE-P model", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.8573375940322876}]}, {"text": "We first evaluate the discovered AD-expressions by comparing results with and without using the phrase ranking method in Section 4, and then evaluate the classification of interaction nature of pairs.", "labels": [], "entities": []}, {"text": "We crawled debate/discussion forum posts from Volconvo.com.", "labels": [], "entities": [{"text": "Volconvo.com", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.9279851317405701}]}, {"text": "The forum is divided into various domains.", "labels": [], "entities": []}, {"text": "Each domain consists of multiple threads of discussions.", "labels": [], "entities": []}, {"text": "For each post, we extracted the post id, author, domain, ids of all posts to which it replies/quotes, and the post content.", "labels": [], "entities": []}, {"text": "In all, we extracted 26137, 34986, 22354, and 16525 posts from Politics, Religion, Society and Science domains respectively.", "labels": [], "entities": []}, {"text": "Experiment Data: As it is not interesting to study pairs who only exchanged a few posts, we restrict to pairs with at least 20 post exchanges.", "labels": [], "entities": []}, {"text": "This resulted in 1241 authors and 1461 pairs.", "labels": [], "entities": []}, {"text": "The reduced dataset consists of 1095586 tokens (after n-gram preprocessing in \u00a74), 40102 posts with an average of 27 posts or interactions per pair.", "labels": [], "entities": []}, {"text": "Data from all 4 domains are combined for modeling.", "labels": [], "entities": []}, {"text": "Parameter Settings: For all our experiments, we set the hyper-parameters to the heuristic values \u00ed \u00b5\u00ed\u00bb\u00bc \u00ed \u00b5\u00ed\u00b1\u0087 = 50/\u00ed \u00b5\u00ed\u00b1\u0087, \u00ed \u00b5\u00ed\u00bb\u00bc \u00ed \u00b5\u00ed\u00b0\u00b8=\u00b5\u00ed\u00b0\u00b8= 50/\u00ed \u00b5\u00ed\u00b0\u00b8, \u00ed \u00b5\u00ed\u00bb\u00bd \u00ed \u00b5\u00ed\u00b1\u0087 = \u00ed \u00b5\u00ed\u00bb\u00bd \u00ed \u00b5\u00ed\u00b0\u00b8=\u00b5\u00ed\u00b0\u00b8= 0.1 suggested in ().", "labels": [], "entities": []}, {"text": "We set the number of topics, \u00ed \u00b5\u00ed\u00b1\u0087 = 50 and the number of ADexpression types, \u00ed \u00b5\u00ed\u00b0\u00b8=\u00b5\u00ed\u00b0\u00b8= 2 (agreement and disagreement) as in discussion/debate forums, there are usually two expression types . To learn Values for \u00ed \u00b5\u00ed\u00b0\u00b8>\u00b5\u00ed\u00b0\u00b8> 2 were also tried.", "labels": [], "entities": [{"text": "ADexpression", "start_pos": 59, "end_pos": 71, "type": "METRIC", "confidence": 0.8634284734725952}]}, {"text": "However, they did not produce any new dominant expression type.", "labels": [], "entities": []}, {"text": "There was also a slight increase in the model perplexity showing that values of \u00ed \u00b5\u00ed\u00b0\u00b8>\u00b5\u00ed\u00b0\u00b8> 2 do not fit the debate forum data well.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b0\u00b8>\u00b5\u00ed\u00b0\u00b8> 2", "start_pos": 80, "end_pos": 94, "type": "METRIC", "confidence": 0.7707604418198267}]}, {"text": "the Max-Ent parameters \u00ed \u00b5\u00ed\u00bc\u0086, we randomly sampled 500 terms from the held-out data (10 threads in our corpus which were excluded from the evaluation of tasks in \u00a76.2, \u00a76.3) appearing at least 10 times and labeled them as topical or AD-expressions (139) and used the corresponding features of each term (in the context of posts where it occurs, \u00a73) to train the Max-Ent model.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00bc\u0086", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.5581052601337433}]}, {"text": "We first list some discovered top AD-expressions in for qualitative inspection.", "labels": [], "entities": []}, {"text": "From, we can see that JTE-P can cluster many correct AD-expressions, e.g., \"I accept\", \"I agree\", \"you're correct\", etc.", "labels": [], "entities": [{"text": "JTE-P", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.8096763491630554}]}, {"text": "in agreement and \"I disagree\", \"don't accept\", \"I refute\", etc.", "labels": [], "entities": []}, {"text": "In addition, it also discovers and clusters highly specific and more \"distinctive\" expressions beyond those used in Max-Ent training, e.g., \"valid point\", \"I do support\", and \"rightly said\" in agreement; and phrases like \"can you prove\", \"I don't buy your\", and \"you fail to\" in disagreement.", "labels": [], "entities": []}, {"text": "Note that terms in black in were used in Max-Ent training.", "labels": [], "entities": []}, {"text": "The newly discovered terms are marked blue in italics.", "labels": [], "entities": []}, {"text": "Clustering errors are in red (bold).", "labels": [], "entities": []}, {"text": "For quantitative evaluation, topic models are often compared using perplexity.", "labels": [], "entities": []}, {"text": "However, perplexity does not reflect our purpose since we are not trying to evaluate how well the ADexpressions in an unseen discussion data fit our learned models.", "labels": [], "entities": []}, {"text": "Instead our focus is to evaluate how well our learned AD-expression types perform in clustering semantic phrases of agreement/disagreement.", "labels": [], "entities": [{"text": "clustering semantic phrases of agreement/disagreement", "start_pos": 85, "end_pos": 138, "type": "TASK", "confidence": 0.8703735300472805}]}, {"text": "Since AD-expressions (according to top terms in \u00ed \u00b5\u00ed\u00bc\u0091 \u00ed \u00b5\u00ed\u00b0\u00b8)\u00b5\u00ed\u00b0\u00b8) produced by JTE-P are rankings, we choose precision @ n (p@n) as our metric.", "labels": [], "entities": [{"text": "JTE-P", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.9415388107299805}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9921980500221252}]}, {"text": "p@n is commonly used to evaluate a ranking when the total number of correct items is unknown (e.g., Web search results, aspect terms in topic models for sentiment analysis, etc.).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.9492077231407166}]}, {"text": "This situation is similar to our AD-expression rankings, \u00ed \u00b5\u00ed\u00bc\u0091 \u00ed \u00b5\u00ed\u00b0\u00b8.\u00b5\u00ed\u00b0\u00b8.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00bc\u0091 \u00ed \u00b5\u00ed\u00b0\u00b8.\u00b5\u00ed\u00b0\u00b8", "start_pos": 57, "end_pos": 75, "type": "METRIC", "confidence": 0.6886057890951633}]}, {"text": "Further, as \u00ed \u00b5\u00ed\u00bc\u0091 \u00ed \u00b5\u00ed\u00b0\u00b8~\u00ed\u00b5\u00ed\u00b0\u00b8~\u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u009f, the Dirichlet smoothing effect ensures that every term in the vocabulary has some nonzero mass to agreement or disagreement expression type.", "labels": [], "entities": []}, {"text": "Thus, it is the ranking of terms in each AD-expression type that matters (i.e., whether the model is able to rank highly relevant terms at the top).", "labels": [], "entities": []}, {"text": "The above method evaluates the original ranking.", "labels": [], "entities": []}, {"text": "Another way of evaluating the ADexpression rankings is to evaluate only those newly discovered terms, i.e., beyond those labeled terms used in Max-Ent training.", "labels": [], "entities": []}, {"text": "For this evaluation, we remove those terms that have been used in Max-Ent (ME) training.", "labels": [], "entities": []}, {"text": "We report both results in.", "labels": [], "entities": []}, {"text": "We also studied interrater agreement using two judges who independently labeled the top n terms as corrector incorrect.", "labels": [], "entities": [{"text": "interrater agreement", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.9271918535232544}]}, {"text": "A term was marked correct if both judges deemed it so which was then used to compute p@n.", "labels": [], "entities": []}, {"text": "Agreement using \u00ed \u00b5\u00ed\u00bc \u00ed \u00b5\u00ed\u00b0 \u00b6\u00ed \u00b5\u00ed\u00b1\u009c\u210e\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u009b was greater than 0.78 for all p@n computations implying substantial and good agreements as identifying whether a phrase implies agreement or disagreement or none is an easy task.", "labels": [], "entities": []}, {"text": "P@n excluding ME labeled terms, second column) are slightly lower than those using all terms but are still decent.", "labels": [], "entities": []}, {"text": "This is because p@n excluding ME labeled terms removes many correct AD-expressions used in training.", "labels": [], "entities": []}, {"text": "Further to evaluate the sensitivity of performance on the amount of labeled terms for Max-Ent, we computed p@n across different sizes of labeled terms.", "labels": [], "entities": []}, {"text": "shows p@n for agreement and disagreement expressions across different sizes of labeled terms (L).", "labels": [], "entities": []}, {"text": "We find that more labeled terms improves p@n which is intuitive.", "labels": [], "entities": []}, {"text": "We used 500 labeled terms in all our subsequent experiments.", "labels": [], "entities": []}, {"text": "The result in uses relevance ranking ( \u00a74).", "labels": [], "entities": [{"text": "relevance ranking", "start_pos": 19, "end_pos": 36, "type": "METRIC", "confidence": 0.8844537138938904}]}, {"text": "Disagreement expressions (\u00ed \u00b5\u00ed\u00bc\u0091 \u00ed \u00b5\u00ed\u00b1\u0092=\u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u008e\u00ed \u00b5\u00ed\u00b1\u0094\u00ed \u00b5\u00ed\u00b1\u009f\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u009a\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u009b\u00ed \u00b5\u00ed\u00b1\u00a1 \u00ed \u00b5\u00ed\u00b0\u00b8) \u00b5\u00ed\u00b0\u00b8) I, disagree, I don't, I disagree, argument, reject, claim, I reject, I refute, and, your, I refuse, won't, the claim, nonsense, I contest, dispute, I think, completely disagree, don't accept, don't agree, incorrect, doesn't, hogwash, I don't buy your, I really doubt, your nonsense, true, can you prove, argument fails, you fail to, your assertions, bullshit, sheer nonsense, doesn't make sense, you have no clue, how can you say, do you even, contradict yourself, \u2026 Agreement expressions (\u00ed \u00b5\u00ed\u00bc\u0091 \u00ed \u00b5\u00ed\u00b1\u0092=\u00ed \u00b5\u00ed\u00b0\u00b4\u00ed \u00b5\u00ed\u00b1\u0094\u00ed \u00b5\u00ed\u00b1\u009f\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u009a\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u009b\u00ed \u00b5\u00ed\u00b1\u00a1 \u00ed \u00b5\u00ed\u00b0\u00b8) \u00b5\u00ed\u00b0\u00b8) agree, I, correct, yes, true, accept, I agree, don't, indeed correct, your, I accept, point, that, I concede, is valid, your claim, not really, would agree, might, agree completely, yes indeed, absolutely, you're correct, valid point, argument, the argument, proves, do accept, support, agree with you, rightly said, personally, well put, I do support, personally agree, doesn't necessarily, exactly, very well put, kudos, point taken, ...: Precision (P), recall (R), and F 1 scores of pair interaction evaluation.", "labels": [], "entities": []}, {"text": "Improvements in F 1 using AD-expression features (\u00ed \u00b5\u00ed\u00bc\u0091 \u00ed \u00b5\u00ed\u00b0\u00b8)\u00b5\u00ed\u00b0\u00b8) are statistically significant (p<0.01) using paired t-test across 5-fold CV.", "labels": [], "entities": [{"text": "F 1", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9454453587532043}]}, {"text": "We now compare with the performance of the model without using phrase relevance ranking.", "labels": [], "entities": []}, {"text": "P@n results using all tokens (4356787) are shown in (with 500 labeled terms for Max-Ent training).", "labels": [], "entities": []}, {"text": "Clearly, P@n is lower than in (last row; with phrase relevance ranking) because without phrase relevance ranking many irrelevant terms can rank high due to cooccurrences which may not be semantically related.", "labels": [], "entities": []}, {"text": "This shows that relevance ranking of phrases is beneficial.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Coverage (in %) of AD-expressions.", "labels": [], "entities": []}, {"text": " Table 3: Top terms (comma delimited) of two expression types. Red (bold) terms denote possible errors.  Blue (italics) terms are newly discovered; rest (black) terms have been used in Max-Ent training.", "labels": [], "entities": []}, {"text": " Table 4: Results using terms based on phrase relevance ranking for P @ n= 50, 100, 150 across 100, 200,  \u2026, 500 labeled examples (L) used for Max-Ent (ME) training.", "labels": [], "entities": []}, {"text": " Table 5: Results using all tokens (without applying phrase relevance ranking) for P@50, 100, 150 and 500  labeled examples were used for Max-Ent (ME) training).", "labels": [], "entities": []}, {"text": " Table 6: Precision (P), recall (R), and F 1 scores of pair interaction evaluation. Improvements in F 1 using  AD-expression features (\u00ed \u00b5\u00ed\u00bc\u0091 \u00ed \u00b5\u00ed\u00b0\u00b8)\u00b5\u00ed\u00b0\u00b8) are statistically significant (p<0.01) using paired t-test across 5-fold CV.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9243540167808533}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9386838227510452}, {"text": "F 1", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9937629997730255}, {"text": "F 1", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9482616484165192}]}]}