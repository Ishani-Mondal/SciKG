{"title": [{"text": "Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics", "labels": [], "entities": [{"text": "Derived Representations of Morphologically Complex Words", "start_pos": 17, "end_pos": 73, "type": "TASK", "confidence": 0.8620132505893707}]}], "abstractContent": [{"text": "Speakers of a language can construct an unlimited number of new words through morphological derivation.", "labels": [], "entities": []}, {"text": "This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning.", "labels": [], "entities": []}, {"text": "We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts.", "labels": [], "entities": []}, {"text": "Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data.", "labels": [], "entities": []}, {"text": "Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Effective ways to represent word meaning are needed in many branches of natural language processing.", "labels": [], "entities": []}, {"text": "In the last decades, corpus-based methods have achieved some degree of success in modeling lexical semantics.", "labels": [], "entities": []}, {"text": "Distributional semantic models (DSMs) in particular represent the meaning of a word by a vector, the dimensions of which encode corpus-extracted co-occurrence statistics, under the assumption that words that are semantically similar will occur in similar contexts.", "labels": [], "entities": [{"text": "Distributional semantic models (DSMs", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6193769156932831}]}, {"text": "Reliable distributional vectors can only be extracted for words that occur in many contexts in the corpus.", "labels": [], "entities": []}, {"text": "Not surprisingly, there is a strong correlation between word frequency and vector quality, and since most words occur only once even in very large corpora, DSMs suffer data sparseness.", "labels": [], "entities": []}, {"text": "While word rarity has many sources, one of the most common and systematic ones is the high productivity of morphological derivation processes, whereby an unlimited number of new words can be constructed by adding affixes to existing stems (.", "labels": [], "entities": [{"text": "word rarity", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.7372065782546997}]}, {"text": "For example, in the multi-billion-word corpus we introduce below, perfectly reasonable derived forms such as lexicalizable or affixless never occur.", "labels": [], "entities": []}, {"text": "Even without considering the theoretically infinite number of possible derived nonce words, and restricting ourselves instead to words that are already listed in dictionaries, complex forms cover a high portion of the lexicon.", "labels": [], "entities": []}, {"text": "For example, morphologically complex forms account for 55% of the lemmas in the CELEX English database (see Section 4.1 below).", "labels": [], "entities": [{"text": "CELEX English database", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.9692341486612955}]}, {"text": "In most of these cases (80% according to our corpus) the stem is more frequent than the complex form (e.g., the stem build occurs 15 times more often than the derived form rebuild, and the latter is certainly not an unusual derived form).", "labels": [], "entities": []}, {"text": "DSMs ignore derivational morphology altogether.", "labels": [], "entities": []}, {"text": "Consequently, they cannot provide meaning representations for new derived forms, nor can they harness the systematic relation existing between stems and derivations (any English speaker can infer that to rebuild is to build again, whether they are familiar with the prefixed form or not) in order to mitigate derived-form sparseness problems.", "labels": [], "entities": []}, {"text": "A simple way to handle derivational mor-1 Morphological derivation constructs new words (in the sense of lemmas) from existing lexical items (resource+ful\u2192resourceful).", "labels": [], "entities": [{"text": "Morphological derivation constructs new words", "start_pos": 42, "end_pos": 87, "type": "TASK", "confidence": 0.8677573442459107}]}, {"text": "In this work, we do not treat inflectional morphology, pertaining to affixes that encode grammatical features such as number or tense (dog+s).", "labels": [], "entities": []}, {"text": "We use morpheme for any component of a word (resource and -ful are both morphemes).", "labels": [], "entities": []}, {"text": "We use stem for the lexical item that constitutes the base of derivation (resource) and affix (prefix or suffix) for the element attached to the stem to derive the new form (-ful).", "labels": [], "entities": []}, {"text": "In English, stems are typically independent words, affixes bound morphemes, i.e., they cannot standalone.", "labels": [], "entities": []}, {"text": "Note that a stem can in turn be morphologically derived, e.g., point+less in pointless+ly.", "labels": [], "entities": []}, {"text": "Finally, we use morphologically complex as synonymous with derived.", "labels": [], "entities": []}, {"text": "phology would be to identify the stem of rare derived words and use its distributional vector as a proxy to derived-form meaning.", "labels": [], "entities": []}, {"text": "The meaning of rebuild is not that far from that of build, so the latter might provide a reasonable surrogate.", "labels": [], "entities": [{"text": "rebuild", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.9228569269180298}]}, {"text": "Still, something is clearly lost (if the author of a text felt the need to use the derived form, the stem was not fully appropriate), and sometimes the jump in meaning can be quite dramatic (resourceless and resource mean very different things!).", "labels": [], "entities": []}, {"text": "In the past few years there has been much interest in how DSMs can scale up to represent the meaning of larger chunks of text such as phrases or even sentences.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 58, "end_pos": 62, "type": "TASK", "confidence": 0.9462249875068665}]}, {"text": "Trying to represent the meaning of arbitrarily long constructions by directly collecting co-occurrence statistics is obviously ineffective and thus methods have been developed to derive the meaning of larger constructions as a function of the meaning of their constituents.", "labels": [], "entities": []}, {"text": "Compositional distributional semantic models (cDSMs) of word units aim at handling, compositionally, the high productivity of phrases and consequent data sparseness.", "labels": [], "entities": []}, {"text": "It is natural to hypothesize that the same methods can be applied to morphology to derive the meaning of complex words from the meaning of their parts: For example, instead of harvesting a rebuild vector directly from the corpus, the latter could be constructed from the distributional representations of re-and build.", "labels": [], "entities": []}, {"text": "Besides alleviating data sparseness problems, a system of this sort, that automatically induces the semantic contents of morphological processes, would also be of tremendous theoretical interest, given that the semantics of derivation is a central and challenging topic in linguistic morphology.", "labels": [], "entities": []}, {"text": "In this paper, we explore, for the first time (except for the proof-of-concept study in), the application of cDSMs to derivational morphology.", "labels": [], "entities": [{"text": "derivational morphology", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.8113113939762115}]}, {"text": "We adapt a number of composition methods from the literature to the morphological setting, and we show that some of these methods can provide better distributional representations of derived forms than either those directly harvested from a large corpus, or those obtained by using the stem as a proxy to derived-form meaning.", "labels": [], "entities": []}, {"text": "Our results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first experiment investigates to what extent composition models can approximate high-quality (HQ) corpus-extracted vectors representing derived forms.", "labels": [], "entities": []}, {"text": "Note that since the test items were excluded from training, we are simulating a scenario in which composition models must generate representations for nonce derived forms.", "labels": [], "entities": []}, {"text": "Cosine similarity between model-generated and corpus-extracted vectors were computed for all models, including the stem baseline (i.e., cosine between stem and derived form).", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.8505043387413025}]}, {"text": "The first row of reports mean similarities.", "labels": [], "entities": [{"text": "similarities", "start_pos": 30, "end_pos": 42, "type": "METRIC", "confidence": 0.9639546275138855}]}, {"text": "The stem method sets the level of performance relatively high, confirming its soundness.", "labels": [], "entities": []}, {"text": "Indeed, the parameter-free mult model performs below the baseline.", "labels": [], "entities": []}, {"text": "As expected, dilation performs simi-8 More accurately, we relied on semi-manual CELEX information to identify derived forms.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.6702597141265869}]}, {"text": "A further step towards a fully knowledge-free system would be to pre-process the corpus with an unsupervised morphological induction system to extract stem/derived pairs.", "labels": [], "entities": []}, {"text": "The other models have thousands of weights to be estimated, so we cannot summarize the outcome of parameter estimation here.", "labels": [], "entities": []}, {"text": "This result does not necessarily contradict those of: Mean similarity of composed vectors to high-quality corpus-extracted derived-form vectors, for all as well as high-(HR) and lowrelatedness (LR) test items larly to the baseline, while wadd outperforms it, although the effect does not reach significance (p=.06).", "labels": [], "entities": []}, {"text": "11 Both fulladd and lexfunc perform significantly better than stem (p < .001).", "labels": [], "entities": []}, {"text": "Lexfunc provides a flexible way to account for affixation, since it models it directly as a function mapping from and onto word vectors, without requiring a vector representation of bound affixes.", "labels": [], "entities": [{"text": "Lexfunc", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9303989410400391}]}, {"text": "The reason at the base of its good performance is thus quite straightforward.", "labels": [], "entities": []}, {"text": "On the other hand, it is surprising that a simple representation of bound affixes (i.e., as vectors aggregating the contexts of words containing them) can work so well, at least when used in conjunction with the granular dimension-by-dimension weights assigned by the fulladd method.", "labels": [], "entities": []}, {"text": "We hypothesize that these aggregated contexts, by providing information about the set of stems an affix combines with, capture the shared semantic features that the affix operates on.", "labels": [], "entities": []}, {"text": "When the meaning of the derived form is far from that of its stem, the stem baseline should no longer constitute a suitable surrogate of derivedform meaning.", "labels": [], "entities": []}, {"text": "The LR cases (see Section 4.1 above) are thus crucial to understand how well composition methods capture not only stem meaning, but also affix-triggered semantics.", "labels": [], "entities": []}, {"text": "The HR and LR rows of present the results for the respective test subsets.", "labels": [], "entities": [{"text": "LR", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.6977125406265259}]}, {"text": "As expected, the stem approach undergoes a strong drop when performance is measured on LR items.", "labels": [], "entities": []}, {"text": "At the other extreme, fulladd and lexfunc, while also finding the LR cases more difficult, still clearly outperform the baseline (p<.001), confirming that they capture the meaning of derived forms beyond what their stems contribute to it.", "labels": [], "entities": []}, {"text": "The effect of wadd, again, approaches significance when compared to the baseline (p = .05).", "labels": [], "entities": [{"text": "significance", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.9924557209014893}]}, {"text": "Very encouragingly, both stem mult wadd dil.: Mean similarity of composed vectors to high-quality corpus-extracted derived-form vectors with negative affixes fulladd and lexfunc significantly outperform stem also in the HR subset (p<.001).", "labels": [], "entities": []}, {"text": "That is, the models provide better approximations of derived forms even when the stem itself should already be a good surrogate.", "labels": [], "entities": []}, {"text": "The difference between the two models is not significant.", "labels": [], "entities": []}, {"text": "We noted in Section 4.1 that forms containing the \"negative\" affixes -less, un-and in-received on average low SDR scores, since negation impacts meaning more drastically than other operations.", "labels": [], "entities": []}, {"text": "reports the performance of the models on these affixes.", "labels": [], "entities": []}, {"text": "Indeed, the stem baseline performs quite poorly, whereas fulladd, lexfunc and, to a lesser extent, wadd are quite effective in this condition as well, all performing greatly above the baseline.", "labels": [], "entities": []}, {"text": "These results are intriguing in light of the fact that modeling negation is a challenging task for DSMs () as well as cDSMs (.", "labels": [], "entities": []}, {"text": "To the extent that our best methods have captured the negating function of a prefix such as in-, they might be applied to tasks such as recognizing lexical opposites, or even simple forms of syntactic negation (modeling inoperable is just a short step away from modeling not operable compositionally).", "labels": [], "entities": []}, {"text": "The first experiment simulated the scenario in which derived forms are not in our corpus, so that directly extracting their representation from it is not an option.", "labels": [], "entities": []}, {"text": "The second experiment tests if compositionally-derived representations can be better than those extracted directly from the corpus when the latter is a possible strategy (i.e., the derived forms are attested in the source corpus).", "labels": [], "entities": []}, {"text": "To this purpose, we focused on those 277 test items that were judged as low-quality (LQ, see Section 4.1), which are presumably more challenging to generate, and where the compositional route could be most useful.", "labels": [], "entities": []}, {"text": "We evaluated the derived forms generated by: Examples of model-predicted neighbors for words with LQ corpus-extracted vectors the models that performed best in the first experiment (fulladd, lexfunc and wadd), as well as the stem baseline, by means of another crowdsourcing study.", "labels": [], "entities": []}, {"text": "We followed the same procedure used to assess the quality of corpus-extracted vectors, that is, we asked judges to rate the relatedness of the target forms to their NNs (we obtained on average 29 responses per form).", "labels": [], "entities": []}, {"text": "The first line of reports the average quality (on a 7-point scale) of the representations of the derived forms as produced by the models and baseline, as well as of the corpus-harvested ones (corpus column).", "labels": [], "entities": []}, {"text": "All compositional models produce representations that are of significantly higher quality (p < .001) than the corpus-based ones.", "labels": [], "entities": []}, {"text": "The effect is also evident in qualitative terms.", "labels": [], "entities": []}, {"text": "presents the NNs predicted by the three compositional methods for the same LQ test items whose corpus-based NNs are presented in.", "labels": [], "entities": []}, {"text": "These results indicate that morpheme composition is an effective solution when the quality of corpus-extracted derived forms is low (and the previous experiment showed that, when their quality is high, composition can at least approximate corpus-based vectors).", "labels": [], "entities": []}, {"text": "With respect to Experiment 1, we obtain a different ranking of the models, with lexfunc being outperformed by both wadd and fulladd (p<.001), that are statistically indistinguishable.", "labels": [], "entities": []}, {"text": "The wadd composition is dominated by the stem, and by looking at the examples in we notice that both this model and fulladd tend to feature the stem as NN (100% of the cases for wadd, 73% for fulladd in the complete test set).", "labels": [], "entities": []}, {"text": "The question thus arises as to whether the good performance of these composition techniques is simply due to the fact that they produce derived forms that are near their stems, with no added semantic value from the affix (a \"stemploitation\" strategy).", "labels": [], "entities": []}, {"text": "However, the stemploitation hypothesis is dispelled by the observation that both models significantly outperform the stem baseline (p<.001), despite the fact that the latter, again, has good performance, significantly outperforming the corpusderived vectors (p < .001).", "labels": [], "entities": []}, {"text": "Thus, we confirm that compositional models provide higher quality vectors that are capturing the meaning of derived forms beyond the information provided by the stem.", "labels": [], "entities": []}, {"text": "Indeed, if we focus on the third row of Table 5, reporting performance on low stem-derived relatedness (LR) items (annotated as described in Section 4.1), fulladd and wadd still significantly outperform the corpus representations (p < .001), whereas the quality of the stem representations of LR items is not significantly different form that of the corpus-derived ones.", "labels": [], "entities": []}, {"text": "Interestingly, lexfunc displays the smallest drop in performance when restricting evaluation to LR items; however, since it does not significantly outperform the LQ corpus representations, this is arguably due to a floor effect.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Derivational morphology dataset", "labels": [], "entities": []}, {"text": " Table 3: Mean similarity of composed vectors to  high-quality corpus-extracted derived-form vec- tors, for all as well as high-(HR) and low- relatedness (LR) test items", "labels": [], "entities": []}, {"text": " Table 4: Mean similarity of composed vectors to  high-quality corpus-extracted derived-form vec- tors with negative affixes", "labels": [], "entities": [{"text": "Mean similarity", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.8890520632266998}]}, {"text": " Table 5: Average quality ratings of derived vectors", "labels": [], "entities": []}]}