{"title": [{"text": "Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.800754725933075}]}], "abstractContent": [{"text": "Structural information in web text provides natural annotations for NLP problems such as word segmentation and parsing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7407441288232803}]}, {"text": "In this paper we propose a discrim-inative learning algorithm to take advantage of the linguistic knowledge in large amounts of natural annotations on the In-ternet.", "labels": [], "entities": []}, {"text": "It utilizes the Internet as an external corpus with massive (although slight and sparse) natural annotations, and enables a classifier to evolve on the large-scaled and real-time updated web text.", "labels": [], "entities": []}, {"text": "With Chinese word segmentation as a case study, experiments show that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 5, "end_pos": 30, "type": "TASK", "confidence": 0.6446825365225474}]}], "introductionContent": [{"text": "Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.8016625642776489}, {"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7976840436458588}, {"text": "word segmentation", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.7701930403709412}]}, {"text": "Taking Chinese word segmentation for example, the state-of-the-art models () are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (), and perform quite well on corresponding test sets.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 7, "end_pos": 32, "type": "TASK", "confidence": 0.6675166289011637}, {"text": "Penn Chinese Treebank (CTB)", "start_pos": 136, "end_pos": 163, "type": "DATASET", "confidence": 0.9654688636461893}]}, {"text": "Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in think that NLP has already ...", "labels": [], "entities": []}, {"text": "size (e.g. tens of thousands), the performance of word segmentation tends to degrade sharply when applied to new domains.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7341961860656738}]}, {"text": "Internet provides large amounts of raw text, and statistics collected from it have been used to improve parsing performance).", "labels": [], "entities": [{"text": "parsing", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.9619364142417908}]}, {"text": "The Internet also gives massive (although slight and sparse) natural annotations in the forms of structural information including hyperlinks, fonts, colors and layouts.", "labels": [], "entities": []}, {"text": "These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7466127276420593}]}, {"text": "The hyperlink indicates a Chinese phrase (meaning NLP), and it probably corresponds to a connected sub-graph for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.8383570611476898}]}, {"text": "Creators of web text give valuable annotations during editing, the whole Internet can be treated as a wide-coveraged and real-time updated corpus.", "labels": [], "entities": []}, {"text": "Different from the dense and accurate annotations in human-annotated corpora, natural annotations in web text are sparse and slight, it makes direct training of NLP models impracticable.", "labels": [], "entities": []}, {"text": "In this work we take for example a most important problem, word segmentation, and propose a novel discriminative learning algorithm to leverage the knowledge in massive natural annotations of web text.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7800767719745636}]}, {"text": "Character classification models for word segmentation usually factorize the whole prediction into atomic predictions on characters).", "labels": [], "entities": [{"text": "Character classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.763941079378128}, {"text": "word segmentation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7687373459339142}]}, {"text": "Natural annotations in web text can be used to get rid of implausible predication candidates for related characters, knowledge in the natural annotations is therefore introduced in the manner of searching space pruning.", "labels": [], "entities": []}, {"text": "Since constraint decoding in the pruned searching space integrates the knowledge of the baseline model and natural annotations, it gives predictions not worse than the normal decoding does.", "labels": [], "entities": []}, {"text": "Annotation differences between the outputs of constraint decoding and normal decoding are used to train the enhanced classifier.", "labels": [], "entities": []}, {"text": "This strategy makes the usage of natural annotations simple and universal, which facilitates the utilization of massive web text and the extension to other NLP problems.", "labels": [], "entities": []}, {"text": "Although there are lots of choices, we choose the Chinese wikipedia as the knowledge source due to its high quality.", "labels": [], "entities": [{"text": "Chinese wikipedia", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.9145821928977966}]}, {"text": "Structural information, including hyperlinks, fonts and colors are used to determine the boundaries of meaningful fragments.", "labels": [], "entities": []}, {"text": "Experimental results show that, the knowledge implied in the natural annotations can significantly improve the performance of a baseline segmenter trained on CTB 5.0, an F-measure increment of 0.93 points on CTB test set, and an average increment of 1.53 points on 7 other domains.", "labels": [], "entities": [{"text": "CTB 5.0", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.9421692788600922}, {"text": "F-measure increment", "start_pos": 170, "end_pos": 189, "type": "METRIC", "confidence": 0.9488259851932526}, {"text": "CTB test set", "start_pos": 208, "end_pos": 220, "type": "DATASET", "confidence": 0.9742534955342611}]}, {"text": "It is an effective and inexpensive strategy to build word segmenters adaptive to different domains.", "labels": [], "entities": []}, {"text": "We hope to extend this strategy to other NLP problems such as named entity recognition and parsing.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.6298074225584666}, {"text": "parsing", "start_pos": 91, "end_pos": 98, "type": "TASK", "confidence": 0.8487536311149597}]}, {"text": "In the rest of the paper, we first briefly introduce the problems of Chinese word segmentation and the character classification model in section T (C \u22122:2 )= 44444: Feature templates and instances for character classification-based word segmentation model.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.589472363392512}, {"text": "character classification", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.7337734699249268}, {"text": "character classification-based word segmentation", "start_pos": 201, "end_pos": 249, "type": "TASK", "confidence": 0.685721293091774}]}, {"text": "Suppose we are considering the i-th character \"\" in \"...", "labels": [], "entities": []}, {"text": "2, then describe the representation of the knowledge in natural annotations of web text in section 3, and finally detail the strategy of discriminative learning on natural annotations in section 4.", "labels": [], "entities": []}, {"text": "After giving the experimental results and analysis in section 5, we briefly introduce the previous related work and then give the conclusion and the expectation of future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Penn Chinese Treebank 5.0 (CTB) () as the existing annotated corpus for Chinese word segmentation.", "labels": [], "entities": [{"text": "Penn Chinese Treebank 5.0 (CTB)", "start_pos": 11, "end_pos": 42, "type": "DATASET", "confidence": 0.9727566242218018}, {"text": "Chinese word segmentation", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.6283967792987823}]}, {"text": "For convenient of comparison with other work in word segmentation, the whole corpus is split into three partitions as follows: chapters 271-300 for testing, chapters 301-325 for developing, and others for training.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7864727675914764}]}, {"text": "We choose the Chinese wikipedia 1 (version 20120812) as the external knowledge source, because it has high quality in contents and it is much better than usual web text.", "labels": [], "entities": [{"text": "Chinese wikipedia 1 (version 20120812)", "start_pos": 14, "end_pos": 52, "type": "DATASET", "confidence": 0.9135809115001133}]}, {"text": "Structural informations, including hyperlinks, fonts and colors are used to derive the annotation information.", "labels": [], "entities": []}, {"text": "To further evaluate the improvement brought by the fuzzy knowledge in Chinese wikipedia, a series of testing sets from different domains are adopted.", "labels": [], "entities": []}, {"text": "Figure 3: Learning curve of the averaged perceptron classifier on the CTB developing set.", "labels": [], "entities": [{"text": "CTB developing set", "start_pos": 70, "end_pos": 88, "type": "DATASET", "confidence": 0.9360296527544657}]}, {"text": "word segmentation standard (), the quantity of accuracy improvement is still illustrative since there are no vast diversities between the two segmentation standards.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6651741564273834}, {"text": "accuracy improvement", "start_pos": 47, "end_pos": 67, "type": "METRIC", "confidence": 0.962746649980545}]}, {"text": "We also annotated another three testing sets 2 , their texts are drawn from the domains of chemistry, physics and machinery, and each contains 500 sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data partitioning for CTB 5.0.", "labels": [], "entities": [{"text": "Data partitioning", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6292347013950348}, {"text": "CTB 5.0", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9118369817733765}]}, {"text": " Table 3: Performance of the baseline classifier and  the classifier enhanced with natural annotations in  Chinese wikipedia.", "labels": [], "entities": []}, {"text": " Table 3. On the  CTB testing set, training data from the Chinese", "labels": [], "entities": [{"text": "CTB testing set", "start_pos": 18, "end_pos": 33, "type": "DATASET", "confidence": 0.9768051902453104}]}]}