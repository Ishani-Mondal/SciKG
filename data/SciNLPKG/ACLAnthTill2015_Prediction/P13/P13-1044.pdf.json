{"title": [{"text": "Nonconvex Global Optimization for Latent-Variable Models *", "labels": [], "entities": []}], "abstractContent": [{"text": "Many models in NLP involve latent variables , such as unknown parses, tags, or alignments.", "labels": [], "entities": []}, {"text": "Finding the optimal model parameters is then usually a difficult noncon-vex optimization problem.", "labels": [], "entities": []}, {"text": "The usual practice is to settle for local optimization methods such as EM or gradient ascent.", "labels": [], "entities": [{"text": "gradient ascent", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.6927973330020905}]}, {"text": "We explore how one might instead search fora global optimum in parameter space, using branch-and-bound.", "labels": [], "entities": []}, {"text": "Our method would eventually find the global maximum (up to a user-specified) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum.", "labels": [], "entities": []}, {"text": "As an illustrative case, we study a gener-ative model for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.8387013375759125}]}, {"text": "We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints.", "labels": [], "entities": []}, {"text": "We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints.", "labels": [], "entities": []}, {"text": "We use the Reformulation Lin-earization Technique to produce convex relaxations during branch-and-bound.", "labels": [], "entities": []}, {"text": "Although these techniques do not yet provide a practical solution to our instance of this NP-hard problem, they sometimes find better solutions than Viterbi EM with random restarts, in the same time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Rich models with latent linguistic variables are popular in computational linguistics, but in general it is not known how to find their optimal parameters.", "labels": [], "entities": []}, {"text": "In this paper, we present some \"new\" attacks for this common optimization setting, drawn from the mathematical programming toolbox.", "labels": [], "entities": []}, {"text": "We focus on the well-studied but unsolved task of unsupervised dependency parsing (i.e., depen- * This research was partially funded by the JHU Human Language Technology Center of Excellence.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.710550844669342}, {"text": "JHU Human Language Technology Center", "start_pos": 140, "end_pos": 176, "type": "DATASET", "confidence": 0.9248272180557251}]}, {"text": "given by the best solution seen so far, the incumbent.", "labels": [], "entities": []}, {"text": "The upper bound, -298, is the min of all remaining leaf nodes.", "labels": [], "entities": []}, {"text": "The node with a local bound of -467.5 can be pruned because no solution within its subspace could be better than the incumbent.", "labels": [], "entities": []}, {"text": "This maybe a particularly hard case, but its structure is typical.", "labels": [], "entities": []}, {"text": "Many parameter estimation techniques have been attempted, including expectation-maximization (EM) (), contrastive estimation (), Viterbi EM (, and variational EM ().", "labels": [], "entities": []}, {"text": "These are all local search techniques, which improve the parameters by hill-climbing.", "labels": [], "entities": []}, {"text": "The problem with local search is that it gets stuck in local optima.", "labels": [], "entities": []}, {"text": "This is evident for grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.801389753818512}]}, {"text": "An algorithm such as EM will find numerous different solutions when randomly initialized to different points.", "labels": [], "entities": []}, {"text": "A variety of ways to find better local optima have been explored, including heuristic initialization of the model parameters (), random restarts), and annealing ().", "labels": [], "entities": []}, {"text": "Others have achieved accuracy improvements by enforcing linguistically motivated posterior constraints on the parameters (, such as requiring most sentences to have verbs or encouraging nouns to be children of verbs or prepositions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9976670742034912}]}, {"text": "We introduce a method that performs global search with certificates of -optimality for both the corpus parse and the model parameters.", "labels": [], "entities": []}, {"text": "Our search objective is log-likelihood.", "labels": [], "entities": []}, {"text": "We can also impose posterior constraints on the latent structure.", "labels": [], "entities": []}, {"text": "As we show, maximizing the joint loglikelihood of the parses and the parameters can be formulated as a mathematical program (MP) with a nonconvex quadratic objective and with integer linear and nonlinear constraints.", "labels": [], "entities": []}, {"text": "Note that this objective is that of hard (Viterbi) EM-we do not marginalize over the parses as in classical EM.", "labels": [], "entities": []}, {"text": "To globally optimize the objective function, we employ a branch-and-bound algorithm that searches the continuous space of the model parameters by branching on individual parameters (see).", "labels": [], "entities": []}, {"text": "Thus, our branch-and-bound tree serves to recursively subdivide the global parameter hypercube.", "labels": [], "entities": []}, {"text": "Each node represents a search problem over one of the resulting boxes (i.e., orthotopes).", "labels": [], "entities": []}, {"text": "The crucial step is to prune nodes high in the tree by determining that their boxes cannot contain the global maximum.", "labels": [], "entities": []}, {"text": "We compute an upper bound at each node by solving a relaxed maximization problem tailored to its box.", "labels": [], "entities": []}, {"text": "If this upper bound is worse than our current best solution, we can prune the node.", "labels": [], "entities": []}, {"text": "If not, we split the box again via another branching decision and retry on the two halves.", "labels": [], "entities": []}, {"text": "At each node, our relaxation derives a linear programming problem (LP) that can be efficiently solved by the dual simplex method.", "labels": [], "entities": []}, {"text": "First, we linearly relax the constraints that grammar rule probabilities sum to 1-these constraints are nonlinear in our parameters, which are log-probabilities.", "labels": [], "entities": []}, {"text": "Second, we linearize the quadratic objective by applying the Reformulation Linearization Technique (RLT), a method of forming tight linear relaxations of various types of MPs: the reformulation step multiplies together pairs of the original linear constraints to generate new quadratic constraints, and then the linearization step replaces quadratic terms in the new constraints with auxiliary variables.", "labels": [], "entities": [{"text": "Reformulation Linearization Technique (RLT)", "start_pos": 61, "end_pos": 104, "type": "METRIC", "confidence": 0.5420867105325063}]}, {"text": "Finally, if the node is not pruned, we search fora better incumbent solution under that node by projecting the solution of the RLT relaxation back onto the feasible region.", "labels": [], "entities": []}, {"text": "In the relaxation, the model parameters might sum to slightly more than one and the parses can consist of fractional dependency edges.", "labels": [], "entities": []}, {"text": "We project in order to compute the true objective and compare with other solutions.", "labels": [], "entities": []}, {"text": "Our results demonstrate that our method can obtain higher likelihoods than Viterbi EM with random restarts.", "labels": [], "entities": []}, {"text": "Furthermore, we show how posterior constraints inspired by and can easily be applied in our framework to obtain competitive accuracies using a simple model, the Dependency Model with Valence ().", "labels": [], "entities": []}, {"text": "We also obtain an -optimal solution on a toy dataset.", "labels": [], "entities": []}, {"text": "We caution that the linear relaxations are very loose on larger boxes.", "labels": [], "entities": []}, {"text": "Since we have many dimensions, the binary branch-and-bound tree may have to grow quite deep before the boxes become small enough to prune.", "labels": [], "entities": []}, {"text": "This is why nonconvex quadratic optimization by LP-based branch-and-bound usually fails with more than 80 variables.", "labels": [], "entities": []}, {"text": "Even our smallest (toy) problems have hundreds of variables, so our experimental results mainly just illuminate the method's behavior.", "labels": [], "entities": []}, {"text": "Nonetheless, we offer the method as anew tool which, just as for local search, might be combined with other forms of problem-specific guidance to produce more practical results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first analyze the behavior of our method on a toy synthetic dataset.", "labels": [], "entities": []}, {"text": "Next, we compare various parameter settings for branch-and-bound by estimating the total solution time.", "labels": [], "entities": []}, {"text": "Finally, we compare our search method to Viterbi EM on a small subset of the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.9969990849494934}]}, {"text": "All our experiments use the DMV for unsupervised dependency parsing of part-of-speech (POS) tag sequences.", "labels": [], "entities": [{"text": "dependency parsing of part-of-speech (POS) tag sequences", "start_pos": 49, "end_pos": 105, "type": "TASK", "confidence": 0.829969498846266}]}, {"text": "For Viterbi EM we initialize the parameters of the model uniformly, breaking parser ties randomly in the first E-step ().", "labels": [], "entities": []}, {"text": "This initializer is state-of-the-art for Viterbi EM.", "labels": [], "entities": [{"text": "Viterbi EM", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.9121857583522797}]}, {"text": "We also apply add-one smoothing during each M-step.", "labels": [], "entities": []}, {"text": "We use random restarts, and select the model with the highest likelihood.", "labels": [], "entities": []}, {"text": "We add posterior constraints to Viterbi EM's Estep.", "labels": [], "entities": [{"text": "Estep", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.7866149544715881}]}, {"text": "First, we run a relaxed linear programming (LP) parser, then project the (possibly fractional) parses back to the feasible region.", "labels": [], "entities": []}, {"text": "If the resulting parse does not respect the posterior constraints, we discard it.", "labels": [], "entities": []}, {"text": "The posterior constraint in the LP parser is tighter 4 than the one used in the true optimization problem, so the projections tends to be feasible under the true (looser) posterior constraints.", "labels": [], "entities": []}, {"text": "In our experiments, all but one projection respected the constraints.", "labels": [], "entities": []}, {"text": "We solve all LPs with CPLEX.", "labels": [], "entities": [{"text": "CPLEX", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.7660768628120422}]}], "tableCaptions": [{"text": " Table 1: Branch-and-bound node count and com- pletion time estimates. Each standard deviation  was close in magnitude to the estimate itself. We  ran for 8 hours, stopping at 10,000 samples on 8  synthetic sentences.", "labels": [], "entities": [{"text": "com- pletion time", "start_pos": 42, "end_pos": 59, "type": "METRIC", "confidence": 0.8485236316919327}]}]}