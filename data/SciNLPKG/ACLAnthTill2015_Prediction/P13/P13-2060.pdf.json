{"title": [{"text": "Stacking for Statistical Machine Translation *", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.8616231282552084}]}], "abstractContent": [{"text": "We propose the use of stacking, an ensemble learning technique, to the statistical machine translation (SMT) models.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 71, "end_pos": 108, "type": "TASK", "confidence": 0.7737285693486532}]}, {"text": "A diverse ensemble of weak learners is created using the same SMT engine (a hierarchical phrase-based system) by manipulating the training data and a strong model is created by combining the weak models on-the-fly.", "labels": [], "entities": [{"text": "SMT", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9455955624580383}]}, {"text": "Experimental results on two language pairs and three different sizes of training data show significant improvements of up to 4 BLEU points over a conventionally trained SMT model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9989762306213379}, {"text": "SMT", "start_pos": 169, "end_pos": 172, "type": "TASK", "confidence": 0.985711395740509}]}], "introductionContent": [{"text": "Ensemble-based methods have been widely used in machine learning with the aim of reducing the instability of classifiers and regressors and/or increase their bias.", "labels": [], "entities": []}, {"text": "The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error.", "labels": [], "entities": []}, {"text": "It has also been successfully applied to a wide variety of tasks in NLP (; F.", "labels": [], "entities": []}, {"text": "T. and recently has attracted attention in the statistical machine translation community in various work (.", "labels": [], "entities": [{"text": "statistical machine translation community", "start_pos": 47, "end_pos": 88, "type": "TASK", "confidence": 0.7747784107923508}]}, {"text": "In this paper, we propose a method to adopt stacking, an ensemble learning technique, to SMT.", "labels": [], "entities": [{"text": "stacking", "start_pos": 44, "end_pos": 52, "type": "TASK", "confidence": 0.9677481055259705}, {"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9938051700592041}]}, {"text": "We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition.", "labels": [], "entities": []}, {"text": "This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has different feature function values for the SMT log-linear model.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.615959495306015}, {"text": "SMT", "start_pos": 154, "end_pos": 157, "type": "TASK", "confidence": 0.9830741286277771}]}, {"text": "The weights of model are then tuned using minimum error rate training on the held-out fold to provide k weak models.", "labels": [], "entities": []}, {"text": "We then create a strong model by stacking another meta-learner on top of weak models to combine them into a single model.", "labels": [], "entities": []}, {"text": "The particular second-tier model we use is a model combination approach called ensemble decoding which combines hypotheses from the weak models on-the-fly in the decoder.", "labels": [], "entities": []}, {"text": "Using this approach, we take advantage of the diversity created by manipulating the training data and obtain a significant and consistent improvement over a conventionally trained SMT model with a fixed training and tuning set.", "labels": [], "entities": [{"text": "SMT", "start_pos": 180, "end_pos": 183, "type": "TASK", "confidence": 0.9842901825904846}]}], "datasetContent": [{"text": "We experimented with two language pairs: French to English and Spanish to English on the Europarl corpus (v7) ( and used ACL/WMT 2005 2 data for dev and test sets.", "labels": [], "entities": [{"text": "Europarl corpus (v7)", "start_pos": 89, "end_pos": 109, "type": "DATASET", "confidence": 0.9661836981773376}, {"text": "ACL/WMT 2005 2 data", "start_pos": 121, "end_pos": 140, "type": "DATASET", "confidence": 0.9049009283383688}]}, {"text": "For the base models, we used an in-house implementation of hierarchical phrase-based systems,, which uses the same features mentioned in: forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and gluerules penalty.", "labels": [], "entities": []}, {"text": "GIZA++ has been used for word alignment with phrase length limit of 10.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.8019718825817108}]}, {"text": "Feature weights were optimized using MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.5261722207069397}]}, {"text": "We built a 5-gram language model on the English side of Europarl and used the Kneser-Ney smoothing method and SRILM) as the language model toolkit.: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.", "labels": [], "entities": [{"text": "English side of Europarl", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.7742849439382553}, {"text": "SRILM", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.9152299761772156}, {"text": "BLEU", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.9989010095596313}]}], "tableCaptions": [{"text": " Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9964730143547058}]}, {"text": " Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9959803819656372}]}]}