{"title": [{"text": "Using Syntactic Dependencies to Solve Coreferences", "labels": [], "entities": [{"text": "Coreferences", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.7565779089927673}]}], "abstractContent": [{"text": "This paper describes the structure of the LTH coreference solver used in the closed track of the CoNLL 2012 shared task (Pradhan et al., 2012).", "labels": [], "entities": [{"text": "LTH coreference solver", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7273619771003723}, {"text": "CoNLL 2012 shared task", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.7610302418470383}]}, {"text": "The solver core is a mention classifier that uses Soon et al.", "labels": [], "entities": [{"text": "solver", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.9868964552879333}]}, {"text": "(2001)'s algorithm and features extracted from the dependency graphs of the sentences.", "labels": [], "entities": []}, {"text": "This system builds on Bj\u00f6rkelund and Nugues (2011)'s solver that we extended so that it can be applied to the three languages of the task: English, Chinese, and Arabic.", "labels": [], "entities": [{"text": "solver", "start_pos": 53, "end_pos": 59, "type": "TASK", "confidence": 0.9115106463432312}]}, {"text": "We designed anew mention detection module that removes pleonastic pronouns, prunes constituents, and recovers mentions when they do not match exactly a noun phrase.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7306888997554779}]}, {"text": "We carefully redesigned the features so that they reflect more complex linguistic phenomena as well as discourse properties.", "labels": [], "entities": []}, {"text": "Finally, we introduced a minimal cluster model grounded in the first mention of an entity.", "labels": [], "entities": []}, {"text": "We optimized the feature sets for the three languages: We carried out an extensive evaluation of pairs of features and we complemented the single features with associations that improved the CoNLL score.", "labels": [], "entities": []}, {"text": "We obtained the respective scores of 59.57, 56.62, and 48.25 on English, Chinese, and Arabic on the development set, 59.36, 56.85, and 49.43 on the test set, and the combined official score of 55.21.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we present the LTH coreference solver used in the closed track of the CoNLL 2012 shared task ().", "labels": [], "entities": [{"text": "LTH coreference solver", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.6955448091030121}, {"text": "CoNLL 2012 shared task", "start_pos": 85, "end_pos": 107, "type": "DATASET", "confidence": 0.833381250500679}]}, {"text": "We started from an earlier version of the system by, to which we added substantial improvements.", "labels": [], "entities": []}, {"text": "As base learning and decoding algorithm, our solver extracts noun phrases and possessive pronouns and uses's pairwise classifier to decide if a pair corefers or not.", "labels": [], "entities": [{"text": "solver extracts noun phrases and possessive pronouns", "start_pos": 45, "end_pos": 97, "type": "TASK", "confidence": 0.8407041941370282}]}, {"text": "Similarly to the earlier LTH system, we constructed a primary feature set from properties extracted from the dependency graphs of the sentences.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Precision and recall for the mention detection  stage on the training set.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9971336126327515}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.999147891998291}, {"text": "mention detection", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.8204645216464996}]}, {"text": " Table 4: Impact on the overall score on the English devel- opment set by addition of named entities extracted from  the corpus.", "labels": [], "entities": [{"text": "Impact", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9808651804924011}, {"text": "English devel- opment set", "start_pos": 45, "end_pos": 70, "type": "DATASET", "confidence": 0.7609893679618835}]}, {"text": " Table 5: Results on running the system on the develop- ment set with and without pruning for all the languages.", "labels": [], "entities": []}, {"text": " Table 8: Scores on the development set, test set, and test set with gold mentions for English, Chinese, and Arabic:  recall R, precision P, and harmonic mean F1. The official CoNLL score is computed as the arithmetic mean of MUC,  BCUB, and CEAFE.", "labels": [], "entities": [{"text": "recall R", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9465074837207794}, {"text": "precision P", "start_pos": 128, "end_pos": 139, "type": "METRIC", "confidence": 0.9692693650722504}, {"text": "harmonic mean F1", "start_pos": 145, "end_pos": 161, "type": "METRIC", "confidence": 0.7914561033248901}, {"text": "MUC", "start_pos": 226, "end_pos": 229, "type": "DATASET", "confidence": 0.6218878626823425}, {"text": "BCUB", "start_pos": 232, "end_pos": 236, "type": "METRIC", "confidence": 0.8753593564033508}, {"text": "CEAFE", "start_pos": 242, "end_pos": 247, "type": "DATASET", "confidence": 0.9266433715820312}]}]}