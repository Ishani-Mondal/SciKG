{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 1-11, Question Ranking and Selection in Tutorial Dialogues", "labels": [], "entities": []}], "abstractContent": [{"text": "A key challenge for dialogue-based intelligent tutoring systems lies in selecting follow-up questions that are not only context relevant but also encourage self-expression and stimulate learning.", "labels": [], "entities": []}, {"text": "This paper presents an approach to ranking candidate questions fora given dialogue context and introduces an evaluation framework for this task.", "labels": [], "entities": []}, {"text": "We learn to rank using judgments collected from expert human tutors, and we show that adding features derived from a rich, multi-layer dialogue act representation improves system performance over baseline lexical and syntactic features to a level in agreement with the judges.", "labels": [], "entities": []}, {"text": "The experimental results highlight the important factors in modeling the questioning process.", "labels": [], "entities": []}, {"text": "This work provides a framework for future work in automatic question generation and it represents a step toward the larger goal of directly learning tutorial dialogue policies directly from human examples.", "labels": [], "entities": [{"text": "automatic question generation", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.6290662089983622}]}], "introductionContent": [{"text": "Socratic tutoring styles place an emphasis on eliciting information from the learner to help them build their own connections to the material.", "labels": [], "entities": []}, {"text": "The role of a tutor in a Socratic dialogue is to scaffold the material and present questions that ultimately lead the student to an \"A-ha!\" moment.", "labels": [], "entities": [{"text": "A-ha", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9865087866783142}]}, {"text": "Numerous studies have illustrated the effectiveness of Socratic-style tutoring (; consequently recreating the behavior on a computer has long been a goal of research in Intelligent Tutoring Systems (ITS).", "labels": [], "entities": []}, {"text": "Recent successes have shown the efficacy of conversational ITS (, however these systems are still not as effective as human tutors, and much improvement is needed before they can truly claim to be Socratic.", "labels": [], "entities": []}, {"text": "Furthermore, development and tuning of tutorial dialogue behavior requires significant human effort.", "labels": [], "entities": []}, {"text": "While our overarching goal is to improve ITS by automatically learning tutorial dialogue strategies directly from expert tutor behavior, we focus on the crucial subtask of selecting follow-up questions.", "labels": [], "entities": [{"text": "ITS", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9729814529418945}]}, {"text": "Although asking questions is only a subset of the overall tutoring process, it is still a complex process that requires understanding of the dialogue state, the student's ability, and the learning goals.", "labels": [], "entities": []}, {"text": "This work frames question selection as a task of scoring and ranking candidate questions fora specific point in the tutorial dialogue.", "labels": [], "entities": [{"text": "question selection", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7588159441947937}]}, {"text": "Since dialogue is a dynamic process with multiple correct possibilities, we do not restrict ourselves only to the moves and questions found in a corpus of transcripts.", "labels": [], "entities": []}, {"text": "Instead we posit \"What if we had a fully automatic question generation system?\" and subsequently use candidate questions hand-authored for each dialogue context.", "labels": [], "entities": [{"text": "question generation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7235263586044312}]}, {"text": "To explore the mechanisms involved in ranking follow-up questions against one other, we pair these questions with judgments of quality from expert human tutors and extract surface form and dialogue-based features to train machine learning classification models to rank the appropriateness of questions for specific points in a dialogue.", "labels": [], "entities": []}, {"text": "Our results show promise with our best question ranking models exhibiting performance on par with expert human tutors.", "labels": [], "entities": []}, {"text": "Furthermore these experiments demonstrate the utility and importance of rich dialogue move annotation for modeling decision making in conversation and tutoring.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our systems' performance in ranking, we use two measures commonly used in information retrieval: the Mean Kendall's-\u03c4 measure described in section 3.4.1 and Mean Reciprocal Rank (MRR).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.8027015328407288}, {"text": "Mean Kendall's-\u03c4 measure", "start_pos": 113, "end_pos": 137, "type": "METRIC", "confidence": 0.9071624279022217}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 169, "end_pos": 195, "type": "METRIC", "confidence": 0.907808393239975}]}, {"text": "MRR is the average of the multiplicative inverse of the rank of the highest ranking question across all contexts.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8659576773643494}]}, {"text": "To account for ties we use the Tau-b variant of Kendall's-\u03c4 , and for MRR we compute reciprocal rank by averaging the system rankings for all of the questions tied for first.", "labels": [], "entities": [{"text": "MRR", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.38484203815460205}]}, {"text": "To obtain a goldstandard ranking for comparison, we combine individual raters' ratings using the approached described in section 4.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement for DISCUSS types  (DA=Dialogue Act, RF=Rhetorical Form, PT=Predicate  Type)", "labels": [], "entities": []}, {"text": " Table 2: Example dialogue context snippet and a collection of candidate questions. The frame, element, and DISCUSS  columns show how the questions vary from one another.", "labels": [], "entities": [{"text": "DISCUSS", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9816772937774658}]}, {"text": " Table 3: Inter-rater rank agreement (Kendall's-\u03c4 ). The  bottom row is the self-agreement for contexts they rated  in two separate trials.", "labels": [], "entities": [{"text": "Inter-rater rank agreement", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.6530131101608276}]}]}