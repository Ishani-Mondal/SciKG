{"title": [{"text": "Multi-modal Sensing and Analysis of Poster Conversations toward Smart Posterboard", "labels": [], "entities": []}], "abstractContent": [{"text": "Conversations in poster sessions in academic events, referred to as poster conversations, pose interesting and challenging topics on multi-modal analysis of multi-party dialogue.", "labels": [], "entities": []}, {"text": "This article gives an overview of our project on multi-modal sensing, analysis and \"under-standing\" of poster conversations.", "labels": [], "entities": []}, {"text": "We focus on the audience's feedback behaviors such as non-lexical backchannels (reactive tokens) and noddings as well as joint eye-gaze events by the presenter and the audience.", "labels": [], "entities": []}, {"text": "We investigate whether we can predict when and who will ask what kind of questions, and also interest level of the audience.", "labels": [], "entities": []}, {"text": "Based on these analyses , we design a smart posterboard which can sense human behaviors and annotate interactions and interest level during poster sessions.", "labels": [], "entities": []}], "introductionContent": [{"text": "As a variety of spoken dialogue systems have been developed and deployed in the real world, the frontier of spoken dialogue research, with engineering applications in scope, has been extended from the conventional human-machine speech interface.", "labels": [], "entities": []}, {"text": "One direction is a multi-modal interface, which includes not only graphics but also humanoid robots.", "labels": [], "entities": []}, {"text": "Another new direction is a multi-party dialogue system that can talk with multiple persons as an assistant agent (D.) or a companion robot (S..", "labels": [], "entities": []}, {"text": "While these are extensions of the human-machine speech interface, several projects have focused on humanhuman interactions such as meetings (S.) and free conversations (K.), toward ambient systems supervising the human communications.", "labels": [], "entities": []}, {"text": "We have been conducting a project which focuses on conversations in poster sessions, hereafter referred to as poster conversations.", "labels": [], "entities": []}, {"text": "Poster sessions have become a norm in many academic conventions and open laboratories because of the flexible and interactive characteristics.", "labels": [], "entities": []}, {"text": "Poster conversations have a mixture characteristics of lectures and meetings; typically a presenter explains his/her work to a small audience using a poster, and the audience gives feedback in real time by nodding and verbal backchannels, and occasionally makes questions and comments.", "labels": [], "entities": []}, {"text": "Conversations are interactive and also multimodal because people are standing and moving unlike in meetings.", "labels": [], "entities": []}, {"text": "Another good point of poster conversations is that we can easily make a setting for data collection, which is controlled in terms of familiarity with topics or other participants and yet is \"natural and real\".", "labels": [], "entities": [{"text": "data collection", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.7843175530433655}]}, {"text": "The goal of the project is signal-level sensing and high-level \"understanding\" of human interactions, including speaker diarization and annotation of comprehension and interest level of the audience.", "labels": [], "entities": [{"text": "signal-level sensing", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.690821498632431}]}, {"text": "These will realize anew indexing scheme of speech archives.", "labels": [], "entities": []}, {"text": "For example, after along session of poster presentation, we often want to get a short review of the question-answers and what looked difficult for audience to follow.", "labels": [], "entities": []}, {"text": "The research will also provide a model of intelligent conversational agents that can make autonomous presentation.", "labels": [], "entities": []}, {"text": "As opposed to the conventional content-based indexing approach which focuses on the presenter's speech by conducting speech recognition and natural language analysis, we adopt an interactionoriented approach which looks into the audience's reaction.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7200638800859451}]}, {"text": "Specifically we focus on non-linguistic information such as backchannel, nodding and eyegaze information, because we assume the audience better understands the key points of the presentation than the current machines.", "labels": [], "entities": []}, {"text": "An overview of the proposed scheme is depicted in.", "labels": [], "entities": []}, {"text": "Therefore, we setup an infrastructure for multimodal sensing and analysis of multi-party interactions.", "labels": [], "entities": [{"text": "multimodal sensing", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.758787602186203}]}, {"text": "Its process overview is shown in.", "labels": [], "entities": []}, {"text": "From the audio channel, we detect utterances as well as laughters and backchannels.", "labels": [], "entities": []}, {"text": "We also detect eye-gaze, nodding, and pointing information.", "labels": [], "entities": []}, {"text": "Special devices such as a motion-capturing system and eye-tracking recorders are used to make a \"goldstandard\" corpus, but only video cameras and distant microphones will be used in the practical system.", "labels": [], "entities": []}, {"text": "Our goal is then annotation of comprehension and interest level of the audience by combining these information sources.", "labels": [], "entities": []}, {"text": "This annotation will be useful in speech archives because people would be interested in listening to the points other people were interested in.", "labels": [], "entities": []}, {"text": "Since this is apparently difficult to be well-defined, however, we setup several milestones that can be formulated in objective manners and presumably related with the above-mentioned goal.", "labels": [], "entities": []}, {"text": "They are introduced in this article after description of the sensing environment and the collected corpus in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, annotation of interest level is addressed through detection of laughters and non-lexical kinds of backchannels, referred to as reactive tokens.", "labels": [], "entities": []}, {"text": "In Section 4 and 5, eye-gaze and nodding information is incorporated to predict when and who in the audience will ask questions, and also what kind of questions.", "labels": [], "entities": []}, {"text": "With these analyses, we expect that we can get clues to high-level \"understanding\" of the conversations, for example, whether the presentation is understood or liked by the audience.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this subsection, we define those segments which induced (or elicited) laughters or non-lexical reactive tokens as hot spots, 2 and investigate whether these hot spots are really funny or interesting to the third-party viewers of the poster session.", "labels": [], "entities": []}, {"text": "We had four subjects, who had not attended the presentation nor listened the recorded audio content.", "labels": [], "entities": []}, {"text": "They were asked to listen to each of the segmented hot spots in the original time sequence, and to make evaluations on the questionnaire, as below.", "labels": [], "entities": []}, {"text": "Q1: Do you understand the reason why the reactive token/laughter occurred?", "labels": [], "entities": []}, {"text": "Q2: Do you find this segment interesting/funny?", "labels": [], "entities": []}, {"text": "Q3: Do you think this segment is necessary or useful for listening to the content?", "labels": [], "entities": []}, {"text": "The percentage of \"yes\" on Question 1 was 89% for laughters and 95% for reactive tokens, confirming that a large majority of the hot spots are appropriate.", "labels": [], "entities": [{"text": "Question 1", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.8983878195285797}]}, {"text": "The answers to Questions 2 and 3 are more subjective, but suggest the usefulness of the hot spots.", "labels": [], "entities": []}, {"text": "It turned out that only a half of the spots associated with laughters are funny for the subjects (Q2), and they found 35% of the spots not funny.", "labels": [], "entities": []}, {"text": "The result suggests that feeling funny largely depends on the person.", "labels": [], "entities": []}, {"text": "And we should note that there are not many funny parts in poster sessions by nature.", "labels": [], "entities": []}, {"text": "On the other hand, more than 90% of the spots associated with reactive tokens are interesting (Q2), and useful or necessary (Q3) for the subjects.", "labels": [], "entities": []}, {"text": "The result supports the effectiveness of the hot spots extracted based on the reaction of the audience.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Duration (sec.) of eye-gaze and its relationship  with turn-taking", "labels": [], "entities": [{"text": "Duration", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9558148980140686}, {"text": "turn-taking", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.5766314268112183}]}, {"text": " Table 3: Definition of joint eye-gaze events by presenter  and audience", "labels": [], "entities": []}, {"text": " Table 4: Statistics of joint eye-gaze events by presenter  and audience in relation with turn-taking", "labels": [], "entities": [{"text": "turn-taking", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.5743107199668884}]}, {"text": " Table 5: Prediction result of speaker change", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9669139385223389}]}, {"text": " Table 5. Here, we compute  recall, precision and F-measure for speaker change,  or turn-taking by the audience. This case accounts  for only 11.9% and its prediction is very challeng- ing, while we can easily get an accuracy of over 90%  for prediction of turn-holding by the presenter. We  are particularly concerned on the recall of speaker  change, considering the nature of the task and appli- cation scenarios.", "labels": [], "entities": [{"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9987030029296875}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9995526671409607}, {"text": "F-measure", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9995537400245667}, {"text": "accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.9979617595672607}]}, {"text": " Table 6: Prediction result of the next speaker", "labels": [], "entities": [{"text": "Prediction result", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9653649926185608}]}, {"text": " Table 7: Frequencies (per sec.) of verbal backchannels  and their relationship with question type", "labels": [], "entities": []}, {"text": " Table 8: Frequencies (per sec.) of non-verbal noddings  and their relationship with question type", "labels": [], "entities": [{"text": "Frequencies", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.964015781879425}]}, {"text": " Table 9: Duration (ratio) of joint eye-gaze events and  their relationship with question type", "labels": [], "entities": [{"text": "Duration (ratio)", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8344311863183975}]}]}