{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 22-32, Automatic Grading of Scientific Inquiry", "labels": [], "entities": [{"text": "Automatic Grading of Scientific Inquiry", "start_pos": 98, "end_pos": 137, "type": "TASK", "confidence": 0.7971098303794861}]}], "abstractContent": [{"text": "The SAVE Science project is an attempt to address the shortcomings of current assessments of science.", "labels": [], "entities": [{"text": "SAVE Science project", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.6970476508140564}]}, {"text": "The project has developed two virtual worlds that each have a mystery or natural phenomenon requiring scientific explanation ; by recording students' behavior as they investigate the mystery, these worlds can be used to assess their understanding of the scientific method.", "labels": [], "entities": []}, {"text": "Currently, however, the scoring of the assessment depends either on manual grading of students' written responses, or, on multiple choice questions.", "labels": [], "entities": []}, {"text": "This paper presents an automated grader that can combine with SAVE Science's virtual worlds to provide a cheap mechanism for assessments of the ability to apply scientific methodology.", "labels": [], "entities": [{"text": "SAVE Science's virtual worlds", "start_pos": 62, "end_pos": 91, "type": "DATASET", "confidence": 0.835319185256958}]}, {"text": "In experiments on over 300 middle school students, our best automated grader improves by over 50% relative to the closest system from previous work in predicting grades supplied by human judges.", "labels": [], "entities": []}], "introductionContent": [{"text": "Education researchers criticize current standardized tests of science on many grounds.", "labels": [], "entities": []}, {"text": "First, they lack context (, which complicates a student's task of applying classroom-based learning, as the theory of situated cognition suggests (.", "labels": [], "entities": []}, {"text": "Second, many have criticized such tests for failing to engage students long enough to apply their understanding to the question.", "labels": [], "entities": []}, {"text": "Furthermore and perhaps worst of all, standardized tests fail to assess scientific inquiry-the ability of students to apply the scientific method-authentically rather than as scientific content.", "labels": [], "entities": []}, {"text": "We consider an assessment conducted by the Situated Assessment using Virtual Environments for Science Content and Inquiry (SAVE Science) project (, whose long-term goal is to address the shortcomings of current standardized tests of science.", "labels": [], "entities": []}, {"text": "The assessments from SAVE Science have produced an abundance of data on how students interact with a virtual world, when trying to conduct scientific inquiry.", "labels": [], "entities": []}, {"text": "Observing student behavior in virtual environments offers the potential for new insights into both how students learn and what they know.", "labels": [], "entities": []}, {"text": "However, this benefit can only be realized if we can make sense of the stream of data and text produced by the students.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to automate the process of grading students in SAVE Science assessments, to make the evaluations as cost-effective as standardized tests.", "labels": [], "entities": []}, {"text": "Unlike most previous systems for automated grading, the data for this task includes a short paragraph (usually 50-60 words) natural language response stating a hypothesis and evidence in support of it.", "labels": [], "entities": []}, {"text": "In addition, there is a wealth of relational data about student behavior in a virtual environment.", "labels": [], "entities": []}, {"text": "We develop novel predictors for automatically grading the written responses using a wide variety of natural language features, as well as features from the data on student behavior in the virtual world.", "labels": [], "entities": []}, {"text": "On student data from two virtual worlds, our best automated grader has correlations of r = 0.58 and 0.44 with human judgments, improving over the closest technique from previous work by 56% for the first world, and by 120% for the second.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section contrasts this project with previous work.", "labels": [], "entities": []}, {"text": "Section 3 describes the SAVE Science project and the student data it has produced.", "labels": [], "entities": [{"text": "SAVE Science project", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.6722613374392191}]}, {"text": "Section 4 details our automated grading models.", "labels": [], "entities": []}, {"text": "Section 5 reports on experiments, and Section 6 concludes.", "labels": [], "entities": []}, {"text": "have previously conducted a study on assessing creative problem-solving in science education by automatically grading student essays.", "labels": [], "entities": []}, {"text": "Our techniques improve substantially over theirs, as we demonstrate empirically.", "labels": [], "entities": []}, {"text": "In part, we improve by including more sophisticated languageprocessing features in our model than the unigram and bigram features they use; as others have noted, bag-of-words representations and latent semantic indexing become less useful as word order and causal relationships become important for judging an essay's quality (.", "labels": [], "entities": []}, {"text": "A secondary reason for our improvement is that we also have access to nonlinguistic data about the students that we can mine for additional patterns.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use a dataset collected by the SAVE Science project, consisting of the world data, freeform responses, and quiz answers from public middleschool students in a major urban area of the United States.", "labels": [], "entities": []}, {"text": "120 students completed the Weather Trouble module, and 184 students completed Basketball.", "labels": [], "entities": [{"text": "Weather Trouble module", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.6904597381750742}, {"text": "Basketball", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9805688858032227}]}, {"text": "After manually correcting spelling errors in the freeform responses, we extracted features as described above.", "labels": [], "entities": []}, {"text": "Following, we evaluate our regression models using Pearson correlation between the predicted outcome and the gold standard outcome.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 51, "end_pos": 70, "type": "METRIC", "confidence": 0.9688635766506195}]}, {"text": "Four different gold standards are considered for each module: manually-assigned grades for the freeform text, and three versions of the number of correctly-answered quiz questions (contextualized only, non-contextualized only, and all).", "labels": [], "entities": []}, {"text": "We use a \u03c7 2 test with a threshold of p < 0.05 to determine statistical significance.", "labels": [], "entities": []}, {"text": "We train and test models using 10-fold cross-validation to reduce variability, and the results are averaged over the folds.", "labels": [], "entities": []}, {"text": "We evaluate several variants of our system, including a World variant that only includes features from the world data; an NLP variant that only includes features from the freeform responses; and a combined World+NLP variant that includes all features before feature selection is performed.", "labels": [], "entities": []}, {"text": "Our evaluation compares against the essay grading technique by Wang et al.", "labels": [], "entities": []}, {"text": "Like ours, their system uses RBF-SVM regression with default parameter settings as implemented in Weka, and like ours the system is trained on student texts proposing solutions to a science problem (in their case, a high school chemistry problem).", "labels": [], "entities": [{"text": "Weka", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.9685202836990356}]}, {"text": "The system is trained on human judgments of the quality of the student answers.", "labels": [], "entities": []}, {"text": "The major difference between our technique and theirs lies in the representation of the data; Wang et al. use two types of features: unigrams, and bigrams that occur at least five times during training.", "labels": [], "entities": []}, {"text": "In our implementation of their technique, we use a lower threshold for bigrams -they must occur at least twice.", "labels": [], "entities": []}, {"text": "This is because we have less text to work with, and the higher threshold yields too few bigrams.", "labels": [], "entities": []}, {"text": "Using the lower threshold improved performance slightly, so we report only those results below.", "labels": [], "entities": []}], "tableCaptions": []}