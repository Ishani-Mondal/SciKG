{"title": [{"text": "Illinois-Coref: The UI System in the CoNLL-2012 Shared Task", "labels": [], "entities": [{"text": "Illinois-Coref", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9258295297622681}, {"text": "CoNLL-2012 Shared Task", "start_pos": 37, "end_pos": 59, "type": "DATASET", "confidence": 0.8935776352882385}]}], "abstractContent": [{"text": "The CoNLL-2012 shared task is an extension of the last year's coreference task.", "labels": [], "entities": [{"text": "coreference task", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.8825080394744873}]}, {"text": "We participated in the closed track of the shared tasks in both years.", "labels": [], "entities": []}, {"text": "In this paper, we present the improvements of Illinois-Coref system from last year.", "labels": [], "entities": [{"text": "Illinois-Coref system", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.9656785428524017}]}, {"text": "We focus on improving mention detection and pronoun coreference resolution, and present anew learning protocol.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7779450416564941}, {"text": "pronoun coreference resolution", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.7882100840409597}]}, {"text": "These new strategies boost the performance of the system by 5% MUC F1, 0.8% BCUB F1, and 1.7% CEAF F1 on the OntoNotes-5.0 development set.", "labels": [], "entities": [{"text": "MUC F1", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.5957379043102264}, {"text": "BCUB F1", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.8843273520469666}, {"text": "CEAF", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.7803508043289185}, {"text": "F1", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.7490671277046204}, {"text": "OntoNotes-5.0 development set", "start_pos": 109, "end_pos": 138, "type": "DATASET", "confidence": 0.9371971289316813}]}], "introductionContent": [{"text": "Coreference resolution has been a popular topic of study in recent years.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9289946556091309}]}, {"text": "In the task, a system requires to identify denotative phrases (\"mentions\") and to cluster the mentions into equivalence classes, so that the mentions in the same class refer to the same entity in the real world.", "labels": [], "entities": []}, {"text": "Coreference resolution is a central task in the Natural Language Processing research.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9279417097568512}]}, {"text": "Both the and) shared tasks focus on resolving coreference on the OntoNotes corpus.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.8764072954654694}]}, {"text": "We also participated in the CoNLL-2011 shared task.", "labels": [], "entities": [{"text": "CoNLL-2011 shared task", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.7093793352444967}]}, {"text": "Our system) ranked first in two out of four scoring metrics (BCUB and BLANC), and ranked third in the average score.", "labels": [], "entities": [{"text": "BCUB", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.6112858653068542}, {"text": "BLANC", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.995180606842041}, {"text": "average score", "start_pos": 102, "end_pos": 115, "type": "METRIC", "confidence": 0.9799022376537323}]}, {"text": "This year, we further improve the system in several respects.", "labels": [], "entities": []}, {"text": "2, we describe the Illinois-Coref system for the CoNLL-2011 shared task, which we take as the baseline.", "labels": [], "entities": []}, {"text": "Then, we discuss the improvements on mention detection (Sec. 3.1), pronoun resolution (Sec. 3.2), and learning algorithm (Sec. 3.3).", "labels": [], "entities": [{"text": "mention detection", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7613195478916168}, {"text": "pronoun resolution", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.6798608750104904}]}, {"text": "Section 4 shows experimental results and Section 5 offers a brief discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we demonstrate the performance of Illinois-Coref on the OntoNotes-5.0 data set.", "labels": [], "entities": [{"text": "Illinois-Coref", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9296857118606567}, {"text": "OntoNotes-5.0 data set", "start_pos": 73, "end_pos": 95, "type": "DATASET", "confidence": 0.9609946608543396}]}, {"text": "A previous experiment using an earlier version of this data can be found in (.", "labels": [], "entities": []}, {"text": "We first show the improvement of the mention detection system.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8704962134361267}]}, {"text": "Then, we compare different learning protocols for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.9619077742099762}]}, {"text": "Finally, we show the overall performance improvement of Illinois-Coref system.", "labels": [], "entities": [{"text": "Illinois-Coref", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.919171154499054}]}, {"text": "First, we analyze the performance of mention detection before the coreference stage.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7923682332038879}]}, {"text": "Note that singleton mentions are included since it is not possible to identify singleton mentions before running coreference.", "labels": [], "entities": []}, {"text": "They are removed in the post-processing stage.", "labels": [], "entities": []}, {"text": "The mention detection performance of the end-to-end system will be discussed later in this section.", "labels": [], "entities": []}, {"text": "With the strategy described in Section 3.1, we improve the F1 score for mention detection from 55.92% to 57.89%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9834056794643402}, {"text": "mention detection", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.6557200253009796}]}, {"text": "Moreover, we improve the detection performance on short named entity mentions (name entity with less than 5 words) from 61.36 to 64.00 in F1 scores.", "labels": [], "entities": [{"text": "F1", "start_pos": 138, "end_pos": 140, "type": "METRIC", "confidence": 0.9966832995414734}]}, {"text": "Such mentions are more important because they are easier to resolve in the coreference layer.", "labels": [], "entities": []}, {"text": "Regarding the learning algorithm, shows the performance of the two learning protocols with/without separating pronoun anaphora resolver.", "labels": [], "entities": []}, {"text": "The results show that both strategies of using a pronoun classifier and training a latent structured model with a online algorithm improve the system performance.", "labels": [], "entities": []}, {"text": "Combining the two strategies, the avg F1 score is improved by 2.45%.", "labels": [], "entities": [{"text": "avg F1 score", "start_pos": 34, "end_pos": 46, "type": "METRIC", "confidence": 0.8927746812502543}]}, {"text": "Finally, we compare the final system with the baseline system.", "labels": [], "entities": []}, {"text": "We evaluate both systems on the CoNLL-11 DEV data set, as the baseline system is tuned on it.", "labels": [], "entities": [{"text": "CoNLL-11 DEV data set", "start_pos": 32, "end_pos": 53, "type": "DATASET", "confidence": 0.9712086319923401}]}, {"text": "The results show that Illinois-Coref achieves better scores on all the metrics.", "labels": [], "entities": [{"text": "Illinois-Coref", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.8914034366607666}]}, {"text": "The mention detection performance after coreference resolution is also significantly improved.: The results of our submitted system on the TEST set.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.6502800136804581}, {"text": "coreference resolution", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.9270434975624084}, {"text": "TEST set", "start_pos": 139, "end_pos": 147, "type": "DATASET", "confidence": 0.9020419716835022}]}, {"text": "The systems are trained on a collection of TRAIN and DEV sets.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9141905307769775}]}], "tableCaptions": [{"text": " Table 1: The performance of different learning strategies for best-link decoding algorithm. We show the results  with/without using separate pronoun anaphora resolver. The systems are trained on the TRAIN set and evaluated on  the CoNLL-2012 DEV set. We report the F1 scores (%) on mention detection (MD) and coreference metrics (MUC,  BCUB, CEAF). The column AVG shows the averaged scores of the three coreference metrics.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 200, "end_pos": 205, "type": "METRIC", "confidence": 0.9491912722587585}, {"text": "CoNLL-2012 DEV set", "start_pos": 232, "end_pos": 250, "type": "DATASET", "confidence": 0.9670765201250712}, {"text": "F1", "start_pos": 266, "end_pos": 268, "type": "METRIC", "confidence": 0.9987003803253174}, {"text": "BCUB", "start_pos": 337, "end_pos": 341, "type": "METRIC", "confidence": 0.6417168378829956}, {"text": "AVG", "start_pos": 361, "end_pos": 364, "type": "METRIC", "confidence": 0.9938249588012695}]}, {"text": " Table 2: The improvement of Illinois-Coref. We report  the F1 scores (%) on the DEV set from CoNLL-2011  shared task. Note that the CoNLL-2011 data set does not  include corpora of bible and of telephone conversation.", "labels": [], "entities": [{"text": "Illinois-Coref", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.9474257230758667}, {"text": "F1", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9994845390319824}, {"text": "DEV set", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.8500508666038513}, {"text": "CoNLL-2011  shared task", "start_pos": 94, "end_pos": 117, "type": "DATASET", "confidence": 0.7816770871480306}, {"text": "CoNLL-2011 data set", "start_pos": 133, "end_pos": 152, "type": "DATASET", "confidence": 0.9737372001012167}]}, {"text": " Table 3: The results of our submitted system on the TEST set. The systems are trained on a collection of TRAIN and  DEV sets.", "labels": [], "entities": [{"text": "TEST set", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.8252837657928467}, {"text": "TRAIN", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.7454636096954346}]}]}