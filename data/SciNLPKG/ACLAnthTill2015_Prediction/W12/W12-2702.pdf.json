{"title": [{"text": "Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.7891972462336222}]}], "abstractContent": [{"text": "Language models play an important role in large vocabulary speech recognition and statistical machine translation systems.", "labels": [], "entities": [{"text": "large vocabulary speech recognition", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.6076991483569145}, {"text": "statistical machine translation", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.6698963244756063}]}, {"text": "The dominant approach since several decades are back-off language models.", "labels": [], "entities": []}, {"text": "Some years ago, there was a clear tendency to build huge language models trained on hundreds of billions of words.", "labels": [], "entities": []}, {"text": "Lately, this tendency has changed and recent works concentrate on data selection.", "labels": [], "entities": [{"text": "data selection", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.7820661664009094}]}, {"text": "Continuous space methods area very competitive approach, but they have a high computational complexity and are not yet in widespread use.", "labels": [], "entities": []}, {"text": "This paper presents an experimental comparison of all these approaches on a large statistical machine translation task.", "labels": [], "entities": [{"text": "statistical machine translation task", "start_pos": 82, "end_pos": 118, "type": "TASK", "confidence": 0.6955531015992165}]}, {"text": "We also describe an open-source implementation to train and use continuous space language models (CSLM) for such large tasks.", "labels": [], "entities": []}, {"text": "We describe an efficient implementation of the CSLM using graphical processing units from Nvidia.", "labels": [], "entities": [{"text": "Nvidia", "start_pos": 90, "end_pos": 96, "type": "DATASET", "confidence": 0.9411588311195374}]}, {"text": "By these means, we are able to train an CSLM on more than 500 million words in 20 hours.", "labels": [], "entities": []}, {"text": "This CSLM provides an improvement of up to 1.8 BLEU points with respect to the best back-off language model that we were able to build.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9992768168449402}]}], "introductionContent": [{"text": "Language models are used to estimate the probability of a sequence of words.", "labels": [], "entities": []}, {"text": "They are an important module in many areas of natural language processing, in particular large vocabulary speech recognition (LVCSR) and statistical machine translation (SMT).", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.6558038194974264}, {"text": "large vocabulary speech recognition", "start_pos": 89, "end_pos": 124, "type": "TASK", "confidence": 0.6528197079896927}, {"text": "statistical machine translation (SMT)", "start_pos": 137, "end_pos": 174, "type": "TASK", "confidence": 0.8198657731215159}]}, {"text": "The goal of LVCSR is to convert a speech signal x into a sequence of words w.", "labels": [], "entities": []}, {"text": "This is usually approached with the following fundamental equation: In SMT, we are faced with a sequence of words e in the source language and we are looking for its best translation f into the target language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9931324124336243}]}, {"text": "Again, we apply Bayes rule to introduce a language model: Although we use a language model to evaluate the probability of the produced sequence of words, wand f respectively, we argue that the task of the language model is not exactly the same for both applications.", "labels": [], "entities": []}, {"text": "In LVCSR, the LM must choose among a large number of possible segmentations of the phoneme sequence into words, given the pronunciation lexicon.", "labels": [], "entities": []}, {"text": "It is also the only component that helps to select among homonyms, i.e. words that are pronounced in the same way, but that are written differently and which have usually different meanings (e.g. ate/eight or build/billed).", "labels": [], "entities": []}, {"text": "In SMT, on the other hand, the LM has the responsibility to chose the best translation of a source word given the context.", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9904333353042603}]}, {"text": "More importantly, the LM is a key component which has to sort out good and bad word reorderings.", "labels": [], "entities": [{"text": "sort out good and bad word reorderings", "start_pos": 57, "end_pos": 95, "type": "TASK", "confidence": 0.659833299262183}]}, {"text": "This is known to be a very difficult issue when translating from or into languages like Chinese, Japanese or German.", "labels": [], "entities": []}, {"text": "In LVCSR, the word order is given by the time-synchronous processing of the speech signal.", "labels": [], "entities": []}, {"text": "Finally, the LM helps to deal with gender, number, etc accordance of morphologically rich languages, when used in an LVCSR as well as an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.9826776385307312}]}, {"text": "Overall, one can say that the semantic level seems to be more important for language modeling in SMT than LVCSR.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.731594055891037}, {"text": "SMT", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.9677936434745789}]}, {"text": "In both applications, so called back-off n-gram language models are the de facto standard since several decades.", "labels": [], "entities": []}, {"text": "They were first introduced in the eighties, followed by intensive research on smoothing methods.", "labels": [], "entities": [{"text": "smoothing", "start_pos": 78, "end_pos": 87, "type": "TASK", "confidence": 0.9803277850151062}]}, {"text": "An extensive comparison can be found in).", "labels": [], "entities": []}, {"text": "ModifiedKneser Ney smoothing seems to be the best performing method and it is this approach that is almost exclusively used today.", "labels": [], "entities": [{"text": "ModifiedKneser Ney smoothing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5043936272462209}]}, {"text": "Some years ago, there was a clear tendency in SMT to use huge LMs trained on hundreds on billions (10 1 1) of words (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9917904138565063}]}, {"text": "The authors report continuous improvement of the translation quality with increasing size of the LM training data, but these models require a large cluster to train and to perform inference using distributed storage.", "labels": [], "entities": [{"text": "translation", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.951318621635437}]}, {"text": "Therefore, several approaches were proposed to limit the storage size of large LMs, for instance).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we present comparative results for various LMs when integrated into a large-scale SMT system to translate from Arabic into English.", "labels": [], "entities": [{"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9860548377037048}]}, {"text": "We use the popular Moses toolkit to build the SMT system (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9887519478797913}]}, {"text": "As in our previous works, the CSLM is used to rescore 1000-best lists.", "labels": [], "entities": []}, {"text": "The sentence probability calculated by the CSLM is added: Perplexities on the development data (news wire genre) of the individual sub-corpora in the LDC Gigaword corpus, before and after data selection by the method of (Moore and as 15th feature function and the coefficients of all the feature functions are optimized by MERT.", "labels": [], "entities": [{"text": "LDC Gigaword corpus", "start_pos": 150, "end_pos": 169, "type": "DATASET", "confidence": 0.8912739555040995}, {"text": "MERT", "start_pos": 323, "end_pos": 327, "type": "METRIC", "confidence": 0.9183445572853088}]}, {"text": "The CSLM toolkit includes scripts to perform this task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexities on the development data (news wire genre) of the individual sub-corpora in the LDC Gigaword  corpus, before and after data selection by the method of (Moore and", "labels": [], "entities": [{"text": "LDC Gigaword  corpus", "start_pos": 102, "end_pos": 122, "type": "DATASET", "confidence": 0.8836082816123962}]}, {"text": " Table 2: Comparison of several 4-gram back-off lan- guage models. See text for explanation of the models.", "labels": [], "entities": []}, {"text": " Table 3: Perplexity on the development data (news genre)  for back-off and continuous space language models.", "labels": [], "entities": []}, {"text": " Table 4: BLEU scores on the test set for the translation  from Arabic into English for various language models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993786811828613}]}]}