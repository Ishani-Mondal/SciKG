{"title": [{"text": "Learning a Vector-Based Model of American Sign Language Inflecting Verbs from Motion-Capture Data", "labels": [], "entities": []}], "abstractContent": [{"text": "American Sign Language (ASL) synthesis software can improve the accessibility of information and services for deaf individuals with low English literacy.", "labels": [], "entities": [{"text": "American Sign Language (ASL) synthesis", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7760263085365295}]}, {"text": "The synthesis component of current ASL animation generation and scripting systems have limited handling of the many ASL verb signs whose movement path is inflected to indicate 3D locations in the signing space associated with discourse refer-ents.", "labels": [], "entities": [{"text": "ASL animation generation", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.953049103418986}]}, {"text": "Using motion-capture data recorded from human signers, we model how the motion paths of verb signs vary based on the location of their subject and object.", "labels": [], "entities": []}, {"text": "This model yields a lexicon for ASL verb signs that is pa-rameterized on the 3D locations of the verb's arguments; such a lexicon enables more realistic and understandable ASL animations.", "labels": [], "entities": [{"text": "ASL verb signs", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.9124991496404012}, {"text": "ASL animations", "start_pos": 172, "end_pos": 186, "type": "TASK", "confidence": 0.8922030627727509}]}, {"text": "A new model presented in this paper, based on identifying the principal movement vector of the hands, shows improvement in modeling ASL verb signs, including when trained on movement data from a different human signer.", "labels": [], "entities": [{"text": "ASL verb signs", "start_pos": 132, "end_pos": 146, "type": "TASK", "confidence": 0.9020276069641113}]}], "introductionContent": [{"text": "American Sign Language (ASL) is a primary means of communication for over 500,000 people in the U.S. ().", "labels": [], "entities": [{"text": "American Sign Language (ASL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.77599036693573}]}, {"text": "As a natural language that is not merely an encoding of English, ASL has a distinct syntax, word order, and lexicon.", "labels": [], "entities": []}, {"text": "Someone can be fluent in ASL yet have significant difficulty reading English; in fact, due to various educational factors, the majority of deaf high school graduates (age 18+) in the U.S. have a fourth-grade (age 10) English reading level or lower.", "labels": [], "entities": []}, {"text": "This leads to accessibility challenges for deaf adults when faced with English text on computers, video captions, or other sources.", "labels": [], "entities": []}, {"text": "Technologies for automatically generating computer animations of ASL can make information and services accessible to deaf people with lower English literacy.", "labels": [], "entities": [{"text": "automatically generating computer animations of ASL", "start_pos": 17, "end_pos": 68, "type": "TASK", "confidence": 0.5818940252065659}]}, {"text": "While videos of sign language are feasible to produce in some contexts, animated avatars are more advantageous than video when the information content is often modified, the content is generated or translated automatically, or signers scripting a message in ASL wish to preserve anonymity.", "labels": [], "entities": []}, {"text": "This paper focuses on ASL and producing accessible sign language animations for people who are deaf in the U.S., but many of the linguistic issues, literacy rates, and animation technologies discussed within are also applicable to other sign languages used internationally.", "labels": [], "entities": [{"text": "ASL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.986189067363739}]}], "datasetContent": [{"text": "Because the premise of this paper is that models of ASL verbs based on a motion vector representation would do a better job of capturing the essential aspects of a verb's motion path across signers, we conducted an inter-signer cross-validation of our new model.", "labels": [], "entities": []}, {"text": "We built separate models on the data from each of our three signers, and then we compared the resulting model's predictions for all 42 verb instances collected from the other two signers.", "labels": [], "entities": []}, {"text": "For comparison purposes, we also trained three models (one per signer) using the \"point\"-based model from our prior work ().", "labels": [], "entities": []}, {"text": "presents the results; the values of each bar are the average \"error\" for each synthesized verb example for all five ASL verbs in.", "labels": [], "entities": []}, {"text": "The error score fora verb example is the average of four values: (1) Euclidean distance between the start location of the right hand as predicted by the model and the start location of the right hand of the human signer data being used for evaluation, (2) same for the end location for the right hand, (3) same for the start location for the left hand, and (4) same for end location for the left hand.", "labels": [], "entities": [{"text": "error score", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9567726850509644}, {"text": "Euclidean distance", "start_pos": 69, "end_pos": 87, "type": "METRIC", "confidence": 0.9738357067108154}]}, {"text": "shows that the new \"vector\" model has lower error scores than our older \"point\" model presented in prior work.", "labels": [], "entities": [{"text": "error scores", "start_pos": 44, "end_pos": 56, "type": "METRIC", "confidence": 0.9671213924884796}]}, {"text": "To interpret the Euclidean distance value, it is useful to know that the scale of the coordinate space used for the verb model is set such that shoulder width of a signer would be 1.0.", "labels": [], "entities": [{"text": "shoulder width", "start_pos": 144, "end_pos": 158, "type": "METRIC", "confidence": 0.8889168202877045}]}, {"text": "As a baseline for comparison, the average intersigner variation (based on the values shown in) is also plotted in..", "labels": [], "entities": [{"text": "intersigner variation", "start_pos": 42, "end_pos": 63, "type": "METRIC", "confidence": 0.9236701130867004}]}, {"text": "Evaluation of the \"Point\" and \"Vector\" models for all five ASL verbs listed in.", "labels": [], "entities": []}, {"text": "Next, we wanted to compare the two models under two assumptions: (1) it may not be possible to gather a large number of examples of a verb from a single signer and (2) it maybe necessary to mix data from multiple signers when assembling a training data set fora verb model.", "labels": [], "entities": []}, {"text": "For instance, these conditions would hold if a researcher were using examples of a verb performance extracted from a multi-signer corpus to assemble a training set.", "labels": [], "entities": []}, {"text": "Due to the limited size of most sign language corpora (and the many possible combinations of subject and object position in the signing space), a training set gathered in this manner would likely contain a relatively small number of training examples -possibly gathered from multiple signers.", "labels": [], "entities": []}, {"text": "To test the models under these conditions, we assembled three training data sets -using the data from our three recorded signers.", "labels": [], "entities": []}, {"text": "Each data set included 22 examples of the performance of an ASL inflected verb fora subset of the various possible combinations of subject and object locations in the signing space -with half of the examples from one signer and half from another.", "labels": [], "entities": []}, {"text": "After training a model on each data set, then the model was evaluated against the 42 examples of each verb performance recorded from the third signer (who was not part of the training data used for that model).", "labels": [], "entities": []}, {"text": "This process was repeated fora total of three times (for all combinations of the data from the three sign-ers).", "labels": [], "entities": []}, {"text": "For comparison purposes, we also trained three models (one based on each of the three twosigner data sets) using the \"point\"-based model from our prior work (.", "labels": [], "entities": []}, {"text": "shows the results for two of the verbs in (ASK and GIVE); the \"vector\" model has lower error scores than our older \"point\" model..", "labels": [], "entities": [{"text": "GIVE", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9546176195144653}]}, {"text": "Evaluation of the \"Point\" and \"Vector\" models trained on a small \"mixed\" data set from two signers.", "labels": [], "entities": []}, {"text": "Examples of animations of the ASL verbs synthesized using each of these models are on our lab website: http://latlab.cs.qc.cuny.edu/slpat2012/", "labels": [], "entities": []}], "tableCaptions": []}