{"title": [{"text": "Data Issues of the Multilingual Translation Matrix", "labels": [], "entities": [{"text": "Multilingual Translation Matrix", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.8030886848767599}]}], "abstractContent": [{"text": "We describe our experiments with phrase-based machine translation for the WMT 2012 Shared Task.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.6635647018750509}, {"text": "WMT 2012 Shared Task", "start_pos": 74, "end_pos": 94, "type": "DATASET", "confidence": 0.7201138585805893}]}, {"text": "We trained one system for 14 translation directions between English or Czech on one side and English, Czech, German, Spanish or French on the other side.", "labels": [], "entities": []}, {"text": "We describe a set of results with different training data sizes and subsets .", "labels": [], "entities": []}], "introductionContent": [{"text": "With so many official languages, Europe is a paradise for machine translation research.", "labels": [], "entities": [{"text": "machine translation research", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.8873075842857361}]}, {"text": "One of the largest bodies of electronically available parallel texts is being nowadays generated by the European Union and its institutions.", "labels": [], "entities": []}, {"text": "At the same time, the EU also provides motivation and boosts potential market for machine translation outcomes.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.8439559042453766}]}, {"text": "Most of the major European languages belong to one of three branches of the Indo-European language family: Germanic, Romance or Slavic.", "labels": [], "entities": []}, {"text": "Such relatedness is responsible for many structural similarities in European languages, although significant differences still exist.", "labels": [], "entities": []}, {"text": "Within the language portfolio selected for the WMT shared task, English, French and Spanish seem to be closer to each other than to the rest.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.8591707348823547}]}, {"text": "German, despite being genetically related to English, differs in many properties.", "labels": [], "entities": []}, {"text": "Its word order rules, shifting verbs from one end of the sentence to the other, easily create long-distance dependencies.", "labels": [], "entities": []}, {"text": "Long German compound words are notorious for increasing out-of-vocabulary rate, which has led many researchers to devising unsupervised compound-splitting techniques.", "labels": [], "entities": []}, {"text": "Also, uppercase/lowercase distinction is more important because all German nouns start with an uppercase letter by the rule.", "labels": [], "entities": []}, {"text": "Czech is a language with rich morphology (both inflectional and derivational) and relatively free word order.", "labels": [], "entities": []}, {"text": "In fact, the predicateargument structure, often encoded by fixed word order in English, is usually captured by inflection (especially the system of 7 grammatical cases) in Czech.", "labels": [], "entities": []}, {"text": "While the free word order of Czech is a problem when translating to English (the text should be parsed first in order to determine the syntactic functions and the English word order), generating correct inflectional affixes is indeed a challenge for English-to-Czech systems.", "labels": [], "entities": []}, {"text": "Furthermore, the multitude of possible Czech word forms (at least order of magnitude higher than in English) makes the data sparseness problem really severe, hindering both directions.", "labels": [], "entities": []}, {"text": "Our goal is to run one system under as similar conditions as possible to all fourteen translation directions, to compare their translation accuracies and see why some directions are easier than others.", "labels": [], "entities": []}, {"text": "Future work will benefit from knowing what are the special processing needs fora given language pair.", "labels": [], "entities": []}, {"text": "The current version of the system does not include really language-specific techniques: we neither split German compounds, nor do we address the peculiarities of Czech mentioned above.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following section we describe several different settings and corpora combinations we experimented with.", "labels": [], "entities": []}, {"text": "BLEU scores have been computed by our system, comparing truecased tokenized hypothesis with truecased tokenized reference translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9893929958343506}]}, {"text": "Such scores must differ from the official evaluation-see Section 4.4 for discussion of the final results.", "labels": [], "entities": []}, {"text": "The confidence interval for most of the scores lies between \u00b10.5 and \u00b10.6 BLEU % points.", "labels": [], "entities": [{"text": "confidence interval", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9844222664833069}, {"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9841070175170898}]}, {"text": "The set of baseline experiments were trained on the supervised truecased combination of News Commentary and Europarl.", "labels": [], "entities": [{"text": "News Commentary", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.9233138561248779}, {"text": "Europarl", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.9390823841094971}]}, {"text": "As we had lemmatizers for the languages, word alignment was computed on lemmas.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.8011291027069092}]}, {"text": "(But our previous experiments showed that there was little difference between using lemmas and lowercased 4-character \"stems\".)", "labels": [], "entities": []}, {"text": "A hexagram language model was trained on the monolingual version of the News Commentary + Europarl corpus (typically a slightly larger superset of the target side of the parallel corpus).", "labels": [], "entities": [{"text": "News Commentary + Europarl corpus", "start_pos": 72, "end_pos": 105, "type": "DATASET", "confidence": 0.8720512628555298}]}], "tableCaptions": [{"text": " Table 1: Number of sentence pairs and tokens for  every language pair in the parallel training corpus.  Languages are identified by their ISO 639 codes: cs  = Czech, de = German, en = English, es = Spanish,  fr = French. Every line corresponds to the respective  version of EuroParl + News Commentary.", "labels": [], "entities": [{"text": "EuroParl + News Commentary", "start_pos": 275, "end_pos": 301, "type": "DATASET", "confidence": 0.9700782746076584}]}, {"text": " Table 2: Number of segments (paragraphs in Giga- word, sentences elsewhere) and tokens of additional  monolingual training corpora. \"newsc+euro\" are the  monolingual versions of the News Commentary and  Europarl parallel corpora. \"news.all\" denotes all  years of the Crawled News corpus for the given lan- guage.", "labels": [], "entities": [{"text": "Europarl parallel corpora", "start_pos": 204, "end_pos": 229, "type": "DATASET", "confidence": 0.8877006570498148}, {"text": "Crawled News corpus", "start_pos": 268, "end_pos": 287, "type": "DATASET", "confidence": 0.7919877568880717}]}, {"text": " Table 3: BLEU scores of the baseline experiments  (left column) on News Test 2012 data, computed by  the system on tokenized data, versus similar setup  with large monolingual corpus (news.all, middle col- umn). Gigaword never brought significant improve- ment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994024038314819}, {"text": "News Test 2012 data", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.9787683039903641}, {"text": "improve- ment", "start_pos": 248, "end_pos": 261, "type": "METRIC", "confidence": 0.9497895240783691}]}, {"text": " Table 5: BLEU scores with the large language mod- els. BLEU is computed by the system, BLEU l is the  official lowercased evaluation by matrix.statmt.  org. BLEU t is official truecased evaluation. Al- though lower official scores are expected, notice the  larger gap in en-fr and cs-fr translation. There seems  to be a problem in our French detokenization proce- dure.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9976727366447449}, {"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9987866282463074}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9967237114906311}, {"text": "BLEU", "start_pos": 158, "end_pos": 162, "type": "METRIC", "confidence": 0.9950143694877625}]}]}