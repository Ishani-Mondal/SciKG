{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 105-115, On using context for automatic correction of non-word misspellings in student essays", "labels": [], "entities": [{"text": "automatic correction of non-word misspellings in student essays", "start_pos": 121, "end_pos": 184, "type": "TASK", "confidence": 0.8122714906930923}]}], "abstractContent": [{"text": "In this paper we present anew spell-checking system that utilizes contextual information for automatic correction of non-word misspel-lings.", "labels": [], "entities": []}, {"text": "The system is evaluated with a large corpus of essays written by native and non-native speakers of English to the writing prompts of high-stakes standardized tests (TOEFL \u00ae and GRE \u00ae).", "labels": [], "entities": [{"text": "TOEFL \u00ae", "start_pos": 165, "end_pos": 172, "type": "METRIC", "confidence": 0.9553211331367493}, {"text": "GRE \u00ae)", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9662760496139526}]}, {"text": "We also present comparative evaluations with Aspell and the spel-ler from Microsoft Office 2007.", "labels": [], "entities": [{"text": "Microsoft Office 2007", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.8190902670224508}]}, {"text": "Using context-informed re-ranking of candidate suggestions , our system exhibits superior error-correction results overall and also corrects errors generated by non-native English writers with almost same rate of success as it does for writers who are native English speakers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Misspellings are ubiquitous in student writing.", "labels": [], "entities": []}, {"text": "have found that spelling errors accounted for about one quarter of all errors found in a random sample of 300 student essays.", "labels": [], "entities": [{"text": "spelling errors", "start_pos": 16, "end_pos": 31, "type": "METRIC", "confidence": 0.8497493267059326}]}, {"text": "found that spelling errors are among the five most frequent errors in first-year college composition of US students.", "labels": [], "entities": []}, {"text": "found that spelling errors constituted about 6.5% of all errors found in a US national sample of 3000 college composition essays, despite the fact that writers had access to spellcheckers.", "labels": [], "entities": []}, {"text": "Misspellings are even more ubiquitous in texts written by non-native speakers of English, especially English Language Learners (ELL).", "labels": [], "entities": []}, {"text": "The types of misspellings produced by L2 writers are typically different from errors produced by native speakers).", "labels": [], "entities": []}, {"text": "In the area of automatic assessment of writing, detection of misspellings is utilized in computeraided language learning applications and in some automatic scoring systems, especially when feedback to users is involved).", "labels": [], "entities": [{"text": "automatic assessment of writing", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.7277536243200302}]}, {"text": "Yet spelling errors may have a deeper influence on automated text assessment.", "labels": [], "entities": [{"text": "automated text assessment", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.6280857821305593}]}, {"text": "As noted by, sub-optimal automatic detection of grammar and mechanics errors maybe attributed to poor performance of NLP tools over noisy text.", "labels": [], "entities": []}, {"text": "Presence of spelling errors also hinders systems that require only lexical analysis of text).", "labels": [], "entities": []}, {"text": "have shown that spelling errors can affect automated estimates of lexical variation, which in turn are used as predictors of text quality.", "labels": [], "entities": []}, {"text": "In the context of automated preposition and determiner error correction in L2 English, De Felice and Pulman noted that the process is often disrupted by misspellings.", "labels": [], "entities": [{"text": "automated preposition and determiner error correction", "start_pos": 18, "end_pos": 71, "type": "TASK", "confidence": 0.5596063931783041}]}, {"text": "Futagi (2010) described how misspellings pose problems in development of a tool for detection of phraseological collocation errors.", "labels": [], "entities": [{"text": "detection of phraseological collocation errors", "start_pos": 84, "end_pos": 130, "type": "TASK", "confidence": 0.7025707781314849}]}, {"text": "Given this state of affairs, it is only natural for automatic text assessment systems to utilize automatic spellchecking components.", "labels": [], "entities": [{"text": "text assessment", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.710427388548851}]}, {"text": "However, generic spellcheckers are typically oriented for errors produced by writers who are native speakers of a language.) have demonstrated that a generic speller has poor performance on data from German language learners. and have demonstrated similar results on data from ELL.", "labels": [], "entities": [{"text": "ELL", "start_pos": 277, "end_pos": 280, "type": "DATASET", "confidence": 0.9393073320388794}]}, {"text": "Many researchers have suggested that spellcheckers for L2 users need to be adapted for the particular patterns of errors that characterize each native language (L1), by studying patterns of interference and influence from L1 to L2).", "labels": [], "entities": []}, {"text": "We have setup to explore a different path, in the context of automated text assessment.", "labels": [], "entities": [{"text": "text assessment", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.7248321324586868}]}, {"text": "Our goal in the present study is to examine to what extent detection and automatic correction of nonword misspellings can be improved by utilizing essay context, for data from both native and nonnative English speakers.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides a description of the corpus of texts and misspellings that was used in this study.", "labels": [], "entities": []}, {"text": "Section 3 describes the ConSpel automatic spellchecking system.", "labels": [], "entities": []}, {"text": "Section 4 presents results from a comparative evaluation of our system, ConSpel, the popular Aspell speller and the Microsoft Office 2007 speller.", "labels": [], "entities": [{"text": "ConSpel", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.8774212002754211}, {"text": "Microsoft Office 2007 speller", "start_pos": 116, "end_pos": 145, "type": "DATASET", "confidence": 0.6974266022443771}]}, {"text": "Section 5 compares our findings with some recent studies and discusses implications for further development of automatic spell-checking systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we report the results of evaluation on data from our gold-standard corpus of 3,000 essays described in section 2.", "labels": [], "entities": []}, {"text": "This evaluation focuses on detection and correction of the 21,212 single-token non-word misspellings (types 1 and 2 in) as well as false alarms raised by spellcheckers.", "labels": [], "entities": []}, {"text": "In addition to ConSpel, we tested Aspell (version 0.60.6), a popular open-source spell checking library).", "labels": [], "entities": []}, {"text": "The third system is spellchecker included in Microsoft Office 2007 (hereafter 'MS Word').", "labels": [], "entities": [{"text": "Microsoft Office 2007", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.7771954735120138}, {"text": "MS Word", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.8251685202121735}]}, {"text": "All evaluations were performed \"in full context\" (rather than word-by-word) -each essay in the corpus was submitted to each system separately, as a simple text file.", "labels": [], "entities": []}, {"text": "All evaluations used standard measures of recall, precision and F-score.", "labels": [], "entities": [{"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9997308850288391}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9996567964553833}, {"text": "F-score", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9987019300460815}]}, {"text": "Evaluations for Aspell and MS Word were conducted twice -once with their original dictionaries and once with the ConSpel spelling dictionary of about 360,000 word forms.", "labels": [], "entities": [{"text": "MS Word", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.8155409097671509}, {"text": "ConSpel spelling dictionary", "start_pos": 113, "end_pos": 140, "type": "DATASET", "confidence": 0.8351961572964987}]}, {"text": "Evaluations where Aspell and MS Word were bundled with ConSpel dictionary are marked below as Aspell+ and MS Word+.", "labels": [], "entities": [{"text": "MS Word", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.8632147908210754}, {"text": "MS Word+", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.9127250512441}]}, {"text": "In this section we report the results of spell-check evaluation with data breakdown by native and nonnative English speakers.", "labels": [], "entities": []}, {"text": "Out of 21,212 singletoken non-word misspellings in our corpus, 2,859 came from 570 essays written by native English speakers (NS) and 18,353 misspellings came from 2,282 essays written by test-takers who are not native speakers of English (NNS).", "labels": [], "entities": []}, {"text": "Comparison of error-detection for five systems is presented in.", "labels": [], "entities": []}, {"text": "All systems show very strong recall results for both types of populations (all values are above 99%).", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9993317723274231}]}, {"text": "The results area bit different for error-detection precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.905529797077179}]}, {"text": "ConSpel achieves best results in both populations (the differences with second-best, MS Word+, are statistically significant at p<.01).", "labels": [], "entities": [{"text": "MS Word+", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.8842078248659769}]}, {"text": "MS Word has precision around 91%, approximately same in both populations.", "labels": [], "entities": [{"text": "MS Word", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9058889448642731}, {"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9990884065628052}]}, {"text": "Compared to MS Word, MS Word+ has better recall rates, in both populations -due to a larger dictionary, it raises much less false alarms.", "labels": [], "entities": [{"text": "MS Word", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.8667980432510376}, {"text": "MS Word+", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.8363451957702637}, {"text": "recall rates", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9807706773281097}]}, {"text": "Aspell lags behind in this comparison.", "labels": [], "entities": []}, {"text": "Using a larger dictionary helps, as Aspell+ precision is better than that of Aspell in both populations; improvement is manifest for NNS data and only 2% for NS data.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.8813103437423706}, {"text": "NNS data", "start_pos": 133, "end_pos": 141, "type": "DATASET", "confidence": 0.7748360931873322}]}, {"text": "Aspell detection precision on NS data (77%) is lower than its precision on NNS data (88%).", "labels": [], "entities": [{"text": "Aspell detection", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.7547354400157928}, {"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.5317289233207703}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9987496137619019}, {"text": "NNS data", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.835188239812851}]}, {"text": "This maybe due to Aspell having a problem with possessive forms (80% of the false alarms on NS data are possessives, but only 70% for NNS data).", "labels": [], "entities": [{"text": "NS data", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.8930890262126923}, {"text": "NNS data", "start_pos": 134, "end_pos": 142, "type": "DATASET", "confidence": 0.917225182056427}]}], "tableCaptions": [{"text": " Table 2. All systems show very strong  recall rates, above 99%. There is more variability  when precision of error detection is concerned.  Both MS Word and Aspell benefit from using the  larger dictionary -they raise much less false  alarms than with original dictionaries (Aspell im- proves precision by about 4% and MS Word by  about 6%). ConSpel shows best precision, the dif- ference with second-best (MS Word+) is statisti- cally significant at p<.01.", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9986730813980103}, {"text": "precision of error detection", "start_pos": 97, "end_pos": 125, "type": "TASK", "confidence": 0.6694498434662819}, {"text": "MS Word", "start_pos": 146, "end_pos": 153, "type": "DATASET", "confidence": 0.8644993901252747}, {"text": "precision", "start_pos": 294, "end_pos": 303, "type": "METRIC", "confidence": 0.8796040415763855}, {"text": "precision", "start_pos": 362, "end_pos": 371, "type": "METRIC", "confidence": 0.9987623691558838}]}, {"text": " Table 2. Evaluation results: non-word error detection", "labels": [], "entities": [{"text": "non-word error detection", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.573927253484726}]}, {"text": " Table 3. Evaluation results: non-word error correction  (top ranked candidates only)", "labels": [], "entities": []}, {"text": " Table 4. Evaluation results: percent correct for  non-word error detection, with breakdown for data from  native (ns) and non-native (nns) English speakers", "labels": [], "entities": [{"text": "correct", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9760338664054871}, {"text": "non-word error detection", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.6719663043816885}]}, {"text": " Table 5. Error-correction evaluation results:  F-scores for six systems, data from native (upper value  in each cell) and non-native English speakers", "labels": [], "entities": [{"text": "Error-correction", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9511295557022095}, {"text": "F-scores", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9984956979751587}]}, {"text": " Table 6. Native  English speakers make significantly more simple  errors (edit distance 1) than non-native speakers,  while the latter make more complex errors (edit  distance 4+).", "labels": [], "entities": [{"text": "edit distance 1)", "start_pos": 75, "end_pos": 91, "type": "METRIC", "confidence": 0.9046451151371002}]}, {"text": " Table 6. Percent of non-word misspellings (tokens)  by edit distance to correct word,  for native and non-native populations.", "labels": [], "entities": [{"text": "Percent", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9760183095932007}]}]}