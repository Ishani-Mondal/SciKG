{"title": [{"text": "Putting Human Assessments of Machine Translation Systems in Order", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7306653559207916}]}], "abstractContent": [{"text": "Human assessment is often considered the gold standard in evaluation of translation systems.", "labels": [], "entities": [{"text": "Human assessment", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6839303225278854}]}, {"text": "But in order for the evaluation to be meaningful, the rankings obtained from human assessment must be consistent and repeatable.", "labels": [], "entities": []}, {"text": "Recent analysis by Bojar et al.", "labels": [], "entities": []}, {"text": "(2011) raised several concerns about the rankings derived from human assessments of English-Czech translation systems in the 2010 Workshop on Machine Translation.", "labels": [], "entities": [{"text": "English-Czech translation systems in the 2010 Workshop on Machine Translation", "start_pos": 84, "end_pos": 161, "type": "TASK", "confidence": 0.5888348191976547}]}, {"text": "We extend their analysis to all of the ranking tasks from 2010 and 2011, and show through an extension of their reasoning that the ranking is naturally cast as an instance of finding the minimum feedback arc set in a tournament, a well-known NP-complete problem.", "labels": [], "entities": []}, {"text": "All instances of this problem in the workshop data are efficiently solvable, but in some cases the rank-ings it produces are surprisingly different from the ones previously published.", "labels": [], "entities": []}, {"text": "This leads to strong caveats and recommendations for both producers and consumers of these rankings.", "labels": [], "entities": []}], "introductionContent": [{"text": "The value of machine translation depends on its utility to human users, either directly through their use of it, or indirectly through downstream tasks such as cross-lingual information extraction or retrieval.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7227474600076675}, {"text": "cross-lingual information extraction", "start_pos": 160, "end_pos": 196, "type": "TASK", "confidence": 0.6369418799877167}]}, {"text": "It is therefore essential to assess machine translation systems according to this utility, but there is a widespread perception that direct human assessment is costly, unreproducible, and difficult to interpret.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.6959946006536484}]}, {"text": "Automatic metrics that predict human utility have therefore attracted substantial attention since they are at least cheap and reproducible given identical data conditions, though they are frequently and correctly criticized for low interpretability and correlation with true utility.", "labels": [], "entities": []}, {"text": "Their use (and abuse) remains contentious.", "labels": [], "entities": []}, {"text": "The organizers of the annual Workshop on Machine Translation (WMT) have taken a strong stance in this debate, asserting the primacy of human evaluation.", "labels": [], "entities": [{"text": "annual Workshop on Machine Translation (WMT)", "start_pos": 22, "end_pos": 66, "type": "TASK", "confidence": 0.7730892449617386}]}, {"text": "Every annual report of their findings since 2007 has included a variant of the following statement: It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality.", "labels": [], "entities": []}, {"text": "Therefore, we define the manual evaluation to be primary, and use the human judgments to validate automatic metrics.", "labels": [], "entities": []}, {"text": "The workshop's human evaluation component has been gradually refined over several years, and as a consequence it has produced a fantastic collection of publicly available data consisting primarily of pairwise judgements of translation systems made by human assessors across a wide variety of languages and tasks.", "labels": [], "entities": []}, {"text": "Despite superb effort in the collection of these assessments, less attention has been focused on the final product derived from them: a totallyordered ranking of translation systems participating in each task.", "labels": [], "entities": []}, {"text": "Many of the official workshop results depend crucially on this ranking, including the evaluation of both machine translation systems and automatic metrics.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7982993125915527}]}, {"text": "Considering the enormous costs and consequences of the ranking, it is important to ask: is the method of constructing it accurate?", "labels": [], "entities": []}, {"text": "The number of possible rankings is combinatorially largewith at least ten systems (accounting for more than half the cases we analyzed) there are over three million possible rankings, and with at least twenty (occurring a few times), there are over 10 18 possible rankings.", "labels": [], "entities": []}, {"text": "Exceptional care is therefore required in producing the rankings.", "labels": [], "entities": []}, {"text": "observed a number of discrepancies in the ranking of English-Czech systems from the 2010 workshop, making these questions evermore pressing.", "labels": [], "entities": []}, {"text": "We extend their analysis in several ways.", "labels": [], "entities": []}, {"text": "1. We show, through a logical extension of their reasoning about flaws in the evaluation, that the final ranking can be naturally cast as an instance of the minimal feedback arc set problem, a well-known NP-Hard problem.", "labels": [], "entities": []}, {"text": "2. We analyze 25 tasks that were evaluated using pairwise assessments from human annotators in 2010 and 2011.", "labels": [], "entities": []}, {"text": "3. We produce new rankings for each of the tasks, which are in some cases surprisingly different from the published rankings.", "labels": [], "entities": []}, {"text": "4. We identify anew set of concerns about sources of error and uncertainty in the data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with 25 relative ranking tasks pro-.", "labels": [], "entities": []}, {"text": "For each task we considered four possible methods of ranking the data: sorting by any of Equation 1 through 3, and sorting consistent with reversal of a minimum feedback arc set (MFAS).", "labels": [], "entities": [{"text": "minimum feedback arc set (MFAS)", "start_pos": 153, "end_pos": 184, "type": "METRIC", "confidence": 0.7294660551207406}]}, {"text": "To weight the edges for the latter approach, we simply used the difference in number of assessments preferring one system over the other; that is, an edge from A to B is weighted count(A B) \u2212 count(A B).", "labels": [], "entities": [{"text": "weighted count(A B) \u2212 count(A B)", "start_pos": 170, "end_pos": 202, "type": "METRIC", "confidence": 0.771684909860293}]}, {"text": "If this quantity is negative, there is instead an edge from B to A.", "labels": [], "entities": []}, {"text": "An MFAS solution written in Python took only a few minutes to produce rankings for all 25 tasks on a 2.13 GHz Intel Core 2 Duo processor, demonstrating that it is completely feasible despite being theoretically intractible.", "labels": [], "entities": []}, {"text": "One value of computing this solution is that it enables us to answer several questions, shorter sentences tend to be more consistent with each other, so perhaps they should be weighted more highly.", "labels": [], "entities": []}, {"text": "Unfortunately, it is not clear how to evaluate alternative weighting schemes, since there is no ground truth for such meta-evaluations.", "labels": [], "entities": []}, {"text": "Taking the analysis above further, we find that the total number of violations of pairwise preferences across all tasks stands at 396 for the MFAS solution, and at 1140, 1215, 979 for Equations 1 through 3.", "labels": [], "entities": []}, {"text": "This empirically validates the suggestion by to remove ties from both the numerator and denominator of the heuristic measure.", "labels": [], "entities": []}, {"text": "On the other hand, despite the intuitive arguments in its favor, the empirical evidence does not strongly favor any of the heuristic measures, all of which are substantially worse than the MFAS solution.", "labels": [], "entities": []}, {"text": "In fact, HEURISTIC 2 (Eq. 2) fails quite spectacularly in one case: on the ranking of the systems produced by the tunable metrics task of WMT 2011 (.", "labels": [], "entities": [{"text": "HEURISTIC", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9772631525993347}, {"text": "WMT 2011", "start_pos": 138, "end_pos": 146, "type": "DATASET", "confidence": 0.9113035798072815}]}, {"text": "Apart from producing a ranking very inconsistent with the pairwise judgements, it achieves a Spearman's rank correlation coefficent of 0.43 with the MFAS solution.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 93, "end_pos": 120, "type": "METRIC", "confidence": 0.8837305158376694}, {"text": "MFAS", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.8171408772468567}]}, {"text": "By comparison, WMT-OFFICIAL (Eq. 1) produces the best ranking, with a correlation of 0.93 with the MFAS solution.", "labels": [], "entities": [{"text": "WMT-OFFICIAL (Eq. 1", "start_pos": 15, "end_pos": 34, "type": "DATASET", "confidence": 0.778844878077507}, {"text": "MFAS", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.732781708240509}]}, {"text": "The two heuristic measures obtain an even lower correlation of 0.19 with each other.", "labels": [], "entities": []}, {"text": "This difference in the two rankings was noted in the WMT 2011 report; however comparison with the MFAS ranker suggests that the published rankings according to the official metric are about as accurate as those based on other heuristic metrics.", "labels": [], "entities": [{"text": "WMT 2011 report", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.9459256529808044}, {"text": "MFAS ranker", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.762728750705719}]}], "tableCaptions": [{"text": " Table 1: The set of tasks we analyzed, including the number of participating systems (excluding the reference, #sys),  and the number of implicit pairwise judgements collected (including the reference, #pairs).", "labels": [], "entities": []}, {"text": " Table 2: Different rankings of the 2011 Czech-English  task. Only the MFAS ranking is acyclic with respect to  pairwise judgements. The final row indicates the weight  of the voilated edges.", "labels": [], "entities": [{"text": "MFAS", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.43179765343666077}]}]}