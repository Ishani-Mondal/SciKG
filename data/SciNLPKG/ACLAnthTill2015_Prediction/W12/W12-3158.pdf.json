{"title": [{"text": "Leave-One-Out Phrase Model Training for Large-Scale Deployment", "labels": [], "entities": [{"text": "Leave-One-Out Phrase Model", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6508943736553192}]}], "abstractContent": [{"text": "Training the phrase table by force-aligning (FA) the training data with the reference translation has been shown to improve the phrasal translation quality while significantly reducing the phrase table size on medium sized tasks.", "labels": [], "entities": [{"text": "FA", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.7154256105422974}]}, {"text": "We apply this procedure to several large-scale tasks, with the primary goal of reducing model sizes without sacrificing translation quality.", "labels": [], "entities": []}, {"text": "To deal with the noise in the automatically crawled parallel training data, we introduce on-demand word deletions, insertions, and backoffs to achieve over 99% successful alignment rate.", "labels": [], "entities": []}, {"text": "We also add heuristics to avoid any increase in OOV rates.", "labels": [], "entities": [{"text": "OOV rates", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.971913605928421}]}, {"text": "We are able to reduce already heavily pruned baseline phrase tables by more than 50% with little to no degradation in quality and occasionally slight improvement, without any increase in OOVs.", "labels": [], "entities": [{"text": "OOVs", "start_pos": 187, "end_pos": 191, "type": "METRIC", "confidence": 0.9644570350646973}]}, {"text": "We further introduce two global scaling factors for re-estimation of the phrase table via posterior phrase alignment probabilities and a modified absolute discounting method that can be applied to fractional counts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Extracting phrases from large amounts of noisy word-aligned training data for statistical machine translation (SMT) generally has the disadvantage of producing many unnecessary phrases.", "labels": [], "entities": [{"text": "Extracting phrases", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.883379340171814}, {"text": "statistical machine translation (SMT)", "start_pos": 78, "end_pos": 115, "type": "TASK", "confidence": 0.771231601635615}]}, {"text": "These can include poor quality phrases, composite phrases that are concatenations of shorter ones, or phrases that are assigned very low probabilities, so that they have no realistic chance when competing against higher scoring phrase pairs.", "labels": [], "entities": []}, {"text": "The goal of this work is two-fold: (i) investigating forced alignment training as a phrase table pruning method for large-scale commercial SMT systems and (ii) proposing several extensions to the training procedure to deal with practical issues and stimulate further research.", "labels": [], "entities": [{"text": "forced alignment training", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7790361046791077}, {"text": "SMT", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.988853931427002}]}, {"text": "Generative phrase translation models have the inherent problem of over-fitting to the training data ().) introduce a leave-one-out procedure which is shown to counteract over-fitting effects.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7484774589538574}]}, {"text": "The authors report significant improvements on the German-English Europarl data with the additional benefit of a severely reduced phrase table size.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.8978985846042633}]}, {"text": "This paper investigates its impact on a number of commercial large-scale systems and presents several extensions.", "labels": [], "entities": []}, {"text": "The first extension is to deal with the highly noisy training data, which is automatically crawled and sentence aligned.", "labels": [], "entities": []}, {"text": "The noise and the baseline pruning of the phrase table lead to low success rates when aligning the source sentence with the target sentence.", "labels": [], "entities": []}, {"text": "We introduce on-demand word deletions, insertions, and backoff phrases to increase the success rate so that we can cover essentially the entire training data.", "labels": [], "entities": []}, {"text": "Secondly, phrase table pruning makes out-of-vocabulary (OOV) issues even more pronounced.", "labels": [], "entities": []}, {"text": "To avoid an increased OOV rate, we retrieve single-word translations from the baseline phrase table.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9819729626178741}]}, {"text": "Lastly, we propose two global scaling factors to allow fine-tuning of the phrase counts in an attempt to re-estimate the translation probabilities and a modification of absolute discounting that can be applied to fractional counts.", "labels": [], "entities": []}, {"text": "Our main contribution is applying forcedalignment on the training data to prune the phrase table.", "labels": [], "entities": []}, {"text": "The rationale behind this is that by decoding the training data, we can identify the phrases that are actually used by the decoder.", "labels": [], "entities": []}, {"text": "Further, we present preliminary experiments on re-estimating the channel models in the phrase table based on counts extracted from the force-aligned data.", "labels": [], "entities": []}, {"text": "This work is organized as follows.", "labels": [], "entities": []}, {"text": "We discuss related work in Section 2, describe our decoder and training procedure in Section 3 and the experiments in Section 4.", "labels": [], "entities": []}, {"text": "A conclusion and discussion of future work is given in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe our experiments on large-scale training data.", "labels": [], "entities": []}, {"text": "First, we prune the original phrase table without re-estimation of the models.", "labels": [], "entities": []}, {"text": "We conducted experiments on many language pairs.", "labels": [], "entities": []}, {"text": "But due to the limited space here, we chose to present two high traffic systems and the two worst systems so that readers can set the correct expectation with the worst-case scenario.", "labels": [], "entities": []}, {"text": "The four systems are: Italian (it), Portuguese (pt), Dutch (nl) and Estonian (et), all translating to English (en).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data sizes of the four systems Italian, Por- tuguese, Dutch and Estonian to English. All numbers  refer to sentence pairs.", "labels": [], "entities": []}, {"text": " Table 2: BLEU scores of forced-alignment-based phrase- table pruning using weak lambda training. n-best size is  100 except for nl-en, where it is 160. We contrast forced  alignment with and without on-demand insertion/deletion  phrases. With the on-demand artificial phrases, FA suc- cess rate is over 99%.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993513226509094}, {"text": "FA suc- cess rate", "start_pos": 278, "end_pos": 295, "type": "METRIC", "confidence": 0.9547183513641357}]}, {"text": " Table 4: Comparison of average source phrase length in  the phrase table.", "labels": [], "entities": []}, {"text": " Table 5: BLEU scores of different n-best sizes for the  highly inflected Dutch system and the noisy Estonian sys- tem.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993707537651062}, {"text": "Estonian sys- tem", "start_pos": 101, "end_pos": 118, "type": "DATASET", "confidence": 0.824938639998436}]}]}