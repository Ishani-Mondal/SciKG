{"title": [{"text": "Kriya -The SFU System for Translation Task at WMT-12", "labels": [], "entities": [{"text": "Translation Task", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.9781549572944641}, {"text": "WMT-12", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.7656090259552002}]}], "abstractContent": [{"text": "This paper describes our submissions for the WMT-12 translation task using Kriya-our hierarchical phrase-based system.", "labels": [], "entities": [{"text": "WMT-12 translation task", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.9273130297660828}]}, {"text": "We submitted systems in French-English and English-Czech language pairs.", "labels": [], "entities": []}, {"text": "In addition to the baseline system following the standard MT pipeline, we tried ensemble decoding for French-English.", "labels": [], "entities": []}, {"text": "The ensemble decoding method improved the BLEU score by 0.4 points over the baseline in newstest-2011.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9831827282905579}, {"text": "newstest-2011", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.9774421453475952}]}, {"text": "For English-Czech, we segmented the Czech side of the corpora and trained two different segmented models in addition to our baseline system.", "labels": [], "entities": []}, {"text": "1 Baseline Systems Our shared task submissions are trained in the hierarchical phrase-based model (Chiang, 2007) framework.", "labels": [], "entities": []}, {"text": "Specifically, we use Kriya (Sankaran et al., 2012)-our in-house Hiero-style system for training and decoding.", "labels": [], "entities": [{"text": "Kriya (Sankaran et al., 2012)-", "start_pos": 21, "end_pos": 51, "type": "DATASET", "confidence": 0.6731069795787334}]}, {"text": "We now briefly explain the baseline systems in French-English and English-Czech language pairs.", "labels": [], "entities": []}, {"text": "We use GIZA++ for word alignments and the Moses (Koehn et al., 2007) phrase-extractor for extracting the initial phrases.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7296561002731323}]}, {"text": "The translation models are trained using the rule extraction module in Kriya.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.7354606688022614}]}, {"text": "In both cases, we pre-processed the training data by running it through the usual pre-processing pipeline of tokenization and lowercasing.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 109, "end_pos": 121, "type": "TASK", "confidence": 0.9620366096496582}]}, {"text": "For French-English baseline system, we trained a simplified hierarchical phrase-based model where the right-hand side can have at most one non-terminal (denoted as 1NT) instead of the usual two non-terminal (2NT) model.", "labels": [], "entities": []}, {"text": "In our earlier experiments we found the 1NT model to perform comparably to the 2NT model for close language pairs such as French-English (Sankaran et al., 2012) at the same time resulting in a smaller model.", "labels": [], "entities": []}, {"text": "We used the shared-task training data consisting of Europarl (v7), News commentary and UN documents for training the translation models having a total of 15 M sentence pairs (we did not use the Fr-En Giga parallel corpus for the training).", "labels": [], "entities": [{"text": "Europarl (v7)", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9163391292095184}, {"text": "News commentary and UN documents", "start_pos": 67, "end_pos": 99, "type": "DATASET", "confidence": 0.776654714345932}, {"text": "Fr-En Giga parallel corpus", "start_pos": 194, "end_pos": 220, "type": "DATASET", "confidence": 0.8785589039325714}]}, {"text": "We trained a 5-gram language model for English using the English Gi-gaword (v4).", "labels": [], "entities": []}, {"text": "For English-Czech, we trained a standard Hiero model that has up to two non-terminals on the right-hand side.", "labels": [], "entities": []}, {"text": "We used the Europarl (v7), news commentary and CzEng (v0.9) corpora having 7.95M sentence pairs for training translation models.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.9868215918540955}]}, {"text": "We trained a 5-gram language model using the Czech side of the parallel corpora and did not use the Czech monolingual corpus.", "labels": [], "entities": []}, {"text": "The baseline systems use the following 8 standard Hiero features: rule probabilities p(e|f) and p(f |e); lexical weights pl (e|f) and pl (f |e); word penalty, phrase penalty, language model and glue rule penalty.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In the English-Czech experiments, we used the same datasets for the dev and test sets as in FrenchEnglish experiments with 7567 sentence pairs and test: newstest2011 with 3003 sentence pairs).", "labels": [], "entities": []}, {"text": "Similarly, MERT has been used to tune the feature weights and we report the BLEU scores of two testsets computed using the official evaluation script (mteval-v11b.pl)..2 shows the results of different segmentation schemes on the WMT-11 and WMT-12 test-sets.", "labels": [], "entities": [{"text": "MERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.929764449596405}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.999426007270813}, {"text": "WMT-11", "start_pos": 229, "end_pos": 235, "type": "DATASET", "confidence": 0.9571744799613953}, {"text": "WMT-12 test-sets", "start_pos": 240, "end_pos": 256, "type": "DATASET", "confidence": 0.9025284647941589}]}, {"text": "SM1 slightly outperformed the other two models in Test-11, however the unsegmented model performed best in Test-12, though marginally.", "labels": [], "entities": [{"text": "Test-11", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9595431089401245}, {"text": "Test-12", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.9787048101425171}]}, {"text": "We are currently investigating this and are also considering the possibility employing the idea of morpheme prediction in the post-decoding step in combination with this morpheme-based translation as suggested by Clifton  and Sarkar (2011).", "labels": [], "entities": [{"text": "morpheme prediction", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7300346195697784}]}], "tableCaptions": [{"text": " Table 1: French-English BLEU scores. Best performing  setting is shown in Boldface.", "labels": [], "entities": [{"text": "French-English", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.6245174407958984}, {"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9669737219810486}]}, {"text": " Table 2: Applying ensemble decoding with different mix- ture operations on the Test-11 dataset. Best performing  setting is shown in Boldface.", "labels": [], "entities": [{"text": "Applying", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9585526585578918}, {"text": "Test-11 dataset", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.9850369691848755}]}, {"text": " Table 3: The English-Czech results for different segmen- tation settings. Best performing setting is shown in Bold- face.", "labels": [], "entities": [{"text": "segmen- tation", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.7690507769584656}]}]}