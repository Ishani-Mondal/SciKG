{"title": [{"text": "Unsupervised vs. supervised weight estimation for semantic MT evaluation metrics", "labels": [], "entities": [{"text": "MT evaluation metrics", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.8832593957583109}]}], "abstractContent": [{"text": "We present an unsupervised approach to estimate the appropriate degree of contribution of each semantic role type for semantic translation evaluation, yielding a semantic MT evaluation metric whose correlation with human adequacy judgments is comparable to that of recent supervised approaches but without the high cost of a human-ranked training corpus.", "labels": [], "entities": [{"text": "semantic translation evaluation", "start_pos": 118, "end_pos": 149, "type": "TASK", "confidence": 0.8461366097132365}]}, {"text": "Our new unsupervised estimation approach is motivated by an analysis showing that the weights learned from supervised training are distributed in a similar fashion to the relative frequencies of the semantic roles.", "labels": [], "entities": []}, {"text": "Empirical results show that even without a training corpus of human adequacy rankings against which to optimize correlation, using instead our relative frequency weighting scheme to approximate the importance of each semantic role type leads to a semantic MT evaluation metric that correlates comparable with human adequacy judgments to previous metrics that require far more expensive human rankings of adequacy over a training corpus.", "labels": [], "entities": []}, {"text": "As a result, the cost of semantic MT evaluation is greatly reduced.", "labels": [], "entities": [{"text": "semantic MT evaluation", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7737908363342285}]}], "introductionContent": [{"text": "In this paper we investigate an unsupervised approach to estimate the degree of contribution of each semantic role type in semantic translation evaluation in low cost without using a human-ranked training corpus but still yields a evaluation metric that correlates comparably with human adequacy judgments to that of recent supervised approaches as in.", "labels": [], "entities": [{"text": "semantic translation evaluation", "start_pos": 123, "end_pos": 154, "type": "TASK", "confidence": 0.8593661983807882}]}, {"text": "The new approach is motivated by an analysis showing that the distribution of the weights learned from the supervised training is similar to the relative frequencies of the occurrences of each semantic role in the reference translation.", "labels": [], "entities": []}, {"text": "We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type.", "labels": [], "entities": []}, {"text": "With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced.", "labels": [], "entities": []}, {"text": "For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (), NIST), METEOR (Banerjee and), PER (), CDER () and WER () because of their support on fast and inexpensive evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9900222420692444}, {"text": "MT evaluation", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.8969232738018036}, {"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9984655380249023}, {"text": "NIST", "start_pos": 156, "end_pos": 160, "type": "DATASET", "confidence": 0.7926124930381775}, {"text": "METEOR", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.972796618938446}, {"text": "PER", "start_pos": 186, "end_pos": 189, "type": "METRIC", "confidence": 0.9201307892799377}, {"text": "WER", "start_pos": 206, "end_pos": 209, "type": "METRIC", "confidence": 0.9825977683067322}]}, {"text": "These metrics are good at ranking overall systems by averaging their scores over the entire document.", "labels": [], "entities": []}, {"text": "As MT systems improve, the focus of MT evaluation changes from generally reflecting the quality of each system to assisting error analysis on each MT output in detail.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9839242100715637}, {"text": "MT evaluation", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9675464928150177}]}, {"text": "The failure of such metrics in evaluating translation quality on sentence level are becoming more apparent.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.842840701341629}]}, {"text": "Though containing roughly the correct words, the MT output as a whole sentence is still quite incomprehensible and fails to express meaning that is close to the input.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9594040513038635}]}, {"text": "Lexical n-gram based evaluation metrics are surface-oriented and do not do so well at ranking translations according to adequacy and are particularly poor at reflecting significant translation quality improvements on more meaningful word sense or semantic frame choices which human judges can indicate clearly. and even reported cases where BLEU strongly disagrees with human judgment on translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 341, "end_pos": 345, "type": "METRIC", "confidence": 0.9585776329040527}]}, {"text": "proposed STM, a structural approach based on syntax to addresses the failure of lexical similarity based metrics in evaluating translation grammaticality.", "labels": [], "entities": [{"text": "STM", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9457968473434448}]}, {"text": "However, a grammatical translation can achieve a high syntax-based score but still contains meaning errors arising from confusion of semantic roles.", "labels": [], "entities": []}, {"text": "On the other hand, despite the fact that non-automatic, manually evaluations, such as HTER (), are more adequacy oriented and show a high correlation with human adequacy judgment, the high labor cost prohibits their widespread use.", "labels": [], "entities": []}, {"text": "There was also work on explicitly evaluating MT adequacy with aggregated linguistic features ( and textual entailment (.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9950873255729675}]}, {"text": "In the work of Lo and Wu (2011a), MEANT and its human variants HMEANT were introduced and empirical experimental results showed that HMEANT, which can be driven by low-cost monolingual semantic roles annotators with high interannotator agreement, correlates as well as HTER and far superior than BLEU and other surfaced oriented evaluation metrics.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.8574301600456238}, {"text": "HTER", "start_pos": 269, "end_pos": 273, "type": "METRIC", "confidence": 0.992651641368866}, {"text": "BLEU", "start_pos": 296, "end_pos": 300, "type": "METRIC", "confidence": 0.9936378598213196}]}, {"text": "Along with additional improvements to the MEANT family of metrics, Lo and Wu (2011b) detailed the studies of the impact of each individual semantic role to the metric's correlation with human adequacy judgments.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.7938215136528015}]}, {"text": "further discussed that with a proper weighting scheme of semantic frame in a sentence, structured semantic role representation is more accurate and intuitive than flattened role representation for semantic MT evaluation metrics.", "labels": [], "entities": [{"text": "MT evaluation metrics", "start_pos": 206, "end_pos": 227, "type": "TASK", "confidence": 0.8743739525477091}]}, {"text": "The recent trend of incorporating more linguistic features into MT evaluation metrics raise the discussion on the appropriate approach in weighting and combining them.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.9234327971935272}]}, {"text": "ULC ( uses uniform weights to aggregate linguistic features.", "labels": [], "entities": [{"text": "ULC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8707632422447205}]}, {"text": "This approach does not capture the importance of each feature to the overall translation quality to the MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.975585401058197}]}, {"text": "One obvious example of different semantic roles contribute differently to the overall meaning is that readers usually accept translations with errors in adjunct arguments as a valid translation but not those with errors in core arguments.", "labels": [], "entities": []}, {"text": "Unlike ULC,; Lo and Wu (2011a) approach the weight estimation problem by maximum correlation training which directly optimize the correlation with human adequacy judg- ments.", "labels": [], "entities": [{"text": "weight estimation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7384804785251617}, {"text": "correlation", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.9152353405952454}]}, {"text": "However, the shortcomings of this approach is that it requires a human-ranked training corpus which is expensive, especially for languages with limited resource.", "labels": [], "entities": []}, {"text": "We argue in this paper that for semantic MT evaluation, the importance of each semantic role type can easily be estimated using a simple unsupervised approach which leverage the relative frequencies of the semantic roles appeared in the reference translation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.8529669046401978}]}, {"text": "Our proposed weighting scheme is motivated by an analysis showing that the weights learned from supervised training are distributed in a similar fashion to the relative frequencies of the semantic roles.", "labels": [], "entities": []}, {"text": "Our results show that the semantic MT evaluation metric using the relative frequency weighting scheme to approximate the importance of each semantic role type correlates comparably with human adequacy judgments to previous metrics that use maximum correlation training, which requires expensive human rankings of adequacy over a training corpus.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.8175403475761414}]}, {"text": "Therefore, the cost of semantic MT evaluation is greatly reduced.", "labels": [], "entities": [{"text": "semantic MT evaluation", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7769866983095804}]}], "datasetContent": [{"text": "Adopting the principle that a good translation is one from which human readers may successfully understand at least the basic event structure-\"who did what to whom, when, where and why\")-which represents the most essential meaning of the source utterances, proposed HMEANT to evaluate translation utility based on semantic frames reconstructed by human reader of machine translation output.", "labels": [], "entities": []}, {"text": "Monolingual (or bilingual) annotators must label the semantic roles in both the reference and machine translations, and then to align the semantic predicates and role fillers in the MT output to the reference translations.", "labels": [], "entities": []}, {"text": "These annotations allow HMEANT to then look at the aligned role fillers, and aggregate the translation accuracy for each role.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.8244526386260986}]}, {"text": "In the spirit of Occam's razor and representational transparency, the HMEANT score is defined simply in terms of a weighted f-score over these aligned predicates and role fillers.", "labels": [], "entities": [{"text": "HMEANT score", "start_pos": 70, "end_pos": 82, "type": "METRIC", "confidence": 0.8048235774040222}]}, {"text": "More precisely, HMEANT is defined as follows: 1.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.6966741681098938}]}, {"text": "Human annotators annotate the shallow semantic structures of both the references and MT output.", "labels": [], "entities": []}, {"text": "2. Human judges align the semantic frames between the references and MT output by judging the correctness of the predicates.", "labels": [], "entities": []}, {"text": "3. For each pair of aligned semantic frames, where mi and r i are the weights for frame,i, in the MT/REF respectively.", "labels": [], "entities": [{"text": "MT/REF", "start_pos": 98, "end_pos": 104, "type": "DATASET", "confidence": 0.7226733962694804}]}, {"text": "These weights estimate the degree of contribution of each frame to the overall meaning of the sentence.", "labels": [], "entities": []}, {"text": "M i, j and R i, j are the total counts of argument of type j in frame i in the MT/REF respectively.", "labels": [], "entities": [{"text": "MT/REF", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.6459473768870035}]}, {"text": "Ci, j and P i, j are the count of the correctly and partial correctly translated argument of type j in frame i in the MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 118, "end_pos": 120, "type": "TASK", "confidence": 0.7045919299125671}]}, {"text": "w pred is the weight for the predicate and w j is the weights for the arguments of type j.", "labels": [], "entities": []}, {"text": "These weights estimate the degree of contribution of different types of semantic roles to the overall meaning of the semantic frame they attached to.", "labels": [], "entities": []}, {"text": "The frame precision/recall is the weighted sum of the number of correctly translated roles in a frame normalized by the weighted sum of the total number of all roles in that frame in the MT/REF respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.724044919013977}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9754810333251953}, {"text": "MT/REF", "start_pos": 187, "end_pos": 193, "type": "TASK", "confidence": 0.49930821855862934}]}, {"text": "The sentence precision/recall is the weighted sum of the frame precision/recall for all frames normalized by the weighted sum of the total number of frames in MT/REF respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.4972198009490967}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.5241523385047913}, {"text": "frame precision", "start_pos": 57, "end_pos": 72, "type": "METRIC", "confidence": 0.7633191645145416}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.553112804889679}, {"text": "MT/REF", "start_pos": 159, "end_pos": 165, "type": "DATASET", "confidence": 0.5704365372657776}]}, {"text": "shows the internal structure of HMEANT.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.8177459239959717}]}, {"text": "In the work of Lo and Wu (2011b), the correlation of all individual roles with the human adequacy judgments were found to be non-negative.", "labels": [], "entities": []}, {"text": "Therefore, grid search was used to estimate the weights of each roles by optimizing the correlation with human adequacy judgments.", "labels": [], "entities": [{"text": "grid search", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.8658148944377899}]}, {"text": "This approach requires an expensive human-ranked training corpus which may not be available for languages with sparse resources.Unlike the supervised training approach, our proposed relative frequency weighting scheme does not require additional resource other than the SRL annotated reference translation.", "labels": [], "entities": [{"text": "SRL annotated reference translation", "start_pos": 270, "end_pos": 305, "type": "TASK", "confidence": 0.5234933793544769}]}, {"text": "3 Which roles contribute more in the semantic MT evaluation metric?", "labels": [], "entities": [{"text": "MT evaluation metric", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.8839068015416464}]}, {"text": "We begin with an investigation that suggests that the relative frequency of each semantic role (which can be estimated in unsupervised fashion without human rankings) approximates fairly closely its importance as determined by previous supervised optimization approaches.", "labels": [], "entities": []}, {"text": "Since there is no ground truth on which  Following the benchmark assessment in NIST MetricsMaTr 2010  metric at the sentence level using Kendall's rank correlation coefficient which evaluate the correlation of the proposed metric with human judgments on translation adequacy ranking.", "labels": [], "entities": [{"text": "NIST MetricsMaTr 2010", "start_pos": 79, "end_pos": 100, "type": "DATASET", "confidence": 0.9123351176579794}, {"text": "Kendall's rank correlation coefficient", "start_pos": 137, "end_pos": 175, "type": "METRIC", "confidence": 0.6586306035518646}]}, {"text": "A higher the value for indicates a higher similarity to the ranking by the evaluation metric to the human judgment.", "labels": [], "entities": []}, {"text": "The range of possible values of correlation coefficient is, where 1 means the systems are ranked in the same order as the human judgment and -1 means the systems are ranked in the reverse order as the human judgment.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 32, "end_pos": 55, "type": "METRIC", "confidence": 0.9802158772945404}]}, {"text": "For GALE-A and GALE-B, the human judgment on adequacy was obtained by showing all three MT outputs together with the Chinese source input to a human reader.", "labels": [], "entities": [{"text": "GALE-A", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.6056686043739319}, {"text": "GALE-B", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.8201008439064026}]}, {"text": "The human reader was instructed to order the sentences from the three MT systems according to the accuracy of meaning in the translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.8924402594566345}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9982701539993286}]}, {"text": "For WMT12, the human adequacy judgments are provided by the organizers.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.5330897569656372}]}, {"text": "The rest of the experimental setup is the same as that used in section 3.", "labels": [], "entities": []}, {"text": "shows that HMEANT with the proposed unsupervised semantic role weighting scheme correlate comparably with human adequacy judgments to that optimized with a more expensive human-ranked training corpus, and, outperforms all other commonly used automatic metrics (except for METEOR in Czech).", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.7808172106742859}, {"text": "METEOR", "start_pos": 272, "end_pos": 278, "type": "METRIC", "confidence": 0.8434036374092102}]}, {"text": "The results from GALE-A, GALE-B and WMT12 are consistent.", "labels": [], "entities": [{"text": "GALE-B", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.5957446098327637}, {"text": "WMT12", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.6919724941253662}]}, {"text": "These encouraging results show that semantic MT evaluation metric could be widely applicable to languages other than English.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.7815604209899902}]}, {"text": "Having seen that the weights of the predicate and semantic roles estimated by the unsupervised approach fairly closely approximate those learned from the supervised approach, we now show that the unsupervised approach leads to a semantic MT evaluation metric that correlates comparably with human adequacy judgments to one that is trained on afar more expensive human-ranked training corpus.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 238, "end_pos": 251, "type": "TASK", "confidence": 0.8731958568096161}]}], "tableCaptions": [{"text": " Table 1: Deviation of relative frequency from optimized weight of each semantic role in GALE-A, GALE-B and  WMT12", "labels": [], "entities": [{"text": "WMT12", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.6425227522850037}]}, {"text": " Table 3: Average sentence-level correlation with human adequacy judgments of HMEANT using supervised and  unsupervised weight scheme on GALE-A, GALE-B and WMT12, (with baseline comparison of commonly used  automatic MT evaluation metric.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.7608875632286072}, {"text": "MT evaluation", "start_pos": 217, "end_pos": 230, "type": "TASK", "confidence": 0.9099462032318115}]}]}