{"title": [{"text": "Position Paper: Towards Standardized Metrics and Tools for Spoken and Multimodal Dialog System Evaluation", "labels": [], "entities": [{"text": "Spoken and Multimodal Dialog System Evaluation", "start_pos": 59, "end_pos": 105, "type": "TASK", "confidence": 0.7567203640937805}]}], "abstractContent": [{"text": "We argue that standardized metrics and automatic evaluation tools are necessary for speeding up knowledge generation and development processes for dialog systems.", "labels": [], "entities": [{"text": "knowledge generation", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.7467887103557587}]}], "introductionContent": [{"text": "The Spoken Dialogue Challenge launched by CMU) provides a common platform for dialog researchers in order to test the performance of their systems and components against the state-of-the-art.", "labels": [], "entities": [{"text": "CMU", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.9286433458328247}]}, {"text": "Still, evaluations are individual undertakings inmost areas, as common metrics and procedures which would be applicable fora range of systems are sparse.", "labels": [], "entities": []}, {"text": "In the following, it is argued that significant progress can be made if three prerequisites are available: \uf0a7 Common metrics for quantifying user and system interaction behavior and perceived quality \uf0a7 Reliable models for predicting user judgments on the basis of automatically-extracted or annotated interaction metrics \uf0a7 Methods for realistically simulating user behavior in response to dialog systems The state-of-the-art and necessary research in these three areas is outlined in the following paragraphs.", "labels": [], "entities": []}, {"text": "The Spoken Dialogue Challenge can contribute to validating such metrics and models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}