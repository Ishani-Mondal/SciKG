{"title": [{"text": "PLUTO: Automated Solutions for Patent Translation i", "labels": [], "entities": [{"text": "Patent Translation", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7777177095413208}]}], "abstractContent": [], "introductionContent": [{"text": "PLUTO is a commercial development project supported by the European Commission as part of the FP7 programme which aims to eliminate the language barriers that exist worldwide in the provision of multilingual access to patent information.", "labels": [], "entities": [{"text": "PLUTO", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.722351610660553}, {"text": "FP7", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.9390822052955627}]}, {"text": "The project consortium comprises four partners: the Centre for Next Generation Localisation at Dublin City University, 1 ESTeam AB, CrossLang, and the Dutch Patent Information User Group (WON).", "labels": [], "entities": [{"text": "Dublin City University", "start_pos": 95, "end_pos": 117, "type": "DATASET", "confidence": 0.9646416306495667}, {"text": "CrossLang", "start_pos": 132, "end_pos": 141, "type": "DATASET", "confidence": 0.9535686373710632}, {"text": "Dutch Patent Information User Group (WON)", "start_pos": 151, "end_pos": 192, "type": "DATASET", "confidence": 0.6999237835407257}]}, {"text": "Research and development is carried out in close collaboration with user groups and intellectual property (IP) professionals to ensure solutions and software are delivered that meet actual user needs.", "labels": [], "entities": []}], "datasetContent": [{"text": "The performance of the patent MT systems in PLUTO is evaluated using a range of methods aimed not only at gauging general quality, but also identifying areas for improvement and relative performance against similar systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9647523760795593}, {"text": "PLUTO", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.7882717251777649}]}, {"text": "In addition to assessing the MT systems using automatic evaluation metrics such as BLEU () and METEOR (), large-scale human evaluations are also carried out.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9854267835617065}, {"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9987003803253174}, {"text": "METEOR", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9842151999473572}]}, {"text": "MT system output is ranked from 1-5 based on the overall quality of translation, and individual translation errors are identified and classified in an error categorisation task.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9561160802841187}]}, {"text": "On top of this standalone evaluation, the PLUTO MT systems are also benchmarked against leading commercial systems across two MT paradigms: Google Translate for statistical MT and Systran (Enterprise) for rule-based MT.", "labels": [], "entities": [{"text": "PLUTO MT", "start_pos": 42, "end_pos": 50, "type": "TASK", "confidence": 0.47841791808605194}, {"text": "MT", "start_pos": 173, "end_pos": 175, "type": "TASK", "confidence": 0.8685374855995178}, {"text": "MT", "start_pos": 216, "end_pos": 218, "type": "TASK", "confidence": 0.8530552983283997}]}, {"text": "A comparative analysis is carried out using both the automatic and human evaluation techniques described above.", "labels": [], "entities": [{"text": "comparative analysis", "start_pos": 2, "end_pos": 22, "type": "TASK", "confidence": 0.8565307259559631}]}, {"text": "This comparison is also applied to the output of the PLUTO MT systems and the output of the integrated TM/MT system in order to quantify the improvements achieved using the translation memories.", "labels": [], "entities": [{"text": "PLUTO MT", "start_pos": 53, "end_pos": 61, "type": "TASK", "confidence": 0.5262873768806458}]}, {"text": "The main findings from the first round of evaluations for our French-English and Portuguese-English systems showed that our MT systems score relatively high based on human judgments --3.8 out of 5 on average --while being ranked higher than the commercial systems approximately 75% of the time.", "labels": [], "entities": [{"text": "MT", "start_pos": 124, "end_pos": 126, "type": "TASK", "confidence": 0.9800906777381897}]}, {"text": "More details on these experiments can be found in Ceausu et al.", "labels": [], "entities": []}], "tableCaptions": []}