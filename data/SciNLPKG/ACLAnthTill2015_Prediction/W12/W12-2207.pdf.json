{"title": [{"text": "Do NLP and machine learning improve traditional readability formulas?", "labels": [], "entities": []}], "abstractContent": [{"text": "Readability formulas are methods used to match texts with the readers' reading level.", "labels": [], "entities": []}, {"text": "Several methodological paradigms have previously been investigated in the field.", "labels": [], "entities": []}, {"text": "The most popular paradigm dates several decades back and gave rise to well known readability formulas such as the Flesch formula (among several others).", "labels": [], "entities": [{"text": "Flesch formula", "start_pos": 114, "end_pos": 128, "type": "METRIC", "confidence": 0.9573529064655304}]}, {"text": "This paper compares this approach (henceforth \"classic\") with an emerging paradigm which uses sophisticated NLP-enabled features and machine learning techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "Readability studies date back to the 1920's and have already spawned probably more than a hundred papers with research on the development of efficient methods to match readers and texts relative to their reading difficulty.", "labels": [], "entities": []}, {"text": "During this period of time, several methodological trends have appeared in succession (reviewed in,).", "labels": [], "entities": []}, {"text": "We can group these trends in three major approaches: the \"classic studies\", the \"structurocognitivist paradigm\" and the \"AI readability\", a term suggested by.", "labels": [], "entities": []}, {"text": "The classic period started right after the seminal work of and and is characterized by an ideal of simplicity.", "labels": [], "entities": []}, {"text": "The models (readability formulas) proposed to predict text difficulty fora given population are kept simple, using multiple linear regression with two, or sometimes, three predictors.", "labels": [], "entities": []}, {"text": "The predictors are simple surface features, such as the average number of syllables per word and the average number of words per sentence.", "labels": [], "entities": []}, {"text": "The and formulas are probably the best-known examples of this period.", "labels": [], "entities": []}, {"text": "With the rise of cognitivism in psychological sciences in the 70's and 80's, new dimensions of texts are highlighted such as coherence, cohesion, and other discourse aspects.", "labels": [], "entities": []}, {"text": "This led some scholars ( to adopt a critical attitude to classic readability formulas which could only take into account superficial features, ignoring other important aspects contributing to text difficulty. and, among others, suggested new features for readability, based on those newly discovered text dimensions.", "labels": [], "entities": []}, {"text": "However, despite the fact that the proposed models made use of more sophisticated features, they failed to outperform the classic formulas.", "labels": [], "entities": []}, {"text": "It is probably not coincidental that after these attempts readability research efforts declined in the 90s.", "labels": [], "entities": []}, {"text": "More recently, however, the development of efficient natural language processing (NLP) systems and the success of machine learning methods led to a resurgence of interest in readability as it became clear that these developments could impact the design and performance of readability measures.", "labels": [], "entities": []}, {"text": "Several studies) have used NLP-enabled feature extraction and state-of-the-art machine learning algorithms and have reported significant gains in performance, suggesting that the AI approach might be superior to previous attempts.", "labels": [], "entities": [{"text": "NLP-enabled feature extraction", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.6130433181921641}]}, {"text": "Going beyond reports of performance which are often hard to compare due to alack of a common gold standard, we are interested in investigating AI approaches more closely with the aim of understanding the reasons behind the reported superiority over classic formulas.", "labels": [], "entities": []}, {"text": "AI readability systems use NLP for richer feature extraction and a machine learning algorithm.", "labels": [], "entities": [{"text": "richer feature extraction", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.7382292946179708}]}, {"text": "Given that the classic formulas are also statistical, is performance boosted because of the addition of NLP-enabled feature extraction or by better machine learning algorithms?", "labels": [], "entities": [{"text": "NLP-enabled feature extraction", "start_pos": 104, "end_pos": 134, "type": "TASK", "confidence": 0.58087291320165}]}, {"text": "In this paper, we report initial findings of three experiments designed to explore this question.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews previous findings in the field and the challenge of providing a uniform explanation for these findings.", "labels": [], "entities": []}, {"text": "Section 3 gives a brief overview of prior work on French readability, which is the context of our experiments (evaluating the readability of French texts).", "labels": [], "entities": []}, {"text": "Because there is no prior work comparing classic formulas with AI readablity measures for French, we first report the results of this comparison in Section 3.", "labels": [], "entities": []}, {"text": "Then, we proceed with the results of three experiments (2-4), comparing the contributions of the AI enabled features with features used in classic formulas, different machine learning algorithms and the interactions of features with algorithms.", "labels": [], "entities": []}, {"text": "There results are reported in Sections 4, 5, and 6, respectively.", "labels": [], "entities": []}, {"text": "We conclude in Section 7 with a summary of the main findings and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To compute a classic readability formula for FFL, we used the formula proposed for French by.", "labels": [], "entities": [{"text": "FFL", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9283899068832397}]}, {"text": "We compared the results of this formula with the AI model trained on the FFL data used by Fran\u00e7ois (2011b).", "labels": [], "entities": [{"text": "FFL data", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.7871307134628296}]}, {"text": "The formula is an adaptation of the Flesch formula for French, based on a study of a bilingual corpus: where Y is a readability score ranging from 100 (easiest) to 0 (harder); lp is the average number of words per sentence and lm is the average number of syllables per 100 words.", "labels": [], "entities": []}, {"text": "Although this formula is not specifically designed for FFL, we chose to implement it over formulas proposed for FFL).", "labels": [], "entities": [{"text": "FFL", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9014226198196411}, {"text": "FFL", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.6925827264785767}]}, {"text": "FFL-specific formulas are optimized for English-speaking learners of French while our dataset is agnostic to the native language of the learners.", "labels": [], "entities": []}, {"text": "The computation of the Kandel and Moles (1958) formula requires a syllabification system for French.", "labels": [], "entities": []}, {"text": "Due to unavailability of such a system for French, we adopted a hybrid syllabification method.", "labels": [], "entities": []}, {"text": "For words included in Lexique (), we used the gold syllabification included in the dictionary.", "labels": [], "entities": []}, {"text": "For all other words, we generated API phonetic representations with espeak , and then applied the syllabification tool used for Lexique3.", "labels": [], "entities": []}, {"text": "The accuracy of this process exceeded 98%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997281432151794}]}, {"text": "For the comparison with an AI model, we extracted the same 46 features (see for the complete list) used in Fran\u00e7ois' model and trained a SVM model.", "labels": [], "entities": []}, {"text": "For all the study, the gold-standard consisted of data taken from textbooks and labeled according to the classification made by the publishers.", "labels": [], "entities": []}, {"text": "The corpus includes a wide range of texts, including extracts from novels, newspapers articles, songs, mail, dialogue, etc.", "labels": [], "entities": []}, {"text": "The difficulty levels are defined by the Common European Framework of Reference for Languages (CEFR) (Council of Europe, 2001) as follows: A1 (Breakthrough); A2 (Waystage); B1 (Threshold); B2 (Vantage); C1 (Effective Operational Proficiency) and C2 (Mastery).", "labels": [], "entities": [{"text": "Common European Framework of Reference for Languages (CEFR) (Council of Europe, 2001)", "start_pos": 41, "end_pos": 126, "type": "DATASET", "confidence": 0.6846262865206775}, {"text": "Mastery", "start_pos": 250, "end_pos": 257, "type": "METRIC", "confidence": 0.9785885810852051}]}, {"text": "The test corpus includes 68 texts per level, fora total of 408 documents (see).", "labels": [], "entities": []}, {"text": "We applied both readability models to this test corpus.", "labels": [], "entities": []}, {"text": "Assessing and comparing the performance of the two models with accuracy scores (acc), as is common in classification tasks, has proved challenging and, in the end, uninformative.", "labels": [], "entities": [{"text": "accuracy scores (acc)", "start_pos": 63, "end_pos": 84, "type": "METRIC", "confidence": 0.9380826473236084}]}, {"text": "This is because the Kandel and Moles formula's output scores are not an ordinal variable, but intervals.", "labels": [], "entities": []}, {"text": "To compute accuracy we would have to define a set of rather arbitrary cutoff points in the intervals and correspond them with level boundaries.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9992207288742065}]}, {"text": "We tried three approaches to achieve this task.", "labels": [], "entities": []}, {"text": "First, we used correspondences between Flesch scores and seven difficulty levels proposed for French by: \"very easy\" (70 to 80) to \"very difficult\" (-20 to 10).", "labels": [], "entities": [{"text": "Flesch scores", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.9668874442577362}]}, {"text": "Collapsing the \"difficult\" and \"very difficult\" categories into one, we were able to roughly match this scale with the A1-C2 scale.", "labels": [], "entities": []}, {"text": "The second method was similar, except that those levels were mapped on the values from the original Flesch scale instead of the one adapted for French.", "labels": [], "entities": [{"text": "Flesch", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.8361685276031494}]}, {"text": "The third approach was to estimate normal distribution parameters \u00b5 j and \u03c3 j for each level j for the Kandel and Moles' formula output scores obtained on our corpus.", "labels": [], "entities": []}, {"text": "The class membership of a given observation i was then computed as follows: Since the parameters were trained on the same corpus used for the evaluation, this computation should yield optimal class membership thresholds for our data.", "labels": [], "entities": []}, {"text": "Given the limitations of all three approaches, it is not surprising that accuracy scores were very low: 9% for the first and 12% for the second, which is worse than random (16.6%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9994592070579529}]}, {"text": "The third approach gave a much improved accuracy score, 33%, but still quite low.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 40, "end_pos": 54, "type": "METRIC", "confidence": 0.981812447309494}]}, {"text": "The problem is that, in a continuous formula, predictions that are very close to the actual will be classified as errors if they fall on the wrong side of the cutoff threshold.", "labels": [], "entities": []}, {"text": "These results are, in any case, clearly inferior to the AI formula based on SVM, which classified correctly 49% of the texts.", "labels": [], "entities": [{"text": "AI", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9824673533439636}, {"text": "SVM", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.8233588337898254}]}, {"text": "A more suitable evaluation measure fora continuous formula would be to compute the multiple correlation (R).", "labels": [], "entities": [{"text": "multiple correlation (R)", "start_pos": 83, "end_pos": 107, "type": "METRIC", "confidence": 0.8485997915267944}]}, {"text": "The multiple correlation indicates the extent to which predictions are close to the actual classes, and, when R 2 is used, it describes the percentage of the dependent variable variation which is explained by the model.", "labels": [], "entities": [{"text": "multiple correlation", "start_pos": 4, "end_pos": 24, "type": "METRIC", "confidence": 0.8109468221664429}]}, {"text": "Kandel and Moles' formula got a slightly better performance (R = 0.551), which is still substantially lower that the score (R = 0.728) obtained for the SVM model.", "labels": [], "entities": [{"text": "R", "start_pos": 61, "end_pos": 62, "type": "METRIC", "confidence": 0.9957724213600159}]}, {"text": "To check if the difference between the two correlation scores was significant, we applied the Hotelling's T-test for dependent correlation) (required given that the two models were evaluated on the same data).", "labels": [], "entities": [{"text": "correlation", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.9551094770431519}, {"text": "T-test", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.8132421374320984}]}, {"text": "The result of the testis highly significant (t = \u221219.5; p = 1.83 e\u221260 ), confirming that the SVM model performed better that the classic formula.", "labels": [], "entities": []}, {"text": "Finally, we computed a partial Spearman correlation for both models.", "labels": [], "entities": []}, {"text": "We considered the output of each model as a single variable and we could, therefore, evaluate the relative predictive power of each variable when the other variable is controlled.", "labels": [], "entities": []}, {"text": "The partial correlation for the Kandel and Moles formula is very low (\u03c1 = \u22120.11; p = 0.04) while the SVM model retains a good partial correlation (\u03c1 = \u22120.53; p < 0.001).", "labels": [], "entities": []}, {"text": "In this section, we compared the contribution of the features used in classic formulas with the more sophisticated NLP-enabled features used in the machine learning models of readability.", "labels": [], "entities": []}, {"text": "Given that the features used in classic formulas are very easy to compute and require minimal processing by comparison to the NLP features that require heavy preprocessing (e.g., parsing), we are, also, interested in finding out how much gain we obtain from the NLP features.", "labels": [], "entities": [{"text": "parsing", "start_pos": 179, "end_pos": 186, "type": "TASK", "confidence": 0.9681710600852966}]}, {"text": "A consideration that becomes important for tasks requiring real time evaluation of reading difficulty.", "labels": [], "entities": []}, {"text": "To evaluate the relative contribution of each set of features, we experiment with two sets of features (see.", "labels": [], "entities": []}, {"text": "We labeled as \"classic\", not only   the features that are commonly used in traditional formulas like Flesch (length of words and number of words per sentence) but also other easy to compute features that were identified in readability work.", "labels": [], "entities": [{"text": "Flesch", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9964781403541565}]}, {"text": "Specifically, in the \"classic\" set we include number of personal pronouns (given as a list) (, the Type Token Ratio (TTR), or even simple ratios of POS.", "labels": [], "entities": [{"text": "Type Token Ratio (TTR)", "start_pos": 99, "end_pos": 121, "type": "METRIC", "confidence": 0.7491510411103567}]}, {"text": "The \"non-classic\" set includes more complex NLP-enabled features (coherence measured through LSA, MWE, n-grams, etc.) and features suggested by the structuro-cognitivist research (e.g., information about tense and variables based on orthographical neighbors).", "labels": [], "entities": []}, {"text": "As a second test, we trained a SVM model on each set and evaluated performances in a ten-fold crossvalidation.", "labels": [], "entities": []}, {"text": "For this test, we reduced the number of classic features by six to equal the number of predictors of the non-classic set.", "labels": [], "entities": []}, {"text": "Our hypothesis was the SVM model using non-classic features would outperform the classic set because the non-classic features bring richer information.", "labels": [], "entities": []}, {"text": "This assumption was not strictly confirmed as the non-classic set performed only slightly better than the classic set.", "labels": [], "entities": []}, {"text": "The difference in the correlation scores was small (0.01) and non-significant (t(9) = 0.49; p = 0.32), but the difference inaccuracy was larger (3.8%) and close to significance (t(9) = 1.50; p = 0.08).", "labels": [], "entities": [{"text": "correlation", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9784135818481445}]}, {"text": "Then, in an effort to pin down the source of the SVM gain that did not come out in the comparison above, we defined a SVM baseline model (b) that included only two typical features of the classic set: the average number of letter per word (NLM) and the average number of word per sentence (NMP).", "labels": [], "entities": []}, {"text": "Then, for each of the i remaining variables (44), we trained a model mi including three predictors: NLM, NMP, and i.", "labels": [], "entities": []}, {"text": "The difference between the correlation of the baseline model and that of the model mi was interpreted as the information gain carried by the feature i.", "labels": [], "entities": []}, {"text": "Therefore, for both sets, of cardinality N s , we computed: where R(m i ) is the multiple correlation of model mi . Our assumption was that, if the non-classic set brings in more varied information, every predictor should, on average, improve more the R of the baseline model, while the classic variables, more redundant with NLM and NP, would be less efficient.", "labels": [], "entities": []}, {"text": "In this test, the mean gain for R was 0.017 for the classic set and 0.022 for the non-classic set.", "labels": [], "entities": [{"text": "R", "start_pos": 32, "end_pos": 33, "type": "METRIC", "confidence": 0.9598588943481445}]}, {"text": "Although the difference was once more small, this test yielded a similar trend than the previous test.", "labels": [], "entities": []}, {"text": "As a final test, we compared the performance of the SVM model trained only on the \"classic\" set with the SVM trained on both sets.", "labels": [], "entities": []}, {"text": "In this case, the improvement was significant (t(9) = 3.82; p = 0.002) with accuracy rising from 37.5% to 49%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9996514320373535}]}, {"text": "Although this test does not help us decide on the nature of the gain as it could becoming just from the increased number of features, it shows that combining \"classic\" and \"non-classic\" variables is valuable.", "labels": [], "entities": []}, {"text": "In this section, we explore the hypothesis that AI models outperform classic formulas because they use better statistical algorithms.", "labels": [], "entities": []}, {"text": "We compare the performance of a\"classic\" algorithm, multiple linear regression, with the performance of a machine learning algorithm, in this case SVM.", "labels": [], "entities": []}, {"text": "Note that an SVMs have an advantage over linear regression for features non-linearly related with difficulty.", "labels": [], "entities": []}, {"text": "Bormuth showed that several classic features, especially those focusing on the word level, were indeed non-linear.", "labels": [], "entities": []}, {"text": "To control for linearity, we split the 46 features into a linear and a non-linear subset, using the Guilford's F test for linearity and an \u03b1 = 0.05.", "labels": [], "entities": [{"text": "Guilford's F test", "start_pos": 100, "end_pos": 117, "type": "METRIC", "confidence": 0.6240045130252838}]}, {"text": "This classification yielded two equal sets of 23 variables (see).", "labels": [], "entities": []}, {"text": "In, we report the performance of the four models in terms of R, accuracy, and adjacent accuracy.", "labels": [], "entities": [{"text": "R", "start_pos": 61, "end_pos": 62, "type": "METRIC", "confidence": 0.9987452030181885}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9996387958526611}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.795413613319397}]}, {"text": "Following,, we define \"adjacent accuracy\" as the proportion of predictions that were within one level of the assigned label in the corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.6300184726715088}]}, {"text": "Adjacent accuracy is closer to R as it is less sensitive to minor classification errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9632612466812134}, {"text": "R", "start_pos": 31, "end_pos": 32, "type": "METRIC", "confidence": 0.9972646236419678}]}, {"text": "Our results showed a contradictory pattern, yielding a different result depending on type of evalution: accuracy or Rand adjacent accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9997629523277283}, {"text": "Rand adjacent accuracy", "start_pos": 116, "end_pos": 138, "type": "METRIC", "confidence": 0.7651521563529968}]}, {"text": "With respect to accuracy scores, the SVM performed better in the classification task, with a significant performance gain for both linear (gain = 9%; t(9) = 2.42; p = 0.02) and non-linear features (gain = 8%; t(9) = 3.01; p = 0.007).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9987804293632507}]}, {"text": "On the other hand, the difference in R was non-significant for linear (gain = 0.06; t(9) = 0.80; p = 0.22) and even negative and close to significance for non-linear (gain = \u22120.05; t(9) = 1.61; p = 0.07).", "labels": [], "entities": [{"text": "R", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.9754103422164917}]}, {"text": "In the light of these results, linear regression (LR) appears to be as efficient as SVM accounting for variation in the dependant variable (their R 2 are pretty similar), but produces poorer predictions.", "labels": [], "entities": []}, {"text": "This is an interesting finding, which suggests that the contradictory results in prior literature with regard to performance of different readability models (see Section 2) might be related to the evaluation measure used., who compared linear and logistic regressions, found that the R of the linear model was significantly higher than the R of the logistic model (p < 0.01).", "labels": [], "entities": [{"text": "R", "start_pos": 284, "end_pos": 285, "type": "METRIC", "confidence": 0.9496231079101562}]}, {"text": "In contrast, the logistic model behaved significantly better (p < 0.01) in terms of adjacent accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.905643880367279}]}, {"text": "Similarly,, which used R as evaluation measure, reported that their preliminary results \"verified that regression performed better than classification\".", "labels": [], "entities": []}, {"text": "Once they compared linear regression and SVM regression, they noticed similar correlations for both techniques (respectively 0.7984 and 0.7915).", "labels": [], "entities": []}, {"text": "To conclude this section, our findings suggest that (1) linear regression and SVM are comparable in accounting for the variance of text difficulty and (2) SVM has significantly better accuracy scores than linear regression.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9971851706504822}]}, {"text": "In Experiment 2, we saw that \"non-classic\" features are slightly, but non-significantly, better than the \"classic\" features.", "labels": [], "entities": []}, {"text": "In Experiment 3, we saw that SVM performs better than linear regression when the evaluation is done by accuracy but both demonstrate similar explanatory power in accounting for the variation.", "labels": [], "entities": [{"text": "SVM", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9587820172309875}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9972060322761536}]}, {"text": "In this section, we report evaluation results for four models, derived by combining two sets of features, classic and non-classic, with two algorithms, linear regression and SVM.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The results are consistent with the findings in the previous sections.", "labels": [], "entities": []}, {"text": "When evaluated with accuracy scores SVM performs better with both classic (t(9) = 3.15; p = 0.006) and non-classic features (t(9) = 3.32; p = 0.004).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9994221925735474}, {"text": "SVM", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.8458095192909241}]}, {"text": "The larger effect obtained for the non-classic features might be due to an interaction, i.e., an SVM trained with non-classic features might be better at discriminating reading levels.", "labels": [], "entities": []}, {"text": "However, with respect to R, both algorithms are similar, with linear regression outperforming SVM in adjacent accuracy (non-significant).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.8776324987411499}]}, {"text": "Linear regression and SVM, then, appear to have equal explanatory power.", "labels": [], "entities": []}, {"text": "As regards the type of features, the explanatory power of both models seems to increase with nonclassic features as shown in the increased R, although significance is not reached (t(9) = 0.49; p = 0.32 for the regression and t(9) = 1.5; p = 0.08 for the SVM).", "labels": [], "entities": [{"text": "explanatory", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9598909616470337}, {"text": "significance", "start_pos": 151, "end_pos": 163, "type": "METRIC", "confidence": 0.9766160249710083}, {"text": "SVM", "start_pos": 254, "end_pos": 257, "type": "DATASET", "confidence": 0.821225643157959}]}], "tableCaptions": [{"text": " Table 2: List of the 46 features used by Fran\u00e7ois (2011b) in his model. The Spearman correlation reported here also  comes from this study.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 77, "end_pos": 97, "type": "METRIC", "confidence": 0.9553118646144867}]}, {"text": " Table 3: Multiple correlation coefficient (R), accuracy  and adjacent accuracy for linear regression and SVM  models, using the set of features either linearly or non  linearly related to difficulty.", "labels": [], "entities": [{"text": "Multiple correlation coefficient (R)", "start_pos": 10, "end_pos": 46, "type": "METRIC", "confidence": 0.8271733870108923}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9995235204696655}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.7894454598426819}]}, {"text": " Table 4: Multiple correlation coefficient (R), accuracy and adjacent accuracy for linear regression and SVM models  with either the classic or the non-classic set of predictors.", "labels": [], "entities": [{"text": "Multiple correlation coefficient (R)", "start_pos": 10, "end_pos": 46, "type": "METRIC", "confidence": 0.8216483294963837}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9995495676994324}, {"text": "adjacent", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9348040223121643}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.8393971920013428}]}]}