{"title": [{"text": "Combining Different Summarization Techniques for Legal Text", "labels": [], "entities": []}], "abstractContent": [{"text": "Summarization, like other natural language processing tasks, is tackled with a range of different techniques-particularly machine learning approaches, where human intuition goes into attribute selection and the choice and tuning of the learning algorithm.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9835067987442017}]}, {"text": "Such techniques tend to apply differently in different contexts, so in this paper we describe a hybrid approach in which a number of different summarization techniques are combined in a rule-based system using manual knowledge acquisition, where human intuition, supported by data, specifies not only attributes and algorithms, but the contexts where these are best used.", "labels": [], "entities": []}, {"text": "We apply this approach to automatic sum-marization of legal case reports.", "labels": [], "entities": [{"text": "sum-marization of legal case reports", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.7779726624488831}]}, {"text": "We show how a preliminary knowledge base, composed of only 23 rules, already outperforms competitive baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic summarization tasks are often addressed with statistical methods: a first type of approach, introduced by, involves using a set of features of different types to describe sentences, and supervised learning algorithms to learn an empirical model of how those features interact to identify important sentences.", "labels": [], "entities": [{"text": "Automatic summarization tasks", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7455754081408182}]}, {"text": "This kind of approach has been very popular in summarization; however the difficulty of this task often requires more complex representations, and different kinds of models to learn relevance in text have been proposed, such as discourse-based (Marcu, 1997) or network-based () models and many others.", "labels": [], "entities": [{"text": "summarization", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9915944933891296}]}, {"text": "Domain knowledge usually is present in the choice of features and algorithms, but it is still an open issue how best to capture the domain knowledge required to identify what is relevant in the text; manual approaches to build knowledge bases tend to be tedious, while automatic approaches require large amounts of training data and the result may still be inferior.", "labels": [], "entities": []}, {"text": "In this paper we present our approach to summarize legal documents, using knowledge acquisition to combine different summarization techniques.", "labels": [], "entities": [{"text": "summarize legal documents", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.9371064503987631}, {"text": "summarization", "start_pos": 117, "end_pos": 130, "type": "TASK", "confidence": 0.9657920598983765}]}, {"text": "In summarization, different kinds of information can betaken in account to locate important content, at the sentence level (e.g. particular terms or patterns), at the document level (e.g. frequency information, discourse information) and at the collection level (e.g. document frequencies or citation analysis); however, the way such attributes interact is likely to depend on the context of specific cases.", "labels": [], "entities": [{"text": "summarization", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.974418580532074}]}, {"text": "For this reason we have developed a set of methods for identifying important content, and we propose the creation of a Knowledge Base (KB) that specifies which content should be used in different contexts, and how this should be combined.", "labels": [], "entities": []}, {"text": "We propose to use the Ripple Down Rules (RDR)) methodology to build this knowledge base: RDR has already proven to be a very effective way of building KBs, had has been used successfully in several NLP task (see Section 2).", "labels": [], "entities": []}, {"text": "This kind of approach differs from the dominant supervised learning approach, in which we first annotate text to identify relevant fragments, and then we use supervised learning algorithms to learn a model; one example in the legal domain being the work of.", "labels": [], "entities": []}, {"text": "Our approach eliminates the need for separate manual annotation of text, as the rules are built by a human who judges the relevance of text and directly creates the set of rules as the one process, rather than annotating the text and then separately tuning the learning model.", "labels": [], "entities": []}, {"text": "We apply this approach to the summarization of legal case reports, a domain which has an increasing need for automatic text processing, to cope with the large body of documents that is case law.", "labels": [], "entities": [{"text": "summarization of legal case reports", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.9039124846458435}]}, {"text": "Automatic summarization can greatly enhance access to legal repositories; however, legal cases, rather than summaries, often contain lists of catchphrases: phrases that present the important legal points of a case.", "labels": [], "entities": [{"text": "summarization", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9624126553535461}]}, {"text": "The presence of catchphrases can aid research of case law, as they give a quick impression of what the case is about: \"the function of catchwords is to give a summary classification of the matters dealt within a case.", "labels": [], "entities": [{"text": "case law", "start_pos": 49, "end_pos": 57, "type": "TASK", "confidence": 0.6930177211761475}]}, {"text": "Their purpose is to tell the researcher whether there is likely to be anything in the case relevant to the research topic\".", "labels": [], "entities": []}, {"text": "For this reason, rather than constructing summaries, we aim at extracting catchphrases from the full text of a case report.", "labels": [], "entities": []}, {"text": "Examples of catchphrases from two case reports are shown in.", "labels": [], "entities": []}, {"text": "In this paper we present our approach towards automatic catchphrase extraction from legal case reports, using a knowledge acquisition approach according to which rules are manually created to combine a range of diverse methods to locate catchphrase candidates in the text.", "labels": [], "entities": [{"text": "automatic catchphrase extraction from legal case reports", "start_pos": 46, "end_pos": 102, "type": "TASK", "confidence": 0.8097119459084102}, {"text": "knowledge acquisition", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.6953151226043701}]}], "datasetContent": [{"text": "We use as the source of our data the legal database AustLII 1 , the Australasian Legal Information Institute (, one of the largest sources of legal material on the net, which provides free access to reports on court decisions in all major courts in Australia.", "labels": [], "entities": [{"text": "AustLII 1", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.7853076159954071}, {"text": "Australasian Legal Information Institute", "start_pos": 68, "end_pos": 108, "type": "DATASET", "confidence": 0.6458820328116417}]}, {"text": "We created an initial corpus of 2816 cases accessing case reports from the Federal Court of Australia, for the years 2007 to 2009, for which author-made catchphrases are given and extracted the full text and the catchphrases of every document.", "labels": [], "entities": []}, {"text": "Each document contains on average 221 sentences and 8.3 catchphrases.", "labels": [], "entities": []}, {"text": "In total we collected 23230 catchphrases, of which 15359 (92.7%) were unique, appearing only in one document in the corpus.", "labels": [], "entities": []}, {"text": "These catchphrases are used to evaluate our extracts using Rouge, as described in Section 4.", "labels": [], "entities": []}, {"text": "To have a more complete representation of these cases, we also included citation information.", "labels": [], "entities": []}, {"text": "Citation analysis has proven to be very useful in automatic summarization (.", "labels": [], "entities": [{"text": "Citation analysis", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.880389392375946}, {"text": "summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.7508878707885742}]}, {"text": "We downloaded citation data from LawCite 2 . It is a service provided by AustLII which, fora given case, lists cited cases and more recent cases that cite the case.", "labels": [], "entities": [{"text": "LawCite 2", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.8847797513008118}, {"text": "AustLII", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9043942093849182}]}, {"text": "We downloaded the full texts and the catchphrases (where available) from AustLII, of both cited (previous) cases and more recent cases that cite the current one (citing cases).", "labels": [], "entities": [{"text": "AustLII", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9597160220146179}]}, {"text": "Of the 2816 cases, 1904 are cited at least by one other case (on average by 4.82 other cases).", "labels": [], "entities": []}, {"text": "We collected the catchphrases of these citing cases, searched the full texts to extract the location where a citation is explicitly made, and extracted the containing paragraph(s).", "labels": [], "entities": []}, {"text": "For each of the 1904 cases we collected on average 21.17 citing sentences, and we extracted an average of 35.36 catchphrases (from one or more other documents).", "labels": [], "entities": []}, {"text": "From previous cases referenced by the judge, we extracted on average 67.41 catchphrases for each case.", "labels": [], "entities": []}, {"text": "We also extracted, using LawCite, references to any type of legislation made in the report.", "labels": [], "entities": [{"text": "LawCite", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.9882981181144714}]}, {"text": "We located in the full text the sentences where each section or Act is mentioned; then we accessed the full texts of the legislation on AustLII, and extracted the title of the sections (for example, if section 477 is mentioned in the text, we extract the corresponding title: CORPORATIONS ACT 2001 -SECT 477 Powers of liquidator).", "labels": [], "entities": [{"text": "AustLII", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.918638288974762}, {"text": "CORPORATIONS ACT 2001 -SECT 477 Powers of liquidator", "start_pos": 276, "end_pos": 328, "type": "DATASET", "confidence": 0.8070774575074514}]}, {"text": "Our dataset thus contains the initial 2816 cases with given catchphrases, and all cases related to them by incoming or outgoing citations, with catchphrases and citing sentences explicitly identified, and the references to Acts and sections of the law.", "labels": [], "entities": []}, {"text": "As it was not reasonable to involve legal experts in this sort of exploratory study, we looked fora simple way to evaluate candidate catchphrases automatically by comparing them with the authormade catchphrases from our AustLII corpus (considered as our \"gold standard\"), to quickly assess the performances of various methods on a large number of documents.", "labels": [], "entities": [{"text": "AustLII corpus", "start_pos": 220, "end_pos": 234, "type": "DATASET", "confidence": 0.9019477069377899}]}, {"text": "As our system extracts sentences from text as candidate catchphrases, we propose an evaluation method which is based on Rouge) scores between extracted sentences and given catchphrases.", "labels": [], "entities": []}, {"text": "This method was used also in (.", "labels": [], "entities": []}, {"text": "Rouge includes several measures to quantitatively compare system-generated summaries to human-generated summaries, counting the number of overlapping n-grams of various lengths, word pairs and word sequences between two or more summaries.", "labels": [], "entities": []}, {"text": "Somewhat different from the standard use of Rouge (which would involve comparing the whole block of catchphrases to the whole block of extracted sentences), we evaluated extracted sentences individually so that the utility of anyone catchphrase is minimally affected by the others, or by their particular order.", "labels": [], "entities": []}, {"text": "On the other hand we want to extract sentences that contain an entire individual catchphrase, while a sentence that contains small pieces of different catchphrases is not as useful.", "labels": [], "entities": []}, {"text": "We therefore compare each extracted sentence with each catchphrase individually, using Rouge.", "labels": [], "entities": []}, {"text": "If the recall (on the catchphrase) is higher than a threshold, the catchphrase-sentence pair is considered a match.", "labels": [], "entities": [{"text": "recall", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9989614486694336}]}, {"text": "For example if we have a 10-word catchphrase, and a 15 words candidate sentence, if they have 6 words in common we consider this as a match using Rouge-1 with a threshold of 0.5, but not a match with a threshold of 0.7 (requiring at least 7/10 words from the catchphrase to appear in the sentence).", "labels": [], "entities": []}, {"text": "Using other Rouge scores (Rouge-SU or Rouge-W), the order and sequence of tokens are also considered in defining a match.", "labels": [], "entities": []}, {"text": "In this way, once a matching criterion is defined, we can divide all the sentences in \"relevant\" sentences (those that match at least one catchphrase) and \"not relevant\" sentences (those that do not match any catchphrase).", "labels": [], "entities": []}, {"text": "Once the matches between single sentences and catchphrases are defined fora single document and a set of extracted (candidate) sentences, we can compute precision and recall as: The recall is the number of catchphrases matched by at least one extracted sentence, divided by the total number of catchphrases; the precision is the number of sentences extracted which match at least one catchphrase, divided by the number of extracted sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.999119222164154}, {"text": "recall", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.9994365572929382}, {"text": "recall", "start_pos": 182, "end_pos": 188, "type": "METRIC", "confidence": 0.9977013468742371}, {"text": "precision", "start_pos": 312, "end_pos": 321, "type": "METRIC", "confidence": 0.9987934827804565}]}, {"text": "This evaluation method gives us away to compare the performance of different extraction systems automatically, by giving a simple but reasonable measure of how many of the desired catchphrases are generated by the systems, and how many of the sentences extracted are useful.", "labels": [], "entities": []}, {"text": "This is different from the use of standard Rouge overall scores, where precision and recall do not relate to the number of catchphrases or sentences, but to the number of smaller units such as n-grams, skip-bigrams or sequences, which makes it more difficult to interpret the results.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.999035120010376}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9988810420036316}]}], "tableCaptions": [{"text": " Table 3: Performances measured using Rouge-1 with  threshold 0.5. SpD is the average number of extracted  sentences per document.", "labels": [], "entities": [{"text": "SpD", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9980716109275818}]}, {"text": " Table 4: Performances measured using Rouge-1 with  threshold 0.7. SpD is the average number of extracted  sentences per document.", "labels": [], "entities": [{"text": "SpD", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9981476068496704}]}]}