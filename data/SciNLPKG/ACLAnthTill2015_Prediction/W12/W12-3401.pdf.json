{"title": [{"text": "Probabilistic Lexical Generalization for French Dependency Parsing", "labels": [], "entities": [{"text": "Lexical Generalization", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.697818785905838}, {"text": "Parsing", "start_pos": 59, "end_pos": 66, "type": "TASK", "confidence": 0.45825880765914917}]}], "abstractContent": [{"text": "This paper investigates the impact on French dependency parsing of lexical generalization methods beyond lemmatization and morphological analysis.", "labels": [], "entities": [{"text": "French dependency parsing", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6290985743204752}]}, {"text": "A distributional thesaurus is created from a large text corpus and used for distributional clustering and WordNet automatic sense ranking.", "labels": [], "entities": [{"text": "distributional clustering", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.7319190502166748}, {"text": "WordNet automatic sense ranking", "start_pos": 106, "end_pos": 137, "type": "TASK", "confidence": 0.7380686700344086}]}, {"text": "The standard approach for lexical generalization in parsing is to map a word to a single generalized class, either replacing the word with the class or adding anew feature for the class.", "labels": [], "entities": [{"text": "lexical generalization in parsing", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.6131341755390167}]}, {"text": "We use a richer framework that allows for probabilistic generalization , with a word represented as a probability distribution over a space of generalized classes: lemmas, clusters, or synsets.", "labels": [], "entities": []}, {"text": "Probabilistic lexical information is introduced into parser feature vectors by modifying the weights of lexical features.", "labels": [], "entities": []}, {"text": "We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9715517163276672}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9513312578201294}, {"text": "French Treebank", "start_pos": 117, "end_pos": 132, "type": "DATASET", "confidence": 0.9937438368797302}]}], "introductionContent": [{"text": "In statistical, data-driven approaches to natural language syntactic parsing, a central problem is that of accurately modeling lexical relationships from potentially sparse counts within a training corpus.", "labels": [], "entities": [{"text": "natural language syntactic parsing", "start_pos": 42, "end_pos": 76, "type": "TASK", "confidence": 0.6985407471656799}]}, {"text": "Our particular interests are centered on reducing lexical data sparseness for linear classification approaches for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.8500943779945374}]}, {"text": "In these approaches, linear models operate over feature vectors that generally represent syntactic structure within a sentence, and feature templates are defined in part over the word forms of one or more tokens in a sentence.", "labels": [], "entities": []}, {"text": "Because treebanks used for training are often small, lexical features may appear relatively infrequently during training, especially for languages with richer morphology than English.", "labels": [], "entities": []}, {"text": "This may, in turn, impede the parsing model's ability to generalize well outside of its training set with respect to lexical features.", "labels": [], "entities": []}, {"text": "Past approaches for achieving lexical generalization in dependency parsing have used WordNet semantic senses in parsing experiments for English), and word clustering overlarge corpora in parsing experiments for English () as well as for French).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.750134289264679}]}, {"text": "These approaches map each word to a single corresponding generalized class (synset or cluster), and integrate generalized classes into parsing models in one of two ways: (i) the replacement strategy, where each word form is simply replaced with a corresponding generalized class; (ii) a strategy where an additional feature is created for the corresponding generalized class.", "labels": [], "entities": []}, {"text": "Our contribution in this paper is applying probabilistic lexical generalization, a richer framework for lexical generalization, to dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.8562695980072021}]}, {"text": "Each word form is represented as a categorical distribution over a lexical target space of generalized classes, for which we consider the spaces of lemmas, synsets, and clusters.", "labels": [], "entities": []}, {"text": "The standard single-mapping approach from previous work can be seen as a subcase: each categorical distribution assigns a probability of 1 to a single generalized class.", "labels": [], "entities": []}, {"text": "The method we use for introducing probabilistic information into a feature vector is based on that used by, who tested the use of probabilistic part-ofspeech (POS) tags through an NLP pipeline.", "labels": [], "entities": []}, {"text": "In this paper, we perform experiments for French that use the replacement strategy for integrating generalized classes into parsing models, comparing the single-mapping approach for lexical generalization with our probabilistic lexical generalization approach.", "labels": [], "entities": []}, {"text": "In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of.", "labels": [], "entities": [{"text": "French parsing of WordNet automatic sense ranking (ASR)", "start_pos": 60, "end_pos": 115, "type": "TASK", "confidence": 0.589748877286911}]}, {"text": "For clustering we deviate from most previous work, which has integrated Brown clusters) into parsing models, and instead use distributional lexical semantics to create both a distributional thesaurus -for probabilistic generalization in the lemma space and ASR calculationand to perform hierarchical agglomerative clustering (HAC).", "labels": [], "entities": [{"text": "ASR calculationand", "start_pos": 257, "end_pos": 275, "type": "TASK", "confidence": 0.9340603947639465}, {"text": "hierarchical agglomerative clustering (HAC)", "start_pos": 287, "end_pos": 330, "type": "TASK", "confidence": 0.6845879654089609}]}, {"text": "Though unlexicalized syntactic HAC clustering has been used to improve English dependency parsing, we provide first results on using distributional lexical semantics for French parsing.", "labels": [], "entities": [{"text": "English dependency parsing", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.5856690605481466}, {"text": "French parsing", "start_pos": 170, "end_pos": 184, "type": "TASK", "confidence": 0.6175234317779541}]}, {"text": "We also include an out-of-domain evaluation on medical and parliamentary text in addition to an in-domain evaluation.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the lexical target spaces used in this paper, as well as the method of integrating probabilistic lexical information into a feature vector for classification.", "labels": [], "entities": []}, {"text": "In Section 3 we discuss dependency structure and transition-based parsing.", "labels": [], "entities": [{"text": "dependency structure", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8678490817546844}, {"text": "transition-based parsing", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.5197542905807495}]}, {"text": "In Section 4 we present the experimental setup, which includes our parser implementation, the construction of our probabilistic lexical resources, and evaluation settings.", "labels": [], "entities": []}, {"text": "We report parsing results both in-domain and out-of-domain in Section 5, we provide a summary of related work in Section 6, and we conclude in Section 7.", "labels": [], "entities": [{"text": "parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9740715622901917}]}], "datasetContent": [{"text": "We now discuss the treebanks used for training and evaluation, the parser implementation and baseline settings, the construction of the probabilistic lexical resources, and the parameter tuning and evaluation settings.", "labels": [], "entities": []}, {"text": "We evaluate four lexical target space configurations against the baseline of lemmatization, tuning parameters using ten-fold cross-validation on the FTB training set.", "labels": [], "entities": [{"text": "FTB training set", "start_pos": 149, "end_pos": 165, "type": "DATASET", "confidence": 0.9671671191851298}]}, {"text": "The feature templates are the same as those in, with the difference that features involving lemmas are modified by the probabilistic feature generalization technique described in Section 2.4, using the appropriate categorical distributions.", "labels": [], "entities": []}, {"text": "In all configurations, we exclude the French auxiliary verbs\u00eatreverbs\u02c6verbs\u00eatre and avoir from participation in lexical generalization, and we replace proper nouns with a special lemma . Below we describe the tuned parameters for each configuration.", "labels": [], "entities": [{"text": "lexical generalization", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.7400392591953278}]}], "tableCaptions": [{"text": " Table 4: Parsing model lexical features (rounded to near- est thousand) and average lexical feature use in classifi- cation instances across different training and evaluation  sets, for the baseline (Lemmas) and four lexical general- ization configurations (PKNL, RC, PKPS, and RS).", "labels": [], "entities": []}]}