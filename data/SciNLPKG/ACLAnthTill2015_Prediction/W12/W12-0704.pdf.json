{"title": [{"text": "Clustered Word Classes for Preordering in Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6813092231750488}]}], "abstractContent": [{"text": "Clustered word classes have been used in connection with statistical machine translation , for instance for improving word alignments.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.6508384446303049}, {"text": "word alignments", "start_pos": 118, "end_pos": 133, "type": "TASK", "confidence": 0.738950103521347}]}, {"text": "In this work we investigate if clustered word classes can be used in a pre-ordering strategy, where the source language is reordered prior to training and translation.", "labels": [], "entities": []}, {"text": "Part-of-speech tagging has previously been successfully used for learning reordering rules that can be applied before training and translation.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7757278084754944}, {"text": "translation", "start_pos": 131, "end_pos": 142, "type": "TASK", "confidence": 0.9552949070930481}]}, {"text": "We show that we can use word clusters for learning rules, and significantly improve on a baseline with only slightly worse performance than for standard POS-tags on an English-German translation task.", "labels": [], "entities": []}, {"text": "We also show the usefulness of the approach for the less-resourced language Haitian Creole, for translation into English, where the suggested approach is significantly better than the baseline.", "labels": [], "entities": [{"text": "translation", "start_pos": 96, "end_pos": 107, "type": "TASK", "confidence": 0.9797661900520325}]}], "introductionContent": [{"text": "Word order differences between languages are problematic for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 61, "end_pos": 98, "type": "TASK", "confidence": 0.8137970268726349}]}, {"text": "If the word orders of two languages have large differences, the standard methods do not tend to work well, with difficulties in many steps such as word alignment and modelling of reordering in the decoder.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 147, "end_pos": 161, "type": "TASK", "confidence": 0.7846243977546692}]}, {"text": "This can be addressed by applying a preordering method, that is, to reorder the source side of the corpus to become similar to the target side, prior to training and translation.", "labels": [], "entities": []}, {"text": "The rules used for reordering are generally based on some kind of linguistic annotation, such as partof-speech tags (POS-tags).", "labels": [], "entities": []}, {"text": "For many languages in the world, so called lessresourced languages, however, part-of-speech taggers, or part-of-speech tagged corpora that can be used for training a tagger, are not available.", "labels": [], "entities": []}, {"text": "In this study we investigate if it is possible to use unsupervised POS-tags, in the form of clustered word classes, as a basis for learning reordering rules for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9878007173538208}]}, {"text": "Unsupervised tagging methods can be used for any language where a corpus is available.", "labels": [], "entities": []}, {"text": "This means that we can potentially benefit from preordering even for languages where taggers are available.", "labels": [], "entities": []}, {"text": "We present experiments on two data sets.", "labels": [], "entities": []}, {"text": "First an English-German test set, where we can compare the results of clustered word classes with standard tags.", "labels": [], "entities": []}, {"text": "We show that both types of tags beat a baseline without preordering, and that clustered tags perform nearly as well as standard tags.", "labels": [], "entities": []}, {"text": "English and German is an interesting case for reordering experiments, since there are both long distance movement of verbs and local word order differences, for instance due to differences in adverb placements.", "labels": [], "entities": []}, {"text": "We also apply the method to translation from the less-resourced language Haitian Creole into English, and show that it leads to an improvement over a baseline.", "labels": [], "entities": [{"text": "translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9630098342895508}]}, {"text": "The differences in word order between these two languages are smaller than for English-German.", "labels": [], "entities": []}, {"text": "Besides potentially improving SMT for lessresourced languages, the presented approach can also be used as an extrinsic evaluation method for unsupervised POS-tagging methods.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9959226846694946}]}, {"text": "This is especially useful for the task of word class clustering which is hard to evaluate.", "labels": [], "entities": [{"text": "word class clustering", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7080410917599996}]}], "datasetContent": [{"text": "We conducted experiments for two language pairs, English-German and Haitian CreoleEnglish.", "labels": [], "entities": []}, {"text": "We always applied the reordering rules to the translation input, creating a lattice of possible reorderings as input to the decoder.", "labels": [], "entities": []}, {"text": "For the training data we applied two strategies.", "labels": [], "entities": []}, {"text": "As the first option we used training data from the baseline system with original word order.", "labels": [], "entities": []}, {"text": "As the second option we reordered the training data as well, using the learnt reordering rules to create reordering lattices for the training data, from which we extracted the 1-best reordering, as suggested by.", "labels": [], "entities": []}, {"text": "For the supervised tagging of the English source side we use a commercial functional dependency parser.", "labels": [], "entities": []}, {"text": "The main reason for using a parser instead of a tagger was that we wanted to explore the effect of different tagging schemes, which was available from this parser.", "labels": [], "entities": []}, {"text": "An example of a tagged English text can be seen in.", "labels": [], "entities": []}, {"text": "In this work we used four types of tags extracted from the parser output, part-of-speech tags (pos), dependency tags (dep), functional tags (func) and shallow syntax tags (syntax).", "labels": [], "entities": []}, {"text": "The dependency tags consist of the dependency label of the word and the POS-tag of its dependent.", "labels": [], "entities": []}, {"text": "For the example in, the sequence of dependency tags is: main TOP mod N attr N pcomp PREP.", "labels": [], "entities": [{"text": "main TOP mod N attr N pcomp PREP", "start_pos": 56, "end_pos": 88, "type": "METRIC", "confidence": 0.7514804042875767}]}, {"text": "The other tag types are directly exemplified in Table 1.", "labels": [], "entities": []}, {"text": "The tagsets have different sizes, as shown in.", "labels": [], "entities": []}, {"text": "For the unsupervised tags, we used clustered word classes obtained using the mkcls software, 2 which implements the approach of Och (1999).", "labels": [], "entities": []}, {"text": "We explored three different numbers of clusters, 50, 125, and 625.", "labels": [], "entities": []}, {"text": "The clustering was performed on the same corpus as the SMT training.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9934290051460266}]}, {"text": "The translation system used is a standard phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.8434099555015564}]}, {"text": "The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++, which are then symmetrized by the grow-diag-final-and method ( . From this many-to-many alignment, consistent phrases of up to length 7 were extracted. A 5-gram language model was used, produced by SRILM).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 314, "end_pos": 319, "type": "DATASET", "confidence": 0.9348797798156738}]}, {"text": "For training and decoding we used the Moses toolkit () and the feature weights were optimized using minimum error rate training  The baseline systems were trained using no additional preordering, only a distance-based reordering penalty for modelling reordering.", "labels": [], "entities": []}, {"text": "For the Haitian Creole-English experiments we also added a lexicalized reordering model (), both to the baseline and to the reordered systems.", "labels": [], "entities": []}, {"text": "For the English-German experiments, the translation system was trained and tested using apart of the Europarl corpus ().", "labels": [], "entities": [{"text": "translation", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9605743288993835}, {"text": "Europarl corpus", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.9932239055633545}]}, {"text": "The training part contained 439513 sentences and 9.4 million words.", "labels": [], "entities": []}, {"text": "Sentences longer than 40 words were filtered out.", "labels": [], "entities": []}, {"text": "The test set has 2000 sentences and the development set has 500 sentences.", "labels": [], "entities": []}, {"text": "For the Haitian Creole-English experiments we used the SMS corpus released for WMT11).", "labels": [], "entities": [{"text": "SMS corpus released", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.7852874596913656}, {"text": "WMT11", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.5086014270782471}]}, {"text": "The corpus contains 17192 sentences and 352326 words.", "labels": [], "entities": []}, {"text": "The test and development data both contain 900 sentences each.", "labels": [], "entities": []}, {"text": "Since we know of no POS-tagger for Haitian Creole, we only compare the clustered result to a baseline system.", "labels": [], "entities": []}, {"text": "Reordering rules were extracted from the same corpora that were used for training the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9933184385299683}]}, {"text": "The word alignments needed for reordering were created using GIZA++, an implementation of the IBM models ( of alignment, which is trained in a fully unsupervised manner based on the EM algorithm.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6874126046895981}]}, {"text": "shows the number of rules, and the average number of paths for each sentence in the test data lattice, using each tagset.", "labels": [], "entities": []}, {"text": "For the standard tagsets the number of rules is relatively constant, despite the fact that the number of tags in the tagsets are quite different.", "labels": [], "entities": []}, {"text": "For the clustered word classes, there are slightly fewer rules with 50 classes than for the standard tags, and the number of rules decreases with a higher number of classes.", "labels": [], "entities": []}, {"text": "For the average number of lattice paths per sentence, there are some differences for the standard tags, but it is not related to tagset size.", "labels": [], "entities": []}, {"text": "Again, the clustering with 50 classes has a similar number as the standard classes, but here there is a sharp decrease of lattice paths with a higher number of classes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1.  In this work we used four types of tags extracted  from the parser output, part-of-speech tags (pos),  dependency tags (dep), functional tags (func) and  shallow syntax tags (syntax). The dependency  tags consist of the dependency label of the word  and the POS-tag of its dependent. For the exam- ple in", "labels": [], "entities": []}, {"text": " Table 3: Translation results for English-German. Sta- tistically significant differences from baseline scores  are marked * (p < 0.05), ** (p < 0.01).", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8699249625205994}]}, {"text": " Table 4: Number of classes for Haitian Creole, number  of rules extracted for each tagset, and average numbers  of paths per sentence in the testset lattice using each  tagset to create rules", "labels": [], "entities": []}, {"text": " Table 5: Translation results for Haitian Creole- English. Statistically significant differences from  baseline BLEU score are marked ** (p < 0.01).", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9460217952728271}, {"text": "BLEU score", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9708268344402313}]}]}