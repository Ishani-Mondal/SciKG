{"title": [], "abstractContent": [{"text": "Building shallow semantic representations from text corpora is the first step to perform more complex tasks such as text entailment, enrichment of knowledge bases, or question answering.", "labels": [], "entities": [{"text": "text entailment", "start_pos": 116, "end_pos": 131, "type": "TASK", "confidence": 0.750015377998352}, {"text": "question answering", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.8635986149311066}]}, {"text": "Open Information Extraction (OIE) is a recent unsupervised strategy to extract billions of basic assertions from massive corpora, which can be considered as being a shallow semantic representation of those corpora.", "labels": [], "entities": [{"text": "Open Information Extraction (OIE)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7877943515777588}]}, {"text": "In this paper, we propose anew multilingual OIE system based on robust and fast rule-based dependency parsing.", "labels": [], "entities": [{"text": "rule-based dependency parsing", "start_pos": 80, "end_pos": 109, "type": "TASK", "confidence": 0.6113864978154501}]}, {"text": "It permits to extract more precise assertions (verb-based triples) from text than state of the art OIE systems, keeping a crucial property of those systems: scaling to Web-size document collections.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is an increasing interest in capturing shallow semantic representations from large amounts of text, with the aim of elaborating more complex semantic tasks involved in text understanding, such as textual entailment, filling knowledge gaps in text, or integration of text information into background knowledge bases.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 174, "end_pos": 192, "type": "TASK", "confidence": 0.7126210629940033}, {"text": "textual entailment", "start_pos": 202, "end_pos": 220, "type": "TASK", "confidence": 0.7198341935873032}]}, {"text": "Two recent approaches to text understanding are interested in shallow semantics: Machine Reading () and Learning by Reading (.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8144022226333618}, {"text": "Machine Reading", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.7474277019500732}]}, {"text": "Both approaches aim at understanding text by starting with a very basic representation of the facts conveyed by the input text.", "labels": [], "entities": []}, {"text": "In addition, they rely on unsupervised strategies.", "labels": [], "entities": []}, {"text": "There are, however, two significant differences between Machine Reading and Learning by Reading: The first difference concerns the basic representation required at the beginning of the understanding process.", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.746277391910553}]}, {"text": "While Machine Reading is focused on fixed structures (triples), constituted by a relation (a verb or verb phrase) and two arguments, in Learning by Reading the text is represented by means of more flexible predicateargument structures (n-tuples) derived from syntactic dependency trees.", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.800573855638504}]}, {"text": "In Learning by Reading, on the one hand, relations with more than two arguments are also extracted, and on the other, relations are not restricted to verb phrases but to whatever relation expressed by a dependency based triple, (head, relation, modifier), also called Basic Element ().", "labels": [], "entities": []}, {"text": "The second difference is related to the notion of text domain.", "labels": [], "entities": []}, {"text": "Whereas Machine Reading works on open relations and unrestricted topics and domains, Learning by Reading prefers being focused on domainspecific texts in order to build a semantic model of a particular topic.", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7684717178344727}]}, {"text": "One of the major contributions of Machine Reading is the development of an extraction paradigm, called Open Information Extraction (OIE), which aims at extracting a large set of verbbased triples (or assertions) from unrestricted text.", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.8257820010185242}, {"text": "Open Information Extraction (OIE)", "start_pos": 103, "end_pos": 136, "type": "TASK", "confidence": 0.7647097706794739}]}, {"text": "An OIE system reads in sentences and rapidly extracts one or more textual assertions, consisting in a verb relation and two arguments, which try to capture the main relationships in each sentence (.", "labels": [], "entities": []}, {"text": "Unlike most relation extraction methods which are focused on a predefined set of target relations, OIE is not limited to a small set of target relations known in advance, but extracts all types of (verbal) binary relations found in the text.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7631678581237793}]}, {"text": "The OIE system with best performance, called, is a logistic regression classifier that takes as input PoS-tagged and NP-chunked sentences.", "labels": [], "entities": []}, {"text": "So, it only requires shallow syntactic features to generate semantic relations, guaranteeing robustness and scalability with the size of the corpus.", "labels": [], "entities": []}, {"text": "One of the main critics within the OIE paradigm against dependency based methods, such as Learning by Reading, concerns the computational cost associated with rich syntactic features.", "labels": [], "entities": []}, {"text": "Dependency parsing could improve precision and recall over shallow syntactic features, but at the cost of extraction speed ).", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6561635732650757}, {"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.998890221118927}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9982122182846069}]}, {"text": "In order to operate at the Web scale, OIE systems needs to be very fast and efficient.", "labels": [], "entities": []}, {"text": "In this paper, we describe an OIE method to generate verb-based triples by taking into account the positive properties of the two traditions: considering Machine Reading requirements, our system is efficient and fast guaranteeing scalability as the corpus grows.", "labels": [], "entities": []}, {"text": "And considering ideas behind Learning by Reading, we use a dependency parser in order to obtain fine-grained information (e.g., internal heads and dependents) on the arguments and relations extracted from the text.", "labels": [], "entities": []}, {"text": "In addition, we make extraction multilingual.", "labels": [], "entities": []}, {"text": "More precisely, our system has the following properties: \u2022 Unsupervised extraction of triples represented at different levels of granularity: surface forms and dependency level.", "labels": [], "entities": []}, {"text": "\u2022 Multilingual extraction (English, Spanish, Portuguese, and Galician) by making use of a multilingual rule-based parser, called DepPattern ().", "labels": [], "entities": [{"text": "Multilingual extraction", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.8260926902294159}]}, {"text": "Our claim is that it is possible to perform Open Information Extraction by making use of very conventional tools, namely rule-based dependency analysis and simple post-processing extraction rules.", "labels": [], "entities": [{"text": "Open Information Extraction", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.7111201882362366}]}, {"text": "In addition, we also show that we can deal with knowledge-rich syntactic information while remaining scalable.", "labels": [], "entities": []}, {"text": "This article is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces previous work on OIE: in particular it describes three of the best known OIE systems up to date.", "labels": [], "entities": [{"text": "OIE", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.656722366809845}]}, {"text": "Next, in Section 3, the proposed method is described in detail.", "labels": [], "entities": []}, {"text": "Then, some experiments are performed in Section 4, where our OIE system is compared against ReVerb.", "labels": [], "entities": [{"text": "ReVerb", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.8908858895301819}]}, {"text": "In 5, we sketch some applications that use the output of our OIE system, and finally, conclusions and current work are addressed in 6.", "labels": [], "entities": [{"text": "OIE system", "start_pos": 61, "end_pos": 71, "type": "DATASET", "confidence": 0.8745108544826508}]}], "datasetContent": [{"text": "We compare Dep-OE to ReVerb 5 , regarding the quantity and quality of extracted triples just in English, since ReVerb only can be applied on this language.", "labels": [], "entities": []}, {"text": "Each system is given a set of sentences as input, and returns a set of triples as output.", "labels": [], "entities": []}, {"text": "A test set of 200 sentences was created by randomly selecting sentences from the English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 81, "end_pos": 98, "type": "DATASET", "confidence": 0.824183315038681}]}, {"text": "Each test sentence was independently examined by two judges in order to, on the one hand, identify the triples actually contained in the sentence, and on the other, evaluate each extraction as corrector incorrect.", "labels": [], "entities": []}, {"text": "Incoherent and uninformative extractions were considered as incorrect.", "labels": [], "entities": []}, {"text": "Given the sentence \"The relationship between the Taliban and Bin Laden was close\", an example of incoherent extraction is: Uninformative extractions occur when critical information is omitted, for instance, when one of 5 http://reverb.cs.washington.edu/ the arguments is truncated.", "labels": [], "entities": []}, {"text": "Given the sentence \"FBI examined the relationship between Bin Laden and the Taliban\", an OIE system could return a truncated triple: (FBI, examined the relationship between, Bin Landen) We follow similar criteria to those defined in previous OIE evaluations . Concerning the decisions taken by the judges on the extractions made by the systems, the judges reached a very high agreement, 93%, with an agreement score of \u03ba = 0.83.", "labels": [], "entities": [{"text": "agreement score", "start_pos": 400, "end_pos": 415, "type": "METRIC", "confidence": 0.9851457476615906}]}, {"text": "They also reached a high agreement, 86%, with regard to the number of triples (gold standard) found in the test sentences.", "labels": [], "entities": [{"text": "number of triples (gold standard)", "start_pos": 60, "end_pos": 93, "type": "METRIC", "confidence": 0.6851321884563991}]}, {"text": "The precision of a system is the number of extractions returned as correct by the system divided by the number of returned extractions.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9987168312072754}]}, {"text": "Recall is the number of extractions returned as correct by the system divided by the number of triples identified by the judges (i.e., the size of the gold standard).", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9932969212532043}]}, {"text": "Moreover, to compare our rule-based system DepOE to ReVerb, we had to select a particular threshold restricting the extractions made by ReVerb.", "labels": [], "entities": [{"text": "ReVerb", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.8862475752830505}, {"text": "ReVerb", "start_pos": 136, "end_pos": 142, "type": "DATASET", "confidence": 0.9308478236198425}]}, {"text": "Let us note that this extractor is a logistic regression classifier that assign confidence scores to its extractions.", "labels": [], "entities": []}, {"text": "We computed precision and recall for many threshold and selected that giving rise to the best f-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9992954730987549}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.999110996723175}]}, {"text": "Such a threshold was 0.15.", "labels": [], "entities": []}, {"text": "So, we compare DepOE to the results given by ReVerb for those extractions whose confidence score is higher than 0.15.", "labels": [], "entities": [{"text": "DepOE", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.4806581735610962}, {"text": "ReVerb", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.9015141129493713}, {"text": "confidence score", "start_pos": 80, "end_pos": 96, "type": "METRIC", "confidence": 0.9536092877388}]}, {"text": "As it was done in previous OIE evaluations, the judges evaluated two different aspects of the extraction: \u2022 how well the system identify correct relation phrases, \u2022 the full extraction task, i.e., whether the system identifies correct triples (both the relation and its arguments).", "labels": [], "entities": [{"text": "OIE", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.541378915309906}]}, {"text": "Figures 2 and 3 represent the score average obtained by the two judges.", "labels": [], "entities": []}, {"text": "They show that DepOE system is more precise than ReVerb.", "labels": [], "entities": [{"text": "ReVerb", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.829719603061676}]}, {"text": "This is clear in the full extraction task, where DepOE achieves 68% precision while ReVerb reaches 52%.", "labels": [], "entities": [{"text": "full extraction task", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7007274826367696}, {"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9993689656257629}]}, {"text": "By contrast, as it was expected, DepOE has lower recall because of the low coverage of the grammars it depends on.", "labels": [], "entities": [{"text": "DepOE", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8083713054656982}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9994279742240906}]}, {"text": "Regarding f-score, DepOE performs better than ReVerb in the full extraction task, but when only relations are considered, ReVerb achieves the highest score.", "labels": [], "entities": []}, {"text": "We found that most of the incorrect extractions returned by the two systems where cases where the relation phrase was correctly identified, but not one of the arguments.", "labels": [], "entities": []}, {"text": "However, there are significant differences between the two systems concerning the type of problems arising in argument identification.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.8387481272220612}]}, {"text": "The most common errors of ReVerb are both: incorrect identification of the first argument (arg1) and extraction of only a truncated part of the second argument (arg2), as in the case of coordinating conjunctions.", "labels": [], "entities": []}, {"text": "These two problems are crucial for ReVerb since more than 60% of incorrect extractions were cases with incorrect arguments and correct relations.", "labels": [], "entities": []}, {"text": "DepOE has more precise extractions of the two arguments, in particular of arg1, since the parser is able to correctly identify the subject.", "labels": [], "entities": [{"text": "DepOE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8828167915344238}, {"text": "arg1", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9626767039299011}]}, {"text": "Nevertheless, it also produces many truncated arg2.", "labels": [], "entities": [{"text": "arg2", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.8898516297340393}]}, {"text": "Let us see an example.", "labels": [], "entities": []}, {"text": "Given the sentence \"Cities and towns in Romania can have the status either of municipiu or oras\", ReVerb was notable to identify the correct arg1 and returned a truncated arg2: (Romania, can have, the status) DepOE correctly identified the subject (arg1) but also failed to return the correct arg2: (Cities and towns in Romania, can have, the status) In general, when DepOE fails to correctly identify an argument, it is often trivial to find the reason of the problem.", "labels": [], "entities": []}, {"text": "In the example above, arg2 was truncated because the English grammar has not any specific rule linking the particle \"either\" to a coordinate expression.", "labels": [], "entities": [{"text": "arg2", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9894529581069946}]}, {"text": "So, the improvement of DepOE depends on improving the grammars it is based on.", "labels": [], "entities": [{"text": "DepOE", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.7205359935760498}]}, {"text": "Besides the low coverage of the grammar, there are other sources of problems concerning the correct identification of arguments.", "labels": [], "entities": [{"text": "coverage", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9592798948287964}]}, {"text": "In particular, it is worth mentioning that the English version of DepOE is not provided with an efficient Named Entity Recognition system.", "labels": [], "entities": [{"text": "English version of DepOE", "start_pos": 47, "end_pos": 71, "type": "DATASET", "confidence": 0.5513864681124687}, {"text": "Named Entity Recognition", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.5966630776723226}]}, {"text": "This makes it difficult to correctly identify multiword arguments with Named Entities, quantities, measures, and dates.", "labels": [], "entities": []}, {"text": "Such a problem was partially solved by the use of FreeLing in the Portuguese, Spanish, and Galician DepOE versions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of sentences and triples from four  Wikipedias", "labels": [], "entities": []}]}