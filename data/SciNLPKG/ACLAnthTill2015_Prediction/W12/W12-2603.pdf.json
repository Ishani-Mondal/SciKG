{"title": [{"text": "Machine Translation for Multilingual Summary Content Evaluation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7406970858573914}, {"text": "Multilingual Summary Content Evaluation", "start_pos": 24, "end_pos": 63, "type": "TASK", "confidence": 0.7806316912174225}]}], "abstractContent": [{"text": "The multilingual summarization pilot task at TAC'11 opened a lot of problems we are facing when we try to evaluate summary quality in different languages.", "labels": [], "entities": [{"text": "multilingual summarization", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.5476206839084625}, {"text": "TAC'11", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.7641898989677429}]}, {"text": "The additional language dimension greatly increases annotation costs.", "labels": [], "entities": []}, {"text": "For the TAC pilot task English articles were first translated to other 6 languages, model summaries were written and submitted system summaries were evaluated.", "labels": [], "entities": [{"text": "TAC pilot task English articles", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.5398094832897187}]}, {"text": "We start with the discussion whether ROUGE can produce system rankings similar to those received from manual summary scoring by measuring their correlation.", "labels": [], "entities": []}, {"text": "We study then three ways of projecting summaries to a different language: projection through sentence alignment in the case of parallel corpora, simple summary translation and summarizing machine translated articles.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7091826498508453}, {"text": "summary translation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.7167991101741791}, {"text": "summarizing machine translated articles", "start_pos": 176, "end_pos": 215, "type": "TASK", "confidence": 0.89724300801754}]}, {"text": "Building such summaries gives opportunity to run additional experiments and reinforce the evaluation.", "labels": [], "entities": []}, {"text": "Later, we investigate whether an evaluation based on machine translated models can perform close to an evaluation based on original models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluation of automatically produced summaries in different languages is a challenging problem for the summarization community, because human efforts are multiplied to create model summaries for each language.", "labels": [], "entities": [{"text": "summarization", "start_pos": 103, "end_pos": 116, "type": "TASK", "confidence": 0.977922797203064}]}, {"text": "Unavailability of parallel corpora suitable for news summarization adds even another annotation load because documents need to be translated to other languages.", "labels": [], "entities": [{"text": "news summarization", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7242526561021805}]}, {"text": "At the last TAC'11 campaign, six research groups spent a lot of work on creating evaluation resources in seven languages (.", "labels": [], "entities": []}, {"text": "Thus compared to the monolingual evaluation, which requires writing model summaries and evaluating outputs of each system by hand, in the multilingual setting we need to obtain translations of all documents into the target language, write model summaries and evaluate the peer summaries for all the languages.", "labels": [], "entities": []}, {"text": "In the last fifteen years, research on Machine Translation (MT) has made great strides allowing human beings to understand documents written in various languages.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.8727843761444092}]}, {"text": "Nowadays, on-line services such as Google Translate and Bing Translator 1 can translate text into more than 50 languages showing that MT is not a pipe-dream.", "labels": [], "entities": [{"text": "MT", "start_pos": 134, "end_pos": 136, "type": "TASK", "confidence": 0.9919671416282654}]}, {"text": "In this paper we investigate how machine translation can be plugged in to evaluate quality of summarization systems, which would reduce annotation efforts.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8028172552585602}, {"text": "summarization", "start_pos": 94, "end_pos": 107, "type": "TASK", "confidence": 0.9645426273345947}]}, {"text": "We also discuss projecting summaries to different languages with the aim to reinforce the evaluation procedure (e.g. obtaining additional peers for comparison in different language or studying their language-independence).", "labels": [], "entities": []}, {"text": "This paper is structured as follows: after discussing the related work in section 2, we give a short overview of the TAC'11 multilingual pilot task (section 3).", "labels": [], "entities": [{"text": "TAC'11 multilingual pilot task", "start_pos": 117, "end_pos": 147, "type": "TASK", "confidence": 0.48838113993406296}]}, {"text": "We compare average model and system manual scores and we also study ROUGE correlation to the manual scores.", "labels": [], "entities": [{"text": "ROUGE correlation", "start_pos": 68, "end_pos": 85, "type": "METRIC", "confidence": 0.8964601159095764}]}, {"text": "We run our experiments on a subset of languages of the TAC multilingual task corpus (English, French and Czech).", "labels": [], "entities": [{"text": "TAC multilingual task corpus", "start_pos": 55, "end_pos": 83, "type": "DATASET", "confidence": 0.604838602244854}]}, {"text": "Section 4 introduces our translation system.", "labels": [], "entities": [{"text": "translation", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.9836071729660034}]}, {"text": "We mention its translation quality for language pairs used later in this study.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9488734602928162}]}, {"text": "Then we move onto the problem of projecting summaries to different languages in section 5.", "labels": [], "entities": []}, {"text": "We discuss three approaches: projecting summary through sentence alignment in a parallel corpus, translating a summary, and summarizing translated source texts.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7114096581935883}, {"text": "translating a summary", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.8817649284998575}, {"text": "summarizing translated source texts", "start_pos": 124, "end_pos": 159, "type": "TASK", "confidence": 0.9012393057346344}]}, {"text": "Then, we try to answer the question whether using translated models produces similar system rankings as when using original models (section 6), accompanied by a discussion of discriminative power difference and cross-language model comparison.", "labels": [], "entities": [{"text": "cross-language model comparison", "start_pos": 211, "end_pos": 242, "type": "TASK", "confidence": 0.6783074537913004}]}], "datasetContent": [{"text": "When we look at the manually assigned grades we see that there is a clear gap between human and automatic summaries (see the first two rows in table 1).", "labels": [], "entities": []}, {"text": "While the average grade for models were always over 4, peers were graded lower by 33% for English and by 54% for French and Czech.", "labels": [], "entities": []}, {"text": "However, there were 5 systems for English and 1 system for French which were not significantly worse than at least one model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average ROUGE-2 and ROUGE-SU4 scores for models and peers, and their correlation to the manual  evaluation (grades). We report levels of significance (p) for two-tailed test. Cells with missing p-values denote non- significant correlations (p > .1).", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.8757320046424866}, {"text": "ROUGE-SU4", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9565657377243042}]}, {"text": " Table 2: ROUGE results of translated summaries, evaluated against target language models (e.g., cz\u2192en against  English models).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9756317734718323}]}, {"text": " Table 4: ROUGE results of using translated model summaries, which evaluate both peer and model summaries in the  particular language.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9702870845794678}]}]}