{"title": [{"text": "Opinum: statistical sentiment analysis for opinion classification", "labels": [], "entities": [{"text": "statistical sentiment analysis", "start_pos": 8, "end_pos": 38, "type": "TASK", "confidence": 0.8174781997998556}, {"text": "opinion classification", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.804335206747055}]}], "abstractContent": [{"text": "The classification of opinion texts in positive and negative can be tackled by evaluating separate key words but this is a very limited approach.", "labels": [], "entities": []}, {"text": "We propose an approach based on the order of the words without using any syntactic and semantic information.", "labels": [], "entities": []}, {"text": "It consists of building one probabilistic model for the positive and another one for the negative opinions.", "labels": [], "entities": []}, {"text": "Then the test opinions are compared to both models and a decision and confidence measure are calculated.", "labels": [], "entities": []}, {"text": "In order to reduce the complexity of the training corpus we first lem-matize the texts and we replace most named-entities with wildcards.", "labels": [], "entities": []}, {"text": "We present an accuracy above 81% for Spanish opinions in the financial products domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9985700845718384}]}], "introductionContent": [{"text": "Most of the texts written by humans reflect some kind of sentiment.", "labels": [], "entities": []}, {"text": "The interpretation of these sentiments depend on the linguistic skills and emotional intelligence of both the author and the reader, but above all, this interpretation is subjective to the reader.", "labels": [], "entities": []}, {"text": "They don't really exist in a string of characters, for they are subjective states of mind.", "labels": [], "entities": []}, {"text": "Therefore sentiment analysis is a prediction of how most readers would react to a given text.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9443415403366089}]}, {"text": "There are texts which intend to be objective and texts which are intentionally subjective.", "labels": [], "entities": []}, {"text": "The latter is the case of opinion texts, in which the authors intentionally use an appropriate language to express their positive or negative sentiments about something.", "labels": [], "entities": []}, {"text": "In this paper we work on the classification of opinions in two classes: those expressing positive sentiment (the author is in favour of something) and those expressing negative sentiment, and we will refer to them as positive opinions and negative opinions.", "labels": [], "entities": []}, {"text": "Sentiment analysis is possible thanks to the opinions available online.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9764755368232727}]}, {"text": "There are vast amounts of text in fora, user reviews, comments in blogs and social networks.", "labels": [], "entities": []}, {"text": "It is valuable for marketing and sociological studies to analyse these freely available data on some definite subject or entity.", "labels": [], "entities": []}, {"text": "Some of the texts available do include opinion information like stars, or recommend-or-not, but most of them do not.", "labels": [], "entities": []}, {"text": "A good corpus for building sentiment analysis systems would be a set of opinions separated by domains.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8690662682056427}]}, {"text": "It should include some information about the cultural origin of authors and their job, and each opinion should be sentiment-evaluated not only by its own author, but by many other readers as well.", "labels": [], "entities": []}, {"text": "It would also be good to have a marking of the subjective and objective parts of the text.", "labels": [], "entities": []}, {"text": "Unfortunately this kind of corpora are not available at the moment.", "labels": [], "entities": []}, {"text": "In the present work we place our attention at the supervised classification of opinions in positive and negative.", "labels": [], "entities": []}, {"text": "Our system, which we call Opinum 1 , is trained from a corpus labeled with a value indicating whether an opinion is positive or negative.", "labels": [], "entities": []}, {"text": "The corpus was crawled from the web and it consists of a 160MB collection of Spanish opinions about financial products.", "labels": [], "entities": []}, {"text": "Opinum's approach is general enough and it is not limited to this corpus nor to the financial domain.", "labels": [], "entities": []}, {"text": "There are state-of-the-art works on sentiment analysis which care about differentiating between the objective and the subjective part of a text.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.9439384639263153}]}, {"text": "For instance, in the review of a film there is an objective part and then the opinion ().", "labels": [], "entities": []}, {"text": "In our case we work directly with opinion texts and we do not make such difference.", "labels": [], "entities": []}, {"text": "We have noticed that in customer reviews, even when stating objective facts, some positive or negative sentiment is usually expressed.", "labels": [], "entities": []}, {"text": "Many works in the literature of sentiment analysis take lexicon-based approaches).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.9423520863056183}]}, {"text": "For instance () use WordNet to extend the relation of positive and negative words to other related lexical units.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.9661492109298706}]}, {"text": "However the combination of which words appear together may also be important and there are comparisons of different Machine learning approaches () in the literature, like Support Vector Machines, kNearest Neighbours, Naive-Bayes, and other classifiers based on global features.", "labels": [], "entities": []}, {"text": "In () structured models are used to infer the sentiment from different levels of granularity.", "labels": [], "entities": []}, {"text": "They score cliques of text based on a high-dimensional feature vector.", "labels": [], "entities": []}, {"text": "In the Opinum approach we score each sentence based on its n-gram probabilites.", "labels": [], "entities": []}, {"text": "For a complete opinion we sum the scores of all its sentences.", "labels": [], "entities": []}, {"text": "Thus, if an opinion has several positive sentences and it finally concludes with a negative sentence which settles the whole opinion as negative, Opinum would probably fail.", "labels": [], "entities": []}, {"text": "The n-gram sequences are good at capturing phrasemes (multiwords), the motivation for which is stated in Section 2.", "labels": [], "entities": []}, {"text": "Basically, there are phrasemes which bear sentiment.", "labels": [], "entities": []}, {"text": "They maybe different depending on the domain and it is recommendable to build the models with opinions belonging to the target domain, for instance, financial products, computers, airlines, etc.", "labels": [], "entities": []}, {"text": "A study of domain adaptation for sentiment analysis is presented in.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.9685427844524384}]}, {"text": "In Opinum different classifiers would be built for different domains.", "labels": [], "entities": []}, {"text": "Building the models does not require the aid of experts, only a labeled set of opinions is necessary.", "labels": [], "entities": []}, {"text": "Another contribution of Opinum is that it applies some simplifications on the original text of the opinions for improving the performance of the models.", "labels": [], "entities": []}, {"text": "In the remainder of the paper we first state the motivation of our approach in Section 2, then in Section 3 we describe in detail the Opinum approach.", "labels": [], "entities": []}, {"text": "In Section 4 we present our experiments with Spanish financial opinions and we state some conclusions and future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the Opinum system we query the M p , Mn models with the KenLM) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application.", "labels": [], "entities": []}, {"text": "It also has an efficient memory management which is positive for simultaneous queries to the server.", "labels": [], "entities": []}, {"text": "The queries are performed at sentence level.", "labels": [], "entities": []}, {"text": "Each sentence s \u2208 o t is assigned a score which is the log probability of the sentence being generated by the language model.", "labels": [], "entities": []}, {"text": "The decision is taken by comparing its scores for the positive and for the negative models.", "labels": [], "entities": []}, {"text": "For a given opinion o t , the log-probability sums can be taken: If this difference is close to zero, |d ot |/w ot < \u03b5 0 , it can be considered that the classification is neutral.", "labels": [], "entities": []}, {"text": "The number of words wot is used as a normalization factor.", "labels": [], "entities": []}, {"text": "If it is large, |d ot |/w ot > \u03b5 1 , it can be considered that the opinion has a very positive or very negative sentiment.", "labels": [], "entities": []}, {"text": "Therefore Opinum classifies the opinions with qualifiers: very/somewhat/little positive/negative depending on the magnitude |d ot |/w ot and sign(d ot ), respectively.", "labels": [], "entities": []}, {"text": "The previous assessment is also accompanied by a confidence measure given by the level of agreement among the different sentences of an opinion.", "labels": [], "entities": []}, {"text": "If all its sentences have the same positivity/negativity, measured by sign(d s j ), s j \u2208 o, with large magnitudes then the confidence is the highest.", "labels": [], "entities": [{"text": "confidence", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.951745867729187}]}, {"text": "In the opposite casein which there is the same number of positive and negative sentences with similar magnitudes the confidence is the lowest.", "labels": [], "entities": [{"text": "confidence", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.9731354713439941}]}, {"text": "The intermediate cases are those with sentences agreeing in sign but some of them with very low magnitude, and those with most sentences of the same sign and some with different sign.", "labels": [], "entities": []}, {"text": "We use Shannon's entropy measure H(\u00b7) to quantify the amount of disagreement.", "labels": [], "entities": [{"text": "entropy measure H", "start_pos": 17, "end_pos": 34, "type": "METRIC", "confidence": 0.6387342611948649}]}, {"text": "The reason for this is that they are used to tune subjective qualifiers (very/little, high/low confidence) and will usually depend on the training set and on the requirements of the application.", "labels": [], "entities": []}, {"text": "Note that the classification in positive or negative sentiment is not affected by these parameters.", "labels": [], "entities": []}, {"text": "From a human point of view it is also a subjective assessment but in our setup it is looked at as a feature implicitly given by the labeled opinions of the training set.", "labels": [], "entities": []}, {"text": "In our experimental setup we have a set of positive and negative opinions in Spanish, collected from a website for user reviews and opinions.", "labels": [], "entities": []}, {"text": "The opinions are constrained to the financial field including banks, savings accounts, loans, mortgages, investments, credit cards, and all other related topics.", "labels": [], "entities": []}, {"text": "The authors of the opinions are not professionals, they are mainly customers.", "labels": [], "entities": []}, {"text": "There is no structure required for their opinions, and they are free to tell their experience, their opinion or their feeling about the entity or the product.", "labels": [], "entities": []}, {"text": "The users meant to communicate their review to other humans and they don't bear in mind any natural language processing tools.", "labels": [], "entities": []}, {"text": "The authors decide whether their own opinion is positive or negative and this field is mandatory.", "labels": [], "entities": []}, {"text": "The users provide a number of stars as well: from one to five, but we have not used this information.", "labels": [], "entities": []}, {"text": "It is interesting to note that there are 66 opinions with only one star which are marked as positive.", "labels": [], "entities": []}, {"text": "There are also 67 opinions with five stars which are marked as negative.", "labels": [], "entities": []}, {"text": "This is partially due to human errors, a human can notice when reading them.", "labels": [], "entities": []}, {"text": "However we have not filtered these noisy data, as removing human errors could be regarded as biasing the data set with our own subjective criteria.", "labels": [], "entities": []}, {"text": "Regarding the size of the corpus, it consists of 9320 opinions about 180 different Spanish banks and financial products.", "labels": [], "entities": []}, {"text": "From these opinions 5877 are positive and 3443 are negative.", "labels": [], "entities": []}, {"text": "There is a total of 709741 words and the mean length of the opinions is 282 words for the positive and 300 words for the negative ones.", "labels": [], "entities": []}, {"text": "In the experiments we present in this work, we randomly divide the data set in 75% for training and 25% for testing.", "labels": [], "entities": []}, {"text": "We check that the distribution of positive and negative remains the same among test and train.", "labels": [], "entities": []}, {"text": "After the L 2 (\u00b7) lemmatization and entity substitution, the number of different words in the data set is 13067 in contrast with the 78470 different words in the original texts.", "labels": [], "entities": [{"text": "entity substitution", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.75137859582901}]}, {"text": "In other words, the lexical complexity is reduced by 83%.", "labels": [], "entities": []}, {"text": "Different substitutions play a different role in this simplification.", "labels": [], "entities": []}, {"text": "The \"Unknown\" wildcard represents a 7,13% of the original text.", "labels": [], "entities": [{"text": "Unknown\" wildcard", "start_pos": 5, "end_pos": 22, "type": "DATASET", "confidence": 0.801374097665151}]}, {"text": "Entities were detected and replaced 33858 times (7807 locations, 5409 people, 19049 companies, 502 e-mails addresses and phone numbers, 2055 URLs, 1136 dates) which is a 4,77% of the text.", "labels": [], "entities": []}, {"text": "There are also 46780 number substitutions, a 7% of the text.", "labels": [], "entities": []}, {"text": "The rest of complexity reduction is due to the removal of the morphology as explained in Subsection 3.1.", "labels": [], "entities": []}, {"text": "In our experiments, the training of Opinum consisted of lemmatizing and susbstituting entities of the 6990 opinions belonging the training set and building the language models.", "labels": [], "entities": []}, {"text": "The positive model is built from 4403 positive opinions and the negative model is built from 2587 negative opinions.", "labels": [], "entities": []}, {"text": "Balancing the amount of positive and negative samples does not improve the performance.", "labels": [], "entities": []}, {"text": "Instead, it obliges us to remove an important amount of positive opinions and the classification results are decreased by approximately 2%.", "labels": [], "entities": []}, {"text": "This is why we use all the opinions available in the training set.", "labels": [], "entities": []}, {"text": "Both language models are n-grams with n \u2208.", "labels": [], "entities": []}, {"text": "Having a 37% less samples for the negative opinions is not a problem thank to the smoothing techniques applied by IRSTLM.", "labels": [], "entities": [{"text": "IRSTLM", "start_pos": 114, "end_pos": 120, "type": "DATASET", "confidence": 0.9241217374801636}]}, {"text": "Nonetheless if the amount of training texts is too low we would recommend taking a lower n.", "labels": [], "entities": []}, {"text": "A simple way to set n is to take the lowest value of n for which classification performance is improved.", "labels": [], "entities": [{"text": "classification", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.9611703157424927}]}, {"text": "An unnecessarily high n could overfit the models.", "labels": [], "entities": []}, {"text": "The tests are performed with 2330 opinions (not involved in building the models).", "labels": [], "entities": []}, {"text": "For measuring the accuracy we do not use the qualifiers information but only the decision about the positive or negative class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9995775818824768}]}, {"text": "In we show the scores of the opinions for the positive and negative models.", "labels": [], "entities": []}, {"text": "The score is the sum of scores of the sentences, thus it can be seen that longer opinions (bigger markers) have bigger scores.", "labels": [], "entities": []}, {"text": "Independence of the size is not necessary for classifying in positive and negative.", "labels": [], "entities": []}, {"text": "In the diagonal it can be seen that positive samples are close to the negative ones, this is to be expected: both positive and negative language models are built for the same language.", "labels": [], "entities": []}, {"text": "However the small difference in their scores yields an 81,98% success rate in the classification.", "labels": [], "entities": []}, {"text": "An improvement of this rate would be difficult to achieve taking into account that there is As you have the website you don't waste time on the phone.", "labels": [], "entities": []}, {"text": "Positive \"En el telfono os hacen perder el tiempo y no tienen web.\"", "labels": [], "entities": []}, {"text": "They waste your time on the phone and they don't have a website.", "labels": [], "entities": []}, {"text": "Negative \"De todas formas me solucionaron el problema.\"", "labels": [], "entities": []}, {"text": "Anyway, they solved my problem.", "labels": [], "entities": []}, {"text": "Positive \"No hay forma de que me solucionen el problema.\"", "labels": [], "entities": []}, {"text": "There is noway to make them solve my problem.", "labels": [], "entities": []}, {"text": "Negative A negative opinion of several sentences \"Con XXXXXX me fue muy bien.\"", "labels": [], "entities": []}, {"text": "I was fine with XXXXXX.", "labels": [], "entities": [{"text": "XXXXXX", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.7632157802581787}]}, {"text": "Positive \"Hasta que surgieron los problemas.\"", "labels": [], "entities": []}, {"text": "Negative \"Por hacerme cliente me regalaban 100 euros.\"", "labels": [], "entities": []}, {"text": "They gave me 100 euros for becoming a client.", "labels": [], "entities": []}, {"text": "Positive \"Pero una vez que eres cliente note aportan nada bueno.\"", "labels": [], "entities": []}, {"text": "But once you area client, they they do not offer anything good.", "labels": [], "entities": []}, {"text": "Negative  noise in the training set and that there are opinions without a clear positive or negative feeling.", "labels": [], "entities": []}, {"text": "A larger corpus would also contribute to a better result.", "labels": [], "entities": []}, {"text": "Even though we have placed many efforts in simplifying the text, this does not help in the cases in which a construction of words is never found in the corpus.", "labels": [], "entities": []}, {"text": "A construction could even be present in the corpus but in the wrong class.", "labels": [], "entities": []}, {"text": "For instance, in our corpus \"no estoy satisfecho\" (meaning \"I am not satisfied\") appears 3 times among the positive opinions and 0 times among the negative ones.", "labels": [], "entities": []}, {"text": "This weakness of the corpus is due to sentences referring to a money back guarantee: \"si no esta satisfecho le devolvemos el dinero\" which are used in a positive context.", "labels": [], "entities": []}, {"text": "Usually in long opinions a single sentence does not change the positiveness score.", "labels": [], "entities": []}, {"text": "In long opinions every sentence is prone to show the sentiment except for the cases of irony or opinions with an objective part.", "labels": [], "entities": []}, {"text": "The performance of Opinum depending on the size of the opinions of the test set is shown in. the ROC curve of the classifier shows its stability against changing the true-positive versus falsenegative rates.", "labels": [], "entities": [{"text": "ROC", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9965546131134033}]}, {"text": "A comparison with other methods would be a valuable source of evaluation.", "labels": [], "entities": []}, {"text": "It is not feasible at this moment because of the lack of free customers opinions databases and opionion classifiers as well.", "labels": [], "entities": []}, {"text": "The success rate we obtain can be compared to the 69% baseline given by a classifier based on the frequencies of single words.", "labels": [], "entities": []}, {"text": "The query time of Opinum on a standard computer ranges from 1, 63 s for the shortest opinions to 1, 67 s for those with more than 1000 words.", "labels": [], "entities": []}, {"text": "In our setup, most of the time is spent in loading the morphological dictionary, few milliseconds are spent in the morphological analysis of the opinion and the named entity substitution, and less than a millisecond is spent in querying each model.", "labels": [], "entities": []}, {"text": "Ina batch mode, the morphological analysis could be done for all the opinions together and thousands of them could be evaluated in seconds.", "labels": [], "entities": []}, {"text": "In Opinum's web interface we only provide the single opinion queries and we output the decision, the qualifiers information and the confidence measure.", "labels": [], "entities": []}], "tableCaptions": []}