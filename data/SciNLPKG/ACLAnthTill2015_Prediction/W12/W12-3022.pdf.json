{"title": [], "abstractContent": [{"text": "In data integration we transform information from a source into a target schema.", "labels": [], "entities": [{"text": "data integration", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.7303372770547867}]}, {"text": "A general problem in this task is loss of fidelity and coverage: the source expresses more knowledge than can fit into the target schema, or knowledge that is hard to fit into any schema at all.", "labels": [], "entities": [{"text": "coverage", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9725200533866882}]}, {"text": "This problem is taken to an extreme in information extraction (IE) where the source is natural language.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.8596935093402862}]}, {"text": "To address this issue, one can either automatically learn a latent schema emergent in text (a brittle and ill-defined task), or manually extend schemas.", "labels": [], "entities": []}, {"text": "We propose instead to store data in a probabilistic database of universal schema.", "labels": [], "entities": []}, {"text": "This schema is simply the union of all source schemas, and the proba-bilistic database learns how to predict the cells of each source relation in this union.", "labels": [], "entities": []}, {"text": "For example , the database could store Freebase relations and relations that correspond to natural language surface patterns.", "labels": [], "entities": []}, {"text": "The database would learn to predict what freebase relations hold true based on what surface patterns appear , and vice versa.", "labels": [], "entities": []}, {"text": "We describe an analogy between such databases and collaborative filtering models, and use it to implement our paradigm with probabilistic PCA, a scalable and effective collaborative filtering method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language is a highly expressive representation of knowledge.", "labels": [], "entities": []}, {"text": "Yet, for many tasks databases are more suitable, as they support more effective decision support, question answering and data mining.", "labels": [], "entities": [{"text": "decision support", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.8604384362697601}, {"text": "question answering", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.9294712543487549}, {"text": "data mining", "start_pos": 121, "end_pos": 132, "type": "TASK", "confidence": 0.806422084569931}]}, {"text": "But given a fixed schema, any database can only capture so much of the information natural language can express, even if we restrict us to factual knowledge.", "labels": [], "entities": []}, {"text": "For example, Freebase captures the content of Wikipedia to some extent, but has no criticized(Person,Person) relation and hence cannot answer a question like \"Who criticized George Bush?\", even though partial answers are expressed in Wikipedia.", "labels": [], "entities": []}, {"text": "This makes the database schema a major bottleneck in information extraction (IE).", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.8456651926040649}]}, {"text": "From a more general point of view, data integration always suffers from schema mismatch between knowledge source and knowledge target.", "labels": [], "entities": [{"text": "data integration", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7489093542098999}]}, {"text": "To overcome this problem, one could attempt to manually extend the schema whenever needed, but this is a time-consuming and expensive process.", "labels": [], "entities": []}, {"text": "Alternatively, in the case of IE, we can automatically induce latent schemas from text, but this is a brittle, ill-defined and error-prone task.", "labels": [], "entities": [{"text": "IE", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9545955657958984}]}, {"text": "This paper proposes a third alternative: sidestep the issue of incomplete schemas altogether, by simply combining the relations of all knowledge sources into what we will refer to as a universal schema.", "labels": [], "entities": []}, {"text": "In the case of IE this means maintaining a database with one table per natural language surface pattern.", "labels": [], "entities": [{"text": "IE", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.926304280757904}]}, {"text": "For data integration from structured sources it simply means storing the original tables as is.", "labels": [], "entities": []}, {"text": "Crucially, the database will not only store what each source table does contain, it will also learn a probabilistic model about which other rows each source table should correctly contain.", "labels": [], "entities": []}, {"text": "Let us illustrate this approach in the context of IE.", "labels": [], "entities": [{"text": "IE", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9539365768432617}]}, {"text": "First we copy tables such as profession from a structured source (say, DBPedia).", "labels": [], "entities": [{"text": "DBPedia", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9144194722175598}]}, {"text": "Next we create one table per surface pattern, such as was-criticizedby and was-attacked-by and fill these tables with the entity pairs that appear with this pattern in some natural language corpus (say, the NYT Corpus).", "labels": [], "entities": [{"text": "NYT Corpus", "start_pos": 207, "end_pos": 217, "type": "DATASET", "confidence": 0.9781325161457062}]}, {"text": "At this point, our database is a simple combination of 116 a structured and an knowledge representation.", "labels": [], "entities": []}, {"text": "However, while we insert this knowledge, we can learn a probabilistic model which is able to predict was-criticized-by pairs based on information from the was-attacked-by relation.", "labels": [], "entities": []}, {"text": "In addition, it learns that the profession relation in Freebase can help disambiguate between physical attacks in sports and verbal attacks in politics.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9625778794288635}]}, {"text": "At the same time, the model learns that the natural language relation was-criticized-by can help predict the profession information in Freebase.", "labels": [], "entities": []}, {"text": "Moreover, often users of the database will not need to study a particular schema-they can use their own expressions (say, works-at instead of profession) and still find the right answers.", "labels": [], "entities": []}, {"text": "In the previous scenario we could answer more questions than our structured sources alone, because we learn how to predict new Freebase rows.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 127, "end_pos": 135, "type": "DATASET", "confidence": 0.7345005869865417}]}, {"text": "We could answer more questions than the text corpus and OpenIE alone, because we learn how to predict new rows in surface pattern tables.", "labels": [], "entities": [{"text": "OpenIE", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.8895535469055176}]}, {"text": "We could also answer more questions than in Distant Supervision (, because our schema is not limited to the relations in the structured source.", "labels": [], "entities": []}, {"text": "We could even go further and import additional structured sources, such as Yago (.", "labels": [], "entities": [{"text": "Yago", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.948140561580658}]}, {"text": "In this case the probabilistic database would have integrated, and implicitly aligned, several different data sources, in the sense that each helps predict the rows of the other.", "labels": [], "entities": []}, {"text": "In this paper we present results of our first technical approach to probabilistic databases with universal schema: collaborative filtering, which has been successful in modeling movie recommendations.", "labels": [], "entities": []}, {"text": "Here each entity tuple explicitly \"rates\" source tables as \"I appear in it\" or \"I don't\", and the recommender system model predicts how the tuple would \"rate\" other tables-this amounts to the probability of membership in the corresponding table.", "labels": [], "entities": []}, {"text": "Collaborative filtering provides us with a wide range of scalable and effective machine learning techniques.", "labels": [], "entities": []}, {"text": "In particular, we are free to choose models that use no latent representations at all (such as a graphical model with one random variable per database cell), or models with latent representations that do not directly correspond to interpretable semantic concepts.", "labels": [], "entities": []}, {"text": "In this paper we explore the latter and use a probabilistic generalization to PCA for recommen- dation.", "labels": [], "entities": []}, {"text": "In our experiments we integrate Freebase data and information from the New York Times Corpus.", "labels": [], "entities": [{"text": "Freebase data and information from the New York Times Corpus", "start_pos": 32, "end_pos": 92, "type": "DATASET", "confidence": 0.7278744250535965}]}, {"text": "We show that our probabilistic database can answer questions neither of the sources can answer, and that it uses information from one source to improve predictions for the other.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our work aims to predict new rows of source tables, where tables correspond to either surface patterns in natural language sources, or tables in structured sources.", "labels": [], "entities": []}, {"text": "In this paper we concentrate on binary relations, but note that in future work we will use unary, and generally n-ary, tables as well.", "labels": [], "entities": []}, {"text": "In our experiments, we holdout some of the observed source rows and try to predict these based on other observed rows.", "labels": [], "entities": []}, {"text": "In particular, for each entity pair, we traverse overall source relations.", "labels": [], "entities": []}, {"text": "For each relation we throw an unbiased coin to determine whether it is observed for the given pair.", "labels": [], "entities": []}, {"text": "Then we train a gPCA model of 50 components on the observed rows, and use it to predict the unobserved ones.", "labels": [], "entities": []}, {"text": "Here a pair e is set to be in a given relation r if P (r (e)) > 0.5 according to our model.", "labels": [], "entities": []}, {"text": "Since we generally do not have observed negative information, 1 we sub-sample a set of negative rows for each relation r to create a more balanced training set.", "labels": [], "entities": []}, {"text": "We evaluate recall of our method by measuring how many of the true held out rows we predict.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9987102746963501}]}, {"text": "We could use a similar approach to measure precision by considering each positive prediction to be a false positive if the observed held-out data does not contain the corresponding fact.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9986629486083984}]}, {"text": "However, this approach underestimates precision since our sources are generally incomplete.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9993448853492737}]}, {"text": "To overcome this issue, we use human annotations for the precision measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9993504881858826}]}, {"text": "In particular, we randomly sample a subset of entity pairs and ask human annotators to assess the predicted positive relations of each.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Relation predictions w/o Freebase.  without Freebase  with Freebase  Prec.  0.687  0.666", "labels": [], "entities": [{"text": "Relation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9371147751808167}, {"text": "Freebase", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.9504562616348267}, {"text": "Freebase  Prec.  0.687  0.666", "start_pos": 69, "end_pos": 98, "type": "DATASET", "confidence": 0.8832314133644104}]}]}