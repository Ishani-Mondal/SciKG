{"title": [{"text": "Capitalization Cues Improve Dependency Grammar Induction", "labels": [], "entities": [{"text": "Capitalization Cues Improve Dependency Grammar Induction", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.7993787527084351}]}], "abstractContent": [{"text": "We show that orthographic cues can be helpful for unsupervised parsing.", "labels": [], "entities": []}, {"text": "In the Penn Tree-bank, transitions between upper-and lowercase tokens tend to align with the boundaries of base (English) noun phrases.", "labels": [], "entities": [{"text": "Penn Tree-bank", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.9945299029350281}]}, {"text": "Such signals can be used as partial bracketing constraints to train a grammar inducer: in our experiments, directed dependency accuracy increased by 2.2% (average over 14 languages having case information).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9823305606842041}]}, {"text": "Combining capitalization with punctuation-induced constraints in inference further improved parsing performance, attaining state-of-the-art levels for many languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency grammar induction and related problems of unsupervised syntactic structure discovery are attracting increasing attention.", "labels": [], "entities": [{"text": "Dependency grammar induction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8812037309010824}, {"text": "syntactic structure discovery", "start_pos": 66, "end_pos": 95, "type": "TASK", "confidence": 0.7318357626597086}]}, {"text": "Since sentence structure is underdetermined by raw text, there have been efforts to simplify the task, via (i) pooling features of syntax across languages; as well as (ii) identifying universal rules) -such as verbocentricity () -that need not be learned at all.", "labels": [], "entities": []}, {"text": "Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs.", "labels": [], "entities": []}, {"text": "As standard practice shifts away from relying on gold part-of-speech (POS) tags, inter alia), lighter cues to inducing linguistic structure become more important.", "labels": [], "entities": []}, {"text": "Examples of useful POS-agnostic clues include punctuation boundaries and various kinds of bracketing constraints (.", "labels": [], "entities": []}, {"text": "We propose adding capitalization to this growing list of sources of partial bracketings.", "labels": [], "entities": []}, {"text": "Our intuition stems from English, where (maximal) spans of capitalized words -such as Apple II, World War I, Mayor William H. Hudnut III, International Business Machines Corp. and Alexandria, Va -tend to demarcate proper nouns.", "labels": [], "entities": [{"text": "International Business Machines Corp. and Alexandria, Va", "start_pos": 138, "end_pos": 194, "type": "DATASET", "confidence": 0.7250610403716564}]}, {"text": "Consider a motivating example (all of our examples are from WSJ) without punctuation, in which all (eight) capitalized word clumps and uncased numerals match base noun phrase constituent boundaries: in] because he decided sales would be even weaker than he had expected. and another (whose first word happens to be a leaf), where capitalization complements punctuation cues: Could such chunks help bootstrap grammar induction and/or improve the accuracy of already-trained unsupervised parsers?", "labels": [], "entities": [{"text": "WSJ", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9234060049057007}, {"text": "bootstrap grammar induction", "start_pos": 398, "end_pos": 425, "type": "TASK", "confidence": 0.6835055450598398}, {"text": "accuracy", "start_pos": 445, "end_pos": 453, "type": "METRIC", "confidence": 0.9969587326049805}]}, {"text": "In answering these questions, we will focus predominantly on sentence-internal capitalization.", "labels": [], "entities": []}, {"text": "But we will also show that first words -those capitalized by convention -and uncased segments -whose characters are not even drawn from an alphabet -could play a useful role as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "We gauged the suitability of capitalization-induced fragments for guiding dependency grammar induction by assessing accuracy, in WSJ, 1 of parsing constraints derived from their end-points.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.7741152048110962}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9993433356285095}, {"text": "WSJ", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.8556615710258484}]}, {"text": "Following the suite of increasingly-restrictive constraints on how dependencies may interact with fragments, introduced by.2), we tested several such heuristics.", "labels": [], "entities": []}, {"text": "The most lenient constraint, thread, only asks that no dependency path from the root to a leaf enter the fragment twice; tear requires any incoming arcs to come from the same side of the fragment; sprawl demands that there be exactly one incoming arc; loose further constrains any outgoing arcs to be from the fragment's head; and strict -the most stringent constraint -bans external dependents.", "labels": [], "entities": [{"text": "thread", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9499295949935913}]}, {"text": "Since only strict is binding for single words, we experimented also with strict : applying strict solely to multi-token fragments (ignoring singletons).", "labels": [], "entities": []}, {"text": "In sum, we explored six ways in which dependency parse trees can be constrained by fragments whose end-points could be defined by capitalization (or in other various ways, e.g., semantic an- We converted labeled constituents into unlabeled dependencies using deterministic \"head-percolation\" rules  notations, punctuation or HTML tags in web pages).", "labels": [], "entities": [{"text": "dependency parse trees", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.794500470161438}]}, {"text": "For example, in the sentence about Cordis, the strict hypothesis would be wrong about five of the eight fragments: Jurors attaches in; Court takes the second in; Hershhenson and Pagones derive their titles, president; and (at least in our reference) Vadas attaches and, Ciporkin and who.", "labels": [], "entities": []}, {"text": "Based on this, we would consider strict to be 37.5%-accurate.", "labels": [], "entities": [{"text": "strict", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9860914349555969}, {"text": "accurate", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.968845784664154}]}, {"text": "But loose -and the rest of the more relaxed constraints -would get perfect scores.", "labels": [], "entities": []}, {"text": "(And strict would retract the mistake about Jurors but also the correct guesses about Miami and Cordis, scoring only 20%.)", "labels": [], "entities": [{"text": "Jurors", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.8543910384178162}, {"text": "Miami", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.9105822443962097}]}, {"text": "(capital) shows scores averaged over the entire treebank.", "labels": [], "entities": []}, {"text": "Columns markup (Spitkovsky et al., 2010b) and punct () indicate that capitalization yields across-the-board more accurate constraints (for English) compared with fragments derived from punctuation or markup (i.e., anchor text, bold, italics and underline tags in HTML), for which such constraints were originally intended.", "labels": [], "entities": []}, {"text": "To further test the potential of capitalization-induced constraints, we applied them in the Viterbi-decoding phase of a simple (unlexicalized) supervised dependency parser -an instance of DBM-1 (Spitkovsky et al., 2012, \u00a72.1), trained on WSJ sentences with up   to 45 words (excluding Section 23).", "labels": [], "entities": []}, {"text": "shows evaluation results on held-out data (all sentences), using \"add-one\" smoothing.", "labels": [], "entities": []}, {"text": "All constraints other than strict improve accuracy by about a half-a-point, from 71.8 to 72.4%, suggesting that capitalization is informative of certain regularities not captured by DBM grammars; moreover, it still continues to be useful when punctuation-based constraints are also enforced, boosting accuracy from 74.5 to 74.9%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9987799525260925}, {"text": "accuracy", "start_pos": 301, "end_pos": 309, "type": "METRIC", "confidence": 0.9990599751472473}]}], "tableCaptions": [{"text": " Table 1: Top 10 fragments of POS tag sequences in WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.8898003697395325}]}, {"text": " Table 2: Several sources of fragments' end-points and  %-correctness of their derived constraints (for English).", "labels": [], "entities": [{"text": "correctness", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.6251631379127502}]}, {"text": " Table 3: Supervised (directed) accuracy on Section 23  of WSJ using capitalization-induced constraints (vertical)  jointly with punctuation (horizontal) in Viterbi-decoding.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9957519769668579}, {"text": "Section 23  of WSJ", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.6674394384026527}]}, {"text": " Table 4: Parsing performance for grammar inducers trained with capitalization-based initial constraints, tested against  14 held-out sets from 2006/7 CoNLL shared tasks, and ordered by number of multi-token fragments in training data.", "labels": [], "entities": []}, {"text": " Table 6: Accuracies for capitalization-and punctuation-induced constraints on all (full) 2006/7 CoNLL training sets.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9962134957313538}, {"text": "2006/7 CoNLL training sets", "start_pos": 90, "end_pos": 116, "type": "DATASET", "confidence": 0.7724879682064056}]}, {"text": " Table 7: Unsupervised accuracies for uniform-at-random projective parse trees (init), also after a step of Viterbi EM,  and supervised performance with induced constraints, on 2006/7 CoNLL evaluation sets (sentences under 145 tokens).", "labels": [], "entities": [{"text": "Viterbi EM", "start_pos": 108, "end_pos": 118, "type": "DATASET", "confidence": 0.8011703789234161}, {"text": "2006/7 CoNLL evaluation sets", "start_pos": 177, "end_pos": 205, "type": "DATASET", "confidence": 0.6792603731155396}]}]}