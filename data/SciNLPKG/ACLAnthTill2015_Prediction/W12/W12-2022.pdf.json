{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 190-200", "labels": [], "entities": []}], "abstractContent": [{"text": "A number of different research subfields are concerned with the automatic assessment of student answers to comprehension questions, from language learning contexts to computer science exams.", "labels": [], "entities": []}, {"text": "They share the need to evaluate free-text answers but differ in task setting and grading/evaluation criteria, among others.", "labels": [], "entities": []}, {"text": "This paper has the intention of fostering synergy between the different research strands.", "labels": [], "entities": []}, {"text": "It discusses the different research strands, details the crucial differences, and explores under which circumstances systems can be compared given publicly available data.", "labels": [], "entities": []}, {"text": "To that end, we present results with the CoMiC-EN Content Assessment system (Meurers et al., 2011a) on the dataset published by Mohler et al.", "labels": [], "entities": [{"text": "CoMiC-EN Content Assessment system (Meurers et al., 2011a)", "start_pos": 41, "end_pos": 99, "type": "DATASET", "confidence": 0.7969726865941827}]}, {"text": "(2011) and outline what was necessary to perform this comparison.", "labels": [], "entities": []}, {"text": "We conclude with a general discussion on comparability and evaluation of short answer assessment systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Short answer assessment systems compare students' responses to questions with manually defined target responses or answer keys in order to judge the appropriateness of the responses, or in order to automatically assign a grade.", "labels": [], "entities": [{"text": "Short answer assessment", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7684831023216248}]}, {"text": "A number of approaches have emerged in recent years, each of them with different aims and different backgrounds.", "labels": [], "entities": []}, {"text": "In this paper, we will draw a map of the short answer assessment landscape, highlighting the similarities and differences between approaches and the data used for evaluation.", "labels": [], "entities": []}, {"text": "We will provide an overview of 12 systems and sketch their attributes.", "labels": [], "entities": []}, {"text": "Subsequently, we will zoom into the comparison of two of them, namely CoMiC-EN () and the one which we call the Texas system) and discuss the issues that arise with this endeavor.", "labels": [], "entities": [{"text": "Texas system", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.9543552994728088}]}, {"text": "Returning to the bigger picture, we will explore how such systems could be compared in general, in the belief that meaningful comparison of approaches across research strands will bean important ingredient in advancing this relatively new research field.", "labels": [], "entities": []}, {"text": "2 The short answer assessment landscape", "labels": [], "entities": [{"text": "short answer assessment", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.4958337148030599}]}], "datasetContent": [{"text": "We now turn to the evaluation of CoMiC-EN on the Texas corpus as it is a publicly available dataset.", "labels": [], "entities": [{"text": "Texas corpus", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.9762347638607025}]}, {"text": "As mentioned before, CoMiC-EN performs meaning comparison based on a system of categories while the Texas system is a scoring approach, trying to predict a grade.", "labels": [], "entities": [{"text": "meaning comparison", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8323217928409576}]}, {"text": "While the former is a classification task, the latter is better characterized as a regression problem because of the desired numerical outcome.", "labels": [], "entities": []}, {"text": "Of course, one could simply pretend that individual grades are classes and treat scoring as a classification task.", "labels": [], "entities": []}, {"text": "However, a classification approach has no knowledge of numerical relationships, i.e., it does not 'know' that 4 is a higher grade than 3 and a much higher grade than 1 (assuming a 0-5 scale).", "labels": [], "entities": []}, {"text": "As a result, if an evaluation metric such as Pearson correlation is used, classification systems are at a disadvantage because some misclassifications are punished more than others.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 45, "end_pos": 64, "type": "METRIC", "confidence": 0.6577374339103699}]}, {"text": "We discuss this point further in Section 4.", "labels": [], "entities": []}, {"text": "For these reasons, to obtain a more interesting comparison, we modified CoMiC-EN to perform scoring instead of meaning comparison.", "labels": [], "entities": [{"text": "meaning comparison", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7443782687187195}]}, {"text": "This means that the memory-based learning approach CoMiC-EN had employed so far was no longer applicable and had to be replaced with a regression-capable learning strategy.", "labels": [], "entities": []}, {"text": "We chose Support Vector Regression (SVR) using libSVM since that is one of the methods employed by.", "labels": [], "entities": []}, {"text": "However, all other parts of CoMiC-EN such as the processing pipeline and the alignment approach and the extracted features remained the same.", "labels": [], "entities": [{"text": "CoMiC-EN", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.8677363991737366}, {"text": "alignment", "start_pos": 77, "end_pos": 86, "type": "TASK", "confidence": 0.9532761573791504}]}, {"text": "The evaluation procedure was carried out as a 12-fold cross-validation due to the 12 assignments in the Texas corpus.", "labels": [], "entities": [{"text": "Texas corpus", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.9737337231636047}]}, {"text": "For each fold, one complete assignment was held out as test set.", "labels": [], "entities": []}, {"text": "Parameters for the SVR were determined using a grid search using the tools provided with libSVM.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9630875587463379}, {"text": "SVR", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.6977676749229431}]}, {"text": "As kernel function, we used a linear kernel as it was also used in the evaluation of the Texas system and thus constitutes a vital part of the evaluation setup.", "labels": [], "entities": [{"text": "Texas system", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.932242214679718}]}, {"text": "In general, we designed to evaluation procedure to be as close as possible to the Texas one.", "labels": [], "entities": []}, {"text": "The CoMiC-EN system on the Texas data set does not quite reach the level achieved by the Texas system on their data set.", "labels": [], "entities": [{"text": "Texas data set", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.9905513525009155}]}, {"text": "We obtained a Pearson correlation of r = 0.405 and an RMSE of 1.016 overall 12 folds.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 14, "end_pos": 33, "type": "METRIC", "confidence": 0.924645721912384}, {"text": "RMSE", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9991021156311035}]}, {"text": "However, let us keep in mind the objective of this experiment as exemplifying the process needed to directly compare two systems from different research strands on the same dataset.", "labels": [], "entities": []}, {"text": "It seems clear that for systems to be comparable and results to be reproducible, datasets must be publicly available, as is the case with the Texas corpus.", "labels": [], "entities": [{"text": "Texas corpus", "start_pos": 142, "end_pos": 154, "type": "DATASET", "confidence": 0.9562889635562897}]}, {"text": "However, data availability alone does not ensure meaningful comparison.", "labels": [], "entities": []}, {"text": "Depending on the context the corpus was drawn from, datasets will differ just like the corresponding systems: \u2022 Data source: Reading comprehension task in language learning setting, language tutoring context, automated grading of short answer exams \u2022 Language properties: Native vs. learner language, domain-specific language (e.g., computer science) \u2022 Assessment scheme: nominal vs. interval scale Especially the last point deserves some further discussion.", "labels": [], "entities": []}, {"text": "Depending on the kind of assessment scheme, which in turn is motivated by the task, different evaluation methods maybe chosen.", "labels": [], "entities": []}, {"text": "Scoring systems are often evaluated using a correlation metric in order to capture the systems' tendency to assign similar but not necessary equal grades as the human raters.", "labels": [], "entities": []}, {"text": "Conversely, with category-based schemes one usually reports accuracy, which expresses how many items were classified correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9991641044616699}]}, {"text": "The question that arises is how a system coming from one paradigm can be compared to one from the other paradigm in a meaningful way.", "labels": [], "entities": []}, {"text": "One might argue that the tasks are simply too different: scoring might take form errors into account while meaning comparison by definition does not.", "labels": [], "entities": []}, {"text": "Moreover, while classification labels say something explicit and absolute about apiece of data, grades by definition are relative to the scale they come from.", "labels": [], "entities": []}, {"text": "It thus seems impossible to somehow unify the two schemes as they express fundamentally different ideas.", "labels": [], "entities": []}, {"text": "However, the strategies systems use to tackle scoring or meaning comparison are undoubtedly similar and should be comparable, as we argue in this paper.", "labels": [], "entities": [{"text": "meaning comparison", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7233010232448578}]}, {"text": "So in order for researchers to learn from other approaches and also compare their results to those of other systems which tackle a different task, changes to systems seem necessary and should be preferred over changes to the gold standard data.", "labels": [], "entities": []}, {"text": "In the case presented here, a meaning comparison system was turned into a scoring system by changing the machine learning component from classification to regression, which requires a certain level of system modularity.", "labels": [], "entities": []}, {"text": "Having compared the two systems using Pearson correlation and RMSE, it also makes sense to consider the relevance of these evaluation metrics.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 38, "end_pos": 57, "type": "METRIC", "confidence": 0.740472823381424}, {"text": "RMSE", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.7684940695762634}]}, {"text": "For example, it is the case that pairwise correlation assumes a normal distribution whereas datasets like the Texas corpus are heavily skewed towards correct answers (see).", "labels": [], "entities": [{"text": "Texas corpus", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.9623334109783173}]}, {"text": "also note that in distributions with zero variance, correlation is undefined, which is not a problem as such but limits the use of correlation as evaluation metric.", "labels": [], "entities": [{"text": "correlation", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.9858335852622986}]}, {"text": "propose that RMSE is better suited to the task since it captures the relative error a system makes when trying to predict scores.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.5116056799888611}]}, {"text": "However, RMSE is scale-dependent and thus RMSE values across different studies cannot be compared.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.7125405073165894}]}, {"text": "We can only suggest that in order to sufficiently describe a system's performance, several metrics need to be reported.", "labels": [], "entities": []}, {"text": "Finally, an important point concerns the quality of gold standards.", "labels": [], "entities": []}, {"text": "Given the relatively low interannotator agreement in the Texas corpus (r = 0.586, RM SE = 0.659) it seems fair to ask whether answers without perfect agreement should be used in training and testing systems at all.", "labels": [], "entities": [{"text": "interannotator agreement", "start_pos": 25, "end_pos": 49, "type": "METRIC", "confidence": 0.9054534435272217}, {"text": "Texas corpus", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9696114361286163}, {"text": "RM SE", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9357501566410065}]}, {"text": "In the CREE and CREG corpora, answers with disagreement among the annotators have either been excluded from experiments or resolved by an additional judge.", "labels": [], "entities": [{"text": "CREG corpora", "start_pos": 16, "end_pos": 28, "type": "DATASET", "confidence": 0.7378378510475159}]}, {"text": "This approach is also supported by recent literature (cf., e.g., . However, for the Texas corpus, have opted to use the arithmetic mean of the two graders as gold standard.", "labels": [], "entities": [{"text": "Texas corpus", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.9540226757526398}]}, {"text": "While mathematically a viable solution, it seems questionable whether the mean is reliable with only two graders, especially if they have not operated on the grounds of explicit guidelines.", "labels": [], "entities": []}, {"text": "It would be interesting to see whether in this case, a system trained on more, singly annotated data would perform better than one on less, doubly annotated data, as argued for by.", "labels": [], "entities": []}, {"text": "In any case, if many disagreements occur, one should ask the question whether the annotation task is defined well enough and whether machines should really be expected to perform it consistently if humans have trouble doing so.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Details on the gold standard scores in the Texas  corpus. Non-integer scores result from averaging between  raters and normalization onto the 0-5 scale.", "labels": [], "entities": [{"text": "Texas  corpus", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9615989029407501}]}, {"text": " Table 4: Detailed results of CoMiC-EN on Texas corpus", "labels": [], "entities": [{"text": "CoMiC-EN", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.87775719165802}, {"text": "Texas corpus", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.9438553154468536}]}]}