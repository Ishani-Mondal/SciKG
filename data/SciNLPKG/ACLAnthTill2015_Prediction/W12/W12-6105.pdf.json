{"title": [{"text": "Semi-supervised Learning of Naive Bayes Classifier with feature constraints", "labels": [], "entities": [{"text": "Naive Bayes Classifier", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.8226717710494995}]}], "abstractContent": [{"text": "Semi-supervised learning methods address the problem of building classifiers when labeled data is scarce.", "labels": [], "entities": []}, {"text": "Text classification is often augmented by rich set of labeled features representing a particular class.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.719950333237648}]}, {"text": "As tuple level labling is resource consuming, semi-supervised and weakly supervised learning methods are explored recently.", "labels": [], "entities": []}, {"text": "Compared to labeling data instances (documents), feature labeling takes much less effort and time.", "labels": [], "entities": [{"text": "labeling data instances (documents)", "start_pos": 12, "end_pos": 47, "type": "TASK", "confidence": 0.8091865678628286}, {"text": "feature labeling", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.753139466047287}]}, {"text": "Posterior regularization (PR) is a framework recently proposed for incorporating bias in the form prior knowledge into posterior for the label.", "labels": [], "entities": [{"text": "Posterior regularization (PR)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.820467746257782}]}, {"text": "Our work focuses on incorporating labeled features into a naive bayes classifier in a semi-supervised setting using PR.", "labels": [], "entities": []}, {"text": "Generative learning approaches utilize the unlabeled data more effectively compared to discriminative approaches in a semi-supervised setup.", "labels": [], "entities": []}, {"text": "In the current study we formulate a classification method which uses the labeled features as constraints for the posterior in a semi-supervised generative learning setting.", "labels": [], "entities": []}, {"text": "Our empirical study shows that performance gains are significant compared to an approach solely based on Generelized Expectation(GE) or limited amount of labeled data alone.", "labels": [], "entities": [{"text": "Generelized Expectation(GE)", "start_pos": 105, "end_pos": 132, "type": "METRIC", "confidence": 0.8960279941558837}]}, {"text": "We also show an application of our framework in a transfer learning setup for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.8387824594974518}]}, {"text": "As we allow labeled data as well as labeled features to be used, our setup allows the presence of limited amount of labeled data on the target side of transfer learning where feature constraints are used for transferring knowledge from source domain to target domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semi-supervised learning methods (O.) address the difficulties of integration of information contained in labeled data and unlabeled data.", "labels": [], "entities": []}, {"text": "Though labeled data is scarce, unlabeled data is abundantly available.", "labels": [], "entities": []}, {"text": "The success of Semi-supervised methods is based on the premise of using the hidden structure of unlabeled data and aligning it with the limited amount of labeled data.", "labels": [], "entities": []}, {"text": "Apart from unlabeled data, there are auxiliary forms of information in the form of labeled features.", "labels": [], "entities": []}, {"text": "For example, sentiment analysis which focuses on sentiment classification is often leveraged with sentiment lexicon, which contains prior sentiment orientation of commonly occurring words.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.9626181721687317}, {"text": "sentiment classification", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.873158186674118}]}, {"text": "They can be used as prior knowledge in building the classifiers.", "labels": [], "entities": []}, {"text": "Such auxiliary information has to be incorporated in the form of bias into the learning algorithm.", "labels": [], "entities": []}, {"text": "Though there have been efforts to build informative priors () or lexicon based classifiers in (, they have been of limited success and often interact with the model in complex ways.", "labels": [], "entities": []}, {"text": "Recently there have been research efforts from multiple perspectives addressing the same issue.", "labels": [], "entities": []}, {"text": "Generalized Expectation Criteria (GE) is one such approach for regularizing the model based on rich set of constraints.", "labels": [], "entities": [{"text": "Generalized Expectation Criteria (GE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7400711178779602}]}, {"text": "GE allows us to specify global constraints which are allowed to be arbitrary combinations of features.", "labels": [], "entities": []}, {"text": "() used GE for building classifier solely based on labeled features.", "labels": [], "entities": []}, {"text": "But one of the problems encountered while using GE criteria is, the model parameters and constraint parameters when exist together in a semi-supervised setup, increases the computational complexity of the algorithm.", "labels": [], "entities": []}, {"text": "( and addressed this issue using alternative projections approach, which is an extension of EM to discriminative learning methods.", "labels": [], "entities": []}, {"text": "Another approach for parameter estimation which is developed simultaneously is Posterior Regularization framework () which is initially proposed for generative learning methods ().", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.6764678508043289}, {"text": "Posterior Regularization", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.48730479180812836}, {"text": "generative learning", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.9386805295944214}]}, {"text": "In the initial framework in (, constraints chosen were of limited expressibility (instance based).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our algorithm on 5 datasets which are previously used by Gregory Druck at.al (.", "labels": [], "entities": []}, {"text": "We further divide the dataset into binary classification problems.", "labels": [], "entities": []}, {"text": "For datasets involving more than two classes we use multi-class classification.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.7177498042583466}]}, {"text": "We use 65/35 training/test split throughout our experiments.", "labels": [], "entities": []}, {"text": "We took 20 newsgroup, sraa and webkb from the same source as that of).", "labels": [], "entities": []}, {"text": "Ohscal and News3 represent classification problems with large number of classes.", "labels": [], "entities": [{"text": "Ohscal", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9708735942840576}, {"text": "News3", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.8951563835144043}]}, {"text": "They are taken from 2 and 3 respectively.", "labels": [], "entities": []}, {"text": "We use information gain to simulate a human-expert providing us the labeled features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance Results for varying number of labeled samples", "labels": [], "entities": []}, {"text": " Table 4: Multi Domain Sentiment Dataset for Transfer Learning", "labels": [], "entities": [{"text": "Transfer Learning", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.9674863815307617}]}]}