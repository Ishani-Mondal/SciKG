{"title": [{"text": "Automatically generated NE tagged corpora for English and Hungarian", "labels": [], "entities": []}], "abstractContent": [{"text": "Supervised Named Entity Recognizers require large amounts of annotated text.", "labels": [], "entities": []}, {"text": "Since manual annotation is a highly costly procedure, reducing the annotation cost is essential.", "labels": [], "entities": [{"text": "annotation", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9540438652038574}]}, {"text": "We present a fully automatic method to build NE annotated corpora from Wikipedia.", "labels": [], "entities": []}, {"text": "In contrast to recent work, we apply anew method, which maps the DBpedia classes into CoNLL NE types.", "labels": [], "entities": []}, {"text": "Since our method is mainly language-independent, we used it to generate corpora for English and Hungarian.", "labels": [], "entities": []}, {"text": "The corpora are freely available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named Entity Recognition (NER), the task of identifying Named Entities (NEs) in unstructured texts and classifying them into pre-selected classes, is one of the most important subtasks in many NLP tasks, such as information retrieval, information extraction or machine translation.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7832954029242197}, {"text": "information retrieval", "start_pos": 212, "end_pos": 233, "type": "TASK", "confidence": 0.8132770955562592}, {"text": "information extraction", "start_pos": 235, "end_pos": 257, "type": "TASK", "confidence": 0.8394518494606018}, {"text": "machine translation", "start_pos": 261, "end_pos": 280, "type": "TASK", "confidence": 0.7844018638134003}]}, {"text": "The NER task was introduced with the 6th Message Understanding Conference (MUC) in 1995 ().", "labels": [], "entities": [{"text": "NER task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.9242754876613617}, {"text": "Message Understanding Conference (MUC)", "start_pos": 41, "end_pos": 79, "type": "TASK", "confidence": 0.8090658138195673}]}, {"text": "In MUC shared tasks the NER consists of three subtasks: entity names, temporal and number expressions.", "labels": [], "entities": [{"text": "MUC shared tasks the NER", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.7844087958335877}]}, {"text": "Although there is a general agreement in the NER community about the inclusion of temporal expressions and some numerical expressions, the most studied types are names of persons, locations and organizations.", "labels": [], "entities": [{"text": "NER", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9510028958320618}]}, {"text": "The fourth type, called \"miscellaneous\", was introduced in the CoNLL NER tasks in 2002) and 2003, and includes proper names falling outside the three classic types.", "labels": [], "entities": [{"text": "CoNLL NER tasks", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.8259036739667257}]}, {"text": "Since then, MUC and CoNLL datasets and annotation schemes have been the major standards applied in the field of NER.", "labels": [], "entities": [{"text": "MUC", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.9030918478965759}, {"text": "CoNLL datasets", "start_pos": 20, "end_pos": 34, "type": "DATASET", "confidence": 0.8406985104084015}, {"text": "NER", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9611552953720093}]}, {"text": "The standard datasets are highly domain-specific (mostly newswire) and are restricted in size.", "labels": [], "entities": []}, {"text": "Researchers attempting to merge these datasets to get a bigger training corpus are faced with the problem of combining different tagsets and annotation schemes.", "labels": [], "entities": []}, {"text": "Manually annotating large amounts of text with linguistic information is a time-consuming, highly skilled and delicate job, but large, accurately annotated corpora are essential for building robust supervised machine learning NER systems.", "labels": [], "entities": []}, {"text": "Therefore, reducing the annotation cost is a key challenge.", "labels": [], "entities": []}, {"text": "One approach is to generate the resources automatically, another one is to use collaborative annotation and/or collaboratively constructed resources, such as Wikipedia, Wiktionary, Linked Open Data, or DBpedia.", "labels": [], "entities": []}, {"text": "In this paper we combine these approaches by automatically generating freely available NE tagged corpora from Wikipedia.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we give an overview of related work.", "labels": [], "entities": []}, {"text": "Section 3 contains a description of our method, and Section 4 shows how it is applied to Hungarian.", "labels": [], "entities": []}, {"text": "The corpus format is described in Section 5.", "labels": [], "entities": []}, {"text": "In Section 6 we present experiments and results on the newly generated datasets.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper with a summary.", "labels": [], "entities": []}], "datasetContent": [{"text": "Having the obvious advantages, an automatically generated corpus cannot serve as a gold standard dataset.", "labels": [], "entities": []}, {"text": "Then what can we do with silver standard corpora?", "labels": [], "entities": []}, {"text": "They can be very useful for improving NER in several ways: (a) for less resourced languages, they can serve as training corpora in lieu of gold standard datasets; (b) they can serve as supplementary or independent training sets for domains differing from newswire; (c) they can be sources of huge entity lists, and (d) feature extraction.", "labels": [], "entities": [{"text": "NER", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9215929508209229}, {"text": "feature extraction", "start_pos": 319, "end_pos": 337, "type": "TASK", "confidence": 0.7819328904151917}]}, {"text": "To evaluate our corpora we used a maximum entropy NE tagger, which was originally developed for labeling NEs in Hungarian texts, but can be tuned for different languages as well.", "labels": [], "entities": [{"text": "labeling NEs in Hungarian texts", "start_pos": 96, "end_pos": 127, "type": "TASK", "confidence": 0.8499980568885803}]}, {"text": "Corpus-specific features (e.g. NP chunks, WP links) were removed to get better comparability, so the feature set consists of gazetteer features; sentence start and end position; Boolean-valued orthographic properties of the word form; string-valued surface properties of the word form; and morphological information.", "labels": [], "entities": []}, {"text": "We used the CoNLL standard method for evaluation.", "labels": [], "entities": [{"text": "CoNLL standard", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.9111471176147461}]}, {"text": "According to this, an automatic labeling is correct if it gives the same start and end position, and the same NE class as the gold standard.", "labels": [], "entities": [{"text": "NE class", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9052781760692596}]}, {"text": "Based on this, precision and recall can be calculated, and the F-measure, as usual, the harmonic mean of these two values.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9996334314346313}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996763467788696}, {"text": "F-measure", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9976154565811157}]}, {"text": "The English WP corpus was evaluated against itself and a manually annotated English corpus.", "labels": [], "entities": [{"text": "English WP corpus", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8434897462526957}]}, {"text": "Since the filtered English WP corpus, containing only the sentences with NEs, is still very large, our experiments were performed with a sample of 3.5 million tokens, the size of our filtered Hungarian corpus, divided into train and test sets (90%-10%).", "labels": [], "entities": [{"text": "English WP corpus", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.6094880898793539}, {"text": "Hungarian corpus", "start_pos": 192, "end_pos": 208, "type": "DATASET", "confidence": 0.8757450580596924}]}, {"text": "For English cross-corpus evaluation the CoNLL-2003 corpus was chosen.", "labels": [], "entities": [{"text": "CoNLL-2003 corpus", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.9698081016540527}]}, {"text": "As is well known, training and testing across different corpora decreases Fmeasure.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9965506792068481}]}, {"text": "Domain differences certainly affect NER performance, and the different annotation schemes pose several compatibility problems.", "labels": [], "entities": [{"text": "NER", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9687373042106628}]}, {"text": "showed that each set of gold standard training data performs better on corresponding test sets than on test sets from other sources.", "labels": [], "entities": []}, {"text": "The situation here is similar (see for results): the NE tagger trained on WP does not achieve as high performance tested against CoNLL test set (enwikiCoNLL) as one trained on its own train set (enwikienwiki).", "labels": [], "entities": [{"text": "NE tagger", "start_pos": 53, "end_pos": 62, "type": "TASK", "confidence": 0.8491112291812897}, {"text": "CoNLL test set", "start_pos": 129, "end_pos": 143, "type": "DATASET", "confidence": 0.967948317527771}]}, {"text": "WP-derived corpora can also be used for improving NER accuracy in other ways.", "labels": [], "entities": [{"text": "NER", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.982696533203125}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9154260158538818}]}, {"text": "First, we collected gazetteer lists from the corpus for each NE category, which improved the overall F-measure given to the NE tagger training and testing on CoNLL dataset (CoNLL with wikilists).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9982081651687622}, {"text": "CoNLL dataset", "start_pos": 158, "end_pos": 171, "type": "DATASET", "confidence": 0.9529355764389038}]}, {"text": "A second trial was labeling the CoNLL datasets by the model trained on WP corpus, and giving these labels as extra features to the next CoNLL train (CoNLL with wikitags).", "labels": [], "entities": [{"text": "CoNLL datasets", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.9456925690174103}, {"text": "WP corpus", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.9639498293399811}, {"text": "CoNLL train", "start_pos": 136, "end_pos": 147, "type": "DATASET", "confidence": 0.9078588485717773}]}, {"text": "Both methods result in improved F-measure on CoNLL test set.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9982901215553284}, {"text": "CoNLL test set", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.9513948758443197}]}, {"text": "Since in Hungarian NE tagging we followed the Szeged NER corpus annotation guidelines, we performed the experiments on this dataset.", "labels": [], "entities": [{"text": "NE tagging", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.662049800157547}, {"text": "Szeged NER corpus annotation", "start_pos": 46, "end_pos": 74, "type": "DATASET", "confidence": 0.7566898167133331}]}, {"text": "Hungarian results are similar to the English ones (see), the only difference is that F-measures for Hungarian are significantly higher.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9985815286636353}]}, {"text": "This can be due to the fact that the MISC category for Hungarian contains less types of names, thus the inconsistency of this class is smaller (cf. Section 4).", "labels": [], "entities": [{"text": "MISC category for Hungarian", "start_pos": 37, "end_pos": 64, "type": "DATASET", "confidence": 0.7830709964036942}]}, {"text": "In contrast to the CoNLL corpus, the Szeged NER corpus was accurately annotated with an inter-annotator agreement over 99%.", "labels": [], "entities": [{"text": "CoNLL corpus", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9422282576560974}, {"text": "Szeged NER corpus", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.821286678314209}]}, {"text": "Due to the quite good F-measure of training onour Hungarian train corpus and testing on the corresponding test set, our Hungarian corpus can serve as a training corpus to build NE taggers for nonnewswire domains.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.998784601688385}, {"text": "Hungarian train corpus", "start_pos": 50, "end_pos": 72, "type": "DATASET", "confidence": 0.9079440633455912}, {"text": "Hungarian corpus", "start_pos": 120, "end_pos": 136, "type": "DATASET", "confidence": 0.9436618983745575}]}], "tableCaptions": [{"text": " Table 2: Corpus size and NE density.", "labels": [], "entities": []}]}