{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 54-62, HOO 2012: A Report on the Preposition and Determiner Error Correction Shared Task", "labels": [], "entities": [{"text": "HOO 2012", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.8814672827720642}]}], "abstractContent": [{"text": "Incorrect usage of prepositions and determin-ers constitute the most common types of errors made by non-native speakers of English.", "labels": [], "entities": []}, {"text": "It is not surprising, then, that there has been a significant amount of work directed towards the automated detection and correction of such errors.", "labels": [], "entities": [{"text": "automated detection and correction", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.7229516208171844}]}, {"text": "However, to date, the use of different data sets and different task definitions has made it difficult to compare work on the topic.", "labels": [], "entities": []}, {"text": "This paper reports on the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners, where systems developed by 14 teams from around the world were evaluated on the same previously unseen errorful text.", "labels": [], "entities": [{"text": "HOO 2012 shared task", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.5545788258314133}, {"text": "error detection and correction", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.6906668245792389}]}], "introductionContent": [{"text": "It is widely recognized that the correct usage of determiners and prepositions in English is a major problem area for non-native speakers of the language.", "labels": [], "entities": []}, {"text": "The issues here have been explored and discussed extensively in the literature; an excellent and up-to-date summary is available in (.", "labels": [], "entities": []}, {"text": "However, the various teams that have attempted to tackle these problems so far have tended to use slightly different task specifications, and different data sets for evaluation; this makes it very dif-ficult to compare the results achieved using different approaches.", "labels": [], "entities": []}, {"text": "To address this problem, the aim of the HOO 2012 Shared Task was to provide a forum for the comparative evaluation of different approaches to the correction of these errors.", "labels": [], "entities": [{"text": "HOO 2012 Shared Task", "start_pos": 40, "end_pos": 60, "type": "DATASET", "confidence": 0.7716877460479736}]}, {"text": "The shared task provides a common training dataset, a shared evaluation framework, and a set of previously unseen test data.", "labels": [], "entities": []}, {"text": "These proceedings contain detailed reports by all 14 teams who participated in HOO 2012.", "labels": [], "entities": [{"text": "HOO 2012", "start_pos": 79, "end_pos": 87, "type": "TASK", "confidence": 0.5882681310176849}]}, {"text": "The present paper provides a summary of the task and its evaluation, and a report on the results of that evaluation.", "labels": [], "entities": []}, {"text": "Section 2 provides an overview of the task and the timeline across which it was carried out; Section 3 provides details of the participating teams; Section 4 describes the training and test data in more detail; Section 5 presents the results of the evaluation; and Section 6 provides some concluding remarks and discussion, reflecting on lessons learned.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given real non-native speaker data that contains a wide range of errors other than those that we were particularly concerned within this shared task, we were faced with three alternatives in how we prepared the data for use in the task.", "labels": [], "entities": []}, {"text": "1. We could provide the data with all original errors in place.", "labels": [], "entities": []}, {"text": "2. We could provide the data with all but the preposition and determiner errors corrected.", "labels": [], "entities": []}, {"text": "3. We could provide the data with selected errors corrected or replaced.", "labels": [], "entities": []}, {"text": "The problem with the first of these options, of course, is that other errors that appear in the context: Results after revisions, preposition errors only of a preposition or determiner error could confuse a system focussed only on preposition or determiner errors; if the surrounding context contains errors, then it cannot be relied upon to deliver the kinds of features that one would expect to find in wellformed text.", "labels": [], "entities": []}, {"text": "To partially address this, many teams ran a spelling correction process on the texts prior to applying their techniques; but this only catches a small proportion of the potential problems.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.9000680148601532}]}, {"text": "However, the second option has the opposite problem: by removing all the other errors from the text, we would be providing a very artificial dataset where one assumes some other process has fixed all the other errors before the errors of interest here are addressed.", "labels": [], "entities": []}, {"text": "While there are some types of errors that might sensibly be addressed before others in a pipeline, in general this is not a very plausible model; any real system is going to have to address noisy data containing many different kinds of errors simultaneously.", "labels": [], "entities": []}, {"text": "A third alternative, that of selectively removing or correcting errors, is something of a middle road, and has been used in other work using the CLC data: in particular, removes from the data sentences where some other error appears immediately next to a preposition or determiner error.", "labels": [], "entities": [{"text": "CLC data", "start_pos": 145, "end_pos": 153, "type": "DATASET", "confidence": 0.9028169810771942}]}, {"text": "In the end, we opted for the first alternative here, on the grounds that this is the best approximation to the real task of non-native speaker error correction.", "labels": [], "entities": [{"text": "speaker error correction", "start_pos": 135, "end_pos": 159, "type": "TASK", "confidence": 0.6855363647143046}]}, {"text": "The third alternative would also have been possible, but we were concerned about the impact on the size of our test data set that would result from carrying out this process across the board.", "labels": [], "entities": []}, {"text": "However, in the revision step described in Section 4.2, we did remove instances of a particular error type, where a preposition error was immediately followed by a verb error; consider the following sentence and its correction.", "labels": [], "entities": []}, {"text": "What do you do for trying to save the wild life?", "labels": [], "entities": []}, {"text": "b. What do you do to try to save the wild life?", "labels": [], "entities": []}, {"text": "The compound nature of these errors meant that teams were unlikely to correct them; and it might be argued that they are not preposition errors in the conventional sense.", "labels": [], "entities": []}, {"text": "However, we did not remove these instances uniformly, so some still remain in the test data.", "labels": [], "entities": []}, {"text": "An orthogonal issue with regard to the HOO annotation scheme is that we require precise identification of error locations and accurate specification of these locations at a character-offset level in our standoff edit notation.", "labels": [], "entities": []}, {"text": "It is often inaccuracies at this level that contribute to the differences between a team's detection score and the corresponding recognition score.", "labels": [], "entities": []}, {"text": "While precise character offset information is important for some error correction tasks (for example, one would not want an automated corrector to insert corrections misplaced by one character), arguably it is too strict in the present circumstances.", "labels": [], "entities": [{"text": "error correction", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7428267896175385}]}, {"text": "propose an alternative evaluation scheme which, along with other properties, overcomes this by operating in terms of tokens rather than character offsets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples of the six base error types", "labels": [], "entities": []}, {"text": " Table 4: Requests for corrections to the gold- standard data", "labels": [], "entities": [{"text": "corrections", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.7785186171531677}, {"text": "gold- standard data", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.8065487593412399}]}, {"text": " Table 5: Results before revisions, all errors", "labels": [], "entities": []}, {"text": " Table 6: Results after revisions, all errors", "labels": [], "entities": []}, {"text": " Table 7: Results before revisions, preposition errors only", "labels": [], "entities": []}, {"text": " Table 8: Results after revisions, preposition errors only", "labels": [], "entities": []}, {"text": " Table 9: Results before revisions, determiner errors only", "labels": [], "entities": [{"text": "determiner errors", "start_pos": 36, "end_pos": 53, "type": "METRIC", "confidence": 0.7065195143222809}]}, {"text": " Table 10: Results after revisions, determiner errors only", "labels": [], "entities": [{"text": "determiner errors", "start_pos": 36, "end_pos": 53, "type": "METRIC", "confidence": 0.7611877620220184}]}]}