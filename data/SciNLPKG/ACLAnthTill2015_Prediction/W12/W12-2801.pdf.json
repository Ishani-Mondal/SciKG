{"title": [{"text": "Learning to Interpret Natural Language Instructions", "labels": [], "entities": [{"text": "Learning to Interpret Natural Language Instructions", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.6329882244269053}]}], "abstractContent": [{"text": "This paper addresses the problem of training an artificial agent to follow verbal instructions representing high-level tasks using a set of instructions paired with demonstration traces of appropriate behavior.", "labels": [], "entities": []}, {"text": "From this data, a mapping from instructions to tasks is learned, enabling the agent to carryout new instructions in novel environments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning to interpret language from a situated context has become a topic of much interest in recent years ().", "labels": [], "entities": [{"text": "interpret language from a situated context", "start_pos": 12, "end_pos": 54, "type": "TASK", "confidence": 0.7989809910456339}]}, {"text": "Instead of using annotated training data consisting of sentences and their corresponding logical forms), most of these approaches leverage non-linguistic information from a situated context as their primary source of supervision.", "labels": [], "entities": []}, {"text": "These approaches have been applied to various tasks such as following navigational instructions), software control (, semantic parsing () and learning to play games based on text.", "labels": [], "entities": [{"text": "following navigational instructions", "start_pos": 60, "end_pos": 95, "type": "TASK", "confidence": 0.7068203091621399}, {"text": "semantic parsing", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.7060934901237488}]}, {"text": "In this paper, we present an approach to interpreting language instructions that describe complex multipart tasks by learning from pairs of instructions and behavioral traces containing a sequence of primitive actions that result in these instructions being properly followed.", "labels": [], "entities": [{"text": "interpreting language instructions", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.8595700860023499}]}, {"text": "We do not assume a oneto-one mapping between instructions and primitive actions.", "labels": [], "entities": []}, {"text": "Our approach uses three main subcomponents: (1) recognizing intentions from observed behavior using variations of Inverse Reinforcement Learning (IRL) methods; (2) translating instructions to task specifications using Semantic Parsing (SP) techniques; and (3) creating generalized task specifications to match user intentions using probabilistic Task Abstraction (TA) methods.", "labels": [], "entities": []}, {"text": "We describe our system architecture and a learning scenario.", "labels": [], "entities": []}, {"text": "We present preliminary results fora simplified version of our system that uses a unigram language model, minimal abstraction, and simple inverse reinforcement learning.", "labels": [], "entities": []}, {"text": "Early work on grounded language learning used features based on n-grams to represent the natural language input (.", "labels": [], "entities": []}, {"text": "More recent methods have relied on a richer representation of linguistic data, such as syntactic dependency trees) and semantic templates) to address the complexity of the natural language input.", "labels": [], "entities": []}, {"text": "Our approach uses a flexible framework that allows us to incorporate various degrees of linguistic knowledge available at different stages in the learning process (e.g., from dependency relations to a full-fledged semantic model of the domain learned during training).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present a simplified version of our system with a unigram language model, inverse reinforcement learning and minimal abstraction.", "labels": [], "entities": []}, {"text": "We call this version Model 0.", "labels": [], "entities": []}, {"text": "The input to Model 0 is a set of verbal instructions paired with demonstrations of appropriate behavior.", "labels": [], "entities": []}, {"text": "It uses an EM-style algorithm to estimate the probability distribution of words conditioned on reward functions (the parameters).", "labels": [], "entities": []}, {"text": "With this information, when the system receives anew command, it can behave in away that maximizes its reward given the posterior probabilities of the possible reward functions given the words.", "labels": [], "entities": []}, {"text": "Algorithm 1 shows our EM-style Model 0.", "labels": [], "entities": []}, {"text": "For all possible reward-demonstration pairs, the E-step of EM estimates z ji = Pr(R j |(S i , Ti )), the probability that reward function R j produced sentencetrajectory pair (S i , Ti ), This estimate is given by the equation below: where Si is the i th sentence, Ti is the trajectory demonstrated for verbal command Si , and wk is an element in the set of all possible words (vocabulary).", "labels": [], "entities": [{"text": "Pr", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9521262049674988}]}, {"text": "If the reward functions R j are known ahead of time, Pr(T i |R j ) can be obtained directly by solving the MDP and estimating the probability of trajectory Ti under a Boltzmann policy with respect to R j . If the R j s are not known, EM can estimate them by running IRL during the M-step).", "labels": [], "entities": [{"text": "Pr", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9839221239089966}, {"text": "IRL", "start_pos": 266, "end_pos": 269, "type": "METRIC", "confidence": 0.7747184038162231}]}, {"text": "The M-step in Algorithm 1 uses the current estimates of z ji to further refine the probabilities x kj = Pr(w k |R j ): where is a smoothing parameter, X is a normalizing factor and N (S i ) is the number of words in sentence Si . To illustrate our Model 0 performance, we selected as training data six sentences for two tasks (three sentences for each task) from a dataset we have collected using Amazon Mechanical Turk for the Cleanup Domain.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 397, "end_pos": 419, "type": "DATASET", "confidence": 0.9099754492441813}]}, {"text": "We show the training data in.", "labels": [], "entities": []}, {"text": "We obtained the reward function for each task using MLIRL, computed the Pr(T i |R j ), then ran Algorithm 1 and obtained the parameters Pr(w k |R j ).", "labels": [], "entities": [{"text": "MLIRL", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.8191861510276794}, {"text": "Algorithm", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.8517280220985413}]}, {"text": "After this training process, we presented the agent with anew task.", "labels": [], "entities": []}, {"text": "She is given the instruction S N : Go to greenroom. and a starting state, somewhere in the same grid.", "labels": [], "entities": []}, {"text": "Using parameters Pr(w k |R j ), the agent can estimate: 4", "labels": [], "entities": []}], "tableCaptions": []}