{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 157-162, A Comparison of Greedy and Optimal Assessment of Natural Language Student Input Using Word-to-Word Similarity Metrics", "labels": [], "entities": []}], "abstractContent": [{"text": "We present in this paper a novel, optimal semantic similarity approach based on word-to-word similarity metrics to solve the important task of assessing natural language student input in dialogue-based intelligent tutoring systems.", "labels": [], "entities": []}, {"text": "The optimal matching is guaranteed using the sailor assignment problem, also known as the job assignment problem, a well-known combinatorial optimization problem.", "labels": [], "entities": [{"text": "job assignment problem", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.7806213100751241}]}, {"text": "We compare the optimal matching method with a greedy method as well as with a baseline method on data sets from two intelligent tutoring systems, AutoTutor and iSTART.", "labels": [], "entities": []}], "introductionContent": [{"text": "We address in this paper the important task of assessing natural language student input in dialogue-based tutoring systems where the primary form of interaction is natural language.", "labels": [], "entities": []}, {"text": "Students provide their responses to tutor's requests by typing or speaking their responses.", "labels": [], "entities": []}, {"text": "Therefore, in dialogue-based tutoring systems understanding students' natural language input becomes a crucial step towards building an accurate student model, i.e. assessing the student's level of understanding, which in turn is important for optimum feedback and scaffolding and ultimately impacts the tutoring's effectiveness at inducing learning gains on the student user.", "labels": [], "entities": []}, {"text": "We adopt a semantic similarity approach to assess students' natural language input in intelligent tutoring systems.", "labels": [], "entities": []}, {"text": "The semantic similarity approach to language understanding derives the meaning of a target text, e.g. a student sentence, by comparing it with another text whose meaning is known.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.735614001750946}]}, {"text": "If the target text is semantically similar to the known-meaning text then we know the target's meaning as well.", "labels": [], "entities": []}, {"text": "Semantic similarity is one of the two major approaches to language understanding, a central topic in Artificial Intelligence.", "labels": [], "entities": [{"text": "Semantic similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8459352552890778}, {"text": "language understanding", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7265292704105377}]}, {"text": "The alternative approach is full understanding.", "labels": [], "entities": []}, {"text": "The full understanding approach is not scalable due to prohibitive costs to encode world and domain knowledge which are needed for full understanding of natural language.", "labels": [], "entities": []}, {"text": "To illustrate the problem of assessing natural language student input in dialogue-based tutoring systems using a semantic similarity approach, we consider the example below from experiments with AutoTutor (), a dialogue-based tutoring system.", "labels": [], "entities": []}, {"text": "Expert Answer: The force of the earth's gravity, being vertically down, has no effect on the object's horizontal velocity Student Input: The horizontal component of motion is not affected by vertical forces In this example, the student input, also called contribution, is highly similar to the correct expert answer, called expectation, allowing us to conclude that the student contribution is correct.", "labels": [], "entities": []}, {"text": "A correct response typically triggers positive feedback from the tutor.", "labels": [], "entities": []}, {"text": "The expert answer could also bean anticipated wrong answer, usually called a misconception.", "labels": [], "entities": []}, {"text": "A student contribution similar to a misconception would trigger a misconception correction strategy.", "labels": [], "entities": [{"text": "misconception correction", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.743981122970581}]}, {"text": "We model the problem of assessing natural language student input in tutoring systems as a paraphrase identification problem ().", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.7132313400506973}]}, {"text": "The student input assessment problem has been also modeled as a textual entailment task in the past).", "labels": [], "entities": []}, {"text": "Our novel method to assess a student contribution against an expert-generated answer relies on the compositionality principle and the sailor assignment algorithm that was proposed to solve the assignment problem, a well-known combinatorial optimization problem.", "labels": [], "entities": []}, {"text": "The sailor assignment algorithm optimally assigns sailors to ships based on the fitness of the sailors' skills to the ships' needs.", "labels": [], "entities": [{"text": "sailor assignment", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8790125548839569}]}, {"text": "In our case, we would like to optimally match words in the student input (the sailors) to words in the expert-generated answer (the ships) based on how well the words in student input (the sailors) fit the words in the expert answer (the ships).", "labels": [], "entities": []}, {"text": "The fitness between the words is nothing else but their similarity according to some metric of word similarity.", "labels": [], "entities": []}, {"text": "We use the WordNet word-to-word similarity metrics () and Latent Semantic Analysis (.", "labels": [], "entities": []}, {"text": "The methods proposed so far that rely on the principle of compositionality to compute the semantic similarity of longer texts have been primarily greedy methods.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, nobody proposed an optimal solution based on the principle of compositionality and word-to-word similarity metrics for the student input assessment problem.", "labels": [], "entities": []}, {"text": "It is important to note that the optimal method proposed here is generally applicable to compute the similarity of any texts.", "labels": [], "entities": []}, {"text": "We provide experimental results on two datasets provided to us by researchers developing two world-class dialogue-based tutoring systems: AutoTutor () and iSTART ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We present in this section the datasets we used in our experiments and the results obtained.", "labels": [], "entities": []}, {"text": "As we already mentioned, we use two datasets containing real student answers from two dialogue-based tutoring systems: AutoTutor () and iSTART).", "labels": [], "entities": []}, {"text": "The AutoTutor dataset contains 125 student contribution -expert answer pairs and the correct paraphrase judgment, TRUE or FALSE, as assigned by human experts.", "labels": [], "entities": [{"text": "AutoTutor dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8749614953994751}, {"text": "TRUE", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9944076538085938}, {"text": "FALSE", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.9090834259986877}]}, {"text": "The target domain is conceptual physics.", "labels": [], "entities": []}, {"text": "One expert physicist rated the degree to which particular speech acts expressed during AutoTutor training matched particular expert answers.", "labels": [], "entities": []}, {"text": "These judgments were made on a sample of 25 physics expectations (i.e., correct expert answers) and 5 randomly sampled student answers per expectation, yielding a total of 125 pairs of expressions.", "labels": [], "entities": []}, {"text": "The learner answers were always responses to the first hint for that expectation.", "labels": [], "entities": []}, {"text": "The E-S pairs were graded by Physics experts on a scale of 1-4 (4 being perfect answer).", "labels": [], "entities": []}, {"text": "This rubric could be mapped onto a binary TRUE-FALSE rubric: scores 3 and 4 equal a TRUE decision and 1 and 2 equal a FALSE decision.", "labels": [], "entities": [{"text": "TRUE", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.8815138936042786}, {"text": "FALSE", "start_pos": 118, "end_pos": 123, "type": "METRIC", "confidence": 0.7572464942932129}]}, {"text": "We ended up with 36 FALSE and 89 TRUE entailment pairs, i.e. a 28.8% versus 71.2% split (as compared to the 50-50% split of RTE data).", "labels": [], "entities": [{"text": "FALSE", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9952064156532288}, {"text": "TRUE", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9539703130722046}, {"text": "RTE data", "start_pos": 124, "end_pos": 132, "type": "DATASET", "confidence": 0.8596217036247253}]}, {"text": "The iSTART data set, also known as the User Language Paraphrase Corpus) comprises annotations of paraphrase relations between student responses and ideal answers.", "labels": [], "entities": [{"text": "iSTART data set", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8889637589454651}]}, {"text": "The corpus contains 1998 pairs collected from previous student iSTART sessions and is divided into training (1499 instances) and testing (499 instances) subsets.", "labels": [], "entities": []}, {"text": "The training subset contains 54% positive instances while testing contains 55% positive instances.", "labels": [], "entities": []}, {"text": "The iSTART texts represent high school students' attempts to self-explain biology textbook texts.", "labels": [], "entities": [{"text": "iSTART texts", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8854144513607025}]}, {"text": "To evaluate the performance of our methods, we compare the methods' judgments with the expert judgments.", "labels": [], "entities": []}, {"text": "The percentage of matching judgments provides the accuracy of the run, i.e. the fraction of correct responses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9994341731071472}]}, {"text": "We also report kappa statistics which indicate agreement between our methods' output and the human-expert judgments for each), the re-annotated AutoTutor data by a second rater with inter-annotator agreement of 0.606, and the ULPC test subset).", "labels": [], "entities": [{"text": "AutoTutor data", "start_pos": 144, "end_pos": 158, "type": "DATASET", "confidence": 0.8190154135227203}, {"text": "ULPC test subset", "start_pos": 226, "end_pos": 242, "type": "DATASET", "confidence": 0.8548021912574768}]}, {"text": "For the ULPC corpus the methods have been trained on the training subset, an optimum threshold has been learned (such that scores above the threshold mean TRUE paraphrases) which is then used on the test data.", "labels": [], "entities": [{"text": "ULPC corpus", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.9242088496685028}, {"text": "TRUE", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9922174215316772}]}, {"text": "Since the AutoTutor dataset is small, we only report results on it as a whole, i.e. only training.", "labels": [], "entities": [{"text": "AutoTutor dataset", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.9137711822986603}]}, {"text": "We report for each corpus a baseline method of guessing all the time the dominant class in the dataset (which is TRUE paraphrase for all three datasets), a pure greedy method (Greedy label in the first column of the tables), a greedy method applied to the words paired by the optimum method (optGreedy), and the results with the optimum matching method (Optimum).", "labels": [], "entities": [{"text": "TRUE", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.8756897449493408}]}, {"text": "Overall, the optimum method offered better performance in terms of accuracy and kappa statistics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9996092915534973}]}, {"text": "The greedy method yields results that are close.", "labels": [], "entities": []}, {"text": "In fact, when analyzed as raw scores instead of binary decisions (as is the case when computing accuracy) the greedy raw score are on average very similar to the optimum scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9910629987716675}]}, {"text": "For instance, for the LSA word-to-word similarity metric which provided best accuracy results on the ULPC dataset (accuracy=.643 for optimum and .615 for greedy), the average raw scores are .563 (using optimum matching) and .567 (using greedy matching).", "labels": [], "entities": [{"text": "LSA word-to-word similarity", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.5371084610621134}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.998174786567688}, {"text": "ULPC dataset", "start_pos": 101, "end_pos": 113, "type": "DATASET", "confidence": 0.9798211455345154}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9981513619422913}]}, {"text": "One reason for why they are so closed is that in optimum matching we have one-to-one word matches while in the greedy matching manyto-one matches are possible.", "labels": [], "entities": []}, {"text": "That is, two words v and w from text T1 can be matched to same wordy in text T2 in the greedy method.", "labels": [], "entities": []}, {"text": "If we enforce that only one-to-one matches are possible in the greed method as in the optimum method, then we obtain the optGreedy method.", "labels": [], "entities": []}, {"text": "The optGreedy method does work better than the pure greedy method (Greedy in the tables).", "labels": [], "entities": [{"text": "Greedy", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9643402695655823}]}, {"text": "Another reason for why the raw scores are close for greedy and optimum is the fact that student input and expert answers in both the AutoTutor and ULPC corpora are sharing many words in common (>.50).", "labels": [], "entities": [{"text": "ULPC corpora", "start_pos": 147, "end_pos": 159, "type": "DATASET", "confidence": 0.8243029713630676}]}, {"text": "This is the case because the dialogue is highly contextualized around a given, e.g. physics, problem.", "labels": [], "entities": []}, {"text": "In the answer, both students and experts refer to the entities and interactions in the problem statement which leads to high identical word overlap.", "labels": [], "entities": []}, {"text": "Identical words lead to perfect word-to-word similarity scores (=1.00) increasing the overall similarity score of the two sentences in both the greedy and optimum method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Accuracy/kappa on AutoTutor data (* indicates statistical significance over the baseline method at p<0.005 level).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991758465766907}, {"text": "AutoTutor data", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.8611405491828918}]}, {"text": " Table 2. Accuracy/kappa on AutoTutor data with user annotations (* indicates statistical significance over the baseline  method at p<0.005 level).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989933371543884}, {"text": "AutoTutor data", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.8690202534198761}]}, {"text": " Table 3. Accuracy/kappa on ULPC test data (all results are statistically different from the baseline at p<0.005 level).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991033673286438}, {"text": "ULPC test data", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9515854716300964}]}]}