{"title": [{"text": "A Challenge Set for Advancing Language Modeling", "labels": [], "entities": [{"text": "Advancing Language Modeling", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.7995504140853882}]}], "abstractContent": [{"text": "In this paper, we describe anew, publicly available corpus intended to stimulate research into language modeling techniques which are sensitive to overall sentence coherence.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7209837734699249}]}, {"text": "The task uses the Scholastic Aptitude Test's sentence completion format.", "labels": [], "entities": []}, {"text": "The test set consists of 1040 sentences, each of which is missing a content word.", "labels": [], "entities": []}, {"text": "The goal is to select the correct replacement from amongst five alternates.", "labels": [], "entities": []}, {"text": "In general, all of the options are syntactically valid, and reasonable with respect to local N-gram statistics.", "labels": [], "entities": []}, {"text": "The set was generated by using an N-gram language model to generate along list of likely words, given the immediate context.", "labels": [], "entities": []}, {"text": "These options were then hand-groomed, to identify four decoys which are globally incoherent, yet syntactically correct.", "labels": [], "entities": []}, {"text": "To ensure the right to public distribution, all the data is derived from out-of-copyright materials from Project Gutenberg.", "labels": [], "entities": []}, {"text": "The test sentences were derived from five of Conan Doyle's Sherlock Holmes novels, and we provide a large set of Nineteenth and early Twentieth Century texts as training material.", "labels": [], "entities": []}], "introductionContent": [{"text": "Perhaps beginning with Claude Shannon's use of N-gram statistics to compute the perplexity of letter sequences, N-gram models have grown to be the most commonly used type of language model inhuman language technologies.", "labels": [], "entities": []}, {"text": "At the word level, N-gram modeling techniques have been extensively refined, with stateof-the-art techniques based on smoothed N-gram counts (), multi-layer perceptrons ( and maximumentropy models.", "labels": [], "entities": [{"text": "N-gram modeling", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7161926925182343}]}, {"text": "Trained on large amounts of data, these methods have proven very effective in both speech recognition and machine translation applications.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8354738056659698}, {"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7502968907356262}]}, {"text": "Concurrent with the refinement of N-gram modeling techniques, there has been an important stream of research focused on the incorporation of syntactic and semantic information).", "labels": [], "entities": []}, {"text": "Since intuitively, language is about expressing meaning in a highly structured syntactic form, it has come as something of a surprise that the improvements from these methods have been modest, and the methods have yet to be widely adopted in non-research systems.", "labels": [], "entities": []}, {"text": "One explanation for this is that the tasks to which language modeling has been most extensively applied are largely soluble with local information.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7190420627593994}]}, {"text": "In the speech recognition application, there is a fundamental confluence of acoustic and linguistic information, and the language model can bethought of as resolving ambiguity only between acoustically confusable words ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7753553688526154}]}, {"text": "Since words which are acoustically similar, e.g. \"bill\" and \"spill\" usually appear in very different textual contexts, the local information of an N-gram language model maybe adequate to distinguish them.", "labels": [], "entities": []}, {"text": "To a lesser degree, in a machine translation application,.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7416538596153259}]}, {"text": "One of the characters in Milton Murayama's novel is considered because he deliberately defies an oppressive hierarchical society.", "labels": [], "entities": []}, {"text": "the potential phrase translations maybe similar in meaning and local information may again suffice to make a good selection.", "labels": [], "entities": []}, {"text": "In this paper, we present a language processing corpus which has been explicitly designed to be nonsolvable using purely N-gram based methods, and which instead requires some level of semantic processing.", "labels": [], "entities": []}, {"text": "To do this, we draw inspiration from the standardized testing paradigm, and propose a sentence completion task along the lines of that found in the widely used Scholastic Aptitude Test.", "labels": [], "entities": [{"text": "sentence completion task", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.7773332397143046}]}, {"text": "In this type of question, one is given a sentence with one or two words removed, and asked to select from among a set of five possible insertions.", "labels": [], "entities": []}, {"text": "Two examples of SAT test questions are shown in.", "labels": [], "entities": []}, {"text": "As can be seen, the options available all make sense from the local N-gram point of view, and are all syntactically valid; only semantic considerations allow the correct answer to be distinguished.", "labels": [], "entities": []}, {"text": "We believe this sort of question is useful for two key reasons: first, its full solution will require language modeling techniques which are qualitatively different than N-grams; and secondly, the basic task formulation has been externally determined and is a widely used method for assessing human abilities.", "labels": [], "entities": []}, {"text": "Unfortunately, to date no publicly available corpus of such questions has been released.", "labels": [], "entities": []}, {"text": "The contribution of this work is to release a public corpus of sentence completion questions designed to stimulate research in language modeling technology which moves beyond N-grams to explicitly address global sentence coherence.", "labels": [], "entities": [{"text": "sentence completion questions", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.8028831481933594}]}, {"text": "The corpus is based purely on out-of-copyright data from Project Gutenberg, thus allowing us to distribute it.", "labels": [], "entities": []}, {"text": "The test questions consist of sentences taken from five Sherlock Holmes novels.", "labels": [], "entities": []}, {"text": "In each, a word has been removed, and the task is to choose from among five alternatives.", "labels": [], "entities": []}, {"text": "One of the options is the original word, and the other four \"decoys\" have been generated from an Ngram language model using local context.", "labels": [], "entities": []}, {"text": "Sampling from an N-gram model is done to generate alternates which make sense locally, but for which there is no other reason to expect them to make sense globally.", "labels": [], "entities": []}, {"text": "To ensure that synonyms of the correct answer are not present, and that the options are syntactically reasonable, the decoys have been hand selected from among a large number of possibilities suggested by the N-gram model.", "labels": [], "entities": []}, {"text": "The training data consists of approximately 500 out-of-copyright Nineteenth and early Twentieth century novels, also from Project Gutenberg.", "labels": [], "entities": []}, {"text": "We expect that the successful development of models of global coherence will be useful in a variety of tasks, including: \u2022 the interactive generation of sentence completion questions for vocabulary tutoring applications; \u2022 proof-reading; \u2022 automated grading of essays and other student work; and \u2022 sentence generation in free-form dialog applications.", "labels": [], "entities": [{"text": "interactive generation of sentence completion questions", "start_pos": 127, "end_pos": 182, "type": "TASK", "confidence": 0.6433609426021576}, {"text": "sentence generation", "start_pos": 298, "end_pos": 317, "type": "TASK", "confidence": 0.7828492224216461}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the process by which we made the corpus.", "labels": [], "entities": []}, {"text": "Section 3 provides guidance as to the proper use of the data.", "labels": [], "entities": []}, {"text": "In Section 4, we present baseline results using several simple automated methods for answering the questions.", "labels": [], "entities": []}, {"text": "Finally, in Section 5, we discuss related work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 summarizes our benchmark study. First, for  reference, we had an unaffiliated human answer a  random subset of 100 questions. Ninety-one per- cent were answered correctly, showing that scores  in the range of 90% are reasonable to expect. Sec- ondly, we tested the performance of the same model", "labels": [], "entities": []}]}