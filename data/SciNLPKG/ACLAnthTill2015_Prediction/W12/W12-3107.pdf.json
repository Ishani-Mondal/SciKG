{"title": [{"text": "SPEDE: Probabilistic Edit Distance Metrics for MT Evaluation", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9638613164424896}]}], "abstractContent": [{"text": "This paper describes Stanford University's submission to the Shared Evaluation Task of WMT 2012.", "labels": [], "entities": [{"text": "Shared Evaluation Task of WMT 2012", "start_pos": 61, "end_pos": 95, "type": "TASK", "confidence": 0.7117203076680502}]}, {"text": "Our proposed metric (SPEDE) computes probabilistic edit distance as predictions of translation quality.", "labels": [], "entities": []}, {"text": "We learn weighted edit distance in a probabilistic finite state machine (pFSM) model, where state transitions correspond to edit operations.", "labels": [], "entities": []}, {"text": "While standard edit distance models cannot capture long-distance word swapping or cross alignments, we rectify these shortcomings using a novel pushdown automaton extension of the pFSM model.", "labels": [], "entities": [{"text": "word swapping", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.7438437342643738}]}, {"text": "Our models are trained in a regression framework, and can easily incorporate a rich set of linguistic features.", "labels": [], "entities": []}, {"text": "Evaluated on two different prediction tasks across a diverse set of datasets, our methods achieve state-of-the-art correlation with human judgments.", "labels": [], "entities": []}], "introductionContent": [{"text": "We describe the Stanford Probabilistic Edit Distance Evaluation (SPEDE) metric, which makes predictions of translation quality by computing weighted edit distance.", "labels": [], "entities": []}, {"text": "We model weighted edit distance in a probabilistic finite state machine (pFSM), where state transitions correspond to edit operations.", "labels": [], "entities": []}, {"text": "The weights of the edit operations are then automatically learned in a regression framework.", "labels": [], "entities": []}, {"text": "One of the major contributions of this paper is a novel extension of the pFSM model into a probabilistic Pushdown Automaton (pPDA), which enhances traditional editdistance models with the ability to model phrase shift and word swapping.", "labels": [], "entities": [{"text": "phrase shift", "start_pos": 205, "end_pos": 217, "type": "TASK", "confidence": 0.7592313885688782}, {"text": "word swapping", "start_pos": 222, "end_pos": 235, "type": "TASK", "confidence": 0.732301726937294}]}, {"text": "Furthermore, we give anew loglinear parameterization to the pFSM model, which allows it to easily incorporate rich linguistic features.", "labels": [], "entities": []}, {"text": "We conducted extensive experiments on a diverse set of standard evaluation data sets (NIST OpenMT06, 08; WMT06, 07, 08).", "labels": [], "entities": [{"text": "NIST OpenMT06, 08", "start_pos": 86, "end_pos": 103, "type": "DATASET", "confidence": 0.8299224674701691}, {"text": "WMT06", "start_pos": 105, "end_pos": 110, "type": "DATASET", "confidence": 0.5942644476890564}]}, {"text": "Our models achieve or surpass state-of-the-art results on all test sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our experiments is to test both the accuracy and robustness of the proposed new models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.999256432056427}]}, {"text": "We then show that modeling word swapping and rich linguistics features further improve our results.", "labels": [], "entities": [{"text": "word swapping", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.7585287392139435}]}, {"text": "To better situate our work among past research and to draw meaningful comparison, we use exactly the same standard evaluation data sets and metrics as, which is currently the stateof-the-art result for regression-based MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 219, "end_pos": 232, "type": "TASK", "confidence": 0.9326658546924591}]}, {"text": "We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and) (v0.7), and TER) as our baselines.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9607569575309753}, {"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.995672881603241}, {"text": "NIST", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.8320697546005249}, {"text": "METEOR", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9877309203147888}, {"text": "TER", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9981657862663269}]}, {"text": "Since our models are trained to regress human evaluation scores, to make a direct comparison in the same regression setting, we also train a small linear regression model for each baseline metric in the same way as descried in.", "labels": [], "entities": []}, {"text": "These regression models are strictly more powerful than the baseline metrics and show higher robustness and better correlation with human increased by 0.5 * max(|s|, |r|) in the worst case, and by and large swapping is rare in comparison to basic edits.: Overall results on OpenMT08 and OpenMT06 evaluation data sets.", "labels": [], "entities": [{"text": "OpenMT08", "start_pos": 274, "end_pos": 282, "type": "DATASET", "confidence": 0.964139461517334}, {"text": "OpenMT06 evaluation data sets", "start_pos": 287, "end_pos": 316, "type": "DATASET", "confidence": 0.9481290876865387}]}, {"text": "The R (as in BLEUR) refers to the regression model trained for each baseline metric, same as.", "labels": [], "entities": [{"text": "R", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9912917017936707}, {"text": "BLEUR", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9967516660690308}]}, {"text": "The first three rows are round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu).", "labels": [], "entities": [{"text": "OpenMT08", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.9674561619758606}]}, {"text": "The last row are results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06.", "labels": [], "entities": [{"text": "OpenMT08 (A+C+U)", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.8784018009901047}, {"text": "OpenMT06", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.9831569790840149}]}, {"text": "Numbers in this table are Spearman's rank correlation \u03c1 between human assessment scores and model predictions.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 26, "end_pos": 53, "type": "METRIC", "confidence": 0.5784657299518585}]}, {"text": "The pPDA column describes our pPDA model with jump distance limit 5.", "labels": [], "entities": []}, {"text": "METR is shorthand for METEORR.", "labels": [], "entities": [{"text": "METR", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.4546721875667572}, {"text": "METEORR", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.6782011389732361}]}, {"text": "+f means the model includes synonyms and paraphrase features (cf. Section 4).", "labels": [], "entities": []}, {"text": "Best results and scores that are not statistically significantly worse are highlighted in bold in each row. judgments.", "labels": [], "entities": []}, {"text": "We also compare our models with the state-of-the-art linear regression models reported in that combine features from multiple MT evaluation metrics (MT), as well as rich linguistic features from a textual entailment system (RTE).", "labels": [], "entities": [{"text": "MT evaluation metrics (MT)", "start_pos": 126, "end_pos": 152, "type": "TASK", "confidence": 0.7008655617634455}]}, {"text": "In all of our experiments, each reference and system translation sentence pair is tokenized using the PTB () tokenization script, and lemmatized by the Porter Stemmer.", "labels": [], "entities": []}, {"text": "Statistical significance tests are performed using the paired bootstrap resampling method.", "labels": [], "entities": []}, {"text": "We divide our experiments into two sections, based on two different prediction tasks -predicting absolute scores and predicting pairwise preference.", "labels": [], "entities": [{"text": "predicting pairwise preference", "start_pos": 117, "end_pos": 147, "type": "TASK", "confidence": 0.8186757564544678}]}], "tableCaptions": [{"text": " Table 1: Overall results on OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR) refers to the  regression model trained for each baseline metric, same as", "labels": [], "entities": [{"text": "OpenMT08", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9427915215492249}, {"text": "OpenMT06 evaluation data sets", "start_pos": 42, "end_pos": 71, "type": "DATASET", "confidence": 0.9303094148635864}, {"text": "R", "start_pos": 77, "end_pos": 78, "type": "METRIC", "confidence": 0.967482328414917}, {"text": "BLEUR", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.9954236149787903}]}, {"text": " Table 2: Pairwise preference prediction results on WMT08  test set. Each column shows a different training data set.  Numbers in this table are model's consistency with human  pairwise preference judgments. Best result on each test  set is highlighted in bold.", "labels": [], "entities": [{"text": "Pairwise preference prediction", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7427681883176168}, {"text": "WMT08  test set", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9838825265566508}]}]}