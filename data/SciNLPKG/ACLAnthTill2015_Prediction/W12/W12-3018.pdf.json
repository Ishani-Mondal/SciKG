{"title": [], "abstractContent": [{"text": "We have created layers of annotation on the English Gigaword v.5 corpus to render it useful as a standardized corpus for knowledge extraction and distributional semantics.", "labels": [], "entities": [{"text": "English Gigaword v.5 corpus", "start_pos": 44, "end_pos": 71, "type": "DATASET", "confidence": 0.8834827840328217}, {"text": "knowledge extraction", "start_pos": 121, "end_pos": 141, "type": "TASK", "confidence": 0.7423847168684006}]}, {"text": "Most existing large-scale work is based on inconsistent corpora which often have needed to be re-annotated by research teams independently, each time introducing biases that manifest as results that are only comparable at a high level.", "labels": [], "entities": []}, {"text": "We provide to the community a public reference set based on current state-of-the-art syntactic analysis and coreference resolution, along with an interface for programmatic access.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.9314789175987244}]}, {"text": "Our goal is to enable broader involvement in large-scale knowledge-acquisition efforts by researchers that otherwise may not have had the ability to produce such a resource on their own.", "labels": [], "entities": []}], "introductionContent": [{"text": "Gigaword is currently the largest static corpus of English news documents available.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9332550764083862}]}, {"text": "The most recent addition, Gigaword v.5 (Parker et al., 2011), contains nearly 10-million documents from seven news outlets, with a total of more than 4-billion words.", "labels": [], "entities": [{"text": "Gigaword v.5 (Parker et al., 2011)", "start_pos": 26, "end_pos": 60, "type": "DATASET", "confidence": 0.8127786252233717}]}, {"text": "We have annotated this collection with syntactic and discourse structure, for release to the community through the Linguistic Data Consortium (LDC) as a static, large-scale resource for knowledge acquisition and computational semantics.", "labels": [], "entities": [{"text": "Linguistic Data Consortium (LDC)", "start_pos": 115, "end_pos": 147, "type": "DATASET", "confidence": 0.765901098648707}, {"text": "knowledge acquisition", "start_pos": 186, "end_pos": 207, "type": "TASK", "confidence": 0.7191895544528961}]}, {"text": "This resource will (1) provide a consistent dataset of state-of-theart annotations, over which researchers can compare results, (2) prevent the reduplication of annotation efforts by different research groups, and (3) \"even the playing field\" by better enabling those lacking the computational capacity to generate such annotations at this scale.", "labels": [], "entities": []}, {"text": "The Brown Laboratory for Linguistic Information Processing (BLLIP) corpus () contains approximately 30-million words of Wall Street Journal text, annotated with automatically derived Treebank-style parses and part-of-speech tags.", "labels": [], "entities": [{"text": "Brown Laboratory for Linguistic Information Processing (BLLIP) corpus", "start_pos": 4, "end_pos": 73, "type": "DATASET", "confidence": 0.6043221533298493}, {"text": "Wall Street Journal text", "start_pos": 120, "end_pos": 144, "type": "DATASET", "confidence": 0.9387692958116531}]}, {"text": "This was followed by the BLLIP North American News Text corpus (, containing approximately 350-million words of syntactically parsed newswire.", "labels": [], "entities": [{"text": "BLLIP North American News Text corpus", "start_pos": 25, "end_pos": 62, "type": "DATASET", "confidence": 0.9155414501825968}]}, {"text": "Through the Web-as-Corpus kool ynitiative (WaCky) project, two large-scale English corpora have been created.", "labels": [], "entities": []}, {"text": "The ukWaC corpus was developed by crawling the .uk domain, resulting in nearly 2-billion words then annotated with part-ofspeech tags and lemmas.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.984496682882309}]}, {"text": "ukWaC was later extended to include dependency parses extracted using the MaltParser () (PukWaC).", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9445513486862183}]}, {"text": "PukWaC thus represents a large amount of British English text, less formally edited than newswire.", "labels": [], "entities": [{"text": "PukWaC", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9471604228019714}]}, {"text": "The WaCkypedia EN corpus contains roughly 800-million tokens from a 2009 capture of English Wikipedia, with the same annotations as PukWaC.", "labels": [], "entities": [{"text": "WaCkypedia EN corpus", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8685802419980367}]}, {"text": "Here we relied on the Stanford typed dependencies, rather than the Malt parser, owing to their relative dominance in recent work in distributional semantics and information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 161, "end_pos": 183, "type": "TASK", "confidence": 0.8416938483715057}]}, {"text": "In comparison to previous annotated corpora, Annotated Gigaword is a larger resource, based on formally edited ma-terial, that has additional levels of annotation, and reflects the current state of the art in text processing.", "labels": [], "entities": []}, {"text": "In particular, our collection provides the following for English Gigaword v.5 (referred to as Gigaword below): 1.", "labels": [], "entities": [{"text": "English Gigaword v.5", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.8523324728012085}]}, {"text": "tokenized and segmented sentences, 2.", "labels": [], "entities": []}, {"text": "Treebank-style constituent parse trees, 3.", "labels": [], "entities": [{"text": "Treebank-style constituent parse trees", "start_pos": 0, "end_pos": 38, "type": "DATASET", "confidence": 0.8135220557451248}]}, {"text": "syntactic dependency trees, 4.", "labels": [], "entities": []}, {"text": "named entities, and 5.", "labels": [], "entities": []}, {"text": "The following provides motivation for such a resource, the tools employed, a description of the programmatic interface provided alongside the data, and examples of ongoing work already enabled by this resource.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}