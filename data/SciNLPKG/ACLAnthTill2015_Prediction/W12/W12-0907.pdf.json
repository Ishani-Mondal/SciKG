{"title": [], "abstractContent": [{"text": "In this demonstration we present our web services to perform Bayesian learning for classification tasks.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.9040985405445099}]}], "introductionContent": [{"text": "The Bayesian framework for probabilistic inference has been proposed (for instance, for language related topics) as a general approach to understanding how problems of induction can be solved given only the sparse and noisy data that humans observe.", "labels": [], "entities": []}, {"text": "In particular, how human acquire words if the available data severely limit the possibility of making inferences.", "labels": [], "entities": []}, {"text": "Bayesian framework has been proposed as way to introduce a priori knowledge to guide the inference process.", "labels": [], "entities": []}, {"text": "In particular for Lexical Acquisition, proposed that given a hypothesis space (all what a word can be, according to a set of existing classes) and one or more examples of anew word, the learner evaluates all hypotheses for candidate word classes by computing their posterior probabilities, proportional to the product of prior probabilities and likelihood.", "labels": [], "entities": [{"text": "Lexical Acquisition", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.851451963186264}]}, {"text": "The prior probabilities are the learner's beliefs about which hypotheses are more or less plausible.", "labels": [], "entities": []}, {"text": "The likelihood reflects the learner's expectations about which examples are likely to be observed given a particular hypothesis about a word class.", "labels": [], "entities": []}, {"text": "And the decision on new words is determined by averaging the predictions of all hypothesis weighted by their posterior probabilities.", "labels": [], "entities": []}, {"text": "The hypothesis behind is that natural language characteristics, such as the Zipfian distribution of words and considerations as the classic argument on sparse data, make it necessary to postulate that the learning of words must be guided by the knowledge of the lexical system itself, information about abstracted, not directly observable categories.", "labels": [], "entities": []}, {"text": "In order to test this hypothesis we developed a series of tools for the task of noun classification into lexical semantic classes (such as EVENT, HUMAN, LOCATION, etc.).", "labels": [], "entities": [{"text": "noun classification", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8486589193344116}, {"text": "EVENT", "start_pos": 139, "end_pos": 144, "type": "METRIC", "confidence": 0.987454891204834}, {"text": "HUMAN", "start_pos": 146, "end_pos": 151, "type": "METRIC", "confidence": 0.8898189663887024}, {"text": "LOCATION", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.8968629240989685}]}, {"text": "The tools perform Bayesian parameter estimation where prior knowledge is included into the parameters as virtual evidence (following) and a Naive Bayes based classification.", "labels": [], "entities": [{"text": "Bayesian parameter estimation", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.7114435036977133}]}, {"text": "Our assumption is that, if introducing prior knowledge improves the classification results, it may give some insights about the way humans learn lexical classes.", "labels": [], "entities": []}, {"text": "The developed tools have been deployed as web services (following web-based architecture of the PANACEA project 1 ) in order to make them easily available to the community.", "labels": [], "entities": []}, {"text": "They can be used in the task just mentioned but also in other tasks that may profit from a Bayesian approach.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}