{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 307-315, Crowdsourced Comprehension: Predicting Prerequisite Structure in Wikipedia", "labels": [], "entities": []}], "abstractContent": [{"text": "The growth of open-access technical publications and other open-domain textual information sources means that there is an increasing amount of online technical material that is in principle available to all, but in practice , incomprehensible to most.", "labels": [], "entities": []}, {"text": "We propose to address the task of helping readers comprehend complex technical material, by using statistical methods to model the \"prereq-uisite structure\" of a corpus-i.e., the semantic impact of documents on an individual reader's state of knowledge.", "labels": [], "entities": []}, {"text": "Experimental results using Wikipedia as the corpus suggest that this task can be approached by crowd-sourcing the production of ground-truth labels regarding prerequisite structure, and then generalizing these labels using a learned classifier which combines signals of various sorts.", "labels": [], "entities": []}, {"text": "The features that we consider relate pairs of pages by analyzing not only textual features of the pages, but also how the containing corpora is connected and created.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "As discussed above, we focus in this paper on predicting prerequisite structure in Wikipedia.", "labels": [], "entities": []}, {"text": "While most Wikipedia pages are accessible to a general reader, there are many pages that describe technical concepts, such as \"conditional random fields\", \"cloud radiative forcing\", and \"Corticotropin-releasing factor\".", "labels": [], "entities": []}, {"text": "Most of these technical pages are not self-contained: for instance, to read and comprehend the page on \"conditional random fields\", one will have to first understand \"graphical model\", and soon, as suggested by Figure 1.", "labels": [], "entities": []}, {"text": "In this section, we evaluate the following questions: \u2022 Can we train a statistical classifier for prerequisite classification in a target domain, where the classifier is trained on out of domain (i.e., non-target domain) data annotated using Amazon Mechanical Turk service?", "labels": [], "entities": [{"text": "Amazon Mechanical Turk service", "start_pos": 242, "end_pos": 272, "type": "DATASET", "confidence": 0.8817340433597565}]}, {"text": "\u2022 What are the effects of different types of signals on the performance of such a classifier?", "labels": [], "entities": []}, {"text": "\u2022 How does out of domain training compare to in domain training?", "labels": [], "entities": []}, {"text": "For our experiments, we choose five targets from differing areas for experimentation, listed in.", "labels": [], "entities": []}, {"text": "Several of the techniques we used are based on graph analysis.", "labels": [], "entities": []}, {"text": "The full graphs associated with Wikipedia are unwieldy to use for experimentation because of their size: therefore, for each target concept, we extracted a moderate-sized low-conductance subgraph of Wikipedia's link graph containing the target, using a variant of the PageRank-Nibble algorithm).", "labels": [], "entities": []}, {"text": "As parameters we used \u03b1 = 0.15 and = 10 \u22127 , yielding graphs with approximately 15-20,000 nodes and 350-500,000 edges each.", "labels": [], "entities": []}, {"text": "We also collected the edit history for each page in every subgraph forming a second graph for each sub-domain 2 . On average, each page from these subgraphs had been edited about 20 times, by about 8 unique editors.", "labels": [], "entities": []}, {"text": "For classification, we used a Maximum Entropy (MaxEnt) classifier.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9690958261489868}]}, {"text": "Given a pair of Wikipedia pages ) connected by a directed edge (hyperlink) from d to d , the classifier will predict with probability p(+1|x) whether the main concept in page dis a prerequisite for the main concept in page d.", "labels": [], "entities": []}, {"text": "The classifier has the form where \u03c6(x, y) is a feature function which represents the pair of pages x = (d, d ) in a high dimensional  2.1.1 Gold-standard Annotation from Mechanical Turk 4 In order to evaluate different prerequisite classification systems and also to train the MaxEnt classifier, we collected gold prerequisite decisions using Amazon Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 343, "end_pos": 371, "type": "DATASET", "confidence": 0.9462936619917551}]}, {"text": "Since preparing annotated gold data for entire graphs in would be prohibitively expensive, we used the following strategy to sample a smaller subgraph from the larger domain-specific subgraph, which in turn will be used for training and evaluation purposes.", "labels": [], "entities": []}, {"text": "Preliminary investigation suggested that most of the pages in the prerequisite structure rooted at a target concept dare connected to d via many short hyperlink paths.", "labels": [], "entities": []}, {"text": "Hence, for each target domain, we first selected the top 20 nodes with highest RWR scores, relative to the target concept, in the subgraph for that target concept (as listed in.)", "labels": [], "entities": [{"text": "RWR scores", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.8987182974815369}]}, {"text": "We then sampled a total of 400 edges from these selected nodes, with outgoing edges from anode sampled with a frequency proportional to its RWR score.", "labels": [], "entities": [{"text": "RWR score", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9636439681053162}]}, {"text": "Thus, using this strategy, we selected up to 400 pairs of pages ), where each pair has a hyperlink from d to d . Classification of a pair of hyperlinked Wikipedia pages ) into one of the four following classes constituted a Human Intelligence Task (HIT): the two pages are unrelated; (4) Don't know.", "labels": [], "entities": []}, {"text": "Subsequently, based on the feedback from the workers, a fifth option was also added: the two concepts are related, but they don't have any prerequisite relationship between them.", "labels": [], "entities": []}, {"text": "Based on the available workers and turnaround time, the number of assignments per HIT (i.e., number of unique workers assigned to a particular HIT) was either 3 or 5; and the number of HITs used was either 200 or 400.", "labels": [], "entities": []}, {"text": "Depending on the hardness of domain and availability of workers opting to work on a domain, reward per HIT assignment was varied from $0.02 (for Global Warming and Newton's Laws) to $0.08 (for Public-key Cryptography, Meiosis and Parallel Postulate).", "labels": [], "entities": []}, {"text": "This data collection stage spanning all five domains was completed in about a week at a total cost of $278.", "labels": [], "entities": []}, {"text": "Statistics about the data are presented in . Starting with the AMT data collected as above, we next created a binary-labeled training dataset, where each instance corresponds to a pair of pages.", "labels": [], "entities": [{"text": "AMT data collected", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.8632134000460306}]}, {"text": "We ignored all \"Don't Know\" labels, treated option (1) above as vote for the corresponding prerequisite edge, and treated all other options as votes against.", "labels": [], "entities": []}, {"text": "We then assigned the final label fora node pair using majority vote (breaking ties arbitrarily).", "labels": [], "entities": []}, {"text": "The MaxEnt classifier evaluated in the previous section had access to all three types of features: WikiEdits, WikiHyperLinks, and WikiPageContent, as described in the beginning of this section.", "labels": [], "entities": []}, {"text": "In order to evaluate the contribution of each such signal, we created ablated versions of the full MaxEnt classifier which uses only one of these three subsets.", "labels": [], "entities": []}, {"text": "We call these thee variants: MaxEntWikiEdits, MaxEnt-WikiHyperLinks, and MaxEntWikiPageContent, respectively.", "labels": [], "entities": []}, {"text": "Average accuracies across all five domains comparing these three variants, in comparison to the Random baseline and the full classifier (MaxEnt-Full, as in previous section) are presented in.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 8, "end_pos": 18, "type": "METRIC", "confidence": 0.9654591679573059}, {"text": "Random", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9029374122619629}]}, {"text": "From this, we observe that all three variants perform better than the random baseline, with maximum gains achieved by the MaxEnt-WikiPageContent classifier, which uses page content-based features exclusively.", "labels": [], "entities": []}, {"text": "We    also note that the full classifier MaxEnt-Full, is able to effectively combine three types of signals improving performance even further.", "labels": [], "entities": []}, {"text": "In, we present a per-domain breakdown of the gains achieved by these four classifiers over the random baseline.", "labels": [], "entities": []}, {"text": "From this, we observe that the MaxEntWikiEdits classifier outperforms the random baseline only in 2 out of 5 domains.", "labels": [], "entities": []}, {"text": "This might be due to the fact that the MaxEnt-WikiEdits uses uses only one feature-the RWR score of the target page relative to the source page on the Wikipedia edits graph.", "labels": [], "entities": [{"text": "RWR score", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9793358445167542}]}, {"text": "We hope that use of more discriminating features should further help this classifier.", "labels": [], "entities": []}, {"text": "From, we also observe that MaxEnt-WikiHyperLinks is able to outperform the random baseline in 4 out of 5 cases, and the MaxEnt-WikiPageContent (as well as the full classifier) outperforms the random baseline in all 5 domains, sometimes with large gains (as in the case of Public-key Cryptography domain).", "labels": [], "entities": []}, {"text": "From this we observe that good generalization performance is possible even when there is no in domain training data available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Target concepts used in the experiments.", "labels": [], "entities": []}, {"text": " Table 2: Statistics about the Gold-standard data prepared  using Amazon Mechanical Turk. Also shown are the  averaged \u03ba statistics-based inter-annotator agreement in  each domain. The last row corresponds to the \u03ba value  averaged across all five domains.", "labels": [], "entities": [{"text": "Gold-standard data", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.9206084311008453}, {"text": "Amazon Mechanical Turk", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.9399193723996481}]}, {"text": " Table 3: Comparison of accuracies (averaged across all  five domains) of the full MaxEnt classifier with its ablated  versions which use a subset of the features, and also the  random baseline. The full classifier, which exploits all  three types of signals (viz., WikiEdits, WikiHyperlinks,  and WikiPageContent) achieves the highest performance.", "labels": [], "entities": []}, {"text": " Table 4: Accuracy gains (absolute) relative to the Ran- dom baseline achieved by the full MaxEnt classifier as  well as its ablated versions trained with three different  subsets of the full classifier. Positive gains are marked in  bold.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992907047271729}, {"text": "Ran- dom baseline", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.7241635322570801}]}]}