{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 136-146, Generating Diagnostic Multiple Choice Comprehension Cloze Questions", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes and evaluates DQGen, which automatically generates multiple choice cloze questions to test a child's comprehension while reading a given text.", "labels": [], "entities": []}, {"text": "Unlike previous methods, it generates different types of distracters designed to diagnose different types of comprehension failure, and tests comprehension not only of an individual sentence but of the context that precedes it.", "labels": [], "entities": []}, {"text": "We evaluate the quality of the overall questions and the individual distracters, according to 8 human judges blind to the correct answers and intended distracter types.", "labels": [], "entities": []}, {"text": "The results, errors, and judges' comments reveal limitations and suggest how to address some of them.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents an automated method to check a reader's comprehension of a given text while reading it, and to diagnose comprehension failures.", "labels": [], "entities": []}, {"text": "In contrast to testing reading comprehension skill, for which there are published tests with wellestablished psychometric properties (e.g.,, testing comprehension during reading of a given text requires generating a test for that specific text.", "labels": [], "entities": []}, {"text": "A widely used solution is to replace some of the words with blanks for the student to fill, typically by selecting from multiple candidates.", "labels": [], "entities": []}, {"text": "Such multiple choice fill-in-the-blank questions are called cloze questions.", "labels": [], "entities": []}, {"text": "They are trivial to score because the correct answer is simply the original text word.", "labels": [], "entities": []}, {"text": "Cloze questions test the ability to decide which word is consistent with the surrounding context.", "labels": [], "entities": []}, {"text": "Thus it taps the comprehension processes that judge various types of consistency, such as syntactic, semantic, and inter-sentential.", "labels": [], "entities": []}, {"text": "Ina nutshell, these processes successively encode sentences, integrate them into an overall representation of meaning, notice gaps and inconsistencies, and repair them (see, e.g.,).", "labels": [], "entities": []}, {"text": "The reader's resulting situation model represents \"the content or microworld that the text is about\".", "labels": [], "entities": []}, {"text": "In this paper, we introduce DQGen (Diagnostic Question Generator), a system that uses natural language processing to generate diagnostic cloze questions that check the comprehension of someone reading a given text.", "labels": [], "entities": []}, {"text": "DQGen differs from previous methods for generating cloze questions in that it is designed to minimize disruption to the reading process, and to diagnose different types of comprehension failure.", "labels": [], "entities": [{"text": "DQGen", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.85268634557724}]}, {"text": "The intended application context that motivated the development of DQGen is an automated reading tutor that listens to children read aloud and helps them build their oral reading fluency.", "labels": [], "entities": []}, {"text": "Periodic comprehension checks should deter children from reading as fast as they can and ignoring what the text means.", "labels": [], "entities": []}, {"text": "When the child answers incorrectly, the wrong answers should provide clues to why they are wrong.", "labels": [], "entities": []}, {"text": "The rest of this article is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the generated questions.", "labels": [], "entities": []}, {"text": "Section 3 describes how DQGen generates distracters.", "labels": [], "entities": []}, {"text": "Section 4 reports a pilot evaluation of it.", "labels": [], "entities": []}, {"text": "Section 6 relates DQGen to prior work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}