{"title": [{"text": "Latent Structure Perceptron with Feature Induction for Unrestricted Coreference Resolution", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a machine learning system based on large margin structure perceptron for unrestricted coreference resolution that introduces two key modeling techniques: latent corefer-ence trees and entropy guided feature induction.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.8308474719524384}, {"text": "entropy guided feature induction", "start_pos": 196, "end_pos": 228, "type": "TASK", "confidence": 0.6164250746369362}]}, {"text": "The proposed latent tree modeling turns the learning problem computationally feasible.", "labels": [], "entities": []}, {"text": "Additionally, using an automatic feature induction method, we are able to efficiently build nonlinear models and, hence, achieve high performances with a linear learning algorithm.", "labels": [], "entities": []}, {"text": "Our system is evaluated on the CoNLL-2012 Shared Task closed track, which comprises three languages: Arabic, Chinese and English.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task closed track", "start_pos": 31, "end_pos": 66, "type": "DATASET", "confidence": 0.8652933239936829}]}, {"text": "We apply the same system to all languages , except for minor adaptations on some language dependent features, like static lists of pronouns.", "labels": [], "entities": []}, {"text": "Our system achieves an official score of 58.69, the best one among all the competitors.", "labels": [], "entities": []}], "introductionContent": [{"text": "The CoNLL-2012 Shared Task () is dedicated to the modeling of coreference resolution for multiple languages.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.9453965723514557}]}, {"text": "The participants are provided with corpora for three languages: Arabic, Chinese and English.", "labels": [], "entities": []}, {"text": "These corpora are provided by the OntoNotes project and, besides accurate anaphoric coreference information, contain various annotation layers such as part-of-speech (POS) tagging, syntax parsing, named entities (NE) and semantic role labeling (SRL).", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 151, "end_pos": 179, "type": "TASK", "confidence": 0.6913277626037597}, {"text": "syntax parsing", "start_pos": 181, "end_pos": 195, "type": "TASK", "confidence": 0.71014603972435}, {"text": "semantic role labeling (SRL)", "start_pos": 221, "end_pos": 249, "type": "TASK", "confidence": 0.75436170399189}]}, {"text": "The shared task consists in the automatic identification of coreferring mentions of entities and events, given predicted information on other OntoNotes layers.", "labels": [], "entities": []}, {"text": "We propose a machine learning system for coreference resolution that is based on the large margin structure perceptron algorithm.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.9744511544704437}]}, {"text": "Our system learns a predictor that takes as input a set of candidate mentions in a document and directly outputs the clusters of coreferring mentions.", "labels": [], "entities": []}, {"text": "This predictor comprises an optimization problem whose objective is a function of the clustering features.", "labels": [], "entities": []}, {"text": "To embed classic cluster metrics in this objective function is practically infeasible since most of such metrics lead to NP-hard optimization problems.", "labels": [], "entities": []}, {"text": "Thus, we introduce coreference trees in order to represent a cluster by a directed tree over its mentions.", "labels": [], "entities": []}, {"text": "In that way, the prediction problem optimizes over trees instead of clusters, which makes our approach computationally feasible.", "labels": [], "entities": []}, {"text": "Since coreference trees are not given in the training data, we assume that these structures are latent and use the latent structure perceptron as the learning algorithm.", "labels": [], "entities": []}, {"text": "To provide high predicting power features to our model, we use entropy guided feature induction.", "labels": [], "entities": []}, {"text": "By using this technique, we automatically generate several feature templates that capture coreference specific local context knowledge.", "labels": [], "entities": [{"text": "coreference specific local context knowledge", "start_pos": 90, "end_pos": 134, "type": "TASK", "confidence": 0.8511559128761291}]}, {"text": "Furthermore, this feature induction technique extends the structure perceptron framework by providing an efficient general method to build strong nonlinear classifiers.", "labels": [], "entities": []}, {"text": "Our system is evaluated on the CoNLL-2012 Shared Task closed track and achieves the scores 54.22, 58.49 and 63.37 on Arabic, Chinese and English test sets, respectively.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task closed track", "start_pos": 31, "end_pos": 66, "type": "DATASET", "confidence": 0.8621105313301086}]}, {"text": "The official score -the mean over the three languages -is 58.69, which is the best score achieved in the shared task.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present our machine learning modeling for the unrestricted coreference resolution task.", "labels": [], "entities": [{"text": "coreference resolution task", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.8945090572039286}]}, {"text": "In Section 3, we present the corpus preprocessing steps.", "labels": [], "entities": [{"text": "corpus preprocessing", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.6551636010408401}]}, {"text": "The experimental findings are depicted in Section 4 and, in Section 5, we present our final remarks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on the development sets.", "labels": [], "entities": []}, {"text": " Table 2: Results on the development sets without root loss value.", "labels": [], "entities": []}, {"text": " Table 3: Official results on the test sets.", "labels": [], "entities": []}]}