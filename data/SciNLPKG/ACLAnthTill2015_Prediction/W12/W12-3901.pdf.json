{"title": [], "abstractContent": [{"text": "The current paper presents a language-independent methodology, which facilitates the creation of machine translation (MT) systems for various language pairs.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.8505906224250793}]}, {"text": "This methodology is implemented in the PRESEMT hybrid MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.5360941290855408}]}, {"text": "PRESEMT has the lowest possible requirements on specialised resources and tools, given that for many languages (especially less widely used ones) only limited linguistic resources are available.", "labels": [], "entities": [{"text": "PRESEMT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8512206673622131}]}, {"text": "In PRESEMT, the main translation process comprises two phases.", "labels": [], "entities": []}, {"text": "The first one, Structure selection, determines the overall structure of a target language (TL) sentence, drawing on syntactic information from a small bilingual corpus.", "labels": [], "entities": [{"text": "Structure selection", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7879369854927063}]}, {"text": "The second phase, Translation equivalent selection, relies on models extracted solely from monolingual corpora to implement translation disambiguation, determine intra-phrase word order and handle functional words.", "labels": [], "entities": [{"text": "Translation equivalent selection", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.9184075792630514}, {"text": "translation disambiguation", "start_pos": 124, "end_pos": 150, "type": "TASK", "confidence": 0.8991864323616028}]}, {"text": "This paper proposes extracting information for disambiguation from the monolingual corpus.", "labels": [], "entities": []}, {"text": "Experimental results indicate that such information substantially contributes in improving translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.9696029424667358}]}], "introductionContent": [{"text": "Currently most language-independent MT approaches are based on the statistical machine translation (SMT) paradigm.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.8840925693511963}, {"text": "statistical machine translation (SMT)", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.7694441080093384}]}, {"text": "SMT has proved to be particularly amenable to new language pairs, provided the necessary training data are available.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9656243920326233}]}, {"text": "The main SMT constraint is the need for SL-TL bilingual corpora of a sufficient size (at least several hundreds of thousands of sentences) to allow the building of accurate translation models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9934595823287964}]}, {"text": "Such corpora are hard to find, particularly for less widely used languages.", "labels": [], "entities": []}, {"text": "Furthermore, SMT translation accuracy largely depends on the quality of the bilingual corpora as well as their relevance to the domain of text to be translated.", "labels": [], "entities": [{"text": "SMT translation", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.9699947535991669}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.8170861601829529}]}, {"text": "For instance, parliament proceedings (among the most widely available corpora) may not suffice to train MT systems aimed towards technical manuals or news articles.", "labels": [], "entities": [{"text": "MT", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.9927700161933899}]}, {"text": "Example-Based Machine Translation (EBMT) is another MT paradigm, where a set of SL sentences are provided together with their TL reference translations.", "labels": [], "entities": [{"text": "Example-Based Machine Translation (EBMT)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7896357327699661}, {"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9858595728874207}]}, {"text": "Translations are generated by analogy, where for an input sentence the most similar SL side from the sentence set is determined and the corresponding TL side sentence is used to generate the translation.", "labels": [], "entities": []}, {"text": "Hybrid MT systems combining EBMT and SMT techniques have been proposed (cf..", "labels": [], "entities": [{"text": "MT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9870979189872742}, {"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9777323007583618}]}, {"text": "As an alternative to SMT, techniques for creating MT systems using more limited but easily obtainable resources have been proposed.", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9913903474807739}, {"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9868867993354797}]}, {"text": "Even if these methods do not achieve an accuracy as high as that of SMT, their ability to develop MT systems with very limited resources confers to them an important advantage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9989771842956543}, {"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.98563551902771}, {"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.983007550239563}]}, {"text": "The present article focusses on the development of such a methodology.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation results reported here concern the Greek -English language 7 pair and were based on the development datasets used in PRESEMT for studying the system performance.", "labels": [], "entities": [{"text": "PRESEMT", "start_pos": 131, "end_pos": 138, "type": "DATASET", "confidence": 0.8152547478675842}]}, {"text": "For each SL, these datasets contain 1,000 sentences, collected via web-crawling.", "labels": [], "entities": []}, {"text": "Sentence length ranges from 7 to 40 words.", "labels": [], "entities": []}, {"text": "From these datasets, 200 sentences were randomly chosen, and manually translated into each of the target languages.", "labels": [], "entities": []}, {"text": "The correctness of these reference translations was checked independently by native speakers.", "labels": [], "entities": []}, {"text": "When using the base PRESEMT system with the phrase-frequency disambiguation component deactivated (denoted as PRESEMT 1), a BLEU score of 0.1297 and a Meteor score of 0.2669 are obtained.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9838395416736603}, {"text": "Meteor score", "start_pos": 151, "end_pos": 163, "type": "METRIC", "confidence": 0.6745435893535614}]}, {"text": "When the disambiguation component is activated (PRESEMT 2), these scores increase substantially, reaching a BLEU score of just over 0.20.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9866357743740082}]}, {"text": "The BLEU improvement over PRESEMT 1 is 0.07 points (representing a 50% improvement), while NIST is increased by 0.85 and Meteor by over 0.06.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9995193481445312}, {"text": "NIST", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.5631589293479919}, {"text": "Meteor", "start_pos": 121, "end_pos": 127, "type": "DATASET", "confidence": 0.7882635593414307}]}, {"text": "TER is reduced by 7 points, also marking an improvement.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9935571551322937}]}, {"text": "To put these scores into perspective, a comparison is made to MT systems available on the Internet, both rule-based (SYSTRAN) and SMT ones (Google Translate).", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9775132536888123}, {"text": "SMT", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.9491875171661377}]}, {"text": "In addition, the results of METIS-II are quoted, to compare PRESEMT with a system based on monolingual corpora.", "labels": [], "entities": [{"text": "METIS-II", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.4768415093421936}]}, {"text": "As can be seen, web-based MT systems produce higher scores for all metrics, with Google Translate possessing the best values.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9702157974243164}]}, {"text": "Yet these scores are, especially in the case of Systran and WordLingo, not far off the scores obtained for PRESEMT with disambiguation.", "labels": [], "entities": [{"text": "WordLingo", "start_pos": 60, "end_pos": 69, "type": "DATASET", "confidence": 0.9341875314712524}]}, {"text": "In particular NIST scores are directly comparable whilst the Meteor ones are not substantially higher.", "labels": [], "entities": [{"text": "NIST", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.8066439032554626}, {"text": "Meteor", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.9253990054130554}]}, {"text": "It can be reasonably assumed that due to the language-independent methodology without 8 translate.google.com 9 www.systranet.com 10 www.worldlingo.com direct provision of language-specific information, the scores obtained via PRESEMT will be lower.", "labels": [], "entities": []}, {"text": "Still, it is expected that refined versions of the PRESEMT algorithm will allow the achievement of higher scores that render its performance directly comparable to that of Systran and WordLingo, for the given language pair.", "labels": [], "entities": [{"text": "WordLingo", "start_pos": 184, "end_pos": 193, "type": "DATASET", "confidence": 0.9167457818984985}]}, {"text": "In comparison to METIS-II, PRESEMT offers a substantial improvement for all metrics, with for instance BLEU and NIST scores increased by over 50%.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9984697699546814}, {"text": "NIST scores", "start_pos": 112, "end_pos": 123, "type": "METRIC", "confidence": 0.6865980327129364}]}, {"text": "This illustrates the improvements conferred by the new translation methodology.", "labels": [], "entities": []}, {"text": "As noted, PRESEMT is still underdevelopment and it is anticipated that more extensive experiments involving additional language pairs will provide improvements in the translation quality.", "labels": [], "entities": [{"text": "PRESEMT", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.6424456238746643}]}], "tableCaptions": [{"text": " Table 1. Matrix defining phrase correspondence of  sentences", "labels": [], "entities": [{"text": "Matrix defining phrase correspondence of  sentences", "start_pos": 10, "end_pos": 61, "type": "TASK", "confidence": 0.6749902814626694}]}]}