{"title": [{"text": "Sentence-Level Quality Estimation for MT System Combination", "labels": [], "entities": [{"text": "MT System Combination", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.8647091388702393}]}], "abstractContent": [{"text": "This paper provides the system description of the Dublin City University system combination module for our participation in the system combination task in the Second Workshop on Applying Machine Learning Techniques to Optimize the Division of Labour in Hybrid MT (ML4HMT-12).", "labels": [], "entities": [{"text": "Dublin City University system combination module", "start_pos": 50, "end_pos": 98, "type": "DATASET", "confidence": 0.9680329660574595}, {"text": "Division of Labour in Hybrid MT (ML4HMT-12)", "start_pos": 231, "end_pos": 274, "type": "TASK", "confidence": 0.684990170929167}]}, {"text": "We incorporated a sentence-level quality score, obtained by sentence-level Quality Estimation (QE), as meta information guiding system combination.", "labels": [], "entities": [{"text": "sentence-level Quality Estimation (QE)", "start_pos": 60, "end_pos": 98, "type": "METRIC", "confidence": 0.5761034339666367}]}, {"text": "Instead of using BLEU or (minimum average) TER, we select a backbone for the confusion network using the estimated quality score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9988844990730286}, {"text": "TER", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.6754934191703796}]}, {"text": "For the Spanish-English data, our strategy improved 0.89 BLEU points absolute compared to the best single score and 0.20 BLEU points absolute compared to the standard system combination strategy.", "labels": [], "entities": [{"text": "Spanish-English data", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.7807264924049377}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9984234571456909}, {"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9986762404441833}]}], "introductionContent": [{"text": "This paper describes anew extension to our system combination module in Dublin City University.", "labels": [], "entities": [{"text": "Dublin City University", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.9862038890520731}]}, {"text": "We deployed a Quality Estimation technique () in our system combination module for the system combination task in the ML4HMT-2012 workshop.", "labels": [], "entities": [{"text": "system combination task", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.7742098768552145}, {"text": "ML4HMT-2012 workshop", "start_pos": 118, "end_pos": 138, "type": "DATASET", "confidence": 0.777771919965744}]}, {"text": "System combination is a strategy () that provides away to combine multiple translation outputs from potentially very different MT systems including Rule-based MT (RBMT) and SMT.", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.9599006175994873}, {"text": "SMT", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.9446849822998047}]}, {"text": "It is often the case that a practical system combination strategy involves a confusion network (), which is also the casein our system, in order to combine fragments from a number of systems.", "labels": [], "entities": []}, {"text": "The standard process to build such confusion networks consists of two steps: (1) a selection of a backbone (or a skeleton), and (2) monolingual word alignment () between a backbone and other hypotheses in a pairwise manner.", "labels": [], "entities": []}, {"text": "Once such a confusion network is built, we can search for the best path using a (monotonic) consensus network decoder.", "labels": [], "entities": []}, {"text": "It is noted that there are also approaches which select multiple possible hypotheses as backbones (.", "labels": [], "entities": []}, {"text": "One important factor in the overall performance of such a system combination method resides in the selection of a backbone, which is the main focus in this paper.", "labels": [], "entities": []}, {"text": "There are several reasons why a good backbone selection is very important.", "labels": [], "entities": []}, {"text": "First, in practice, it is often the case that the final translation output is identical to the backbone even if the overall combination method includes a confusion network.", "labels": [], "entities": []}, {"text": "Second, it depends on the backbone whether some segments which do not match with the backbone will be discarded.", "labels": [], "entities": []}, {"text": "In fact, important segments potentially contributing to good translation quality, may not be considered only because such fragments do not match with the backbone.", "labels": [], "entities": []}, {"text": "propose (minimum average) TER to select a backbone.", "labels": [], "entities": [{"text": "TER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9924548864364624}]}, {"text": "This alignment metric selects the hypotheses that agrees with the other hypotheses on average.", "labels": [], "entities": []}, {"text": "Another common alignment metric is BLEU (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9985542893409729}]}, {"text": "This metric selects a hypothesis that performs best.", "labels": [], "entities": []}, {"text": "This paper proposes a novel method to use (sentence-level) Quality Estimation (QE) to select a backbone.", "labels": [], "entities": [{"text": "sentence-level) Quality Estimation (QE)", "start_pos": 43, "end_pos": 82, "type": "METRIC", "confidence": 0.6040908396244049}]}, {"text": "Since QE quantifies the confidence of the MT output (, this selection would roughly inline with BLEU, which selects the best performing hypothesis as a backbone.", "labels": [], "entities": [{"text": "QE", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.8976084589958191}, {"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9339457154273987}, {"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.998884379863739}]}, {"text": "Note that one difference is that BLEU and TER are used as a loss function in MBR decoding (), while we select the best sentence in terms of (sentence-level) QE.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9991029500961304}, {"text": "TER", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9963212013244629}, {"text": "MBR decoding", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.5376656949520111}, {"text": "QE", "start_pos": 157, "end_pos": 159, "type": "METRIC", "confidence": 0.8882420659065247}]}, {"text": "Hence, in doing so, we do not minimize the worst case risk.", "labels": [], "entities": []}, {"text": "The main part of this paper provides an algorithm to use QE as the selection mechanism of a backbone of a confusion network.", "labels": [], "entities": []}, {"text": "However, such a selection, by itself, can be considered as one method of (sentence-level) system combination.", "labels": [], "entities": []}, {"text": "What is more, the two QE-based methods yield translation outputs which differ in quality.", "labels": [], "entities": []}, {"text": "Because of this, this paper presents two algorithms: (1) system combination via QE-selected backbone, and (2) QE-based sentence selection.", "labels": [], "entities": [{"text": "QE-based sentence selection", "start_pos": 110, "end_pos": 137, "type": "TASK", "confidence": 0.6853592097759247}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our algorithms.", "labels": [], "entities": []}, {"text": "In Section 3, our experimental results are presented.", "labels": [], "entities": []}, {"text": "We conclude in Section 3.2.", "labels": [], "entities": []}], "datasetContent": [{"text": "The machine learning toolkit used in our experiments is LIBSVM (, an implementation of the Support Vector Regression (SVR) method.", "labels": [], "entities": []}, {"text": "We use the Radial Basis Function (RBF) kernel as it is widely used in the QE for the MT community and it usually achieves good performance ().", "labels": [], "entities": [{"text": "Radial Basis Function (RBF)", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.7391446530818939}, {"text": "MT community", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.8879364132881165}]}, {"text": "An important aspect of SVR with RBF kernel is hyper parameter optimization.", "labels": [], "entities": [{"text": "hyper parameter optimization", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.6728007098038992}]}, {"text": "In our setup, three parameters have to be optimized: c (the penalty factor), \u03b3 (the kernel parameter) and \u03b5 (the approximated function accuracy level).", "labels": [], "entities": [{"text": "accuracy level", "start_pos": 135, "end_pos": 149, "type": "METRIC", "confidence": 0.9417774081230164}]}, {"text": "We optimize these parameters using grid-search, an iterative process computing n-fold crossvalidation on each possible triplet of parameters and selecting the best set of parameters according to a score (usually the Mean Absolute Error or the Root Mean Square Error, described in 1 and 2, where n is the number of test instances, re f and pr ed are the reference and predicted TER scores of the ith test instance respectively).", "labels": [], "entities": [{"text": "Mean Absolute Error", "start_pos": 216, "end_pos": 235, "type": "METRIC", "confidence": 0.9526905417442322}, {"text": "TER", "start_pos": 377, "end_pos": 380, "type": "METRIC", "confidence": 0.9857245087623596}]}, {"text": "This method is expensive in terms of computing time (we use 5-fold cross-validation at each iteration) and it is not feasible dodo this in an acceptable amount of time for the whole tuning set provided by the shared task (20k sentences pairs for each MT system).", "labels": [], "entities": []}, {"text": "To tackle this issue, we extract a reduced development set from the tuning set using the cosine distance to measure the proximity between the test and the tuning feature vectors.", "labels": [], "entities": []}, {"text": "For each MT system, we iterate over the corresponding test feature vectors and measure the cosine distance with all the feature vectors of the tuning set.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9776613116264343}]}, {"text": "We keep the tuning instances which are most similar to the test instances to build our reduced development set.", "labels": [], "entities": []}, {"text": "This set is used to optimize the three hyper parameters of \u03b5-SVR.", "labels": [], "entities": []}, {"text": "Finally, four regression models are built (one for each MT system) using the complete tuning set and the optimized parameters.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.861844003200531}]}, {"text": "ML4HMT-2012 provides four translation outputs s1 to s4 from APERTIUM, LUCY, PB-SMT (MOSES) and HPB-SMT (MOSES).", "labels": [], "entities": [{"text": "ML4HMT-2012", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8452924489974976}, {"text": "APERTIUM", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.91029292345047}, {"text": "LUCY", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.92668217420578}]}, {"text": "The tuning data consists of 20,000 sentence pairs while the test data consists of 3,003 sentence pairs.", "labels": [], "entities": []}, {"text": "Our experimental setting is as follows.", "labels": [], "entities": []}, {"text": "We use our system combination module () which includes a language modeling tool, a MERT process, and MBR decoding of its own.", "labels": [], "entities": [{"text": "MERT", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.799468457698822}]}, {"text": "We use the BLEU metric as loss function in MBR decoding.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.9752911627292633}]}, {"text": "We use TERP 5 as alignment metrics in monolingual word alignment.", "labels": [], "entities": [{"text": "monolingual word alignment", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6480624874432882}]}, {"text": "We evaluated our QE model on the test set by predicting TER scores at the sentence level and comparing them with the reference.", "labels": [], "entities": [{"text": "TER scores", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9656933546066284}]}, {"text": "We used two measures described by the equations 1 and 2.", "labels": [], "entities": []}, {"text": "The scores are presented in.", "labels": [], "entities": []}, {"text": "These results were quite surprising because the larger feature set (R2) did not reach the best results in terms of TER score prediction.", "labels": [], "entities": [{"text": "TER score prediction", "start_pos": 115, "end_pos": 135, "type": "METRIC", "confidence": 0.9508735537528992}]}, {"text": "Using only target LM features based on a small dataset and the MT output differences (R1) leads to MAE scores between 0.21 and 0.17.", "labels": [], "entities": [{"text": "MT output differences (R1)", "start_pos": 63, "end_pos": 89, "type": "METRIC", "confidence": 0.7175242155790329}, {"text": "MAE", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9945834279060364}]}, {"text": "For this feature set, the most accurate sentence level score prediction was obtained on the MT system s3, which corresponds to the PBSMT implementation MOSES, while the system s2, which corresponds to the RBMT system LUCY, leads to the worse score prediction.", "labels": [], "entities": [{"text": "MT system s3", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.8057371179262797}, {"text": "PBSMT implementation MOSES", "start_pos": 131, "end_pos": 157, "type": "DATASET", "confidence": 0.6142359177271525}, {"text": "RBMT system LUCY", "start_pos": 205, "end_pos": 221, "type": "DATASET", "confidence": 0.7541687885920206}]}, {"text": "In other words, it is more difficult to predict sentencelevel scores of s2 compared to s3.: Error scores of the QE model when predicting TER scores at the sentence level on the test set for the four MT systems.", "labels": [], "entities": [{"text": "Error", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.9920952916145325}, {"text": "TER", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.9478529691696167}]}, {"text": "shows the performance on the development set.", "labels": [], "entities": []}, {"text": "shows the results of Algorithm 1 and 2.", "labels": [], "entities": []}, {"text": "The first four lines show the single best performance of each translation output where s4 achieves 25.31 BLEU points which is the best among four MT systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.999329686164856}, {"text": "MT", "start_pos": 146, "end_pos": 148, "type": "TASK", "confidence": 0.9497635364532471}]}, {"text": "The standard system combination results, shown in the next line, was 26.00 BLEU points, which improved 0.69 BLEU points absolute.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9996737241744995}, {"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9989270567893982}]}, {"text": "We used two different feature set in the QE method: R1 corresponds to the small feature set, while R2 corresponds to the larger feature set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Error scores of the QE model when predicting TER scores at the sentence level on the  test set for the four MT systems.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.992954671382904}, {"text": "TER", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9549978971481323}]}, {"text": " Table 2: Table shows the performance of translation outputs s1 to s4 and results of system  combination on development set.", "labels": [], "entities": []}, {"text": " Table 3: This table includes our results by 1st algorithm and 2nd algorithm.", "labels": [], "entities": []}, {"text": " Table 4: This table shows the performance when the backbone was selected by average TER  and by one of the good backbone.", "labels": [], "entities": [{"text": "TER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9544157385826111}]}]}