{"title": [{"text": "WSD for n-best reranking and local language modeling in SMT", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6132937073707581}, {"text": "local language modeling", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.6384744246800741}, {"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9652858376502991}]}], "abstractContent": [{"text": "We integrate semantic information at two stages of the translation process of a state-of-the-art SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.9908815622329712}]}, {"text": "A Word Sense Disam-biguation (WSD) classifier produces a probability distribution over the translation candidates of source words which is exploited in two ways.", "labels": [], "entities": [{"text": "Word Sense Disam-biguation (WSD) classifier", "start_pos": 2, "end_pos": 45, "type": "TASK", "confidence": 0.7122356849057334}]}, {"text": "First, the probabilities serve to rerank a list of n-best translations produced by the system.", "labels": [], "entities": []}, {"text": "Second, the WSD predictions are used to build a supplementary language model for each sentence, aimed to favor translations that seem more adequate in this specific sen-tential context.", "labels": [], "entities": [{"text": "WSD", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.7752068638801575}]}, {"text": "Both approaches lead to significant improvements in translation performance , highlighting the usefulness of source side disambiguation for SMT.", "labels": [], "entities": [{"text": "translation", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9723268747329712}, {"text": "SMT", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.9940060377120972}]}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is the task of identifying the sense of words in texts by reference to some pre-existing sense inventory.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7788523634274801}, {"text": "identifying the sense of words in texts", "start_pos": 47, "end_pos": 86, "type": "TASK", "confidence": 0.7445134605680194}]}, {"text": "The selection of the appropriate inventory and WSD method strongly depends on the goal WSD intends to serve: recent methods are increasingly oriented towards the disambiguation needs of specific end applications, and explicitly aim at improving the overall performance of complex Natural Language Processing systems.", "labels": [], "entities": []}, {"text": "This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9668735861778259}]}, {"text": "Accordingly, pre-defined semantic inventories, that usually served to provide the lists of candidate word senses, are being replaced by senses relevant to the considered applications and directly identified from corpora by means of word sense induction methods.", "labels": [], "entities": []}, {"text": "Ina multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus, or by using more complex representations of the semantics of translations.", "labels": [], "entities": []}, {"text": "Seemingly, the optimal way to take advantage of WSD predictions remains an open issue.", "labels": [], "entities": [{"text": "WSD predictions", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.9496057629585266}]}, {"text": "In this work, we carryout a set of experiments to investigate the impact of integrating the predictions of a cross-lingual WSD classifier into an SMT system, at two different stages of the translation process.", "labels": [], "entities": [{"text": "WSD classifier", "start_pos": 123, "end_pos": 137, "type": "TASK", "confidence": 0.8167230188846588}, {"text": "SMT", "start_pos": 146, "end_pos": 149, "type": "TASK", "confidence": 0.9735515713691711}]}, {"text": "The first approach exploits the probability distribution built by the WSD classifier over the set of translations of words found in the parallel corpus, for reranking the translations in the n-best list generated by the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 220, "end_pos": 223, "type": "TASK", "confidence": 0.9854158759117126}]}, {"text": "Words in the list that match one of the proposed translations are boosted and are thus more likely to appear in the final translation.", "labels": [], "entities": []}, {"text": "Our results on the English-French IWSLT'11 task show substantial improvements in translation quality.", "labels": [], "entities": [{"text": "IWSLT'11 task", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.4427608549594879}]}, {"text": "The second approach provides a tighter integration of the WSD classifier with the rest of the system: using the WSD predictions, an additional sentence specific language model is estimated and used during decoding.", "labels": [], "entities": [{"text": "WSD classifier", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.47988298535346985}]}, {"text": "These additional local models can be used as an external knowledge source to reinforce translation hypotheses matching the prediction of the WSD system.", "labels": [], "entities": [{"text": "WSD", "start_pos": 141, "end_pos": 144, "type": "TASK", "confidence": 0.638588547706604}]}, {"text": "In the rest of the paper, we present related work on integrating semantic information into SMT (Section 2).", "labels": [], "entities": [{"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.986194372177124}]}, {"text": "The WSD classifier used in the current study is described in Section 3.", "labels": [], "entities": [{"text": "WSD classifier", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.6607715487480164}]}, {"text": "We then present the two approaches adopted for integrating the WSD output into SMT (Section 4).", "labels": [], "entities": [{"text": "WSD", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.8498727679252625}, {"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9558612704277039}]}, {"text": "Evaluation results are presented in Section 5, before concluding and discussing some avenues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In all our experiments, we considered the TEDtalk English to French data set provided by the IWSLT'11 evaluation campaign, a collection of public speeches on a variety of topics.", "labels": [], "entities": [{"text": "TEDtalk English to French data set", "start_pos": 42, "end_pos": 76, "type": "DATASET", "confidence": 0.6946094830830892}, {"text": "IWSLT'11 evaluation campaign", "start_pos": 93, "end_pos": 121, "type": "DATASET", "confidence": 0.8156324028968811}]}, {"text": "We used the Moses decoder (.", "labels": [], "entities": []}, {"text": "The TED-talk corpus is a small data set made of a monolingual corpus (111, 431 sentences) used to estimate a 4-gram language model with KNsmoothing, and a bilingual corpus (107, 268 sentences) used to extract the phrase table.", "labels": [], "entities": [{"text": "TED-talk corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7580944001674652}]}, {"text": "All data are tokenized, cleaned and converted to lowercase letters using the tools provided by the WMT organizers.", "labels": [], "entities": [{"text": "WMT organizers", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.8244932591915131}]}, {"text": "We then use a standard training pipeline to construct the translation model: the bitext is aligned using GIZA++, symmetrized using the grow-diagfinal-and heuristic; the phrase table is extracted and scored using the tools distributed with Moses.", "labels": [], "entities": []}, {"text": "Finally, systems are optimized using MERT on the 934 sentences of the dev-2010 set.", "labels": [], "entities": [{"text": "MERT", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.8302753567695618}]}, {"text": "All evaluations are performed on the 1, 664 sentences of the test-2010 set.", "labels": [], "entities": []}, {"text": "All the measures used for evaluating the impact of WSD information on translation show improvements, as discussed in the previous section.", "labels": [], "entities": [{"text": "WSD information", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.8917084634304047}, {"text": "translation", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.8041054606437683}]}, {"text": "We complement these results with another measure of translation performance, proposed by , which allows fora more fine-grained contrastive evaluation of the translations produced by different systems.", "labels": [], "entities": []}, {"text": "The method permits to compare the results produced by the systems on different word classes and to take into account the source words that were actually translated.", "labels": [], "entities": []}, {"text": "We focus this evaluation on the classes of content words (nouns, adjectives, verbs and adverbs) on which WSD had an important coverage.", "labels": [], "entities": [{"text": "WSD", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.6951090693473816}]}, {"text": "Our aim is, first, to explore how these words are handled by a WSDinformed SMT system (the system using the local language models) compared to the baseline system that does not exploit any semantic information; and, second, to investigate whether their disambiguation influences the translation of surrounding non-disambiguated words.", "labels": [], "entities": [{"text": "WSDinformed SMT", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.5409107208251953}]}, {"text": "reports the percentage of words correctly translated by the semantically-informed system within each content word class: consistent gains in translation quality are observed for all parts-ofspeech compared to the baseline, and the best results are obtained for nouns.", "labels": [], "entities": []}, {"text": "shows how the words surrounding a disambiguated word w (noun, verb, adjective or adverb) in the text are handled by the two systems.", "labels": [], "entities": []}, {"text": "More precisely, we look at the translation of words in the immediate context of w, i.e. at positions w \u22122 , w \u22121 , w +1 and w +2 . The left column reports the percentage of correct translations produced by the baseline system (without disambiguation) for words in these positions; the right column shows the positive impact that the disambiguation of a word has on the translation of its neighbors.", "labels": [], "entities": []}, {"text": "Note that this time we look at disambiguated words and their context without evaluating the correctness of the WSD predictions.", "labels": [], "entities": [{"text": "WSD", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9099323153495789}]}, {"text": "Nevertheless, even in this case, consistent gains are observed when WSD information is exploited.", "labels": [], "entities": [{"text": "WSD information", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.8610562980175018}]}, {"text": "For instance, when a noun is disambiguated, 70.46% and 76.3% of the immediately preceding (w \u22121 ) and following (w +1 ) words, respectively, are correctly translated, versus 68.69% and 75.17% of correct translations produced by the baseline system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Coverage of the WSD method", "labels": [], "entities": [{"text": "WSD", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.5947552919387817}]}, {"text": " Table 2: Evaluation results on the TED-talk task of our two methods to integrate WSD predictions.", "labels": [], "entities": [{"text": "TED-talk task", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.6868469715118408}, {"text": "WSD predictions", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.929779589176178}]}, {"text": " Table 3: Contrastive lexical evaluation: % of words correctly translated within each PoS class", "labels": [], "entities": []}, {"text": " Table 4: Impact of WSD prediction on the surrounding words", "labels": [], "entities": [{"text": "WSD prediction", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.9707019031047821}]}]}