{"title": [{"text": "A Diagnostic Evaluation Approach Targeting MT Systems for Indian Languages", "labels": [], "entities": [{"text": "Diagnostic Evaluation Approach Targeting MT", "start_pos": 2, "end_pos": 45, "type": "TASK", "confidence": 0.7005151629447937}]}], "abstractContent": [{"text": "This paper addresses diagnostic evaluation of machine translation (MT) systems for Indian languages, English to Hindi translation in particular.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.8388839602470398}, {"text": "English to Hindi translation", "start_pos": 101, "end_pos": 129, "type": "TASK", "confidence": 0.7278524190187454}]}, {"text": "Evaluation of MT output is an important but difficult task.", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9638238549232483}]}, {"text": "The difficulty arises primarily from some inherent characteristics of the language pairs, which range from simple word-level discrepancies to more difficult structural variations for Hindi from English, such as reduplication of words, free word order etc.", "labels": [], "entities": []}, {"text": "The proposed scheme is based on identification of linguistic units (often referred to as checkpoints).", "labels": [], "entities": []}, {"text": "We use the diagnostic evaluation tool DELiC4MT to analyze the contribution of various PoS classes for different categories.", "labels": [], "entities": [{"text": "DELiC4MT", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.6286755204200745}]}, {"text": "We further suggest some additional checkpoints based on named entities, ambiguous words, word order and inflections that are relevant for the evaluation of Hindi.", "labels": [], "entities": [{"text": "evaluation of Hindi", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.7755284110705057}]}, {"text": "The evaluation of these checkpoints provides a detailed analysis and helps in monitoring how an MT system handles these linguistic phenomena as well.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9817724227905273}]}, {"text": "This also provides valuable feedback to MT developers as to where the system is performing poorly and how the output can possibly be improved.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9774150252342224}]}, {"text": "The effectiveness of the approach was tested on 5 English to Hindi MT systems and it was observed that the system-level DELiC4MT scores correlate well with the scores produced by the most commonly used automatic evaluation metrics (BLEU, NIST, METEOR and TER) while providing finer-grained information.", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.8513578772544861}, {"text": "DELiC4MT", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9480038285255432}, {"text": "BLEU", "start_pos": 232, "end_pos": 236, "type": "METRIC", "confidence": 0.9907991290092468}, {"text": "NIST", "start_pos": 238, "end_pos": 242, "type": "DATASET", "confidence": 0.878080427646637}, {"text": "METEOR", "start_pos": 244, "end_pos": 250, "type": "METRIC", "confidence": 0.7698562145233154}, {"text": "TER", "start_pos": 255, "end_pos": 258, "type": "METRIC", "confidence": 0.9171343445777893}]}], "introductionContent": [{"text": "Evaluation of MT systems has received a lot of attention over the last decade or so, yet no generally ideal automatic metric could be designed so far.", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9867874383926392}]}, {"text": "The problem becomes even more pronounced when the source and target languages are distant, (e.g. they belong to different language families).", "labels": [], "entities": []}, {"text": "The MT community is very much in need of a suitable evaluation methodology for evaluating translation quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9667069315910339}, {"text": "translation", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.9695832133293152}]}, {"text": "This is particularly true with respect to Indian languages.", "labels": [], "entities": []}, {"text": "In the last 15 years or so, MT into Indian languages (especially Hindi) has gained tremendous research interest in India and elsewhere.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9952803254127502}]}, {"text": "Many English to Hindi and Indian Languages to Indian Languages MT systems have been designed, for example AnglaBharati (, Anusaaraka 2 (Chaudhury et al., 2010), Anuvadaksh 3 , Google , Sampark 5 , MaTra (), to name just a few.", "labels": [], "entities": [{"text": "Indian Languages to Indian Languages MT", "start_pos": 26, "end_pos": 65, "type": "TASK", "confidence": 0.533871074517568}, {"text": "AnglaBharati", "start_pos": 106, "end_pos": 118, "type": "DATASET", "confidence": 0.6712245345115662}, {"text": "MaTra", "start_pos": 197, "end_pos": 202, "type": "DATASET", "confidence": 0.8444566130638123}]}, {"text": "However, the issue of evaluating the output of these MT systems has remained rather unexplored.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9802917242050171}]}, {"text": "The state-of-the-art methods for automatic MT evaluation are represented by BLEU () and closely related NIST), METEOR (Banerjee and and TER ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9668022990226746}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9971734285354614}, {"text": "NIST", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.9494306445121765}, {"text": "METEOR", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9383718371391296}, {"text": "TER", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.8396208882331848}]}, {"text": "These metrics have been widely accepted as benchmarks for MT system evaluation.", "labels": [], "entities": [{"text": "MT system evaluation", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.8958743214607239}]}, {"text": "However, the research community is also aware of the deficiencies of these metrics).", "labels": [], "entities": []}, {"text": "Globally, these automatic MT evaluation metrics (BLEU, NIST, TER, METEOR, etc.) are being studied with great interest for different language pairs.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.917538195848465}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9981649518013}, {"text": "TER", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9945228099822998}, {"text": "METEOR", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9806283712387085}]}, {"text": "But their direct applicability to Hindi, or other Indian languages for that matter, needs proper investigation.", "labels": [], "entities": []}, {"text": "Indian languages are characteristically different from English and other related European languages for which these metrics are mostly used.", "labels": [], "entities": []}, {"text": "There have been some efforts in this direction for Indian languages).", "labels": [], "entities": []}, {"text": "Barring these few exceptions, the subject has not been studied deeply.", "labels": [], "entities": []}, {"text": "Most of these approaches, however, either cover human evaluation, or consider modification of existing automatic metrics (like BLEU and METEOR) to make them more suitable for Indian languages.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9970762729644775}, {"text": "METEOR", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.8991130590438843}]}, {"text": "None of these works has been targeted towards diagnostic evaluation (, which not only provides quantitative analysis, but also qualitative feedback of the machine translated text.", "labels": [], "entities": [{"text": "diagnostic evaluation", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.9541377127170563}]}, {"text": "It also provides feedback and detailed analysis of how an MT system performs for different linguistic features like verbs, nouns, compounds etc.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9899032115936279}]}, {"text": "Our final aim is to come up with an approach for diagnostic evaluation of MT that can be adapted to Indian languages.", "labels": [], "entities": [{"text": "diagnostic evaluation of MT", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.7645689994096756}]}, {"text": "In the present work the experiments have been carried outwith the DELiC4MT () toolkit as it is language independent.", "labels": [], "entities": [{"text": "DELiC4MT", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9020540118217468}]}, {"text": "The experiments have been carried out to adapt the tool for Hindi, which can be later extended to evaluation of other Indian languages as well.", "labels": [], "entities": []}, {"text": "To the best of our knowledge this is a pioneering work in the direction of diagnostic evaluation with respect to Indian languages.", "labels": [], "entities": [{"text": "diagnostic evaluation", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.9734268486499786}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Related work on diagnostic evaluation is discussed in Section 2.", "labels": [], "entities": [{"text": "diagnostic evaluation", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.9808290004730225}]}, {"text": "Section 3 gives a brief overview of the diagnostic evaluation tool, DELiC4MT, which has been used for this study.", "labels": [], "entities": [{"text": "DELiC4MT", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.8216434121131897}]}, {"text": "In Section 4, the various linguistic checkpoints considered for the study of English and Hindi have been discussed.", "labels": [], "entities": []}, {"text": "Section 5 discusses the experimental setup and compares the results obtained on the EnglishHindi test set using DELiC4MT and automatic evaluation metrics.", "labels": [], "entities": [{"text": "EnglishHindi test set", "start_pos": 84, "end_pos": 105, "type": "DATASET", "confidence": 0.9861413836479187}, {"text": "DELiC4MT", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.5633490085601807}]}, {"text": "This is followed by conclusions and avenues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The test set considered for this study consists of 1,000 sentences from the tourism domain.", "labels": [], "entities": []}, {"text": "DELiC4MT has been used for diagnostic evaluation of five English to Hindi MT systems: Google Translate (MT1), Bing Translator 12 (MT2), Free-translations 13 (MT3), MaTra2 (MT4) and Anusaaraka (MT5).", "labels": [], "entities": []}, {"text": "GIZA++ was used for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.8350543975830078}]}, {"text": "Since the test set is very small, an additional parallel corpus comprising of 25,000 sentences from the same domain was used to avoid data sparseness during word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 157, "end_pos": 171, "type": "TASK", "confidence": 0.7656359970569611}]}, {"text": "The test set was appended to the additional corpus and the word alignments were generated.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.6737725287675858}]}, {"text": "Finally the word alignments for the test set sentences were extracted.", "labels": [], "entities": []}, {"text": "Treetagger was used to PoS-tag the English dataset, while the Hindi dataset was PoS-tagged using the PoS tagger developed by IIIT, Hyderabad", "labels": [], "entities": [{"text": "English dataset", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.6435918062925339}]}], "tableCaptions": []}