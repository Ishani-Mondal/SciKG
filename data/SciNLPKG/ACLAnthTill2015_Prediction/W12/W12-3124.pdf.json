{"title": [{"text": "Review of Hypothesis Alignment Algorithms for MT System Combination via Confusion Network Decoding", "labels": [], "entities": [{"text": "Hypothesis Alignment Algorithms", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.8008688688278198}, {"text": "MT System Combination", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.8755186200141907}]}], "abstractContent": [{"text": "Confusion network decoding has proven to be one of the most successful approaches to machine translation system combination.", "labels": [], "entities": [{"text": "Confusion network decoding", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7947309414545695}, {"text": "machine translation system combination", "start_pos": 85, "end_pos": 123, "type": "TASK", "confidence": 0.8775427043437958}]}, {"text": "The hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have been proposed in the literature.", "labels": [], "entities": [{"text": "hypothesis alignment", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7379781305789948}]}, {"text": "This paper describes a systematic comparison of five well known hypothesis alignment algorithms for MT system combination via confusion network decoding.", "labels": [], "entities": [{"text": "hypothesis alignment", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.7536326348781586}, {"text": "MT system combination", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.9181654453277588}]}, {"text": "Controlled experiments using identical pre-processing, decoding, and weight tuning methods on standard system combination evaluation sets are presented.", "labels": [], "entities": []}, {"text": "Translation quality is assessed using case insensitive BLEU scores and bootstrapping is used to establish statistical significance of the score differences.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.947617769241333}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9917336702346802}]}, {"text": "All aligners yield significant BLEU score gains over the best individual system included in the combination.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9801121354103088}]}, {"text": "Incremental indirect hidden Markov model and a novel incre-mental inversion transduction grammar with flexible matching consistently yield the best translation quality, though keeping all things equal, the differences between aligners are relatively small.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current machine translation (MT) systems are based on different paradigms, such as rule-based, phrasebased, hierarchical, and syntax-based.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.8575114727020263}]}, {"text": "Due to the complexity of the problem, systems make various assumptions at different levels of processing and modeling.", "labels": [], "entities": []}, {"text": "Many of these assumptions maybe suboptimal and complementary.", "labels": [], "entities": []}, {"text": "The complementary information in the outputs from multiple MT systems maybe exploited by system combination.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9684097170829773}]}, {"text": "Availability of multiple system outputs within the DARPA GALE program as well as NIST Open MT and Workshop on Statistical Machine Translation evaluations has led to extensive research in combining the strengths of diverse MT systems, resulting in significant gains in translation quality.", "labels": [], "entities": [{"text": "DARPA GALE program", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.7457693815231323}, {"text": "NIST Open MT", "start_pos": 81, "end_pos": 93, "type": "TASK", "confidence": 0.7030966877937317}, {"text": "Statistical Machine Translation evaluations", "start_pos": 110, "end_pos": 153, "type": "TASK", "confidence": 0.7247515246272087}, {"text": "translation", "start_pos": 268, "end_pos": 279, "type": "TASK", "confidence": 0.9582321643829346}]}, {"text": "System combination methods proposed in the literature can be roughly divided into three categories: (i) hypothesis selection (, (ii) re-decoding (), and (iii) confusion network decoding.", "labels": [], "entities": [{"text": "System combination", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7115141153335571}]}, {"text": "Confusion network decoding has proven to be the most popular as it does not require deep Nbest lists and operates on the surface strings.", "labels": [], "entities": []}, {"text": "It has also been shown to be very successful in combining speech recognition outputs (Fiscus, 1997;).", "labels": [], "entities": [{"text": "speech recognition outputs", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.7421672344207764}]}, {"text": "The first application of confusion network decoding in MT system combination appeared in () where a multiple string alignment (MSA), made popular in biological sequence analysis, was applied to the MT system outputs.", "labels": [], "entities": [{"text": "MT system combination", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.9141677419344584}, {"text": "multiple string alignment (MSA)", "start_pos": 100, "end_pos": 131, "type": "METRIC", "confidence": 0.6387005696694056}, {"text": "biological sequence analysis", "start_pos": 149, "end_pos": 177, "type": "TASK", "confidence": 0.6468256314595541}, {"text": "MT system outputs", "start_pos": 198, "end_pos": 215, "type": "TASK", "confidence": 0.864669402440389}]}, {"text": "proposed an alignment based on GIZA++ Toolkit which introduced word reordering not present in MSA, and used the alignments produced by the translation edit rate (TER)) scoring.", "labels": [], "entities": [{"text": "GIZA++ Toolkit", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.8284642299016317}, {"text": "translation edit rate (TER)) scoring", "start_pos": 139, "end_pos": 175, "type": "METRIC", "confidence": 0.7317661046981812}]}, {"text": "Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) ( and inversion transduction grammars (ITG).", "labels": [], "entities": []}, {"text": "System combinations produced via confusion network decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation).", "labels": [], "entities": [{"text": "hypothesis alignment", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7584882974624634}, {"text": "Statistical Machine Translation", "start_pos": 183, "end_pos": 214, "type": "TASK", "confidence": 0.7131804426511129}]}, {"text": "However, there has not been a comparison of the most popular hypothesis alignment algorithms using the same sets of MT system outputs and otherwise identical combination pipelines.", "labels": [], "entities": [{"text": "hypothesis alignment", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.7369418144226074}, {"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9537782073020935}]}, {"text": "This paper attempts to systematically compare the quality of five hypothesis alignment algorithms.", "labels": [], "entities": [{"text": "hypothesis alignment", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7707440555095673}]}, {"text": "Alignments were produced for the same system outputs from three common test sets used in the 2009 NIST Open MT Evaluation and the 2011 Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "NIST Open MT Evaluation", "start_pos": 98, "end_pos": 121, "type": "DATASET", "confidence": 0.6728512495756149}, {"text": "Statistical Machine Translation", "start_pos": 147, "end_pos": 178, "type": "TASK", "confidence": 0.6020087401072184}]}, {"text": "Identical pre-processing, decoding, and weight tuning algorithms were used to quantitatively evaluate the alignment quality.", "labels": [], "entities": []}, {"text": "Case insensitive BLEU score () was used as the translation quality metric.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.978713721036911}, {"text": "translation", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.9654799103736877}]}], "datasetContent": [{"text": "Combination experiments were performed on (i) Arabic-English, from the informal system combination track of the 2009 NIST Open MT Evaluation 4 ; (ii) German-English from the system combination evaluation of the 2011 Workshop on Statistical Machine Translation (Callison-Burch et al., 2011) (WMT11) and (iii) Spanish-English, again from WMT11.", "labels": [], "entities": [{"text": "NIST Open MT Evaluation 4", "start_pos": 117, "end_pos": 142, "type": "DATASET", "confidence": 0.7703775525093078}, {"text": "system combination evaluation of the 2011 Workshop on Statistical Machine Translation (Callison-Burch et al., 2011)", "start_pos": 174, "end_pos": 289, "type": "TASK", "confidence": 0.6949290484189987}, {"text": "WMT11", "start_pos": 291, "end_pos": 296, "type": "DATASET", "confidence": 0.7415616512298584}, {"text": "WMT11", "start_pos": 336, "end_pos": 341, "type": "DATASET", "confidence": 0.9845400452613831}]}, {"text": "Eight top-performing systems (as evaluated using case-insensitive BLEU) were used in each language pair.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9830214977264404}]}, {"text": "Case insensitive BLEU scores for the individual system outputs on the tuning and test sets are shown in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9989895224571228}]}, {"text": "About 300 and 800 sentences with four reference translations were available for Arabic-English tune and test sets, respectively, and about 500 and 2500 sentences with a single reference translation were available for both German-English and Spanish-English tune and test sets.", "labels": [], "entities": []}, {"text": "The system outputs were lower-cased and tokenized before building confusion networks using the five hypothesis alignment algorithms described above.", "labels": [], "entities": []}, {"text": "Unpruned English bi-gram and 5-gram language models were trained with about 6 billion words available for these evaluations.", "labels": [], "entities": []}, {"text": "Multiple component language models were trained after dividing the monolingual corpora by source.", "labels": [], "entities": []}, {"text": "Separate sets of interpolation weights were tuned for the NIST and WMT experiments to minimize perplexity on the English reference translations of the previous evaluations, NIST MT08 and WMT10.", "labels": [], "entities": [{"text": "NIST", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.9660001397132874}, {"text": "WMT", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.571955144405365}, {"text": "NIST MT08", "start_pos": 173, "end_pos": 182, "type": "DATASET", "confidence": 0.8611471354961395}, {"text": "WMT10", "start_pos": 187, "end_pos": 192, "type": "DATASET", "confidence": 0.9101075530052185}]}, {"text": "The system combination weights, both bi-gram lattice decoding and 5-gram 300-best list re-scoring weights, were tuned separately for lattices build with each hypothesis alignment algorithm.", "labels": [], "entities": []}, {"text": "The final re-scoring outputs were detokenized before computing case insensitive BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.995491087436676}]}, {"text": "Statistical significance was computed for each pairwise comparison using bootstrapping  The BLEU scores for Arabic-English system combination outputs are shown in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9989847540855408}]}, {"text": "The first column (Decode) shows the scores on tune and test sets for the decoding outputs.", "labels": [], "entities": []}, {"text": "The second column (Oracle) shows the scores for oracle hypotheses obtained by aligning the reference translations with the confusion networks and choosing the path with lowest graph TER ().", "labels": [], "entities": [{"text": "Oracle)", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.8685632348060608}, {"text": "TER", "start_pos": 182, "end_pos": 185, "type": "METRIC", "confidence": 0.9867483377456665}]}, {"text": "The rows representing different aligners are sorted according to the test set decoding scores.", "labels": [], "entities": []}, {"text": "The order of the BLEU scores for the oracle translations do not always follow the order for the decoding outputs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9982705116271973}]}, {"text": "This maybe due to differences in the compactness of the confusion networks.", "labels": [], "entities": []}, {"text": "A more compact network has fewer paths and is therefore less likely to contain significant parts of the reference translation, whereas a reference translation maybe generated from a less compact network.", "labels": [], "entities": []}, {"text": "On Arabic-English, all incremental alignment algorithms are significantly better than the pairwise GIZA, incremental IHMM and ITG with flexible matching are significantly better than all other algorithms, but not significantly different from each other.", "labels": [], "entities": []}, {"text": "The incremental TER and TERp were statistically indistinguishable.", "labels": [], "entities": [{"text": "TER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9601776003837585}, {"text": "TERp", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9696372151374817}]}, {"text": "Without flexible matching, iITG yields a BLEU score of 58.85 on test.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9849519729614258}]}, {"text": "The absolute BLEU gain over the best individual system was between 6.2 and 7.6 points on the test set.", "labels": [], "entities": [{"text": "BLEU gain", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9563223719596863}]}, {"text": "The BLEU scores for German-English system combination outputs are shown in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9984526634216309}]}, {"text": "Again, the graph TER oracle scores do not follow the same order as the decoding scores.", "labels": [], "entities": [{"text": "TER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.853400468826294}]}, {"text": "The scores for GIZA and iTERp are statistically indistinguishable, and iTER, iIHMM, and iITGp are significantly better than the first two.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.5063604116439819}, {"text": "iTER", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.867540717124939}]}, {"text": "However, they are not statistically different from each other.", "labels": [], "entities": []}, {"text": "Without flexible matching, iITG yields a BLEU score of 26.47 on test.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.98469677567482}]}, {"text": "The absolute BLEU gain over the best individual system was between 1.9 and 2.3 points on the test set.", "labels": [], "entities": [{"text": "BLEU gain", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9533801972866058}]}], "tableCaptions": [{"text": " Table 2: Case insensitive BLEU scores for NIST MT09  Arabic-English system combination outputs. Note, four  reference translations were available. Decode corre- sponds to results after weight tuning and Oracle corre- sponds to graph TER oracle. Dagger ( \u2020) denotes statisti- cally significant difference compared to GIZA and double  dagger ( \u2021) compared to iTERp and the aligners above it.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9911527037620544}, {"text": "NIST MT09  Arabic-English system combination outputs", "start_pos": 43, "end_pos": 95, "type": "DATASET", "confidence": 0.7710694471995035}, {"text": "double  dagger ( \u2021)", "start_pos": 326, "end_pos": 345, "type": "METRIC", "confidence": 0.901027038693428}]}, {"text": " Table 1: Case insensitive BLEU scores for the individual system outputs on the tune and test sets for all three source  languages.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.8933558464050293}]}, {"text": " Table 3: Case insensitive BLEU scores for WMT11  German-English system combination outputs. Note, only  a single reference translation per segment was available.  Decode corresponds to results after weight tuning and  Oracle corresponds to graph TER oracle. Dagger ( \u2020)  denotes statistically significant difference compared to  iTERp and GIZA.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9671871066093445}, {"text": "WMT11  German-English system combination outputs", "start_pos": 43, "end_pos": 91, "type": "TASK", "confidence": 0.7461479663848877}, {"text": "GIZA", "start_pos": 340, "end_pos": 344, "type": "METRIC", "confidence": 0.7891359329223633}]}, {"text": " Table 4: Case insensitive BLEU scores for WMT11  Spanish-English system combination outputs. Note, only  a single reference translation per segment was available.  Decode corresponds to results after weight tuning and  Oracle corresponds to graph TER oracle. Dagger ( \u2020)  denotes statistically significant difference compared to  aligners above iIHMM.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9693819880485535}, {"text": "WMT11  Spanish-English system combination outputs", "start_pos": 43, "end_pos": 92, "type": "TASK", "confidence": 0.7371826529502868}]}, {"text": " Table 6: Regression coefficients of the \"strong\" and  \"weak\" agreement features, as computed with a gener- alized linear model, using TER as the target variable.", "labels": [], "entities": [{"text": "Regression", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9903229475021362}, {"text": "TER", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.982455313205719}]}, {"text": " Table 5: p-values which show which error types are statistically significantly improved for each language and aligner.", "labels": [], "entities": []}]}