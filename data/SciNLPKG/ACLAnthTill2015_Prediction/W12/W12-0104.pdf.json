{"title": [{"text": "An Empirical Evaluation of Stop Word Removal in Statistical Machine Translation", "labels": [], "entities": [{"text": "Stop Word Removal", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.6174445450305939}, {"text": "Statistical Machine Translation", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.718832532564799}]}], "abstractContent": [{"text": "In this paper we evaluate the possibility of improving the performance of a statistical machine translation system by relaxing the complexity of the translation task by removing the most frequent and predictable terms from the target language vocabulary.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.623300701379776}]}, {"text": "After-wards, the removed terms are inserted back in the relaxed output by using an n-gram based word predictor.", "labels": [], "entities": []}, {"text": "Empirically, we have found that when these words are omitted from the text, the perplexity of the text decreases , which may imply the reduction of confusion in the text.", "labels": [], "entities": []}, {"text": "We conducted some machine translation experiments to see if this perplexity reduction produced a better translation output.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7244776040315628}]}, {"text": "While the word prediction results exhibits 77% accuracy in predicting 40% of the most frequent words in the text, the perplexity reduction did not help to produce better translations.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7751807868480682}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9990864992141724}]}], "introductionContent": [{"text": "It is a characteristic of natural language that a large proportion of running words in a corpus corresponds to a very small fraction of the vocabulary.", "labels": [], "entities": []}, {"text": "An analysis of the Brown Corpus has shown that the hundred most frequent words account for 42% of the corpus, while only 0.1% in the vocabulary.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9928665459156036}]}, {"text": "On the other hand, words occurring only once account merely 5.7% in the corpus but 58% in the vocabulary ().", "labels": [], "entities": []}, {"text": "This phenomenon can be explained in terms of Zipf's Law, which states that the product of word ranks and their frequencies approximates a constant, i.e. word-frequency plot is close to a hyperbolic function, and hence the few top ranked words would account fora great portion of the corpus.", "labels": [], "entities": []}, {"text": "Also, it appears that the top ranked words are mainly function words.", "labels": [], "entities": []}, {"text": "For instance, the eight most frequent words in the).", "labels": [], "entities": []}, {"text": "It is a common practice in Information Retrieval (IR) to filter the most frequent words out from processed documents (which are referred to as stop words), as these function words are semantically non-informative and constitute weak indexing terms.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.8816207051277161}]}, {"text": "By removing this great amount of stop words, not only space and time complexities can be reduced, but document content can be better discriminated by the remaining content words;.", "labels": [], "entities": []}, {"text": "Inspired by the concept of stop word removal in Information Retrieval, in this work we study the feasibility of stop word removal in Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "stop word removal", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7286263306935629}, {"text": "Information Retrieval", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7417412400245667}, {"text": "stop word removal", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7643890778223673}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 133, "end_pos": 170, "type": "TASK", "confidence": 0.8144375483194987}]}, {"text": "Different from Information Retrieval, that ranks or classifies documents; SMT hypothesizes sentences in target language.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.7087856382131577}, {"text": "SMT hypothesizes sentences in target language", "start_pos": 74, "end_pos": 119, "type": "TASK", "confidence": 0.7627515296141306}]}, {"text": "Therefore, without explicitly removing frequent words from the documents, we proposed to ignore such words in the target language vocabulary, i.e. by replacing those words with a null token.", "labels": [], "entities": []}, {"text": "We term this process as \"relaxation\" and the omitted words as \"relaxed words\".", "labels": [], "entities": []}, {"text": "Relaxed SMT here refers to a translation task in which target vocabulary words are intentionally omitted from the training dataset for reducing translation complexity.", "labels": [], "entities": [{"text": "Relaxed SMT", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6299583613872528}, {"text": "translation task", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.8902030289173126}]}, {"text": "Since the most frequent words are targeted to be relaxed, as a result, there will be vast amount of null tokens in the output text, which later shall be recovered in a post processing stage.", "labels": [], "entities": []}, {"text": "The idea of relaxation in SMT is motivated by one of our experimental findings, in which the perplexity measured over a test set decreases when most frequent words are relaxed.", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9894019961357117}]}, {"text": "For instance, a 15% of perplexity reduction is observed when the twenty most frequent words are relaxed in the English EPPS dataset.", "labels": [], "entities": [{"text": "perplexity reduction", "start_pos": 23, "end_pos": 43, "type": "METRIC", "confidence": 0.9620307683944702}, {"text": "English EPPS dataset", "start_pos": 111, "end_pos": 131, "type": "DATASET", "confidence": 0.812406082948049}]}, {"text": "The reduction of perplexity allows us to conjecture about the decrease of confusion in the text, from which a SMT system might be benefited.", "labels": [], "entities": [{"text": "SMT", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.99321049451828}]}, {"text": "After applying relaxed SMT, the resulting null tokens in the translated sentences have to be replaced by the corresponding words from the set of relaxed words.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9246336817741394}]}, {"text": "As relaxed words are chosen from the top ranked words, which possess high occurrences in the corpus, their n-gram probabilities could be reliably trained to serve for word prediction.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 167, "end_pos": 182, "type": "TASK", "confidence": 0.8103537857532501}]}, {"text": "Also, these words are mainly function words and, from the human perspective, function words are usually much easier to predict from their neighbor context than content words.", "labels": [], "entities": []}, {"text": "Consider for instance the sentence the house of the president is very nice.", "labels": [], "entities": []}, {"text": "Function words like the, of, and is, are certainly easier to be predicted than content words such as house, president, and nice.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized into four sections.", "labels": [], "entities": []}, {"text": "In section 2, we discuss the relaxation strategy implemented fora SMT system, which generates translation outputs that contain null tokens.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9911705255508423}]}, {"text": "In section 3, we present the word prediction mechanism used to recover the null tokens occurring in the relaxed translation outputs.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7821365296840668}]}, {"text": "In section 4, we present and discuss the experimental results.", "labels": [], "entities": []}, {"text": "Finally, in section 5 we present the most relevant conclusion of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first highlight the Zipfian distribution in the corpus and the reduction of perplexity after removing the top ranked words.", "labels": [], "entities": []}, {"text": "The n-gram probabilities estimated were then used for word prediction, and we report the resulting prediction accuracy at different relaxation orders.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.9084193408489227}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9726816415786743}]}, {"text": "The performance of the SMT system with a relaxed vocabulary is presented and discussed in the last subsection of this section.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9943711161613464}]}], "tableCaptions": []}