{"title": [], "abstractContent": [{"text": "We present the approach we took for our participation to the WMT12 Quality Estimation Shared Task: our main goal is to achieve reasonably good results without appeal to supervised learning.", "labels": [], "entities": [{"text": "WMT12 Quality Estimation Shared Task", "start_pos": 61, "end_pos": 97, "type": "TASK", "confidence": 0.7140649616718292}]}, {"text": "We have used various similarity measures and also an external resource (Google N-grams).", "labels": [], "entities": [{"text": "similarity", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9444736242294312}]}, {"text": "Details of results clarify the interest of such an approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality Estimation (or Confidence Estimation) refers hereto the task of evaluating the quality of the output produced by a Machine Translation (MT) system.", "labels": [], "entities": [{"text": "Quality Estimation (or Confidence Estimation)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7969253063201904}, {"text": "Machine Translation (MT)", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.8410497307777405}]}, {"text": "More precisely it consists in evaluating the quality of every individual sentence, in order (for instance) to decide whether a given sentence can be published as it is, should be post-edited, or is so bad that it should be manually re-translated.", "labels": [], "entities": []}, {"text": "To our knowledge, most approaches so far) use several features combined together using supervised learning in order to predict quality scores.", "labels": [], "entities": []}, {"text": "These features belong to two categories: black box features which can be extracted given only the input sentence and its translated version, and glass box features which rely on various intermediate steps of the internal MT engine (thus require access to this internal data).", "labels": [], "entities": []}, {"text": "For the features they studied,  have shown that black box features are informative enough and glass box features do not significantly contribute to the accuracy of the predicted scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9989976286888123}]}, {"text": "In this study, we use only black box features, and further, eschew supervised learning except in the broadest sense.", "labels": [], "entities": []}, {"text": "Our method requires some reference data, all taken to be equally good exemplars of a positive reference category, against which the experimental sentences are compared automatically.", "labels": [], "entities": []}, {"text": "This is the extent of broader-sense supervision.", "labels": [], "entities": []}, {"text": "The method does not require a training set of items each annotated by human experts with quality scores (except for the purpose of evaluation of course).", "labels": [], "entities": []}, {"text": "Successful unsupervised learning averts risks of the alternative: supervised learning necessarily makes the predicting system dependent on the annotated training data, i.e. less generic, and requires a costly human evalution stage to obtain a reliable model.", "labels": [], "entities": []}, {"text": "Of course, our approach is likely not to perform as well as supervised approaches: here the goal is to find a rather generic robust way to measure quality, not to achieve the best accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9940221905708313}]}, {"text": "Nevertheless, in the context of this Quality Evaluation Shared task (see) fora detailed description) we have also used supervised learning as a final stage, in order to submit results which can be compared to other methods (see \u00a74).", "labels": [], "entities": []}, {"text": "We investigate the use of various similarity measures for evaluating the quality of machine translated sentences.", "labels": [], "entities": []}, {"text": "These measures compare the sentence to be evaluated against a reference text, providing a similarity score result.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 90, "end_pos": 106, "type": "METRIC", "confidence": 0.9670067727565765}]}, {"text": "The reference data is supposed to represent standard (well-formed) language, so that the score is expected to reflect how complex (source side) or how fluent (target side) the given sentence is.", "labels": [], "entities": []}, {"text": "After presenting the similarity measures in sec-tion 2, we will show in section 3 how they perform individually on the ranking task; finally we will explain in section 4 how the results that we submitted were obtained using supervised learning.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Best results by method and by resource on train- ing data. b = sentence boundaries ; lf = log frequency  (Google) ; EP = Europarl.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 131, "end_pos": 139, "type": "DATASET", "confidence": 0.9743927121162415}]}, {"text": " Table 2: Best results on 10-folds cross-validation on the  training data (sorted by DeltaAvg score).", "labels": [], "entities": [{"text": "DeltaAvg score", "start_pos": 85, "end_pos": 99, "type": "METRIC", "confidence": 0.6332152187824249}]}]}