{"title": [{"text": "Probes in a Taxonomy of Factored Phrase-Based Models *", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce a taxonomy of factored phrase-based translation scenarios and conduct a range of experiments in this taxonomy.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6031285226345062}]}, {"text": "We point out several common pitfalls when designing factored setups.", "labels": [], "entities": []}, {"text": "The paper also describes our WMT12 submissions CU-BOJAR and CU-POOR-COMB.", "labels": [], "entities": [{"text": "WMT12 submissions CU-BOJAR", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.5575413902600607}]}], "introductionContent": [{"text": "introduced \"factors\" to phrase-based MT to explicitly capture arbitrary features in the phrase-based model.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.7526934742927551}]}, {"text": "In essence, input and output tokens are no longer atomic units but rather vectors of atomic values encoding e.g. the lexical and morphological information separately.", "labels": [], "entities": []}, {"text": "Factored translation has been successfully applied to many language pairs and with diverse types of information encoded in the additional factors, i.a..", "labels": [], "entities": [{"text": "Factored translation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7114034593105316}]}, {"text": "On the other hand, it happens quite frequently, that the factored setup causes a loss compared to the phrase-based baseline.", "labels": [], "entities": []}, {"text": "The underlying reason is the complexity of the search space which gets boosted when the model explicitly includes detailed information, see e.g. Bojar and or.", "labels": [], "entities": [{"text": "Bojar", "start_pos": 145, "end_pos": 150, "type": "DATASET", "confidence": 0.8215644955635071}]}, {"text": "* This work was supported by the project EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of the Czech Republic) and the Czech Science Foundation grants P406/11/1499 and P406/10/P259.", "labels": [], "entities": [{"text": "EuroMatrixPlus", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.961584210395813}, {"text": "FP7-ICT-2007-3-231720", "start_pos": 57, "end_pos": 78, "type": "DATASET", "confidence": 0.5889670252799988}, {"text": "P406/11/1499", "start_pos": 172, "end_pos": 184, "type": "DATASET", "confidence": 0.7146139860153198}, {"text": "P406/10/P259", "start_pos": 189, "end_pos": 201, "type": "DATASET", "confidence": 0.8226495862007142}]}, {"text": "We are grateful for reviewers' comments but we have to obey the 6 page limit.", "labels": [], "entities": []}, {"text": "Thanks also to Ale\u0161 Tamchyna for supplementary material on MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.8176755905151367}]}, {"text": "In this paper, we first provide a taxonomy of (phrase-based) translation setups and then we examine a range of sample configurations in this taxonomy.", "labels": [], "entities": [{"text": "phrase-based) translation setups", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.6740391850471497}]}, {"text": "We don't state universal rules, because the applicability of each of the setups depends very much on the particular language pair, text domain and amount of data available, but we hope to draw attention to relevant design decisions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Single-step scenarios consist of more than one translation steps within a single search.", "labels": [], "entities": []}, {"text": "We do not distinguish whether all the translation steps belong to the same decoding path or to alternative decoding paths.", "labels": [], "entities": []}, {"text": "lists several single-step configurations (and three direct translations fora comparison).", "labels": [], "entities": []}, {"text": "The single-step configurations always include the linguistically-motivated tL-L+tT-T+gLaT-F with varying language models and optionally with an alternative decoding path to serve as the fallback.", "labels": [], "entities": []}, {"text": "Aware of the low stability of MERT (Clark et al., 2011), we run MERT three times and report the average BLEU score including the standard deviation.", "labels": [], "entities": [{"text": "MERT", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.41320616006851196}, {"text": "MERT", "start_pos": 64, "end_pos": 68, "type": "TASK", "confidence": 0.6646847128868103}, {"text": "BLEU score", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9771396815776825}]}, {"text": "The last column in lists the average number of distinct candidates per sentence in the nbest lists during MERT, dubbed \"effective n-best list size\".", "labels": [], "entities": [{"text": "MERT", "start_pos": 106, "end_pos": 110, "type": "TASK", "confidence": 0.6722615957260132}]}, {"text": "Unless stated otherwise, we used 100-best lists.", "labels": [], "entities": []}, {"text": "We see that due to spurious ambiguity, e.g. various segmentations of the input into phrases, the effective size does not reach even a half of the limit.", "labels": [], "entities": []}, {"text": "We make three observations here: (1) In this small data setting with a very morphologically rich language, the complex setup tL-L+tT-T+gLaT-F does not even need the alternative decoding path tF-F. report gains in English-to-Hindi translation and also probably do not use alternative decoding paths.", "labels": [], "entities": []}, {"text": "(2) Reducing the range of language models used leads to worse scores, which is inline with the observation made with direct setups.", "labels": [], "entities": []}, {"text": "We are surprised by the relative importance of the lemma-based LM.", "labels": [], "entities": []}, {"text": "(3) Alternative decoding paths significantly reduce effective n-best list size to just 12-18 unique candidates per sentence.", "labels": [], "entities": []}, {"text": "However, we don't see an obvious relation to the stability of MERT: the standard deviations of BLEU average are very similar except for two outliers: 13.15\u00b10.24 and 13.01\u00b10.43.", "labels": [], "entities": [{"text": "MERT", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.6733132004737854}, {"text": "BLEU average", "start_pos": 95, "end_pos": 107, "type": "METRIC", "confidence": 0.9698935151100159}]}, {"text": "One of the outliers, 13.15, is actually a repeated run of the 13.31 with n-best-list size set to 200.", "labels": [], "entities": []}, {"text": "Here we see a slight increase in the effective size (20 instead of 12) but also a slight loss in BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9978561997413635}]}, {"text": "We repeated the 13.31 experiment also with n \u2208 {300, 400, 500, 600}, three MERT runs for each n.", "labels": [], "entities": [{"text": "MERT", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9937619566917419}]}, {"text": "All the runs reached BLEU of about 13.30 except for one (n = 600) where the score dropped to 11.50.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9996256828308105}]}, {"text": "The low result was obtained when MERT ended at 25 iterations, the standard limit.", "labels": [], "entities": [{"text": "MERT", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.4273383915424347}]}, {"text": "On the other hand, several successful runs also exhausted the limit.", "labels": [], "entities": []}, {"text": "weight settings from the (accumulated) n-best list.", "labels": [], "entities": []}, {"text": "We plot this predicted BLEU twice: once on the y2 axis alone and for the second time on the primary y axis together with the real BLEU, i.e. the BLEU of the dev set when Moses is actually run with the weight settings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9984140396118164}, {"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9972965121269226}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9987397789955139}]}, {"text": "The real BLEU drops several times, indicating that the prediction was misleading.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9980605244636536}]}, {"text": "Similar drops were observed in all runs.", "labels": [], "entities": []}, {"text": "With bad luck as here, the iteration limit is reached when the optimization is still recovering from such a drop.", "labels": [], "entities": []}, {"text": "To avoid such a pitfall, one should check the real BLEU and continue or simply rerun the optimization if the iteration limit was reached.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9966302514076233}]}, {"text": "The linguistically motivated setups used in the previous sections are prohibitively expensive for large data, see also.", "labels": [], "entities": []}, {"text": "A number of researchers have thus tried diving the complexity of search into two independent phases: (1) translation and reordering, and (2) conjugation and declination.", "labels": [], "entities": [{"text": "translation", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.9784420132637024}]}, {"text": "The most promising results were obtained with the second step predicting individual morphological features using a specialized tool (.", "labels": [], "entities": []}, {"text": "Here, we simply use one more Moses search as.", "labels": [], "entities": []}, {"text": "In the first step, source English gets translated to a simplified Czech and in the second step, the simplified Czech gets fully inflected.", "labels": [], "entities": []}, {"text": "We see an interesting difference between MOT 1 and MOT 0 or 2 . The more fine-grained MOT 0 or 2 work better in the two-factor \"|\" setup that allows to disregard the MOT, while MOT 1 works better in the direct translation \"+\".", "labels": [], "entities": []}, {"text": "Overall, we see no improvement over the tF-F baseline (BLEU of 12.42) and this is mainly due to to the fact that we used Small data in both steps.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9969484210014343}]}], "tableCaptions": [{"text": " Table 3: Results of three MERT runs of several single-step configurations.", "labels": [], "entities": [{"text": "MERT", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.9407649040222168}]}, {"text": " Table 5: Summary of large data runs and systems submitted to WMT12 manual evaluation. The upper part lists the  two submissions in en\u2192cs translation and two more systems used in CU-POOR-COMB. The lower part of the table  shows the scores for CU-BOJAR when translating to English. All systems reported here use the Large and Mono data.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.779626727104187}, {"text": "CU-POOR-COMB", "start_pos": 179, "end_pos": 191, "type": "DATASET", "confidence": 0.9352264404296875}, {"text": "CU-BOJAR", "start_pos": 243, "end_pos": 251, "type": "DATASET", "confidence": 0.8987280130386353}, {"text": "Mono data", "start_pos": 325, "end_pos": 334, "type": "DATASET", "confidence": 0.8013420104980469}]}]}