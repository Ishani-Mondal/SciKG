{"title": [], "abstractContent": [{"text": "This paper describes the French-English translation system developed by the Avenue research group at Carnegie Mellon University for the Seventh Workshop on Statistical Machine Translation (NAACL WMT12).", "labels": [], "entities": [{"text": "Statistical Machine Translation (NAACL WMT12)", "start_pos": 156, "end_pos": 201, "type": "TASK", "confidence": 0.7196351672921862}]}, {"text": "We present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building.", "labels": [], "entities": [{"text": "training data selection", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.6380041340986887}, {"text": "phrase-based translation", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.6761232912540436}]}], "introductionContent": [{"text": "We describe the French-English translation system constructed by the Avenue research group at Carnegie Mellon University for the shared translation task in the Seventh Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "shared translation task in the Seventh Workshop on Statistical Machine Translation", "start_pos": 129, "end_pos": 211, "type": "TASK", "confidence": 0.5687031718817624}]}, {"text": "The core translation system uses the hierarchical phrase-based model described by with sentence-level grammars extracted and scored using the methods described by.", "labels": [], "entities": []}, {"text": "Improved techniques for data selection and monolingual text processing significantly improve the performance of the baseline system.", "labels": [], "entities": [{"text": "data selection", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7603366076946259}, {"text": "monolingual text processing", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6417797605196635}]}, {"text": "Over half of all parallel data for the FrenchEnglish track is provided by the Giga-FrEn corpus.", "labels": [], "entities": [{"text": "FrenchEnglish track", "start_pos": 39, "end_pos": 58, "type": "DATASET", "confidence": 0.9673014581203461}, {"text": "Giga-FrEn corpus", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.9526894688606262}]}, {"text": "Assembled from crawls of bilingual websites, this corpus is known to be noisy, containing sentences that are either not parallel or not natural language.", "labels": [], "entities": []}, {"text": "Rather than simply including or excluding the resource in its entirety, we use a relatively simple technique inspired by work in machine translation quality estimation to select the best portions of the corpus for inclusion in our training data.", "labels": [], "entities": [{"text": "machine translation quality estimation", "start_pos": 129, "end_pos": 167, "type": "TASK", "confidence": 0.8119818717241287}]}, {"text": "Including around 60% of the Giga-FrEn chosen by this technique yields an improvement of 0.7 BLEU.", "labels": [], "entities": [{"text": "Giga-FrEn", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9775652885437012}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9986938834190369}]}, {"text": "Prior to model estimation, we process all parallel and monolingual data using in-house tokenization and normalization scripts that detect word boundaries better than the provided WMT12 scripts.", "labels": [], "entities": []}, {"text": "After translation, we apply a monolingual rule-based postprocessing step to correct obvious errors and make sentences more acceptable to human judges.", "labels": [], "entities": [{"text": "translation", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.9621328115463257}]}, {"text": "The post-processing step alone yields an improvement of 0.3 BLEU to the final system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9993982315063477}]}, {"text": "We conclude with a discussion of the impact of data size on important decisions for system building.", "labels": [], "entities": []}, {"text": "Experimental results show that \"best practice\" decisions for smaller data sizes do not necessarily carryover to systems built with \"WMT-scale\" data, and provide some explanation for why this is the case.", "labels": [], "entities": [{"text": "WMT-scale\" data", "start_pos": 132, "end_pos": 147, "type": "DATASET", "confidence": 0.6446929176648458}]}], "datasetContent": [{"text": "Beginning with a baseline translation system, we incrementally evaluate the contribution of additional data and components.", "labels": [], "entities": []}, {"text": "System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (), Meteor (Denkowski and Lavie, 2011), and TER ().", "labels": [], "entities": [{"text": "newstest 2011", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.8645247519016266}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9977160692214966}, {"text": "Meteor (Denkowski and Lavie, 2011)", "start_pos": 84, "end_pos": 118, "type": "DATASET", "confidence": 0.8508489280939102}, {"text": "TER", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.9864002466201782}]}, {"text": "For full consistency with WMT11, we use the NIST scoring script, TER-0.7.25, and Meteor-1.3 to evaluate cased, detokenized translations.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.9097143411636353}, {"text": "NIST scoring script", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.7237864335378011}, {"text": "TER-0.7.25", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9938241243362427}]}, {"text": "Results are shown in, where each evaluation point is the result of a full tune/test run that includes MERT for parameter optimization.", "labels": [], "entities": [{"text": "MERT", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9764080047607422}]}, {"text": "The baseline translation system is built from 14 million parallel sentences (Europarl, news commentary, and UN doc) and all monolingual data.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.985929548740387}]}, {"text": "Grammars are extracted using the \"tight\" heuristic that requires phrase pairs to be bounded byword alignments.", "labels": [], "entities": []}, {"text": "Both 4-gram and 5-gram language models are evaluated.", "labels": [], "entities": []}, {"text": "Viterbi decoding is conducted with a cube pruning pop limit   sentences and further improves performance.", "labels": [], "entities": []}, {"text": "These runs require new grammars to be extracted, but use the same 4-gram language model and decoding method as the baseline system.", "labels": [], "entities": []}, {"text": "With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding () over 500-best lists collectively show a slight improvement.", "labels": [], "entities": []}, {"text": "Monolingual post-processing yields further improvement.", "labels": [], "entities": []}, {"text": "This decoding/processing scheme corresponds to our final translation system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Results for extraction heuristics (dev-test)", "labels": [], "entities": []}]}