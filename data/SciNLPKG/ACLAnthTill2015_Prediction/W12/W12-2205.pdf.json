{"title": [{"text": "Building Readability Lexicons with Unannotated Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "Lexicons of word difficulty are useful for various educational applications, including read-ability classification and text simplification.", "labels": [], "entities": [{"text": "read-ability classification", "start_pos": 87, "end_pos": 114, "type": "TASK", "confidence": 0.7634919285774231}, {"text": "text simplification", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7915591299533844}]}, {"text": "In this work, we explore automatic creation of these lexicons using methods which go beyond simple term frequency, but without relying on age-graded texts.", "labels": [], "entities": []}, {"text": "In particular, we derive information for each word type from the readability of the web documents they appear in and the words they co-occur with, linearly combining these various features.", "labels": [], "entities": []}, {"text": "We show the efficacy of this approach by comparing our lexicon with an existing coarse-grained, low-coverage resource and anew crowdsourced annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "With its goal of identifying documents appropriate to readers of various proficiencies, automatic analysis of readability is typically approached as a textlevel classification task.", "labels": [], "entities": [{"text": "textlevel classification", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.7007234245538712}]}, {"text": "Although at least one popular readability metric and a number of machine learning approaches to readability rely on lexical features, the readability of individual lexical items is not addressed directly in these approaches.", "labels": [], "entities": []}, {"text": "Nevertheless, information about the difficulty of individual lexical items, in addition to being useful for text readability classification (, can be applied to other tasks, for instance lexical simplification (.", "labels": [], "entities": [{"text": "text readability classification", "start_pos": 108, "end_pos": 139, "type": "TASK", "confidence": 0.7478230992952982}, {"text": "lexical simplification", "start_pos": 187, "end_pos": 209, "type": "TASK", "confidence": 0.7725332677364349}]}, {"text": "Our interest is in providing students with educational software that is sensitive to the difficulty of particular English expressions, providing proactive support for those which are likely to be outside a reader's vocabulary.", "labels": [], "entities": []}, {"text": "However, our existing lexical resource is coarse-grained and lacks coverage.", "labels": [], "entities": []}, {"text": "In this paper, we explore the extent to which an automatic approach could be used to fill in the gaps of our lexicon.", "labels": [], "entities": []}, {"text": "Prior approaches have generally depended on some kind of age-graded corpus, but this kind of resource is unlikely to provide the coverage that we require; instead, our methods here are based on statistics from a huge web corpus.", "labels": [], "entities": []}, {"text": "We show that frequency, an obvious proxy for difficulty, is only the first step; in fact we can derive key information from the documents that words appear in and the words that they appear with, information that can be combined to give high performance in identifying relative difficulty.", "labels": [], "entities": [{"text": "frequency", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.971276044845581}]}, {"text": "We compare our automated lexicon against our existing resource as well as a crowdsourced annotation.", "labels": [], "entities": []}], "datasetContent": [{"text": "All results are based on comparing the relative difficulty judgments made for the word pairs in our test set (or, more often, some subset) by the various sources.", "labels": [], "entities": []}, {"text": "Since even the existing Difficulty lexicon is not entirely reliable, we report agreement rather than accuracy.", "labels": [], "entities": [{"text": "Difficulty lexicon", "start_pos": 24, "end_pos": 42, "type": "DATASET", "confidence": 0.8878734409809113}, {"text": "agreement", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.990881085395813}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9979867935180664}]}, {"text": "Except for agreement of Crowdflower workers, agreement is the percentage of pairs where the sources agreed as compared to the total number of pairs.", "labels": [], "entities": [{"text": "agreement", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.992438018321991}]}, {"text": "For agreement between Crowdflower workers, we follow in calculating agreement across all possible pairings of each worker for each pair.", "labels": [], "entities": []}, {"text": "Although we considered using a more complex metric such as Kappa, we believe that simple pairwise agreement is in fact equally interpretable when the main interest is relative agreement of various methods; besides, Kappa is intended for use with individual annotators with particular biases, an assumption which does not hold here.", "labels": [], "entities": []}, {"text": "To evaluate the reliability of our human-annotated resources, we look first at the agreement within the Crowdflower data, and between the Crowdflower and our Difficulty lexicon, with particular attention to within-class judgments.", "labels": [], "entities": [{"text": "Crowdflower data", "start_pos": 104, "end_pos": 120, "type": "DATASET", "confidence": 0.958053857088089}]}, {"text": "We then compare the predictions of various automatically extracted features and feature combinations with these human judgments; since most of these involve a continuous scale, we focus only on words which were judged to be different.", "labels": [], "entities": []}, {"text": "For the Difficulty lexicon (Diff.), then in this comparison is 3000, while for the Crowdflower (CF) judgments it is 4002.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Agreement (%) of automated methods with man- ual resources on pairwise comparison task (Diff. = Diffi- culty lexicon, CF = Crowdflower)", "labels": [], "entities": []}]}