{"title": [], "abstractContent": [{"text": "Evaluation of the content of free texts is a challenging task for humans.", "labels": [], "entities": [{"text": "Evaluation of the content of free texts", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8452108417238507}]}, {"text": "Automation of this process is largely useful in order to reduce human related errors.", "labels": [], "entities": []}, {"text": "We consider one instance of the \"free texts\" assessment problems; automatic essay grading where the task is to grade student written essays automatically given course materials and a set of human-graded essays as training data.", "labels": [], "entities": []}, {"text": "We use a Latent Semantic Analysis (LSA)-based methodology to accomplish this task.", "labels": [], "entities": []}, {"text": "We experiment on a dataset obtained from an occupational therapy course and report the results.", "labels": [], "entities": []}, {"text": "We also discuss our findings, analyze different problem areas and explain the potential solutions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of assessing free texts involves understanding the inner meaning of the free texts.", "labels": [], "entities": []}, {"text": "Automation of free text assessment is necessary specially when an expert evaluator is unavailable in today's Internet-based learning environment.", "labels": [], "entities": [{"text": "free text assessment", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.6052337189515432}]}, {"text": "This is also useful to reduce human related errors such as \"rater effects\".", "labels": [], "entities": [{"text": "rater", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.9448937773704529}]}, {"text": "Research to automate the assessment of free texts, such as grading student-written essays, has been carried out over the years.", "labels": [], "entities": []}, {"text": "The earlier approaches such as Project Essay Grade (PEG) and e-rater () were solely based on some simple surface features that took essay-length, number of commas etc. into consideration whereas recent research has tended to focus on understanding the inner meaning of the texts.", "labels": [], "entities": []}, {"text": "Latent Semantic Analysis (LSA) () has been shown to fit well in addressing this task previously (.", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6879559059937795}]}, {"text": "LSA uses a sophisticated approach to decode the inherent relationships between a context (typically a sentence, a paragraph or a document) and the words that they contain.", "labels": [], "entities": []}, {"text": "The main idea behind the LSA is to measure the semantic similarities to be found between two texts from words contained within.", "labels": [], "entities": []}, {"text": "In this paper, we use LSA to automatically grade student-written essays.", "labels": [], "entities": []}, {"text": "We experiment with different local and global weighting functions . Experiments on an occupational therapy dataset show that the performance of the LSA varies with respect to the weighting function used.", "labels": [], "entities": []}, {"text": "In the next sections, we present an overview of LSA, describe our approach, and present the evaluation results.", "labels": [], "entities": [{"text": "LSA", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9028289318084717}]}, {"text": "We then discuss various problem areas related to the evaluation framework and explain potential solutions.", "labels": [], "entities": []}, {"text": "Finally, we conclude the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Inspired by the work of Jorge-Botana et al., we experiment with different local and global weighting functions applied to the WCM.", "labels": [], "entities": []}, {"text": "The main idea is to transform the raw frequency cell xi j of the WCM into the product of a local term weight l i j , and a global term weight g j . Given the term/document frequency matrix (WCM), a weighting algorithm is applied to each entry that has three components to makeup the new weighted value in the term/document matrix.", "labels": [], "entities": []}, {"text": "This looks as: where w i j is the weighted value for the i th term in the j th context, l i j is the local weight for term i in the context j, g j is the global weight for the term i across all contexts in the collection, and N j is the normalization factor for context j.", "labels": [], "entities": []}, {"text": "Local Weighting: We use two local weighting methods in this work: 1) Logarithmic: log 1 + f i j , and 2) Term Frequency (TF): f i j , where f i j is the number of times (frequency) the term i appears in the context j.", "labels": [], "entities": [{"text": "Term Frequency (TF)", "start_pos": 105, "end_pos": 124, "type": "METRIC", "confidence": 0.7670107483863831}]}, {"text": "Global Weighting: We experiment with three global weighting methods: 1) Entropy:  The performance of the LSA models can be verified by measuring their correlation with the human-graded essays (as shown in Section 4.3).", "labels": [], "entities": [{"text": "Global Weighting", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6862439215183258}]}, {"text": "To omit the human intervention associated with this method, we can introduce an automatic evaluation module that uses syntactic and/or shallow semantic tree kernels to measure the textual similarity between the student-written essays and the given course materials.", "labels": [], "entities": []}, {"text": "The basic LSA model that uses cosine similarity measure has one problem in automatic grading of academic essays.", "labels": [], "entities": []}, {"text": "In this method, a student essay can obtain a good grade by having a very small number of highly representative terms that correlates the golden essays.", "labels": [], "entities": []}, {"text": "This also means that the repetition of important terms without having any syntactic/semantic appropriateness can lead to a overstated grade).", "labels": [], "entities": []}, {"text": "So, we can check the LSA model's performance by measuring syntactic/semantic similarity of the student-written essays corresponding to the course materials.", "labels": [], "entities": []}, {"text": "Syntactic and semantic features have been used successfully in various NLP tasks).", "labels": [], "entities": []}, {"text": "Based on some preliminary case-by-case analysis, we find the automatic evaluation model to be promising.", "labels": [], "entities": []}], "tableCaptions": []}