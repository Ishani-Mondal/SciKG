{"title": [{"text": "NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 20-28, Deep Neural Network Language Models", "labels": [], "entities": [{"text": "NAACL-HLT 2012 Workshop", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.9614335099856058}]}], "abstractContent": [{"text": "In recent years, neural network language models (NNLMs) have shown success in both peplexity and word error rate (WER) compared to conventional n-gram language models.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 97, "end_pos": 118, "type": "METRIC", "confidence": 0.8895390331745148}]}, {"text": "Most NNLMs are trained with one hidden layer.", "labels": [], "entities": []}, {"text": "Deep neural networks (DNNs) with more hidden layers have been shown to capture higher-level discriminative information about input features, and thus produce better networks.", "labels": [], "entities": []}, {"text": "Motivated by the success of DNNs in acoustic modeling, we explore deep neural network language models (DNN LMs) in this paper.", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7750603556632996}]}, {"text": "Results on a Wall Street Journal (WSJ) task demonstrate that DNN LMs offer improvements over a single hidden layer NNLM.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) task", "start_pos": 13, "end_pos": 43, "type": "DATASET", "confidence": 0.9256424222673688}]}, {"text": "Furthermore, our preliminary results are competitive with a model M language model, considered to be one of the current state-of-the-art techniques for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 152, "end_pos": 169, "type": "TASK", "confidence": 0.7128984332084656}]}], "introductionContent": [{"text": "Statistical language models are used in many natural language technologies, including automatic speech recognition (ASR), machine translation, handwriting recognition, and spelling correction, as a crucial component for improving system performance.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 86, "end_pos": 120, "type": "TASK", "confidence": 0.8256718913714091}, {"text": "machine translation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7984216511249542}, {"text": "handwriting recognition", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.8652815818786621}, {"text": "spelling correction", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.9035900235176086}]}, {"text": "A statistical language model represents a probability distribution overall possible word strings in a language.", "labels": [], "entities": []}, {"text": "In state-of-the-art ASR systems, n-grams are the conventional language modeling approach due to their simplicity and good modeling performance.", "labels": [], "entities": [{"text": "ASR", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9868148565292358}]}, {"text": "One of the problems in n-gram language modeling is data sparseness.", "labels": [], "entities": [{"text": "n-gram language modeling", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.6474441091219584}]}, {"text": "Even with large training corpora, extremely small or zero probabilities can be assigned to many valid word sequences.", "labels": [], "entities": []}, {"text": "Therefore, smoothing techniques) are applied to n-grams to reallocate probability mass from observed n-grams to unobserved n-grams, producing better estimates for unseen data.", "labels": [], "entities": []}, {"text": "Even with smoothing, the discrete nature of ngram language models make generalization a challenge.", "labels": [], "entities": [{"text": "generalization", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.9672580361366272}]}, {"text": "What is lacking is a notion of word similarity, because words are treated as discrete entities.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.6625924855470657}]}, {"text": "In contrast, the neural network language model (NNLM) ( embeds words in a continuous space in which probability estimation is performed using single hidden layer neural networks (feed-forward or recurrent).", "labels": [], "entities": []}, {"text": "The expectation is that, with proper training of the word embedding, words that are semantically or gramatically related will be mapped to similar locations in the continuous space.", "labels": [], "entities": []}, {"text": "Because the probability estimates are smooth functions of the continuous word representations, a small change in the features results in a small change in the probability estimation.", "labels": [], "entities": []}, {"text": "Therefore, the NNLM can achieve better generalization for unseen n-grams.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.9445952773094177}]}, {"text": "Feedforward NNLMs ( and recurrent NNLMs () have been shown to yield both perplexity and word error rate (WER) improvements compared to conventional n-gram language models.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 88, "end_pos": 109, "type": "METRIC", "confidence": 0.9019765059153239}]}, {"text": "An alternate method of embedding words in a continuous space is through tied mixture language models ( , where n-grams frequencies are modeled similar to acoustic features.", "labels": [], "entities": []}, {"text": "To date, NNLMs have been trained with one hid-den layer.", "labels": [], "entities": []}, {"text": "A deep neural network (DNN) with multiple hidden layers can learn more higher-level, abstract representations of the input.", "labels": [], "entities": []}, {"text": "For example, when using neural networks to process a raw pixel representation of an image, lower layers might detect different edges, middle layers detect more complex but local shapes, and higher layers identify abstract categories associated with sub-objects and objects which are parts of the image.", "labels": [], "entities": []}, {"text": "Recently, with the improvement of computational resources (i.e. GPUs, mutli-core CPUs) and better training strategies (), DNNs have demonstrated improved performance compared to shallower networks across a variety of pattern recognition tasks in machine learning).", "labels": [], "entities": [{"text": "pattern recognition tasks", "start_pos": 217, "end_pos": 242, "type": "TASK", "confidence": 0.7933216691017151}]}, {"text": "In the acoustic modeling community, DNNs have proven to be competitive with the wellestablished Gaussian mixture model (GMM) acoustic model.", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7675351202487946}]}, {"text": "The depth of the network (the number of layers of nonlinearities that are composed to make the model) and the modeling a large number of context-dependent states) are crucial ingredients in making neural networks competitive with GMMs.", "labels": [], "entities": []}, {"text": "The success of DNNs in acoustic modeling leads us to explore DNNs for language modeling.", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.6999455839395523}, {"text": "language modeling", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7226694375276566}]}, {"text": "In this paper we follow the feed-forward NNLM architecture given in ( and make the neural network deeper by adding additional hidden layers.", "labels": [], "entities": []}, {"text": "We call such models deep neural network language models (DNN LMs).", "labels": [], "entities": []}, {"text": "Our preliminary experiments suggest that deeper architectures have the potential to improve over single hidden layer NNLMs.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: The next section explains the architecture of the feed-forward NNLM.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.7298719882965088}]}, {"text": "Section 3 explains the details of the baseline acoustic and language models and the set-up used for training DNN LMs.", "labels": [], "entities": [{"text": "DNN LMs", "start_pos": 109, "end_pos": 116, "type": "TASK", "confidence": 0.5292927622795105}]}, {"text": "Our preliminary results are given in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 summarizes the related work to our paper.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our initial experiments are on a single hidden layer NNLM with 100 hidden units and 30 dimensional features.", "labels": [], "entities": []}, {"text": "We chose this configuration for our initial experiments because this models trains in one day of training on an 8-core CPU machine.", "labels": [], "entities": []}, {"text": "However, the performance of this model on both the held-out and test sets was worse than the baseline.", "labels": [], "entities": []}, {"text": "We therefore increased the number of hidden units to 500, while keeping the 30-dimensional features.", "labels": [], "entities": []}, {"text": "Training a single hidden layer NNLM with this configuration required approximately 3 days on an 8-core CPU machine.", "labels": [], "entities": []}, {"text": "Adding additional hidden layers does not have as much an impact in the training time as increased units in the output layer.", "labels": [], "entities": []}, {"text": "This is because the computational complexity of a DNN LM is dominated by the computation in the output layer.", "labels": [], "entities": []}, {"text": "However, increasing the number of hidden units does impact the training time.", "labels": [], "entities": []}, {"text": "We also experimented with different number of dimensions for the features, namely 30, 60 and 120.", "labels": [], "entities": []}, {"text": "Note that these may not be the optimal model configurations for our  set-up.", "labels": [], "entities": []}, {"text": "Exploring several model configurations can be very expensive for DNN LMs, we chose these parameters arbitrarily based on our previous experience with NNLMs.", "labels": [], "entities": []}, {"text": "shows held-out WER as a function of the number of hidden layers for 4-gram DNN LMs with different feature dimensions.", "labels": [], "entities": [{"text": "WER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.993841290473938}]}, {"text": "The same number of hidden units is used for each layer.", "labels": [], "entities": []}, {"text": "WERs are obtained after rescoring ASR lattices with the DNN language models only.", "labels": [], "entities": [{"text": "WERs", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6102343797683716}]}, {"text": "We did not interpolate DNN LMs with the 4-gram baseline language model while exploring the effect of additional layers on DNN LMs.", "labels": [], "entities": []}, {"text": "The performance of the 4-gram baseline language model after rescoring (20.7%) is shown with a dashed line.", "labels": [], "entities": []}, {"text": "h denotes the number of hidden units for each layer and d denotes the feature dimension at the projection layer.", "labels": [], "entities": []}, {"text": "DNN LMs containing only a single hidden layer corresponds to the NNLM.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9551783204078674}]}, {"text": "Note that increasing the dimension of the features improves NNLM performance.", "labels": [], "entities": []}, {"text": "The model with 30 dimensional features has 20.3% WER, while increasing the feature dimension to 120 reduces the WER to 19.6%.", "labels": [], "entities": [{"text": "WER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9970257878303528}, {"text": "WER", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9982439279556274}]}, {"text": "Increasing the feature dimension also shifts the WER curves down for each model.", "labels": [], "entities": [{"text": "WER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9931299090385437}]}, {"text": "More importantly, shows that using deeper networks helps to improve the performance.", "labels": [], "entities": []}, {"text": "The 4-layer DNN LM with 500 hidden units and 30 dimensional features (DNN LM: h = 500 and d = 30) reduces the WER from 20.3% to 19.6%.", "labels": [], "entities": [{"text": "WER", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.8124855160713196}]}, {"text": "For a DNN LM with 500 hidden units and 60 dimensional features (DNN LM: h = 500 and d = 60), the 3-layer model yields the best performance and reduces the WER from 19.9% to 19.4%.", "labels": [], "entities": [{"text": "WER", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.989183783531189}]}, {"text": "For DNN LM with 500 hid-den units and 120 dimensional features (DNN LM: h = 500 and d = 120), the WER curve plateaus after the 3-layer model.", "labels": [], "entities": [{"text": "WER", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9224585294723511}]}, {"text": "For this model the WER reduces from 19.6% to 19.2%.", "labels": [], "entities": [{"text": "WER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9982724189758301}]}, {"text": "We evaluated models that performed best on the held-out set on the test set, measuring both perplexity and WER.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9755685925483704}, {"text": "WER", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9959542751312256}]}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "Note that perplexity and WER for all the models were calculated using the model by itself, without interpolating with a baseline n-gram language model.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9499410390853882}, {"text": "WER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9988076686859131}]}, {"text": "DNN LMs have lower perplexities than their single hidden layer counterparts.", "labels": [], "entities": []}, {"text": "The DNN language models for each configuration yield 0.2-0.4% absolute improvements in WER over NNLMs.", "labels": [], "entities": [{"text": "WER", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.8065682053565979}]}, {"text": "Our best result on the test set is obtained with a 3-layer DNN LM with 500 hidden units and 120 dimensional features.", "labels": [], "entities": []}, {"text": "This model yields 0.4% absolute improvement in WER over the NNLM, and a total of 1.5% absolute improvement in WER over the baseline 4-gram language model.", "labels": [], "entities": [{"text": "WER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9775902628898621}, {"text": "NNLM", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.9714810848236084}, {"text": "WER", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.8514091968536377}]}, {"text": "We also compared our DNN LMs with a model M LM and a recurrent neural network LM (RNNLM) trained on the same data, considered to be current state-of-the-art techniques for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 172, "end_pos": 189, "type": "TASK", "confidence": 0.7198021858930588}]}, {"text": "Model M is a class-based exponential language model which has been shown to yield significant improvements compared to conventional n-gram language models).", "labels": [], "entities": []}, {"text": "Because we used the same set-up as, model M perplexity and WER are reported directly in.", "labels": [], "entities": [{"text": "WER", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9979238510131836}]}, {"text": "Both the 3-layer DNN language model and model M achieve the same WER on the test set; however, the perplexity of model M is lower.", "labels": [], "entities": [{"text": "WER", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9970085024833679}]}, {"text": "The RNNLM is the most similar model to DNN LMs because the RNNLM can be considered to have a deeper architecture thanks to its recurrent connections.", "labels": [], "entities": []}, {"text": "However, the RNNLM proposed in () has a different architecture at the input and output layers than our DNN LMs.", "labels": [], "entities": []}, {"text": "First, RNNLM does not have a projection layer.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.8573693633079529}]}, {"text": "DNN LM has N \u00d7 P parameters in the look-up table and a weight matrix containing (n \u2212 1) \u00d7 P \u00d7 H parameters between the projection and the first hidden layers.", "labels": [], "entities": [{"text": "DNN LM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.92713463306427}]}, {"text": "RNNLM has a weight matrix containing (N + H) \u00d7 H parameters between the input and the hidden layers.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8293155431747437}]}, {"text": "Second, RNNLM uses the full vocabulary (20K words) at the output layer, whereas, DNN LM uses a shortlist containing 10K words.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.7857667207717896}]}, {"text": "Because of the number of output targets in RNNLM, it results in more parameters even with the same number of hidden units with DNN LM.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.8230265378952026}]}, {"text": "Note that the additional hidden layers in DNN LM will introduce extra parameters.", "labels": [], "entities": [{"text": "DNN LM", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.9004996120929718}]}, {"text": "However, these parameters will have a little effect compared to 10, 000 \u00d7 H additional parameters introduced in RNNLM due to the use of the full vocabulary at the output layer.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 112, "end_pos": 117, "type": "DATASET", "confidence": 0.8919191956520081}]}, {"text": "We only compared DNN and RNN language models in terms of perplexity since we cannot directly use RNNLM in our lattice rescoring framework.", "labels": [], "entities": []}, {"text": "We trained two models using the RNNLM toolkit 1 , one with 200 hidden units and one with 500 hidden units.", "labels": [], "entities": [{"text": "RNNLM toolkit 1", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.9430748621622721}]}, {"text": "In order to speedup training, we used 150 classes at the output layer as described in).", "labels": [], "entities": []}, {"text": "These models have 8M and 21M parameters respectively.", "labels": [], "entities": []}, {"text": "RNNLM with 200 hidden units has the same number of parameters with our best DNN LM model, 3-layer DNN LM with 500 hidden units and 120 dimensional features.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "This model results in a lower perplexity than DNN LMs.", "labels": [], "entities": []}, {"text": "RNNLM with 500 hidden units results in the best perplexity in Table 1 but it has much more parameters than DNN LMs.", "labels": [], "entities": []}, {"text": "Note that, RNNLM uses the full history and DNN LM uses only the 3-word context as the history.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.842738687992096}, {"text": "DNN LM", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.8362712264060974}]}, {"text": "Therefore, increasing the n-gram context can help to improve the performance for DNN LMs.", "labels": [], "entities": []}, {"text": "We also tested the performance of NNLM and DNN LM with 500 hidden units and 120-dimensional features after linearly interpolating with the 4-gram baseline language model.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.8921140432357788}]}, {"text": "The interpolation weights were chosen to minimize the perplexity on the held-out set.", "labels": [], "entities": []}, {"text": "After linear interpolation with the 4-gram baseline language model, both the perplexity and WER improve for NNLM and DNN LM.", "labels": [], "entities": [{"text": "WER", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9990839958190918}, {"text": "NNLM", "start_pos": 108, "end_pos": 112, "type": "DATASET", "confidence": 0.8859694004058838}]}, {"text": "However, the gain with 3-layer DNN LM on top of NNLM diminishes.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.8262104392051697}]}, {"text": "One problem with deep neural networks, especially those with more than 2 or 3 hidden layers, is that training can easily get stuck in local 1 http://www.fit.vutbr.cz/\u223cimikolov/rnnlm/ minima, resulting in poor solutions.", "labels": [], "entities": []}, {"text": "Therefore, it maybe important to apply pre-training () instead of randomly initializing the weights.", "labels": [], "entities": []}, {"text": "In this paper we investigate discriminative pre-training for DNN LMs.", "labels": [], "entities": [{"text": "DNN LMs", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.5375364720821381}]}, {"text": "Past work in acoustic modeling has shown that performing discriminative pre-training followed by fine-tuning allows for fewer iterations of fine-tuning and better model performance than generative pre-training followed by fine-tuning).", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7588562667369843}]}, {"text": "In discriminative pre-training, a NNLM (one projection layer, one hidden layer and one output layer) is trained using the cross-entropy criterion.", "labels": [], "entities": []}, {"text": "After one pass through the training data, the output layer weights are discarded and replaced by another randomly initialized hidden layer and output layer.", "labels": [], "entities": []}, {"text": "The initially trained projection and hidden layers are held constant, and discriminative pre-training is performed on the new hidden and output layers.", "labels": [], "entities": []}, {"text": "This discriminative training is performed greedy and layer-wise like generative pre-training.", "labels": [], "entities": []}, {"text": "After pre-training the weights for each layer, we explored two different training (fine-tuning) scenarios.", "labels": [], "entities": []}, {"text": "In the first one, we initialized all the layers, including the output layer, with the pre-trained weights.", "labels": [], "entities": []}, {"text": "In the second one, we initialized all the layers, except the output layer, with the pre-trained weights.", "labels": [], "entities": []}, {"text": "The output layer weights are initialized randomly.", "labels": [], "entities": []}, {"text": "After initializing the weights for each layer, we applied our standard training recipe. and show the held-out WER as a function of the number of hidden layers for the case of no pre-training and the two discriminative pre-training scenarios described above using models with 60-and 120-dimensional features.", "labels": [], "entities": [{"text": "WER", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9959839582443237}]}, {"text": "In the figures, pre-training 1 refers to the first scenario and pre-training 2 refers to the second scenario.", "labels": [], "entities": []}, {"text": "As seen in the figure, pre-training did not give consistent gains for models with different number of hidden layers.", "labels": [], "entities": []}, {"text": "We need to investigate discriminative pretraining and other pre-training strategies further for DNN LMs.", "labels": [], "entities": [{"text": "DNN LMs", "start_pos": 96, "end_pos": 103, "type": "TASK", "confidence": 0.684715747833252}]}], "tableCaptions": [{"text": " Table 1: Test set perplexity and WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.998633086681366}]}, {"text": " Table 2: Test set perplexity and WER. The models have  8M parameters.", "labels": [], "entities": [{"text": "WER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9980095028877258}]}, {"text": " Table 3.  After linear interpolation with the 4-gram baseline  language model, both the perplexity and WER im- prove for NNLM and DNN LM. However, the gain  with 3-layer DNN LM on top of NNLM diminishes.", "labels": [], "entities": [{"text": "WER im-", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9540321429570516}]}, {"text": " Table 3: Test set perplexity and WER with the interpo- lated models.", "labels": [], "entities": [{"text": "WER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9992623925209045}]}]}