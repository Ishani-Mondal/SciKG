{"title": [{"text": "Domain Adaptation of a Dependency Parser with a Class-Class Selectional Preference Model", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6694088280200958}]}], "abstractContent": [{"text": "When porting parsers to anew domain, many of the errors are related to wrong attachment of out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "Since there is no available annotated data to learn the attachment preferences of the target domain words, we attack this problem using a model of selectional preferences based on domain-specific word classes.", "labels": [], "entities": []}, {"text": "Our method uses Latent Dirichlet Allocations (LDA) to learn a domain-specific Selectional Preference model in the target domain using un-annotated data.", "labels": [], "entities": []}, {"text": "The model provides features that model the affinities among pairs of words in the domain.", "labels": [], "entities": []}, {"text": "To incorporate these new features in the parsing model, we adopt the co-training approach and retrain the parser with the selectional preferences features.", "labels": [], "entities": [{"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9727336764335632}]}, {"text": "We apply this method for adapting Easy First, a fast non-directional parser trained on WSJ, to the biomedical domain (Genia Treebank).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.8979455232620239}, {"text": "Genia Treebank)", "start_pos": 118, "end_pos": 133, "type": "DATASET", "confidence": 0.8556857903798422}]}, {"text": "The Selectional Preference features reduce error by 4.5% over the co-training baseline.", "labels": [], "entities": [{"text": "Selectional Preference", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.5232496857643127}, {"text": "error", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9985224604606628}]}], "introductionContent": [{"text": "Dependency parsing captures a useful representation of syntactic structure for information extraction.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.87452432513237}, {"text": "information extraction", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.8716313242912292}]}, {"text": "For example, the Stanford Dependency representation has been used extensively in domain-specific relation extraction tasks such as BioNLP09) and BioNLP11 (Pyysalo,).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7069902569055557}, {"text": "BioNLP11", "start_pos": 145, "end_pos": 153, "type": "DATASET", "confidence": 0.6351354718208313}]}, {"text": "One obstacle to widespread adoption of such syntactic representations is that parsers are generally trained on a specific domain (typically WSJ news data) and it has often been observed that the accuracy of dependency parsers drops significantly when used in a domain other than the training domain.", "labels": [], "entities": [{"text": "WSJ news data", "start_pos": 140, "end_pos": 153, "type": "DATASET", "confidence": 0.9056189258893331}, {"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9988163709640503}]}, {"text": "Domain adaptation for dependency parsing has been explored extensively in the).", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7575392723083496}, {"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8845004439353943}]}, {"text": "The objective in this task is to adapt an existing parser from a source domain in order to achieve high parsing accuracy on a target domain in which no annotated data is available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9430038928985596}]}, {"text": "Common approaches include self-training), using word distribution features) and co-training .) explored a variety of methods for domain adaptation, which consistently showed little improvement and concluded that domain adaptation for dependency parsing is indeed a hard task.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.745042473077774}, {"text": "domain adaptation", "start_pos": 212, "end_pos": 229, "type": "TASK", "confidence": 0.7020392268896103}, {"text": "dependency parsing", "start_pos": 234, "end_pos": 252, "type": "TASK", "confidence": 0.7410299181938171}]}, {"text": "Typically, parsing accuracy drops from 90+% indomain to 80-84% in the target domain.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9787135720252991}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9789606332778931}]}, {"text": "When porting parsers to the target domain, many of the errors are related to wrong attachment of out-ofvocabulary words, i.e., words which were not observed when training on the source domain.", "labels": [], "entities": []}, {"text": "Since there is not sufficient annotated data to learn the attachment preferences of the target domain words, we attack this problem using a model of selectional preferences based on domain-specific word classes.", "labels": [], "entities": []}, {"text": "Selectional preferences (SP) describe the relative affinity of arguments and head of a syntactic relation.", "labels": [], "entities": [{"text": "Selectional preferences (SP)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6391533732414245}]}, {"text": "For example, in the sentence: \"D3 activates receptors in blood cells from patients\", the preposition \"from\" maybe attached to either \"cells\" or \"receptors\".", "labels": [], "entities": []}, {"text": "However, the headword \"cells\" has greater affinity to \"patients\" than the candidate \"receptors\" would have towards \"patients\".", "labels": [], "entities": []}, {"text": "Note that this preference is highly context-specific.", "labels": [], "entities": []}, {"text": "Several methods for learning SP (not in the context of domain adaptation) have been proposed.", "labels": [], "entities": [{"text": "learning SP", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.5891087055206299}]}, {"text": "Commonly, these methods rely on learning semantic classes for arguments and learning the preference of a predicate to a semantic class.", "labels": [], "entities": []}, {"text": "These semantic classes maybe derived from manual knowledge bases such as WordNet or FrameNet, or semantic classes learned from large corpora.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9408389925956726}]}, {"text": "Recently, and both present induction methods of SP of verb-arguments using LDA.", "labels": [], "entities": [{"text": "SP", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9775245189666748}]}, {"text": "extended the LDAbased approach to learning preference for adjectivenoun phrases.", "labels": [], "entities": []}, {"text": "In this work, we tackle the task of domain adaptation by developing a domain-specific SP model.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7959907352924347}]}, {"text": "Our initial observation is that parsers fail on the target domain when trying to attach domain-specific words not seen during training.", "labels": [], "entities": []}, {"text": "We observe as many as 15% of the words are unknown when applying a WSJ-trained parser on Genia and PennBioIE data, compared to only 2.5% in-domain.", "labels": [], "entities": [{"text": "WSJ-trained", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.8663655519485474}, {"text": "PennBioIE data", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.9423146545886993}]}, {"text": "Parsers trained on the source domain cannot learn attachment preferences for such words.", "labels": [], "entities": []}, {"text": "Our motivation is, therefore, to attempt to learn attachment preferences for domain specific words using un-annotated data.", "labels": [], "entities": []}, {"text": "Specifically, we focus on acquiring a domain-specific SP model.", "labels": [], "entities": []}, {"text": "Our approach consists of using the low-accuracy source-domain parser on large quantities of in-domain sentences.", "labels": [], "entities": []}, {"text": "We extract from the resulting parse trees a collection of syntactically related pairs of words.", "labels": [], "entities": []}, {"text": "We then train an LDA model over these pairs of words and derive a domain-specific model of lexical affinities between pairs of words.", "labels": [], "entities": []}, {"text": "We finally re-train a parser model to exploit this domain-specific data.", "labels": [], "entities": []}, {"text": "To this end, we use the approach of co-training, which consists of identifying reliable parse trees in the target domain in an unsupervised manner using an ensemble of two distinct parsers, and extending the annotated training set with these reliable parse trees.", "labels": [], "entities": []}, {"text": "Co-training alone significantly reduces the proportion of unknown words in the re-trained parser -in the extended cotraining dataset, we observe that the unknown words rate drops from 15% to 4.5%.", "labels": [], "entities": []}, {"text": "Data sparseness, however, remains an issue: 1/3 of the domain-specific words added to the model by co-training appear only once in the extended training set, and we observe that many of the attachment errors are concentrated in a few syntactic configurations (e.g., head(V or N)-preppobj, N-N or head(N)-Adj).", "labels": [], "entities": []}, {"text": "We extend co-training by introducing our SP model, which is class-based and specific to these difficult syntactic configurations.", "labels": [], "entities": []}, {"text": "Our method reduces error in the Genia Treebank (Tateisi,) by 3.5% over cotraining.", "labels": [], "entities": [{"text": "error", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9958721995353699}, {"text": "Genia Treebank (Tateisi", "start_pos": 32, "end_pos": 55, "type": "DATASET", "confidence": 0.8567668199539185}]}, {"text": "Introducing additional distributional lexical features (Brown clusters learned in-domain), further reduces error to a total 4.5% reduction.", "labels": [], "entities": [{"text": "error", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.9952065348625183}]}, {"text": "Overall, our parser achieves an accuracy of 83.6% UAS on the Genia domain without annotated data in this domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9994555115699768}, {"text": "UAS", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9927335977554321}, {"text": "Genia domain", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.8873583674430847}]}], "datasetContent": [{"text": "We use a number of baselines for the adaptation task.", "labels": [], "entities": []}, {"text": "Three parsers were evaluated on the target domain: Easy-First, MST second order and MALT arc-eager with a polynomial kernel.", "labels": [], "entities": [{"text": "MALT", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.942467987537384}]}, {"text": "We report UAS scores of trees of length < 40 without punctuation.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.5332469940185547}]}, {"text": "The first baseline setting for each parser is the model trained on WSJ sections 2-21.", "labels": [], "entities": [{"text": "WSJ sections 2-21", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.9137770136197408}]}, {"text": "The second baseline we report is co-training using WSJ 2-21 combined with the 21K full agreement parse trees extracted from Medline, but without new features..", "labels": [], "entities": [{"text": "WSJ 2-21", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.9359637498855591}, {"text": "Medline", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.8970207571983337}]}, {"text": "Accuracy for different parser settings on Genia test set.", "labels": [], "entities": [{"text": "Genia test set", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.8040196299552917}]}], "tableCaptions": [{"text": " Table 2. Statistics for the training data of the SP model.", "labels": [], "entities": []}, {"text": " Table 3. Accuracy for different parser settings on Genia test set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988155364990234}, {"text": "Genia test set", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.8430224855740865}]}, {"text": " Table 4. Accuracy of parsers on PennBioIE Treebank.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9932698607444763}, {"text": "PennBioIE Treebank", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.9956327974796295}]}]}