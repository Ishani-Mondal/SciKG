{"title": [{"text": "A Regularized Compression Method To Unsupervised Word Segmentation", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.6589435189962387}]}], "abstractContent": [{"text": "Languages are constantly evolving through their users due to the need to communicate more efficiently.", "labels": [], "entities": []}, {"text": "Under this hypothesis, we formulate unsupervised word segmentation as a regularized compression process.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7434110939502716}]}, {"text": "We reduce this process to an optimization problem, and propose a greedy inclusion solution.", "labels": [], "entities": []}, {"text": "Preliminary test results on the Bernstein-Ratner corpus and Bakeoff-2005 show that the our method is comparable to the state-of-the-art in terms of effectiveness and efficiency.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised word segmentation has been a popular research subject due to its close connection to language acquisition.", "labels": [], "entities": [{"text": "Unsupervised word segmentation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6444844007492065}, {"text": "language acquisition", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.6987262070178986}]}, {"text": "It has attracted researchers from different communities, including linguistics, cognitive science, and machine learning, to investigate how human beings develop and harness their languages, and, more importantly, how knowledge is acquired.", "labels": [], "entities": []}, {"text": "In this paper we propose anew formulation to the unsupervised word segmentation problem.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7061892002820969}]}, {"text": "Our idea is based on the observation that language evolves because of the need to reduce communication efforts.", "labels": [], "entities": []}, {"text": "For instance, new terminologies, abbreviations, and slang that carry complex semantics which cannot be efficiently expressed in the original languages are invented so that concepts can be conveyed.", "labels": [], "entities": []}, {"text": "Such an evolution, we hypothesize, is limited to the extent where the evolved vocabulary exhibits similar complexity as the original one, in light of reducing the extra cost to pickup the new language.", "labels": [], "entities": []}, {"text": "This process is realized as an optimization problem called regularized compression, which gets this name from its analogy to text compression.", "labels": [], "entities": [{"text": "regularized compression", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.7382276356220245}, {"text": "text compression", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.7167975455522537}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We briefly summarize related work on unsupervised word segmentation in Section 2.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7200550734996796}]}, {"text": "In Section 3, we introduce the proposed formulation.", "labels": [], "entities": []}, {"text": "The iterative algorithm and other technical details for solving the optimization problem are covered in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we describe the evaluation procedure and discuss the experimental results.", "labels": [], "entities": []}, {"text": "Finally, we present concluding remarks in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted the first experiment on the BernsteinRatner corpus, a standard benchmark for English phonetic segmentation.", "labels": [], "entities": [{"text": "BernsteinRatner corpus", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.8305732011795044}, {"text": "English phonetic segmentation", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.5948393642902374}]}, {"text": "We used the version derived by Michael Brent, which is made available in the CHILDES database).", "labels": [], "entities": [{"text": "CHILDES database", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.9642034769058228}]}, {"text": "The corpus comprises 9,790 utterances, which amount to 95,809 words in total.", "labels": [], "entities": []}, {"text": "Its relatively small size allows experimentation with the most computational-intensive Bayesian models.", "labels": [], "entities": []}, {"text": "Parameter estimation for the proposed method has been a challenge due to the lack of appropriate development data.", "labels": [], "entities": [{"text": "Parameter estimation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7175606191158295}]}, {"text": "We first obtained a rough estimate for the compression rate \u03c1 via human inspection into the first 10 lines of the corpus (these 10 lines were later excluded in evaluation) and used that estimate to setup the termination condition.", "labels": [], "entities": [{"text": "compression rate \u03c1", "start_pos": 43, "end_pos": 61, "type": "METRIC", "confidence": 0.816460649172465}]}, {"text": "Since the first Informally speaking, when \u03b1 < \u02dc H(c).", "labels": [], "entities": []}, {"text": "The analysis is not covered in this preliminary study.", "labels": [], "entities": []}, {"text": "10 lines are too small to reveal any useful segmentation cues other than the word/token ration of interest, we considered this setting (\"almost unsupervised\") a reasonable compromise.", "labels": [], "entities": []}, {"text": "In this experiment, \u03c1 is set to 0.37; the trade-off parameter \u03b1 is set to 8.3, optimized using MDL principle in a two-pass grid search (the first pass over {1, 2, . .", "labels": [], "entities": []}, {"text": ", 20} and the second over {8.0, 8.1, . .", "labels": [], "entities": []}, {"text": "A detailed performance result for the proposed method is described in.", "labels": [], "entities": []}, {"text": "A reference run for HDP is included for comparison.", "labels": [], "entities": []}, {"text": "The proposed method achieved satisfactory result at word and boundary levels.", "labels": [], "entities": []}, {"text": "Nevertheless, low type-level numbers (in contrast to those for HDP) together with high boundary recall suggested that we might have experienced over-segmentation.", "labels": [], "entities": [{"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9600343108177185}]}, {"text": "covers the same result with less details in order to compare with other reference methods.", "labels": [], "entities": []}, {"text": "All the reported measures for reference methods are directly taken from the literature.", "labels": [], "entities": []}, {"text": "The result shows that AG achieved the best performance in Fmeasure (other metrics are not reported), surpassing all the other methods by a large margin (10 percent).", "labels": [], "entities": [{"text": "AG", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.745800793170929}, {"text": "Fmeasure", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.8947901129722595}]}, {"text": "Among the other methods, our method paired with MDL achieved comparable performance as the others in precision; it does slightly better than the others in recall (5 percent) and F-measure (2.5 percent).", "labels": [], "entities": [{"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9996246099472046}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9992308616638184}, {"text": "F-measure", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.994314968585968}]}, {"text": "Furthermore, our algorithm also seems to be competitive in terms of computational efficiency.", "labels": [], "entities": []}, {"text": "On this benchmark it demanded only minimal memory low as 4MB and finished the segmentation run in 0.9 second, even less than the reported running time for both MDL-based algorithms.; the other rows represent the proposed method tested on different test corpora.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 78, "end_pos": 90, "type": "TASK", "confidence": 0.9635946750640869}]}, {"text": "Columns indicates performance metrics, which correspond to precision, recall, and F-measure at word (P/R/F), boundary (BP/BR/BF), and type (TP/TR/TF) levels.: A short summary about the subsets in the Bakeoff-2005 dataset.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.999125063419342}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9980317950248718}, {"text": "F-measure", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9978669881820679}, {"text": "word (P/R/F), boundary (BP/BR/BF)", "start_pos": 95, "end_pos": 128, "type": "METRIC", "confidence": 0.6257090480888591}, {"text": "Bakeoff-2005 dataset", "start_pos": 200, "end_pos": 220, "type": "DATASET", "confidence": 0.813188910484314}]}, {"text": "The size of each subset is given in number of words (W) and number of unique word types (T).", "labels": [], "entities": []}, {"text": "The second benchmark that we adopted is the SIGHAN for Chinese word segmentation.", "labels": [], "entities": [{"text": "SIGHAN", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.8505123853683472}, {"text": "Chinese word segmentation", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.5660696824391683}]}, {"text": "The corpus has four separates subsets prepared by different research groups; it is among the largest word segmentation benchmarks available.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.732603907585144}]}, {"text": "briefly summarizes the statistics regarding this dataset.", "labels": [], "entities": []}, {"text": "We decided to compare our algorithm with description length gain (DLG), for that it seems to deliver best segmentation accuracy among other unsupervised approaches ever reported on this benchmark (.", "labels": [], "entities": [{"text": "description length gain (DLG)", "start_pos": 41, "end_pos": 70, "type": "METRIC", "confidence": 0.8074037730693817}, {"text": "segmentation", "start_pos": 106, "end_pos": 118, "type": "TASK", "confidence": 0.9496622681617737}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.8575707674026489}]}, {"text": "Since the reported values for DLG were obtained on another closed dataset), we followed a similar experimental setup as suggested in the literature (: We compared both methods only on the training sets for the common subsets CityU and MSR.", "labels": [], "entities": [{"text": "CityU", "start_pos": 225, "end_pos": 230, "type": "DATASET", "confidence": 0.9620267748832703}]}, {"text": "Note that this experimental setup departed slightly from that of in that all the comparisons were strictly made on the training sets.", "labels": [], "entities": []}, {"text": "The approach is more straightforward than the suggested sampling-based method.", "labels": [], "entities": []}, {"text": "Other baseline methods that we considered include HDP, Ent-MDL, and BVE-MDL, for their representativeness in segmentation performance and  ease of implementation.", "labels": [], "entities": [{"text": "BVE-MDL", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.8042556643486023}]}, {"text": "The HDP implementation we used is a modified version of the offical HDP package ; we patched the package to make it work with Unicode-encoded Chinese characters.", "labels": [], "entities": []}, {"text": "For Ent-MDL and BVE-MDL, we used the software package 4 distributed by.", "labels": [], "entities": [{"text": "BVE-MDL", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.5336999893188477}]}, {"text": "We estimated the parameters using the AS training set as the development data.", "labels": [], "entities": [{"text": "AS training set", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.7398940722147623}]}, {"text": "We set \u03b1 to 6 based on a grid search.", "labels": [], "entities": []}, {"text": "The expected compression rate \u03c1 that we learned from the development data is 0.65.", "labels": [], "entities": [{"text": "compression rate \u03c1", "start_pos": 13, "end_pos": 31, "type": "METRIC", "confidence": 0.9689644773801168}]}, {"text": "In, we give a detailed listing of various performance measures for the proposed method.", "labels": [], "entities": []}, {"text": "Segmentation performance seems moderate at both word and boundary levels.", "labels": [], "entities": []}, {"text": "Nevertheless, high type precision and low type recall on both CityU and MSR training corpora signaled that our algorithm failed to discover most word types.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9611325263977051}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9360084533691406}, {"text": "CityU and MSR training corpora", "start_pos": 62, "end_pos": 92, "type": "DATASET", "confidence": 0.7144538700580597}]}, {"text": "This issue, we suspect, was caused by exclusion of low-frequency candidate bigrams, as discussed in Section 4.3.", "labels": [], "entities": []}, {"text": "summarizes the result for word segmentation conducted on the CityU and MSR subsets of.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.756178468465805}, {"text": "CityU and MSR subsets", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.7338535487651825}]}, {"text": "Due to practical computational limits, we were notable to run HDP and BVE-MDL on any complete subset.", "labels": [], "entities": []}, {"text": "The result shows that our  algorithm outperforms DLG by 8 to 10 percents in F-measure, while Ent-MDL still performs slightly better, achieving the top performance among all the experimental runs on both subsets.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.8333292603492737}]}, {"text": "To compare with HDP, we conducted another test run on top of a random sample of 1,000 lines from each subset.", "labels": [], "entities": []}, {"text": "We chose 1,000 lines because HDP can easily consume more than 4GB of main memory on any larger sample.", "labels": [], "entities": []}, {"text": "We adopted standard settings for HDP: \u03b1 0 = 3, 000, \u03b1 1 = 300, and p b = 0.2.", "labels": [], "entities": []}, {"text": "In each trial run, we ran the Gibbs sampler for 20,000 iterations using simulated annealing ).", "labels": [], "entities": []}, {"text": "We obtained 10 samples from the Gibbs sampler and used the average performance in comparison.", "labels": [], "entities": []}, {"text": "It took slightly more than 50 hours to collect one trial run on one subset.", "labels": [], "entities": []}, {"text": "The evaluation result is summarized in.", "labels": [], "entities": []}, {"text": "We ran our algorithm to the desired compression ratio r = 0.65 on this small sample.", "labels": [], "entities": [{"text": "compression ratio r", "start_pos": 36, "end_pos": 55, "type": "METRIC", "confidence": 0.9540610114733378}]}, {"text": "The result  shows that the performance of regularized compression is inferior to that of HDP by 9 to 13 percents in F-measure for both sets.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9978315234184265}]}, {"text": "To investigate why, we looked into the segmentation output.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9697665572166443}]}, {"text": "We observed that, in the regularized compression output, most of the punctuation marks were incorrectly aligned to their neighboring words, owing to the short of frequency counts in this small sample.", "labels": [], "entities": []}, {"text": "The HDP, however, does not seem to suffer from this issue.", "labels": [], "entities": [{"text": "HDP", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9606595635414124}]}, {"text": "We devised a simple post-processing step, in which each punctuation mark was forced segmented from the surrounding text.", "labels": [], "entities": []}, {"text": "Another outside test was conducted to see how well the algorithm works using heuristics derived from minimal domain knowledge.", "labels": [], "entities": []}, {"text": "The additional run is denoted as RC/punc.", "labels": [], "entities": []}, {"text": "The result is shown in.", "labels": [], "entities": []}, {"text": "From the result, we found that the combined approach works slightly better than HDP in one corpus, but not in the other.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance evaluation on the Bernstein-Ratner  corpus. The reported values for each method indicate  word precision, recall, F-measure and running time, re- spectively. The boldface value for each column indicates  the top performer under the corresponding metric.", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.7271349430084229}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9989135265350342}, {"text": "F-measure", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9964228272438049}]}, {"text": " Table 1: Performance evaluation for the proposed method across different test corpora. The first row indicates a  reference HDP run (Goldwater et", "labels": [], "entities": []}, {"text": " Table 4: Performance evaluation on the common training  subsets in the Bakeoff-2005 and Bakeoff-2006 datasets.  The reported values are token F-measure. The boldface  value in each column indicates the top performer for the  corresponding set.", "labels": [], "entities": [{"text": "Bakeoff-2006 datasets", "start_pos": 89, "end_pos": 110, "type": "DATASET", "confidence": 0.781115859746933}, {"text": "F-measure", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9569587111473083}]}, {"text": " Table 5: Performance evaluation on two random samples  from the common sets (CityU and MSR subsets) in the  Bakeoff-2005 and Bakeoff-2006 datasets.", "labels": [], "entities": [{"text": "Bakeoff-2006 datasets", "start_pos": 126, "end_pos": 147, "type": "DATASET", "confidence": 0.832473486661911}]}]}