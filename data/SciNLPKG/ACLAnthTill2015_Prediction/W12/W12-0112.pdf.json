{"title": [{"text": "Were the clocks striking or surprising? Using WSD to improve MT performance", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9930588006973267}]}], "abstractContent": [{"text": "We report on a series of experiments aimed at improving the machine translation of ambiguous lexical items by using wordnet-based unsupervised Word Sense Disambiguation (WSD) and comparing its results to three MT systems.", "labels": [], "entities": [{"text": "machine translation of ambiguous lexical items", "start_pos": 60, "end_pos": 106, "type": "TASK", "confidence": 0.8548482408126196}, {"text": "wordnet-based unsupervised Word Sense Disambiguation (WSD)", "start_pos": 116, "end_pos": 174, "type": "TASK", "confidence": 0.6675657853484154}]}, {"text": "Our experiments are performed for the English-Slovene language pair using UKB, a freely available graph-based word sense disambiguation system.", "labels": [], "entities": [{"text": "UKB", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.9593816995620728}]}, {"text": "Results are evaluated in three ways: a manual evaluation of WSD performance from MT perspective, an analysis of agreement between the WSD-proposed equivalent and those suggested by the three systems, and finally by computing BLEU, NIST and METEOR scores for all translation versions.", "labels": [], "entities": [{"text": "WSD", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9767261743545532}, {"text": "BLEU", "start_pos": 225, "end_pos": 229, "type": "METRIC", "confidence": 0.9981836676597595}, {"text": "NIST", "start_pos": 231, "end_pos": 235, "type": "DATASET", "confidence": 0.7912206053733826}, {"text": "METEOR", "start_pos": 240, "end_pos": 246, "type": "METRIC", "confidence": 0.9387516975402832}]}, {"text": "Our results show that WSD performs with a MT-relevant precision of 71% and that 21% of sense-related MT errors could be prevented by using unsuper-vised WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9378777146339417}, {"text": "MT-relevant", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9241425395011902}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.7624973058700562}, {"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.8966899514198303}]}], "introductionContent": [{"text": "Ambiguity continues to be a tough nut to crack in MT.", "labels": [], "entities": [{"text": "Ambiguity", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7235313653945923}, {"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9680876135826111}]}, {"text": "In most known languages certain lexical items can refer to more than a single concept, meaning that MT systems need to choose between several translation equivalents representing different senses of the source word.", "labels": [], "entities": [{"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9695769548416138}]}, {"text": "Wrong choices often result in grave translation errors, as words often refer to several completely unrelated concepts.", "labels": [], "entities": []}, {"text": "The adjective striking can mean beautiful, surprising; delivering a hard blow or indicating a certain time, and the noun \"course\" can be something we give, take, teach or eat.", "labels": [], "entities": []}, {"text": "Our aim was to assess the performance of three MT systems for the English-Slovene language pair and to see whether wordnet-based Word Sense Disambiguation (WSD) could improve performance and assist in avoiding grave sense-related translation errors.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9805775880813599}, {"text": "wordnet-based Word Sense Disambiguation (WSD)", "start_pos": 115, "end_pos": 160, "type": "TASK", "confidence": 0.6610251324517387}, {"text": "avoiding grave sense-related translation", "start_pos": 201, "end_pos": 241, "type": "TASK", "confidence": 0.6033389568328857}]}, {"text": "For WSD we use UKB (Agirre and Soroa 2009), a graph-based algorithm that uses wordnet) and computes the probability of each sense of a polysemous word by taking into account the senses of context words.", "labels": [], "entities": [{"text": "WSD", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9298586249351501}, {"text": "UKB", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.8634725213050842}]}, {"text": "In our experiment we use Orwell's notorious novel 1984 as the source and its translation into Slovene by Alenka Puhar as the reference translation.", "labels": [], "entities": [{"text": "Orwell's notorious novel 1984", "start_pos": 25, "end_pos": 54, "type": "DATASET", "confidence": 0.8926441311836243}]}, {"text": "We then disambiguate the English source with UKB, assign each disambiguated English word a Slovene equivalent from sloWNet) and compare these with the equivalents proposed by Google, Bing and Presis.", "labels": [], "entities": [{"text": "UKB", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9767907857894897}]}, {"text": "Results are evaluated in several ways: \u2022 By manually evaluating WSD performance from the MT perspective, \u2022 By analysing the agreement between each of the MT systems and the UKB/wordnet-derived translation, \u2022 By comparing BLEU, NIST and ME-TEOR scores achieved with each translation version.", "labels": [], "entities": [{"text": "WSD", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9681429266929626}, {"text": "UKB/wordnet-derived translation", "start_pos": 173, "end_pos": 204, "type": "DATASET", "confidence": 0.8779608905315399}, {"text": "BLEU", "start_pos": 221, "end_pos": 225, "type": "METRIC", "confidence": 0.9981212019920349}, {"text": "NIST", "start_pos": 227, "end_pos": 231, "type": "DATASET", "confidence": 0.7682048082351685}, {"text": "ME-TEOR", "start_pos": 236, "end_pos": 243, "type": "METRIC", "confidence": 0.8244838714599609}]}, {"text": "Our results show that the ad hoc WSD strategies used by the evaluated MT systems can definitely be improved by a proper WSD algorithm, but also that wordnet is not the ideal semantic resource to help resolve translation dilemmas, mainly due to its fine sense granularity.", "labels": [], "entities": [{"text": "translation dilemmas", "start_pos": 208, "end_pos": 228, "type": "TASK", "confidence": 0.8044761121273041}]}], "datasetContent": [{"text": "The number of meanings a word can have, the degree of translation equivalence or the quality of the target text are all extremely disputable and vague notions.", "labels": [], "entities": []}, {"text": "For this reason we wished to evaluate our results from as many angles as possible, both manually and automatically.", "labels": [], "entities": []}, {"text": "Firstly, we were interested in the performance of the UKB disambiguation tool in the context of MT.", "labels": [], "entities": [{"text": "UKB disambiguation tool", "start_pos": 54, "end_pos": 77, "type": "DATASET", "confidence": 0.8293664256731669}, {"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9856680631637573}]}, {"text": "Since UKB uses wordnet as a sense inventory, the algorithm assigns a probability to each sense of a lexical item according to its context in an unsupervised way.", "labels": [], "entities": [{"text": "UKB", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.9838682413101196}]}, {"text": "The precision of UKB for unsupervised WSD is reported at around 58% for all words and around 72% for nouns, but of course these figures measure the number of cases where the algorithm selected the correct wordnet synset from a relatively fine-grained network of possible senses (Agirre and Soroa 2009).", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.998997151851654}, {"text": "UKB", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.8962859511375427}]}, {"text": "We adjusted the evaluation task to an MT scenario by manually checking 200 disambiguated words and their suggested translation equivalents, and if the equivalent was acceptable we counted it among the positive instances regardless of the selected sense.", "labels": [], "entities": [{"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9839701056480408}]}, {"text": "For example, the English word breast has four senses in wordnet: (1) the upper frontal part of a human chest, (2) one of the two soft milk-secreting glands of a woman,   The precision of WSD using this relaxed criterion was 71%, with 6% so-called borderline cases.", "labels": [], "entities": [{"text": "precision", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9995515942573547}, {"text": "WSD", "start_pos": 187, "end_pos": 190, "type": "TASK", "confidence": 0.8968714475631714}]}, {"text": "These include cases where the equivalent was semantically correct but had the wrong part of speech (eg. glass door -> *steklo instead of steklen).", "labels": [], "entities": []}, {"text": "Finally, we wanted to see how the WSD/wordnet-based translation compares with the three MT systems using the BLEU, NIST and METEOR scores.", "labels": [], "entities": [{"text": "WSD/wordnet-based translation", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.6189089864492416}, {"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9178754687309265}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.99753737449646}, {"text": "NIST", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.8144776225090027}, {"text": "METEOR", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9205285310745239}]}, {"text": "For the purposes of this comparison we pre-processed all five versions of our corpus -original, reference translation, Presis, Google and Bing translation -by lemmatization, removal of all function words, removal of sentences where the alignment was not 1:1, and finally by removal of the sentences which contained lexical items for which there was no equivalent in sloWNet.", "labels": [], "entities": []}, {"text": "We then generated the sixth version by translating all ambiguous words with sloWNet (see Section 3), and for the words not included in the English wordnet we used four alternative translation strategies; a general bilingual dictionary (dict), wiktionary (wikt), a word-alignment lexicon (align) and amending untranslated words to the target language version (amend).: Evaluation with metrics shows the results of automatic evaluation; the corpus consisted of 2,428 segments.", "labels": [], "entities": []}, {"text": "We can see that our generated version using disambiguated equivalents does not outperform any of the MT systems on any metric, except once when the WSD-align version outperforms Presis on the NIST score and comes fairly close to the Bing score.", "labels": [], "entities": [{"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.8546095490455627}, {"text": "Presis", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.9903592467308044}, {"text": "NIST score", "start_pos": 192, "end_pos": 202, "type": "DATASET", "confidence": 0.9278605878353119}]}], "tableCaptions": [{"text": " Table 4: Evaluation with metrics", "labels": [], "entities": []}]}