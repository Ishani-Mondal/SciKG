{"title": [{"text": "On Hierarchical Re-ordering and Permutation Parsing for Phrase-based Decoding", "labels": [], "entities": []}], "abstractContent": [{"text": "The addition of a deterministic permutation parser can provide valuable hierarchical information to a phrase-based statistical machine translation (PBSMT) system.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (PBSMT)", "start_pos": 102, "end_pos": 154, "type": "TASK", "confidence": 0.7047095213617597}]}, {"text": "Permutation parsers have been used to implement hierarchical reordering models (Galley and Manning , 2008) and to enforce inversion trans-duction grammar (ITG) constraints (Feng et al., 2010).", "labels": [], "entities": []}, {"text": "We present a number of theoretical results regarding the use of permutation parsers in PBSMT.", "labels": [], "entities": []}, {"text": "In particular, we show that an existing ITG constraint (Zens et al., 2004) does not prevent all non-ITG permutations, and we demonstrate that the hierarchical reordering model can produce analyses during decoding that are inconsistent with analyses made during training.", "labels": [], "entities": []}, {"text": "Experimentally, we verify the utility of hierarchical reordering , and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 62, "end_pos": 106, "type": "TASK", "confidence": 0.5985851585865021}]}, {"text": "However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars.", "labels": [], "entities": []}, {"text": "Therefore, re-ordering must be modeled and constrained explicitly.", "labels": [], "entities": []}, {"text": "Movement can be modeled with a distortion penalty or lexicalized re-ordering probabilities (, while decoding can be constrained by distortion limits or by mimicking the restrictions of inversion transduction grammars).", "labels": [], "entities": []}, {"text": "Recently, we have begun to see deterministic permutation parsers incorporated into phrase-based decoders.", "labels": [], "entities": []}, {"text": "These efficient parsers analyze the sequence of phrases used to produce the target, and assemble them into a hierarchical translation history that can be used to inform re-ordering decisions.", "labels": [], "entities": []}, {"text": "Thus far, they have been used to enable a hierarchical re-ordering model, or HRM (, as well as an ITG constraint.", "labels": [], "entities": []}, {"text": "We discuss each of these techniques in turn, and then explore the implications of ITG violations on hierarchical re-ordering.", "labels": [], "entities": []}, {"text": "We present one experimental and four theoretical contributions.", "labels": [], "entities": []}, {"text": "Examining the HRM alone, we present an improved algorithm for extracting HRM statistics, reducing the complexity of Galley and Manning's solution from O(n 4 ) to O(n 2 ).", "labels": [], "entities": []}, {"text": "Examining ITG constraints alone, we demonstrate that the three-stack constraint of can be reduced to one augmented stack, and we show that another phrase-based ITG constraint () actually allows some ITG violations to pass.", "labels": [], "entities": []}, {"text": "Finally, we show that in the presence of ITG violations, the original HRM can fail to produce orientations that are consistent with the orientations collected during training.", "labels": [], "entities": []}, {"text": "We propose three HRM variants to address this situation, including an approximate HRM that requires no permutation parser, and compare them experimentally.", "labels": [], "entities": []}, {"text": "The variants perform similarly to the original in terms of BLEU score, but differently in terms of how they permute the source sentence.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9862480163574219}]}, {"text": "We begin by establishing some notation.", "labels": [], "entities": []}, {"text": "We view the phrase-based translation process as producing a sequence of source/target blocks in their target order.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.690070778131485}]}, {"text": "For the purposes of this paper, we disregard the lexical content of these blocks, treating blocks spanning the same source segment as equivalent.", "labels": [], "entities": []}, {"text": "The block [s i , ti ] indicates that the source segment w s i +1 , . .", "labels": [], "entities": []}, {"text": ", wt i was translated as a unit to produce the i th target phrase.", "labels": [], "entities": []}, {"text": "We index between words; therefore, a block's length in tokens is t \u2212 s, and fora sentence of length n, 0 \u2264 s \u2264 t \u2264 n.", "labels": [], "entities": []}, {"text": "Empty blocks have s = t, and are used only in special cases.", "labels": [], "entities": []}, {"text": "Two blocks [s i\u22121 , t i\u22121 ] and [s i , ti ] are adjacent iff t i\u22121 = s i or ti = s i\u22121 . Note that we concern ourselves only with adjacency in the source.", "labels": [], "entities": []}, {"text": "Adjacency in the target is assumed, as the blocks are in target order.", "labels": [], "entities": []}, {"text": "shows an example block sequence, where adjacency corresponds to cases where block corners touch.", "labels": [], "entities": []}, {"text": "In the shift-reduce permutation parser we describe below, the parsing state is encoded as a stack of these same blocks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the LRM, the HRM and the three HRM variants suggested in Section 4 on a Chinese-toEnglish translation task.", "labels": [], "entities": [{"text": "Chinese-toEnglish translation task", "start_pos": 83, "end_pos": 117, "type": "TASK", "confidence": 0.6725210249423981}]}, {"text": "We measure the impact on translation quality in terms of BLEU score), as well as the impact on permutation  complexity, as measured by the largest k required to k-reduce the translations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9836327731609344}]}], "tableCaptions": [{"text": " Table 3: Chinese-to-English translation results, comparing the LRM and 4 HRM variants: the original 2-reducing  parser, the coverage vector approximation, the *-reducing parser, and an ITG-constrained decoder.", "labels": [], "entities": []}]}