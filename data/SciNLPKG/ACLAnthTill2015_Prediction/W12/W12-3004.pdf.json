{"title": [{"text": "Automatic Evaluation of Relation Extraction Systems on Large-scale", "labels": [], "entities": [{"text": "Automatic Evaluation of Relation Extraction", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.6005225062370301}]}], "abstractContent": [{"text": "The extraction of relations between named entities from natural language text is a long-standing challenge in information extraction, especially in large-scale.", "labels": [], "entities": [{"text": "extraction of relations between named entities from natural language text", "start_pos": 4, "end_pos": 77, "type": "TASK", "confidence": 0.8776201963424682}, {"text": "information extraction", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.8475275933742523}]}, {"text": "A major challenge for the advancement of this research field has been the lack of meaningful evaluation frameworks based on realistic-sized corpora.", "labels": [], "entities": []}, {"text": "In this paper we propose a framework for large-scale evaluation of relation extraction systems based on an automatic annotator that uses a public online database and a large web corpus.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7747589945793152}]}], "introductionContent": [{"text": "It is envisioned that in the future, the main source of structured data to build knowledge bases will be automatically extracted from natural language sources (.", "labels": [], "entities": []}, {"text": "One promising technique towards this goal is Relation Extraction (RE): the task of identifying relations among named entities (e.g., people, organizations and geo-political entities) from natural language text.", "labels": [], "entities": [{"text": "Relation Extraction (RE)", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.8325491786003113}, {"text": "identifying relations among named entities (e.g., people, organizations and geo-political entities) from natural language text", "start_pos": 83, "end_pos": 209, "type": "TASK", "confidence": 0.6994989828059548}]}, {"text": "Traditionally, RE systems required each target relation to be given as input along with a set of examples.", "labels": [], "entities": [{"text": "RE", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9749470353126526}]}, {"text": "A new paradigm termed Open RE ( has recently emerged to cope with the scenario where the number of target relations is too large or even unknown.", "labels": [], "entities": [{"text": "Open RE", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.5303903073072433}]}, {"text": "Open RE systems try to extract every relation described in the text, as opposed to focusing on a few relations ().", "labels": [], "entities": []}, {"text": "One challenge in advancing the state-of-the-art in open RE (or any other field for that matter) is having meaningful and fair ways of evaluating and comparing different systems.", "labels": [], "entities": [{"text": "RE", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.691676139831543}]}, {"text": "This is particularly difficult when it comes to evaluating the recall of such systems, as that requires one to enumerate all relations described in a corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9641008973121643}]}, {"text": "In order to scale, a method for evaluation of open RE must have no human involvement.", "labels": [], "entities": [{"text": "RE", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.6897895932197571}]}, {"text": "One way to automatically produce a benchmark is to use an existing database as ground truth . Although a step in the right direction, this approach limits the evaluation to those relations that are present in the database.", "labels": [], "entities": []}, {"text": "Another shortcoming is that the database does not provide \"true\" recall, since it often contains many more facts (for the relations it holds) than described in the corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9797409772872925}]}, {"text": "Measuring true precision and recall In this paper we discuss an automatic method to estimate true precision and recall of open RE systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9392036199569702}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9966020584106445}, {"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.8398388028144836}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9753157496452332}]}, {"text": "We propose the use of an automatic annotator: a system capable of verifying whether or not a fact was correctly extracted.", "labels": [], "entities": []}, {"text": "This is done by leveraging external sources of data and text, which are not available to the systems being evaluated.", "labels": [], "entities": []}, {"text": "The external database used in this work is Freebase, a curated online database maintained by an active community.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.9755985736846924}]}, {"text": "In addition to the external database, our automatic annotator leverages Pointwise Mutual Information (PMI)) from the web.", "labels": [], "entities": []}, {"text": "PMI has been widely accepted to measure the confidence score of an extraction).", "labels": [], "entities": [{"text": "confidence score", "start_pos": 44, "end_pos": 60, "type": "METRIC", "confidence": 0.9394558966159821}]}, {"text": "We show that 19 PMI is also useful to evaluate systems automatically.", "labels": [], "entities": [{"text": "PMI", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.4670083820819855}]}, {"text": "Using our method, we compare two state-of-theart open RE systems,) and SONEX (, applied to the same corpus, namely the New York Times Corpus).", "labels": [], "entities": [{"text": "SONEX", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9193124175071716}, {"text": "New York Times Corpus", "start_pos": 119, "end_pos": 140, "type": "DATASET", "confidence": 0.8300046622753143}]}], "datasetContent": [{"text": "We now describe how our method measures both true precision and true recall, using a database and the web (as a large external text corpus).", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9401029348373413}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9650779366493225}]}, {"text": "A fact is a triple f i = e 1 , r, e 2 associating entities e 1 and e 2 via relation r.", "labels": [], "entities": []}, {"text": "We measure precision by assessing how many of the facts produced by the system have been correctly extracted.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9993124008178711}]}, {"text": "A fact is said to be correct if (1) we can find the fact in the database or (2) we can detect a statistically significant association between e 1 , e 2 and r on the web.", "labels": [], "entities": []}, {"text": "To measure recall, we estimate the size of the ground truth (i.e., the collection of all facts described in the corpus).", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9870645999908447}]}], "tableCaptions": [{"text": " Table 2: The size of all regions for ReVerb and SONEX,  in thousands of facts.", "labels": [], "entities": [{"text": "ReVerb", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.8927582502365112}]}, {"text": " Table 3: Performance results for ReVerb and SONEX.", "labels": [], "entities": [{"text": "ReVerb", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.8716861605644226}, {"text": "SONEX", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.6147210001945496}]}]}