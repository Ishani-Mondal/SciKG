{"title": [{"text": "Reinforcement Learning of Question-Answering Dialogue Policies for Virtual Museum Guides", "labels": [], "entities": [{"text": "Reinforcement Learning of Question-Answering Dialogue Policies", "start_pos": 0, "end_pos": 62, "type": "TASK", "confidence": 0.8832988142967224}]}], "abstractContent": [{"text": "We use Reinforcement Learning (RL) to learn question-answering dialogue policies fora real-world application.", "labels": [], "entities": []}, {"text": "We analyze a corpus of interactions of museum visitors with two virtual characters that serve as guides at the Museum of Science in Boston, in order to build a realistic model of user behavior when interacting with these characters.", "labels": [], "entities": []}, {"text": "A simulated user is built based on this model and used for learning the dialogue policy of the virtual characters using RL.", "labels": [], "entities": []}, {"text": "Our learned policy out-performs two baselines (including the original dialogue policy that was used for collecting the corpus) in a simulation setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the last 10 years Reinforcement Learning (RL) has attracted much attention in the dialogue community, to the extent that we can now consider RL as the state-of-the-art in statistical dialogue management.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.8639697790145874}, {"text": "statistical dialogue management", "start_pos": 174, "end_pos": 205, "type": "TASK", "confidence": 0.7765472730000814}]}, {"text": "RL is used in the framework of Markov Decision Processes (MDPs) or Partially Observable Markov Decision Processes (POMDPs).", "labels": [], "entities": [{"text": "RL", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7714694142341614}]}, {"text": "In this paradigm dialogue moves transition between dialogue states and rewards are given at the end of a successful dialogue.", "labels": [], "entities": []}, {"text": "The goal of RL is to learn a dialogue policy, i.e. the optimal action that the system should take at each possible dialogue state.", "labels": [], "entities": [{"text": "RL", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9525272250175476}]}, {"text": "Typically rewards depend on the domain and can include factors such as task completion, dialogue length, and user satisfaction.", "labels": [], "entities": []}, {"text": "Traditional RL algorithms require on the order * This work was done when the first author was a visiting researcher at USC/ICT. of thousands of dialogues to achieve good performance.", "labels": [], "entities": [{"text": "RL", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9530295729637146}, {"text": "USC/ICT.", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.7028615276018778}]}, {"text": "Because it is very difficult to collect such a large number of dialogues with real users, instead, simulated users (SUs), i.e. models that simulate the behavior of real users, are employed ().", "labels": [], "entities": []}, {"text": "Through the interaction between the system and the SUs thousands of dialogues can be generated and used for learning.", "labels": [], "entities": []}, {"text": "A good SU should be able to replicate the behavior of areal user in the same dialogue context).", "labels": [], "entities": []}, {"text": "Most research in RL for dialogue management has been done in the framework of slot-filling applications (, largely ignoring other types of dialogue.", "labels": [], "entities": [{"text": "RL", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9844279885292053}, {"text": "dialogue management", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8296486437320709}]}, {"text": "In this paper we focus on the problem of learning dialogue policies for question-answering characters.", "labels": [], "entities": []}, {"text": "With question-answering systems (or characters), the natural language understanding task is to retrieve the best response to a user initiative, and the main dialogue policy decision is whether to provide this best response or some other kind of move (e.g. a request for repair, clarification, or topic change), when the best answer does not seem to be good enough.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.7277554273605347}]}, {"text": "Note that often in the literature the term questionanswering is used for slot-filling dialogue systems as well, in the sense that the user asks some questions, for example, about restaurants in a particular area, and the system answers by providing a list of options, for example, restaurants.", "labels": [], "entities": []}, {"text": "We use the term \"question-answering\" for systems where user questions can be independent of one another (followup questions are possible though) and do not have the objective of reducing the search space and retrieving results from a database of e.g. restaurants, flights, etc.", "labels": [], "entities": []}, {"text": "Thus examples of question-answering characters can be virtual interviewees (that can answer questions, e.g. about an incident), virtual scientists (that can answer general science-related questions), and so forth.", "labels": [], "entities": []}, {"text": "For our experiments we use a corpus) of interactions of real users with two virtual characters, the Twins, that serve as guides at the Museum of Science in Boston (.", "labels": [], "entities": []}, {"text": "The role of these virtual characters is to entertain and educate the museum visitors.", "labels": [], "entities": []}, {"text": "They can answer queries about themselves and their technology, generally about science, as well as questions related to the exhibits of the museum.", "labels": [], "entities": []}, {"text": "An example interaction between a museum visitor and the Twins is shown in.", "labels": [], "entities": [{"text": "Twins", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.9216614961624146}]}, {"text": "The dialogue policy of the Twins was arbitrarily hand-crafted (see section 7 for details) and many other policies are possible (including Baseline 2, presented in section 7, and taking more advantage of question topics and context).", "labels": [], "entities": []}, {"text": "We propose to use RL for optimizing the system's response generation.", "labels": [], "entities": []}, {"text": "This is a real-world application for which RL appears to bean appropriate method.", "labels": [], "entities": []}, {"text": "Although there are similarities between questionanswering and slot-filling dialogues there are also a number of differences, such as the reward function and the behavior of the users.", "labels": [], "entities": []}, {"text": "As discussed later in detail, in question-answering the users have a number of questions that they are planning to ask (stock of queries), which can be increased or decreased depending not only on whether they received the information that they wanted but also on how satisfied they are with the interaction.", "labels": [], "entities": []}, {"text": "The system has to plan ahead in order to maximize the number of successful responses that it provides to user queries.", "labels": [], "entities": []}, {"text": "At the same time it needs to avoid providing incorrect or incoherent responses so that the user does not give up the interaction.", "labels": [], "entities": []}, {"text": "One of the challenges of our task is to define an appropriate reward function.", "labels": [], "entities": []}, {"text": "Unlike slot-filling dialogues, it is not clear what makes an interaction with a question-answering system successful.", "labels": [], "entities": [{"text": "slot-filling dialogues", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.8450882732868195}]}, {"text": "A second challenge is that in a museum setting it is not clear what constitutes a dialogue session.", "labels": [], "entities": []}, {"text": "Often two or more users alternate in asking questions, which further complicates the problem of defining a good reward function.", "labels": [], "entities": []}, {"text": "A third challenge is that the domain is not well defined, i.e. users do not know in advance what the system is capable of (what kind of questions the characters can answer).", "labels": [], "entities": []}, {"text": "Moreover, there Grace: And I'm Grace.", "labels": [], "entities": []}, {"text": "We're your Virtual Museum Guides.", "labels": [], "entities": []}, {"text": "With your help, we can suggest exhibits that will get you thinking!", "labels": [], "entities": []}, {"text": "Or answer questions about things you may have seen here.", "labels": [], "entities": []}, {"text": "Ada: What do you want to learn about?", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare our learned policy with two baselines.", "labels": [], "entities": []}, {"text": "The first baseline, Baseline 1, is the dialogue policy that is used by our system that is currently installed at the Museum of Science in Boston.", "labels": [], "entities": []}, {"text": "Baseline 1 selects the best ASR result (i.e. the result with the highest confidence score) out of the results with the 3 different AMs (child, male, and female), and forwards this result to the NPCEditor to retrieve the system's response.", "labels": [], "entities": [{"text": "NPCEditor", "start_pos": 194, "end_pos": 203, "type": "DATASET", "confidence": 0.9478169083595276}]}, {"text": "If the NPCEditor score is higher than an emprically set pre-defined threshold (see (  for details), then the system presents the retrieved response, otherwise it presents an off-topic prompt.", "labels": [], "entities": []}, {"text": "The system presents these off-topic prompts in a fixed order.", "labels": [], "entities": []}, {"text": "First, OT1, then OT2, and then OT3.", "labels": [], "entities": [{"text": "OT1", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.6250002384185791}, {"text": "OT2", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.7452363967895508}, {"text": "OT3", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.8891087174415588}]}, {"text": "We also have Baseline 2, which forwards all 3 ASR results to the NPCEditor (using child, male, and female AMs).", "labels": [], "entities": [{"text": "ASR", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.8282787203788757}, {"text": "NPCEditor", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.935045063495636}]}, {"text": "Then the NPCEditor retrieves 3 results, one for each one of the 3 ASR results, and selects the retrieved result with the highest score.", "labels": [], "entities": []}, {"text": "Again if this score is higher than a threshold, the system will present this result, otherwise it will present an off-topic prompt.", "labels": [], "entities": []}, {"text": "Each policy interacts with the SU for 10,000 dialogue sessions and we calculate the average accumulated reward for each dialogue.", "labels": [], "entities": []}, {"text": "In we can see our results for Reward functions 1 and 2 respectively.", "labels": [], "entities": [{"text": "Reward", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.7025570273399353}]}, {"text": "In both cases the learned policy outperforms both baselines.", "labels": [], "entities": []}, {"text": "For both reward functions the most predictive feature is the ASR confidence score when combined with the NPCEditor's retrieval score and the previous system action.", "labels": [], "entities": [{"text": "ASR confidence score", "start_pos": 61, "end_pos": 81, "type": "METRIC", "confidence": 0.9412855704625448}, {"text": "NPCEditor's retrieval score", "start_pos": 105, "end_pos": 132, "type": "METRIC", "confidence": 0.608539529144764}]}, {"text": "Also, for both reward functions the second best feature is \"voting\" when combined with the retrieval score and the previous system action.", "labels": [], "entities": []}, {"text": "In we can see how often the learned policy, which is based on Reward function 1 using all features, selects each one of the 10 system actions (200,000 system turns in total).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Reward function 1.", "labels": [], "entities": [{"text": "Reward", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.6841815710067749}]}, {"text": " Table 4: Results with reward function 1. The values in  parentheses for Baselines 1 and 2 are the rewards when  the NPCEditor does not use the pre-defined threshold.", "labels": [], "entities": [{"text": "NPCEditor", "start_pos": 117, "end_pos": 126, "type": "DATASET", "confidence": 0.8972070813179016}]}, {"text": " Table 5: Results with reward function 2. The values in  parentheses for Baselines 1 and 2 are the rewards when  the NPCEditor does not use the pre-defined threshold.", "labels": [], "entities": [{"text": "NPCEditor", "start_pos": 117, "end_pos": 126, "type": "DATASET", "confidence": 0.8919722437858582}]}, {"text": " Table 6: Frequency of the system actions of the learned  policy that is based on Reward function 1 using all fea- tures.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9695206880569458}]}]}