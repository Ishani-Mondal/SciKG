{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 216-224, NUS at the HOO 2012 Shared Task", "labels": [], "entities": [{"text": "NUS at the HOO 2012 Shared Task", "start_pos": 100, "end_pos": 131, "type": "DATASET", "confidence": 0.924584014075143}]}], "abstractContent": [{"text": "This paper describes the submission of the National University of Singapore (NUS) to the HOO 2012 shared task.", "labels": [], "entities": [{"text": "National University of Singapore (NUS) to the HOO 2012 shared task", "start_pos": 43, "end_pos": 109, "type": "DATASET", "confidence": 0.7221674919128418}]}, {"text": "Our system uses a pipeline of confidence-weighted linear classi-fiers to correct determiner and preposition errors.", "labels": [], "entities": []}, {"text": "Our system achieves the highest correction F 1 score on the official test set among all 14 participating teams, based on gold-standard edits both before and after revision.", "labels": [], "entities": [{"text": "correction F 1 score", "start_pos": 32, "end_pos": 52, "type": "METRIC", "confidence": 0.9370566308498383}]}], "introductionContent": [{"text": "Grammatical error correction is the task of automatically detecting and correcting erroneous word usage and ill-formed grammatical constructions in text.", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8192303975423177}, {"text": "automatically detecting and correcting erroneous word usage and ill-formed grammatical constructions in text", "start_pos": 44, "end_pos": 152, "type": "TASK", "confidence": 0.7618071368107429}]}, {"text": "Determiner and preposition errors are the two most prominent types of errors made by non-native speakers of English.", "labels": [], "entities": [{"text": "Determiner and preposition errors", "start_pos": 0, "end_pos": 33, "type": "METRIC", "confidence": 0.6795214116573334}]}, {"text": "Although there has been much work on automatic correction of determiner and preposition errors over the last few years, it has so far been impossible to directly compare results because different teams have evaluated on different data sets.", "labels": [], "entities": [{"text": "automatic correction of determiner and preposition errors", "start_pos": 37, "end_pos": 94, "type": "TASK", "confidence": 0.7392148716109139}]}, {"text": "The HOO 2012 shared task evaluates grammatical error correction systems for determiner and preposition errors.", "labels": [], "entities": [{"text": "HOO 2012 shared task", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.78993920981884}, {"text": "grammatical error correction", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.5685417652130127}]}, {"text": "Participants are provided with a set of documents written by non-native speakers of English.", "labels": [], "entities": []}, {"text": "The task is to automatically detect and correct determiner and preposition errors and produce a set of corrections (called edits).", "labels": [], "entities": []}, {"text": "Evaluation is done by computing precision, recall, and F 1 score between the system edits and a manually created set of gold-standard edits.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9995781779289246}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9995194673538208}, {"text": "F 1 score", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.991244912147522}]}, {"text": "The details of the HOO 2012 shared task are described in the official overview paper (.", "labels": [], "entities": [{"text": "HOO 2012 shared task", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.7383521497249603}]}, {"text": "In this paper, we describe the system submission from the National University of Singapore (NUS).", "labels": [], "entities": [{"text": "National University of Singapore (NUS)", "start_pos": 58, "end_pos": 96, "type": "DATASET", "confidence": 0.8739762817110334}]}, {"text": "Our system treats determiner and preposition correction as classification problems.", "labels": [], "entities": [{"text": "determiner and preposition correction", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.8161005079746246}]}, {"text": "We use confidenceweighted linear classifiers to predict the correct word from a confusion set of possible correction options.", "labels": [], "entities": []}, {"text": "Separate classifiers are built for determiner errors, preposition replacement errors, and preposition insertion and deletion errors.", "labels": [], "entities": [{"text": "preposition insertion and deletion", "start_pos": 90, "end_pos": 124, "type": "TASK", "confidence": 0.7393230870366096}]}, {"text": "The classifiers are combined into a pipeline of correction steps to form an end-to-end error correction system.", "labels": [], "entities": []}, {"text": "Our system achieves the highest correction F 1 score on the official test set among all 14 participating teams, based on gold-standard edits both before and after revision.", "labels": [], "entities": [{"text": "correction F 1 score", "start_pos": 32, "end_pos": 52, "type": "METRIC", "confidence": 0.9370566308498383}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section presents our error correction system.", "labels": [], "entities": [{"text": "error correction", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.7752690613269806}]}, {"text": "Section 3 describes the features.", "labels": [], "entities": []}, {"text": "Section 4 presents experimental results.", "labels": [], "entities": []}, {"text": "Section 5 contains further discussion.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report experimental results of our system on two different data sets: a held-out test split of the HOO 2012 training data, and the official HOO 2012 test set.", "labels": [], "entities": [{"text": "HOO 2012 training data", "start_pos": 119, "end_pos": 141, "type": "DATASET", "confidence": 0.9558641910552979}, {"text": "HOO 2012 test set", "start_pos": 160, "end_pos": 177, "type": "DATASET", "confidence": 0.9653715193271637}]}, {"text": "Evaluation is performed by computing detection, recognition, and correction F 1 score between the set of system edits and the set of gold-standard edits as defined in the HOO 2012 overview paper (: Overall precision, recall, and F 1 score on the HOO-DEVTEST data after determiner correction (Det), replacement preposition correction (RT), and missing and unwanted preposition correction (MT/UT).", "labels": [], "entities": [{"text": "correction F 1 score", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.9077068716287613}, {"text": "HOO 2012 overview paper", "start_pos": 171, "end_pos": 194, "type": "DATASET", "confidence": 0.9412173628807068}, {"text": "precision", "start_pos": 206, "end_pos": 215, "type": "METRIC", "confidence": 0.9993657469749451}, {"text": "recall", "start_pos": 217, "end_pos": 223, "type": "METRIC", "confidence": 0.9989743232727051}, {"text": "F 1 score", "start_pos": 229, "end_pos": 238, "type": "METRIC", "confidence": 0.9909671545028687}, {"text": "HOO-DEVTEST data", "start_pos": 246, "end_pos": 262, "type": "DATASET", "confidence": 0.9504061937332153}, {"text": "determiner correction (Det)", "start_pos": 269, "end_pos": 296, "type": "METRIC", "confidence": 0.8342187881469727}, {"text": "replacement preposition correction (RT)", "start_pos": 298, "end_pos": 337, "type": "METRIC", "confidence": 0.5709036290645599}, {"text": "unwanted preposition correction (MT/UT)", "start_pos": 355, "end_pos": 394, "type": "METRIC", "confidence": 0.8001180775463581}]}, {"text": "detection scores due to space limitations.", "labels": [], "entities": [{"text": "detection scores", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9211882650852203}]}, {"text": "Evaluation on the official test set is performed with respect to two different gold standards: the original gold standard from Cambridge University Press and a revised version which was created in the HOO 2012 shared task in response to change requests from participating teams.", "labels": [], "entities": [{"text": "Cambridge University Press", "start_pos": 127, "end_pos": 153, "type": "DATASET", "confidence": 0.930959939956665}, {"text": "HOO 2012 shared task", "start_pos": 201, "end_pos": 221, "type": "DATASET", "confidence": 0.7579432129859924}]}, {"text": "All scores are computed with the official scorer.", "labels": [], "entities": []}, {"text": "The official gold-standard edits are given in character offsets, while our system internally works with token offsets.", "labels": [], "entities": []}, {"text": "Therefore, all token offsets are automatically mapped back to character offsets before we submit our system edits.", "labels": [], "entities": []}, {"text": "We only submitted one run of our system.: Individual scores for each error type on the HOO-DEVTEST data.", "labels": [], "entities": [{"text": "HOO-DEVTEST data", "start_pos": 87, "end_pos": 103, "type": "DATASET", "confidence": 0.8711543381214142}]}, {"text": "show the overall precision, recall and F 1 score of our system after each processing step on the held-out HOO-DEVTEST set and the official test set, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9997898936271667}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9996017813682556}, {"text": "F 1 score", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9908341964085897}, {"text": "HOO-DEVTEST set", "start_pos": 106, "end_pos": 121, "type": "DATASET", "confidence": 0.7803561091423035}]}, {"text": "All numbers are shown in percentages.", "labels": [], "entities": []}, {"text": "We note that each processing step improves the overall performance.", "labels": [], "entities": []}, {"text": "The final F 1 correction score on the official test set is 28.70% before revision and 37.83% after revision, which are the highest scores achieved by any participating team.", "labels": [], "entities": [{"text": "F 1 correction score", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9373822808265686}, {"text": "official test set", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.7334457139174143}, {"text": "revision", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9353840351104736}]}, {"text": "show individual precision, recall, and F 1 score", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9832873344421387}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9998303651809692}, {"text": "F 1", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9890575706958771}]}], "tableCaptions": [{"text": " Table 4: Features for unwanted preposition correc- tion. Example: \"The cat went to home.\"", "labels": [], "entities": []}, {"text": " Table 5: Overview of the data sets.", "labels": [], "entities": []}, {"text": " Table 7: Individual scores for each error type on the  HOO-DEVTEST data.", "labels": [], "entities": [{"text": "HOO-DEVTEST data", "start_pos": 56, "end_pos": 72, "type": "DATASET", "confidence": 0.8496752381324768}]}, {"text": " Table 8: Overall precision, recall, and F 1 score on the HOO-TEST data after determiner correction (Det),  replacement preposition correction (RT), and missing and unwanted preposition correction (MT/UT).", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9996780157089233}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9997015595436096}, {"text": "F 1 score", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9848885734875997}, {"text": "HOO-TEST data", "start_pos": 58, "end_pos": 71, "type": "DATASET", "confidence": 0.7498874962329865}, {"text": "determiner correction (Det)", "start_pos": 78, "end_pos": 105, "type": "METRIC", "confidence": 0.9428041577339172}, {"text": "replacement preposition correction (RT)", "start_pos": 108, "end_pos": 147, "type": "METRIC", "confidence": 0.8102174699306488}, {"text": "unwanted preposition correction (MT/UT)", "start_pos": 165, "end_pos": 204, "type": "METRIC", "confidence": 0.7955239787697792}]}, {"text": " Table 9: Individual scores for each error type on the HOO-TEST data.", "labels": [], "entities": [{"text": "HOO-TEST data", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.8809045553207397}]}]}