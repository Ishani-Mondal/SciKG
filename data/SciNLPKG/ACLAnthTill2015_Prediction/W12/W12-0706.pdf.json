{"title": [{"text": "Robust Induction of Parts-of-Speech in Child-Directed Language by Co-Clustering of Words and Contexts", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce Conflict-Driven Co-Clustering, a novel algorithm for data co-clustering, and apply it to the problem of inducing parts-of-speech in a corpus of child-directed spoken English.", "labels": [], "entities": []}, {"text": "Co-clustering is preferable to unidimensional clustering as it takes into account both item and context ambiguity.", "labels": [], "entities": []}, {"text": "We show that the categorization performance of the algorithm is comparable with the co-clustering algorithm of Leibbrandt and Powers (2008), but out-performs that algorithm in robustly pruning less-useful clusters and merging them into categories strongly corresponding to the three main open classes of English.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of unsupervised part-of-speech induction has received considerable attention in computational linguistics (for a recent comparison of several influential models, see.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.7127334177494049}]}, {"text": "A common approach is to estimate the parameters of a generative model given the natural language data, with the model usually a variant of a Hidden Markov Model (e.g.).", "labels": [], "entities": []}, {"text": "These models are often evaluated on corpora of formal, written English, such as the Penn Treebank, rather than on natural, spoken language, and typically the aim of these studies is to improve the state-of-the-art of POS induction using various techniques from machine learning, with an implicit focus on devising techniques that can be used in practical applications.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.9950882792472839}, {"text": "POS induction", "start_pos": 217, "end_pos": 230, "type": "TASK", "confidence": 0.970610648393631}]}, {"text": "In the current paper, on the other hand, our focus is on part-of-speech induction mechanisms that children might use when learning their first language.", "labels": [], "entities": []}, {"text": "Hence, we are interested in models that are motivated by psychological considerations, rather than by a more abstract mathematical or statistical grounding.", "labels": [], "entities": []}, {"text": "In language acquisition research, atypical approach to part-of-speech induction is to make use of clustering.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.7315491437911987}, {"text": "part-of-speech induction", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.7494454383850098}]}, {"text": "We will review this work and argue for the particular utility of two-mode clustering or co-clustering approaches, before presenting two novel coclustering techniques and evaluating their performance in part-of-speech tagging on a corpus of child-directed English.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 202, "end_pos": 224, "type": "TASK", "confidence": 0.6951356679201126}]}], "datasetContent": [{"text": "The CDCC algorithm was applied to a corpus of child-directed speech, after which individual tokens of word-frame co-occurrences were categorized into one of the co-clusters produced by the algorithm, as described below.", "labels": [], "entities": []}, {"text": "Results are reported in terms of standard measures of precision, recall and F-score, with random baselines in parentheses.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9996418952941895}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9996504783630371}, {"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.999122679233551}]}, {"text": "These measures were calculated, as is customary in unsupervised categorization, by a pair counting approach that constructs a confusion matrix based on whether pairs of elements are assigned to the same category in the gold-standard, and also in the clustering model (see e.g. Mintz,).", "labels": [], "entities": []}, {"text": "Because of several well-known shortcomings of precision and recall (e.g., we also report the Informedness measure, which corresponds to the probability that the predictions made by the algorithm are informed, in the sense of making correct use of information.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9992836117744446}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9981440305709839}, {"text": "Informedness measure", "start_pos": 93, "end_pos": 113, "type": "METRIC", "confidence": 0.9903638660907745}]}, {"text": "Informedness can thus be expressed as Recall fora particular cluster, discounted by the proportion of all non-category items that occur in that cluster.", "labels": [], "entities": [{"text": "Recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9959467053413391}]}, {"text": "Informedness is equivalent to the well-known delta-P formula expressing association strength inhuman associative learning (e.g..", "labels": [], "entities": [{"text": "Informedness", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8986431360244751}]}, {"text": "For a supervised classification problem, with a table of arbitrary dimensions m\u00d7m, Informedness is calculated for the 2\u00d72 contingency table of each category in turn, and the Informedness values for all categories are combined in a weighted sum, where the weight for each category is the proportion of word tokens assigned to that category by the algorithm (i.e. the algorithm\"s bias to assign instances to the category).", "labels": [], "entities": [{"text": "Informedness", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.9808518290519714}, {"text": "Informedness", "start_pos": 174, "end_pos": 186, "type": "METRIC", "confidence": 0.9404460787773132}]}, {"text": "In unsupervised cases, it is not obvious how to associate clusters with gold-standard categories.", "labels": [], "entities": []}, {"text": "In this case, weighted Informedness values are calculated for every possible 1-to-1 mapping between gold standard categories and clusters, and the highest of these Informedness values is selected.", "labels": [], "entities": [{"text": "Informedness", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.888879656791687}]}, {"text": "For evaluation, we made use of only those tokens that were assigned to one of the three major open-class categories (nouns, verbs and adjectives).", "labels": [], "entities": []}], "tableCaptions": []}