{"title": [{"text": "WinkTalk: a demonstration of a multimodal speech synthesis platform linking facial expressions to expressive synthetic voices\u00c9va voices\u00b4voices\u00c9va", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a demonstration of the WinkTalk system, which is a speech synthesis platform using expressive synthetic voices.", "labels": [], "entities": []}, {"text": "With the help of a webcamera and facial expression analysis, the system allows the user to control the expressive features of the synthetic speech fora particular utterance with their facial expressions.", "labels": [], "entities": []}, {"text": "Based on a person-alised mapping between three expressive synthetic voices and the users facial expressions, the system selects a voice that matches their face at the moment of sending a message.", "labels": [], "entities": []}, {"text": "The WinkTalk system is an early research prototype that aims to demonstrate that facial expressions can be used as a more intuitive control over expressive speech synthesis than manual selection of voice types, thereby contributing to an improved communication experience for users of speech generating devices.", "labels": [], "entities": []}], "introductionContent": [{"text": "During a human verbal communication process, expressive features of face and speech are congruent, operating in a synchronised manner, ().", "labels": [], "entities": []}, {"text": "Facial expressions and expressive speech styles often help to convey the emotional intent of the speaker that is only partially contained in the words.", "labels": [], "entities": []}, {"text": "The application described in this paper aims to make use of this synchrony and applies facial expressions as areal time volitional control over the expressive features of synthetic utterance productions of augmented speakers.", "labels": [], "entities": []}, {"text": "The WinkTalk system is currently a research prototype in progress, operating on a personal computer equipped with a webcamera.", "labels": [], "entities": []}, {"text": "The goal of the system is to respond to the need of integrated multimodality in speech generating devices of users of augmentative and alternative communication 1 (AAC) applications.", "labels": [], "entities": []}, {"text": "Being able to correctly link facial expression to synthetic speech output is a step forward to a more intuitive way of controlling the expressiveness of synthetic speech.", "labels": [], "entities": []}, {"text": "The approach can be considered novel, as the authors are not aware of another system using facial expressions to control expressive TTS.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}