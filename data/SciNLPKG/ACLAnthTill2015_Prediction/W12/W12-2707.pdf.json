{"title": [{"text": "NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 50-58, Revisiting the Case for Explicit Syntactic Information in Language Models", "labels": [], "entities": [{"text": "NAACL-HLT 2012 Workshop", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.961713949839274}]}], "abstractContent": [{"text": "Statistical language models used in deployed systems for speech recognition, machine translation and other human language technologies are almost exclusively n-gram models.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7303192168474197}, {"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.754229336977005}]}, {"text": "They are regarded as linguistically na\u00a8\u0131vena\u00a8\u0131ve, but estimating them from any amount of text, large or small, is straightforward.", "labels": [], "entities": []}, {"text": "Furthermore , they have doggedly matched or out-performed numerous competing proposals for syntactically well-motivated models.", "labels": [], "entities": []}, {"text": "This unusual resilience of n-grams, as well as their weaknesses, are examined here.", "labels": [], "entities": []}, {"text": "It is demonstrated that n-grams are good word-predictors, even linguistically speaking, in a large majority of word-positions, and it is suggested that to improve over n-grams, one must explore syntax-aware (or other) language models that focus on positions where n-grams are weak.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models (LM) are crucial components in tasks that require the generation of coherent natural language text, such as automatic speech recognition (ASR) and machine translation (MT).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 124, "end_pos": 158, "type": "TASK", "confidence": 0.8104156951109568}, {"text": "machine translation (MT)", "start_pos": 163, "end_pos": 187, "type": "TASK", "confidence": 0.8424839973449707}]}, {"text": "Most language models rely on simple n-gram statistics and a wide range of smoothing and backoff techniques.", "labels": [], "entities": []}, {"text": "State-of-the-art ASR systems use (n \u2212 1)-gram equivalence classification for the language model (which result in an n-gram language model).", "labels": [], "entities": [{"text": "ASR", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9815595149993896}]}, {"text": "While simple and efficient, it is widely believed that limiting the context to only the (n \u2212 1) most recent words ignores the structure of language, and several statistical frameworks have been proposed to incorporate the \"syntactic structure of language back into language modeling.\"", "labels": [], "entities": []}, {"text": "Yet despite considerable effort on including longer-dependency features, such as syntax, n-gram language models remain the dominant technique in automatic speech recognition and machine translation (MT) systems.", "labels": [], "entities": [{"text": "automatic speech recognition and machine translation (MT)", "start_pos": 145, "end_pos": 202, "type": "TASK", "confidence": 0.7112723092238108}]}, {"text": "While intuition suggests syntax is important, the continued dominance of n-gram models could indicate otherwise.", "labels": [], "entities": []}, {"text": "While no one would dispute that syntax informs word choice, perhaps sufficient information aggregated across a large corpus is available in the local context for n-gram models to perform well even without syntax.", "labels": [], "entities": []}, {"text": "To clearly demonstrate the utility of syntactic information and the deficiency of n-gram models, we empirically show that n-gram LMs lose significant predictive power in positions where the syntactic relation spans beyond the n-gram context.", "labels": [], "entities": []}, {"text": "This clearly shows a performance gap in n-gram LMs that could be bridged by syntax.", "labels": [], "entities": []}, {"text": "As a candidate syntactic LM we consider the Structured Language Model (SLM)), one of the first successful attempts to build a statistical language model based on syntactic information.", "labels": [], "entities": []}, {"text": "The SLM assigns a joint probability P (W, T ) to every word sequence W and every possible binary parse tree T , where T 's terminals are words W with part-of-speech (POS) tags, and its internal nodes comprise non-terminal labels and lexical \"heads\" of phrases.", "labels": [], "entities": []}, {"text": "Other approaches include using the exposed headwords in a maximumentropy based LM), us-ing exposed headwords from full-sentence parse tree in a neural network based LM (, and the use of syntactic features in discriminative training).", "labels": [], "entities": []}, {"text": "We show that the long-dependencies modeled by SLM, significantly improves the predictive power of the LM, specially in positions where the syntactic relation is beyond the reach of regular n-gram models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we explain our experimental evidence for supporting the hypotheses stated above.", "labels": [], "entities": []}, {"text": "First, Section 3.1 presents our experimental design where we use a statistical constituent parser to identify two types of word positions in a test data, namely positions where the headword syntactic relation spans beyond recent words in the history and positions where the headword syntactic relation is within the n-gram window.", "labels": [], "entities": []}, {"text": "The performance of an n-gram LM is measured on both types of positions to show substantial difference in the predictive power of the LM in those positions.", "labels": [], "entities": []}, {"text": "Section 3.3 describes the results and analysis of our experiments which supports our hypotheses.", "labels": [], "entities": []}, {"text": "Throughout the rest of the paper, we refer to a position where the headword syntactic relation reaches further back than the n-gram context as a syntactically-distant position and other type of positions is referred to as a syntactically-local position.", "labels": [], "entities": []}, {"text": "We train a dependency SLM for two different tasks, namely Broadcast News (BN) and Wall Street Journal (WSJ).", "labels": [], "entities": [{"text": "Broadcast News (BN)", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.8971557855606079}, {"text": "Wall Street Journal (WSJ)", "start_pos": 82, "end_pos": 107, "type": "DATASET", "confidence": 0.9390433530012766}]}, {"text": "Unlike Section 3.2, where we swept through multiple training sets of multiple sizes, training the SLM is computationally intensive.", "labels": [], "entities": [{"text": "SLM", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9535362124443054}]}, {"text": "Yet, useful insights maybe gained from the 40M word case.", "labels": [], "entities": []}, {"text": "So we choose the source of text most suitable for each task, and proceed as follows.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexity of 3-gram and 4-gram LMs on syntac- tically local (M) and syntactically distant (N ) positions  in the test set for different training data sizes, showing the  sustained higher perplexity in distant v/s local positions.", "labels": [], "entities": []}, {"text": " Table 2 shows the perplexity results for BN and WSJ  experiments, respectively. It is evident that the 4- gram baseline for BN is stronger than the 40M case  of Table 1. Yet, the interpolated SLM significantly  improves over the 4-gram LM, as it does for WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 256, "end_pos": 259, "type": "DATASET", "confidence": 0.85779869556427}]}]}