{"title": [{"text": "Natural Language Descriptions of Visual Scenes: Corpus Generation and Analysis", "labels": [], "entities": [{"text": "Natural Language Descriptions of Visual Scenes", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7594357232252756}, {"text": "Corpus Generation and Analysis", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.8002044856548309}]}], "abstractContent": [{"text": "As video contents continue to expand, it is increasingly important to properly annotate videos for effective search, mining and retrieval purposes.", "labels": [], "entities": []}, {"text": "While the idea of annotating images with keywords is relatively well explored, work is still needed for annotating videos with natural language to improve the quality of video search.", "labels": [], "entities": []}, {"text": "The focus of this work is to present a video dataset with natural language descriptions which is a step ahead of keywords based tagging.", "labels": [], "entities": []}, {"text": "We describe our initial experiences with a corpus consisting of descriptions for video segments crafted from TREC video data.", "labels": [], "entities": [{"text": "TREC video data", "start_pos": 109, "end_pos": 124, "type": "DATASET", "confidence": 0.7913867632548014}]}, {"text": "Analysis of the descriptions created by 13 annotators presents insights into humans' interests and thoughts on videos.", "labels": [], "entities": []}, {"text": "Such resource can also be used to evaluate automatic natural language generation systems for video.", "labels": [], "entities": [{"text": "automatic natural language generation", "start_pos": 43, "end_pos": 80, "type": "TASK", "confidence": 0.6614444106817245}]}], "introductionContent": [{"text": "This paper presents our experiences in manually constructing a corpus, consisting of natural language descriptions of video segments crafted from a small subset of TREC video 1 data.", "labels": [], "entities": [{"text": "TREC video 1 data", "start_pos": 164, "end_pos": 181, "type": "DATASET", "confidence": 0.7138569429516792}]}, {"text": "Ina broad sense the task can be considered one form of machine translation as it translates video streams into textual descriptions.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7439501285552979}]}, {"text": "To date the number of studies in this field is relatively small partially because of lack of appropriate dataset for such task.", "labels": [], "entities": []}, {"text": "Another obstacle maybe inherently larger variation for descriptions that can be produced for videos than a conventional translation from one language to another.", "labels": [], "entities": []}, {"text": "Indeed humans are very subjective while annotating video www-nlpir.nist.gov/projects/trecvid/ streams, e.g., two humans may produce quite different descriptions for the same video.", "labels": [], "entities": []}, {"text": "Based on these descriptions we are interested to identify the most important and frequent high level features (HLFs); they maybe 'keywords', such as a particular object and its position/moves, used fora semantic indexing task in video retrieval.", "labels": [], "entities": []}, {"text": "Mostly HLFs are related to humans, objects, their moves and properties (e.g., gender, emotion and action) (.", "labels": [], "entities": []}, {"text": "In this paper we present these HLFs in the form of ontologies and provides two hierarchical structures of important concepts -one most relevant for humans and their actions, and another for nonhuman objects.", "labels": [], "entities": []}, {"text": "The similarity of video descriptions is quantified using a bag of word model.", "labels": [], "entities": []}, {"text": "The notion of sequence of events in a video was quantified using the order preserving sequence alignment algorithm (longest common subsequence).", "labels": [], "entities": []}, {"text": "This corpus may also be used for evaluation of automatic natural language description systems.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Average overlapping similarity scores within 13 hand annotations. For each of unigram, bigram and  trigram, scores are calculated for seven categories in two conditions: (A) stop words removed and Porter stemmer  applied, but synonyms NOT replaced; (B) stop words NOT removed, but Porter stemmer applied and synonyms  replaced.", "labels": [], "entities": []}, {"text": " Table 2: Similarity scores based on the longest com- mon subsequence (LCS) in three conditions: scores  without any preprocessing (raw), scores after synonym  replacement (synonym), and scores by keyword com- parison (keyword). For keyword comparison, verbs  and nouns were presented as keywords after stemming  and removing stop words.", "labels": [], "entities": []}, {"text": " Table 3: Results for supervised classification using the  tf-idf features.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6844699382781982}]}]}