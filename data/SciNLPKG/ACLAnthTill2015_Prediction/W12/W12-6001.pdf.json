{"title": [{"text": "Simple or Complex? Classifying the Question by the Answer Complexity", "labels": [], "entities": []}], "abstractContent": [{"text": "Simple questions require small snippets of text as the answers whereas complex questions require inferencing and synthesizing information from multiple documents to have multiple sentences as the answers.", "labels": [], "entities": []}, {"text": "The traditional QA systems can handle simple questions easily but complex questions often need more sophisticated treatment e.g. question decomposition.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.793137788772583}]}, {"text": "Therefore, it is necessary to automatically classify an input question as simple or complex to treat them accordingly.", "labels": [], "entities": []}, {"text": "We apply two machine learning techniques and a Latent Semantic Analysis (LSA) based method to automatically classify the questions as simple or complex.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated Question Answering (QA), the ability of a machine to answer questions asked in natural language, is perhaps the most exciting technological development of the past few years (.", "labels": [], "entities": [{"text": "Automated Question Answering (QA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7740582873423895}]}, {"text": "QA research attempts to deal with a wide range of question types including: fact, list, definition, how, why, hypothetical, semantically-constrained, and cross-lingual questions.", "labels": [], "entities": []}, {"text": "This paper concerns open-domain question answering where the QA system must handle questions of different types: simple or complex.", "labels": [], "entities": [{"text": "question answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7284603267908096}]}, {"text": "Simple questions are easier to answer as they require small snippets of texts as the answers.", "labels": [], "entities": []}, {"text": "For example, with a simple (i.e. factoid) question like: \"What is the magnitude of the earthquake in Japan?\", it can be safely assumed that the submitter of the question is looking fora number.", "labels": [], "entities": []}, {"text": "Current QA systems have been significantly advanced in demonstrating finer abilities to answer simple factoid and list questions.", "labels": [], "entities": []}, {"text": "On the other hand, with complex questions like: \"How is Japan affected by the earthquake?\", the wider focus of this question suggests that the submitter may not have a single or well-defined information need.", "labels": [], "entities": []}, {"text": "Therefore, to answer complex type of questions we often need to go through complex procedures such as question decomposition and multi-document summarization (.", "labels": [], "entities": [{"text": "question decomposition", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.7922407984733582}, {"text": "multi-document summarization", "start_pos": 129, "end_pos": 157, "type": "TASK", "confidence": 0.6118360310792923}]}, {"text": "Hence, it is necessary to automatically classify an input question as simple or complex in order to answer them using the appropriate technique.", "labels": [], "entities": []}, {"text": "Once we classify the questions as simple or complex, we can pass the simple questions to the traditional question answering systems whereas complex questions can be tackled differently in a sophisticated manner.", "labels": [], "entities": [{"text": "question answering", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7283987104892731}]}, {"text": "For example, the above complex question can be decomposed into a series of simple questions such as \"How many people had died by the earthquake?\", \"How many people became homeless?\", and \"Which cities were mostly damaged?\".", "labels": [], "entities": []}, {"text": "These simple questions can then be passed to the state-of-the-art QA systems, and a single answer to the complex question can be formed by combining the individual answers to the simple questions ().", "labels": [], "entities": []}, {"text": "This motivates the significance of classifying a question as simple or complex.", "labels": [], "entities": []}, {"text": "We experiment with two well-known machine learning methods and show that the task can be accomplished effectively using a simple feature set.", "labels": [], "entities": []}, {"text": "We also use a LSA-based technique to automatically classify the questions as simple or complex.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our data set consists of 5, 542 annotated questions.", "labels": [], "entities": []}, {"text": "We split the data set into three equal portions to apply 3-fold cross validation for the SVM and LSA experiments.", "labels": [], "entities": [{"text": "SVM and LSA experiments", "start_pos": 89, "end_pos": 112, "type": "DATASET", "confidence": 0.5901281014084816}]}, {"text": "In run-1, we use the first two portions as training data and the last portion as validation (i.e. testing) data.", "labels": [], "entities": []}, {"text": "Similarly, in run-2 and run-3, we use the first and the third subset of data for testing, respectively.", "labels": [], "entities": []}, {"text": "On the other hand, after a number of iterations (maximum 200), the k-means algorithm converges and each question is assigned to the cluster whose center is the closest according to the Euclidean distance function.", "labels": [], "entities": []}, {"text": "We form three different data sets for the k-means experiments.", "labels": [], "entities": []}, {"text": "In run-1, we use 1, 848 questions for learning while in run-2 and in run-3, we use 3, 695 and 5, 542 questions, respectively.", "labels": [], "entities": []}, {"text": "We can judge the performance of a classifier by calculating its accuracy on a particular test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9988598823547363}]}, {"text": "The accuracy can be defined as: Accur ac y = no. of C or rec t l y Cl assi f ied Quest ions  Accuracy (in %) Run-1 79.97% Run-2 78.94% Run-3 79.65% Average 79.52%", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996243715286255}, {"text": "Accur ac y", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9708916942278544}, {"text": "Accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.997811496257782}, {"text": "Run-1", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.967395544052124}, {"text": "Run-3", "start_pos": 135, "end_pos": 140, "type": "METRIC", "confidence": 0.8799493312835693}]}], "tableCaptions": [{"text": " Table 1: Accuracy of SVM", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985839128494263}, {"text": "SVM", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.8346884250640869}]}, {"text": " Table 2: Accuracy of k-means", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981951117515564}]}, {"text": " Table 3: Accuracy of LSA", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990907907485962}, {"text": "LSA", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.4576946794986725}]}]}