{"title": [{"text": "Fractal Unfolding: A Metamorphic Approach to Learning to Parse Recursive Structure *", "labels": [], "entities": [{"text": "Fractal Unfolding", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8059239685535431}]}], "abstractContent": [{"text": "We describe a computational framework for language learning and parsing in which dy-namical systems navigate on fractal sets.", "labels": [], "entities": []}, {"text": "We explore the predictions of the framework in an artificial grammar task in which humans and recurrent neural networks are trained on a language with recursive structure.", "labels": [], "entities": []}, {"text": "The results provide evidence for the claim of the dynamical systems models that grammatical systems continuously metamorphose during learning.", "labels": [], "entities": []}, {"text": "The present perspective permits structural comparison between the recursive representations in symbolic and neural network models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Some loci in the phrase structure systems of natural languages appear to employ center embedding recursion, or at least an approximation of it).", "labels": [], "entities": []}, {"text": "For example, one can embed a clause within a clause in English, using the object-extracted relative clause construction (e.g., the dog that the goat chased barked.).", "labels": [], "entities": []}, {"text": "But such recursion does not appear in every phrase and may not appear in every language.", "labels": [], "entities": []}, {"text": "Therefore, the system that learns natural languages must have away of recognizing recursion when it occurs.", "labels": [], "entities": []}, {"text": "We are interested in the problem, How does a language learner, seeing only a finite amount of data, decide on an unbounded recursive interpretation?", "labels": [], "entities": []}, {"text": "Here, we use the term \"finite state\" to refer to a system that can only be in a finite number of states.", "labels": [], "entities": []}, {"text": "We use the term \"recursion\" to refer to situations in which multiple embeddings require the use of an unbounded symbol memory to keep track of unfinished dependencies.", "labels": [], "entities": []}, {"text": "We focus hereon the case of centerembedding recursion, which can be generated by a context free grammar (one symbol on the left of each rule, finitely many symbols on the right) or a pushdown automaton (stack memory + finite state controller) but not by a finite state device.", "labels": [], "entities": []}, {"text": "One natural approach to the recursion recognition problem, recently explored by, involves Bayesian grammar selection.s model considered a range of grammars, including both finite state and context free grammars.", "labels": [], "entities": [{"text": "recursion recognition problem", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.9551348288853964}]}, {"text": "Their system, parameterized by data from Englishspeaking children in the Childes Database selected a context free grammar.", "labels": [], "entities": [{"text": "Childes Database", "start_pos": 73, "end_pos": 89, "type": "DATASET", "confidence": 0.8950827419757843}]}, {"text": "Several features of this approach are notable: (i) There is a rich set of structural assumptions (the grammars in the pool of candidates).", "labels": [], "entities": []}, {"text": "(ii) Because many plausible grammars generate overlapping data sets, a complexity ranking is also assumed and the system operates under Occam's Razor: prefer simpler grammars.", "labels": [], "entities": [{"text": "Occam's Razor", "start_pos": 136, "end_pos": 149, "type": "DATASET", "confidence": 0.9193496108055115}]}, {"text": "(iii) Grammar selection and on-line parsing are treated as sep-arate problems in that the system is evaluated for coverage of the observed sentences, but the particular method of parsing plays no role in the selection process.", "labels": [], "entities": [{"text": "Grammar selection", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.8593010306358337}, {"text": "on-line parsing", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.6993145942687988}]}, {"text": "Here, we focus on a contrasting approach: recurrent neural network models discover the structure of grammatical systems by sequentially processing the corpus data, attempting to predict after each word, what word will come next.", "labels": [], "entities": []}, {"text": "With respect to the properties mentioned above, the neural network approach has some advantages: (i) Formal analyses of some of the networks and related systems indicate that these models make even richer structural assumptions than the Bayesian approach: if the networks have infinite precision, then some of them recognize all string languages, including non-computable ones.", "labels": [], "entities": [{"text": "precision", "start_pos": 286, "end_pos": 295, "type": "METRIC", "confidence": 0.9819080233573914}]}, {"text": "For along while, theorists of cognition have adopted the view that positing a restrictive hypothesis space is desirable-otherwise a theory of structure would seem to have little substance.", "labels": [], "entities": []}, {"text": "However, if one offers a hypothesis about the organization of the hypothesis space, and a principle that specifies the way a learning organism navigates in the space, then the theory can still make strong, testable predictions.", "labels": [], "entities": []}, {"text": "We suggest that assuming a very general function class is preferable to presupposing arbitrary grammar or class restrictions.", "labels": [], "entities": []}, {"text": "(ii) The recurrent networks do not employ an independently defined complexity metric.", "labels": [], "entities": []}, {"text": "Instead, the learning process successively breaks symmetries in the initially unbiased weight set, driven by asymmetries in the data.", "labels": [], "entities": []}, {"text": "The result is a bias toward simplicity.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9750919938087463}]}, {"text": "We see this as an advantage in that the simplicity preference stems from the form of the architecture and learning mechanism.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9523153901100159}]}, {"text": "(iii) Word-by-word parsing and grammar selection occur as part of a single process-the network updates its weights every time it processes a word and this results in the formation of a parsing system.", "labels": [], "entities": [{"text": "Word-by-word parsing", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.7114011943340302}]}, {"text": "We see this as an advantage in that the moment-to-moment interaction of the system with data resembles the circumstances of a learning child.", "labels": [], "entities": []}, {"text": "On the other hand, there has long been a serious difficulty with the network approach: the network dynamics and solutions have been very opaque to analysis.", "labels": [], "entities": []}, {"text": "Although the systems sometimes learn well and capture data effectively, they are not scientifically very revealing unless we can interpret them.", "labels": [], "entities": []}, {"text": "The Bayesian grammar-selection approach is much stronger in this regard: the formal properties of the grammars employed are well understood and the selection process is well-grounded in statistical theory-e.g.,.", "labels": [], "entities": []}, {"text": "Here, we take advantage of recent formal results indicating how recurrent neural networks can encode abstract recursive structure) An essential insight is that the network can use a spatial recursive structure, a fractal, to encode the temporal recursive structure of a symbol sequence.", "labels": [], "entities": []}, {"text": "When the network is trained on short sentences exhibiting a few levels of embedding, it tends to generalize to higher levels of embedding, suggesting that it is not merely shaping itself to the training data, but discovers an abstract principle (.", "labels": [], "entities": []}, {"text": "During the course of learning, the fractal comes into being gradually in such away that lower-order finite-state approximations to the recursion develop before higher-order structure does-a complexity cline phenomenon.", "labels": [], "entities": []}, {"text": "We examined human and neural network learning of a recursive language with an artificial grammar paradigm, the Box Prediction paradigm.", "labels": [], "entities": []}, {"text": "Whereas our previous investigations of this task) focused on counting recursion languages (only a single stack symbol is required to track the recursive dependencies), we provide evidence here for mirror recursion learning by a few participants (multiple stack symbols required).", "labels": [], "entities": [{"text": "mirror recursion learning", "start_pos": 197, "end_pos": 222, "type": "TASK", "confidence": 0.7669482429822286}]}, {"text": "We show how the theory of fractal grammars can be used to hand wire a network that processes the recursive language of our task.", "labels": [], "entities": []}, {"text": "We then provide evidence that a Simple Recurrent Network, trained on the same task, also develops a fractal encoding.", "labels": [], "entities": []}, {"text": "Moreover, the network shows evidence of a embodying a complexity-cline-similarly complex grammars are adjacent in the parameter space.", "labels": [], "entities": []}, {"text": "An individual differences analysis indicates that a similar pattern arises in the humans.", "labels": [], "entities": []}, {"text": "We conclude that the network encodings can be formally related to symbolic recursive models, but are different in that learning occurs by continuous grammar metamorphosis.", "labels": [], "entities": []}], "datasetContent": [{"text": "We employed Michal Cernansky's implementation of Elman (1990)'s Simple Recurrent Network (http://www2.fiit.stuba.sk/\u02dc cernans/main/download.html).", "labels": [], "entities": []}, {"text": "The network had five input units, five output units and ten hidden units.", "labels": [], "entities": []}, {"text": "Activations changed as specified in (1) and weights changed according to.", "labels": [], "entities": [{"text": "Activations", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8310149312019348}, {"text": "weights", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9589833617210388}]}], "tableCaptions": [{"text": " Table 2: Input Map for DA 1. The automaton starts at the point, (1, 1). It's Final Region is also (1, 1).", "labels": [], "entities": []}]}