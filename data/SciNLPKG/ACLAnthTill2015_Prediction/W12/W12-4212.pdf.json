{"title": [{"text": "Application of Clause Alignment for Statistical Machine Translation", "labels": [], "entities": [{"text": "Clause Alignment", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.7893211245536804}, {"text": "Statistical Machine Translation", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.8515390157699585}]}], "abstractContent": [{"text": "The paper presents anew resource light flexible method for clause alignment which combines the Gale-Church algorithm with internally collected textual information.", "labels": [], "entities": [{"text": "clause alignment", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.909137636423111}]}, {"text": "The method does not resort to any pre-developed linguistic resources which makes it very appropriate for resource light clause alignment.", "labels": [], "entities": [{"text": "resource light clause alignment", "start_pos": 105, "end_pos": 136, "type": "TASK", "confidence": 0.6300040930509567}]}, {"text": "We experiment with a combination of the method with the original Gale-Church algorithm (1993) applied for clause alignment.", "labels": [], "entities": [{"text": "clause alignment", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.8751733303070068}]}, {"text": "The performance of this flexible method, as it will be referred to hereafter, is measured over a specially designed test corpus.", "labels": [], "entities": []}, {"text": "The clause alignment is explored as means to provide improved training data for the purposes of Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "clause alignment", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7952453196048737}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 96, "end_pos": 133, "type": "TASK", "confidence": 0.8689007560412089}]}, {"text": "A series of experiments with Moses demonstrate ways to modify the parallel resource and effects on translation quality: (1) baseline training with a Bulgarian-English parallel corpus aligned at sentence level; (2) training based on parallel clause pairs; (3) training with clause reordering, where clauses in each source language (SL) sentence are reordered according to order of the clauses in the target language (TL) sentence.", "labels": [], "entities": []}, {"text": "Evaluation is based on BLEU score and shows small improvement when using the clause aligned corpus .", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9981212019920349}]}], "introductionContent": [], "datasetContent": [{"text": "The precision is calculated as the number of true connections (between clauses in the two languages) divided by the number of the proposed connections, while the recall is the proportion of true connections to all connections in the corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995400905609131}, {"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9995415210723877}]}, {"text": "The connections in a bead are the Cartesian product of the clauses in the first and the second language.", "labels": [], "entities": []}, {"text": "The K : 0 and 0 : K bead models are considered as K : 1 and 1 : K by adding a fake clause.", "labels": [], "entities": []}, {"text": "The evaluation is performed both over the corpus as a whole and on each of the domain specific subcorpora included in it.", "labels": [], "entities": []}, {"text": "The evaluation of the clause alignment implementation of the Gale-Church algorithm on the same corpus shows overall precision of 0.902, recall -0.891 and F 1 measure -0.897.", "labels": [], "entities": [{"text": "clause alignment", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7885262370109558}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9990789890289307}, {"text": "recall -0.891", "start_pos": 136, "end_pos": 149, "type": "METRIC", "confidence": 0.9840248425801595}, {"text": "F 1 measure -0.897", "start_pos": 154, "end_pos": 172, "type": "METRIC", "confidence": 0.9764426350593567}]}, {"text": "Although the original Gale-Church method performs very well in terms of both precision and recall, sentence alignment poses a greater challenge.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9990407824516296}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9963216781616211}, {"text": "sentence alignment", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.7776302397251129}]}, {"text": "The explanation for this fact lies  in the broader scope of variations of clause correspondences as compared to sentences.", "labels": [], "entities": []}, {"text": "The bootstrapping method performs better in the translations with clause reordering.", "labels": [], "entities": []}, {"text": "An example is the administrative subcorpus where Gale-Church gives precision/recall -81.5%/79.7% compared to 86.6%/85.8% shown by the bootstrapping method.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9996743202209473}, {"text": "recall -81.5%/79.7%", "start_pos": 77, "end_pos": 96, "type": "METRIC", "confidence": 0.9404300252596537}]}, {"text": "In the texts with less clause order asymmetries the results are close.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of sentences and their length.", "labels": [], "entities": []}, {"text": " Table 2: Number of clauses and their length.", "labels": [], "entities": []}, {"text": " Table 3: Distribution of bead models in the manually  aligned corpus.", "labels": [], "entities": []}, {"text": " Table 4: Performance of the flexible method.", "labels": [], "entities": []}, {"text": " Table 5: Number of tokens in the test corpus.", "labels": [], "entities": []}]}