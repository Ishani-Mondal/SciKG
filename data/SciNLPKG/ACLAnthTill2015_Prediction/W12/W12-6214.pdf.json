{"title": [{"text": "A methodology for obtaining concept graphs from word graphs", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work, we describe a methodology based on the Stochastic Finite State Transducers paradigm for Spoken Language Understanding (SLU) for obtaining concept graphs from word graphs.", "labels": [], "entities": [{"text": "Spoken Language Understanding (SLU)", "start_pos": 102, "end_pos": 137, "type": "TASK", "confidence": 0.7713706841071447}]}, {"text": "In the edges of these concept graphs, both semantic and lexical information are represented.", "labels": [], "entities": []}, {"text": "This makes these graphs a very useful representation of the information for SLU.", "labels": [], "entities": [{"text": "SLU", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.8631684184074402}]}, {"text": "The best path in these concept graphs provides the best sequence of concepts .", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of SLU can be seen as the process that, given an utterance, computes a semantic interpretation of the information contained in it.", "labels": [], "entities": []}, {"text": "This semantic interpretation will be based on a task-dependent set of concepts.", "labels": [], "entities": []}, {"text": "An area where SLU systems are typically applied is the construction of spoken dialog systems.", "labels": [], "entities": []}, {"text": "The goal of the SLU subsystem in the context of a dialog system is to process the information given by the Automatic Speech Recognition (ASR) module, and provide the semantic interpretation of it to the Dialog Manager, which will determine the next action of the dialog.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 107, "end_pos": 141, "type": "TASK", "confidence": 0.7556997140248617}]}, {"text": "Thus, the work of the SLU module can be split into two subtasks, the first of them is the identification of the sequence of concepts and the segments of the original sentence according to them, and the other is the extraction of the relevant information underlying to these labeled segments.", "labels": [], "entities": [{"text": "identification of the sequence of concepts", "start_pos": 90, "end_pos": 132, "type": "TASK", "confidence": 0.7570433417956034}]}, {"text": "In this work we will focus on concept labeling, but we will also consider the other subtask in our evaluation.", "labels": [], "entities": [{"text": "concept labeling", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.6910671293735504}]}, {"text": "We can distinguish between the SLU systems that work with the 1-best transcription and those that take a representation of the n-best).", "labels": [], "entities": []}, {"text": "The use of a word graph as the input of the SLU module makes this task more difficult, as the search space becomes larger.", "labels": [], "entities": []}, {"text": "On the other hand, the advantage of using them is that there is more information that could help to find the correct semantic interpretation, rather than just taking the best sentence given by the ASR.", "labels": [], "entities": []}, {"text": "In the recent literature, a variety of approaches for automatic SLU have been proposed, like those explained in).", "labels": [], "entities": [{"text": "SLU", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.7200747132301331}]}, {"text": "The methodology that we propose in this paper is based on Stochastic Finite State Transducers (SFST).", "labels": [], "entities": []}, {"text": "This is a generative approach that composes several transducers containing acoustic, lexical and semantic knowledge.", "labels": [], "entities": []}, {"text": "Our method performs this composition on-the-fly, obtaining as a result a concept graph, where semantic information is associated with segments of words.", "labels": [], "entities": []}, {"text": "To carryout this step, we use a different language model for each concept and also study the use of lexical categorization and lemmas.", "labels": [], "entities": []}, {"text": "The best sequence of concepts can be determined by finding the best path in the concept graph, with the help of a language model of sequences of the concepts.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, the theoretical model for SLU based on SFST is briefly presented.", "labels": [], "entities": [{"text": "SLU", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9519779682159424}, {"text": "SFST", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.5002021789550781}]}, {"text": "Then, in Section 3 the methodology for converting word graphs into concept graphs is described.", "labels": [], "entities": []}, {"text": "A experimentation to eval-uate this methodology for the SLU task is shown in Section 4.", "labels": [], "entities": [{"text": "SLU task", "start_pos": 56, "end_pos": 64, "type": "TASK", "confidence": 0.8767594993114471}]}, {"text": "Finally, we draw some conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate this methodology, we have performed SLU experiments using the concept graphs obtained as explained in Section 3 and then finding the best path in each of them.", "labels": [], "entities": []}, {"text": "For this experimentation we have used the DIHANA corpus).", "labels": [], "entities": [{"text": "DIHANA corpus", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9441011250019073}]}, {"text": "This is a corpus of telephone spontaneous speech in Spanish composed by 900 dialogs acquired by 225 speakers using the Wizard of Oz technique, with a total of 6,229 user turns.", "labels": [], "entities": []}, {"text": "All these dialogs simulate real conversations in an automatic train information phone service.", "labels": [], "entities": []}, {"text": "The experiments reported here were performed using the user turns of the dialogs, splitting them into a set of 1,340 utterances (turns) for test and all the remaining 4,889 for training.", "labels": [], "entities": []}, {"text": "Some interesting statistics about the DIHANA corpus are given in In the DIHANA corpus, the orthographic transcriptions of the utterances are semi-automatically segmented and labeled in terms of semantic units.", "labels": [], "entities": [{"text": "DIHANA corpus", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.912984311580658}, {"text": "DIHANA corpus", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.9262114465236664}]}, {"text": "This segmentation is used by our methodology as a language model of sequences of words for each concept.", "labels": [], "entities": []}, {"text": "All the language models involved in this experimentation are bigram models trained using WittenBell smoothing and linear interpolation.", "labels": [], "entities": []}, {"text": "In our experimentation, we have considered three different ways for building the \u03bb gen transducer explained in Section 2.", "labels": [], "entities": []}, {"text": "The first way consists of considering a transducer that given a word as its input, outputs that word with probability 1.", "labels": [], "entities": []}, {"text": "This means that no generalization is being done.", "labels": [], "entities": []}, {"text": "The second \u03bb gen transducer performs a lexical categorization of some of the nouns of the vocabulary.", "labels": [], "entities": []}, {"text": "Some extra words have been added to some lexical categories, in order to make the task more realistic, as the lexical coverage is increased.", "labels": [], "entities": []}, {"text": "Nevertheless, it also makes the task harder, as the size of the vocabulary increases.", "labels": [], "entities": []}, {"text": "We have used a total of 11 lexical categories.", "labels": [], "entities": []}, {"text": "Finally, the third \u03bb gen , transducer we have generated performs the same lexical categorization but it also includes a lemmatization of the verbs.", "labels": [], "entities": []}, {"text": "This process is normally needed for real-world systems that work with spontaneous (and maybe telephonic) speech.", "labels": [], "entities": []}, {"text": "We have generated three sets of word graphs to take them as the input for the method.", "labels": [], "entities": []}, {"text": "The first of these sets, G 1 , is made up by the whole graphs obtained from a word graph builder module that works without using any language model.", "labels": [], "entities": []}, {"text": "The Oracle WER of these graphs is 4.10.", "labels": [], "entities": [{"text": "WER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9369843006134033}]}, {"text": "With Oracle WER we mean the WER obtained considering the sequence of words S(G) corresponding to the path in the graph G that is the nearest to the reference sentence.", "labels": [], "entities": [{"text": "WER", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9853324294090271}]}, {"text": "The second set, G 2 , is composed byword graphs that only contain the path corresponding to S(G) for each graph G \u2208 G 1 . These graphs give an idea of the best results we could achieve if we could minimize the confusion due to misrecognized words.", "labels": [], "entities": []}, {"text": "The third set, G 3 is formed by a synthetic word graph for each reference sentence, in which only that sentence is contained.", "labels": [], "entities": []}, {"text": "This set of graphs allows us to simulate an experimentation on plain text.", "labels": [], "entities": []}, {"text": "For our evaluation, we have taken two measures.", "labels": [], "entities": []}, {"text": "First, we have evaluated the Concept Error Rate (CER) over the best sequence of concepts.", "labels": [], "entities": [{"text": "Concept Error Rate (CER)", "start_pos": 29, "end_pos": 53, "type": "METRIC", "confidence": 0.7968770563602448}]}, {"text": "The definition of the CER is analogous to that of the WER but taking concepts instead of words.", "labels": [], "entities": [{"text": "CER", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.6333534717559814}, {"text": "WER", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.6506330966949463}]}, {"text": "Second, we have also evaluated the slot-level error (SLE).", "labels": [], "entities": [{"text": "slot-level error (SLE)", "start_pos": 35, "end_pos": 57, "type": "METRIC", "confidence": 0.9525169014930726}]}, {"text": "The SLE is similar to the CER but deleting the nonrelevant segments (such as courtesies) and substituting the relevant concepts by a canonic value for the sequence of words associated to them.", "labels": [], "entities": []}, {"text": "show the results obtained using the different \u03bb gen transducers explained before.", "labels": [], "entities": []}, {"text": "From the results of, and 4 several facts come to light.", "labels": [], "entities": []}, {"text": "First, we can see that, in all the experiments performed with the G 1 set, the CER is lower than the SLE, while with the other sets the CER is larger than the SLE.", "labels": [], "entities": [{"text": "CER", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9946208000183105}, {"text": "SLE", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.8870416879653931}, {"text": "CER", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.9878720045089722}]}, {"text": "It is due to the fact that the whole graphs obtained from the word graph builder have more lexical confusion than those from G 2 and G 3 , which are based on the reference sentence.", "labels": [], "entities": []}, {"text": "This lexical confusion may cause that a well-recognized concept is associated to a misrecognized sequence of words.", "labels": [], "entities": []}, {"text": "This would imply that a hit would be considered for the CER calculation, while the value for this slot is missed.", "labels": [], "entities": [{"text": "CER", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.593583345413208}]}, {"text": "Other interesting fact is that, for the G 1 set, the more complex \u03bb gen transducers give the worse results.", "labels": [], "entities": []}, {"text": "This is because in these graphs there is a significant confusion between phonetically similar words, as the graphs were generated without any language model.", "labels": [], "entities": []}, {"text": "This phonetic confusion, combined with the generalizations expressed by the lexical categorization and the lemmas, makes the task harder, which leads to worse results.", "labels": [], "entities": []}, {"text": "Nevertheless, in a realworld application of this system these generalizations would be needed in order to have a larger coverage of the lexicon of the language.", "labels": [], "entities": []}, {"text": "The experiments on G 2 and G 3 show that when the confusion introduced in the graphs due to misrecognized words is minimized, the use of lexical categorization and lemmatization helps to improve the results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Characteristics of the DIHANA corpus.", "labels": [], "entities": [{"text": "DIHANA corpus", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9454866051673889}]}, {"text": " Table 2: CER and SLE without any categorization.", "labels": [], "entities": [{"text": "CER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8492805361747742}, {"text": "SLE", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.7655329704284668}]}, {"text": " Table 3: CER and SLE with lexical categorization.", "labels": [], "entities": [{"text": "SLE", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.6165944933891296}]}, {"text": " Table 4: CER and SLE with lemmatization and lexical  categorization.", "labels": [], "entities": []}]}