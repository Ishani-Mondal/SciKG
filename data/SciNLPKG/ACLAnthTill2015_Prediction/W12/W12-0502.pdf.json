{"title": [{"text": "A Study of Hybrid Similarity Measures for Semantic Relation Extraction", "labels": [], "entities": [{"text": "Similarity Measures", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7469736039638519}, {"text": "Semantic Relation Extraction", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.841699461142222}]}], "abstractContent": [{"text": "This paper describes several novel hybrid semantic similarity measures.", "labels": [], "entities": []}, {"text": "We study various combinations of 16 baseline measures based on WordNet, Web as a corpus , corpora, dictionaries, and encyclopedia.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.966425359249115}]}, {"text": "The hybrid measures rely on 8 combination methods and 3 measure selection techniques and are evaluated on (a) the task of predicting semantic similarity scores and (b) the task of predicting semantic relation between two terms.", "labels": [], "entities": [{"text": "predicting semantic similarity", "start_pos": 122, "end_pos": 152, "type": "TASK", "confidence": 0.7800659338633219}]}, {"text": "Our results show that hybrid measures outperform single measures by a wide margin, achieving a correlation up to 0.890 and MAP(20) up to 0.995.", "labels": [], "entities": [{"text": "correlation", "start_pos": 95, "end_pos": 106, "type": "METRIC", "confidence": 0.9663069248199463}, {"text": "MAP(20)", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.9567919969558716}]}], "introductionContent": [{"text": "Semantic similarity measures and relations are proven to be valuable for various NLP and IR applications, such as word sense disambiguation, query expansion, and question answering.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 114, "end_pos": 139, "type": "TASK", "confidence": 0.690018097559611}, {"text": "query expansion", "start_pos": 141, "end_pos": 156, "type": "TASK", "confidence": 0.7326163202524185}, {"text": "question answering", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.9122328162193298}]}, {"text": "Let R be a set of synonyms, hypernyms, and co-hyponyms of terms C, established by a lexicographer.", "labels": [], "entities": []}, {"text": "A semantic relation extraction method aims at discovering a set of relations\u02c6Rrelations\u02c6 relations\u02c6R approximating R.", "labels": [], "entities": [{"text": "semantic relation extraction", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.7141905228296915}]}, {"text": "The quality of the relations provided by existing extractors is still lower than the quality of the manually constructed relations.", "labels": [], "entities": []}, {"text": "This motivates the development of new relation extraction methods.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8649654984474182}]}, {"text": "A well-established approach to relation extraction is based on lexico-syntactic patterns.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.9660766124725342}]}, {"text": "In this paper, we study an alternative approach based on similarity measures.", "labels": [], "entities": []}, {"text": "These methods do not return a type of the relation between words ( \u02c6 R \u2286 C \u00d7 C).", "labels": [], "entities": []}, {"text": "However, we assume that the methods should retrieve a mix of synonyms, hypernyms, and co-hyponyms for practical use in text processing applications and evaluate them accordingly.", "labels": [], "entities": []}, {"text": "A multitude of measures was used in the previous research to extract synonyms, hypernyms, and co-hyponyms.", "labels": [], "entities": []}, {"text": "Five key approaches are those based on a distributional analysis), Web as a corpus, lexico-syntactic patterns (, semantic networks, and definitions of dictionaries or encyclopedias ().", "labels": [], "entities": []}, {"text": "Still, the existing approaches based on these single measures are far from being perfect.", "labels": [], "entities": []}, {"text": "For instance, compared distributional measures and reported Precision@1 of 76% for the best one.", "labels": [], "entities": [{"text": "Precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9991617202758789}]}, {"text": "For improving the performance, some attempts were made to combine single measures, such as.", "labels": [], "entities": []}, {"text": "However, most studies are still not taking into account the whole range of existing measures, combining mostly sporadically different methods.", "labels": [], "entities": []}, {"text": "The main contribution of the paper is a systematic analysis of 16 baseline measures, and their combinations with 8 fusion methods and 3 techniques for the combination set selection.", "labels": [], "entities": []}, {"text": "We are first to propose hybrid similarity measures based on all five extraction approaches listed above; our combined techniques are original as they exploit all key types of resources usable for semantic relation extraction -corpus, web corpus, semantic networks, dictionaries, and encyclopedias.", "labels": [], "entities": [{"text": "semantic relation extraction", "start_pos": 196, "end_pos": 224, "type": "TASK", "confidence": 0.6488907635211945}]}, {"text": "Our experiments confirm that the combined measures are more precise than the single ones.", "labels": [], "entities": [{"text": "precise", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.971473753452301}]}, {"text": "The best found hybrid measure combines 15 baseline measures with the supervised learning.", "labels": [], "entities": []}, {"text": "It outperforms all tested single and combined measures by a large margin, achieving a correlation of 0.870 with human judgements and MAP(20) of 0.995 on the relation recognition task.", "labels": [], "entities": [{"text": "MAP(20)", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9680816233158112}, {"text": "relation recognition task", "start_pos": 157, "end_pos": 182, "type": "TASK", "confidence": 0.9324577848116556}]}], "datasetContent": [{"text": "Evaluation relies on human judgements about semantic similarity and on manually constructed semantic relations.", "labels": [], "entities": []}, {"text": "This kind of ground truth enables direct assessment of measure performance and indirect assessment of extraction quality with this measure.", "labels": [], "entities": []}, {"text": "Each of these datasets consists of N tuples \u27e8c i , c j , s ij \u27e9, where c i , c j are terms, and s ij is their similarity obtained by human judgement.", "labels": [], "entities": []}, {"text": "We use three standard human judgements datasets -MC, RG and), composed of 30, 65, and 353 pairs of terms respectively.", "labels": [], "entities": []}, {"text": "Let s = (s i1 , s i2 , . .", "labels": [], "entities": []}, {"text": ", s iN ) be a vector of ground truth scores, and\u02c6sand\u02c6and\u02c6s = (\u02c6 s i1 , \u02c6 s i2 , . .", "labels": [], "entities": []}, {"text": ", \u02c6 s iN ) be a vector of similarity scores calculated with a similarity measure.", "labels": [], "entities": []}, {"text": "Then, the quality of this measure is assessed with Spearman's correlation between s and\u02c6sand\u02c6and\u02c6s.", "labels": [], "entities": [{"text": "Spearman's correlation", "start_pos": 51, "end_pos": 73, "type": "METRIC", "confidence": 0.6860957046349844}]}, {"text": "This kind of ground truth enables indirect assessment of measure performance and direct assessment of We used Mean as a hybrid measure and the following criteria: MAP(20), MAP(50), P(10), P(20) and P(50).", "labels": [], "entities": [{"text": "MAP", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.8861165046691895}]}, {"text": "We kept measures which were selected by most of the criteria.", "labels": [], "entities": []}, {"text": "This kind of evaluation is based on the number of correctly extracted relations with the method described in Section 2.", "labels": [], "entities": []}, {"text": "Let\u02c6RLet\u02c6 Let\u02c6R k be a set of extracted semantic relations at a certain level of the kNN threshold k.", "labels": [], "entities": []}, {"text": "Then, precision, recall, and mean average precision (MAP) at k are calculated correspondingly as follows: The quality of a similarity measure is assessed with the six following statistics: P (10), P (20), P (50), R(50), M (20), and M (50). and present performance of the single and hybrid measures on the five ground truth datasets listed above.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9995204210281372}, {"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9991108775138855}, {"text": "mean average precision (MAP)", "start_pos": 29, "end_pos": 57, "type": "METRIC", "confidence": 0.9550960063934326}]}, {"text": "The first three columns of the table contain correlations with human judgements, while the other columns present performance on the relation extraction task.", "labels": [], "entities": [{"text": "relation extraction task", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.8958510955174764}]}], "tableCaptions": [{"text": " Table 1: Performance of 16 single and 8 hybrid similarity measures on human judgements datasets (MC, RG,  WordSim353) and semantic relation datasets (BLESS and SN). The best scores in a group (single/hybrid) are in  bold; the very best scores are in grey. Correlations in italics mean p > 0.05, otherwise p \u2264 0.05.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 107, "end_pos": 117, "type": "DATASET", "confidence": 0.9518271684646606}, {"text": "BLESS", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.989575982093811}]}]}