{"title": [{"text": "DEPFIX: A System for Automatic Correction of Czech MT Outputs *", "labels": [], "entities": [{"text": "DEPFIX", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6221988201141357}, {"text": "Automatic Correction of Czech MT Outputs", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.5108072906732559}]}], "abstractContent": [{"text": "We present an improved version of DEPFIX (Mare\u010dek et al., 2011), a system for automatic rule-based post-processing of English-to-Czech MT outputs designed to increase their fluency.", "labels": [], "entities": [{"text": "DEPFIX", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.4949447214603424}, {"text": "MT outputs", "start_pos": 135, "end_pos": 145, "type": "TASK", "confidence": 0.8129859566688538}]}, {"text": "We enhanced the rule set used by the original DEPFIX system and measured the performance of the individual rules.", "labels": [], "entities": [{"text": "DEPFIX system", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.8429127931594849}]}, {"text": "We also modified the dependency parser of McDonald et al.", "labels": [], "entities": []}, {"text": "(2005) in two ways to adjust it for the parsing of MT outputs.", "labels": [], "entities": [{"text": "parsing of MT outputs", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.772949755191803}]}, {"text": "We show that our system is able to improve the quality of the state-of-the-art MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9850831031799316}]}], "introductionContent": [{"text": "The today's outputs of Machine Translation (MT) often contain serious grammatical errors.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.8505870938301087}]}, {"text": "This is particularly apparent in statistical MT systems (SMT), which do not employ structural linguistic rules.", "labels": [], "entities": [{"text": "MT systems (SMT)", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7573898434638977}]}, {"text": "These systems have been dominating the area in the recent years).", "labels": [], "entities": []}, {"text": "Such errors make the translated text less fluent and may even lead to unintelligibility or misleading statements.", "labels": [], "entities": []}, {"text": "The problem is more evident in languages with rich morphology, such as Czech, where morphological agreement is of a relatively high importance for the interpretation of syntactic relations.", "labels": [], "entities": []}, {"text": "The DEPFIX system) attempts to correct some of the frequent SMT sys- * This research has been supported by the European Union Seventh Framework Programme (FP7) under grant agreement n \u2022 247762 (Faust), and by the grants GAUK116310, GA201/09/H057 (Res-Informatica), and LH12093.", "labels": [], "entities": [{"text": "DEPFIX system", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9288927614688873}, {"text": "SMT sys", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.887984961271286}, {"text": "European Union Seventh Framework Programme (FP7)", "start_pos": 111, "end_pos": 159, "type": "DATASET", "confidence": 0.7779170870780945}, {"text": "GAUK116310", "start_pos": 220, "end_pos": 230, "type": "DATASET", "confidence": 0.9334983229637146}, {"text": "GA201/09/H057", "start_pos": 232, "end_pos": 245, "type": "DATASET", "confidence": 0.8821626901626587}]}, {"text": "tems' errors in English-to-Czech translations.", "labels": [], "entities": []}, {"text": "It analyzes the target sentence (the SMT output in Czech language) using a morphological tagger and a dependency parser and attempts to correct it by applying several rules which enforce consistency with the Czech grammar.", "labels": [], "entities": [{"text": "SMT output", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.8777447640895844}]}, {"text": "Most of the rules use the source sentence (the SMT input in English language) as a source of information about the sentence structure.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9702180624008179}]}, {"text": "The source sentence is also tagged and parsed, and word-to-word alignment with the target sentence is determined.", "labels": [], "entities": []}, {"text": "In this paper, we present DEPFIX 2012, an improved version of the original DEPFIX 2011 system.", "labels": [], "entities": [{"text": "DEPFIX 2012", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.8575805127620697}, {"text": "DEPFIX 2011 system", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.9037625988324484}]}, {"text": "It makes use of anew parser, described briefly in Section 3, which is adapted to handle the generally ungrammatical target sentences better.", "labels": [], "entities": []}, {"text": "We have also enhanced the set of grammar correction rules, for which we give a detailed description in Section 4.", "labels": [], "entities": [{"text": "grammar correction", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.6849792748689651}]}, {"text": "Section 5 gives an account of the experiments performed to evaluate the DEPFIX 2012 system and compare it to DEPFIX 2011.", "labels": [], "entities": [{"text": "DEPFIX 2012 system", "start_pos": 72, "end_pos": 90, "type": "DATASET", "confidence": 0.9423234661420187}, {"text": "DEPFIX 2011", "start_pos": 109, "end_pos": 120, "type": "DATASET", "confidence": 0.9445470869541168}]}, {"text": "Section 6 then concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "For parameter tuning, we used datasets from the WMT10 translation task and translations by ON-LINEB and CU-BOJAR systems.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7178350538015366}, {"text": "WMT10 translation task", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7854986786842346}, {"text": "ON-LINEB", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.5480817556381226}]}, {"text": "Manual evaluation of both DEPFIX 2011 and DEP-FIX 2012 was performed on the WMT11 3 test set translated by ONLINEB.", "labels": [], "entities": [{"text": "DEPFIX 2011", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.8223092555999756}, {"text": "DEP-FIX 2012", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.7373137772083282}, {"text": "WMT11 3 test set translated", "start_pos": 76, "end_pos": 103, "type": "DATASET", "confidence": 0.9581127285957336}, {"text": "ONLINEB", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.8050655722618103}]}, {"text": "500 sentences were randomly selected and blind-evaluated by two independent annotators, who were presented with outputs of ONLINEB, DEPFIX 2011 and DEPFIX 2012.", "labels": [], "entities": [{"text": "ONLINEB", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.782986044883728}, {"text": "DEPFIX 2011", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.9071044027805328}, {"text": "DEPFIX 2012", "start_pos": 148, "end_pos": 159, "type": "DATASET", "confidence": 0.8552888631820679}]}, {"text": "(For 246 sentences, at least one of the DEPFIX setups modified the ONLINEB translation.)", "labels": [], "entities": [{"text": "ONLINEB", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9258783459663391}]}, {"text": "They provided us with a pairwise comparison of the three setups, with the possibility to mark the sentence as \"indefinite\" if translations were of equal quality.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "In, we use the manual evaluation to measure the performance of the individual rules in DEP-FIX 2012.", "labels": [], "entities": [{"text": "DEP-FIX 2012", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.9317145347595215}]}, {"text": "For each rule, we ran DEPFIX 2012 with this rule disabled and compared the output to the output of the full DEPFIX 2012.", "labels": [], "entities": [{"text": "DEPFIX 2012", "start_pos": 22, "end_pos": 33, "type": "DATASET", "confidence": 0.941996842622757}, {"text": "DEPFIX 2012", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.9360405802726746}]}, {"text": "The number of affected sentences on the whole WMT11 test set, given as \"changed\", represents the impact of the rule.", "labels": [], "entities": [{"text": "WMT11 test set", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.9689307610193888}]}, {"text": "The number of affected sentences selected for manual evaluation is listed as \"evaluated\".", "labels": [], "entities": []}, {"text": "Finally, the annotators' ratings of the \"evaluated\" sentences  (suggesting whether the rule improved or worsened the translation, or whether the result was indefinite) were counted and divided by the number of annotators to get the average performance of each rule.", "labels": [], "entities": []}, {"text": "Please note that the lower the \"evaluated\" number, the lower the confidence of the results.", "labels": [], "entities": [{"text": "confidence", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9927480816841125}]}, {"text": "The inter-annotator agreement matrix for comparison of ONLINEB + DEPFIX 2012 (denoted as Setup 1) with ONLINEB (Setup 2) is given in Table 3.", "labels": [], "entities": [{"text": "ONLINEB + DEPFIX 2012", "start_pos": 55, "end_pos": 76, "type": "DATASET", "confidence": 0.6788555756211281}, {"text": "ONLINEB", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.7500038743019104}]}, {"text": "The results for the other two setup pairs were similar, with the average inter-annotator agreement being 77%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.6551072001457214}]}, {"text": "We also performed several experiments with automatic evaluation using the standard BLEU metric ().", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 83, "end_pos": 94, "type": "METRIC", "confidence": 0.9690552055835724}]}, {"text": "As the effect of DEPFIX in terms of BLEU is rather small, the results are not as confident as the results of manual evaluation.", "labels": [], "entities": [{"text": "DEPFIX", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.7341547012329102}, {"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9988663196563721}]}, {"text": "In, we compare the DEPFIX 2011 and DEPFIX 2012 systems and measure the contribution of parser adaptation (Section 3) and rule improvements (Section 4).", "labels": [], "entities": [{"text": "DEPFIX 2011 and DEPFIX 2012", "start_pos": 19, "end_pos": 46, "type": "DATASET", "confidence": 0.8037145256996154}, {"text": "parser adaptation", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.909528374671936}]}, {"text": "It can be seen that the combined effect of applying both system modifications is greater than when they are applied alone.", "labels": [], "entities": []}, {"text": "The improvement of DEPFIX 2012 over ONLINEB without DEPFIX is statistically significant at 95% confidence level.", "labels": [], "entities": [{"text": "DEPFIX 2012", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.8026805222034454}, {"text": "ONLINEB", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.6381179690361023}, {"text": "DEPFIX", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9037848114967346}]}, {"text": "The effect of DEPFIX 2012 on the outputs of some of the best-scoring SMT systems in the WMT12 Translation Task 5 is shown in.", "labels": [], "entities": [{"text": "DEPFIX 2012", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.7167830467224121}, {"text": "SMT", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9888243079185486}, {"text": "WMT12 Translation Task", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7525977889696757}]}, {"text": "Although DEPFIX 2012 was tuned only on ONLINEB and CU-BOJAR system outputs, it improves the BLEU score of all the best-scoring systems, which suggests that Setup 1", "labels": [], "entities": [{"text": "DEPFIX 2012", "start_pos": 9, "end_pos": 20, "type": "DATASET", "confidence": 0.8319116830825806}, {"text": "ONLINEB", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9562995433807373}, {"text": "BLEU score", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9807191491127014}]}], "tableCaptions": [{"text": " Table 3: Inter-annotator agreement matrix for ONLINEB  + DEPFIX 2012 as Setup 1 and ONLINEB as Setup 2.", "labels": [], "entities": [{"text": "ONLINEB  + DEPFIX 2012", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.7196417152881622}, {"text": "ONLINEB", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.766789436340332}]}, {"text": " Table 1: Manual pairwise comparison on 500 sentences from WMT11 test set processed by ONLINEB, ONLINEB +  DEPFIX 2011 and ONLINEB + DEPFIX 2012. Evaluated by two independent annotators.", "labels": [], "entities": [{"text": "WMT11 test set processed", "start_pos": 59, "end_pos": 83, "type": "DATASET", "confidence": 0.9569398462772369}, {"text": "ONLINEB", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.48260483145713806}, {"text": "ONLINEB +  DEPFIX 2011", "start_pos": 96, "end_pos": 118, "type": "DATASET", "confidence": 0.6419778764247894}, {"text": "ONLINEB + DEPFIX 2012", "start_pos": 123, "end_pos": 144, "type": "DATASET", "confidence": 0.7512525171041489}]}, {"text": " Table 2: Impact and accuracy of individual DEPFIX 2012 rules using manual evaluation on 500 sentences from  WMT11 test set translated by ONLINEB. The number of changed sentences is counted on the whole WMT11 test  set, i.e. 3003 sentences. The numbers of improved, worsened and indefinite translations are averaged over the annota- tors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9994024038314819}, {"text": "DEPFIX 2012 rules", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.8418666124343872}, {"text": "WMT11 test set translated", "start_pos": 109, "end_pos": 134, "type": "DATASET", "confidence": 0.9677393436431885}, {"text": "ONLINEB", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.6292858123779297}, {"text": "WMT11 test  set", "start_pos": 203, "end_pos": 218, "type": "DATASET", "confidence": 0.9834915995597839}]}, {"text": " Table 4: Performance of ONLINEB and various DEPFIX  setups on the WMT11 test set.", "labels": [], "entities": [{"text": "ONLINEB", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9739047884941101}, {"text": "WMT11 test set", "start_pos": 67, "end_pos": 81, "type": "DATASET", "confidence": 0.9719327092170715}]}, {"text": " Table 5: Comparison of BLEU of baseline system output  and corrected system output on WMT12 test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9990743398666382}, {"text": "WMT12 test set", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.9702822168668112}]}]}