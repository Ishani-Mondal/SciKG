{"title": [{"text": "A Broad Evaluation of Techniques for Automatic Acquisition of Multiword Expressions", "labels": [], "entities": [{"text": "Automatic Acquisition of Multiword Expressions", "start_pos": 37, "end_pos": 83, "type": "TASK", "confidence": 0.7615708947181702}]}], "abstractContent": [{"text": "Several approaches have been proposed for the automatic acquisition of multiword expressions from corpora.", "labels": [], "entities": [{"text": "automatic acquisition of multiword expressions from corpora", "start_pos": 46, "end_pos": 105, "type": "TASK", "confidence": 0.7458082692963737}]}, {"text": "However, there is no agreement about which of them presents the best cost-benefit ratio, as they have been evaluated on distinct datasets and/or languages.", "labels": [], "entities": []}, {"text": "To address this issue, we investigate these techniques analysing the following dimensions: expression type (compound nouns, phrasal verbs), language (English, French) and corpus size.", "labels": [], "entities": []}, {"text": "Results show that these techniques tend to extract similar candidate lists with high recall (\u223c 80%) for nominals and high precision (\u223c 70%) for verbals.", "labels": [], "entities": [{"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9989129304885864}, {"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9956764578819275}]}, {"text": "The use of association measures for candidate filtering is useful but some of them are more onerous and not significantly better than raw counts.", "labels": [], "entities": [{"text": "candidate filtering", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7164265811443329}]}, {"text": "We finish with an evaluation of flexibility and an indication of which technique is recommended for each language-type-size context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Taking into account multiword expressions (MWEs) is important to confer naturalness to the output of NLP systems.", "labels": [], "entities": []}, {"text": "An MT system, for instance, needs to be aware of idiomatic expressions like raining cats and dogs to avoid literal translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9602179527282715}]}, {"text": "Likewise, a parser needs to deal with verb-particle expressions like takeoff from Paris and with light verb constructions like take a walk along the river in order to avoid PP-attachment errors.", "labels": [], "entities": []}, {"text": "Even though the last decade has seen considerable research in the automatic acquisition of MWEs, both in theoretical and in computational linguistics, to date there are few NLP applications integrating explicit MWE treatment.", "labels": [], "entities": [{"text": "automatic acquisition of MWEs", "start_pos": 66, "end_pos": 95, "type": "TASK", "confidence": 0.7127726674079895}, {"text": "MWE treatment", "start_pos": 211, "end_pos": 224, "type": "TASK", "confidence": 0.8924860954284668}]}, {"text": "This maybe partly explained by the complexity of MWEs: as they are heterogeneous and flexible, there is no unique push-button approach to identify all types of MWEs in all languages ().", "labels": [], "entities": []}, {"text": "Existing approaches are either generic but present relatively low pre- The equivalent expressions in French would be raining ropes, in German raining young dogs, in Portuguese raining Swiss knives, etc.", "labels": [], "entities": [{"text": "pre-", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9899393618106842}]}, {"text": "cision or they require a large amount of language-specific resources to yield good results.", "labels": [], "entities": []}, {"text": "The goal of this paper is to evaluate approaches for the automatic acquisition of MWEs from corpora ( \u00a72), examining as parameters of the experimental context the language (English and French), type of target MWE (verbal and nominal) and size of corpus (small, medium, large).", "labels": [], "entities": [{"text": "automatic acquisition of MWEs from corpora", "start_pos": 57, "end_pos": 99, "type": "TASK", "confidence": 0.671691690882047}]}, {"text": "We focus on 4 approaches 2 and the experimental setup is presented in \u00a73.", "labels": [], "entities": []}, {"text": "In \u00a74 we evaluate the following acquisition dimensions: quality of extracted candidates and of association measures, use of computational resources and flexibility.", "labels": [], "entities": []}, {"text": "Thus, this research presents a comparative investigation of available approaches and indicates the best cost-benefit ratio in a given context (language, type, corpus size), pointing out current limitations and suggesting future avenues of research for the field.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of MWE acquisition is an open problem.", "labels": [], "entities": [{"text": "MWE acquisition", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.9720850586891174}]}, {"text": "While classical measures like precision and recall assume that a complete (or at least broad-coverage) gold standard exists, manual annotation of top-n candidates and mean average precision (MAP) are labour-intensive even when applied to a small sample, emphasizing precision regardless of the number of acquired new expressions.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9991419315338135}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9984982013702393}, {"text": "mean average precision (MAP)", "start_pos": 167, "end_pos": 195, "type": "METRIC", "confidence": 0.9318669637044271}, {"text": "precision", "start_pos": 266, "end_pos": 275, "type": "METRIC", "confidence": 0.9968632459640503}]}, {"text": "As approaches differ in the way they allow the description of extraction criteria, we evaluate candidate extraction separately from AMs.", "labels": [], "entities": [{"text": "candidate extraction", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.7101943492889404}]}], "tableCaptions": [{"text": " Table 1: Number of sentences and of words of each fragment of  the Europarl corpus in fr and in en.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.9637342989444733}]}, {"text": " Table 2: (P)recision and (R)ecall of en nominal candidates,  comparison across corpus sizes (S)mall, (M)edium and (L)arge.", "labels": [], "entities": [{"text": "arge", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9008471965789795}]}, {"text": " Table 3: Intersection of the candidate lists extracted from  medium corpus. Nominal candidates en in bottom left, verbal  candidates en in top right.", "labels": [], "entities": [{"text": "Intersection", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9362549185752869}]}, {"text": " Table 4: Mean average precision of AMs in large corpus.", "labels": [], "entities": [{"text": "Mean average precision", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8904927770296732}, {"text": "AMs", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.5872211456298828}]}, {"text": " Table 5: Summary of tools for MWE acquisition.", "labels": [], "entities": [{"text": "MWE acquisition", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.9858965873718262}]}]}