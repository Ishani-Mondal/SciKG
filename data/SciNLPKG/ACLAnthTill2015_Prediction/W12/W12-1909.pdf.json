{"title": [{"text": "The PASCAL Challenge on Grammar Induction", "labels": [], "entities": [{"text": "Grammar Induction", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8067266345024109}]}], "abstractContent": [{"text": "This paper presents the results of the PASCAL Challenge on Grammar Induction, a competition in which competitors sought to predict part-of-speech and dependency syntax from text.", "labels": [], "entities": [{"text": "PASCAL Challenge on Grammar Induction", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.7223035812377929}]}, {"text": "Although many previous competitions have featured dependency grammars or parts-of-speech, these were invariably framed as supervised learning and/or domain adaption.", "labels": [], "entities": []}, {"text": "This is the first challenge to evaluate unsuper-vised induction systems, a sub-field of syntax which is rapidly becoming very popular.", "labels": [], "entities": []}, {"text": "Our challenge made use of a 10 different treebanks annotated in a range of different linguistic formalisms and covering 9 languages.", "labels": [], "entities": []}, {"text": "We provide an overview of the approaches taken by the participants, and evaluate their results on each dataset using a range of different evaluation metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Inducing grammatical structure from text has long been fundamental problem in Computational Linguistics and Natural Language Processing.", "labels": [], "entities": [{"text": "Inducing grammatical structure from text", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.826058852672577}]}, {"text": "In recent years interest has grown, spurred by advances in unsupervised statistical modelling and machine learning.", "labels": [], "entities": [{"text": "statistical modelling", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.8287667632102966}]}, {"text": "The task has relevance to cognitive scientists and linguists attempting to gauge the learnability of natural language by human children, and also natural language processing researchers who seek syntactic representations for languages with few linguistic resources.", "labels": [], "entities": []}, {"text": "Grammar learning has been popular in previous challenges.", "labels": [], "entities": [{"text": "Grammar learning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8766266405582428}]}, {"text": "For example the CoNLL shared tasks in) involved supervised learning of dependency parsers across a wide range of different languages.", "labels": [], "entities": []}, {"text": "Our challenge has many similarities to these, in that we focus on dependency grammars, however we seek to evaluate unsupervised algorithms only using syntactically annotated data for evaluation and not for training.", "labels": [], "entities": []}, {"text": "Additionally we also consider the related task of part-of-speech (POS) induction, and the next logical challenge: the joint task of POS and dependency induction.", "labels": [], "entities": [{"text": "part-of-speech (POS) induction", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.651128101348877}, {"text": "dependency induction", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.7828116118907928}]}, {"text": "Other related challenges can be found in the formal grammar community (e.g., the Omphalos 1 competition) in which competitors seek to learn synthetic languages.", "labels": [], "entities": []}, {"text": "In contrast we seek to model natural language text, which entails many different challenges.", "labels": [], "entities": []}, {"text": "Research into unsupervised grammar and POS induction holds considerable promise, although current approaches are still along way from solving the general problem.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9503643214702606}]}, {"text": "For example, the majority of recent research into dependency grammar induction has adopted the evaluation setting of who learn grammars on strings of POS tags, rather than on words themselves.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.7762432297070821}]}, {"text": "One aim of this challenge is to popularise the more difficult and ambitious task of inducing grammars directly from text, which can be viewed as integrating the POS and grammar induction tasks.", "labels": [], "entities": []}, {"text": "A second aim is to foster grammar and POS induction research across a wider variety of languages, and improving the standard of evaluation.", "labels": [], "entities": [{"text": "POS induction research", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.8968958059946696}, {"text": "standard", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.968299925327301}]}, {"text": "We have collated data from existing treebanks in a variety of different languages, domains and linguistic formalisms.", "labels": [], "entities": []}, {"text": "This gives a diverse range of data upon which to test induction algorithms, yielding a deeper insight into their strengths and shortcomings.", "labels": [], "entities": []}, {"text": "One key problem in grammar induction research is how to evaluate the models' predictions given that often many different analyses are linguistically plausible, e.g., the choice of whether determiners or nouns should head noun phrases, or how to represent coordination.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.8305566012859344}, {"text": "represent coordination", "start_pos": 245, "end_pos": 267, "type": "TASK", "confidence": 0.8017665445804596}]}, {"text": "Simply comparing against a single gold standard often results in poor reported performance because the model has discovered a different analysis to that used when annotating the treebank.", "labels": [], "entities": []}, {"text": "For this reason it has been popular to use lenient measures for comparing predicted trees to the treebank gold standard trees, such as undirected accuracy and the neutral edge distance ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9237408638000488}]}, {"text": "As well as evaluating using these popular metrics, we also propose anew method of evaluation which is also lenient in that it rewards different types of linguistically plausible output, but requires consistency in the output, something the previous methods cannot do.", "labels": [], "entities": []}, {"text": "The paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the tasks and our data format and section 3 outlines the different treebanks used for the challenge.", "labels": [], "entities": []}, {"text": "The baselines, our own benchmark systems and the competitors entries are described in section 5.", "labels": [], "entities": []}, {"text": "In section 6 we present and analyse the results for the three different tracks.", "labels": [], "entities": []}, {"text": "Finally we conclude in section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Properties of the treebanks. We report the linguistic annotation method (dependency vs. phrase-structure),  the size of each treebank, the number of types for the different granularities of part-of-speech tags and morphological  features (note that UPOS has a fixed set of 12 tags), and the proportion of word types that were not present in training.", "labels": [], "entities": []}, {"text": " Table 3: Directed accuracy, undirected accuracy and NED results for the dependency task (using supplied POS). The  first column (BC) is our benchmark system, the next seven are participants systems, and the remaining columns consist  of the DMV benchmark and various simple baselines. The superscripts c, p and u denote which type of POS was used,  and S 1 and S 2 denote two different submissions for S\u00f8gaard (2012).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9763798117637634}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9695336818695068}, {"text": "NED", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.547716498374939}, {"text": "BC", "start_pos": 130, "end_pos": 132, "type": "METRIC", "confidence": 0.9595098495483398}, {"text": "DMV benchmark", "start_pos": 242, "end_pos": 255, "type": "DATASET", "confidence": 0.842367023229599}]}, {"text": " Table 4: Directed, undirected and NED accuracy results  for evaluating the predicted dependency structures in the  joint task (i.e., not using supplied POS tags). The first  column is the participant's system and the next three are  DMV models trained on the Brown word clusters (see  section 6.1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9746809601783752}]}, {"text": " Table 6: Directed accuracy results on the Penn treebank,  stratified by dependency relation. For clarity, only 9 im- portant relation types are shown. The vertical bars sepa- rate different groups of relations, from left to right, relat- ing to the main verb, general modifiers and coordination.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.997314989566803}, {"text": "Penn treebank", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9939570128917694}, {"text": "coordination", "start_pos": 283, "end_pos": 295, "type": "METRIC", "confidence": 0.9410252571105957}]}, {"text": " Table 5: Directed accuracy results measured against different conversions of the Penn Treebank into dependency trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.987621009349823}, {"text": "Penn Treebank", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9948304891586304}]}, {"text": " Table 7: One to one, Many to one, VM and VI scores of POS induction results evaluated against fine POS tags (c.f.,  Table 2 which used UPOS).", "labels": [], "entities": [{"text": "VI", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.7897578477859497}, {"text": "POS induction", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.7167830467224121}]}, {"text": " Table 8: Evaluation of the dependency task using a maximum sentence length of 10. See also", "labels": [], "entities": []}]}