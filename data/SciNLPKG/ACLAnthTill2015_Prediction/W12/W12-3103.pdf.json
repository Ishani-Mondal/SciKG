{"title": [{"text": "Semantic Textual Similarity for MT evaluation", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.720159669717153}, {"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9833342432975769}]}], "abstractContent": [{"text": "This paper describes the system used for our participation in the WMT12 Machine Translation evaluation shared task.", "labels": [], "entities": [{"text": "WMT12 Machine Translation evaluation shared task", "start_pos": 66, "end_pos": 114, "type": "TASK", "confidence": 0.8704739709695181}]}, {"text": "We also present anew approach to Machine Translation evaluation based on the recently defined task Semantic Textual Similarity.", "labels": [], "entities": [{"text": "Machine Translation evaluation", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.9276155233383179}, {"text": "Semantic Textual Similarity", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.788965125878652}]}, {"text": "This problem is addressed using a textual entail-ment engine entirely based on WordNet semantic features.", "labels": [], "entities": []}, {"text": "We described results for the Spanish-English, Czech-English and German-English language pairs according to our submission on the Eight Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "Eight Workshop on Statistical Machine Translation", "start_pos": 129, "end_pos": 178, "type": "TASK", "confidence": 0.6959152519702911}]}, {"text": "Our first experiments reports a competitive score to system level.", "labels": [], "entities": []}], "introductionContent": [{"text": "The evaluation of Machine Translation (MT) has become as important as MT itself over the last few years.", "labels": [], "entities": [{"text": "evaluation of Machine Translation (MT)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.8108966478279659}, {"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9045007824897766}]}, {"text": "This is evidenced by the fact that there are now specific forums to present and test new metrics, such as the Workshop for Statistical MT (WMT) or the NIST MetricsMatr.", "labels": [], "entities": [{"text": "Statistical MT (WMT)", "start_pos": 123, "end_pos": 143, "type": "TASK", "confidence": 0.8194666385650635}, {"text": "NIST MetricsMatr", "start_pos": 151, "end_pos": 167, "type": "DATASET", "confidence": 0.8767678141593933}]}, {"text": "Every year avast number of MT metrics are created, the majority being automatic, and seeking to find an efficient, low labor-intensive and reliable evaluation method as an alternative to human-based evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.988614559173584}]}, {"text": "However, relatively few attempts have been made to use semantic information for MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.9755276143550873}]}, {"text": "Moreover, only one work has been published about using semantic equivalence (known as Textual Entailment) of texts for MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.9727427065372467}]}, {"text": "In this work we propose an improved metric, based on TE features, that indicates to what extent a candidate sentence is equivalent to a reference.", "labels": [], "entities": [{"text": "TE", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9473249316215515}]}, {"text": "The paper is organized as follows: Section 2 describes the relevant work done on semantic oriented MT evaluation, Section 3 describes the architecture of the system to compute our metric, then Section 4 relates TE and semantic textual similarity to MT, and Section 5 presents some results obtained with our TE-based metric; and finally Section 6 summarize some conclusions and future work.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.899867981672287}, {"text": "MT", "start_pos": 249, "end_pos": 251, "type": "TASK", "confidence": 0.9061630964279175}]}], "datasetContent": [{"text": "Following this work, propose the family of metrics discourse representation structure (DRS) based on the Discourse Representation Theory of, where a discourse is represented in structure that is essentially a variation of first-order predicate calculus.", "labels": [], "entities": [{"text": "metrics discourse representation structure (DRS)", "start_pos": 43, "end_pos": 91, "type": "TASK", "confidence": 0.6965284901005881}]}, {"text": "These sets of metrics are then used to evaluate poor quality MT, concluding that semantic oriented metrics are more stable at the system level, while at the sentence level their performance decreases (probably due to external factors, for example if a parse tree of the sentence is not available, the metric cannot be computed).", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9833047389984131}]}, {"text": "More recently, Lo and Wu (2011) present anew semi-automated metric, MEANT, that assesses translation utility by matching semantic role fillers.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9865977764129639}, {"text": "translation", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.9023981094360352}]}, {"text": "Their hypothesis is that a good translation is one that lets a reader get the central information of the sentence.", "labels": [], "entities": []}, {"text": "Conceptually, MEANT is defined in terms of f-score, calculated by averaging the translation accuracy for all frames in the MT output across the number of frames in the MT output/reference translations.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9343972206115723}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.8187705278396606}]}, {"text": "To determine the translation accuracy for each semantic role filler in the reference and machine translations, they ask humans to indicate if a role filler translation is correct, incorrect or partially correct, hence being a semi-automatic metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9467241168022156}]}, {"text": "According to MEANT can be run using inexpensive untrained monolingual human judges and yet it correlates with human judgments on adequacy as well as other labor-intensive metrics, such as HTER (), which needs to train humans to find the closest right translation.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.6373672485351562}]}, {"text": "Sagan for MT evaluation is based on a core development to approach the Semantic Textual Similarity task(STS).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9809431731700897}, {"text": "Semantic Textual Similarity task(STS)", "start_pos": 71, "end_pos": 108, "type": "TASK", "confidence": 0.7794684001377651}]}, {"text": "The pilot task STS was recently defined in Semeval 2012 () and has as main objective measuring the degree of semantic equivalence between two text fragments.", "labels": [], "entities": []}, {"text": "STS is related to both Recognizing Textual Entailment (RTE) and Paraphrase Recognition, but has the advantage of being a more suitable model for multiple NLP applications.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 23, "end_pos": 59, "type": "TASK", "confidence": 0.7127155313889185}, {"text": "Paraphrase Recognition", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.8330659568309784}]}, {"text": "As mentioned before, the goal of the RTE task () is determining whether the meaning of a hypothesis H can be inferred from a text T.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 37, "end_pos": 45, "type": "TASK", "confidence": 0.914283961057663}]}, {"text": "Thus, TE is a directional task and we say that T entails H, if a person reading T would infer that H is most likely true.", "labels": [], "entities": [{"text": "TE", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.7730584740638733}]}, {"text": "The difference with STS is that STS consists in determining how similar two text fragments are, in a range from 5 (total semantic equivalence) to 0 (no relation).", "labels": [], "entities": []}, {"text": "Thus, STS mainly differs from TE and Paraphrasing in that the classification is graded instead of binary.", "labels": [], "entities": [{"text": "STS", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9395278096199036}]}, {"text": "In this manner, STS is filling the gap between TE and Paraphrase.", "labels": [], "entities": [{"text": "STS", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.552149772644043}, {"text": "TE", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.904925525188446}]}, {"text": "In view of this, our claim is that the output of MT systems will be more strongly correlated with humans if we have a higher STS score between MT system output and the reference translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9645678400993347}, {"text": "STS score", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9815986156463623}]}, {"text": "To apply Sagan to MT evaluation, we first, preprocess the pairs from Microsoft Research Paraphrase Corpus) with dates and time normalization, and then optional modules are applied depending on the metric we want to calculate.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.9597112536430359}, {"text": "Microsoft Research Paraphrase Corpus", "start_pos": 69, "end_pos": 105, "type": "DATASET", "confidence": 0.8759676069021225}]}, {"text": "Second, we compute 8 sentence level semantic features, and, finally, for every segment generated by systems participating at WMT 2012, we determine the semantic similarity score between that output and the given reference translation.", "labels": [], "entities": [{"text": "WMT 2012", "start_pos": 125, "end_pos": 133, "type": "DATASET", "confidence": 0.7604100406169891}]}, {"text": "The scores are then normalized to a value in the range 0 -1.", "labels": [], "entities": []}, {"text": "For the WMT 2012 we participated in the CzechEnglish and Spanish-English evaluation task but we did not have enough time to extensively test our metric on a diverse range of settings (i.e. different corpora and language pairs), given that it was developed for the STS task, which released the data and results only a couple of months ago.", "labels": [], "entities": [{"text": "WMT 2012", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.6744060516357422}, {"text": "CzechEnglish", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.9727060198783875}]}, {"text": "However, we are now running experiments to get a better picture of the metric's ability to rate translation quality.", "labels": [], "entities": []}, {"text": "In this section we report results obtained by training the system on the WMT 2011 data and testing on the news test portion, only for the Spanish-English pair.", "labels": [], "entities": [{"text": "WMT 2011 data", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.9727410078048706}]}, {"text": "Although the system handles both SVM with regression and MLP classifiers, well known to have good performance on natural language applications, we only submit the results obtained using SVM with regression due to previous experiments that consistently showed higher accuracy using SVM instead of MLP.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 266, "end_pos": 274, "type": "METRIC", "confidence": 0.9951260685920715}]}, {"text": "At the system level, we calculated the Spearman Rank Correlation Coefficient (\u03c1) to compare our metric's behavior with respect to the human based metric applied in WMT 2011.", "labels": [], "entities": [{"text": "Spearman Rank Correlation Coefficient (\u03c1)", "start_pos": 39, "end_pos": 80, "type": "METRIC", "confidence": 0.9038610884121486}, {"text": "WMT 2011", "start_pos": 164, "end_pos": 172, "type": "DATASET", "confidence": 0.8795020282268524}]}, {"text": "The result is \u03c1 = 0.96 indicating a strong positive correlation.", "labels": [], "entities": [{"text": "\u03c1", "start_pos": 14, "end_pos": 15, "type": "METRIC", "confidence": 0.970195472240448}]}, {"text": "Moreover, we successfully reproduce the systems ranking given by humans regarding the best and worst systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Sagan's score for ES-EN WMT 2011 news test  set.", "labels": [], "entities": [{"text": "ES-EN WMT 2011 news test  set", "start_pos": 28, "end_pos": 57, "type": "DATASET", "confidence": 0.9383490780989329}]}]}