{"title": [], "abstractContent": [{"text": "Probabilistic context-free grammars (PCFGs) area popular cognitive model of syntax (Ju-rafsky, 1996).", "labels": [], "entities": []}, {"text": "These can be formulated to be sensitive to human working memory constraints by application of a right-corner transform (Schuler, 2009).", "labels": [], "entities": []}, {"text": "One side-effect of the transform is that it guarantees at most a single expansion (push) and at most a single reduction (pop) during a syntactic parse.", "labels": [], "entities": []}, {"text": "The primary finding of this paper is that this property of right-corner parsing can be exploited to obtain a dramatic reduction in the number of random variables in a probabilistic sequence model parser.", "labels": [], "entities": []}, {"text": "This yields a simpler structure that more closely resembles existing simple recurrent network models of sentence comprehension .", "labels": [], "entities": []}], "introductionContent": [{"text": "There maybe a benefit to using insights from human cognitive modelling in parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 74, "end_pos": 81, "type": "TASK", "confidence": 0.9753322005271912}]}, {"text": "Evidence for incremental processing can be seen in garden pathing, close shadowing, and eyetracking studies (, which show humans begin attempting to process a sentence immediately upon receiving linguistic input.", "labels": [], "entities": []}, {"text": "In the cognitive science community, this incremental interaction has often been modelled using recurrent neural networks, which utilize a hidden context with a severely bounded representational capacity (a fixed number of continuous units or dimensions), similar to models of activationbased memory in the prefrontal cortex, with the interesting possibility that the distributed behavior of neural columns) may directly implement continuous dimensions of recurrent hidden units.", "labels": [], "entities": []}, {"text": "This paper presents a refinement of a factored probabilistic sequence model of comprehension in the direction of a recurrent neural network model and presents some observed efficiencies due to this refinement.", "labels": [], "entities": []}, {"text": "This paper will adopt an incremental probabilistic context-free grammar (PCFG) parser) that uses a right-corner variant of the leftcorner parsing strategy coupled with strict memory bounds, as a model of human-like parsing.", "labels": [], "entities": []}, {"text": "Syntax can readily be approximated using simple PCFGs, which can be easily tuned.", "labels": [], "entities": []}, {"text": "This paper will show that this representation can be streamlined to exploit the fact that a right-corner parse guarantees at most one expansion and at most one reduction can take place after each word is seen (see Section 2.2).", "labels": [], "entities": []}, {"text": "The primary finding of this paper is that this property of right-corner parsing can be exploited to obtain a dramatic reduction in the number of random variables in a probabilistic sequence model parser) yielding a simpler structure that more closely resembles connectionist models such as TRACE), Shortlist, or recurrent models) which posit functional units only for cognitively-motivated entities.", "labels": [], "entities": [{"text": "Shortlist", "start_pos": 298, "end_pos": 307, "type": "DATASET", "confidence": 0.9179015755653381}]}, {"text": "The rest of this paper is structured as follows: Section 2 gives the formal background of the rightcorner parser transform and probabilistic sequence 51 model parsing.", "labels": [], "entities": [{"text": "probabilistic sequence 51 model parsing", "start_pos": 127, "end_pos": 166, "type": "TASK", "confidence": 0.6341678857803345}]}, {"text": "The simplification of this model is described in Section 3.", "labels": [], "entities": []}, {"text": "A discussion of the interplay between cognitive theory and computational modelling in the resulting model maybe found in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 demonstrates that such factoring also yields large benefits in the speed of probabilistic sequence model parsing.", "labels": [], "entities": [{"text": "probabilistic sequence model parsing", "start_pos": 95, "end_pos": 131, "type": "TASK", "confidence": 0.5791377946734428}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. While the speed is not state-of-the-art in  the field of parsing at large, it does break new ground  for factored sequence model parsers.", "labels": [], "entities": [{"text": "speed", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9830238819122314}, {"text": "parsing", "start_pos": 67, "end_pos": 74, "type": "TASK", "confidence": 0.9704824686050415}, {"text": "factored sequence model parsers", "start_pos": 115, "end_pos": 146, "type": "TASK", "confidence": 0.588503934442997}]}, {"text": " Table 1: Speed comparison with an unfactored proba- bilistic sequence model using a beam-width of 500 ele- ments", "labels": [], "entities": []}]}