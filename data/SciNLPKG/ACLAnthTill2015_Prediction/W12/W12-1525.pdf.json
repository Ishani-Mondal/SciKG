{"title": [{"text": "The Surface Realisation Task: Recent Developments and Future Plans", "labels": [], "entities": [{"text": "Surface Realisation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8673661351203918}]}], "abstractContent": [{"text": "The Surface Realisation Shared Task was first run in 2011.", "labels": [], "entities": [{"text": "Surface Realisation Shared Task", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.847088485956192}]}, {"text": "Two common-ground input representations were developed and for the first time several independently developed surface realisers produced realisations from the same shared inputs.", "labels": [], "entities": []}, {"text": "However, the input representations had several shortcomings which we have been aiming to address in the time since.", "labels": [], "entities": []}, {"text": "This paper reports on our work to date on improving the input representations and on our plans for the next edition of the SR Task.", "labels": [], "entities": [{"text": "SR Task", "start_pos": 123, "end_pos": 130, "type": "TASK", "confidence": 0.6660544276237488}]}, {"text": "We also briefly summarise other related developments in NLG shared tasks and outline how the different ideas maybe usefully brought together in the future.", "labels": [], "entities": [{"text": "NLG shared tasks", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.5284297664960226}]}], "introductionContent": [{"text": "The Surface Realisation (SR) Task was introduced as anew shared task at Generation Challenges 2011).", "labels": [], "entities": [{"text": "Surface Realisation (SR) Task", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.8338564137617747}]}, {"text": "Our aim in developing the SR Task was to make it possible, for the first time, to directly compare different, independently developed surface realisers by developing a 'common-ground' representation that could be used by all participating systems as input.", "labels": [], "entities": [{"text": "SR Task", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.7788987755775452}]}, {"text": "In fact, we created two different input representations, one shallow, one deep, in order to enable more teams to participate.", "labels": [], "entities": []}, {"text": "Correspondingly, there were two tracks in SR'11: In the Shallow Track, the task was to map from shallow syntax-level input representations to realisations; in the Deep Track, the task was to map from deep semantics-level input representations to realisations.", "labels": [], "entities": [{"text": "SR'11", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.8823328018188477}]}, {"text": "By the time teams submitted their system outputs, it had become clear that the inputs required by some types of surface realisers were more easily derived from the common-ground representation than the inputs required by other types.", "labels": [], "entities": []}, {"text": "There were other respects in which the representations were not ideal, e.g. the deep representations retained too many syntactic elements as stopgaps where no deeper information had been available.", "labels": [], "entities": []}, {"text": "It was clear that the input representations had to be improved for the next edition of the SR Task.", "labels": [], "entities": [{"text": "SR Task", "start_pos": 91, "end_pos": 98, "type": "TASK", "confidence": 0.8114116191864014}]}, {"text": "In this paper, we report on our work in this direction so far and relate it to some new shared task proposals which have been developed in part as a response to the above difficulties.", "labels": [], "entities": []}, {"text": "We discuss how these developments might usefully be integrated, and outline plans for SR'13, the next edition of the SR Task.", "labels": [], "entities": [{"text": "SR'13", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.7884489297866821}, {"text": "SR Task", "start_pos": 117, "end_pos": 124, "type": "TASK", "confidence": 0.5476035177707672}]}], "datasetContent": [{"text": "We will once again follow the main data set divisions of the CoNLL'08 data (training set = WSJ Sections 02-21; development set = Section 24; test set = Section 23), with the proviso that we have removed 300 randomly selected sentences from the development set for use inhuman evaluations.", "labels": [], "entities": [{"text": "CoNLL'08 data", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9692334234714508}, {"text": "WSJ Sections 02-21", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.8053585688273112}]}, {"text": "Of these, we used 100 sentences in SR'11 and will use a different 100 in SR'13.", "labels": [], "entities": [{"text": "SR'11", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.9287171363830566}, {"text": "SR'13", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.9642192125320435}]}, {"text": "Evaluation criteria identified as important for evaluation of surface realisation output in previous work include Adequacy (preservation of meaning), Fluency (grammaticality/idiomaticity), Clarity, Humanlikeness and Task Effectiveness.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 150, "end_pos": 157, "type": "METRIC", "confidence": 0.9718345999717712}]}, {"text": "We will aim to evaluate system outputs submitted by SR'13 participants in terms of most of these criteria, using both automatic and human-assessed methods.", "labels": [], "entities": []}, {"text": "As in SR'11, the automatic evaluation metrics (assessing Humanlikeness) will be BLEU, NIST, TER and possibly METEOR.", "labels": [], "entities": [{"text": "SR'11", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.8619467616081238}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9993181228637695}, {"text": "NIST", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.8444806337356567}, {"text": "TER", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9969745874404907}, {"text": "METEOR", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9475330710411072}]}, {"text": "We will apply text normalisation to system outputs before scoring them with the automatic metrics.", "labels": [], "entities": [{"text": "text normalisation", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.6994257718324661}]}, {"text": "For n-best ranked system outputs, we will again compute a single score for all outputs by computing their weighted sum of their individual scores, where a weight is assigned to a system output in inverse proportion to its rank.", "labels": [], "entities": []}, {"text": "For a subset of the test data we may obtain additional alternative realisations via Mechanical Turk for use in the automatic evaluations.", "labels": [], "entities": []}, {"text": "We are planning to expand the range of humanassessed evaluation experiments (assessing Adequacy, Fluency and Clarity) to the following methods: 1.", "labels": [], "entities": [{"text": "Adequacy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9785872101783752}, {"text": "Fluency", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9605180025100708}]}, {"text": "Preference Judgement Experiment (C2, C3): Collect preference judgements using an existing evaluation interface ( and directly recruited evaluators.", "labels": [], "entities": [{"text": "Preference Judgement", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7255109548568726}]}, {"text": "We will present sentences in the context of a chunk of 5 consecutive sentences to the evaluators, and ask for separate judgements for Clarity, Fluency and Meaning Similarity.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.9201579689979553}]}, {"text": "2. HTER (): In this evaluation method, human evaluators are asked to postedit the output of a system, and the edits are then categorised and counted.", "labels": [], "entities": [{"text": "HTER", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.8389027714729309}]}, {"text": "Crucial to this evaluation method is the construction of clear instructions for evaluators and the categorisation of edits.", "labels": [], "entities": []}, {"text": "We will categorise edits as relating to Meaning Similarity, Fluency and/or Clarity; we will also consider further subcategorisations.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.942957878112793}]}, {"text": "We will once again provide evaluation scripts to participants so they can perform automatic evaluations on the development data.", "labels": [], "entities": []}, {"text": "These scores serve two purposes.", "labels": [], "entities": []}, {"text": "Firstly, development data scores must be included in participants' reports.", "labels": [], "entities": []}, {"text": "Secondly, partici-pants may wish to use the evaluation scripts in developing and tuning their systems.", "labels": [], "entities": []}, {"text": "We will report per-system results separately for the automatic metrics (4 sets of results), and for the human-assessed measures (2 sets of results).", "labels": [], "entities": []}, {"text": "For each set of results, we will report single-best and n-best results.", "labels": [], "entities": []}, {"text": "For single-best results, we may furthermore report results both with and without missing outputs.", "labels": [], "entities": []}, {"text": "We will rank systems, and report significance of pairwise differences using bootstrap resampling where necessary).", "labels": [], "entities": []}, {"text": "We will separately report correlation between human and automatic metrics, and between different automatic metrics.", "labels": [], "entities": []}], "tableCaptions": []}