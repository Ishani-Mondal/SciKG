{"title": [{"text": "Generative Constituent Parsing and Discriminative Dependency Reranking: Experiments on English and French", "labels": [], "entities": [{"text": "Generative Constituent Parsing and Discriminative Dependency Reranking", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.7051078123705727}]}], "abstractContent": [{"text": "We present an architecture for parsing in two steps.", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9719645380973816}]}, {"text": "A phrase-structure parser builds for each sentence an n-best list of analyses which are converted to dependency trees.", "labels": [], "entities": []}, {"text": "These dependency structures are then rescored by a dis-criminative reranker.", "labels": [], "entities": []}, {"text": "Our method is language agnostic and enables the incorporation of additional information which are useful for the choice of the best parse candidate.", "labels": [], "entities": []}, {"text": "We test our approach on the the Penn Treebank and the French Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9935809075832367}, {"text": "French Treebank", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.992010772228241}]}, {"text": "Evaluation shows a sig-nificative improvement on different parse met-rics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Two competing approaches exist for parsing natural language.", "labels": [], "entities": [{"text": "parsing natural language", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.9125000437100729}]}, {"text": "The first one, called generative, is based on the theory of formal languages and rewriting systems.", "labels": [], "entities": [{"text": "generative", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.9686374068260193}]}, {"text": "Parsing is defined here as a process that transforms a string into a tree or a tree forest.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9561111927032471}]}, {"text": "It is often grounded on phrase-based grammars -although there are generative dependency parsers -in particular context-free grammars or one of their numerous variants, that can be parsed in polynomial time.", "labels": [], "entities": [{"text": "generative dependency parsers", "start_pos": 66, "end_pos": 95, "type": "TASK", "confidence": 0.6294139325618744}]}, {"text": "However, the independence hypothesis that underlies this kind of formal system does not allow for precise analyses of some linguistic phenomena, such as long distance and lexical dependencies.", "labels": [], "entities": []}, {"text": "In the second approach, known as discriminative, the grammar is viewed as a system of constraints over the correct syntactic structures, the words of the sentence themselves being seen as constraints over the position they occupy in the sentence.", "labels": [], "entities": []}, {"text": "Parsing boils down to finding a solution that is compatible with the different constraints.", "labels": [], "entities": []}, {"text": "The major problem of this approach lies in its complexity.", "labels": [], "entities": []}, {"text": "The constraints can, theoretically, range over any aspect of the final structures, which prevents from using efficient dynamic programming techniques when searching fora global solution.", "labels": [], "entities": []}, {"text": "In the worst case, final structures must be enumerated in order to be evaluated.", "labels": [], "entities": []}, {"text": "Therefore, only a subset of constraints is used in implementations for complexity reasons.", "labels": [], "entities": []}, {"text": "This approach can itself be divided into formalisms relying on logic to describe constraints, as the model-theoretic syntax), or numerical formalisms that associate weights to lexico-syntactic substructures.", "labels": [], "entities": []}, {"text": "The latter has been the object of some recent work thanks to progresses achieved in the field of Machine Learning.", "labels": [], "entities": [{"text": "Machine Learning", "start_pos": 97, "end_pos": 113, "type": "TASK", "confidence": 0.8495076894760132}]}, {"text": "A parse tree is represented as a vector of features and its accuracy is measured as the distance between this vector and the reference.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9993174076080322}]}, {"text": "One way to take advantage of both approaches is to combine them sequentially, as initially proposed by.", "labels": [], "entities": []}, {"text": "A generative parser produces a set of candidates structures that constitute the input of a second, discriminative module, whose search space is limited to this set of candidates.", "labels": [], "entities": [{"text": "generative parser", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.9096927642822266}]}, {"text": "Such an approach, parsing followed by reranking, is used in the Brown parser).", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9795917868614197}]}, {"text": "The approach can be extended in order to feed the reranker with the output of different parsers, as shown by.", "labels": [], "entities": []}, {"text": "In this paper we are interested in applying reranking to dependency structures.", "labels": [], "entities": []}, {"text": "The main reason is that many linguistic constraints are straightforward to implement on dependency structures, as, for example, subcategorization frames or selectional constraints that are closely linked to the notion of de-89 pendents of a predicate.", "labels": [], "entities": []}, {"text": "On the other hand, dependencies extracted from constituent parses are known to be more accurate than dependencies obtained from dependency parsers.", "labels": [], "entities": []}, {"text": "Therefore the solution we choose is an indirect one: we use a phrase-based parser to generate n-best lists and convert them to lists of dependency structures that are reranked.", "labels": [], "entities": []}, {"text": "This approach can be seen as trade-off between phrasebased reranking experiments) and the approach of where a discriminative model is used to score lexical features representing unlabelled dependencies in the Tree Adjoining Grammar formalism.", "labels": [], "entities": []}, {"text": "Our architecture, illustrated in, is based on two steps.", "labels": [], "entities": []}, {"text": "During the first step, a syntagmatic parser processes the input sentence and produces nbest parses as well as their probabilities.", "labels": [], "entities": []}, {"text": "They are annotated with a functional tagger which tags syntagms with standard syntactic functions subject, object, indirect object . .", "labels": [], "entities": []}, {"text": ". and converted to dependency structures by application of percolation rules.", "labels": [], "entities": []}, {"text": "In the second step, we extract a set of features from the dependency parses and the associated probabilities.", "labels": [], "entities": []}, {"text": "These features are used to reorder the n-best list and select a potentially more accurate parse.", "labels": [], "entities": []}, {"text": "Syntagmatic parses are produced by the implementation of a PCFG-LA parser of (, similar to), a functional tagger and dependency converter for the target language.", "labels": [], "entities": []}, {"text": "The reranking model is a linear model trained with an implementation of the MIRA algorithm) . Charniak and Johnson (2005) and Collins (2000) rerank phrase-structure parses and they also include head-dependent information, in other words unlabelled dependencies.", "labels": [], "entities": []}, {"text": "In our approach we take into account grammatical functions or labelled dependencies.", "labels": [], "entities": []}, {"text": "It should be noted that the features we use are very generic and do not depend on the linguistic knowledge of the authors.", "labels": [], "entities": []}, {"text": "We applied our method to English, the de facto standard for testing parsing technologies, and French which exhibits many aspects of a morphologically rich language.", "labels": [], "entities": []}, {"text": "But our approach could be applied to other languages, provided that the resources -treebanks and conversion tools -exist.", "labels": [], "entities": []}, {"text": "(1) PCFG-LA n-best constituency parses The parsing architecture: production of the nbest syntagmatic trees (1) tagged with functional labels (2), conversion to a dependency structure (3) and feature extraction (4), scoring with a linear model (5).", "labels": [], "entities": [{"text": "PCFG-LA n-best constituency parses", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.7430666387081146}, {"text": "feature extraction", "start_pos": 191, "end_pos": 209, "type": "TASK", "confidence": 0.67451311647892}]}, {"text": "The parse with the best score is considered as final.", "labels": [], "entities": []}, {"text": "The structure of the paper is the following: in Section 2 we describe the details of our generative parser and in Section 3 our reranking model together with the features templates.", "labels": [], "entities": []}, {"text": "Section 4 reports the results of the experiments conducted on the Penn Treebank as well as on the Paris 7 Treebank () and Section 5 concludes the paper.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9933830201625824}, {"text": "Paris 7 Treebank", "start_pos": 98, "end_pos": 114, "type": "DATASET", "confidence": 0.9828324913978577}]}], "datasetContent": [{"text": "In this section, we evaluate our architecture on two corpora, namely the Penn Treebank ( and the French Treebank (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.9951266646385193}, {"text": "French Treebank", "start_pos": 97, "end_pos": 112, "type": "DATASET", "confidence": 0.9912124574184418}]}, {"text": "We first present the corpora and the tools used for annotating and converting structures, then the performances of the phrase structure parser alone and with the discriminative reranker.", "labels": [], "entities": []}, {"text": "In order to test our system we first tried to evaluate the impact of the length of the n-best list over the reranking predictions . The results are shown in, parts (c) and (d).", "labels": [], "entities": []}, {"text": "For French, we can see that even though the LAS and UAS are consistently improving with the number of candidates, the F-score is maximal with 50 candidates.", "labels": [], "entities": [{"text": "LAS", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.7771127223968506}, {"text": "UAS", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.8936033844947815}, {"text": "F-score", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.9989938139915466}]}, {"text": "However the difference between 50 candidates and 100 candidates is not statistically significant.", "labels": [], "entities": []}, {"text": "For English, the situation is simpler and scores improve continuously on the three metrics.", "labels": [], "entities": []}, {"text": "Finally we run our system on the test sets for both treebanks.", "labels": [], "entities": []}, {"text": "Results are shown 8 in for English, and for French.", "labels": [], "entities": []}, {"text": "For English the improvement is 0.9% LAS, 0.7% Parseval F-score and The model is always trained with 100 candidates.", "labels": [], "entities": [{"text": "LAS", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9987156391143799}, {"text": "Parseval", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9979111552238464}, {"text": "F-score", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.5089079141616821}]}, {"text": "8 F < 40 is the parseval F-score for sentences with less than 40 words.", "labels": [], "entities": [{"text": "F < 40", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9108113447825114}, {"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.8386760354042053}]}], "tableCaptions": [{"text": " Table 1: System results on PTB Test set", "labels": [], "entities": [{"text": "PTB Test", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.9602145552635193}]}, {"text": " Table 2: System results on FTB Test set", "labels": [], "entities": [{"text": "FTB Test set", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9537112315495809}]}, {"text": " Table 3: Comparison on PTB Test set", "labels": [], "entities": [{"text": "PTB Test set", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9637488722801208}]}, {"text": " Table 4: Comparison on FTB Test set", "labels": [], "entities": [{"text": "FTB Test set", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9581521352132162}]}, {"text": " Table 5: Repartition of weight (in percentage) in the  1,000 highest (+) and lowest (-) weighted features for En- glish and French.", "labels": [], "entities": [{"text": "Repartition of weight (in percentage)", "start_pos": 10, "end_pos": 47, "type": "METRIC", "confidence": 0.8132138933454242}]}]}