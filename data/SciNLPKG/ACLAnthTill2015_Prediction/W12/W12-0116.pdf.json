{"title": [{"text": "Linguistically-Augmented Bulgarian-to-English Statistical Machine Translation Model", "labels": [], "entities": [{"text": "Linguistically-Augmented Bulgarian-to-English Statistical Machine Translation", "start_pos": 0, "end_pos": 77, "type": "TASK", "confidence": 0.5009272158145904}]}], "abstractContent": [{"text": "In this paper, we present our linguistically-augmented statistical machine translation model from Bulgarian to English, which combines a statistical machine translation (SMT) system (as backbone) with deep linguistic features (as factors).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.6369248827298483}, {"text": "statistical machine translation (SMT)", "start_pos": 137, "end_pos": 174, "type": "TASK", "confidence": 0.7840378781159719}]}, {"text": "The motivation is to take advantages of the robust-ness of the SMT system and the linguistic knowledge of morphological analysis and the hand-crafted grammar through system combination approach.", "labels": [], "entities": [{"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9859230518341064}]}, {"text": "The preliminary evaluation has shown very promising results in terms of BLEU scores (38.85) and the manual analysis also confirms the high quality of the translation the system delivers .", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9995653033256531}]}], "introductionContent": [{"text": "In the recent years, machine translation (MT) has achieved significant improvement in terms of translation quality.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8889071226119996}]}, {"text": "Both data-driven approaches (e.g., statistical MT (SMT)) and knowledge-based (e.g., rule-based MT (RBMT)) have achieved comparable results shown in the evaluation campaigns).", "labels": [], "entities": [{"text": "statistical MT (SMT))", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.7134325981140137}]}, {"text": "However, according to the human evaluation, the final outputs of the MT systems are still far from satisfactory.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9840337038040161}]}, {"text": "Fortunately, recent error analysis shows that the two trends of the MT approaches tend to be complementary to each other, in terms of the types of the errors they made.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9815884828567505}]}, {"text": "Roughly speaking, RBMT systems often have missing lexicon and thus lack of robustness, while handling linguistic phenomena requiring syntactic information better.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.9528475999832153}]}, {"text": "SMT systems, on the contrary, are in general more robust, but sometimes output ungrammatical sentences.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9769103527069092}]}, {"text": "In fact, instead of competing with each other, there is also a line of research trying to combine the advantages of the two sides using a hybrid framework.", "labels": [], "entities": []}, {"text": "Although many systems can be put under the umbrella of \"hybrid\" systems, there are various ways to do the combination/integration.", "labels": [], "entities": []}, {"text": "summarized several different architectures of hybrid systems using SMT and RBMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9145123958587646}]}, {"text": "Some widely used ones are: 1) using an SMT to post-edit the outputs of an RBMT; 2) selecting the best translations from several hypotheses coming from different SMT/RBMT systems; and 3) selecting the best segments (phrases or words) from different hypotheses.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9486761093139648}]}, {"text": "For the language pair Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc.", "labels": [], "entities": []}, {"text": "There was a system published by, which was trained and tested on the European Union law data, but not on other domains like news.", "labels": [], "entities": [{"text": "European Union law data", "start_pos": 69, "end_pos": 92, "type": "DATASET", "confidence": 0.8301076740026474}]}, {"text": "They reported a very high BLEU score () on the BulgarianEnglish translation direction (61.3), which inspired us to further investigate this direction.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.984527051448822}, {"text": "BulgarianEnglish translation direction", "start_pos": 47, "end_pos": 85, "type": "DATASET", "confidence": 0.9241319497426351}]}, {"text": "In this paper, we focus on the Bulgarian-toEnglish translation and mainly explore the approach of annotating the SMT baseline with linguistic features derived from the preprocessing and hand-crafted grammars.", "labels": [], "entities": [{"text": "Bulgarian-toEnglish translation", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.6415686309337616}, {"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9673255085945129}]}, {"text": "There are three motivations behind our approach: 1) the SMT baseline trained on a decent amount of parallel corpora outputs surprisingly good results, in terms of both statistical evaluation metrics and preliminary manual evaluation; 2) the augmented model gives 119 us more space for experimenting with different linguistic features without losing the 'basic' robustness; 3) the MT system can profit from continued advances in the development of the deep grammars thereby opening up further integration possibilities.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9845861196517944}, {"text": "MT", "start_pos": 380, "end_pos": 382, "type": "TASK", "confidence": 0.974085807800293}]}, {"text": "The rest of the paper will be organized as follows: Section 2 presents our work on cleaning the corpora and Section 3 briefly describes the preprocessing of the data.", "labels": [], "entities": []}, {"text": "Section 4 introduces our factor-based SMT model which allows us to incorporate various linguistic features into an SMT baseline, among which those features coming from the MRS are described in Section 5 in detail.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9651488661766052}, {"text": "SMT", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9644516706466675}]}, {"text": "We show our experiments in Section 6 as well as both automatic and manual evaluation of the results.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 27, "end_pos": 36, "type": "DATASET", "confidence": 0.8581633269786835}]}, {"text": "Section 7 briefly mentions some related work and then we summarize this paper in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "To run the experiments, we use the phrase-based translation model provided by the open-source statistical machine translation system, Moses 4 ( ).", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.677073061466217}, {"text": "statistical machine translation", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.6840638518333435}]}, {"text": "For training the translation model, the parallel corpora (mentioned in Section 2) were preprocessed with the tokenizer and lowercase converter provided by Moses.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.975245475769043}]}, {"text": "Then the procedure is quite standard: \u2022 We run GIZA++ (Och and Ney, 2003) for bidirectional word alignment, and then obtain the lexical translation table and phrase table.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.6959943324327469}]}, {"text": "\u2022 A tri-gram language model is estimated using the SRILM toolkit).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.8936613500118256}]}, {"text": "\u2022 Minimum error rate training (MERT)) is applied to tune the weights for the set of feature weights that maximizes the official f-score evaluation metric on the development set.", "labels": [], "entities": [{"text": "Minimum error rate training (MERT))", "start_pos": 2, "end_pos": 37, "type": "METRIC", "confidence": 0.9009510278701782}]}, {"text": "The rest of the parameters we use the default setting provided by Moses.", "labels": [], "entities": []}, {"text": "http://www.statmt.org/moses/ We split the corpora into the training set, the development set and the test set.", "labels": [], "entities": []}, {"text": "For SETIMES, the split is 100,000/500/1,000 and for EMEA, it is 700,000/500/1,000.", "labels": [], "entities": [{"text": "SETIMES", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.7587059736251831}, {"text": "split", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.9941549897193909}, {"text": "EMEA", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.875388503074646}]}, {"text": "For reference, we also run tests on the JRC-Acquis corpus   As we mentioned before, the EMEA corpus is mainly about the description of medicine usage, and the format is quite fixed.", "labels": [], "entities": [{"text": "JRC-Acquis corpus", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.9416966140270233}, {"text": "EMEA corpus", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.9124574065208435}, {"text": "description of medicine usage", "start_pos": 120, "end_pos": 149, "type": "TASK", "confidence": 0.7856605499982834}]}, {"text": "Therefore, it is not surprising to see high performance on the in-domain test (2nd row in).", "labels": [], "entities": []}, {"text": "SETIMES, consisting of news articles, is in a less controlled setting.", "labels": [], "entities": [{"text": "SETIMES", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.6699289083480835}]}, {"text": "The BLEU score is lower . The results on the out-of-domain tests are in general much lower with a drop of more than 60% in BLEU score (the last column).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9814001023769379}, {"text": "BLEU score", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.9862811267375946}]}, {"text": "For the JRC-Acquis corpus, in contrast to the in-domain scores given by, the low out-of-domain results shows a very similar situation as EMEA.", "labels": [], "entities": [{"text": "JRC-Acquis corpus", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.9060945212841034}, {"text": "EMEA", "start_pos": 137, "end_pos": 141, "type": "DATASET", "confidence": 0.8781599402427673}]}, {"text": "A brief manual check of the results indicate that the outof-domain tests suffer severely from the missing lexicon, while the in-domain test for the news articles contains more interesting issues to look into.", "labels": [], "entities": []}, {"text": "The better translation quality also makes the system outputs human readable.", "labels": [], "entities": []}, {"text": "As we described the factor-based model in Section 4, we also perform experiments to test the effectiveness of different linguistic annotations.", "labels": [], "entities": []}, {"text": "The different configurations we considered are shown in the first column of.", "labels": [], "entities": []}, {"text": "These models can be roughly grouped into five categories: word form with linguistic features; lemma with linguistic features; models with dependency features; MRS elementary predicates (EP) and the type of the main argument of the predicate (EOV); and MRS features without word forms.", "labels": [], "entities": [{"text": "MRS elementary predicates", "start_pos": 159, "end_pos": 184, "type": "TASK", "confidence": 0.6995872259140015}, {"text": "MRS", "start_pos": 252, "end_pos": 255, "type": "TASK", "confidence": 0.8867620825767517}]}, {"text": "The setting of the system is mostly the same as the previous experiment, except for 1) increasing the training data from 100,000 to 150,000 sentence pairs; 2) specifying the factors during training and decoding; and 3) without doing MERT . We perform the finer-grained model only on the SETIMES data, as the language is more diverse (compared to the other two corpora).", "labels": [], "entities": [{"text": "MERT", "start_pos": 233, "end_pos": 237, "type": "METRIC", "confidence": 0.957664430141449}, {"text": "SETIMES data", "start_pos": 287, "end_pos": 299, "type": "DATASET", "confidence": 0.8401561379432678}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The first model is served as the baseline here.", "labels": [], "entities": []}, {"text": "We show all the n-gram scores besides the final BLEU, since the some of the differences are very small.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9905710220336914}]}, {"text": "In terms of the numbers, POS seems to bean effective factor, as Model 2 has the highest score.", "labels": [], "entities": [{"text": "POS", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9934816360473633}]}, {"text": "Model 3 indicates that linguistic features also improve the performance.", "labels": [], "entities": []}, {"text": "Model 4-6 show the necessity of including the word form as one of the factors, in terms of BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9989728927612305}]}, {"text": "Model 10 shows significant decrease after incorporating HLEMMA feature.", "labels": [], "entities": []}, {"text": "This maybe due to the data sparsity, as we are actually aligning and translating bi-grams instead of tokens.", "labels": [], "entities": []}, {"text": "This may also indicate that increasing the number of factors does not guarantee performance enhancement.", "labels": [], "entities": []}, {"text": "After replacing the HLEMMA with HPOS, the result is close to the others (Model 8).", "labels": [], "entities": [{"text": "HLEMMA", "start_pos": 20, "end_pos": 26, "type": "DATASET", "confidence": 0.7137895822525024}, {"text": "HPOS", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.9333006739616394}]}, {"text": "The experiments with features from the MRS analyses ( show improvements over the baseline consistently and using only the MRS features (Model 17-18) also delivers descent results.", "labels": [], "entities": [{"text": "MRS", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8017208576202393}]}, {"text": "In future experiments we will consider to include more feature from the MRS analyses.", "labels": [], "entities": [{"text": "MRS", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.8155495524406433}]}, {"text": "So far, incorporating additional linguistic knowledge has not shown huge improvement in terms of statistical evaluation metrics.", "labels": [], "entities": []}, {"text": "However, this does not mean that the translations delivered are the same.", "labels": [], "entities": []}, {"text": "In order to fully evaluate the system, manual analysis is absolutely necessary.", "labels": [], "entities": []}, {"text": "We are still far from drawing a conclusion at this point, but the preliminary scores calculated already indicate that the system can deliver decent translation quality consistently.", "labels": [], "entities": [{"text": "translation", "start_pos": 148, "end_pos": 159, "type": "TASK", "confidence": 0.9496177434921265}]}, {"text": "We manually validated the output for all the models mentioned in.", "labels": [], "entities": []}, {"text": "The guideline includes two aspects of the quality of the translation: Grammaticality and Content.", "labels": [], "entities": []}, {"text": "Grammaticality can be evaluated solely on the system output and Content by comparison with the reference translation.", "labels": [], "entities": []}, {"text": "We use a 1-5 score for each aspect as follows: Grammaticality 1.", "labels": [], "entities": []}, {"text": "The translation is not understandable.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9615666270256042}]}, {"text": "2. The evaluator can somehow guess the meaning, but cannot fully understand the whole text.", "labels": [], "entities": []}, {"text": "3. The translation is understandable, but with some efforts.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.980348527431488}]}, {"text": "4. The translation is quite fluent with some minor mistakes or re-ordering of the words.", "labels": [], "entities": []}, {"text": "5. The translation is perfectly readable and grammatical.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of the baseline SMT system  (Bulgarian-English)", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9727036356925964}]}, {"text": " Table 3: Results of the factor-based model (Bulgarian-English, SETIMES 150,000)", "labels": [], "entities": [{"text": "SETIMES 150,000)", "start_pos": 64, "end_pos": 80, "type": "DATASET", "confidence": 0.7478687167167664}]}, {"text": " Table 5: Manual evaluation of the content", "labels": [], "entities": []}]}