{"title": [{"text": "Using Syntactic Head Information in Hierarchical Phrase-Based Translation", "labels": [], "entities": [{"text": "Hierarchical Phrase-Based Translation", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.596958726644516}]}], "abstractContent": [{"text": "Chiang's hierarchical phrase-based (HPB) translation model advances the state-of-the-art in statistical machine translation by expanding conventional phrases to hierarchical phrases-phrases that contain sub-phrases.", "labels": [], "entities": [{"text": "phrase-based (HPB) translation", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.6964418053627014}, {"text": "statistical machine translation", "start_pos": 92, "end_pos": 123, "type": "TASK", "confidence": 0.6410958170890808}]}, {"text": "However , the original HPB model is prone to over-generation due to lack of linguistic knowledge: the grammar may suggest more derivations than appropriate, many of which may lead to ungrammatical translations.", "labels": [], "entities": []}, {"text": "On the other hand, limitations of glue grammar rules in the original HPB model may actually prevent systems from considering some reasonable derivations.", "labels": [], "entities": []}, {"text": "This paper presents a simple but effective translation model, called the Head-Driven HPB (HD-HPB) model, which incorporates head information in translation rules to better capture syntax-driven information in a derivation.", "labels": [], "entities": []}, {"text": "In addition, unlike the original glue rules, the HD-HPB model allows improved reordering between any two neighboring non-terminals to explore a larger reordering search space.", "labels": [], "entities": []}, {"text": "An extensive set of experiments on Chinese-English translation on four NIST MT test sets, using both a small and a large training set, show that our HD-HPB model consistently and statistically significantly outperforms Chiang's model as well as a source side SAMT-style model.", "labels": [], "entities": [{"text": "NIST MT test sets", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.9238793402910233}]}], "introductionContent": [{"text": "Chiang's hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation and has been widely adopted in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "phrase-based (HPB) translation", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.6773794174194336}, {"text": "translation derivation", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.9301728308200836}, {"text": "statistical machine translation (SMT)", "start_pos": 166, "end_pos": 203, "type": "TASK", "confidence": 0.7905056824286779}]}, {"text": "Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion.", "labels": [], "entities": []}, {"text": "However, due to lack of linguistic knowledge, Chiang's HPB model contains only one type of non-terminal symbol X, often making it difficult to select the most appropriate translation rules.", "labels": [], "entities": []}, {"text": "One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while use word tags, generated by either POS analysis or unsupervised word class induction.", "labels": [], "entities": []}, {"text": "use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable.", "labels": [], "entities": []}, {"text": "By contrast, and inspired by previous work in parsing, our HeadDriven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB.", "labels": [], "entities": [{"text": "HeadDriven HPB (HD-HPB)", "start_pos": 59, "end_pos": 82, "type": "DATASET", "confidence": 0.8716596603393555}]}, {"text": "We identify heads using linguistically motivated dependency parsing, and use head information to refine X.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7323032915592194}]}, {"text": "Furthermore, Chiang's HPB model suffers from limited phrase reordering by combining translated phrases in a monotonic way with glue rules.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7214249819517136}]}, {"text": "In addition, once a glue rule is adopted, it requires all rules above it to be glue rules.", "labels": [], "entities": []}, {"text": "For example, given a Chinese-English sentence pair ( /zuotian 1 /chuxi 2 /huiyi 3 , Attended 2 a 3 meeting 3 yesterday 1 ), a correct translation is impossible via HPB derivations in.", "labels": [], "entities": []}, {"text": "For the derivation in(a), swap reordering in the glue rule (i.e., S 1 \u2192 S 2 X 2 , X 2 S 2 ) is disallowed and, even if such a swap reordering is available, it lacks useful information for rule selection.", "labels": [], "entities": [{"text": "rule selection", "start_pos": 188, "end_pos": 202, "type": "TASK", "confidence": 0.7593792676925659}]}, {"text": "For the derivation in(b), the combination of two non-terminals (i.e., X 2 \u2192 X 3 X 4 , X 3 X 4 ) is disallowed to form anew non-terminal which in turn is a sub-phrase of a hierarchical rule.", "labels": [], "entities": []}, {"text": "These limitations prevent traditional HPB systems from even considering some reasonable derivations.", "labels": [], "entities": []}, {"text": "To tackle the problem of glue rules, He (2010) extended the HPB model by using bracketing transduction grammar) instead of the monotone glue rules, and trained an extra classifier for glue rules to predict reorderings of neighboring phrases.", "labels": [], "entities": []}, {"text": "By contrast, our HD-HPB model refines the nonterminal symbol X with syntactic head information and provides flexible reordering rules, including swap, which can mix freely with hierarchical translation rules for better interleaving of translation and reordering in translation derivations.", "labels": [], "entities": []}, {"text": "Different from the soft constraint modeling adopted in), our approach encodes syntactic information in translation rules.", "labels": [], "entities": []}, {"text": "However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model.", "labels": [], "entities": []}, {"text": "Our approach maintains the advantages of Chiang's HPB model while at the same time incorporating head information and flexible reordering in a derivation in a natural way.", "labels": [], "entities": [{"text": "Chiang's HPB model", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.8099183887243271}]}, {"text": "Experiments on Chinese-English translation using four NIST MT test sets show that our HD-HPB model significantly outperforms Chiang's HPB as well as a SAMT-style refined version of HPB.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6214421540498734}, {"text": "NIST MT test sets", "start_pos": 54, "end_pos": 71, "type": "DATASET", "confidence": 0.8964710086584091}, {"text": "Chiang's HPB", "start_pos": 125, "end_pos": 137, "type": "DATASET", "confidence": 0.5809327165285746}, {"text": "HPB", "start_pos": 181, "end_pos": 184, "type": "DATASET", "confidence": 0.9638357162475586}]}, {"text": "The paper is structured as follows: Section 2 describes the synchronous context-free grammar (SCFG) in our HD-HPB translation model.", "labels": [], "entities": []}, {"text": "Section 3 presents our model and features, followed by the decoding algorithm in Section 4.", "labels": [], "entities": []}, {"text": "We report experimental results in Section 5.", "labels": [], "entities": []}, {"text": "Finally we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of our HD-HPB model and compare it with our implementation of Chiang's HPB model, a source-side SAMTstyle refined version of HPB (SAMT-HPB), and the Moses implementation of HPB.", "labels": [], "entities": [{"text": "Chiang's HPB model", "start_pos": 90, "end_pos": 108, "type": "DATASET", "confidence": 0.7679408937692642}, {"text": "HPB", "start_pos": 201, "end_pos": 204, "type": "DATASET", "confidence": 0.9597353339195251}]}, {"text": "For Moses HPB, we use \"grow-diagfinal-and\" to obtain symmetric word alignments, 10 for the maximum phrase length, and the recommended default values for all other parameters.", "labels": [], "entities": [{"text": "Moses HPB", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.8016088306903839}]}, {"text": "To examine the efficacy of our approach on training datasets of different scales, we first train translation models on a small-sized corpus, and then scale to a larger one.", "labels": [], "entities": []}, {"text": "We use the 2002 NIST MT evaluation test data (878 sentence pairs) as the development data, and the-news NIST MT evaluation test data, and 616 sentence pairs, respectively) as the test data.", "labels": [], "entities": [{"text": "NIST MT evaluation test data", "start_pos": 16, "end_pos": 44, "type": "DATASET", "confidence": 0.887342119216919}, {"text": "NIST MT evaluation test data", "start_pos": 104, "end_pos": 132, "type": "DATASET", "confidence": 0.9295798659324646}]}, {"text": "To find heads, we parse the source sentences with the Berkeley Parser 3 (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit to obtain dependency structures.", "labels": [], "entities": [{"text": "Chinese TreeBank 6.0", "start_pos": 108, "end_pos": 128, "type": "DATASET", "confidence": 0.8903976082801819}]}, {"text": "We obtain the word alignments by running GIZA++) on the corpus in both directions, applying \"grow-diag-final-and\" refinement ().", "labels": [], "entities": [{"text": "word alignments", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.6992002427577972}]}, {"text": "We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT to tune the feature weights on the development data.", "labels": [], "entities": [{"text": "SRI language modeling", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.693155825138092}, {"text": "Gigaword corpus", "start_pos": 103, "end_pos": 118, "type": "DATASET", "confidence": 0.8076907992362976}, {"text": "MERT", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.9739799499511719}]}, {"text": "For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the NIST and the BLEU scores, which measures caseinsensitive matching of n-grams with n up to 4.", "labels": [], "entities": [{"text": "NIST", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.8147789239883423}, {"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.8846741914749146}, {"text": "NIST", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.9525101184844971}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9974457025527954}]}, {"text": "To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach.", "labels": [], "entities": []}, {"text": "In this paper, '**' and '*' denote p-values less than 0.01 and in-between [0.01, 0.05), respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: NIST and BLEU (%) scores of different models trained on small data. Note: 1) HD-HR+Glue indicates our  HD-HPB model replacing NRRs with glue rules; 2) Significance tests for Moses HPB, HD-HPB, SAMT-HPB and  HD-HR+Glue are done against HPB.", "labels": [], "entities": [{"text": "NIST", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.7541936635971069}, {"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9987285733222961}, {"text": "Moses HPB", "start_pos": 184, "end_pos": 193, "type": "DATASET", "confidence": 0.9076582491397858}]}, {"text": " Table 5: NIST and BLEU (%) scores of different models trained on large data. Note: System labels and significance  testing as in", "labels": [], "entities": [{"text": "NIST", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.6146762371063232}, {"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9987223744392395}]}, {"text": " Table 6: BLEU (%) scores of models trained on large  data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994344115257263}]}]}