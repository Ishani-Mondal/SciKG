{"title": [{"text": "Classifying Gene Sentences in Biomedical Literature by Combining High-Precision Gene Identifiers", "labels": [], "entities": [{"text": "Classifying Gene Sentences in Biomedical Literature", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8312000632286072}, {"text": "Combining High-Precision Gene Identifiers", "start_pos": 55, "end_pos": 96, "type": "TASK", "confidence": 0.6415429711341858}]}], "abstractContent": [{"text": "Gene name identification is a fundamental step to solve more complicated text mining problems such as gene normalization and protein protein interactions.", "labels": [], "entities": [{"text": "Gene name identification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8135541478792826}, {"text": "text mining", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.7363978624343872}, {"text": "gene normalization", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7097271233797073}]}, {"text": "However, state-of-the-art name identification methods are not yet sufficient for use in a fully automated system.", "labels": [], "entities": [{"text": "name identification", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.9031890034675598}]}, {"text": "In this regard, a relaxed task, gene/protein sentence identification, may serve more effectively for manually searching and browsing biomedical literature.", "labels": [], "entities": [{"text": "gene/protein sentence identification", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.6045464217662812}]}, {"text": "In this paper , we setup anew task, gene/protein sentence classification and propose an ensemble approach for addressing this problem.", "labels": [], "entities": [{"text": "gene/protein sentence classification", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.5963713347911834}]}, {"text": "Well-known named entity tools use similar gold-standard sets for training and testing, which results in relatively poor performance for unknown sets.", "labels": [], "entities": []}, {"text": "We here explore how to combine diverse high-precision gene identifiers for more robust performance.", "labels": [], "entities": []}, {"text": "The experimental results show that the proposed approach out-performs BANNER as a stand-alone classifier for newly annotated sets as well as previous gold-standard sets.", "labels": [], "entities": [{"text": "BANNER", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9153528213500977}]}], "introductionContent": [{"text": "With the rapidly increasing biomedical literature, text mining has become popular for finding biomedical information in text.", "labels": [], "entities": [{"text": "text mining", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.856151282787323}]}, {"text": "Among others, named entity recognition (NER) for bio-entities such as genes and proteins is a fundamental task because extracting biological relationships begins with entity identification.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 14, "end_pos": 44, "type": "TASK", "confidence": 0.792455330491066}, {"text": "entity identification", "start_pos": 167, "end_pos": 188, "type": "TASK", "confidence": 0.7334336340427399}]}, {"text": "However, NER in biomedical literature is challenging due to the irregularities and ambiguities in bio-entities nomenclature).", "labels": [], "entities": [{"text": "NER", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9891258478164673}]}, {"text": "In particular, compound entity names make this problem difficult because it also requires deciding word boundaries.", "labels": [], "entities": []}, {"text": "Recent bio-text competitions such as JNLPBA () and BioCreative () have evaluated NER systems for gene mentions.", "labels": [], "entities": [{"text": "JNLPBA", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.9006065726280212}]}, {"text": "Even though progress has been made in several areas, gene identification methods are not yet sufficient for real-world use without human interaction.", "labels": [], "entities": [{"text": "gene identification", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8546205163002014}]}, {"text": "Thus, at the present, a realistic suggestion is to use these algorithms as an aid to human curation and information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.8180629909038544}]}, {"text": "In this paper, we define anew task, gene/protein sentence classification.", "labels": [], "entities": [{"text": "gene/protein sentence classification", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.6042014300823212}]}, {"text": "A gene or protein sentence means a sentence including at least one specific gene or protein name.", "labels": [], "entities": []}, {"text": "This new task has advantages over gene mention identification.", "labels": [], "entities": [{"text": "gene mention identification", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.8654539982477824}]}, {"text": "First, gene name boundaries are not important at the sentence level and human judges will agree more in their judgments.", "labels": [], "entities": [{"text": "gene name boundaries", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.64278444647789}]}, {"text": "Second, highlighting gene sentences maybe more useful in manual search and browsing environments since this can be done more accurately and with less distraction from incorrect annotations.", "labels": [], "entities": []}, {"text": "To classify gene/protein sentences, we here propose an ensemble approach to combine different NER identifiers.", "labels": [], "entities": [{"text": "classify gene/protein sentences", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.851568627357483}]}, {"text": "Previous NER approaches are mostly developed on a small number of gold-standard sets including GENIA ( and BioCreative () corpora.", "labels": [], "entities": [{"text": "NER", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9734730124473572}, {"text": "GENIA", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.8799422383308411}]}, {"text": "These sets help to find regular name patterns in a limited set of articles, but also limit the NER performance for real-world use.", "labels": [], "entities": []}, {"text": "In the proposed approach, we use a Semantic Model and a Priority Model along with BANNER (.", "labels": [], "entities": [{"text": "BANNER", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9625182151794434}]}, {"text": "The Semantic and Priority Models are used to provide more robust performance on gene/protein sentence classification because they utilize larger resources such as SemCat and PubMed \u25cb R to detect gene names.", "labels": [], "entities": [{"text": "gene/protein sentence classification", "start_pos": 80, "end_pos": 116, "type": "TASK", "confidence": 0.6631860435009003}]}, {"text": "For experiments, we created three new goldstandard sets to include cases appearing in the most recent publications.", "labels": [], "entities": []}, {"text": "The experimental results show that our approach outperforms machine learning classifiers using unigrams and substring features as well as stand-alone BANNER classification on five gold-standard datasets.", "labels": [], "entities": [{"text": "BANNER classification", "start_pos": 150, "end_pos": 171, "type": "TASK", "confidence": 0.6946793794631958}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, the ensemble approach for gene/protein sentence classification is described.", "labels": [], "entities": [{"text": "gene/protein sentence classification", "start_pos": 40, "end_pos": 76, "type": "TASK", "confidence": 0.6280214726924896}]}, {"text": "Section 3 explains the gold-standard sets used for our experiments.", "labels": [], "entities": []}, {"text": "Section 4 presents and discusses the experimental results.", "labels": [], "entities": []}, {"text": "Conclusions are drawn in Section 5..", "labels": [], "entities": []}, {"text": "shows the overall framework for our proposed approach.", "labels": [], "entities": []}, {"text": "We basically assume that a main NER module works as a strong predictor, i.e., the majority of outputs obtained from this module are correct.", "labels": [], "entities": []}, {"text": "We here use BANNER ( as the main NER method because it adopts features and methods which are generally known to be effective for gene name recognition.", "labels": [], "entities": [{"text": "BANNER", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9655339121818542}, {"text": "NER", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8868992328643799}, {"text": "gene name recognition", "start_pos": 129, "end_pos": 150, "type": "TASK", "confidence": 0.7140649755795797}]}, {"text": "While BANNER shows good performance on well-known gold-standard sets, it suffers from relatively poor performance on unknown examples.", "labels": [], "entities": [{"text": "BANNER", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.8769077062606812}]}, {"text": "To overcome this problem, we combine BANNER with two other predictors, a Sematic Model and a Priority Model.", "labels": [], "entities": [{"text": "BANNER", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9534254670143127}]}, {"text": "First, the Semantic Model and the Priority Model do not use previous gold-standard sets for training.", "labels": [], "entities": []}, {"text": "Second, these two models learn name patterns in different ways, i.e., semantic relationships for the Semantic Model and positional and lexical information for the Priority Model.", "labels": [], "entities": []}, {"text": "This combination of a strong predictor and two weaker but more general predictors can respond better to unknown name patterns.", "labels": [], "entities": []}], "datasetContent": [{"text": "For experiments, we rigorously tested the proposed method on gene mention gold-standard sets and newly annotated sets.", "labels": [], "entities": []}, {"text": "GENETAG () is the dataset released for BioCreative I and BioCreative II workshops.", "labels": [], "entities": [{"text": "GENETAG", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8909640312194824}, {"text": "BioCreative I", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.5187049210071564}]}, {"text": "Since it is well-known fora gene mention gold-standard set, we used GENETAG as training data.", "labels": [], "entities": [{"text": "GENETAG", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.7076475620269775}]}, {"text": "For test data, two previous gold-standard sets were selected and new test sets were also built for gene sentence classification.", "labels": [], "entities": [{"text": "gene sentence classification", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.7145242889722189}]}, {"text": "YAPEX) and JNLPBA () are considered of moderate difficulty because they are both related to GENIA corpus, a well-known goldstandard set.", "labels": [], "entities": [{"text": "YAPEX", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.553162157535553}, {"text": "JNLPBA", "start_pos": 11, "end_pos": 17, "type": "DATASET", "confidence": 0.7651875019073486}, {"text": "GENIA corpus", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.9539466500282288}]}, {"text": "However, Disease, Cell Line and Reptiles are considered as more difficult tasks because they represent new areas and contain recently published articles.", "labels": [], "entities": []}, {"text": "The annotation guideline for new test sets basically followed those used in GENETAG (), however domains, complexes, subunits and promoters were not included in new sets.", "labels": [], "entities": [{"text": "GENETAG", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.8374620676040649}]}, {"text": "(1) \"Disease\" Set: This set of 60 PubMed documents was obtained from two sources.", "labels": [], "entities": [{"text": "Disease\" Set", "start_pos": 5, "end_pos": 17, "type": "DATASET", "confidence": 0.7465833028157552}, {"text": "PubMed documents", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9082827568054199}]}, {"text": "Fifty of the documents were obtained from the 793 PubMed documents used to construct the AZDC ().", "labels": [], "entities": [{"text": "PubMed documents", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.9507698714733124}, {"text": "AZDC", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.8696547746658325}]}, {"text": "They are the fifty most recent among these records.", "labels": [], "entities": []}, {"text": "In addition to these fifty documents, ten documents were selected from PubMed on the topic of maize to add variety to the set and because one of the curators who worked with the set had experience studying the maize genome.", "labels": [], "entities": []}, {"text": "These ten were chosen as recent documents as of early March 2012 and which contained the text word maize and discussed genetics.", "labels": [], "entities": []}, {"text": "The whole set of 60 documents were annotated by WJW to produce a gold standard.", "labels": [], "entities": [{"text": "WJW", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9686121940612793}]}, {"text": "(2) \"CellLine\" Set: This set comprised the most recent 50 documents satisfying the query \"cell line\" in PubMed on.", "labels": [], "entities": [{"text": "PubMed on", "start_pos": 104, "end_pos": 113, "type": "DATASET", "confidence": 0.9305764734745026}]}, {"text": "This query was used to obtain documents which discuss cell lines, but most of these documents also discuss genes and for this reason the set was expected to be challenging.", "labels": [], "entities": []}, {"text": "The set was annotated by WJW and DC and after independently annotating the set they reconciled differences to produce a final gold standard.", "labels": [], "entities": [{"text": "WJW", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.9618182182312012}, {"text": "DC", "start_pos": 33, "end_pos": 35, "type": "DATASET", "confidence": 0.8121069073677063}]}, {"text": "(3) \"Reptiles\" Set: This set comprised the most recent 50 documents satisfying the query \"reptiles AND genes\" in PubMed on.", "labels": [], "entities": [{"text": "PubMed on", "start_pos": 113, "end_pos": 122, "type": "DATASET", "confidence": 0.9485716223716736}]}, {"text": "This set was chosen because it would have little about human or model organisms and for this reason it was expected to be challenging.", "labels": [], "entities": []}, {"text": "The set was annotated by WJW and DC and after independently annotating the set they reconciled differences to produce a final gold standard.", "labels": [], "entities": [{"text": "WJW", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.9618182182312012}, {"text": "DC", "start_pos": 33, "end_pos": 35, "type": "DATASET", "confidence": 0.8121069073677063}]}, {"text": "For both \"CellLine\" and \"Reptiles\" Sets, the most recent data was chosen in an effort to make the task more challenging.", "labels": [], "entities": []}, {"text": "Presumably such documents will contain more recently created names and phrases that do not appear in the older training data.", "labels": [], "entities": []}, {"text": "This will then pose a more difficult test for NER systems.", "labels": [], "entities": [{"text": "NER", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9610462784767151}]}, {"text": "shows all datasets used for training and testing.", "labels": [], "entities": []}, {"text": "The new sets, \"Disease\", \"CellLine\" and \"Reptiles\" are also freely available at http://www.ncbi..", "labels": [], "entities": []}, {"text": "\"GENETAG\" was used for training data and others were used for test data.", "labels": [], "entities": [{"text": "GENETAG", "start_pos": 1, "end_pos": 8, "type": "METRIC", "confidence": 0.497274249792099}]}, {"text": "\"YAPEX\" and \"JNLPBA\" were selected from previous gold-standard corpora.", "labels": [], "entities": [{"text": "YAPEX", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.7681792974472046}, {"text": "JNLPBA", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.7160632014274597}]}, {"text": "\"Disease\", \"Cell Line\" and \"Reptiles\" are newly created from recent publications and considered as difficult sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Pairwise overlap between sets representing the  different categories.", "labels": [], "entities": []}, {"text": " Table 2. Row two contains the number of unique strings  in the four different semantic classes studied. The last  row shows the mean average precisions from a 10-fold  cross validation to learn how to distinguish each class  from the union of the other three.", "labels": [], "entities": [{"text": "precisions", "start_pos": 142, "end_pos": 152, "type": "METRIC", "confidence": 0.6114711165428162}]}, {"text": " Table 3. Performance changes on training set for the  Semantic Model (SEM) and the Priority Model (PM).  FP indicates that learned false positives were removed  from predictions.", "labels": [], "entities": [{"text": "FP", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.9971737861633301}]}, {"text": " Table 4. Datasets. \"GENETAG\" was used for training  data and others were used for test data. \"YAPEX\" and  \"JNLPBA\" were selected from previous gold-standard  corpora. \"Disease\", \"Cell Line\" and \"Reptiles\" are new- ly created from recent publications and considered as  difficult sets.", "labels": [], "entities": [{"text": "YAPEX", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.8590319156646729}, {"text": "JNLPBA", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.7802824974060059}]}, {"text": " Table 5. Performance of BANNER on training and test  datasets.", "labels": [], "entities": [{"text": "BANNER", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.7072039842605591}]}, {"text": " Table 6. Average precision results on test sets for different feature combinations.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9176892042160034}]}, {"text": " Table 7. Breakeven results on test sets for different feature combinations.", "labels": [], "entities": [{"text": "Breakeven", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9834282994270325}]}]}