{"title": [{"text": "Focused Meeting Summarization via Unsupervised Relation Extraction", "labels": [], "entities": [{"text": "Summarization", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.8529558777809143}, {"text": "Unsupervised Relation Extraction", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.7140417893727621}]}], "abstractContent": [{"text": "We present a novel unsupervised framework for focused meeting summarization that views the problem as an instance of relation extraction.", "labels": [], "entities": [{"text": "focused meeting summarization", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.5457858542601267}, {"text": "relation extraction", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.8474765419960022}]}, {"text": "We adapt an existing in-domain relation learner (Chen et al., 2011) by exploiting a set of task-specific constraints and features.", "labels": [], "entities": []}, {"text": "We evaluate the approach on a decision summarization task and show that it outper-forms unsupervised utterance-level extractive summarization baselines as well as an existing generic relation-extraction-based summa-rization method.", "labels": [], "entities": [{"text": "decision summarization task", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.715673049290975}]}, {"text": "Moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 122, "end_pos": 133, "type": "METRIC", "confidence": 0.910215437412262}]}], "introductionContent": [{"text": "For better or worse, meetings play an integral role inmost of our daily lives -they let us share information and collaborate with others to solve a problem, to generate ideas, and to weigh options.", "labels": [], "entities": []}, {"text": "Not surprisingly then, there is growing interest in developing automatic methods for meeting summarization (e.g.,,,,,).", "labels": [], "entities": [{"text": "meeting summarization", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.7211276888847351}]}, {"text": "This paper tackles the task of focused meeting summarization , i.e., generating summaries of a particular aspect of a meeting rather than of the meeting as a whole).", "labels": [], "entities": [{"text": "focused meeting summarization", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.5733517209688822}]}, {"text": "For example, one might want a summary of just the DECISIONS made during the meeting, the ACTION ITEMS that emerged, the IDEAS discussed, or the HYPOTHESES put forth, etc.", "labels": [], "entities": [{"text": "ACTION ITEMS", "start_pos": 89, "end_pos": 101, "type": "METRIC", "confidence": 0.72585129737854}]}, {"text": "Consider, for example, the task of summarizing the decisions in the dialogue snippet in.", "labels": [], "entities": [{"text": "summarizing", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9758120179176331}]}, {"text": "The figure shows only the decision-related dialogue acts (DRDAs) -utterances associated with one or more decisions.", "labels": [], "entities": []}, {"text": "Each DRDA is labeled numerically according to the decision it supports; so the first two utterances support DECISION 1 as do the final two utterances in the snippet.", "labels": [], "entities": []}, {"text": "Manually constructed decision abstracts for each decision are shown at the bottom of the figure.", "labels": [], "entities": []}, {"text": "These constitute the decisionfocused summary for the snippet.", "labels": [], "entities": []}, {"text": "Notice that many portions of the DRDAs are not relevant to the decision itself: they often begin with phrases that identify the utterance within the discourse as potentially introducing a decision (e.g., \"Maybe that could be\", \"It seems like you're gonna have\"), but do not themselves describe the decision.", "labels": [], "entities": []}, {"text": "We will refer to this portion of a DRDA (underlined in) as the Decision Cue.", "labels": [], "entities": []}, {"text": "Moreover, the decision cue is generally directly followed by the actual Decision Content (e.g., \"be a little apple\", \"have rubber cases\").", "labels": [], "entities": []}, {"text": "Decision Content phrases are denoted in via italics and square brackets.", "labels": [], "entities": []}, {"text": "Importantly, it is just the decision content portion of the utterance that should be considered for incorporation into the focused summary.).", "labels": [], "entities": []}, {"text": "A, B, C and D refer to distinct speakers; the numbers in parentheses indicate the associated meeting decision: DECI-SION 1, 2 or 3.", "labels": [], "entities": [{"text": "DECI-SION", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9264856576919556}]}, {"text": "Also shown is the gold-standard (manual) abstract (summary) for each decision.", "labels": [], "entities": []}, {"text": "Colors indicate overlapping vocabulary between utterances and the summary. are decscribed in the running text.", "labels": [], "entities": []}, {"text": "This paper presents an unsupervised framework for focused meeting summarization that supports the generation of abstractive summaries.", "labels": [], "entities": [{"text": "focused meeting summarization", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.4988589386145274}]}, {"text": "(Note that we do not currently generate actual abstracts, but rather aim to identify those Content phrases that should comprise the abstract.)", "labels": [], "entities": []}, {"text": "In contrast to existing approaches to focused meeting summarization (e.g.,,, ), we view the problem as an information extraction task and hypothesize that existing methods for domain-specific relation extraction can be modified to identify salient phrases for use in generating abstractive summaries.", "labels": [], "entities": [{"text": "focused meeting summarization", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.5336846907933553}, {"text": "information extraction task", "start_pos": 106, "end_pos": 133, "type": "TASK", "confidence": 0.8023168245951334}, {"text": "domain-specific relation extraction", "start_pos": 176, "end_pos": 211, "type": "TASK", "confidence": 0.6874327858289083}]}, {"text": "Very generally, information extraction methods identify a lexical \"trigger\" or \"indicator\" that evokes a relation of interest and then employ syntactic information, often in conjunction with semantic constraints, to find the \"target phrase\" or \"argument constituent\" to be extracted.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7570482492446899}]}, {"text": "Relation instances, then, are represented by indicator-argument pairs).", "labels": [], "entities": []}, {"text": "shows some possible indicator-argument pairs for identifying the Decision Content phrases in the dialogue sample.", "labels": [], "entities": []}, {"text": "Content indicator words are shown in italics; the Decision Content target phrases are the arguments.", "labels": [], "entities": []}, {"text": "For example, in the fourth DRDA, \"require\" is the indicator, and \"rubber buttons\" and \"rubber case\" are both arguments.", "labels": [], "entities": [{"text": "DRDA", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.8960019946098328}]}, {"text": "Although not shown in, it is also possible to identify relations that correspond to the Decision Cue phrases.", "labels": [], "entities": []}, {"text": "Specifically, we focus on the task of decision summarization and, as in previous work in meeting summarization (e.g.,,), assume that all decision-related utterances (DRDAs) have been identified.", "labels": [], "entities": [{"text": "decision summarization", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7385955154895782}, {"text": "meeting summarization", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7014115452766418}]}, {"text": "We adapt the unsupervised relation learning approach of to separately identify relations associated with decision cues vs. the decision content within DRDAs by defining anew set of task-specific constraints and features to take the place of the domain-specific constraints and features of the original model.", "labels": [], "entities": []}, {"text": "Output of the system is a set of extracted indicator-argument decision content relations (see the \"OUR METHOD\" sample summary of) that can be used as the basis of the decision abstract.", "labels": [], "entities": [{"text": "OUR", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9906795620918274}, {"text": "METHOD", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.7524754405021667}]}, {"text": "We evaluate the approach (using the AMI corpus () under two input settings -in the True Clusterings setting, we assume that the DRDAs for each meeting have been perfectly grouped according to the decision(s) each supports; in the System Clusterings setting, an automated system performs the DRDA-decision pairing.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.9263417720794678}, {"text": "DRDA-decision pairing", "start_pos": 291, "end_pos": 312, "type": "TASK", "confidence": 0.7113982290029526}]}, {"text": "The results show that the relation-based summarization approach outperforms two extractive summarization baselines that select the longest and the most representative utterance for each decision, respectively.", "labels": [], "entities": [{"text": "summarization", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.7036918997764587}]}, {"text": "(ROUGE-1 F score of 37.47% vs. 32.61% and 33.32% for the baselines given the True Clusterings of DRDAs.)", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 1, "end_pos": 8, "type": "METRIC", "confidence": 0.9988974332809448}, {"text": "F score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9675098061561584}]}, {"text": "Moreover, our approach performs admirably in comparison to two supervised learning alternatives (scores of 35.61% and 40.87%) that aim to identify the important tokens to include in the decision abstract given the DRDA clusterings.", "labels": [], "entities": [{"text": "DRDA clusterings", "start_pos": 214, "end_pos": 230, "type": "DATASET", "confidence": 0.8955386281013489}]}, {"text": "In contrast to our approach which is transferable to different domains or tasks, these methods would require labeled data for retraining for each new meeting corpus.", "labels": [], "entities": []}, {"text": "Finally, in order to compare our approach to another relation-based summarization technique, we modify the multi-document summarization system of Hachey to the single-document meeting scenario.", "labels": [], "entities": []}, {"text": "Here again, our proposed approach performs better (37.47% vs. 34.69%).", "labels": [], "entities": []}, {"text": "Experiments under the System Clusterings setting produce the same overall results, albeit with lower scores for all of the systems and baselines.", "labels": [], "entities": []}, {"text": "In the remainder of the paper, we review related work in Section 2 and give a high-level description of the relation-based approach to focused summarization in Section 3.", "labels": [], "entities": [{"text": "focused summarization", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.5281109511852264}]}, {"text": "Sections 4, 5 and 6 present the modifications to the relation extraction model required for its instantiation for the meeting summarization task.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8375021517276764}, {"text": "meeting summarization task", "start_pos": 118, "end_pos": 144, "type": "TASK", "confidence": 0.7670836249987284}]}, {"text": "Sections 7 and 8 provide our experimental setup and results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approach on the AMI meeting corpus) that consists of 140 multi-party meetings with a wide range   of annotations.", "labels": [], "entities": [{"text": "AMI meeting corpus", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.8915011882781982}]}, {"text": "The 129 scenario-driven meetings involve four participants playing different roles on a design team.", "labels": [], "entities": []}, {"text": "Importantly, the corpus includes a short (usually one-sentence), manually constructed abstract summarizing each decision discussed in the meeting.", "labels": [], "entities": []}, {"text": "In addition, all of the dialogue acts that support (i.e., are relevant to) each decision are annotated as such.", "labels": [], "entities": []}, {"text": "We use the manually constructed decision abstracts as gold-standard summaries.", "labels": [], "entities": []}, {"text": "We consider two system input settings.", "labels": [], "entities": []}, {"text": "In the True Clusterings setting, we use the AMI annotations to create perfect partitionings of the DRDAs for input to the summarization system; in the System Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in previous work ().", "labels": [], "entities": []}, {"text": "The Wang and Cardie (2011) clustering method groups DRDAs according to their LDA topic distribution similarity.", "labels": [], "entities": []}, {"text": "As better approaches for DRDA clustering become available, they could be employed instead.", "labels": [], "entities": [{"text": "DRDA clustering", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.8285660445690155}]}, {"text": "We use the widely accepted ROUGE (Lin and Hovy, 2003) evaluation measure.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9956230521202087}]}, {"text": "We adopt the ROUGE-1 and ROUGE-SU4 metrics from, and also use ROUGE-2.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.8396265506744385}, {"text": "ROUGE-SU4", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.8465607166290283}]}, {"text": "We choose the stemming option of the ROUGE software at http://berouge.com/ and remove stopwords from both the system and gold-standard summaries.", "labels": [], "entities": []}, {"text": "The Dirichlet hyperparameters are set to 0.1 for the priors.", "labels": [], "entities": []}, {"text": "When training the model, ten random restarts are performed and each run stops when reaching a convergence threshold (10 \u22125 ).", "labels": [], "entities": []}, {"text": "Then we select the posterior with the lowest final free energy.", "labels": [], "entities": []}, {"text": "For the parameters used in posterior constraints, we either adopt them from (Chen et al., 2011) or choose them arbitrarily without tuning in the spirit of making the approach domain-independent.", "labels": [], "entities": []}, {"text": "We compare our decision summarization approach with (1) two unsupervised baselines, (2) the unsupervised relation-based approach of Hachey (2009), (3) two supervised methods, and (4) an upperbound derived from the gold standard decision abstracts.", "labels": [], "entities": [{"text": "decision summarization", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7805333435535431}]}, {"text": "As in and, this baseline simply selects the longest DRDA in each cluster as the summary.", "labels": [], "entities": []}, {"text": "Thus, this baseline performs utterance-level decision summarization.", "labels": [], "entities": [{"text": "utterance-level decision summarization", "start_pos": 29, "end_pos": 67, "type": "TASK", "confidence": 0.7244824568430582}]}, {"text": "Although it's possible that decision content is spread over multiple DRDAs in the cluster, this baseline and the next allow us to determine summary quality when summaries are restricted to a single utterance.", "labels": [], "entities": []}, {"text": "Following, the second baseline selects the decision cluster prototype (i.e., the DRDA with the largest TF-IDF similarity with the cluster centroid) as the summary.", "labels": [], "entities": []}, {"text": "The Generic Relation Extraction (GRE) Method of.", "labels": [], "entities": [{"text": "Generic Relation Extraction (GRE)", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7602132807175318}]}, {"text": "Hachey (2009) presents a generic relation extraction (GRE) for multidocument summarization.", "labels": [], "entities": [{"text": "generic relation extraction (GRE", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.6169686794281006}, {"text": "multidocument summarization", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.604141116142273}]}, {"text": "Informative sentences are extracted to form summaries instead of relation instances.", "labels": [], "entities": []}, {"text": "Relation types are discovered by Latent Dirichlet Allocation, such that a probability is output for each relation instance given a topic (equivalent to relation).", "labels": [], "entities": []}, {"text": "Their relation instances are named entity(NE)-mention pairs conforming to a set of pre-specified rules.", "labels": [], "entities": []}, {"text": "For comparison, we use these same rules to select noun-mention pairs rather than NE-mention pairs, which is better suited to meetings, which do not contain many NEs.", "labels": [], "entities": []}, {"text": "6 Because an approximate set cover algorithm is used in GRE, one decision-related dialogue act (DRDA) is extracted each time until the summary reaches the desired length.", "labels": [], "entities": [{"text": "GRE", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.7645297050476074}]}, {"text": "We run two sets of experiments using this GRE system with different output summaries -one selects one entire DRDA as the final summary (as Hachey (2009) does), and another one outputs the relation instances with highest probability conditional on each relation type.", "labels": [], "entities": []}, {"text": "We find that the first set of experiments gets better: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) scores for summaries produced by the baselines, GRE (Hachey, 2009)'s best results, the supervised methods, our method and an upperbound -all with perfect/true DRDA clusterings.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9652560353279114}, {"text": "ROUGE-2", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9434188604354858}, {"text": "ROUGE-SU4 (R-SU4) scores", "start_pos": 88, "end_pos": 112, "type": "METRIC", "confidence": 0.8646282315254211}, {"text": "GRE (Hachey, 2009)'", "start_pos": 154, "end_pos": 173, "type": "DATASET", "confidence": 0.778419608871142}]}, {"text": "Supervised Learning (SVMs and CRFs).", "labels": [], "entities": []}, {"text": "We also compare our approach to two supervised learning methods -Support Vector Machines (Joachims, 1998) with RBF kernel and order-1 Conditional Random Fields () -trained using the same features as our system (see) to identify the important tokens to include in the decision abstract.", "labels": [], "entities": []}, {"text": "Three-fold cross validation is conducted for both methods.", "labels": [], "entities": []}, {"text": "We also compute an upperbound that reflects the gap between the best possible extractive summaries and the human-written abstracts according to the ROUGE score: for each cluster of DRDAs, we select the words that also appear in the associated decision abstract.: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) scores for summaries produced by the baselines, GRE (Hachey, 2009)'s best results, the supervised methods and our method -all with system clusterings. lower.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 263, "end_pos": 270, "type": "METRIC", "confidence": 0.920871376991272}, {"text": "GRE (Hachey, 2009)'", "start_pos": 362, "end_pos": 381, "type": "DATASET", "confidence": 0.8103319754203161}]}, {"text": "When measured by ROUGE-2, our method still have better or comparable performances than other unsupervised methods.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9802464246749878}]}, {"text": "Moreover, our system achieves F scores in between those of the supervised learning methods, performing better than the CRF in both recall and F score.", "labels": [], "entities": [{"text": "F", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.9983797073364258}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9991981387138367}, {"text": "F score", "start_pos": 142, "end_pos": 149, "type": "METRIC", "confidence": 0.9796749651432037}]}, {"text": "The recall score for the upperbound in ROUGE-1, on the other hand, indicates that there is still a wide gap between the extractive summaries and human-written abstracts: without additional lexical information (e.g., semantic class information, ontologies) or areal language generation component, recall appears to be a bottleneck for extractive summarization methods that select content only from decision-related dialogue acts (DRDAs).", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9992793202400208}, {"text": "recall", "start_pos": 296, "end_pos": 302, "type": "METRIC", "confidence": 0.9956445693969727}]}, {"text": "Results using the System Clusterings) are comparable, although all of the system and baseline scores are much lower.", "labels": [], "entities": []}, {"text": "Supervised methods get the best F scores largely due to their high precision; but our method attains the best recall in ROUGE-1.", "labels": [], "entities": [{"text": "F scores", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9840198755264282}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.999243974685669}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9996582269668579}, {"text": "ROUGE-1", "start_pos": 120, "end_pos": 127, "type": "METRIC", "confidence": 0.9443892240524292}]}, {"text": "To better exemplify the summaries generated by different systems, sample output for each method is shown in.", "labels": [], "entities": []}, {"text": "The GRE system uses an approximate algorithm for set cover extraction, we list the first three selected DRDA in order.", "labels": [], "entities": [{"text": "GRE", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.6703806519508362}, {"text": "set cover extraction", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.635799249013265}]}, {"text": "We see from the table that utterance-level extractive summaries (Longest DA, Prototype DA, GRE) make more coherent but still far from concise and compact DRDA (1): Uh the batteries, uh we also thought about that already, DRDA (2): uh will be chargeable with uh uh an option fora mount station DRDA (3): Maybe it's better to to include rechargeable batteries DRDA (4): We already decided that on the previous meeting.", "labels": [], "entities": [{"text": "GRE", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9870554208755493}, {"text": "DRDA", "start_pos": 154, "end_pos": 158, "type": "DATASET", "confidence": 0.8122847676277161}]}, {"text": "DRDA (5): which you can recharge through the docking station.", "labels": [], "entities": [{"text": "DRDA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7634537816047668}]}, {"text": "DRDA (6): normal plain batteries you can buy at the supermarket or retail shop.", "labels": [], "entities": [{"text": "DRDA", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.630915641784668}]}, {"text": "Decision Abstract: The remote will use rechargeable batteries which recharge in a docking station.", "labels": [], "entities": [{"text": "Abstract", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.8398961424827576}]}, {"text": "Longest DA & Prototype DA: normal plain batteries you can buy at the supermarket or retail shop.", "labels": [], "entities": [{"text": "Prototype DA", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.8259741961956024}]}, {"text": "GRE: 1st: normal plain batteries you can buy at the supermarket or retail shop.", "labels": [], "entities": [{"text": "GRE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9923140406608582}]}, {"text": "2nd: which you can recharge through the docking station.", "labels": [], "entities": []}, {"text": "3rd: uh will be chargeable with uh uh an option fora mount station SVM: batteries include rechargeable batteries decided recharge docking station CRF: chargeable station rechargeable batteries Our Method: <option, fora mount station>, <include, rechargeable batteries>, <decided, that on the previous meeting>, <recharge, through the docking station>, <buy, normal plain batteries> abstracts.", "labels": [], "entities": []}, {"text": "On the other hand, the supervised methods (SVM, CRF) that produce token-level extracts better identify the overall content of the decision abstract.", "labels": [], "entities": []}, {"text": "Unfortunately, they require human annotation in the training phase; in addition, the output is ungrammatical and lacks coherence.", "labels": [], "entities": []}, {"text": "In comparison, our system presents the decision summary in the form of phrase-based relations that provide a relatively comprehensive expression.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9289172887802124}, {"text": "ROUGE-2", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.8998122811317444}, {"text": "ROUGE", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.8991594314575195}]}, {"text": " Table 5: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.928609311580658}, {"text": "ROUGE-2", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9036426544189453}, {"text": "ROUGE", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9056157469749451}]}, {"text": " Table 6. The GRE system  uses an approximate algorithm for set cover extrac- tion, we list the first three selected DRDA in order.  We see from the table that utterance-level extractive  summaries (Longest DA, Prototype DA, GRE) make  more coherent but still far from concise and compact", "labels": [], "entities": []}]}