{"title": [{"text": "Comparing human versus automatic feature extraction for fine-grained elementary readability assessment", "labels": [], "entities": [{"text": "Comparing human versus automatic feature extraction", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.578511580824852}]}], "abstractContent": [{"text": "Early primary children's literature poses some interesting challenges for automated readabil-ity assessment: for example, teachers often use fine-grained reading leveling systems for determining appropriate books for children to read (many current systems approach read-ability assessment at a coarser whole grade level).", "labels": [], "entities": []}, {"text": "In previous work (Ma et al., 2012), we suggested that the fine-grained assessment task can be approached using a ranking methodology, and incorporating features that correspond to the visual layout of the page improves performance.", "labels": [], "entities": []}, {"text": "However, the previous methodology for using \"found\" text (e.g., scanning in a book from the library) requires human annotation of the text regions and correction of the OCR text.", "labels": [], "entities": []}, {"text": "In this work, we ask whether the annotation process can be automated , and also experiment with richer syntactic features found in the literature that can be automatically derived from either the human-corrected or raw OCR text.", "labels": [], "entities": []}, {"text": "We find that automated visual and text feature extraction work reasonably well and can allow for scaling to larger datasets, but that in our particular experiments the use of syntactic features adds little to the performance of the system, contrary to previous findings.", "labels": [], "entities": [{"text": "text feature extraction", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.6365644335746765}]}], "introductionContent": [{"text": "Knowing the reading level of a children's book is an important task in the educational setting.", "labels": [], "entities": []}, {"text": "Teachers want to have leveling for books in the school library; parents are trying to select appropriate books for their children; writers need guidance while writing for different literacy needs (e.g. text simplification)-reading level assessment is required in a variety of contexts.", "labels": [], "entities": [{"text": "text simplification)-reading level assessment", "start_pos": 202, "end_pos": 247, "type": "TASK", "confidence": 0.7465742280085882}]}, {"text": "The history of assessing readability using simple arithmetic metrics dates back to the 1920s when has measured difficulty of texts by tabulating words according to the frequency of their use in general literature.", "labels": [], "entities": []}, {"text": "Most of the traditional readability formulas were also based on countable features of text, such as syllable counts.", "labels": [], "entities": []}, {"text": "More advanced machine learning techniques such as classification and regression have been applied to the task of reading level prediction); such works are described in further detail in the next Section 2.", "labels": [], "entities": [{"text": "reading level prediction", "start_pos": 113, "end_pos": 137, "type": "TASK", "confidence": 0.7208341360092163}]}, {"text": "In recent work (, we approached the problem of fine-grained leveling of books, demonstrating that a ranking approach to predicting reading level outperforms both classification and regression approaches in that domain.", "labels": [], "entities": []}, {"text": "A further finding was that visually-oriented features that consider the visual layout of the page (e.g. number of text lines per annotated text region, text region area compared to the whole page area and font size etc.) play an important role in predicting the reading levels of children's books in which pictures and textual layout dominate the book content over text.", "labels": [], "entities": []}, {"text": "However, the data preparation process in our previous study involves human intervention-we ask human annotators to draw rectangle markups around text region over pages.", "labels": [], "entities": [{"text": "data preparation", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7390727400779724}]}, {"text": "Moreover, we only use a very shallow surface level text-based feature set to compare with the visually-oriented features.", "labels": [], "entities": []}, {"text": "Hence in this paper, we assess the effect of using completely automated annotation processing within the same framework.", "labels": [], "entities": []}, {"text": "We are interested in exploring how much performance will change by completely eliminating manual intervention.", "labels": [], "entities": []}, {"text": "At the same time, we have also extended our previous feature set by introducing a richer set of automatically derived textbased features, proposed by, which capture deeper syntactic complexities of the text.", "labels": [], "entities": []}, {"text": "Unlike our previous work, the major goal of this paper is not trying to compare different machine learning techniques used in readability assessment task, but rather to compare the performance differences between with and without human labor involved within our previous proposed system framework.", "labels": [], "entities": []}, {"text": "We begin the paper with the description of related work in Section 2, followed by detailed explanation regarding data preparation and automatic annotations in Section 3.", "labels": [], "entities": [{"text": "data preparation", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.7525594532489777}]}, {"text": "The extended features will be covered in Section 4, followed by experimental analysis in Section 5, in which we will compare the results between human annotations and automatic annotations.", "labels": [], "entities": []}, {"text": "We will also report the system performance after incorporating the rich text features (structural features).", "labels": [], "entities": []}, {"text": "Conclusions follow in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiments, we look at how much the performance dropped by switching to zero human inputs.", "labels": [], "entities": []}, {"text": "We also investigate the impact of using a richer set of text-based features.", "labels": [], "entities": []}, {"text": "We apply the ranking-based book leveling algorithm proposed by our previous study) and use the SVM rank ranker) for our experiments.", "labels": [], "entities": [{"text": "ranking-based book leveling", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.5628142257531484}]}, {"text": "In this system, the ranker learns to sort the training books into leveled order.", "labels": [], "entities": []}, {"text": "The unknown test book is inserted into the ordering of the training books by the trained ranking model, and the predicted reading level is calculated by averaging over the levels of the known books above and below the test book.", "labels": [], "entities": []}, {"text": "Following the previous study, each book is uniformly partitioned into 4 parts, treating each sub-book as an individual entity.", "labels": [], "entities": []}, {"text": "A leave-n-out procedure is utilized for evaluation: during each iteration of the training, the system leaves out all n partitions (sub-books) corresponding to one book.", "labels": [], "entities": []}, {"text": "In the testing phase, the trained ranking model tests on all partitions corresponding to the held-out book.", "labels": [], "entities": []}, {"text": "We obtain a single predicted reading level for the held-out book by averaging the results for all its partitions; averaging produces a more robust result.", "labels": [], "entities": []}, {"text": "Two separate experiments are carried out on human-annotated and autoannotated PDF books respectively.", "labels": [], "entities": []}, {"text": "We use two metrics to determine quality: first, the accuracy of the system is computed by claiming it is correct if the predicted book level is within \u00b11 of the true reading level.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9995026588439941}]}, {"text": "The second scoring metric is the absolute error of number of levels away from the key reading level, averaged overall of the books.", "labels": [], "entities": [{"text": "absolute error", "start_pos": 33, "end_pos": 47, "type": "METRIC", "confidence": 0.8347654044628143}]}, {"text": "We report the experiment results on different combinations of feature sets: surface level features plus visually-oriented features, surface level features only, visually-oriented features only, structural features only and finally combining all the features together.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of books over Fountas and Pinnell  reading levels", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9638411402702332}, {"text": "Fountas and Pinnell  reading", "start_pos": 37, "end_pos": 65, "type": "DATASET", "confidence": 0.7134987264871597}]}, {"text": " Table 3: Results on 97 books using human annotations vs. automatic annotations, reporting accuracy within one level  and average error for 4 partitions per book.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9934167861938477}, {"text": "error", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.6625957489013672}]}]}