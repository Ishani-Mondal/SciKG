{"title": [{"text": "Word Segmentation on Chinese Mirco-Blog Data with a Linear-Time Incremental Model", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6660986393690109}, {"text": "Chinese Mirco-Blog Data", "start_pos": 21, "end_pos": 44, "type": "DATASET", "confidence": 0.8591839671134949}]}], "abstractContent": [{"text": "This paper describes the model we designed for the word segmentation bake-off on Chinese micro-blog data in the 2nd CIPS-SIGHAN joint conference on Chi-nese language processing.", "labels": [], "entities": [{"text": "word segmentation bake-off", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.8253607551256815}, {"text": "Chinese micro-blog data in the 2nd CIPS-SIGHAN joint conference", "start_pos": 81, "end_pos": 144, "type": "DATASET", "confidence": 0.7856101393699646}]}, {"text": "We presented a linear-time incremental model for word segmentation where rich features including character-based features, word-based features as well as other possible features can be easily employed.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7977593839168549}]}, {"text": "We report the performances of our model on four datasets in the SIGHAN bake-off 2005.", "labels": [], "entities": [{"text": "SIGHAN bake-off 2005", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.9408140579859415}]}, {"text": "After adding more features designed for the micro-blog data, the performance of our model is further improved.", "labels": [], "entities": []}, {"text": "The F-score of our model for this bake-off is 0.9478 and 44.88% of the sentences are segmented correctly.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9991045594215393}]}], "introductionContent": [{"text": "Chinese word segmentation is an important and fundamental task for Chinese language processing.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5844866633415222}, {"text": "Chinese language processing", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6999548673629761}]}, {"text": "General-purpose word segmentation is widely studied.", "labels": [], "entities": [{"text": "General-purpose word segmentation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6616139610608419}]}, {"text": "Micro-blog-related topic emergences and becomes anew research topic in recent years.", "labels": [], "entities": []}, {"text": "Therefore researchers pay more and more attention to the word segmentation model for Chinese micro-blog data.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7019686251878738}]}, {"text": "Motivated by the linear-time incremental parser proposed by and the word-based word segmentation model proposed by, first we presented a linear-time incremental word segmentation model.", "labels": [], "entities": [{"text": "word-based word segmentation", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.5789304375648499}, {"text": "word segmentation", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.7494803667068481}]}, {"text": "Various features including character-based features and word-based features can be employed while exponentially many segmented results can be tested in linear-time.", "labels": [], "entities": []}, {"text": "We report the performances of our model on four datasets in the SIGHAN bake-off 2005.", "labels": [], "entities": [{"text": "SIGHAN bake-off 2005", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.9408140579859415}]}, {"text": "One of the difficulties of training word segmentation model on micro-blog data is the lack of annotated micro-blog data (only 500 sentences of micro-blog data are provided and used by us).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7326217889785767}]}, {"text": "Following the annotation adaptation method proposed by, we train a general-purpose joint word segmentation and part-of-speech tagging model using People's Daily corpus.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7374602258205414}, {"text": "part-of-speech tagging", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.6890409588813782}, {"text": "People's Daily corpus", "start_pos": 146, "end_pos": 167, "type": "DATASET", "confidence": 0.9441235512495041}]}, {"text": "Then, the decoding results of such a model are used as features in the final word segmentation model for micro-blog data.", "labels": [], "entities": []}, {"text": "Moreover, various lexicon features such as dictionaries and word list of idioms are employed to segment micro-blog data.", "labels": [], "entities": []}, {"text": "Preprocessing is also conducted to deal with URLs and special characters.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.856310248374939}]}, {"text": "Finally, The F-score of our model for the bakeoff is 0.9478 and 44.88% of the sentences are segmented correctly.", "labels": [], "entities": [{"text": "F-score", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9990610480308533}]}, {"text": "The performance of our method is still far from perfect.", "labels": [], "entities": []}, {"text": "The lack of segmented micro-blog data is one of the bottlenecks of our model.", "labels": [], "entities": []}, {"text": "If more training data is provided, our model can reach better performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report the performances of our model on four SIGHAN05 datasets).", "labels": [], "entities": [{"text": "SIGHAN05 datasets", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.9443294405937195}]}, {"text": "Then we report the performance our model on the microblog data.", "labels": [], "entities": [{"text": "microblog data", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.8981411755084991}]}, {"text": "We use 5-fold cross validation for the development and use the whole dataset to train the final model for the test.", "labels": [], "entities": []}, {"text": "The F-score is used to evaluate the performance, which is the harmonic mean of precision (percentage of words that are correctly segmented in the results) and recall (percentage of words that are correctly segmented in the gold standard).", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9986640214920044}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9990237951278687}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9995275735855103}]}, {"text": "The results of our model and related work on the SIGHAN05 datasets are listed in.", "labels": [], "entities": [{"text": "SIGHAN05 datasets", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.8989033997058868}]}, {"text": "The results of the micro-blog data are listed in.", "labels": [], "entities": []}, {"text": "The first row is the final performance on the test data, while the following rows show the performances with different feature sets for the cross validation using 500 micro-blog sentences.", "labels": [], "entities": []}, {"text": "We can see that the additional features of the micro-blog data improve the performance.: Experiment results of our model on the micro-blog data For the annotated micro-blog data for the training is quite limited, the lexical features and taggerbased features are important for the performance.", "labels": [], "entities": []}, {"text": "Note that the F-score for the testis better than the F-score for the cross validation.", "labels": [], "entities": [{"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.998974084854126}, {"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9965112805366516}]}, {"text": "This may caused by that the training set for the former model is onequarter larger.", "labels": [], "entities": []}, {"text": "It may imply that the performance of our model is limited by the size of the training data and the performance of our model will be improved when larger training data was provided.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: F-scores of our model and models in related work on SIGHAN 05 bake-off data", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9970705509185791}, {"text": "SIGHAN 05 bake-off data", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.7781221643090248}]}, {"text": " Table 4: Experiment results of our model on the  micro-blog data", "labels": [], "entities": []}]}