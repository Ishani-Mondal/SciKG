{"title": [{"text": "Multiple TreeBanks Integration for Chinese Phrase Structure Grammar Parsing Using Bagging", "labels": [], "entities": [{"text": "Chinese Phrase Structure Grammar Parsing", "start_pos": 35, "end_pos": 75, "type": "TASK", "confidence": 0.6733837366104126}]}], "abstractContent": [{"text": "We describe our method of traditional Phrase Structure Grammar (PSG) parsing in CIPS-Bakeoff2012 Task3.", "labels": [], "entities": [{"text": "Phrase Structure Grammar (PSG) parsing", "start_pos": 38, "end_pos": 76, "type": "TASK", "confidence": 0.8262693924563271}, {"text": "CIPS-Bakeoff2012 Task3", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.9111455976963043}]}, {"text": "First, bagging is proposed to enhance the base-line performance of PSG parsing.", "labels": [], "entities": [{"text": "PSG parsing", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.9479475319385529}]}, {"text": "Then we suggest exploiting another TreeBank (CTB7.0) to improve the performance further.", "labels": [], "entities": [{"text": "TreeBank (CTB7.0", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.7773915330568949}]}, {"text": "Experimental results on the development data set demonstrate that bagging can boost the baseline F1 score from 81.33% to 84.41%.", "labels": [], "entities": [{"text": "development data set", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.8306641379992167}, {"text": "baseline", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9195221066474915}, {"text": "F1 score", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9336059987545013}]}, {"text": "After exploiting the data of CTB7.0, the F1 score reaches 85.03%.", "labels": [], "entities": [{"text": "CTB7.0", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9326825141906738}, {"text": "F1 score", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9796968400478363}]}, {"text": "Our final results on the official test data set show that the baseline closed system using bagging gets the F1 score of 80.17%.", "labels": [], "entities": [{"text": "official test data set", "start_pos": 25, "end_pos": 47, "type": "DATASET", "confidence": 0.8640496134757996}, {"text": "F1 score", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9919260144233704}]}, {"text": "It outperforms the best closed system by nearly 4% which uses a single model.", "labels": [], "entities": []}, {"text": "After exploiting the CTB7.0 data, the F1 score reaches 81.16%, demonstrating further increases of about 1%.", "labels": [], "entities": [{"text": "CTB7.0 data", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.9749327003955841}, {"text": "F1 score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9861959517002106}]}], "introductionContent": [{"text": "Over the past decade, Phrase Structure Grammar (PSG) parsing has been investigated by many researchers.", "labels": [], "entities": [{"text": "Phrase Structure Grammar (PSG) parsing", "start_pos": 22, "end_pos": 60, "type": "TASK", "confidence": 0.8804428236825126}]}, {"text": "Most methods of PSG parsing exploited some manly annotated corpus and proposed a single statistical model) based on the corpus.", "labels": [], "entities": [{"text": "PSG parsing", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.9737691283226013}]}, {"text": "For Chinese, Tsinghua Chinese Treebank (TCT)) and Penn Chinese TreeBank (CTB) () are two most popular manly annotated corpus.", "labels": [], "entities": [{"text": "Tsinghua Chinese Treebank (TCT))", "start_pos": 13, "end_pos": 45, "type": "DATASET", "confidence": 0.7861071775356928}, {"text": "Penn Chinese TreeBank (CTB)", "start_pos": 50, "end_pos": 77, "type": "DATASET", "confidence": 0.9619967142740885}]}, {"text": "In this paper, we are especially interested in parser combination.", "labels": [], "entities": [{"text": "parser combination", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.887889176607132}]}, {"text": "Many past works have suggest a number of methods for parser combination.", "labels": [], "entities": [{"text": "parser combination", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.8822047412395477}]}, {"text": "These methods concern on combing different parsers which are trained on the same corpus.", "labels": [], "entities": []}, {"text": "proposed a constituent reparsing method for multiple parsers combination.", "labels": [], "entities": []}, {"text": "proposed a linear modelbased general framework to combine several lexicalized parsers and un-lexicalized parsers (;.", "labels": [], "entities": []}, {"text": "Out method is different from the past works in that we combine different parsers which exploit the same method but the models of which are trained on different corpus.", "labels": [], "entities": []}, {"text": "We adopt Berkeley parser) to train our sub-models.", "labels": [], "entities": []}, {"text": "It is an un-lexicalized probabilistic context free grammar (PCFG) parser.", "labels": [], "entities": []}, {"text": "At the beginning, we train a number of submodels by sampling TCT corpus repeatedly, and meanwhile train a number of submodels by sampling CTB corpus repeatedly.", "labels": [], "entities": [{"text": "TCT corpus", "start_pos": 61, "end_pos": 71, "type": "DATASET", "confidence": 0.719849020242691}, {"text": "CTB corpus", "start_pos": 138, "end_pos": 148, "type": "DATASET", "confidence": 0.9357307255268097}]}, {"text": "Then we combine these submodels by reparsing the parsing results of them using the CKY-parsing algorithm (.", "labels": [], "entities": []}, {"text": "To enable using CKY-parsing algorithm for combining, we must handle the following two issues: 1.", "labels": [], "entities": []}, {"text": "Binarization should be applied to the parsing results of submodels.", "labels": [], "entities": [{"text": "Binarization", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9464085102081299}]}, {"text": "2. The grammars of TCT corpus are very different that of CTB corpus.", "labels": [], "entities": [{"text": "TCT corpus", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.8127923607826233}, {"text": "CTB corpus", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.8831114172935486}]}, {"text": "We should transform CTB grammars into TCT grammars before final combination.", "labels": [], "entities": []}, {"text": "If these two issues have been done already, we can apply CKY reparsing algorithm and get the final parsing result.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the overall system architecture.", "labels": [], "entities": []}, {"text": "And then we introduce our method in detail.", "labels": [], "entities": []}, {"text": "In section 3 we present the binarization algorithm used in the system.", "labels": [], "entities": []}, {"text": "Section 4 describes the CKY reparsing algorithm.", "labels": [], "entities": [{"text": "CKY reparsing", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.7242922782897949}]}, {"text": "Section 5 describes our baseline method and multiple TreeBank bagging method systematically.", "labels": [], "entities": []}, {"text": "Section 6 shows the experimental results and finally in section 7 we conclude our method and give our future works.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of Data Set.", "labels": [], "entities": [{"text": "Statistics of Data Set", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.7921621799468994}]}, {"text": " Table 2: Final results on the development set.", "labels": [], "entities": []}, {"text": " Table 3: Final results on the development set.", "labels": [], "entities": []}]}