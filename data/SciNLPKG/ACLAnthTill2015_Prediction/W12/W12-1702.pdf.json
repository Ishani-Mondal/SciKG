{"title": [{"text": "Semi-supervised learning for automatic conceptual property extraction", "labels": [], "entities": [{"text": "automatic conceptual property extraction", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.6543106734752655}]}], "abstractContent": [{"text": "For a given concrete noun concept, humans are usually able to cite properties (e.g., elephant is animal, car has wheels) of that concept ; cognitive psychologists have theorised that such properties are fundamental to understanding the abstract mental representation of concepts in the brain.", "labels": [], "entities": []}, {"text": "Consequently, the ability to automatically extract such properties would be of enormous benefit to the field of experimental psychology.", "labels": [], "entities": []}, {"text": "This paper investigates the use of semi-supervised learning and support vector machines to automatically extract concept-relation-feature triples from two large corpora (Wikipedia and UKWAC) for concrete noun concepts.", "labels": [], "entities": [{"text": "UKWAC", "start_pos": 184, "end_pos": 189, "type": "DATASET", "confidence": 0.9309043288230896}]}, {"text": "Previous approaches have relied on manually-generated rules and hand-crafted resources such as WordNet; our method requires neither yet achieves better performance than these prior approaches, measured both by comparison with a property norm-derived gold standard as well as direct human evaluation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9571232199668884}]}, {"text": "Our technique performs particularly well on extracting features relevant to a given concept, and suggests a number of promising areas for future focus.", "labels": [], "entities": []}], "introductionContent": [{"text": "The representation of concrete concepts (e.g., car, banana, spanner) in the human brain has long been an important area of investigation for cognitive psychologists.", "labels": [], "entities": []}, {"text": "Recent theories of this mental representation have proposed a componential, propertybased and distributed model of conceptual knowledge (e.g.,,,).", "labels": [], "entities": []}, {"text": "In order to empirically test these cognitive theories, researchers have moved towards employing real-world knowledge in their experiments.", "labels": [], "entities": []}, {"text": "This knowledge has usually been procured from humanderived lists of properties taken from property norming studies ().", "labels": [], "entities": []}, {"text": "In such studies, human participants are asked to describe and note properties of a given concept (e.g., has shell for turtle).", "labels": [], "entities": []}, {"text": "Synonymous responses are grouped together as a single property and those meeting a certain minimum responsefrequency threshold are taken as valid properties.", "labels": [], "entities": []}, {"text": "The most wide-ranging study to date was that conducted by: some sample properties from this set are in.", "labels": [], "entities": []}, {"text": "As others have noted), property norming studies are prone to a number of deficiencies.", "labels": [], "entities": []}, {"text": "One such weakness is the incongruity of shared properties across even highlyrelated concepts: human respondents exhibit alack of consistency when listing properties that are common to many similar concepts.", "labels": [], "entities": [{"text": "consistency", "start_pos": 129, "end_pos": 140, "type": "METRIC", "confidence": 0.9645366668701172}]}, {"text": "For example, while has legs is listed as a property of crocodile in the McRae norms, it is absent as a property of alligator.", "labels": [], "entities": [{"text": "McRae norms", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.9745137691497803}]}, {"text": "A related issue is the non-comprehensive nature of the generated norms -although they may cover the most salient properties fora given concept, they are unlikely to comprise all of a concept's properties (e.g., has heart does not appear as a property of any of the 92 animal concepts).", "labels": [], "entities": []}, {"text": "Our research aims to use NLP techniques to create a system able to emulate the output of such studies, and overcome some of the aforementioned weaknesses.", "labels": [], "entities": []}, {"text": "Our proposed system begins by searching dependency-parsed corpora for those sentences containing concept and feature terms which are also found in a McRae norm-derived training set of properties.", "labels": [], "entities": [{"text": "McRae norm-derived training set", "start_pos": 149, "end_pos": 180, "type": "DATASET", "confidence": 0.7629837691783905}]}, {"text": "For these sentences, the system generates grammatical relation/part-of-speech structural attributes and applies support vector machines (SVMs) to learn sets of attributes likely to indicate the instantiation of a property in a sentence.", "labels": [], "entities": []}, {"text": "found in kitchens 7: Top ten properties from McRae norms with production frequencies for turtle and bowl.", "labels": [], "entities": [{"text": "McRae norms", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9528749883174896}]}, {"text": "learned patterns of salient attributes are finally applied to a corpus to derive new properties for unseen concepts.", "labels": [], "entities": []}, {"text": "Our task is a challenging one: the properties we seek are extremely diverse in their form.", "labels": [], "entities": []}, {"text": "They range from the simple (e.g., banana is yellow) to the complex (e.g., bayonet found at the end of a gun).", "labels": [], "entities": []}, {"text": "Although the properties can broadly be divided into a number of categories (encyclopedic, taxonomic, functional, etc) there is not a great deal of regularity in the nature of the properties a given noun will likely possess: it is highly concept-dependent.", "labels": [], "entities": []}, {"text": "Furthermore, we hope to derive these properties from corpora, with the assumption that these properties will manifest themselves therein.", "labels": [], "entities": []}, {"text": "Indeed, discuss a theory of human knowledge which relies on a combination of both distributional (i.e., derived from spoken and written language) and experiential data (i.e., that derived from our interactions with the real world), claiming that the necessary contribution of each data-type fora comprehensive human semantic representation is non-trivial.", "labels": [], "entities": []}, {"text": "Finally, there are difficulties associated with evaluating our system's output directly against a set of human-generated property norms: we discuss these in further detail later.", "labels": [], "entities": []}, {"text": "Given their provenance, the properties found in property norms are free-form.", "labels": [], "entities": []}, {"text": "To simplify our task we apply a more rigid representation to the properties we already have and to those we aim to seek.", "labels": [], "entities": []}, {"text": "We delineate each property into a concept relation feature triple (see Section 2.2) and our task becomes one of finding valid relation feature pairs given a particular concept.", "labels": [], "entities": []}, {"text": "This recoding renders our task more well-defined and makes evaluation of our method more comparable to previous and related work.", "labels": [], "entities": []}, {"text": "Having framed our task in this way, there is an obvious parallel with relation extraction: both necessitate the selection/classification of relationships between individual entities (in our case, between concept and feature).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7923834919929504}, {"text": "selection/classification of relationships between individual entities", "start_pos": 112, "end_pos": 181, "type": "TASK", "confidence": 0.7328037321567535}]}, {"text": "Hearst (1992) was the first to propose a pattern-based approach to this task using lexico-syntactic patterns to automatically extract hyponyms and this technique has frequently been used for ontology learning.", "labels": [], "entities": [{"text": "ontology learning", "start_pos": 191, "end_pos": 208, "type": "TASK", "confidence": 0.8963987231254578}]}, {"text": "For example, linked instantiations of a set of semantic relations into existing semantic ontologies and employed seed concepts from a given semantic class to discover relations shared by concepts in that class.", "labels": [], "entities": []}, {"text": "Our task is more complex than classic relation extraction for two main reasons: 1) the relations which we aim to extract are not limited to a small set of just a few well-defined relations (e.g., is-a and partof) nor to the relations of a specific semantic class (e.g., capital-is for countries).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7774245142936707}]}, {"text": "Indeed the relations can be as many and diverse as the concepts themselves (e.g., each concept could possess a unique and distinguishing relation and feature).", "labels": [], "entities": []}, {"text": "2) We are attempting to simultaneously extract two pieces of information: features of the concept and those features' defining relationship with the concept, but only those relations and features which would be classified as 'common-sense', something which is easy for humans to recognise but difficult (if not impossible) to describe rigorously or formally.", "labels": [], "entities": []}, {"text": "There has recently been work on the automatic ex-traction of binary relations that scale to a web corpus, for example the ReVerb (Etzioni et al., 2011) and WOE ( systems.", "labels": [], "entities": [{"text": "ReVerb", "start_pos": 122, "end_pos": 128, "type": "DATASET", "confidence": 0.8983139991760254}, {"text": "WOE", "start_pos": 156, "end_pos": 159, "type": "DATASET", "confidence": 0.8559625148773193}]}, {"text": "These systems are designed to extract legitimate relations from a given sentence.", "labels": [], "entities": []}, {"text": "In contrast, our aim is to capture more general relationships which are 'commonsense'; just because an extracted relation is correct in a given context does not automatically make it true in general.", "labels": [], "entities": []}, {"text": "Previous reasoned approaches to our task have taken their lead from Hearst and her successors, employing manually-created rulesets to extract such properties from corpora (e.g.,, , and our comparison system ().", "labels": [], "entities": []}, {"text": "Baroni et al. extract relational information in the form of 'type-sketches', which give an approximate, implicit description of the relationship whereas we are aiming to extract explicit relations between the target concept and its corresponding features.", "labels": [], "entities": []}, {"text": "have attempted this, but both employ WordNet to extract semantic relatedness information.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9552175998687744}]}, {"text": "We use semi-supervised learning as it offers a flexible technique of harnessing small amounts of labelled data to derive information from unlabelled datasets/corpora and allows us to guide the extraction towards our desired 'common-sense' output.", "labels": [], "entities": []}, {"text": "We chose SVMs as they have been used fora variety of tasks in NLP (e.g.,,).", "labels": [], "entities": []}, {"text": "We will demonstrate that our system's performance exceeds that of and.", "labels": [], "entities": []}, {"text": "It is, as far as we are aware, the first work to employ semisupervised learning for this task.", "labels": [], "entities": []}], "datasetContent": [{"text": "We also wanted to ascertain the extent to which the output from both our corpora could be combined to improve results, balancing the encyclopedic but somewhat specific nature of Wikipedia with the generality and breadth of the UKWAC corpus.", "labels": [], "entities": [{"text": "breadth", "start_pos": 212, "end_pos": 219, "type": "METRIC", "confidence": 0.9757488965988159}, {"text": "UKWAC corpus", "start_pos": 227, "end_pos": 239, "type": "DATASET", "confidence": 0.9906852543354034}]}, {"text": "We combined the output by summing individual SVM scores of each triple from both corpora to yield a combined SVM score.", "labels": [], "entities": []}, {"text": "PMI and LL scores were then calculated as usual from this combined set of triples.", "labels": [], "entities": [{"text": "PMI", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.720861554145813}, {"text": "LL scores", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.8740767240524292}]}, {"text": "We employ ten-fold cross-validation to ascertain optimal SVM, LL and PMI \u03b2 parameters for our final system.", "labels": [], "entities": [{"text": "PMI \u03b2", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.8888016641139984}]}, {"text": "We exclude 44 concepts from our set of: Our best scores on the ESSLLI set compared to and the ReVerb system).", "labels": [], "entities": [{"text": "ESSLLI set", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.819989949464798}, {"text": "ReVerb system", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.8245561420917511}]}, {"text": "Our results are from the verb-augmented vectortype, using the combined UKWAC-Wikipedia corpus and using the \u03b2 parameters highlighted in 510 to use in our final system testing and split the remaining 466 concepts randomly and evenly into 10 folds.", "labels": [], "entities": [{"text": "UKWAC-Wikipedia corpus", "start_pos": 71, "end_pos": 93, "type": "DATASET", "confidence": 0.994927853345871}]}, {"text": "We apply the training steps above to nine of the folds, generating predictions for the single held-out fold.", "labels": [], "entities": []}, {"text": "We repeat this for all ten folds, yielding relations and features with SVM, LL and PMI scores for our full set of 466 training concepts on the UKWAC, Wikipedia and combined corpora.", "labels": [], "entities": [{"text": "PMI", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.936750590801239}, {"text": "UKWAC", "start_pos": 143, "end_pos": 148, "type": "DATASET", "confidence": 0.9929521679878235}]}, {"text": "We varied the \u03b2 values from our scoring equation in the range (interval 0.05) and compared the top twenty triples for each concept directly against the held-out training set.", "labels": [], "entities": []}, {"text": "The best F-scores and their corresponding \u03b2 values (evaluating on full triples and concept-feature pairs alone) are in Table 3.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9576747417449951}]}, {"text": "We can see that our best results employ the verb-augmented vector-type and the combined corpus, with a best F-score of 0.2859 when ignoring the relation term and 0.1494 when including it in the evaluation.", "labels": [], "entities": [{"text": "F-score", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9979274272918701}]}, {"text": "The main difference between these two results is the relative contribution of the reweighting factors: the SVM score is the most important overall, but the LL and PMI scores come into play when evaluating without the relation.", "labels": [], "entities": [{"text": "reweighting", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.8715142607688904}, {"text": "SVM score", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.8837938010692596}, {"text": "PMI", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.9297745227813721}]}, {"text": "This could be explained by the fact that the PMI and LL scores do not use any relation terms in their calculations.", "labels": [], "entities": []}, {"text": "The unseen subset of the McRae norms is a set of human-generated common-sense properties with which our extracted properties can be compared.", "labels": [], "entities": [{"text": "McRae norms", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.8903716206550598}]}, {"text": "However, an issue with the McRae norms is that semantically identical properties can be represented by lexically different triples.", "labels": [], "entities": [{"text": "McRae norms", "start_pos": 27, "end_pos": 38, "type": "DATASET", "confidence": 0.8370250463485718}]}, {"text": "This problem was acknowledged by: Inter-annotator agreement for our best system, both including and excluding the relation. are correct properties being generated which simply don't appear in the ESSLLI evaluation set.", "labels": [], "entities": [{"text": "ESSLLI evaluation set", "start_pos": 196, "end_pos": 217, "type": "DATASET", "confidence": 0.8439973394076029}]}, {"text": "In order to address these concerns, we also performed a human evaluation on 15 of our concepts.", "labels": [], "entities": []}, {"text": "We asked two native English-speaking judges to decide whether a given triple was correct, 6 plausible, 7 wrong but related, 8 or wrong.", "labels": [], "entities": []}, {"text": "We executed the human evaluation on our two best systems (as described above).", "labels": [], "entities": []}, {"text": "As there were shared triples and concept-feature pairs across the two output sets, each triple and pair was evaluated only once.", "labels": [], "entities": []}, {"text": "The judges were aware of the purposes of the study but were blind to the source sets.", "labels": [], "entities": []}, {"text": "Some example judgements are in.", "labels": [], "entities": []}, {"text": "The agreement results across all 15 concepts together with their \u03ba coefficients (Cohen, 1960) are in.", "labels": [], "entities": []}, {"text": "In our evaluation we conflated the correct/plausible and wrong but related/wrong categories (see also and ).", "labels": [], "entities": []}, {"text": "We did this because of the subjective nature of the judgements, and because we are seeking properties which are indeed corrector at least plausible.", "labels": [], "entities": []}, {"text": "These results indicate that our system is extracting corrector plausible triples 51.1% of the time (rising to 76.8% when considering features only).", "labels": [], "entities": []}, {"text": "They also demonstrate a marked discrepancy between the results for our two evaluations, reflecting the necessity of human evaluation when assessing our particular task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Parameter estimation both with and without relation, using our augmented and non-augmented vector-types  and across our two corpora and the combined corpora set.", "labels": [], "entities": [{"text": "Parameter estimation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6798288524150848}]}, {"text": " Table 4: Our best scores on the ESSLLI set compared to", "labels": [], "entities": [{"text": "ESSLLI set", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.7587903141975403}]}, {"text": " Table 6: Inter-annotator agreement for our best system,  both including and excluding the relation.", "labels": [], "entities": []}]}