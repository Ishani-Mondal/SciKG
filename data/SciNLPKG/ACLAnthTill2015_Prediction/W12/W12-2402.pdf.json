{"title": [{"text": "Bootstrapping Biomedical Ontologies for Scientific Text using NELL", "labels": [], "entities": [{"text": "NELL", "start_pos": 62, "end_pos": 66, "type": "TASK", "confidence": 0.5348498225212097}]}], "abstractContent": [{"text": "We describe an open information extraction system for biomedical text based on NELL (the Never-Ending Language Learner) (Carl-son et al., 2010), a system designed for extraction from Web text.", "labels": [], "entities": []}, {"text": "NELL uses a coupled semi-supervised bootstrapping approach to learn new facts from text, given an initial ontology and a small number of \"seeds\" for each ontology category.", "labels": [], "entities": []}, {"text": "In contrast to previous applications of NELL, in our task the initial ontology and seeds are automatically derived from existing resources.", "labels": [], "entities": []}, {"text": "We show that NELL's bootstrapping algorithm is susceptible to ambiguous seeds, which are frequent in the biomedical domain.", "labels": [], "entities": []}, {"text": "Using NELL to extract facts from biomedical text quickly leads to semantic drift.", "labels": [], "entities": []}, {"text": "To address this problem, we introduce a method for assessing seed quality , based on a larger corpus of data derived from the Web.", "labels": [], "entities": []}, {"text": "In our method, seed quality is assessed at each iteration of the bootstrap-ping process.", "labels": [], "entities": []}, {"text": "Experimental results show significant improvements over NELL's original bootstrapping algorithm on two types of tasks: learning terms from biomedical categories, and named-entity recognition for biomedical entities using a learned lexicon.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 166, "end_pos": 190, "type": "TASK", "confidence": 0.7407592535018921}]}], "introductionContent": [{"text": "NELL (the Never-Ending Language Learner) is a semi-supervised learning system, designed for extraction of information from the Web.", "labels": [], "entities": []}, {"text": "The system uses a coupled semi-supervised bootstrapping approach to learn new facts from text, given an initial ontology and a small number of \"seeds\", i.e., labeled examples for each ontology category.", "labels": [], "entities": []}, {"text": "The new facts are stored in a growing structured knowledge base.", "labels": [], "entities": []}, {"text": "One of the concerns about gathering data from the Web is that it comes from various un-authoritative sources, and may not be reliable.", "labels": [], "entities": []}, {"text": "This is especially true when gathering scientific information.", "labels": [], "entities": []}, {"text": "In contrast to Web data, scientific text is potentially more reliable, as it is guided by the peer-review process.", "labels": [], "entities": []}, {"text": "Open access scientific archives make this information available for all.", "labels": [], "entities": []}, {"text": "In fact, the production rate of publicly available scientific data far exceeds the ability of researchers to \"manually\" process it, and there is a growing need for the automation of this process.", "labels": [], "entities": []}, {"text": "The biomedical field presents a great potential for text mining applications.", "labels": [], "entities": [{"text": "text mining", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.8968071937561035}]}, {"text": "An integral part of life science research involves production and publication of large collections of data by curators, and as part of collaborative community effort.", "labels": [], "entities": []}, {"text": "Prominent examples include: publication of genomic sequence data, e.g., by the Human Genome Project; online collections of three-dimensional coordinates of protein structures; and databases holding data on genes.", "labels": [], "entities": []}, {"text": "An important resource, initiated as a means of enforcing data standardization, are ontologies describing biological, chemical and medical terms.", "labels": [], "entities": []}, {"text": "These are heavily used by the research community.", "labels": [], "entities": []}, {"text": "With this wealth of available data the biomedical field holds many information extraction opportunities.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.7749981880187988}]}, {"text": "We describe an open information extraction system adapting NELL to the biomedical domain.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.7749061584472656}, {"text": "NELL", "start_pos": 59, "end_pos": 63, "type": "TASK", "confidence": 0.8881171345710754}]}, {"text": "We present an implementation of our approach, named BioNELL, which uses three main sources of information: (1) a public corpus of biomedical scientific text,: Two samples of fruit-fly genes, taken from the complete fly gene dictionary.", "labels": [], "entities": []}, {"text": "High PMI Seeds are the top 50 terms selected using PMI ranking, and Random Seeds area random draw of 50 terms from the dictionary.", "labels": [], "entities": []}, {"text": "These are used as seeds for the Fly Gene category (Section 4.2).", "labels": [], "entities": [{"text": "Fly Gene category", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.7142205039660136}]}, {"text": "Notice that the random set contains many terms that are often not used as genes including arm, 28, and dad.", "labels": [], "entities": [{"text": "arm", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9259562492370605}]}, {"text": "Using these as seeds can lead to semantic drift.", "labels": [], "entities": []}, {"text": "In contrast, high PMI seeds exhibit much less ambiguity.", "labels": [], "entities": []}, {"text": "(3) a corpus of Web documents.", "labels": [], "entities": []}, {"text": "NELL's ontology, including categories and seeds, has been manually designed during the system development.", "labels": [], "entities": []}, {"text": "Ontology design involves assembling a set of interesting categories, organized in a meaningful hierarchical structure, and providing representative seeds for each category.", "labels": [], "entities": [{"text": "Ontology design", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8979509770870209}]}, {"text": "Redesigning anew ontology fora technical domain is difficult without non-trivial knowledge of the domain.", "labels": [], "entities": []}, {"text": "We describe a process of merging source ontologies into one structure of categories with seed examples.", "labels": [], "entities": []}, {"text": "However, as we will show, using NELL's bootstrapping algorithm to extract facts from a biomedical corpus is susceptible to noisy and ambiguous terms.", "labels": [], "entities": []}, {"text": "Such ambiguities are common in biomedical terminology (see examples in), and some ambiguous terms are heavily used in the literature.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"We have cloned an induced white mutation and characterized the insertion sequence responsible for the mutant phenotype\", white is an ambiguous term referring to the name of a gene.", "labels": [], "entities": []}, {"text": "In NELL, ambiguity is limited using coupled semi-supervised learning: if two categories in the ontology are declared mutually exclusive, instances of one category are used as negative examples for the other, and the two categories cannot share any instances.", "labels": [], "entities": []}, {"text": "To resolve the ambiguity of white with mutual exclusion, we would have to include a Color category in the ontology, and declare it mutually exclusive with the Gene category.", "labels": [], "entities": []}, {"text": "Then, instances of Color will not be able to refer to genes in the KB.", "labels": [], "entities": []}, {"text": "It is hard to estimate what additional categories should be added, and building a \"complete\" ontology tree is practically infeasible.", "labels": [], "entities": []}, {"text": "NELL also includes a polysemy resolution component that acknowledges that one term, for example white, may refer to two distinct concepts, say a color and a gene, that map to different ontology categories, such as Color and Fly Gene.", "labels": [], "entities": [{"text": "polysemy resolution", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7357649207115173}]}, {"text": "By including a Color category, this component can identify that white is both a color and a gene.", "labels": [], "entities": []}, {"text": "The polysemy resolver performs word sense induction and synonym resolution based on relations defined between categories in the ontology, and labeled synonym examples.", "labels": [], "entities": [{"text": "polysemy resolver", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.66362664103508}, {"text": "word sense induction", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.7956735491752625}, {"text": "synonym resolution", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.9290382564067841}]}, {"text": "However, at present, BioNELL's ontology does not contain relation definitions (it is based only on categories), so we cannot include this component in our experiments.", "labels": [], "entities": []}, {"text": "Additionally, it is unclear how to avoid the use of polysemous terms as category seeds, and no method has been suggested for selecting seeds that are representative of a single specific category.", "labels": [], "entities": []}, {"text": "To address the problem of ambiguity, we introduce a method for assessing the desirability of noun phrases to be used as seeds fora specific target category.", "labels": [], "entities": []}, {"text": "We propose ranking seeds using a Pointwise Mutual Information (PMI) -based collocation measure fora seed and a category name.", "labels": [], "entities": []}, {"text": "Collocation is measured based on a large corpus of domainindependent data derived from the Web, accounting for uses of the seed in many different contexts.", "labels": [], "entities": []}, {"text": "NELL's bootstrapping algorithm uses the morphological and semantic features of seeds to propose new facts, which are added to the KB and used as seeds in the next bootstrapping iteration to learn more facts.", "labels": [], "entities": [{"text": "NELL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8483735918998718}]}, {"text": "This means that ambiguous terms maybe added at any learning iteration.", "labels": [], "entities": []}, {"text": "Since white really is a name of a gene, it is sometimes used in the same semantic context as other genes, and maybe added to the KB despite not being used as an initial seed.", "labels": [], "entities": []}, {"text": "To resolve this problem, we propose measuring seed quality in a Rank-and-Learn bootstrapping methodology: after every iteration, we rank all the instances that have been added to the KB by their quality as potential category seeds.", "labels": [], "entities": []}, {"text": "Only high-ranking instances are used as seeds in the next iteration.", "labels": [], "entities": []}, {"text": "Lowranking instances are stored in the KB and \"remembered\" as true facts, but are not used for learning new information.", "labels": [], "entities": []}, {"text": "This is in contrast to NELL's approach (and most other bootstrapping systems), in which there is no distinction between acquired facts, and facts that are used for learning.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using BioNELL we can learn lexicons, collections of category terms accumulated after running the system.", "labels": [], "entities": []}, {"text": "One evaluation approach is to select a set of learned instances and assess their correctness (.", "labels": [], "entities": []}, {"text": "This is relatively easy for data extracted for general categories like City or Sports Team.", "labels": [], "entities": []}, {"text": "For example, it is easy to evaluate the statement \"London is a City\".", "labels": [], "entities": [{"text": "London is a City", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.9085515737533569}]}, {"text": "This task becomes more difficult when assessing domain-specific facts such as \"Beryllium is an S-block molecular entity\" (in fact, it is).", "labels": [], "entities": []}, {"text": "We cannot, for example, use the help of Mechanical Turk for this task.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.7717623114585876}]}, {"text": "A possible alternative evaluation approach is asking an expert.", "labels": [], "entities": []}, {"text": "On top of being a costly and slow approach, the range of topics covered by BioNELL is large and a single expert is not likely be able to assess all of them.", "labels": [], "entities": []}, {"text": "We evaluated lexicons learned by BioNELL by comparing them to available resources.", "labels": [], "entities": []}, {"text": "Lexicons of gene names for certain species are available, and the Freebase database, an open repository holding data for millions of entities, includes some biomedical concepts.", "labels": [], "entities": [{"text": "Freebase database", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.9806512594223022}]}, {"text": "For most biomedical categories, however, complete lexicons are scarce.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Two samples of fruit-fly genes, taken from the  complete fly gene dictionary. High PMI Seeds are the top  50 terms selected using PMI ranking, and Random Seeds  are a random draw of 50 terms from the dictionary. These  are used as seeds for the Fly Gene category (Section 4.2).", "labels": [], "entities": []}, {"text": " Table 3: Precision, total number of instances (Total),  and correct instances (Correct) of gene lexicons learned  with BioNELL and NELL. BioNELL significantly im- proves the precision of the learned lexicon compared with  NELL. When examining only the first 132 learned items,  BioNELL has both higher precision and more correct in- stances than NELL (last row, NELL by size 132 ).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.998454213142395}, {"text": "BioNELL", "start_pos": 138, "end_pos": 145, "type": "METRIC", "confidence": 0.9261035323143005}, {"text": "precision", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.9975456595420837}, {"text": "precision", "start_pos": 303, "end_pos": 312, "type": "METRIC", "confidence": 0.9988380074501038}]}, {"text": " Table 4: Precision, total number of instances (Total), and correct instances (Correct) of learned lexicons of Chemical  Compound (CC), Drug, and Disease. BioNELL's lexicons have higher precision on all categories compared with  NELL, while learning a similar number of correct instances. When restricting NELL to a total lexicon size similar to  BioNELL's, BioNELL has both higher precision and more correct instances (last row, NELL by size ).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9989635944366455}, {"text": "precision", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.9940443634986877}, {"text": "precision", "start_pos": 382, "end_pos": 391, "type": "METRIC", "confidence": 0.9955090284347534}]}, {"text": " Table 5: Precision, total number of predicted genes (To- tal), and correct predictions (Correct), in a named-entity  recognition task using a complete lexicon, a filtered lex- icon, and lexicons learned with BioNELL and NELL.  BioNELL's lexicon achieves the highest precision, and  makes more correct predictions than NELL.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9929661750793457}, {"text": "correct predictions (Correct)", "start_pos": 68, "end_pos": 97, "type": "METRIC", "confidence": 0.8026642084121705}, {"text": "named-entity  recognition task", "start_pos": 104, "end_pos": 134, "type": "TASK", "confidence": 0.8069157004356384}, {"text": "precision", "start_pos": 267, "end_pos": 276, "type": "METRIC", "confidence": 0.997246265411377}]}]}