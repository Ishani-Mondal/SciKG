{"title": [{"text": "Full Machine Translation for Factoid Question Answering", "labels": [], "entities": [{"text": "Factoid Question Answering", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.733101507027944}]}], "abstractContent": [{"text": "In this paper we present an SMT-based approach to Question Answering (QA).", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 28, "end_pos": 37, "type": "TASK", "confidence": 0.995460569858551}, {"text": "Question Answering (QA)", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.8794319987297058}]}, {"text": "QA is the task of extracting exact answers in response to natural language questions.", "labels": [], "entities": [{"text": "extracting exact answers in response to natural language questions", "start_pos": 18, "end_pos": 84, "type": "TASK", "confidence": 0.5237869620323181}]}, {"text": "In our approach, the answer is a translation of the question obtained with an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9795040488243103}]}, {"text": "We use the n-best translations of a given question to find similar sentences in the document collection that contain the real answer.", "labels": [], "entities": []}, {"text": "Although it is not the first time that SMT inspires a QA system, it is the first approach that uses a full Machine Translation system for generating answers.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9940192699432373}, {"text": "Machine Translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7330524325370789}]}, {"text": "Our approach is validated with the datasets of the TREC QA evaluation.", "labels": [], "entities": [{"text": "TREC QA evaluation", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.836129903793335}]}], "introductionContent": [{"text": "Question Answering (QA) is the task of extracting short, relevant textual answers from a given document collection in response to natural language questions.", "labels": [], "entities": [{"text": "Question Answering (QA) is the task of extracting short, relevant textual answers from a given document collection in response to natural language questions", "start_pos": 0, "end_pos": 156, "type": "Description", "confidence": 0.675909412594942}]}, {"text": "QA extends IR techniques because it outputs concrete answers to a question instead of references to full documents which are relevant to a query.", "labels": [], "entities": [{"text": "IR", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9834632277488708}]}, {"text": "QA has attracted the attention of researchers for some years, and several public evaluations have been recently carried in the TREC, CLEF, and NTCIR conferences (.", "labels": [], "entities": [{"text": "QA", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.47042763233184814}, {"text": "TREC", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.7861381769180298}, {"text": "CLEF", "start_pos": 133, "end_pos": 137, "type": "DATASET", "confidence": 0.5645301342010498}, {"text": "NTCIR", "start_pos": 143, "end_pos": 148, "type": "DATASET", "confidence": 0.8960258960723877}]}, {"text": "All the example questions of this paper are extracted from the TREC evaluations.", "labels": [], "entities": [{"text": "TREC evaluations", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.6566417664289474}]}, {"text": "QA systems are usually classified according to what kind of questions they can answer; factoid, definitional, how to or why questions are treated in a distinct way.", "labels": [], "entities": []}, {"text": "This work focuses on factoid questions, that is, those questions whose answers are semantic entities (e.g., organisation names, person names, numbers, dates, objects, etc.).", "labels": [], "entities": [{"text": "factoid questions, that is, those questions whose answers are semantic entities (e.g., organisation names, person names, numbers, dates, objects, etc.)", "start_pos": 21, "end_pos": 172, "type": "Description", "confidence": 0.7870626807212829}]}, {"text": "For example, the question Q1545: What is a female rabbit called? is factoid and its answer, \"doe\" , is a semantic entity (although not a named entity).", "labels": [], "entities": []}, {"text": "Factoid questions written in natural language contain implicit information about the relations between the concepts expressed and the expected outcomes of the search, and QA explicitly exploits this information.", "labels": [], "entities": []}, {"text": "Using an IR engine to lookup a boolean query would not consider the relations therefore losing important information.", "labels": [], "entities": []}, {"text": "Consider the question Q0677: What was the name of the television show, starring Karl Malden, that had San Francisco in the title? and the candidate answer A.", "labels": [], "entities": []}, {"text": "In this question, two types of constraints are expressed over the candidate answers.", "labels": [], "entities": []}, {"text": "One is that the expected type of A is a kind of \"television show.\"", "labels": [], "entities": []}, {"text": "The rest of the question indicates that \"Karl Malden\" is related to A as being \"starred\" by, and that \"San Francisco\" is a substring of A.", "labels": [], "entities": []}, {"text": "Many factoid questions explicitly express an hyponymy relation about the answer type, and also several other relations describing its context (i.e. spatial, temporal, etc.).", "labels": [], "entities": []}, {"text": "The QA problem can be approached from several points of view, ranging from simple surface pattern matching (), to automated reasoning ( or supercomputing.", "labels": [], "entities": [{"text": "QA problem", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.8785566687583923}, {"text": "surface pattern matching", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.7136591076850891}]}, {"text": "In this work, we propose to use Statistical Machine Translation (SMT) for the task of factoid QA.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 32, "end_pos": 69, "type": "TASK", "confidence": 0.8132536510626475}, {"text": "factoid QA", "start_pos": 86, "end_pos": 96, "type": "TASK", "confidence": 0.6819747388362885}]}, {"text": "Under this perspective, the answer is a translation of the question.", "labels": [], "entities": []}, {"text": "It is not the first time that SMT is used for QA tasks, several works have been using translation models to determine the answers ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9919706583023071}, {"text": "QA tasks", "start_pos": 46, "end_pos": 54, "type": "TASK", "confidence": 0.8802708089351654}]}, {"text": "But to our knowledge this is the first 20 approach that uses a full Machine Translation system for generating answers.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7366077601909637}]}, {"text": "The paper is organised as follows: Section 2 reviews the previous usages of SMT in QA, Section 3 reports our theoretical approach to the task, Section 4 describes our QA system, Section 5 presents the experimental setting, Section 6 analyses the results and Section 7 draws conclusions.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9932377338409424}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of Questions and Answers in our data  sets. The number of TREC evaluation from which are  obtained is indicated.", "labels": [], "entities": [{"text": "TREC", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9338032603263855}]}, {"text": " Table 2: Statistics for the 12,116 Q-A pairs in the train- ing corpus according to the annotation level.", "labels": [], "entities": []}, {"text": " Table 3: Mean and standard deviation for 1000 real- isations of the random baseline for QA and SR. The  upper bound is also shown.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985887408256531}, {"text": "standard deviation", "start_pos": 19, "end_pos": 37, "type": "METRIC", "confidence": 0.9312266409397125}]}, {"text": " Table 4: System performance using an SMT that gen- erates a 100-best list, uses a 5-gram LM and all the  features of the TM.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9303531050682068}]}, {"text": " Table 5: System performance with different combina- tions of the SMT features used in decoding. BR is the  metric used to score the answers.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9737223982810974}, {"text": "BR", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9986950755119324}]}, {"text": " Table 6: System performance according to three dif- ferent ranking strategies: context score (B and R), the  language scores (L x ) and EAT type checking (E).", "labels": [], "entities": [{"text": "EAT type checking (E)", "start_pos": 137, "end_pos": 158, "type": "METRIC", "confidence": 0.8016197631756464}]}]}