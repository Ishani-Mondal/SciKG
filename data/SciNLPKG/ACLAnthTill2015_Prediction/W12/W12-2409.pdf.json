{"title": [], "abstractContent": [{"text": "Active learning can lower the cost of annotation for some natural language processing tasks by using a classifier to select informative instances to send to human annotators.", "labels": [], "entities": []}, {"text": "It has worked well in cases where the training instances are selected one at a time and require minimal context for annotation.", "labels": [], "entities": []}, {"text": "However, coreference annotations often require some context and the traditional active learning approach may not be feasible.", "labels": [], "entities": [{"text": "coreference annotations", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.9630634486675262}]}, {"text": "In this work we explore various active learning methods for coreference resolution that fit more realistically into coreference annotation workflows.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.9502598941326141}]}], "introductionContent": [{"text": "Coreference resolution is the task of deciding which entity mentions in a text refer to the same entity.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9198712408542633}]}, {"text": "Solving this problem is an important part of the larger task of natural language understanding in general.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.6541485289732615}]}, {"text": "The clinical domain offers specific tasks where it is easy to see that correctly resolving coreference is important.", "labels": [], "entities": []}, {"text": "For example, one important task in the clinical domain is template filling for the Clinical Elements Model (CEM).", "labels": [], "entities": [{"text": "template filling", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.7517155408859253}]}, {"text": "This task involves extracting various pieces of information about an entity and fitting the information into a standard data structure that can be reasoned about.", "labels": [], "entities": []}, {"text": "An example CEM template is that for Disease with attributes for Body Location, Associated Sign or Symptom, Subject, Negation, Uncertainty, and Severity.", "labels": [], "entities": []}, {"text": "Since a given entity may have many different attributes and relations, it http://intermountainhealthcare.org/cem maybe mentioned multiple times in a text.", "labels": [], "entities": []}, {"text": "Coreference resolution is important for this task because it must be known that all the attributes and relations apply to the same entity so that a single CEM template is filled in for an entity, rather than creating anew template for each mention of the entity.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9434563517570496}]}], "datasetContent": [{"text": "Evaluations of the active learning models described above took place in a simulation context.", "labels": [], "entities": []}, {"text": "In active learning simulations, a labeled data set is used, and the unlabeled pool is simulated by ignoring or \"covering\" the labels for part of the data until the selection algorithm selects anew instance for annotation.", "labels": [], "entities": []}, {"text": "After selection the next data point is simply put into the training data and its label is uncovered.", "labels": [], "entities": []}, {"text": "The data set used was the Ontology Development and Information Extraction (ODIE) corpus ) used in the 2011 i2b2/VA Challenge on coreference resolution.", "labels": [], "entities": [{"text": "Ontology Development and Information Extraction (ODIE)", "start_pos": 26, "end_pos": 80, "type": "TASK", "confidence": 0.7188880443572998}, {"text": "coreference resolution", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.9637355506420135}]}, {"text": "We used a set of 64 documents from the training set of the Mayo Clinic notes for our simulations.", "labels": [], "entities": [{"text": "training set of the Mayo Clinic notes", "start_pos": 39, "end_pos": 76, "type": "DATASET", "confidence": 0.6998544973986489}]}, {"text": "Instances were created by using the training pipeline from the coreference system described in Section 2.1.", "labels": [], "entities": []}, {"text": "As previously mentioned, this work uses the named entity anaphor classifier as it contains the most data points.", "labels": [], "entities": []}, {"text": "This training set resulted in 6820 instances, with 311 positive instances and 6509 negative instances.", "labels": [], "entities": []}, {"text": "Baseline ten-fold cross validation performance on this data set using an SVM with RBF kernel is an F-score of 0.48.", "labels": [], "entities": [{"text": "F-score", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9992652535438538}]}, {"text": "Simulations are performed using tenfold crossvalidation.", "labels": [], "entities": [{"text": "Simulations", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9784026145935059}]}, {"text": "First, each data point is assigned to one often folds (this is done randomly to avoid any autocorrelation issues).", "labels": [], "entities": []}, {"text": "Then, for each iteration, one fold is made the seed data, another fold is the validation data, and the remainder are the unlabeled pool.", "labels": [], "entities": []}, {"text": "Initially the labeled training data contains only the seed data set.", "labels": [], "entities": []}, {"text": "The model is trained on the labeled training data, tested on the validation set, then used to select the next data point from the pool data set.", "labels": [], "entities": []}, {"text": "The selected data point is then removed from the pool and added to the training data with its gold standard label(s), and the process repeats until the pool of unlabeled data is empty.", "labels": [], "entities": []}, {"text": "Performance is averaged across folds to minimize the effects of randomness in seed and validation set selection.", "labels": [], "entities": [{"text": "validation set selection", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.6934976875782013}]}, {"text": "Typically, active learning is compared to a baseline of passive learning where the next data point to be labeled is selected from the unlabeled pool data set randomly.", "labels": [], "entities": []}, {"text": "Instance selection simulations follow the general template above, with each instance (representing a putative antecedent-anaphor pair) randomly assigned to a fold.", "labels": [], "entities": []}, {"text": "After scoring on the validation set, uncertainty sampling is used to select a single instance from the unlabeled pool, and that instance is added to the training set.", "labels": [], "entities": []}, {"text": "shows the results of active learning using uncertainty selection on instances versus using passive learning (random selection).", "labels": [], "entities": []}, {"text": "This makes it clear that if the classifier is allowed to choose the data, top performance can be achieved much faster than if the data is presented in random order.", "labels": [], "entities": []}, {"text": "Specifically, the performance for uncertainty selection levels off at around 500 instances into the active learning, out of a pool set of around 5500 instances.", "labels": [], "entities": []}, {"text": "In contrast, the passive learning baseline takes basically the entire dataset to reach the same performance.", "labels": [], "entities": []}, {"text": "This is essentially a proof of concept that there is such a thing as a \"better\" or \"worse\" instance when it comes to training a classifier for coreference.", "labels": [], "entities": []}, {"text": "We take this as a validation for attempting a document selection experiment, with many metrics using instance uncertainty as a building block.", "labels": [], "entities": [{"text": "document selection experiment", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.8396565914154053}]}, {"text": "Document selection follows similarly to the instance selection above.", "labels": [], "entities": [{"text": "Document selection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8782262206077576}]}, {"text": "The main difference is that instead of assigning pair vectors to folds, we assign docu- ments to folds.", "labels": [], "entities": []}, {"text": "To make a selection, each instance is labeled according to the model, document level metrics described in Section 3.2 are computed per document, and the document is selected which optimizes the metric being evaluated.", "labels": [], "entities": []}, {"text": "All of that document's instances and labels are added to the training data, and the process repeats as before.", "labels": [], "entities": []}, {"text": "The results of these experiments are divided into two plots for visual clarity.", "labels": [], "entities": [{"text": "visual clarity", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.6243568658828735}]}, {"text": "shows the results of these experiments, roughly divided into those that work as well as a random baseline (left) and those that seem to work worse than a random baseline (right).", "labels": [], "entities": []}, {"text": "The best performing metrics (on the left side of the are Positive Ratio, Least Worst, Highest Average, and Narrow Band, although none of these performs noticeably better than random.", "labels": [], "entities": [{"text": "Positive Ratio", "start_pos": 57, "end_pos": 71, "type": "METRIC", "confidence": 0.8866667151451111}, {"text": "Least Worst, Highest Average", "start_pos": 73, "end_pos": 101, "type": "METRIC", "confidence": 0.8169825315475464}, {"text": "Narrow Band", "start_pos": 107, "end_pos": 118, "type": "METRIC", "confidence": 0.9732928574085236}]}, {"text": "The remaining metrics (on the right) seem to do worse than random, taking more instances to reach the peak performance near the end.", "labels": [], "entities": []}, {"text": "The performance of document selection suggests that it may not be a viable means of active learning.", "labels": [], "entities": [{"text": "document selection", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7387247085571289}]}, {"text": "This maybe due to a model of data distribution in which useful instances are distributed very uniformly throughout the corpus.", "labels": [], "entities": []}, {"text": "In this case, an average document will only have 8-10 useful instances and many times as many that are not useful.", "labels": [], "entities": []}, {"text": "This was investigated by follow-up experiments on the instance selection which kept track of which 77 document each instance came from.", "labels": [], "entities": []}, {"text": "The experiments tracked the first 500 instances only, which is roughly the number of instances shown in to reach peak performance.", "labels": [], "entities": []}, {"text": "(left) shows a histogram with document indices on the x-axis and normalized instance counts on the y-axis.", "labels": [], "entities": []}, {"text": "The counts are normalized by total number of document vectors.", "labels": [], "entities": []}, {"text": "In other words, we wanted to show whether there was a distinction between \"good\" documents containing lots of good instances and \"bad\" documents with few good instances.", "labels": [], "entities": []}, {"text": "The figure shows a few spikes, but most documents have approximately 10% of their instances sampled, and all but one document has at least one instance selected.", "labels": [], "entities": []}, {"text": "Further investigation shows that the spikes in the figure are from shorter documents.", "labels": [], "entities": []}, {"text": "Since shorter documents have few instances overall but always at least one positive instance, they will be biased to have a higher ratio of positive to negative instances.", "labels": [], "entities": []}, {"text": "If positive instances are more uncertain (which maybe the case due to the class imbalance), then shorter documents will have more selected instances per unit length.", "labels": [], "entities": []}, {"text": "We performed another follow-up experiment along these lines using the histogram as a measure of document value.", "labels": [], "entities": []}, {"text": "In this experiment, we took the normalized histogram, selected documents from it in order of normalized number of items selected, and used that as a document selection technique.", "labels": [], "entities": [{"text": "document selection", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.7314179539680481}]}, {"text": "Obviously this would be \"cheating\" if used as a metric for document selection, but it can serve as a check on the viability of document selection.", "labels": [], "entities": [{"text": "cheating", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9547743201255798}, {"text": "document selection", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8082986772060394}, {"text": "document selection", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7419601380825043}]}, {"text": "If the results are better than passive document selection, then there is some hope that a document level metric based on the uncertainty of its instances can be successful.", "labels": [], "entities": []}, {"text": "In fact, the right plot on shows that the \"cheating\" method of document selection still does not look any better than random document selection.", "labels": [], "entities": [{"text": "document selection", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7645061612129211}]}, {"text": "The experiments for document-inertial instance selection were patterned after the instance selection paradigm.", "labels": [], "entities": [{"text": "document-inertial instance selection", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.568893313407898}]}, {"text": "However, each instance was bundled with metadata representing the document from which it came.", "labels": [], "entities": []}, {"text": "In the first selection, the algorithm selects the most uncertain instance, and the document it comes from is recorded.", "labels": [], "entities": []}, {"text": "For subsequent selections, the document which contained the previously selected instance is given priority when looking fora new instance.", "labels": [], "entities": []}, {"text": "Specifically, each instance in that document is classified, and the confidence is compared against a threshold.", "labels": [], "entities": []}, {"text": "If the document contains instances meeting the threshold, the most uncertain instance was selected.", "labels": [], "entities": []}, {"text": "After each instance, the model is retrained as in normal instance selection, and the new model is used in the next iteration of the selection algorithm.", "labels": [], "entities": []}, {"text": "For these experiments, the threshold is set at 0.75, where the distance between the classification boundary and the margin is 1.0.", "labels": [], "entities": []}, {"text": "shows the performance of this algorithm compared to passive and uncertainty sampling.", "labels": [], "entities": []}, {"text": "Per-78  formance using this algorithm is clearly better than passive learning and is similar to standard uncertainty selection ignoring document constraints.", "labels": [], "entities": []}], "tableCaptions": []}