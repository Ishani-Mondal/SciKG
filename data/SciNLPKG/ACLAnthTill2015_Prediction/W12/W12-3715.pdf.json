{"title": [{"text": "How to Evaluate Opinionated Keyphrase Extraction?", "labels": [], "entities": [{"text": "Evaluate Opinionated Keyphrase Extraction", "start_pos": 7, "end_pos": 48, "type": "TASK", "confidence": 0.8910618275403976}]}], "abstractContent": [{"text": "Evaluation often denotes a key issue in semantics-or subjectivity-related tasks.", "labels": [], "entities": []}, {"text": "Here we discuss the difficulties of evaluating opinionated keyphrase extraction.", "labels": [], "entities": [{"text": "opinionated keyphrase extraction", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.6569515367348989}]}, {"text": "We present our method to reduce the subjectivity of the task and to alleviate the evaluation process and we also compare the results of human and machine-based evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluation is a key issue in natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "Although for more basic tasks such as tokenization or morphological parsing, the level of ambiguity and subjectivity is essentially lower than for higher-level tasks such as question answering or machine translation, it is still an open question to find a satisfactory solution for the (automatic) evaluation of certain tasks.", "labels": [], "entities": [{"text": "tokenization or morphological parsing", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.6695334762334824}, {"text": "question answering or machine translation", "start_pos": 174, "end_pos": 215, "type": "TASK", "confidence": 0.6875202238559723}]}, {"text": "Here we present the difficulties of finding an appropriate way of evaluating a highly semantics-and subjectivity-related task, namely opinionated keyphrase extraction.", "labels": [], "entities": [{"text": "opinionated keyphrase extraction", "start_pos": 134, "end_pos": 166, "type": "TASK", "confidence": 0.6573532621065775}]}, {"text": "There has been a growing interest in the NLP treatment of subjectivity and sentiment analysissee e.g. -on the one hand and on keyphrase extraction () on the other hand.", "labels": [], "entities": [{"text": "sentiment analysissee", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.896170437335968}, {"text": "keyphrase extraction", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.7239663749933243}]}, {"text": "The tasks themselves are demanding for automatic systems due to the variety of the linguistic ways people can express the same linguistic content.", "labels": [], "entities": []}, {"text": "Here we focus on the evaluation of subjective information mining through the example of assigning opinionated keyphrases to product reviews and compare the results of human-and machine-based evaluation on finding opinionated keyphrases.", "labels": [], "entities": [{"text": "evaluation of subjective information mining", "start_pos": 21, "end_pos": 64, "type": "TASK", "confidence": 0.6844609141349792}]}], "datasetContent": [{"text": "Since the comparison of annotations highlighted the subjectivity of the task, we voted for smoothing the divergences of annotations.", "labels": [], "entities": []}, {"text": "We wanted to take into account all the available annotations which were manually prepared and regarded as acceptable.", "labels": [], "entities": []}, {"text": "Thus, an annotator formed the union and the intersection of the pro and con features given by each annotator either including or excluding those defined by the original author.", "labels": [], "entities": []}, {"text": "With this, we aimed at eliminating subjectivity since in the case of union, every keyphrase mentioned by at least one annotator was taken into consideration while in the case of intersection, it is possible to detect keyphrases that seem to be the most salient for the annotators as regards the given document.", "labels": [], "entities": []}, {"text": "Thus, four sets of pros and cons were finally yielded for each review depending on whether the unions or intersections were determined purely on the phrases of the annotators excluding the original phrases of the author or including them.", "labels": [], "entities": []}, {"text": "Comparing the automatic keyphrases to the union of human annotations means that a bigger number of keyphrases is to be identified, however, with a bigger number of gold standard keywords it is more probable that the automatic keywords occur among them.", "labels": [], "entities": []}, {"text": "At the same time having a larger set of gold standard tags might affect the recall negatively since there are more keyphrases to return.", "labels": [], "entities": [{"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9992311000823975}]}, {"text": "On the other hand, in the case of intersection it can be measured whether the most important features (i.e. those that every annotator felt relevant) can be extracted from the text.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement among the author's  and annotators' sets of opinion phrases. Elements above  and under the main diagonal refer to the agreement rates  in Dice coefficient for pro and con phrases, respectively.", "labels": [], "entities": []}, {"text": " Table 2: F-scores of the human evaluation of the automat- ically extracted opinion phrases. Columns Eval and Ref  show the way gold standard phrases were obtained and if  they were refined manually or automatically.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9963698387145996}, {"text": "Eval", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9321348667144775}, {"text": "Ref", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.7974036335945129}]}, {"text": " Table 3: F-scores achieved with different keyphrase re- finement strategies. A and M as the first (second) charac- ter indicate the fact that the training (testing) was based  on the automatically and manually defined sets of gold  standard expressions, respectively.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99396812915802}, {"text": "A", "start_pos": 78, "end_pos": 79, "type": "METRIC", "confidence": 0.9570525884628296}]}]}