{"title": [{"text": "Hierarchical clustering of word class distributions", "labels": [], "entities": [{"text": "Hierarchical clustering of word class", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8139487862586975}]}], "abstractContent": [{"text": "We propose an unsupervised approach to POS tagging where first we associate each word type with a probability distribution over word classes using Latent Dirichlet Allocation.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.9555276036262512}]}, {"text": "Then we create a hierarchical clustering of the word types: we use an agglomer-ative clustering algorithm where the distance between clusters is defined as the Jensen-Shannon divergence between the probability distributions over classes associated with each word-type.", "labels": [], "entities": []}, {"text": "When assigning POS tags, we find the tree leaf most similar to the current word and use the prefix of the path leading to this leaf as the tag.", "labels": [], "entities": []}, {"text": "This simple labeler outper-forms a baseline based on Brown clusters on 9 out of 10 datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised induction of word categories has been approached from three broad perspectives.", "labels": [], "entities": [{"text": "induction of word categories", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.7491029798984528}]}, {"text": "First, it is of interest to cognitive scientists who model syntactic category acquisition by children (, where the primary concern is matching human performance patterns and satisfying cognitively motivated constraints such as incremental learning.", "labels": [], "entities": [{"text": "syntactic category acquisition", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.697990894317627}]}, {"text": "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes,,,), and primarily motivated as useful for tagging under-resourced languages.", "labels": [], "entities": [{"text": "tagging under-resourced languages", "start_pos": 151, "end_pos": 184, "type": "TASK", "confidence": 0.8651085694630941}]}, {"text": "Finally, learning categories has also been researched from the point of view of feature learning, where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application ().", "labels": [], "entities": []}, {"text": "The main difference from the part-of-speech setting is that the focus is on evaluating the performance of the learned categories in real tasks rather than on measuring how closely they match gold part-of-speech tags.", "labels": [], "entities": []}, {"text": "Some researchers have used both approaches to evaluation.", "labels": [], "entities": []}, {"text": "This difference in evaluation methodology also naturally leads to differing constraints on the nature of the induced representations.", "labels": [], "entities": []}, {"text": "For part-of-speech tagging what is needed is a mapping from word tokens to a small set of discrete, atomic labels.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7061398029327393}]}, {"text": "For feature learning, there are is no such limitation, and other types of representations have been used, such as low-dimensional continuous vectors learned by neural network language models as in,, or distributions over word classes learned using Latent Dirichlet Allocation as in.", "labels": [], "entities": [{"text": "feature learning", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7630926966667175}]}, {"text": "In this paper we propose a simple method of mapping distributions over word classes to a set of discrete labels by hierarchically clustering word class distributions using Jensen-Shannon divergence as a distance metric.", "labels": [], "entities": []}, {"text": "This allows us to effectively use the algorithm of Chrupala (2011) and similar ones in settings where using distributions directly is not possible or desirable.", "labels": [], "entities": []}, {"text": "Equivalently, our approach can be seen as a generic method to convert a soft clustering to hard clustering while conserving much of the information encoded in the original soft cluster assignments.", "labels": [], "entities": []}, {"text": "We evaluate this method on the unsupervised part-of-speech tagging task on ten datasets 100 in nine languages as part of the shared task at the NAACL-HLT 2012 Workshop on Inducing Linguistic Structure.", "labels": [], "entities": [{"text": "part-of-speech tagging task", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.7520025869210561}, {"text": "NAACL-HLT 2012 Workshop on Inducing Linguistic Structure", "start_pos": 144, "end_pos": 200, "type": "TASK", "confidence": 0.6981348310198102}]}], "datasetContent": [{"text": "We evaluate our method on the unsupervised partof-speech tagging task on ten dataset in nine languages as part of the shared task.", "labels": [], "entities": [{"text": "partof-speech tagging task", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.7736144959926605}]}, {"text": "For each dataset we run LDA word class induction 1 on the union of the unlabeled sentences in the train, development and test sets, setting the number of classes K \u2208 {10, 20, 40, 80}, and build a hierarchy on top of the learned word-class probability distributions as explained above.", "labels": [], "entities": []}, {"text": "We then label the development set using path prefixes of length L \u2208 {8, 9, . .", "labels": [], "entities": []}, {"text": ", 20} for each of the trees, and record  the V-measure (Rosenberg and Hirschberg 2007) against gold part-of-speech tags.", "labels": [], "entities": []}, {"text": "We choose the best-performing pair of K and Land use this setting to label the test set.", "labels": [], "entities": []}, {"text": "We tune separately for coarsegrained and fine-grained POS tags.", "labels": [], "entities": []}, {"text": "Other than using the development set labels to tune these two parameters our system is unsupervised and uses no data other than the sentences in the provided data files.", "labels": [], "entities": []}, {"text": "show the best settings for the coarse-and fine-grained POS tagging for all the datasets, and the V-measure scores on the test set achieved by our labeler (HCD for Hierarchy over Class Distributions).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.6488339900970459}]}, {"text": "Also included are the scores of the official baseline, i.e. labeling with Brown clusters (, with the number of clusters set to match the number of POS tags in each dataset.", "labels": [], "entities": []}, {"text": "The best K stays the same when increasing the granularity in the majority of cases (7 out of 10).", "labels": [], "entities": [{"text": "K", "start_pos": 9, "end_pos": 10, "type": "METRIC", "confidence": 0.9502411484718323}, {"text": "granularity", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9605578184127808}]}, {"text": "On the CHILDES dataset of child-directed speech, which has the smallest vocabulary of all, the optimal number of LDA classes is also the smallest (10).", "labels": [], "entities": [{"text": "CHILDES dataset", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.884270429611206}]}, {"text": "As expected, the best path prefix length L is typically larger for the fine-grained labeling.", "labels": [], "entities": []}, {"text": "Our labels outperform the baseline on 9 out of 10 datasets, for both levels of granularity.", "labels": [], "entities": []}, {"text": "The only exception is the English Penn Treebank dataset, where the HCD V-measure scores are slightly lower than Brown cluster scores.", "labels": [], "entities": [{"text": "English Penn Treebank dataset", "start_pos": 26, "end_pos": 55, "type": "DATASET", "confidence": 0.9258062392473221}]}, {"text": "This maybe taken as an illustration of the danger arising if NLP systems are exclusively evaluated on a single dataset: such a dataset may well prove to not be very representative.", "labels": [], "entities": []}, {"text": "Part of the story seems to be that our method tends to outperform the baseline by larger margins on datasets with smaller vocabularies 2 . The scatterplot in illustrates this tendency for coarsegrained POS tagging: Pearson's correlation is \u22120.6.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 202, "end_pos": 213, "type": "TASK", "confidence": 0.7745471298694611}, {"text": "Pearson's correlation", "start_pos": 215, "end_pos": 236, "type": "METRIC", "confidence": 0.8694889346758524}]}], "tableCaptions": [{"text": " Table 1: Evaluation of coarse-grained POS tagging on  test data", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.8172381520271301}]}, {"text": " Table 2: Evaluation of coarse-grained POS tagging on  test data", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.8139248788356781}]}]}