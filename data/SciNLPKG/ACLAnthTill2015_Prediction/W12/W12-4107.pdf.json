{"title": [], "abstractContent": [{"text": "To be able to answer the question What causes tumors to shrink?, one would require a large cause-effect relation repository.", "labels": [], "entities": []}, {"text": "Many efforts have been payed on is-a and part-of relation leaning, however few have focused on cause-effect learning.", "labels": [], "entities": []}, {"text": "This paper describes an automated bootstrapping procedure which can learn and produce with minimal effort a cause-effect term repository.", "labels": [], "entities": []}, {"text": "To filter out the erroneously extracted information, we incorporate graph-based methods.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the acquired cause-effect terms, we conduct three evaluations: (1) human-based, (2) comparison with existing knowledge bases and (3) application driven (SemEval-1 Task 4) in which the goal is to identify the relation between pairs of nominals.", "labels": [], "entities": []}, {"text": "The results show that the extractions at rank 1500 are 89% accurate , they comprise 61% from the terms used in the SemEval-1 Task 4 dataset and can be used in the future to produce additional training examples for the same task.", "labels": [], "entities": [{"text": "SemEval-1 Task 4 dataset", "start_pos": 115, "end_pos": 139, "type": "DATASET", "confidence": 0.6830536872148514}]}], "introductionContent": [{"text": "Over the years, researchers have successfully shown how to build ground facts (), semantic lexicons (), encyclopedic knowledge (, and concept lists ().", "labels": [], "entities": []}, {"text": "Among the most well developed repositories are those focusing on is-a and partof () relations.", "labels": [], "entities": []}, {"text": "However, to be able to answer the question \"What causes tumors to shrink?\", one requires knowledge about cause-effect relation.", "labels": [], "entities": []}, {"text": "Other applications that can benefit from causeeffect knowledge are the relational search engines which have to retrieve all terms relevant to a query like: \"find all X such that X causes wrinkles\").", "labels": [], "entities": []}, {"text": "Unfortunately to date, there is no universal repository of cause-effect relations that can be used or consulted.", "labels": [], "entities": []}, {"text": "However, one would still like to dispose of an automated procedure that can accurately and quickly acquire the terms expressing this relation.", "labels": [], "entities": []}, {"text": "Multiple algorithms have been created to learn relations.", "labels": [], "entities": []}, {"text": "Some like TextRunner () rely on labeled data, which is used to train a sequence-labeling graphical model (CRF) and then the system uses the model to extract terms and relations from unlabeled texts.", "labels": [], "entities": []}, {"text": "Although very accurate, such methods require labeled data which is difficult, expensive and time consuming to create.", "labels": [], "entities": []}, {"text": "Other more simplistic methods that rely on lexico-syntactic patterns) have shown to be equally successful at learning relations, temporal verb order () and entailment ().", "labels": [], "entities": []}, {"text": "Therefore, in this paper, we have incorporated an automated bootstrapping procedure, which given a pattern representing the relation of interest can quickly and easily learn the terms associated with the relation.", "labels": [], "entities": []}, {"text": "In our case, the pattern captures the cause-effect relation.", "labels": [], "entities": []}, {"text": "After extraction, we apply graph-based metrics to rerank the information and filter out the erroneous terms.", "labels": [], "entities": []}, {"text": "The contributions of the paper are: \u2022 an automated procedure, which can learn terms expressing cause-effect relation.", "labels": [], "entities": []}, {"text": "\u2022 an exhaustive human-based evaluation.", "labels": [], "entities": []}, {"text": "\u2022 a comparison of the extracted knowledge with the terms available in the SemEval-1 Task 4 dataset for interpreting the relation between pairs of nominals.", "labels": [], "entities": [{"text": "SemEval-1 Task 4 dataset", "start_pos": 74, "end_pos": 98, "type": "DATASET", "confidence": 0.6136176213622093}, {"text": "interpreting the relation between pairs of nominals", "start_pos": 103, "end_pos": 154, "type": "TASK", "confidence": 0.7735065988131932}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section describes the term extraction procedure.", "labels": [], "entities": []}, {"text": "Section 3 and 4 describe the extracted data and its characteristics.", "labels": [], "entities": []}, {"text": "Section 5 focuses on the evaluation and finally we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the results of the term extraction procedure.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.6867837160825729}]}, {"text": "To the extend to which it is possible, we conduct a human-based evaluation, we compare results to knowledge bases that have been extracted in a similar way (i.e., through pattern application over unstructured text) and we show how the extracted knowledge can be used by NLP applications such as relation identification between nominals.", "labels": [], "entities": [{"text": "relation identification between nominals", "start_pos": 295, "end_pos": 335, "type": "TASK", "confidence": 0.8933741301298141}]}, {"text": "For the human based evaluation, we use two annotators to judge the correctness of the extracted terms.", "labels": [], "entities": []}, {"text": "We estimate the correctness of the produced extractions by measuring Accuracy as the number of correctly tagged examples divided by the total number of examples., shows the accuracy of the bootstrapping algorithm with graph re-ranking in blue and without graph re-ranking in red.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9995744824409485}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9993441700935364}]}, {"text": "The figure shows that graph re-ranking is effective and can separate out the erroneous extractions.", "labels": [], "entities": []}, {"text": "The overall extractions produced by the algorithm are very precise, at rank 1500 the accuracy is 89%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9997077584266663}]}, {"text": "Next, in, we also show a detailed evaluation of the extracted X and Z terms.", "labels": [], "entities": []}, {"text": "We define five types according to which the humans can classify the extracted terms.", "labels": [], "entities": []}, {"text": "The types are: PhysicalObject, NonPhysicalObject, Event, State and Other.", "labels": [], "entities": []}, {"text": "We used Other to indicate erroneous extractions or terms which do not belong to any of the previous four types.", "labels": [], "entities": []}, {"text": "The Kappa agreement for the produced annotations is 0.80.: Term Classification.", "labels": [], "entities": [{"text": "Term Classification", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.6844164729118347}]}], "tableCaptions": []}