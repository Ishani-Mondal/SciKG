{"title": [{"text": "System Combination Using Joint, Binarised Feature Vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a method for system combination based on joint, binarised feature vectors.", "labels": [], "entities": []}, {"text": "Our method can be used to combine several black-box source systems.", "labels": [], "entities": []}, {"text": "We first define a total order on given translation output which can be used to partition an n-best list of translations into a set of pairwise system comparisons.", "labels": [], "entities": []}, {"text": "Using this data, we explain how an SVM-based classification model can be trained and how this classifier can be applied to combine translation output on the sentence level.", "labels": [], "entities": [{"text": "SVM-based classification", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.8691087663173676}]}, {"text": "We describe our experiments for the ML4HMT-12 shared task and conclude by giving a summary of our findings and by discussing future extensions and experiments using the proposed approach.", "labels": [], "entities": [{"text": "ML4HMT-12 shared task", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.5971502264340719}]}], "introductionContent": [{"text": "Research efforts on machine translation (MT) have resulted in many different methods and MT paradigms, each having individual strengths and weaknesses.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.881551718711853}, {"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9796755313873291}]}, {"text": "There exist approaches following linguistic theory as well as data-driven methods relying on statistical processing with only little linguistic knowledge involved.", "labels": [], "entities": []}, {"text": "In recent years, there has also been a lot of research on the automatic combination of machine translation output, resulting in so-called hybrid MT engines.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.6750643849372864}, {"text": "MT", "start_pos": 145, "end_pos": 147, "type": "TASK", "confidence": 0.9568502902984619}]}, {"text": "Regardless of the actual method implemented in a given machine translation system, creating translation output usually requires several, often heterogeneous, features.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7365471720695496}]}, {"text": "These can be 1) simple scores, e.g., for language model scores, parser or phrase table probabilities; 2) more complex data such as hierarchical parse trees or word alignment links; or 3) even full parse forests or n-best lists.", "labels": [], "entities": []}, {"text": "Given this wide range of heterogenous features and their diversity, it is very difficult to get an intuitive understanding of the inner workings of the MT engine in question; thus, further research work on the combination of machine translation systems into better, hybrid MT systems seems to be of high importance to the field.", "labels": [], "entities": [{"text": "MT engine", "start_pos": 152, "end_pos": 161, "type": "TASK", "confidence": 0.9289126694202423}, {"text": "machine translation", "start_pos": 225, "end_pos": 244, "type": "TASK", "confidence": 0.7193923592567444}, {"text": "MT", "start_pos": 273, "end_pos": 275, "type": "TASK", "confidence": 0.9709222316741943}]}, {"text": "To overcome the aforementioned problem of incomprehensible feature values, we propose a method based on Machine Learning (ML) tools, leaving the exact interpretation and weighting of features to the ML algorithms.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured in the following way.", "labels": [], "entities": []}, {"text": "After having set the topic in this section, we briefly describe relevant related work in Section 2 before defining and explaining our Machine-Learning-based hybrid MT framework in Section 3.", "labels": [], "entities": [{"text": "MT", "start_pos": 164, "end_pos": 166, "type": "TASK", "confidence": 0.8134300708770752}]}, {"text": "We first give an overview on the basic approach in Subsection 3.1 and then discuss the two most important components: the order on translations is defined in Subsection 3.2 while the notion of joint, binarised feature vectors for ML is introduced in Subsection 3.3.", "labels": [], "entities": []}, {"text": "We discuss the experiments conducted for the ML4HMT-12 shared task in Section 4 and then conclude by giving a summary of our findings and by discussing upcoming research in Section 5.", "labels": [], "entities": [{"text": "ML4HMT-12 shared task", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.5349935293197632}]}], "datasetContent": [{"text": "We worked on a submission for language pair Spanish\u2192English.", "labels": [], "entities": [{"text": "language pair Spanish\u2192English", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.819218373298645}]}, {"text": "For this language pair, translation output from four different translation engines was made available by the organisers of the shared task.", "labels": [], "entities": []}, {"text": "For each of the systems both translation output and system-specific annotations could be used.", "labels": [], "entities": []}, {"text": "As our method relies on comparable features, we decided to extract features for all candidate systems ourselves, hence constraining ourselves to only using the given translation output.", "labels": [], "entities": []}, {"text": "We created the data set for classifier training using the following selection of linguistic features: -number of target tokens, parse tree nodes, and parse tree depth; -ratio of target/source tokens, parse tree nodes, and parse tree depth; -n-gram score for n-gram order n \u2208 {1, . .", "labels": [], "entities": [{"text": "classifier training", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.918380081653595}]}, {"text": ", 5}; -perplexity for n-gram order n \u2208 {1, . .", "labels": [], "entities": []}, {"text": "These features represent a combination of (shallow) parsing and language model scoring and are derived from the set of features that are most often used in the Machine-Learning-based system combination literature.", "labels": [], "entities": []}, {"text": "We use the Stanford Parser ( to process the source text and the corresponding translations.", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.8897622525691986}]}, {"text": "For language model scoring, we use the SRILM toolkit) training a 5-gram language model for English.", "labels": [], "entities": [{"text": "language model scoring", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7042304674784342}, {"text": "SRILM toolkit", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.8427841067314148}]}, {"text": "In this work, we do not consider any source language models.", "labels": [], "entities": []}, {"text": "We ended up using a sigmoid kernel (C = 2, \u03b3 = 0.015625) and observed a prediction rate of 68.9608% on the training instances.", "labels": [], "entities": [{"text": "prediction rate", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.9796128869056702}]}], "tableCaptions": []}