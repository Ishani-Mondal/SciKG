{"title": [{"text": "Discrepancy Between Automatic and Manual Evaluation of Summaries", "labels": [], "entities": [{"text": "Evaluation of Summaries", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.6888265609741211}]}], "abstractContent": [{"text": "Today, automatic evaluation metrics such as ROUGE have become the de-facto mode of evaluating an automatic summarization system.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9779022932052612}, {"text": "summarization", "start_pos": 107, "end_pos": 120, "type": "TASK", "confidence": 0.9247893691062927}]}, {"text": "However, based on the DUC and the TAC evaluation results, (Conroy and Schlesinger, 2008; Dang and Owczarzak, 2008) showed that the performance gap between human-generated summaries and system-generated summaries is clearly visible in manual evaluations but is often not reflected in automated evaluations using ROUGE scores.", "labels": [], "entities": [{"text": "DUC", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.9526311755180359}]}, {"text": "In this paper , we present our own experiments in comparing the results of manual evaluations versus automatic evaluations using our own text summarizer: BlogSum.", "labels": [], "entities": [{"text": "BlogSum", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.9572761654853821}]}, {"text": "We have evaluated BlogSum-generated summary content using ROUGE and compared the results with the original candidate list (OList).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9522364735603333}]}, {"text": "The t-test results showed that there is no significant difference between BlogSum-generated summaries and OList summaries.", "labels": [], "entities": [{"text": "OList summaries", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.5443374812602997}]}, {"text": "However, two manual evaluations for content using two different datasets show that BlogSum performed significantly better than OList.", "labels": [], "entities": [{"text": "OList", "start_pos": 127, "end_pos": 132, "type": "DATASET", "confidence": 0.8516502976417542}]}, {"text": "A manual evaluation of summary coherence also shows that Blog-Sum performs significantly better than OList.", "labels": [], "entities": []}, {"text": "These results agree with previous work and show the need fora better automated summary evaluation metric rather than the standard ROUGE metric.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.7403667569160461}]}], "introductionContent": [{"text": "Today, any NLP task must be accompanied by a well-accepted evaluation scheme.", "labels": [], "entities": [{"text": "NLP task", "start_pos": 11, "end_pos": 19, "type": "TASK", "confidence": 0.8686122298240662}]}, {"text": "This is why, for the last 15 years, to evaluate automated summarization systems, sets of evaluation data (corpora, topics, . .", "labels": [], "entities": []}, {"text": ") and baselines have been established in text summarization competitions such as TREC 1 , DUC 2 , and TAC . Although evaluation is essential to verify the quality of a summary or to compare different summarization approaches, the evaluation criteria used are by no means universally accepted).", "labels": [], "entities": [{"text": "text summarization", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.556449219584465}, {"text": "TAC", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.7389740943908691}]}, {"text": "Summary evaluation is a difficult task because no ideal summary is available fora set of input documents.", "labels": [], "entities": [{"text": "Summary evaluation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9610937237739563}]}, {"text": "In addition, it is also difficult to compare different summaries and establish a baseline because of the absence of standard human or automatic summary evaluation metrics.", "labels": [], "entities": []}, {"text": "On the other hand, manual evaluation is very expensive.", "labels": [], "entities": []}, {"text": "According to), large scale manual evaluations of all participants' summaries in the DUC 2003 conference would require over 3000 hours of human efforts to evaluate summary content and linguistic qualities.", "labels": [], "entities": [{"text": "DUC 2003 conference", "start_pos": 84, "end_pos": 103, "type": "DATASET", "confidence": 0.9475018382072449}]}, {"text": "The goal of this paper is to show that the literature and our own work empirically point out the need fora better automated summary evaluation metric rather than the standard ROUGE metric 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "The available summary evaluation techniques can be divided into two categories: manual and automatic.", "labels": [], "entities": [{"text": "summary evaluation", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8331079185009003}]}, {"text": "To do a manual evaluation, human experts assess different qualities of the system generated summaries.", "labels": [], "entities": []}, {"text": "On the other hand, for an automatic eval-uation, tools are used to compare the system generated summaries with human generated gold standard summaries or reference summaries.", "labels": [], "entities": []}, {"text": "Although they are faster to perform and result in consistent evaluations, automatic evaluations can only address superficial concepts such as n-grams matching, because many required qualities such as coherence and grammaticality cannot be measured automatically.", "labels": [], "entities": []}, {"text": "As a result, human judges are often called for to evaluate or crosscheck the quality of the summaries, but in many cases human judges have different opinions.", "labels": [], "entities": []}, {"text": "Hence inter-annotator agreement is often computed as well.", "labels": [], "entities": []}, {"text": "The quality of a summary is assessed mostly on its content and linguistic quality.", "labels": [], "entities": []}, {"text": "Content evaluation of a query-based summary is performed based on the relevance with the topic and the question and the inclusion of important contents from the input documents.", "labels": [], "entities": [{"text": "Content evaluation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7409124374389648}]}, {"text": "The linguistic quality of a summary is evaluated manually based on how it structures and presents the contents.", "labels": [], "entities": []}, {"text": "Mainly, subjective evaluation is done to assess the linguistic quality of an automatically generated summary.", "labels": [], "entities": []}, {"text": "Grammaticality, non-redundancy, referential clarity, focus, structure and coherence are commonly used factors considered to evaluate the linguistic quality.", "labels": [], "entities": [{"text": "Grammaticality", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8316126465797424}]}, {"text": "A study by shows that evaluating the content of a summary is more difficult compared to evaluating its linguistic quality.", "labels": [], "entities": []}, {"text": "There exist different measures to evaluate an output summary.", "labels": [], "entities": []}, {"text": "The most commonly used metrics are recall, precision, F-measure, Pyramid score, and ROUGE/BE.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9996187686920166}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9995585083961487}, {"text": "F-measure", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9970894455909729}, {"text": "Pyramid score", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.9642575979232788}, {"text": "ROUGE/", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9295163452625275}, {"text": "BE", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.5095301866531372}]}, {"text": "Based on an analysis of the showed that the ROUGE evaluation and a human evaluation can significantly vary due to the fact that ROUGE ignores linguistic quality of summaries, which has a huge influence inhuman evaluation.) also pointed out that automatic evaluation is rather different than the one based on manual assessment.", "labels": [], "entities": []}, {"text": "They explained this the following way: \"automatic metrics, based on string matching, are unable to appreciate a summary that uses different phrases than the reference text, even if such a summary is perfectly fine by human standards\".", "labels": [], "entities": []}, {"text": "To evaluate both opinionated and news article based summarization approaches, previously mentioned evaluation metrics such as ROUGE or Pyramid are used.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.9933360815048218}, {"text": "Pyramid", "start_pos": 135, "end_pos": 142, "type": "METRIC", "confidence": 0.7764182686805725}]}, {"text": "Shared evaluation tasks such as DUC and TAC competitions also use these methods to evaluate participants' summary. shows.", "labels": [], "entities": [{"text": "DUC", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.7826671004295349}]}, {"text": "In this evaluation, the pyramid score was used to calculate the content relevance and the responsiveness of a summary was used to judge the overall quality or usefulness of the summary, considering both the information content and linguistic quality.", "labels": [], "entities": []}, {"text": "These two criteria were evaluated manually.", "labels": [], "entities": []}, {"text": "The pyramid score was calculated out of 1 and the responsiveness measures were calculated on a scale of 1 to 5 (1, being the worst).", "labels": [], "entities": [{"text": "pyramid score", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9414404630661011}]}, {"text": "However, in 2009, responsiveness was calculated on a scale of 10.", "labels": [], "entities": [{"text": "responsiveness", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.876910388469696}]}, {"text": "Table 1 also shows a comparison between automatic systems and human participants (model).", "labels": [], "entities": []}, {"text": "In, the first 3 rows show the evaluation results of the TAC Update Summarization (Upd.) initial summary generation task (which were generated for news articles) and the last row shows the evaluation results of the TAC 2008 Opinion Summarization track (Opi.) where summaries were generated from blogs.", "labels": [], "entities": [{"text": "TAC Update Summarization (Upd.) initial summary generation task", "start_pos": 56, "end_pos": 119, "type": "TASK", "confidence": 0.7441784739494324}, {"text": "TAC 2008 Opinion Summarization", "start_pos": 214, "end_pos": 244, "type": "TASK", "confidence": 0.7432039529085159}]}, {"text": "From, we can see that in both criteria, automatic systems are weaker than humans.", "labels": [], "entities": []}, {"text": "(Note that in the table, Unk. refers to unknown.)", "labels": [], "entities": []}, {"text": "Interestingly, in an automatic evaluation, often, not only is there no significant gap between models and systems, but in many cases, automatic systems scored higher than some human models.", "labels": [], "entities": []}, {"text": "shows the performance of human (H.) and automated systems (S.) (participants) using automated and manual evaluation in the TAC 2008 up-45 shows that there is no significant difference between human and participants in automated evaluation but that there is a significant performance difference between them in the manual evaluation.", "labels": [], "entities": [{"text": "TAC 2008 up-45", "start_pos": 123, "end_pos": 137, "type": "DATASET", "confidence": 0.9633421301841736}]}, {"text": "These findings indicate that ROUGE is not the most effective tool to evaluate summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.8524292707443237}, {"text": "summaries", "start_pos": 78, "end_pos": 87, "type": "TASK", "confidence": 0.9329499006271362}]}, {"text": "Our own experiments described below arrive at the same conclusion.", "labels": [], "entities": []}, {"text": "BlogSum-generated summaries have been evaluated for content and linguistic quality, specifically discourse coherence.", "labels": [], "entities": []}, {"text": "The evaluation of the content was done both automatically and manually and the evaluation of the coherence was done manually.", "labels": [], "entities": []}, {"text": "Our evaluation results also reflect the discrepancy between automatic and manual evaluation schemes of summaries described above.", "labels": [], "entities": []}, {"text": "In our evaluation, BlogSum-generated summaries were compared with the original candidate list generated by our approach without the discourse reordering (OList).", "labels": [], "entities": []}, {"text": "However, we have validated our original candidate list with a publicly available sentence ranker.", "labels": [], "entities": []}, {"text": "Specifically, we have conducted an experiment to verify whether MEAD-generated summaries (), a widely used publicly available summarizer 5 , were better than our candidate list (OList).", "labels": [], "entities": [{"text": "MEAD-generated summaries", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.40489108860492706}]}, {"text": "In this evaluation, we have generated summaries using MEAD with centroid, query title, and query narrative features.", "labels": [], "entities": []}, {"text": "In MEAD, query title and query narrative features are implemented using cosine similarity based on the tf-idf value.", "labels": [], "entities": []}, {"text": "In this evaluation, we used the TAC 2008 opinion summarization dataset (described later in this section) and summaries were evaluated using the ROUGE-2 and ROUGE-SU4 scores.", "labels": [], "entities": [{"text": "TAC 2008 opinion summarization dataset", "start_pos": 32, "end_pos": 70, "type": "DATASET", "confidence": 0.8856040596961975}, {"text": "ROUGE-2", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.9430533051490784}, {"text": "ROUGE-SU4", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.7612781524658203}]}, {"text": "shows the results of the automatic evaluation using ROUGE based on summary content.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9705572724342346}]}, {"text": "shows that MEAD-generated summaries achieved weaker ROUGE scores compared to that of our candidate list (OList).", "labels": [], "entities": [{"text": "MEAD-generated", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.525853157043457}, {"text": "ROUGE", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9984473586082458}]}, {"text": "The table also shows that MEAD performs weaker than the average performance of the participants of TAC 2008 (Average).", "labels": [], "entities": [{"text": "MEAD", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9496099948883057}, {"text": "TAC 2008", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.7135344743728638}]}, {"text": "We suspect that these poor results are due to several reasons.", "labels": [], "entities": []}, {"text": "First, in MEAD, we cannot use opinionated terms or polarity information as a sentence selection feature.", "labels": [], "entities": []}, {"text": "On the other hand, most of the summarizers, which deal with opinionated texts, use opinionated terms and polarity information for this purpose.", "labels": [], "entities": []}, {"text": "In addition, in this experiment, for some of the TAC 2008 questions, MEAD was unable to create any summary.", "labels": [], "entities": [{"text": "TAC 2008 questions", "start_pos": 49, "end_pos": 67, "type": "DATASET", "confidence": 0.8864438533782959}, {"text": "MEAD", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.8345351815223694}]}, {"text": "This evaluation results prompted us to develop our own candidate sentence selector.", "labels": [], "entities": []}, {"text": "First, we have automatically evaluated the summaries generated by our approach for content.", "labels": [], "entities": []}, {"text": "As a baseline, we used the original ranked list of candidate sentences (OList), and compared them to the final summaries (BlogSum).", "labels": [], "entities": []}, {"text": "We have used the data from the TAC 2008 opinion summarization track for the evaluation.", "labels": [], "entities": [{"text": "TAC 2008 opinion summarization track", "start_pos": 31, "end_pos": 67, "type": "DATASET", "confidence": 0.9032320976257324}]}, {"text": "The dataset consists of 50 questions on 28 topics; on each topic one or two questions are asked and 9 to 39 relevant documents are given.", "labels": [], "entities": []}, {"text": "For each question, one summary was generated by OList and one by BlogSum and the maximum summary length was restricted to 250 words.", "labels": [], "entities": [{"text": "OList", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.8787895441055298}, {"text": "BlogSum", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9295366406440735}]}, {"text": "This length was chosen cause in the DUC conference from 2005 to 2007, in the main summarization task, the summary length was 250 words.", "labels": [], "entities": [{"text": "length", "start_pos": 5, "end_pos": 11, "type": "METRIC", "confidence": 0.9506508708000183}, {"text": "DUC conference", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9427345991134644}]}, {"text": "In addition,) also created summaries of length 250 words in their participation in the TAC 2008 opinion summarization task and performed well.", "labels": [], "entities": [{"text": "TAC 2008 opinion summarization task", "start_pos": 87, "end_pos": 122, "type": "TASK", "confidence": 0.7920475721359252}]}, {"text": "() also pointed out that if the summaries were too long this adversely affected their scores.", "labels": [], "entities": []}, {"text": "Moreover, according to the same authors shorter summaries are easier to read.", "labels": [], "entities": []}, {"text": "Based on these observations, we have restricted the maximum summary length to 250 words.", "labels": [], "entities": []}, {"text": "However, in the TAC 2008 opinion summarization track, the allowable summary length is very long (the number of nonwhitespace characters in the summary must not exceed 7000 times the number of questions for the target of the summary).", "labels": [], "entities": [{"text": "TAC 2008 opinion summarization track", "start_pos": 16, "end_pos": 52, "type": "DATASET", "confidence": 0.9481437087059021}, {"text": "allowable summary length", "start_pos": 58, "end_pos": 82, "type": "METRIC", "confidence": 0.6053048372268677}]}, {"text": "In this experiment, we used the ROUGE metric using answer nuggets (provided by TAC), which had been created to evaluate participants' summaries at TAC, as gold standard summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9940763711929321}, {"text": "TAC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.9107573628425598}, {"text": "TAC", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.8376359343528748}]}, {"text": "F-scores are calculated for BlogSum and OList using ROUGE-2 and ROUGE-SU4.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9826210141181946}, {"text": "BlogSum", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9184306859970093}, {"text": "OList", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.6870189905166626}, {"text": "ROUGE-2", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.8106815218925476}]}, {"text": "In this experiment, ROUGE scores are also calculated for all 36 submissions in the TAC 2008 opinion summarization track.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9975886344909668}, {"text": "TAC 2008 opinion summarization track", "start_pos": 83, "end_pos": 119, "type": "DATASET", "confidence": 0.8746974110603333}]}, {"text": "The evaluation results are shown in.", "labels": [], "entities": []}, {"text": "Note that in the table Rank refers to the rank of the system compared to the other 36 systems.", "labels": [], "entities": [{"text": "Rank", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9914963245391846}]}, {"text": "shows that BlogSum achieved a better FMeasure (F) for ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) compared to OList.", "labels": [], "entities": [{"text": "BlogSum", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.7366483211517334}, {"text": "FMeasure (F)", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.8724902421236038}]}, {"text": "From the results, we can see that BlogSum gained 18% and 16% in F- However, a further analysis of the results of shows that there is no significant difference between BlogSum-generated summaries and OList summaries using the t-test with a p-value of 0.228 and 0.464 for ROUGE-2 and ROUGE-SU4, respectively.", "labels": [], "entities": [{"text": "F-", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9806507527828217}]}, {"text": "This is inline with who showed that the performance gap between humangenerated summaries and system-generated summaries at DUC and TAC is clearly visible in a manual evaluation, but is often not reflected in automated evaluations using ROUGE scores.", "labels": [], "entities": [{"text": "DUC", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.894666314125061}, {"text": "TAC", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.4310183823108673}]}, {"text": "Based on these findings, we suspected that there might be a performance difference between BlogSum-generated summaries and OList which is not reflected in ROUGE scores.", "labels": [], "entities": [{"text": "BlogSum-generated summaries", "start_pos": 91, "end_pos": 118, "type": "DATASET", "confidence": 0.8651528656482697}, {"text": "ROUGE", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.9477959275245667}]}, {"text": "To verify our suspicion, we have conducted manual evaluations for content.", "labels": [], "entities": []}, {"text": "We have conducted two manual evaluations using two different datasets to better quantify BlogSumgenerated summary content.", "labels": [], "entities": []}, {"text": "In this second evaluation, we have used a subset of the OpinRank dataset and (Jindal and Liu, 2008)'s dataset.", "labels": [], "entities": [{"text": "OpinRank dataset", "start_pos": 56, "end_pos": 72, "type": "DATASET", "confidence": 0.874882161617279}]}, {"text": "The OpinRank dataset contains reviews on cars and hotels collected from Tripadvisor (about 259,000 reviews) and Edmunds (about 42,230 reviews).", "labels": [], "entities": [{"text": "OpinRank dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8733791410923004}, {"text": "Tripadvisor", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.9638498425483704}, {"text": "Edmunds", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.939698338508606}]}, {"text": "The OpinRank dataset contains 42,230 reviews on cars for different model-years and 259,000 reviews on different hotels in 10 different cities.", "labels": [], "entities": [{"text": "OpinRank dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8847184479236603}]}, {"text": "For this dataset, we created a total of 21 questions including 12 reason questions and 9 suggestions.", "labels": [], "entities": []}, {"text": "For each question, 1500 to 2500 reviews were provided as input documents to create the summary.", "labels": [], "entities": []}, {"text": "('s dataset consists of 905 comparison and 4985 non-comparison sentences.", "labels": [], "entities": []}, {"text": "Four human annotators labeled these data manually.", "labels": [], "entities": []}, {"text": "This dataset consists of reviews, forum, and news articles on different topics from different sources.", "labels": [], "entities": []}, {"text": "We have created 9 comparison questions for this dataset.", "labels": [], "entities": []}, {"text": "For each question, 700 to 1900 reviews were provided as input documents to create the summary.", "labels": [], "entities": []}, {"text": "For each question, one summary was generated by OList and one by BlogSum and the maximum summary length was restricted to 250 words again.", "labels": [], "entities": [{"text": "OList", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.8759015202522278}, {"text": "BlogSum", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9296560287475586}]}, {"text": "To evaluate question relevance, 3 participants manually rated 30 summaries from OList and 30 summaries from BlogSum using a blind evaluation.", "labels": [], "entities": [{"text": "OList", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.8945209383964539}, {"text": "BlogSum", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9463385343551636}]}, {"text": "These summaries were again rated on a likert scale of 1 to 5.", "labels": [], "entities": []}, {"text": "Evaluators rated each summary with respect to the question for which it was generated.", "labels": [], "entities": []}, {"text": "shows the performance comparison between BlogSum and OList.", "labels": [], "entities": [{"text": "BlogSum", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9097169041633606}]}, {"text": "The results show that 67% of the time BlogSum summaries were rated better than OList summaries.", "labels": [], "entities": [{"text": "BlogSum summaries", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.8247718214988708}]}, {"text": "The table also shows that 30% of the time both approaches performed equally well and 3% of the time BlogSum was weaker than OList.", "labels": [], "entities": [{"text": "BlogSum", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.7011590003967285}]}, {"text": "Comparison % BlogSum Score > OList Score 67% BlogSum Score = OList Score 30% BlogSum Score < OList Score 3% demonstrates that 44% of the time BlogSum summaries were rated as \"very good\", 33% of the time rated as \"good\", 13% of the time they were rated as \"barely acceptable\" and 10% of the time they were rated as \"poor\" or \"very poor\".", "labels": [], "entities": [{"text": "BlogSum Score", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.9558399617671967}, {"text": "BlogSum", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9744500517845154}, {"text": "BlogSum Score", "start_pos": 77, "end_pos": 90, "type": "METRIC", "confidence": 0.9677219688892365}]}, {"text": "From Table 8, we can also see that BlogSum outperformed OList in the scale of \"very good\" by 34% and improved the performance in \"poor\" and \"very poor\" categories by 23% and 10%, respectively.", "labels": [], "entities": []}, {"text": "We have conducted a second evaluation using the OpinRank dataset 6 and ('s dataset to evaluate BlogSum-generated summary content.", "labels": [], "entities": [{"text": "OpinRank dataset 6 and ('s dataset", "start_pos": 48, "end_pos": 82, "type": "DATASET", "confidence": 0.8498850294521877}, {"text": "BlogSum-generated summary content", "start_pos": 95, "end_pos": 128, "type": "DATASET", "confidence": 0.8091582854588827}]}, {"text": "Our next experiments were geared at evaluating the linguistic quality of our summaries.", "labels": [], "entities": []}, {"text": "To test the linguistic qualities, we did not use an automatic evaluation because found that the ordering of content within the summaries is an aspect which is not evaluated by ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 176, "end_pos": 181, "type": "METRIC", "confidence": 0.8118582963943481}]}, {"text": "Moreover, in the TAC 2008 opinion summarization track, on each topic, answer snippets were provided which had been used as summarization content units (SCUs) in pyramid evaluation to evaluate TAC 2008 participants summaries but no complete summaries is provided to which we can compare BlogSum-generated summaries for co-49 herence.", "labels": [], "entities": [{"text": "TAC 2008 opinion summarization track", "start_pos": 17, "end_pos": 53, "type": "DATASET", "confidence": 0.7758377075195313}]}, {"text": "As a result, we only performed two manual evaluations using two different datasets again to see whether BlogSum performs significantly better than OList for linguistic qualities too.", "labels": [], "entities": []}, {"text": "The positive results of the next experiments will ensure that BlogSum-generated summaries are really significantly better than OList summaries.", "labels": [], "entities": [{"text": "OList summaries", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.5655913352966309}]}, {"text": "Coherence using the Blog Dataset In this evaluation, we have again used the TAC 2008 opinion summarization track data.", "labels": [], "entities": [{"text": "Blog Dataset", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.9463922381401062}, {"text": "TAC 2008 opinion summarization track data", "start_pos": 76, "end_pos": 117, "type": "DATASET", "confidence": 0.8895034790039062}]}, {"text": "For each question, one summary was generated by OList and one by BlogSum and the maximum summary length was restricted to 250 words again.", "labels": [], "entities": [{"text": "OList", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.8759015202522278}, {"text": "BlogSum", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9296560287475586}]}, {"text": "Four participants manually rated 50 summaries from OList and 50 summaries from BlogSum for coherence.", "labels": [], "entities": [{"text": "OList", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.888129711151123}, {"text": "BlogSum", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.9163944721221924}]}, {"text": "These summaries were again rated on a likert scale of 1 to 5.", "labels": [], "entities": []}, {"text": "In this evaluation, we have again used the OpinRank dataset and ('s dataset to conduct the second evaluation of content.", "labels": [], "entities": [{"text": "OpinRank dataset", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.907413512468338}]}, {"text": "In this evaluation, for each question, one summary was generated by OList and one by BlogSum and the maximum summary length was restricted to 250 words.", "labels": [], "entities": [{"text": "OList", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.8680601716041565}, {"text": "BlogSum", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.9019811749458313}]}, {"text": "Three participants manually rated 30 summaries from OList and 30 summaries from BlogSum for coherence.", "labels": [], "entities": [{"text": "OList", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.8799559473991394}, {"text": "BlogSum", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.9195324778556824}]}], "tableCaptions": [{"text": " Table 1: Human and Automatic System Performance at  Various TAC Competitions", "labels": [], "entities": []}, {"text": " Table 2: Automated vs. Manual Evaluation at TAC 2008", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.8455614447593689}]}, {"text": " Table 3: Automatic Evaluation of MEAD based on Sum- mary Content on TAC 2008", "labels": [], "entities": [{"text": "MEAD", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.8119763731956482}, {"text": "Sum- mary Content on TAC 2008", "start_pos": 48, "end_pos": 77, "type": "DATASET", "confidence": 0.7115590274333954}]}, {"text": " Table 4: Automatic Evaluation of BlogSum based on  Summary Content on TAC 2008", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.8695637583732605}]}, {"text": " Table 6: Manual Evaluation of BlogSum and OList based  on Summary Content on TAC 2008", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.871610015630722}]}, {"text": " Table 8: Manual Evaluation of BlogSum and OList based  on Summary Content on the Review Dataset", "labels": [], "entities": [{"text": "Review Dataset", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.7599733173847198}]}, {"text": " Table 10: Manual Evaluation of BlogSum and OList  based on Discourse Coherence on TAC 2008", "labels": [], "entities": []}, {"text": " Table 12: Manual Evaluation of BlogSum and OList  based on Discourse Coherence on the Review Dataset", "labels": [], "entities": [{"text": "Review Dataset", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.6980663388967514}]}]}