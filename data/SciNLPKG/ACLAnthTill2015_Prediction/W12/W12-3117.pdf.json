{"title": [{"text": "DCU-Symantec Submission for the WMT 2012 Quality Estimation Task", "labels": [], "entities": [{"text": "WMT 2012 Quality Estimation Task", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.7161810517311096}]}], "abstractContent": [{"text": "This paper describes the features and the machine learning methods used by Dublin City University (DCU) and SYMANTEC for the WMT 2012 quality estimation task.", "labels": [], "entities": [{"text": "Dublin City University (DCU)", "start_pos": 75, "end_pos": 103, "type": "DATASET", "confidence": 0.9689085483551025}, {"text": "WMT 2012 quality estimation task", "start_pos": 125, "end_pos": 157, "type": "TASK", "confidence": 0.8617010831832885}]}, {"text": "Two sets of features are proposed: one constrained, i.e. respecting the data limitation suggested by the workshop organisers, and one unconstrained, i.e. using data or tools trained on data that was not provided by the workshop organisers.", "labels": [], "entities": []}, {"text": "In total, more than 300 features were extracted and used to train classifiers in order to predict the translation quality of unseen data.", "labels": [], "entities": []}, {"text": "In this paper, we focus on a subset of our feature set that we consider to be relatively novel: features based on a topic model built using the Latent Dirichlet Allocation approach, and features based on source and target language syntax extracted using part-of-speech (POS) tag-gers and parsers.", "labels": [], "entities": []}, {"text": "We evaluate nine feature combinations using four classification-based and four regression-based machine learning techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "For the first time, the WMT organisers this year propose a Quality Estimation (QE) shared task, which is divided into two sub-tasks: scoring and ranking automatic translations.", "labels": [], "entities": [{"text": "WMT organisers", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.5718542635440826}]}, {"text": "The aim of this workshop is to define useful sets of features and machine learning techniques in order to predict the quality of a machine translation (MT) output T (Spanish) given a source segment S (English).", "labels": [], "entities": [{"text": "machine translation (MT) output T", "start_pos": 131, "end_pos": 164, "type": "TASK", "confidence": 0.8406121475355965}]}, {"text": "Quality is measured using a 5-point likert scale which is based on postediting effort, following the scoring scheme: 1.", "labels": [], "entities": []}, {"text": "The MT output is incomprehensible 2.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.7239224910736084}]}, {"text": "About 50-70% of the MT output needs to be edited 3.", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9607129096984863}]}, {"text": "About 25-50% of the MT output needs to be edited 4.", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9576070308685303}]}, {"text": "About 10-25% of the MT output needs to be edited 5.", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9595904350280762}]}, {"text": "The MT output is perfectly clear and intelligible The final score is a combination of the scores assigned by three evaluators.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9488309621810913}]}, {"text": "The use of a 5-point scale makes the scoring task more difficult than a binary classification task where a translation is considered to be either good or bad.", "labels": [], "entities": []}, {"text": "However, if the task is successfully carried out, the score produced is more useful.", "labels": [], "entities": []}, {"text": "Dublin City University and Symantec jointly address the scoring task.", "labels": [], "entities": [{"text": "Dublin City University", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.9819885889689127}]}, {"text": "For each pair (S, T ) of source segment Sand machine translation T , we train three classifiers and one classifier combination using the training data provided by the organisers to predict 5-point Likert scores.", "labels": [], "entities": [{"text": "Sand machine translation", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.5983783503373464}]}, {"text": "In this paper, we present the classification results on the test set along with additional results obtained using regression techniques.", "labels": [], "entities": []}, {"text": "We evaluate the usefulness of two new sets of features: 1.", "labels": [], "entities": []}, {"text": "topic-based features using Latent Dirichlet Allocation (LDA (), 2.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA", "start_pos": 27, "end_pos": 59, "type": "METRIC", "confidence": 0.8340357422828675}]}, {"text": "syntax-based features using POS taggers and parsers () The remainder of this paper is organised as follows.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.6447795927524567}]}, {"text": "In Section 2, we give an overview of all the features employed in our QE system.", "labels": [], "entities": []}, {"text": "Then, in Section 3, we describe the topic and syntax-based features in more detail.", "labels": [], "entities": []}, {"text": "Section 4 presents the various classification and regression techniques we explored.", "labels": [], "entities": []}, {"text": "Our results are presented and discussed in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we summarise and outline our plans in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of quality estimation using classification methods show that the baseline and the syntaxbased features with the classifier combination leads to the best results with an MAE of 0.71 and an RMSE of 0.87.", "labels": [], "entities": [{"text": "MAE", "start_pos": 181, "end_pos": 184, "type": "METRIC", "confidence": 0.9979269504547119}, {"text": "RMSE", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.9951125979423523}]}, {"text": "However, these scores are substantially lower than the ones obtained using regression, where the unconstrained set of features with SVM leads to an MAE of 0.62 and an RMSE of 0.78.", "labels": [], "entities": [{"text": "MAE", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.9986404776573181}, {"text": "RMSE", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9966099858283997}]}, {"text": "It seems that the classification methods are not suitable for this task according to the different sets of features studied.", "labels": [], "entities": []}, {"text": "Furthermore, the topic-distance feature is not correlated with the quality scores, according to the regression results.", "labels": [], "entities": []}, {"text": "On the other hand, the syntax-based features appear to be the most informative and lead to an MAE of 0.70.", "labels": [], "entities": [{"text": "MAE", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9990243911743164}]}], "tableCaptions": [{"text": " Table 1: MAE and RMSE results for different sets of features using three classification methods. The results with \u2022  and \u2022 correspond to the DCU-SYMC constrained and the DCU-SYMC unconstrained systems respectively, submitted  for the shared task.", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6710048913955688}, {"text": "RMSE", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.527995765209198}]}]}