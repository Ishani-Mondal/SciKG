{"title": [{"text": "Generating Natural Language Summaries for Multimedia", "labels": [], "entities": [{"text": "Generating Natural Language Summaries", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.696910485625267}]}], "abstractContent": [{"text": "In this paper we introduce an automatic system that generates textual summaries of Inter-net-style video clips by first identifying suitable high-level descriptive features that have been detected in the video (e.g. visual concepts, recognized speech, actions, objects, persons, etc.).", "labels": [], "entities": [{"text": "Inter-net-style video clips by first identifying suitable high-level descriptive features that have been detected in the video (e.g. visual concepts, recognized speech, actions, objects, persons, etc.", "start_pos": 83, "end_pos": 283, "type": "Description", "confidence": 0.8113032104447484}]}, {"text": "Then a natural language generator is constructed using SimpleNLG to compile the high-level features into a textual form.", "labels": [], "entities": [{"text": "SimpleNLG", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.8193817734718323}]}, {"text": "The generated summary contains information from both visual and acoustic sources, intending to give a general review and summary of the video.", "labels": [], "entities": []}, {"text": "To reduce the complexity of the task, we restrict ourselves to work with videos that show a limited number of \"events\".", "labels": [], "entities": []}, {"text": "In this demo paper, we describe the design of the system and present example outputs generated by the video summarization system.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Internet allows us to browse millions of videos.", "labels": [], "entities": []}, {"text": "For some of them, the content is well organized with human-generated tags and labels (e.g. wedding ceremony, birthday party, etc.), but the rate at which content is uploaded daily makes it unrealistic to expect that user-provided labels will be sufficient for organizing this information in the future.", "labels": [], "entities": []}, {"text": "We believe that automatically generating a brief summary (or \"abstract\") of videos is both an attractive solution to this problem and an exciting challenge for the natural language generation community.", "labels": [], "entities": []}, {"text": "Converting audio and video output into natural language to create a human readable summary that facilitates effective browsing, supports classification decisions, or helps differentiating videos from one another without having to watch them in their entirety has both academic and practical value.", "labels": [], "entities": []}, {"text": "In this paper, we introduce an automatic video summary generation system that uses a natural language realization engine ( to create sentences based on state-of-the-art video classification features.", "labels": [], "entities": [{"text": "video summary generation", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.673040638367335}]}, {"text": "These features are computed on a large corpus from the TrecVID evaluation.", "labels": [], "entities": [{"text": "TrecVID evaluation", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.7582436203956604}]}, {"text": "Ina recent user study, we compared automatically generated and manually generated summaries with respect to several tasks.", "labels": [], "entities": []}, {"text": "The study shows, for example, that more specific information (e.g. \"food\" instead of \"some object\") and temporal information (something happened first and then\u2026) is helpful in improving the quality of machine-generated summaries.", "labels": [], "entities": []}, {"text": "This is a first step to implement an automatic system which is not only able to describe videos using natural language, but accomplishes more sophisticated tasks such as differentiating videos, finding supporting evidence for video classification and other tasks.", "labels": [], "entities": [{"text": "video classification", "start_pos": 226, "end_pos": 246, "type": "TASK", "confidence": 0.7085659056901932}]}], "datasetContent": [], "tableCaptions": []}