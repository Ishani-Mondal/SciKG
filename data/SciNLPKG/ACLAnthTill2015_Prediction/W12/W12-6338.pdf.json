{"title": [{"text": "Improving PCFG Chinese Parsing with Context-Dependent Probability Re-estimation", "labels": [], "entities": [{"text": "Improving PCFG Chinese Parsing", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6899807825684547}, {"text": "Re-estimation", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.5293636322021484}]}], "abstractContent": [{"text": "Selecting the best structure from several ambiguous structures produced by a syntactic parser is a challenging issue.", "labels": [], "entities": []}, {"text": "The quality of the solution depends on the precision of the structure evaluation methods.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9956045150756836}]}, {"text": "In this paper, we propose a general model (context-dependent probability re-estimation model, CDM) to enhance the structure probabilities estimation.", "labels": [], "entities": [{"text": "structure probabilities estimation", "start_pos": 114, "end_pos": 148, "type": "TASK", "confidence": 0.6680743098258972}]}, {"text": "Compared with using rule probabilities only, the CDM has the advantage of an effective, flexible, and broader range of contexture-feature selection.", "labels": [], "entities": []}, {"text": "We conduct experiments on the CDM parsing model by using Sinica Chi-nese Treebank.", "labels": [], "entities": [{"text": "CDM parsing", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.7253822684288025}, {"text": "Sinica Chi-nese Treebank", "start_pos": 57, "end_pos": 81, "type": "DATASET", "confidence": 0.8740165630976359}]}, {"text": "The results show that our proposed model significantly outperforms the baseline parser and the open source Berkeley statistical parser.", "labels": [], "entities": []}, {"text": "More importantly, we demonstrate that the basic framework of the parsing model does not need to be changed, and the proposed re-estimation functions will adjust the probability estimation for every particular structure, and obtaining the better parsing results.", "labels": [], "entities": [{"text": "parsing", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.9745280742645264}, {"text": "parsing", "start_pos": 245, "end_pos": 252, "type": "TASK", "confidence": 0.9614891409873962}]}], "introductionContent": [{"text": "Structure evaluation method is an important task in selecting the best structure from several ambiguous structures produced by a syntactic parser, particularly for Chinese.", "labels": [], "entities": [{"text": "Structure evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7901287078857422}]}, {"text": "Since Chinese is an analytic language, words can play different grammatical functions without inflection.", "labels": [], "entities": []}, {"text": "To implement a structure evaluation model, treebank is a necessary resource, since it provides useful statistical distributions regarding grammar rules, words, and part-of-speeches.", "labels": [], "entities": []}, {"text": "Learning grammar rules and probabilities from treebanks is an effective way to improve parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9624825119972229}]}, {"text": "Unfortunately, sizes of treebanks are generally small; certain strategies of rule generalization and specialization have to be devised to improve the coverage and precision of the extracted grammar rules.", "labels": [], "entities": [{"text": "rule generalization", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.6991470009088516}, {"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9969016313552856}]}, {"text": "However no matter how the grammar rules are refined, syntactic ambiguities are unavoidable.", "labels": [], "entities": []}, {"text": "The ambiguous structures should be ranked according to their structural evaluation scores, which maybe an accumulated score of rule probabilities and featurebased scores.", "labels": [], "entities": []}, {"text": "In general, the evaluation functions are derived from very limited and biased resources, such as treebanks.", "labels": [], "entities": []}, {"text": "Therefore we need to find away to improve the evaluation functions under the constraint of very limited resources.", "labels": [], "entities": []}, {"text": "Suppose that the parsing environment is a model of probabilistic context-free grammar (PCFG).", "labels": [], "entities": []}, {"text": "Several researchers are attaching many useful features to the grammar rules to improve the precision of the grammar rules.", "labels": [], "entities": [{"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9992262125015259}]}, {"text": "In this paper, we follow grammar representation in, and propose a context-dependent probability re-estimation model (CDM) to enhance the performance of the original PCFG model.", "labels": [], "entities": []}, {"text": "CDM combines rule probabilities and machine learning techniques in structure evaluation.", "labels": [], "entities": [{"text": "CDM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8079431653022766}, {"text": "structure evaluation", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.8588516116142273}]}, {"text": "Similar to other machine learning methods), the CDM has the flexibility to adjust the features, and to obtain better re-estimated structure probabilities.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides background on PCFG parsing with grammar rule representation.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.8135760426521301}, {"text": "grammar rule representation", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6011735200881958}]}, {"text": "Section 3 describes the proposed CDM and our selected features.", "labels": [], "entities": []}, {"text": "The experimental evaluation and results are in Section 4.", "labels": [], "entities": []}, {"text": "The last section contains some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the experiment design, and then evaluate the proposed models based on Sinica Treebank.", "labels": [], "entities": [{"text": "Sinica Treebank", "start_pos": 99, "end_pos": 114, "type": "DATASET", "confidence": 0.949747622013092}]}, {"text": "We also analyze the results, and compare them with the results derived by the open source Berkeley statistical parser on the same test set.", "labels": [], "entities": []}, {"text": "Treebank: We employ Sinica Treebank as our experimental corpus.", "labels": [], "entities": [{"text": "Sinica Treebank", "start_pos": 20, "end_pos": 35, "type": "DATASET", "confidence": 0.7807720005512238}]}, {"text": "It contains 61,087 syntactic tree structures and 361,834 words.", "labels": [], "entities": []}, {"text": "The syntactic theory of Sinica Treebank is based on the HeadDriven Principle (; that is, a sentence or phrase is composed of a phrasal head and its arguments or adjuncts.", "labels": [], "entities": [{"text": "Sinica Treebank", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.7583014667034149}]}, {"text": "We divide the treebank into four parts: the training data (55,888 sentences), the development set (1,068 sentences), the test data T06 (867 sentences), and the test data T07 (689 sentences).", "labels": [], "entities": []}, {"text": "The test datasets (T06, T07) were used in CoNLL06 and CoNLL07 dependent parsing evaluation individually.", "labels": [], "entities": [{"text": "CoNLL06", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.893589973449707}, {"text": "CoNLL07 dependent parsing evaluation", "start_pos": 54, "end_pos": 90, "type": "TASK", "confidence": 0.7490182369947433}]}, {"text": "The main difference between Sinica Treebank data and CoNLL data is that the CoNLL is in dependency format.", "labels": [], "entities": [{"text": "Sinica Treebank data", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.8545244534810384}, {"text": "CoNLL data", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.8404332101345062}]}, {"text": "Word Sense: With regard to semantic features, we use the head senses of words expressed in EHowNet (http://ehownet.iis.sinica.edu.tw/) as words' sense types.", "labels": [], "entities": [{"text": "EHowNet", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.9682249426841736}]}, {"text": "For example, the E-HowNet definition of \u8eca \u8f1b (Na), is {LandVehicle| \u8eca:quantity={mass|\u773e}}, and its head sense is \"LandVehicle|\u8eca\".", "labels": [], "entities": []}, {"text": "For detailed description about E-HowNet, readers may refer to.", "labels": [], "entities": []}, {"text": "Estimate Parsing Performance: To evaluate a model, we compare the parsing results with the gold standard.", "labels": [], "entities": []}, {"text": "proposed a structural evaluation system is called PARSE-VAL.", "labels": [], "entities": [{"text": "PARSE-VAL", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.6058416366577148}]}, {"text": "In all the experiments, we used the bracketed f-score (BF) as the parsing performance metric.", "labels": [], "entities": [{"text": "bracketed f-score (BF)", "start_pos": 36, "end_pos": 58, "type": "METRIC", "confidence": 0.8191516160964966}]}, {"text": "For training CDP in CDM model, we extract relevant features from each parse tree in training data, in accordance with features setting in.", "labels": [], "entities": []}, {"text": "Zhang (2004) provides a maximum entropy toolkit (MaxEnt) to help us training.", "labels": [], "entities": []}, {"text": "We use option \"-i 30 -gis -c 0\" in MaxEnt training parameter.", "labels": [], "entities": []}, {"text": "The training scale is 407 outcomes, 2438366 parameters and 1593985 predicates.", "labels": [], "entities": []}, {"text": "shows the parsing performances on the developing data for different values of the parameter \uf06c in Formula 2.", "labels": [], "entities": []}, {"text": "The appropriate setting ( \uf06c =0.6) is learned and adopted for the future experiments.", "labels": [], "entities": []}, {"text": "The results in show that the integrated a general PCFG model with a CDM can improve the parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 88, "end_pos": 95, "type": "TASK", "confidence": 0.9770871996879578}]}, {"text": "Implementing the integrated CDM on the T06 and T07 test datasets indicted improved the parsing performance by 1.45% and 1.53% respectively.", "labels": [], "entities": [{"text": "T06", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.9653428196907043}, {"text": "T07 test datasets", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9176336924235026}, {"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9734288454055786}]}, {"text": "The purpose in this research is to incorporate the rich contextual features to assist the constituent parsing.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.6620577424764633}]}, {"text": "Results in prove our method to be useful.", "labels": [], "entities": []}, {"text": "As shown in the bracketed f-scores, about 20% of the errors are reduced.", "labels": [], "entities": [{"text": "errors", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9958056211471558}]}, {"text": "For instance, the ambiguous structures like \"((Nh Nc) Nc)\" and \"(Nh (Nc Nc))\" can be better resolved by our CDM model, since it can provide rich contextual features as additional information to help the parser making more precise evaluation scores in resolving ambiguous structures..", "labels": [], "entities": []}, {"text": "The bracketed f-score of the integrated CDM.", "labels": [], "entities": []}, {"text": "Task 4 of CLP2012 includes two sub-tasks: sentence parsing and semantic role labeling task.", "labels": [], "entities": [{"text": "CLP2012", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.9253599047660828}, {"text": "sentence parsing", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7833760976791382}, {"text": "semantic role labeling task", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.7470523566007614}]}, {"text": "For each sub-task, the testing data are complete Chinese sentence with gold standard word segmentation.", "labels": [], "entities": []}, {"text": "Therefore, a pipeline process is needed to solve the POS tagging, syntactic parsing and semantic role assignment in our experiment.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.8333676159381866}, {"text": "syntactic parsing", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7030710875988007}, {"text": "semantic role assignment", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.6224936147530874}]}, {"text": "We adopt the context-rule tagger proposed by for the POS tagging.", "labels": [], "entities": [{"text": "context-rule tagger", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.6362914741039276}, {"text": "POS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.6842630207538605}]}, {"text": "For syntactic parsing, we use the CDM parser with same training data in Section 4.1.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7616339027881622}]}, {"text": "For semantic role labeling, we follow method to assignment semantic role automatically.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7318076093991598}]}, {"text": "The detail parsing results of our systems on the test set can be found on the official evaluation report.", "labels": [], "entities": []}, {"text": "Our system obtains acceptable results on both sentence parsing and semantic role labeling tasks..", "labels": [], "entities": [{"text": "sentence parsing and semantic role labeling", "start_pos": 46, "end_pos": 89, "type": "TASK", "confidence": 0.6521393954753876}]}, {"text": "Official scores of sentence parsing (task4-1) and semantic role labeling (task4-2).", "labels": [], "entities": [{"text": "sentence parsing", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7435921728610992}, {"text": "semantic role labeling", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.673800935347875}]}, {"text": "shows the F1-score results are reported by the official organizer of the 2012 CIPS-SIGHAN bakeoff task.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990059733390808}, {"text": "2012 CIPS-SIGHAN bakeoff task", "start_pos": 73, "end_pos": 102, "type": "DATASET", "confidence": 0.7315006628632545}]}, {"text": "The result of the first sub-task (Task4-1) is about 0.7448.", "labels": [], "entities": []}, {"text": "The POS tagging accuracy directly influences the sentential structure.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.7243847250938416}, {"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.8632304072380066}]}, {"text": "Therefore, F1-score will be improved with better POS tagging accuracy.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9994344115257263}, {"text": "POS tagging", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.6684364080429077}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9298738837242126}]}, {"text": "On the other hand, the result of the semantic role labeling (Task 4-2) is about 0.6249.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.6703420182069143}]}, {"text": "Semantic role labeling is processed after sentence parsing.", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7481037576993307}, {"text": "sentence parsing", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7136363387107849}]}, {"text": "Our labeling system is based on different decision features, such as head-argument/modifier pairs, special cases, sentence structures, etc.", "labels": [], "entities": []}, {"text": "These statistical information are extracted from training data (see Section 4.1), and we use a backoff approach to decide the best semantic role.", "labels": [], "entities": []}, {"text": "In future work, we will try using lexical semantic and context information to improve accuracy of semantic role labeling.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9982249140739441}, {"text": "semantic role labeling", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.6235451201597849}]}], "tableCaptions": [{"text": " Table 2. The bracketed f-score of the integrated CDM.", "labels": [], "entities": []}, {"text": " Table 4. Official scores of sentence parsing (task4-1)  and semantic role labeling (task4-2).", "labels": [], "entities": [{"text": "sentence parsing", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7260424941778183}, {"text": "semantic role labeling", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.6645753582318624}]}]}