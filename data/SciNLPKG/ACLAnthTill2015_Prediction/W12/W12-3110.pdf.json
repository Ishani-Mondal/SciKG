{"title": [], "abstractContent": [{"text": "This paper describes a study on the contribution of linguistically-informed features to the task of quality estimation for machine translation at sentence level.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.74268639087677}]}, {"text": "A standard regression algorithm is used to build models using a combination of linguistic and non-linguistic features extracted from the input text and its machine translation.", "labels": [], "entities": []}, {"text": "Experiments with English-Spanish translations show that linguistic features , although informative on their own, are not yet able to outperform shallower features based on statistics from the input text, its translation and additional corpora.", "labels": [], "entities": []}, {"text": "However, further analysis suggests that linguistic information is actually useful but needs to be carefully combined with other features in order to produce better results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Estimating the quality of automatic translations is becoming a subject of increasing interest within the Machine Translation (MT) community fora number of reasons, such as helping human translators post-editing MT, warning users about non-reliable translations or combining output from multiple MT systems.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.8505499124526977}]}, {"text": "Different from most classic approaches for measuring the progress of an MT system or comparing MT systems, which assess quality by contrasting system output to reference translations such as BLEU (), Quality Estimation (QE) is a more challenging task, aimed at MT systems in use, and therefore without access to reference translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9728618264198303}, {"text": "BLEU", "start_pos": 191, "end_pos": 195, "type": "METRIC", "confidence": 0.9951950907707214}, {"text": "Quality Estimation (QE)", "start_pos": 200, "end_pos": 223, "type": "TASK", "confidence": 0.5896476566791534}]}, {"text": "From the findings of previous work on referencedependent MT evaluation, it is clear that metrics exploiting linguistic information can achieve significantly better correlation with human judgments on quality, particularly at the level of sentences ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.8720112442970276}]}, {"text": "Intuitively, this should also apply for quality estimation metrics: while evaluation metrics compare linguistic representations of the system output and reference translations (e.g. matching of n-grams of part-of-speech tags or predicate-argument structures), quality estimation metrics would perform the (more complex) comparison og linguistic representations of the input and translation texts.", "labels": [], "entities": []}, {"text": "The hypothesis put forward in this paper is therefore that using linguistic information to somehow contrast the input and translation texts can be beneficial for quality estimation.", "labels": [], "entities": []}, {"text": "We test this hypothesis as part of the WMT-12 shared task on quality estimation.", "labels": [], "entities": [{"text": "WMT-12 shared task", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.5465042591094971}, {"text": "quality estimation", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.5353175401687622}]}, {"text": "The system submitted to this task (WLV-SHEF) integrates linguistic information to a strong baseline system using only shallow statistics from the input and translation texts, with no explicit information from the MT system that produced the translations.", "labels": [], "entities": [{"text": "WLV-SHEF", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.7266008257865906}]}, {"text": "A variant also tests the addition of linguistic information to a larger set of shallow features.", "labels": [], "entities": []}, {"text": "The quality estimation problem is modelled as a supervised regression task using Support Vector Machines (SVM), which has been shown to achieve good performance in previous work.", "labels": [], "entities": [{"text": "quality estimation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6459345817565918}]}, {"text": "Linguistic features are computed using a number of auxiliary resources such as parsers and monolingual corpora.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives an overview of previous work on quality estimation, Section 3 describes the set of linguistic features proposed in this paper, along with general experimental settings, Section 4 presents our evaluation and Section 5 provides conclusions and a brief discussion of future work.", "labels": [], "entities": [{"text": "quality estimation", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.6312552392482758}]}], "datasetContent": [{"text": "Results reveal that our models fall slightly below the baseline, although this drop is not statistically significant in any of the cases (paired t-tests for Baseline vs WLV-SHEF FS and Baseline vs WLV-SHEF BL yield p > 0.05).", "labels": [], "entities": [{"text": "WLV-SHEF", "start_pos": 169, "end_pos": 177, "type": "DATASET", "confidence": 0.6102806329727173}, {"text": "FS", "start_pos": 178, "end_pos": 180, "type": "METRIC", "confidence": 0.6869280338287354}, {"text": "WLV-SHEF", "start_pos": 197, "end_pos": 205, "type": "DATASET", "confidence": 0.5396278500556946}, {"text": "BL", "start_pos": 206, "end_pos": 208, "type": "METRIC", "confidence": 0.712146520614624}]}, {"text": "This may suggest that for this particular dataset the baseline features already coverall relevant aspects of quality on their own, or simply that the representation of the linguistic features is not appropriate for the task.", "labels": [], "entities": []}, {"text": "The quality of the resources used to extract the linguistic features may also have been an issue.", "labels": [], "entities": []}, {"text": "However, a feature selection method may find a different com- A correlation analysis between our predicted scores and the gold standard) shows some dispersion, especially for the WLV-SHEF FS set, with lower Pearson coefficients when compared to the baseline.", "labels": [], "entities": [{"text": "WLV-SHEF FS set", "start_pos": 179, "end_pos": 194, "type": "DATASET", "confidence": 0.6857837537924448}, {"text": "Pearson", "start_pos": 207, "end_pos": 214, "type": "METRIC", "confidence": 0.9876825213432312}]}, {"text": "The fluctuation of predicted values fora single score is also very noticeable, spanning more than one score band in some cases.", "labels": [], "entities": [{"text": "fluctuation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.968134880065918}]}, {"text": "However, if we consider the RMSE achieved by our models, we find that, on average, predictions deviate less than 0.9 absolute points.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.788264274597168}]}, {"text": "A closer look at the score distribution) reveals our models had some difficulty predicting scores in the 1-2 range, possibly affected by the lower proportion of these cases in the training data.", "labels": [], "entities": []}, {"text": "In addition, it is interesting to see that the only sentence with a true score of 1 is predicted as a very good translation (with a score greater than 3.5).", "labels": [], "entities": []}, {"text": "The reason for this is that the translation has isolated grammatical segments that our features might regard as good but it is actually not faithful to the original.", "labels": [], "entities": []}, {"text": "Although the cause for this behaviour can be traced to inaccurate tokenisation, this reveals that our features assess fidelity only superficially and deeper semantically-aware indicators should be explored.", "labels": [], "entities": []}, {"text": "Results for the ranking task also fall below the baseline as shown in, according to the two official metrics: DeltaAvg and Spearman rank correlation coefficient.", "labels": [], "entities": [{"text": "DeltaAvg", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.7811166644096375}, {"text": "Spearman rank correlation coefficient", "start_pos": 123, "end_pos": 160, "type": "METRIC", "confidence": 0.8170091211795807}]}], "tableCaptions": [{"text": " Table 4: An optimal set of features for the test set. The  number of iteration indicates the order in which features  were selected, giving a rough ranking of features by their  performance.", "labels": [], "entities": []}]}