{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 116-121, Judging Grammaticality with Count-Induced Tree Substitution Grammars", "labels": [], "entities": [{"text": "Judging Grammaticality", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.8592529296875}]}], "abstractContent": [{"text": "Prior work has shown the utility of syntactic tree fragments as features in judging the gram-maticality of text.", "labels": [], "entities": []}, {"text": "To date such fragments have been extracted from derivations of Bayesian-induced Tree Substitution Grammars (TSGs).", "labels": [], "entities": []}, {"text": "Evaluating on discriminative coarse and fine grammaticality classification tasks, we show that a simple, deterministic, count-based approach to fragment identification performs on par with the more complicated grammars of Post (2011).", "labels": [], "entities": [{"text": "grammaticality classification", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.7500088214874268}, {"text": "fragment identification", "start_pos": 144, "end_pos": 167, "type": "TASK", "confidence": 0.8534685969352722}]}, {"text": "This represents a significant reduction in complexity for those interested in the use of such fragments in the development of systems for the educational domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatically judging grammaticality is an important component in computer-assisted education, with potential applications including large-scale essay grading and helping to interactively improve the writing of both native and L2 speakers.", "labels": [], "entities": [{"text": "Automatically judging grammaticality", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6218778987725576}]}, {"text": "While ngram models have been productive throughout natural language processing (NLP), they are obviously insufficient as models of languages, since they do not model language structure or correspondences beyond the narrow Markov context.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.6993824938933054}]}, {"text": "Context-free grammars (CFGs) address many of the problems inherent in n-grams, and are therefore intuitively much better suited for grammaticality judgments.", "labels": [], "entities": [{"text": "Context-free grammars (CFGs", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6856225579977036}]}, {"text": "Unfortunately, CFGs used in practice are permissive) and make unrealistic independence and structural assumptions, resulting in \"leaky\" grammars that overgenerate and thus serve poorly as models of language.", "labels": [], "entities": []}, {"text": "However, approaches that make use of the CFG productions as discriminative features have performed better.", "labels": [], "entities": [{"text": "CFG productions", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.9213053286075592}]}, {"text": "improved upon an ngram baseline in grammatical classification by adjusting CFG production weights with a latent SVM, while others have found it useful to use comparisons between scores of different parsers ( or the use of CFG productions in linear classification settings) in classifying sentences in different grammaticality settings.", "labels": [], "entities": [{"text": "grammatical classification", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.7424214482307434}]}, {"text": "Another successful approach in grammaticality tasks has been the use of grammars with an extended domain of locality.", "labels": [], "entities": [{"text": "grammaticality tasks", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.8903539478778839}]}, {"text": "Post (2011) demonstrated that larger syntactic patterns obtained from Tree Substitution Grammars outperformed the Cherry and Quirk models.", "labels": [], "entities": []}, {"text": "The intuitions underlying their approach were that larger fragments are more natural atomic units in modeling grammatical text, and that larger fragments reduce the independence assumptions of context-free generative models since there are fewer substitution points in a derivation.", "labels": [], "entities": []}, {"text": "Their grammars were learned in a Bayesian setting with Dirichlet Process priors, which have simple formal specifications (c.f.,), but which can become quite complicated in implementation.", "labels": [], "entities": []}, {"text": "In this paper, we observe that fragments used for classification do not require an underlying probabilistic model.", "labels": [], "entities": []}, {"text": "Here, we present a simple extraction method that elicits a classic formal non-probabilistic grammar from training data by deterministically counting fragments.", "labels": [], "entities": []}, {"text": "Whereas Post parses with his TSG and extracts the Viterbi derivation, we use an  off-the-shelf parser and pattern match the fragments in our grammar against the tree.", "labels": [], "entities": [{"text": "TSG", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.8344057202339172}]}, {"text": "With enough positive and negative training data (in the form of automatic parses of good and bad sentences), we can construct classifiers that learn which fragments correlate with grammaticality.", "labels": [], "entities": []}, {"text": "The resulting model results in similar classification accuracy while doing away with the complexity of Bayesian techniques.", "labels": [], "entities": [{"text": "classification", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.9543218016624451}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9690110087394714}]}], "datasetContent": [{"text": "We view grammaticality judgment as a binary classification task: is a sequence of words grammatical or not?", "labels": [], "entities": [{"text": "grammaticality judgment", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.9001531004905701}]}, {"text": "We evaluate on two tasks of differing granularity: the first, a coarse-grain classification, follows; the other, a fine-grain analogue, is built upon Foster and Andersen (2009).", "labels": [], "entities": []}, {"text": "For the coarse-grained task, we use the BLLIP 5 -inspired dataset, as in Post (2011), which discriminates between BLLIP sentences and KneyserNey trigram generated sentences (of equal length).", "labels": [], "entities": [{"text": "BLLIP 5 -inspired dataset", "start_pos": 40, "end_pos": 65, "type": "DATASET", "confidence": 0.8426373958587646}]}, {"text": "Grammatical and ungrammatical examples are given in 1 and 2 below, respectively: (1) The most troublesome report maybe the August merchandise trade deficit due out tomorrow . (2) To and , would come Hughey Co. maybe crash victims , three billion . For the fine-grained task we use aversion of the BNC that has been automatically modified to be Code is available at: cs.jhu.edu/ \u02dc ferraro.", "labels": [], "entities": [{"text": "August merchandise trade deficit", "start_pos": 123, "end_pos": 155, "type": "DATASET", "confidence": 0.8405589014291763}, {"text": "Hughey Co.", "start_pos": 199, "end_pos": 209, "type": "DATASET", "confidence": 0.9565320312976837}, {"text": "BNC", "start_pos": 297, "end_pos": 300, "type": "DATASET", "confidence": 0.9657861590385437}, {"text": "Code", "start_pos": 344, "end_pos": 348, "type": "DATASET", "confidence": 0.9496592283248901}, {"text": "ferraro", "start_pos": 380, "end_pos": 387, "type": "METRIC", "confidence": 0.8818736672401428}]}, {"text": "5 LDC2000T43 ungrammatical, via insertions, deletions or substitutions of grammatically important words.", "labels": [], "entities": []}, {"text": "As has been argued in previous work, these automatically generated errors, simulate more realistic errors.", "labels": [], "entities": []}, {"text": "Example 3 gives an original sentence, with an italicized substitution error: (3) The league 's promoters hope retirees and tourists will join die-hard fans like Mr. de Castro and pack then stands to seethe seniors . Both sets contain train/dev/test splits with an equal number of positive and negative examples, and all instances have an available gold-standard parse 6 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Development accuracy results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9917169213294983}]}]}