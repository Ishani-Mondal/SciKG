{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 242-250", "labels": [], "entities": []}], "abstractContent": [{"text": "Previous work on automated error recognition and correction of texts written by learners of English as a Second Language has demonstrated experimentally that training classifiers on error-annotated ESL text generally outper-forms training on native text alone and that adaptation of error correction models to the native language (L1) of the writer improves performance.", "labels": [], "entities": [{"text": "automated error recognition and correction of texts written by learners of English as a Second Language", "start_pos": 17, "end_pos": 120, "type": "TASK", "confidence": 0.8355756811797619}]}, {"text": "Nevertheless, most extant models have poor precision, particularly when attempting error correction, and this limits their usefulness in practical applications requiring feedback.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9989424347877502}, {"text": "error correction", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.6954350173473358}]}, {"text": "We experiment with various feature types, varying quantities of error-corrected data, and generic versus L1-specific adaptation to typical errors using Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (NB) classifiers and develop one model which maximizes precision.", "labels": [], "entities": [{"text": "Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (", "start_pos": 152, "end_pos": 172, "type": "METRIC", "confidence": 0.6679114529064724}, {"text": "precision", "start_pos": 226, "end_pos": 235, "type": "METRIC", "confidence": 0.998139500617981}]}, {"text": "We report and discuss the results for 8 models, 5 trained on the HOO data and 3 (partly) on the full error-coded Cambridge Learner Corpus, from which the HOO data is drawn.", "labels": [], "entities": [{"text": "HOO data", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.9458855986595154}, {"text": "Cambridge Learner Corpus", "start_pos": 113, "end_pos": 137, "type": "DATASET", "confidence": 0.9764556487401327}, {"text": "HOO data", "start_pos": 154, "end_pos": 162, "type": "DATASET", "confidence": 0.9133872985839844}]}], "introductionContent": [{"text": "The task of detecting and correcting writing errors made by learners of English as a Second Language (ESL) has recently become a focus of research.", "labels": [], "entities": [{"text": "detecting and correcting writing errors made by learners of English as a Second Language (ESL)", "start_pos": 12, "end_pos": 106, "type": "TASK", "confidence": 0.8610616536701426}]}, {"text": "The majority of previous papers in this area have presented machine learning methods with models being trained on well-formed native English text;.", "labels": [], "entities": []}, {"text": "However, some recent approaches have explored ways of using annotated non-native text either by incorporating error-tagged data into the training process, or by using native language-specific error statistics.", "labels": [], "entities": []}, {"text": "Both approaches show improvements over the models trained solely on well-formed native text.", "labels": [], "entities": []}, {"text": "Training a model on error-tagged non-native text is expensive, as it requires large amounts of manually-annotated data, not currently publically available.", "labels": [], "entities": []}, {"text": "In contrast, using native language-specific error statistics to adapt a model to a writer's first or native language (L1) is less restricted by the amount of training data.", "labels": [], "entities": []}, {"text": "show that adapting error corrections to the writer's L1 and incorporating artificial errors, in away that mimics the typical error rates and confusion patterns of nonnative text, improves both precision and recall compared to classifiers trained on native data only.", "labels": [], "entities": [{"text": "precision", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.9989833235740662}, {"text": "recall", "start_pos": 207, "end_pos": 213, "type": "METRIC", "confidence": 0.9989535808563232}]}, {"text": "The approach proposed in uses L1-specific error correction patterns as a distribution on priors over the corrections, incorporating the appropriate priors into a generic Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (NB) model.", "labels": [], "entities": []}, {"text": "This approach is both cheaper to implement, since it does not require a separate classifier to be trained for every L1, and more effective, since the priors condition on the writer's L1 as well as on the possible confusion sets.", "labels": [], "entities": []}, {"text": "Some extant approaches have achieved good results on error detection.", "labels": [], "entities": [{"text": "error detection", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7023251205682755}]}, {"text": "However, error correction is much harder and on this task precision remains low.", "labels": [], "entities": [{"text": "error correction", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.8067906498908997}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9993693232536316}]}, {"text": "This is a disadvantage for applications such as self-tutoring or writing assistance, which require feedback to the user.", "labels": [], "entities": []}, {"text": "A high proportion of errorful suggestions is likely to further confuse learners and/or non-native writers rather than improve their writing or assist learning.", "labels": [], "entities": []}, {"text": "Instead a system which maximizes precision over recall returning accurate suggestions fora small proportion of errors is likely to be more helpful.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9990453124046326}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.984022855758667}]}, {"text": "In section 2 we describe the data used for training and testing the systems we developed.", "labels": [], "entities": []}, {"text": "In section 3 we describe the preprocessing of the ESL text undertaken to provide a source of features for the classifiers.", "labels": [], "entities": [{"text": "ESL text", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.8350752890110016}]}, {"text": "We also discuss the feature types that we exploit in our classifiers.", "labels": [], "entities": []}, {"text": "In section 4 we describe and report results fora high precision system which makes no attempt to generalize from training data.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9850082397460938}]}, {"text": "In section 5 we describe our approach to adapting multiclass NB classifiers to characteristic errors and L1s.", "labels": [], "entities": []}, {"text": "We also report the performance of some of these NB classifiers on the training and test data.", "labels": [], "entities": []}, {"text": "In section 6 we report the official results of all our submitted runs on the test data and also on the HOO training data, cross-validated where appropriate.", "labels": [], "entities": [{"text": "HOO training data", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.7952048579851786}]}, {"text": "Finally, we briefly discuss our main results, further work, and lessons learnt.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results presented below are calculated using the evaluation tool provided by the organizers, implementing the scheme specified in the HOO shared task.", "labels": [], "entities": [{"text": "HOO shared task", "start_pos": 138, "end_pos": 153, "type": "TASK", "confidence": 0.49816465377807617}]}, {"text": "The results on the test set, presented in Tables 7-9 are from the final official run after correction of errors in the annotation and score calculation scripts.", "labels": [], "entities": []}, {"text": "The test set results for NB classifiers (Runs 1-7) are significantly worse than our preliminary results obtained on the training data partitions, especially for determiners.", "labels": [], "entities": [{"text": "NB classifiers", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.6388998478651047}]}, {"text": "Use of additional training data (Runs 6 and 7) improves recall, but does not improve precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9996324777603149}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.998867392539978}]}, {"text": "Adaptation to the input preposition improves precision as compared to the unadapted classifier for prepositions (Run 4), whereas training on the determiner-specific subsets improves precision for determiners (Run 3).", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.999496340751648}, {"text": "precision", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9991251826286316}]}, {"text": "However, generally these results are worse than the results of the similar classifiers on the training data subsets.", "labels": [], "entities": []}, {"text": "We calculated the upper bound recall for our classifiers on the test data.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9577156901359558}]}, {"text": "The upper bound recall on the test data is 93.20 for recognition, and 86.39 for correction, given our confusion sets for both determiners and prepositions.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9991432428359985}, {"text": "correction", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9976981282234192}]}, {"text": "However, the actual upper bound recall is 71.82, with upper bound recall on determiners at 71.74 and on prepositions at 71.90, because 65 out of 230 determiner errors, and 68 out of 243 preposition errors are not considered by our classifiers, primarily because when the parser fails to find a full analysis, the grammatical context is often not recovered accurately enough to identify missing input positions or relevant GRs.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9107458591461182}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.5601000785827637}]}, {"text": "This is an inherent weakness of using only parser-extracted features from noisy and often ungrammatical input.", "labels": [], "entities": []}, {"text": "Taking this into account, some models (Runs 1, 2, 6 and 7) achieved quite high recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9992294311523438}]}, {"text": "We suspect the considerable drop in precision is explained by the differences in the training and test data.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9996743202209473}]}, {"text": "The training set contains answers from learners of a smaller group of L1s from one examination year to a much more restricted set of prompts.", "labels": [], "entities": []}, {"text": "The wellknown weaknesses of generative NB classifiers may prevent effective exploitation of the additional information in the full CLC over the HOO training data.", "labels": [], "entities": [{"text": "generative NB classifiers", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.7220211029052734}, {"text": "HOO training data", "start_pos": 144, "end_pos": 161, "type": "DATASET", "confidence": 0.8128827412923177}]}, {"text": "Experimentation with count weighting schemes and optimized interpolation of adapted priors may well be beneficial ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: All errors included. Unadapted classifier (U) vs.  two L1-adapted classifiers (C1 and C2). Results on the  training set.", "labels": [], "entities": []}, {"text": " Table 2: Determiner errors. Unadapted classifier (U) vs.  two L1-adapted classifiers (C1 and C2). Results on the  training set.", "labels": [], "entities": []}, {"text": " Table 3: Preposition errors. Unadapted classifier (U) vs.  two L1-adapted classifiers (C1 and C2). Results on the  training set.", "labels": [], "entities": []}, {"text": " Table 4: Training set results, all errors", "labels": [], "entities": []}, {"text": " Table 5: Training set results, determiner errors", "labels": [], "entities": [{"text": "determiner errors", "start_pos": 32, "end_pos": 49, "type": "METRIC", "confidence": 0.859268307685852}]}, {"text": " Table 6: Training set results, preposition errors", "labels": [], "entities": []}, {"text": " Table 7: Test set results, all errors", "labels": [], "entities": []}, {"text": " Table 8: Test set results, determiner errors", "labels": [], "entities": [{"text": "determiner errors", "start_pos": 28, "end_pos": 45, "type": "METRIC", "confidence": 0.8846926689147949}]}, {"text": " Table 9: Test set results, preposition errors", "labels": [], "entities": []}]}