{"title": [{"text": "Linguistically-Enriched Models for Bulgarian-to-English Machine Translation", "labels": [], "entities": [{"text": "Bulgarian-to-English Machine Translation", "start_pos": 35, "end_pos": 75, "type": "TASK", "confidence": 0.615047017733256}]}], "abstractContent": [{"text": "In this paper, we present our linguistically-enriched Bulgarian-to-English statistical machine translation model, which takes a statistical machine translation (SMT) system as backbone various linguistic features as factors.", "labels": [], "entities": [{"text": "Bulgarian-to-English statistical machine translation", "start_pos": 54, "end_pos": 106, "type": "TASK", "confidence": 0.5144246816635132}, {"text": "statistical machine translation (SMT)", "start_pos": 128, "end_pos": 165, "type": "TASK", "confidence": 0.785727933049202}]}, {"text": "The motivation is to take advantages of both the robustness of the SMT system and the rich linguistic knowledge from morphological analysis as well as the hand-crafted grammar resources.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9883999824523926}]}, {"text": "The automatic evaluation has shown promising results and our extensive manual analysis confirms the high quality of the translation the system delivers.", "labels": [], "entities": []}, {"text": "The whole framework is also extensible for incorporating information provided by different sources.", "labels": [], "entities": []}], "introductionContent": [{"text": "Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.6601282159487406}]}, {"text": "The same story happens in the machine translation community.", "labels": [], "entities": [{"text": "machine translation community", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.8379331827163696}]}, {"text": "Along with the success of statistical machine translation (SMT) models (summarized by), various approaches have been proposed to include linguistic information, ranging from early work by to recent work by, from deep transferbased models to mapping rules at the syntactic level (;.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.7854685634374619}]}, {"text": "Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory.", "labels": [], "entities": [{"text": "SMT", "start_pos": 170, "end_pos": 173, "type": "TASK", "confidence": 0.988506555557251}]}, {"text": "proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level.  and have shown the effectiveness of adding supertags on the target side, and have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9406630992889404}, {"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.8249791860580444}]}, {"text": "However, all of them attempt to enrich the English part of the language pairs being translated.", "labels": [], "entities": []}, {"text": "For the language pairs like Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc, on the Bulgarian part.", "labels": [], "entities": []}, {"text": "There was a system published by, which was trained and tested on the European Union law data, but not on other popular domains like news.", "labels": [], "entities": [{"text": "European Union law data", "start_pos": 69, "end_pos": 92, "type": "DATASET", "confidence": 0.8213193416595459}]}, {"text": "They reported a very high BLEU score () on the Bulgarian-English translation direction (61.3).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9844638705253601}]}, {"text": "Apart from being morphologically-rich, Bulgarian has a number of challenging linguistic phenomena to consider, including free word order, long distance dependency, coreference relations, clitic doubling, etc.", "labels": [], "entities": [{"text": "clitic doubling", "start_pos": 187, "end_pos": 202, "type": "TASK", "confidence": 0.7817115187644958}]}, {"text": "For instance, the following two sentences:  gives.", "labels": [], "entities": []}, {"text": "The boy gives it to her. are difficult for the traditional phrase-based SMT system, because the clitic in the first sentence must not be translated, while in the second case it is obligatory.", "labels": [], "entities": [{"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9005903005599976}]}, {"text": "Via the semantic analysis (e.g., Minimal Recursion Semantics), the clitic information will be incorporated in the representation of the corresponding arguments.", "labels": [], "entities": [{"text": "Minimal Recursion Semantics", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.7618797818819681}]}, {"text": "In this work, we rely on the linguistic processing to cope with some of these phenomena and improve the correspondences between the two languages: 1) The lemmatization factors out the difference between word forms and ensures better coverage of the Bulgarian-English lexicon.", "labels": [], "entities": []}, {"text": "2) The dependency parsing helps to identify the grammatical functions such as subject, object in sentences with a non-standard word order.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7554173171520233}]}, {"text": "3) The semantic analysis provides a further abstraction which hides some of the language specific features.", "labels": [], "entities": []}, {"text": "Example of the last is the case of clitic doubling.", "labels": [], "entities": [{"text": "clitic doubling", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.807155191898346}]}, {"text": "As for the Bulgarian-to-English translation model, we basically 'annotate' the SMT baseline with various linguistic features derived from the preprocessing and hand-crafted grammars.", "labels": [], "entities": [{"text": "Bulgarian-to-English translation", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.6368788331747055}, {"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9691224694252014}]}, {"text": "There are three contributions of this work: \u2022 The models trained on a decent amount of parallel corpora output surprisingly good results, in terms of automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "\u2022 The enriched models give us more space for experimenting with different linguistic features without losing the 'basic' robustness.", "labels": [], "entities": []}, {"text": "\u2022 According to our extensive manual analyses, the approach has shown promising results for future integration of more knowledge from the continued advances of the deep grammars.", "labels": [], "entities": []}, {"text": "The rest of the paper will be organized as follows: Section 2 briefly introduces some background of the hand-crafted grammar resources we use and also some previous related work on transfer-based MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 196, "end_pos": 198, "type": "TASK", "confidence": 0.7985665202140808}]}, {"text": "Section 3 describes the linguistic analyses we perform on the Bulgarian text, whose output is used in the factored SMT model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.910243570804596}]}, {"text": "We show our experiments in Section 4 as well as both automatic and detailed manual evaluation of the results.", "labels": [], "entities": []}, {"text": "We summarize this paper in Section 5 and point out several directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To run the experiments, we use the phrase-based translation model provided by the open-source statistical machine translation system, Moses).", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6614906340837479}, {"text": "statistical machine translation", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.6781251430511475}]}, {"text": "For training the translation model, the SETIMES parallel corpus has been used, which is part of the OPUS parallel corpus . As for the choice of the datasets, the language is more diverse in the news articles, compared with other corpora in more controlled settings, e.g., the JRC-Acquis corpus 6 used by.", "labels": [], "entities": [{"text": "SETIMES parallel corpus", "start_pos": 40, "end_pos": 63, "type": "DATASET", "confidence": 0.6782143314679464}, {"text": "OPUS parallel corpus", "start_pos": 100, "end_pos": 120, "type": "DATASET", "confidence": 0.7741869489351908}, {"text": "JRC-Acquis corpus 6", "start_pos": 276, "end_pos": 295, "type": "DATASET", "confidence": 0.975613554318746}]}, {"text": "We split the corpus into the training set and the test set by 150,000 and 1,000 sentence pairs respectively . Both datasets are preprocessed with the tokenizer and lowercase converter provided by Moses.", "labels": [], "entities": []}, {"text": "Then the procedure is quite standard: We run GIZA++ (Och and Ney, 2003) for bi-directional word alignment, and then obtain the lexical translation table and phrase table.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.707493245601654}]}, {"text": "A tri-gram language model is estimated using the SRILM toolkit).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.9005372226238251}]}, {"text": "For the rest of the parameters we use the default setting provided by Moses.", "labels": [], "entities": []}, {"text": "Notice that, since on the target language side (i.e., English) we do not have any other factors than the word form, the factor-based models we use here only differentiate from each other in the translation phase, i.e., there is no 'generation' models involved.", "labels": [], "entities": []}, {"text": "The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of in terms of BLEU () and METEOR ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9990882873535156}, {"text": "METEOR", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9835801720619202}]}, {"text": "We then design various configurations to test the effectiveness of different linguistic annotations described in Section 3.", "labels": [], "entities": []}, {"text": "The detailed configurations we considered are shown in the first column of.", "labels": [], "entities": []}, {"text": "The first impression is that the BLEU scores in general are high.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9988412261009216}]}, {"text": "These models can be roughly grouped into six categories (separated by double lines): word form with linguistic features; lemma with linguistic features; models with dependency features; MRS elementary predicates (EP) and the type of the main argument of the predicate (EOV); EP features without word forms; and EP features with MRS ARG n features.", "labels": [], "entities": [{"text": "MRS ARG n", "start_pos": 328, "end_pos": 337, "type": "DATASET", "confidence": 0.852165162563324}]}, {"text": "In terms of the resulting scores, POS and Lemma seem to be effective features, as Model 2 has the highest BLEU score and Model 4 the best METEOR score.", "labels": [], "entities": [{"text": "POS", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9935190081596375}, {"text": "Lemma", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.977049708366394}, {"text": "BLEU score", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.983998715877533}, {"text": "METEOR", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9943600296974182}]}, {"text": "Model 3 indicates that linguistic features also improve the performance.", "labels": [], "entities": []}, {"text": "Model 4-6 show the necessity of including the word form as one of the factors.", "labels": [], "entities": []}, {"text": "Incorporating HLEMMA feature largely decreases the results due to the vastly increasing vocabulary, i.e., aligning and translating bi-grams instead of tokens.", "labels": [], "entities": []}, {"text": "Therefore, we did not include the results in the table.", "labels": [], "entities": []}, {"text": "After replacing the HLEMMA with HPOS, the result is close to the others (Model 8).", "labels": [], "entities": [{"text": "HLEMMA", "start_pos": 20, "end_pos": 26, "type": "DATASET", "confidence": 0.7137895822525024}, {"text": "HPOS", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.9333006739616394}]}, {"text": "Model 9 may also indicate that increasing the number of factors does not guarantee performance enhancement.", "labels": [], "entities": []}, {"text": "The experiments with predicate features (EP and EOV) from the MRS analyses (Model 10-12) show improvements over the baseline consistently and using only the MRS features (Model 13-14) also delivers descent results.", "labels": [], "entities": [{"text": "EOV", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.8956032395362854}]}, {"text": "Concerning the MRS ARG n features, the models with ARG n EP again suffer from the sparseness problem as the dependency HLEMMA features, but the models with ARG n POS (Model 15-16) achieve better performance than those with dependency HPOS features.", "labels": [], "entities": [{"text": "MRS ARG n", "start_pos": 15, "end_pos": 24, "type": "DATASET", "confidence": 0.6806363066037496}]}, {"text": "This is mainly because the dependency information is encoded together with the (syntactically) dependent word, while the MRS arguments are grouped around the semantic heads.", "labels": [], "entities": [{"text": "MRS arguments", "start_pos": 121, "end_pos": 134, "type": "TASK", "confidence": 0.7551331222057343}]}, {"text": "So far, incorporating additional linguistic knowledge has not shown huge improvement in terms of statistical evaluation metrics.", "labels": [], "entities": []}, {"text": "However, this does not mean that the translations delivered are the same.", "labels": [], "entities": []}, {"text": "In order to fully evaluate the system, manual analysis is absolutely necessary.", "labels": [], "entities": []}, {"text": "We are still far from drawing a conclusion at this point, but the automatic evaluation scores already indicate that the system can deliver decent translation quality consistently.", "labels": [], "entities": []}, {"text": "We manually validated the output for all the models mentioned in  aspects of the quality of the translation: Grammaticality and Content.", "labels": [], "entities": []}, {"text": "Grammaticality can be evaluated solely on the system output and Content by comparison with the reference translation.", "labels": [], "entities": []}, {"text": "We use a 1-5 score for each aspect as follows: The translation is not understandable.", "labels": [], "entities": []}, {"text": "2. The evaluator can somehow guess the meaning, but cannot fully understand the whole text.", "labels": [], "entities": []}, {"text": "3. The translation is understandable, but with some efforts.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.980348527431488}]}, {"text": "4. The translation is quite fluent with some minor mistakes or re-ordering of the words.", "labels": [], "entities": []}, {"text": "5. The translation is perfectly readable and grammatical.", "labels": [], "entities": []}, {"text": "Although the reported manual evaluation in the previous section demonstrates that linguistic knowledge improves the translation, we notice that the evaluators tend to give marks at the two ends of scale, and less in the middle.", "labels": [], "entities": [{"text": "translation", "start_pos": 116, "end_pos": 127, "type": "TASK", "confidence": 0.9725649356842041}]}, {"text": "Generally, this is because the measurement is done on the basis of the content that the evaluators extract from the Bulgarian sentence using there own cognitive capacity.", "labels": [], "entities": []}, {"text": "Then they start to overestimate or underestimate the translation, knowing in advance what has to be translated.", "labels": [], "entities": []}, {"text": "In order to avoid this subjectivity, we design a different manual evaluation in which the evaluator does not know the original Bulgarian sentences.", "labels": [], "entities": []}, {"text": "Then the evaluation is based only on the content represented within the English translation.", "labels": [], "entities": []}, {"text": "In order to do this, we represent the content of the Bulgarian sentences as a set of questions that have a list of possible answers, assigned to them.", "labels": [], "entities": []}, {"text": "During the judgement of the content transfer, the evaluators need to answer these questions.", "labels": [], "entities": []}, {"text": "As the list of answers also contains false answers, the evaluators are forced to select the right answer which can be inferred from the English translation.", "labels": [], "entities": []}, {"text": "The actual questions are created semiautomatically from the dependency analysis of the sentences.", "labels": [], "entities": []}, {"text": "We defined a set of rules for generation of the questions on the basis of the dependency relations.", "labels": [], "entities": []}, {"text": "For example, if a sentence has only a subject relation presented within the analysis, the question will be about who is doing the event.", "labels": [], "entities": []}, {"text": "If the analysis presents subject and direct object, the question will be about who is doing something with what/whom.", "labels": [], "entities": []}, {"text": "These automatically generated questions are manually investigated and, if necessary, edited.", "labels": [], "entities": []}, {"text": "Also, additional answers are formulated on the basis of general language knowledge.", "labels": [], "entities": []}, {"text": "The main idea is that the possible answers are conceptually close to each other, but not in a hypernymy relation.", "labels": [], "entities": []}, {"text": "Always there is an answer \"none\".", "labels": [], "entities": []}, {"text": "Then the questions are divided into small groups and distributed to be answered by three evaluators in such away that each question is answered by two evaluators, but no evaluator answers the whole set of questions fora given sentence.", "labels": [], "entities": []}, {"text": "In this way, we try to minimize the influence of one question to the answers of the next questions.", "labels": [], "entities": []}, {"text": "The answers are compared to the true answers of the questions for each given sentence.", "labels": [], "entities": []}, {"text": "We evaluated 192 questions for each model and sum up the scores (correctly answered questions) in.", "labels": [], "entities": []}, {"text": "This evaluation is more expensive, but we expect them to be more objective.", "labels": [], "entities": []}, {"text": "As fora related work, used textual entailment to evaluate different parser outputs.", "labels": [], "entities": []}, {"text": "The way they constructed the hypotheses is similar to our creation of questions (based on dependency relations).", "labels": [], "entities": []}, {"text": "However, they focused on the automatic evaluation and we adopt it for the manual evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The sentence analysis with added head information -HLemma and HPOS.", "labels": [], "entities": [{"text": "HLemma", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.8268190026283264}, {"text": "HPOS", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.8827760815620422}]}, {"text": " Table 2: Representation of MRS factors for each wordform in the sentence.", "labels": [], "entities": [{"text": "Representation of MRS", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6115096112092336}]}, {"text": " Table 3: Results of the factor-based model (Bulgarian-English, SETIMES 150,000/1,000)", "labels": [], "entities": [{"text": "SETIMES", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.8838840126991272}]}, {"text": " Table 4: Manual evaluation of the grammaticality and the content", "labels": [], "entities": []}]}