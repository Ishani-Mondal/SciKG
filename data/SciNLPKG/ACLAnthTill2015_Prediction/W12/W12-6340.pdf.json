{"title": [{"text": "A Conditional Random Field-based Traditional Chinese Base-Phrase Parser for SIGHAN Bake-off 2012 Evaluation", "labels": [], "entities": [{"text": "Conditional Random Field-based Traditional Chinese Base-Phrase Parser", "start_pos": 2, "end_pos": 71, "type": "TASK", "confidence": 0.5329276280743735}, {"text": "SIGHAN Bake-off 2012", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7375953396161398}]}], "abstractContent": [{"text": "This paper describes our system for the sub-task 1 of traditional Chinese Parsing of SIGHAN Bake-off 2012 evaluation.", "labels": [], "entities": [{"text": "Chinese Parsing of SIGHAN Bake-off 2012 evaluation", "start_pos": 66, "end_pos": 116, "type": "DATASET", "confidence": 0.63309132201331}]}, {"text": "Since this research mainly focuses on speech recognition and synthesis applications, only base phrase chunking was implemented using three Conditional Random Field (CRF) modules, including word segmentation, POS tagging and base phrase chunking subsystems.", "labels": [], "entities": [{"text": "speech recognition and synthesis", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.8141483068466187}, {"text": "base phrase chunking", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.6946019132932028}, {"text": "word segmentation", "start_pos": 189, "end_pos": 206, "type": "TASK", "confidence": 0.7531749308109283}, {"text": "POS tagging", "start_pos": 208, "end_pos": 219, "type": "TASK", "confidence": 0.827435314655304}]}, {"text": "The official evaluation results show that the system achieved 0.5038 (0.7210/0.387) micro-and 0.5301 (0.7343/0.4147) macro-averaging F1 (precision/recall) rates on full sentence parsing task.", "labels": [], "entities": [{"text": "F1", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.7938185930252075}, {"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.7792698740959167}, {"text": "recall)", "start_pos": 147, "end_pos": 154, "type": "METRIC", "confidence": 0.8745680749416351}, {"text": "full sentence parsing task", "start_pos": 164, "end_pos": 190, "type": "TASK", "confidence": 0.6612061262130737}]}, {"text": "However, if only the performance of base phrase chunking was considered, the F-measures maybe around 0.70 and is somehow good enough for speech recognition and synthesis applications.", "labels": [], "entities": [{"text": "base phrase chunking", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7012673815091451}, {"text": "F-measures", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9929025769233704}, {"text": "speech recognition and synthesis", "start_pos": 137, "end_pos": 169, "type": "TASK", "confidence": 0.7478083819150925}]}], "introductionContent": [{"text": "For NLP researches, a semantic parser is used for mapping a natural-language sentence into a formal representation of its meaning.", "labels": [], "entities": []}, {"text": "It usually first groups the elements in a sentence into words, phrases and clause and then tags each word, phrase and clause with a semantic label.", "labels": [], "entities": []}, {"text": "There are still many challenges in semantic parsing, but the intermediate results of the semantic parsing are already quite useful for speech recognition and text-to-speech applications.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.8128937184810638}, {"text": "semantic parsing", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.7637665867805481}, {"text": "speech recognition", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7976587414741516}]}, {"text": "For example, word sequences information could be used to build the language model in automatic speech recognition (ASR), and the phrase and clause results can be used to further verify the recognition result.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 85, "end_pos": 119, "type": "TASK", "confidence": 0.8143454889456431}]}, {"text": "In text-to-speech system, boundary information of the words, phrases and clauses can be used to better predict the prosody of synthesis speech.", "labels": [], "entities": []}, {"text": "There are many tasks in the Chinese parser, such as word segmentation, POS tagging, base phrase chunking and full parsing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7639011144638062}, {"text": "POS tagging", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.8649655878543854}, {"text": "base phrase chunking", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.6261750360329946}, {"text": "full parsing", "start_pos": 109, "end_pos": 121, "type": "TASK", "confidence": 0.7149920463562012}]}, {"text": "They are basically sequential learning problems.", "labels": [], "entities": []}, {"text": "Thus in the past decade, many statistical methods, such as Support Vector Machine (SVM), conditional random field (CRF) (), Maximum entropy Markov models (MEMMs), etc. were proposed for handling this sequential learning task.", "labels": [], "entities": []}, {"text": "Among them, CRF-based approach has been shown to be especially effective and with very low computational complexity by past studies).", "labels": [], "entities": []}, {"text": "Thus, in this paper, the CRF-based method was adopted to implement our system.", "labels": [], "entities": []}, {"text": "Instead of full parsing, base phrase chunking that identifies non-recursively cores of various types of phrases is possibly just the precursor of full parsing.", "labels": [], "entities": []}, {"text": "However, in our text-to-speech and speech recognition applications, the information of base phrase is somehow the most useful cues.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7171755731105804}]}, {"text": "Moreover, the complexity of base phrase chunking is much lower than full chunking.", "labels": [], "entities": [{"text": "base phrase chunking", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.6777219971021017}]}, {"text": "Therefore, only base phrase chunking was implemented in our system.", "labels": [], "entities": [{"text": "base phrase chunking", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.6711495916048685}]}, {"text": "In this paper, a traditional Chinese base phrase chunking system developed for the Bakeoff-2012 evaluation was described in section 2.", "labels": [], "entities": []}, {"text": "In section 3, the evaluation result of our system was discussed.", "labels": [], "entities": []}, {"text": "Finally, the conclusion was given in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Chinese Parsing Sub-task 1 The system use for Bakeoff-2012 Traditional Chinese Parsing sub-task 1 is modified from the basic parser described in last section.", "labels": [], "entities": [{"text": "Bakeoff-2012", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.7374703884124756}]}, {"text": "In the Bakeoff-2012 Traditional Chinese Parsing sub-task 1, the input sentences were segmented with gold standard word sequences.", "labels": [], "entities": [{"text": "Bakeoff-2012 Traditional Chinese Parsing sub-task", "start_pos": 7, "end_pos": 56, "type": "TASK", "confidence": 0.7004113316535949}]}, {"text": "Thus, the basic system was modified to generate the nbest word sequences in POS tagging and compound word construction stages for this evaluation.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.6931578516960144}]}, {"text": "The n-best word sequences satisfied with the defined principles, minimum edit-distance and maximum log-likelihood, in the test data set were returned as pre-processing word sequences.", "labels": [], "entities": []}, {"text": "Finally, the n-best word sequences with their corresponding POS tags can be sent into basephrase chunking module forgetting the basephrase chunking results.", "labels": [], "entities": []}, {"text": "The official evaluation report of our system for Traditional Chinese Parsing sub-Task 1 is shown in Basically, the evaluation results show that our system achieved 0.5038 (0.7210/0.387) microand 0.5301 (0.7343/0.4147) macro-averaging F1 (precision/recall) on full sentence parsing task.", "labels": [], "entities": [{"text": "Traditional Chinese Parsing", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.522666722536087}, {"text": "F1", "start_pos": 234, "end_pos": 236, "type": "METRIC", "confidence": 0.8749353289604187}, {"text": "precision", "start_pos": 238, "end_pos": 247, "type": "METRIC", "confidence": 0.9580576419830322}, {"text": "recall", "start_pos": 248, "end_pos": 254, "type": "METRIC", "confidence": 0.80060875415802}, {"text": "full sentence parsing task", "start_pos": 259, "end_pos": 285, "type": "TASK", "confidence": 0.6937646940350533}]}, {"text": "However, it is believed that the main reason for low recall rate is only base phrases were tagged in our system.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9836538136005402}]}, {"text": "Therefore, if only the performance of base phrase chunking were considered, the F-measures maybe around 0.70.", "labels": [], "entities": [{"text": "base phrase chunking", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.6595738132794698}, {"text": "F-measures", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9980929493904114}]}, {"text": "The results are somehow good enough for speech recognition and synthesis applications.", "labels": [], "entities": [{"text": "speech recognition and synthesis", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.7876337766647339}]}, {"text": "Another possibility of performance degradation is that the number of (X\u2027DE) phrases in the training corpus is above 13% of total base phrases (In fact, \u7684(/de/) should be one of the most frequently occurred words in traditional Chinese text).", "labels": [], "entities": []}, {"text": "But, there is no (X\u2027DE) phrase in the evaluation data.", "labels": [], "entities": [{"text": "DE", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.8177846074104309}]}, {"text": "It maybe the reason why the performance of base phrase chunking was degenerate from 0.84 to 0.70.", "labels": [], "entities": [{"text": "base phrase chunking", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.61141037940979}]}], "tableCaptions": [{"text": " Table 1. The performance of base phrase chunking  in training and self-evaluation database.", "labels": [], "entities": [{"text": "base phrase chunking", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.7029815316200256}]}]}