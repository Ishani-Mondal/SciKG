{"title": [{"text": "Modeling covert event retrieval in logical metonymy: probabilistic and distributional accounts", "labels": [], "entities": [{"text": "covert event retrieval", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.6721843282381693}]}], "abstractContent": [{"text": "Logical metonymies (The student finished the beer) represent a challenge to composi-tionality since they involve semantic content not overtly realized in the sentence (covert events \u2192 drinking the beer).", "labels": [], "entities": []}, {"text": "We present a contrastive study of two classes of computational models for logical metonymy in German, namely a probabilistic and a distributional, similarity-based model.", "labels": [], "entities": []}, {"text": "These are built using the SDEWAC corpus and evaluated against a dataset from a self-paced reading and a probe recognition study for their sensitivity to thematic fit effects via their accuracy in predicting the correct covert event in a metonymical context.", "labels": [], "entities": [{"text": "SDEWAC corpus", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.8824684619903564}, {"text": "probe recognition", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.690336748957634}]}, {"text": "The similarity-based models allow for better coverage while maintaining the accuracy of the probabilistic models.", "labels": [], "entities": [{"text": "coverage", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.8784186244010925}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9993937015533447}]}], "introductionContent": [{"text": "Logical metonymies (The student finished the beer) require the interpretation of a covert event which is not overtly realized in the sentence (\u2192 drinking the beer).", "labels": [], "entities": []}, {"text": "Logical metonymy has received much attention as it raises issues that are relevant to both theoretical as well as cognitive accounts of language.", "labels": [], "entities": [{"text": "Logical metonymy", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7748880982398987}]}, {"text": "On the theoretical side, logical metonymies constitute a challenge for theories of compositionality in press) since their interpretation requires additional, inferred information.", "labels": [], "entities": []}, {"text": "There are two main accounts of logical metonymy: According to the lexical account, a type clash between an event-subcategorizing verb (finish) and an entity-denoting object (beer) triggers the recovery of a covert event from complex lexical entries, such as qualia structures.", "labels": [], "entities": []}, {"text": "The pragmatic account of logical metonymy suggests that covert events are retrieved through post-lexical inferences triggered by our world knowledge and communication principles).", "labels": [], "entities": []}, {"text": "On the experimental side, logical metonymy leads to higher processing costs;.", "labels": [], "entities": []}, {"text": "As to covert event retrieval, it has been found that verbs cue fillers with a high thematic fit for their argument positions (e.g. arrest We evaluate two classes of computational models for logical metonymy.", "labels": [], "entities": [{"text": "covert event retrieval", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.6561372379461924}]}, {"text": "The classes represent the two main current approaches in lexical semantics: probabilistic and distributional models.", "labels": [], "entities": []}, {"text": "Probabilistic models view the interpretation as the assignment of values to random variables.", "labels": [], "entities": []}, {"text": "Their advantage is that they provide a straightforward way to include context, by simply including additional random variables.", "labels": [], "entities": []}, {"text": "However, practical estimation of complex models typically involves independence assumptions, which mayor may not be appropriate, and such models only take first-order co-occurrence into account . In contrast, distributional models represent linguistic entities as co-occurrence vectors and phrase interpretation as a vector similarity maximization problem.", "labels": [], "entities": [{"text": "phrase interpretation", "start_pos": 290, "end_pos": 311, "type": "TASK", "confidence": 0.7472492456436157}]}, {"text": "Distributional models typically do not require any independence assumptions, and include second-order co-occurrences.", "labels": [], "entities": []}, {"text": "At the same time, how to integrate context into the vector computation is essentially an open research question.", "labels": [], "entities": []}, {"text": "In this paper, we provide the first (to our knowledge) distributional model of logical metonymy by extending the context update of Lenci's ECU model.", "labels": [], "entities": []}, {"text": "We compare this model to a previous probabilistic approach (.", "labels": [], "entities": []}, {"text": "In contrast to most experimental studies on logical metonymy, which deal with English data (with the exception of), we focus on German.", "labels": [], "entities": []}, {"text": "We estimate our models on a large web corpus and evaluate them on a psycholinguistic dataset ().", "labels": [], "entities": []}, {"text": "The task we use to evaluate our models is to distinguish covert events with a high typicality / thematic fit (e.g. The student finished the beer \u2212\u2192 drinking) from low typicality / thematic fit covert events (\u2212\u2192 brewing).", "labels": [], "entities": []}, {"text": "model the interpretation of a logical metonymy (e.g. The student finished the beer) as the joint distribution P (s, v, o, e) of the variables s (the subject, e.g. student), v (the metonymic verb, e.g. finish), o (the object, e.g. beer), e (the covert event, drinking).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the probabilistic models (Sec. 2) and the similarity-based models (Sec.", "labels": [], "entities": []}, {"text": "3) on a dataset constructed from two German psycholinguistic studies on logical metonymy.", "labels": [], "entities": []}, {"text": "One study used self-paced reading and the second one probe recognition.", "labels": [], "entities": [{"text": "probe recognition", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.5392413437366486}]}, {"text": "Dataset The dataset we use is composed of 96 sentences.", "labels": [], "entities": []}, {"text": "There are 24 sets of four s, v, o, e tuples, where sis the object, v the metonymic verb, o the object and e the covert event.", "labels": [], "entities": []}, {"text": "The materials are illustrated in.", "labels": [], "entities": []}, {"text": "As can be seen, all tuples within a set share the same metonymic verb and the same object.", "labels": [], "entities": []}, {"text": "Each of the two subject e is matched once with a high-typicality covert event and once with a low-typicality covert event.", "labels": [], "entities": []}, {"text": "This results in 2 hightypicality tuples and 2 low-typicality tuples in each set.", "labels": [], "entities": []}, {"text": "Typical events (e) were elicited by 20 participants given the corresponding object o, subjects were elicited by 10 participants as the prototypical agents subjects for each e, o combination.", "labels": [], "entities": []}, {"text": "The experiments yielded a main effect of typicality on self-paced reading times ( and on probe recognition latencies (: typical events involved in logical metonymy interpretation are read faster and take longer to be rejected as probe words after sentences which evoke them.", "labels": [], "entities": [{"text": "probe recognition", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7034831047058105}, {"text": "logical metonymy interpretation", "start_pos": 147, "end_pos": 178, "type": "TASK", "confidence": 0.6537084480126699}]}, {"text": "The effect is seen early on (after the patient position in the self-paced reading and at short ISI for the probe recognition), suggesting that knowledge of typical events is quickly integrated in processing and that participants access a broader pool of knowledge than what has traditionally been argued to be in the lexical entries of nouns.", "labels": [], "entities": [{"text": "probe recognition", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.7079791873693466}]}, {"text": "The finding is in agreement with results of psycholinguistic studies which challenge the very distinction between world knowledge and linguistic knowledge.", "labels": [], "entities": []}, {"text": "DM for German Since DM exists only for English, we constructed a German analog using the 884M word SDEWAC web corpus) parsed with the MATE German dependency parser.", "labels": [], "entities": [{"text": "884M word SDEWAC web corpus", "start_pos": 89, "end_pos": 116, "type": "DATASET", "confidence": 0.7913957118988038}, {"text": "MATE German dependency parser", "start_pos": 134, "end_pos": 163, "type": "DATASET", "confidence": 0.8406187891960144}]}, {"text": "From this corpus, we extract 55M instances of simple syntactic relations (subj_tr, subj_intr, obj, iobj, comp, nmod) and 104M instances of lexicalized patterns such as noun-prep-noun e.g. Recht auf Auskunft (right to information), or adj-noun-(of)-noun such as strittig Entscheidung Schiedsrichter (contested decision referee).", "labels": [], "entities": []}, {"text": "These lexicalized patterns make our model roughly similar to the English TypeDM model (Sec. 3.1.1).", "labels": [], "entities": [{"text": "English TypeDM model", "start_pos": 65, "end_pos": 85, "type": "DATASET", "confidence": 0.7556807200113932}]}, {"text": "As for \u03c3, we used local mutual information (LMI) as proposed by.", "labels": [], "entities": []}, {"text": "The LMI of a triple is defined as O w 1 lw 2 log(O w 1 lw 2 /E w 1 lw 2 ), where O w 1 lw 2 is the observed co-occurrence frequency of the triple and E w 1 lw 2 its expected cooccurrence frequency (under the assumption of independence).", "labels": [], "entities": [{"text": "LMI", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9164302945137024}]}, {"text": "Like standard MI, LMI measures the informativity or surprisal of a co-occurrence, but weighs it by the observed frequency to avoid the overestimation for low-probability events.", "labels": [], "entities": [{"text": "LMI", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.8542203903198242}]}, {"text": "We evaluate the output of the model with the standard measures coverage and accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9983818531036377}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9988560676574707}]}, {"text": "Coverage is defined as the percentage of datapoints for which a model can make a prediction.", "labels": [], "entities": []}, {"text": "Lack of coverage arises primarily from sparsity, that is, zero counts for co-occurrences that are necessary in the estimation of a model.", "labels": [], "entities": [{"text": "coverage", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9687407612800598}]}, {"text": "Accuracy is computed on the covered contexts only, as the ratio of correct predictions to the number of predictions of the model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.995797872543335}]}, {"text": "This allows us to judge the quality of the model's predictions independent of its coverage.", "labels": [], "entities": []}, {"text": "We also consider a measure that combines coverage and accuracy, Backoff Accuracy, defined as: coverage\u00d7accuracy+((1\u2212coverage)\u00d70.5).", "labels": [], "entities": [{"text": "coverage", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9961456060409546}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9979439377784729}, {"text": "Backoff Accuracy", "start_pos": 64, "end_pos": 80, "type": "METRIC", "confidence": 0.8525399565696716}, {"text": "coverage", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9901123642921448}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.7258312106132507}]}, {"text": "Backoff Accuracy emulates a backoff procedure: the model's predictions are adopted where they are available; for the remaining datapoints, it assumes baseline performance (in the current setup, 50%).", "labels": [], "entities": []}, {"text": "The Backoff Accuracy of low-coverage models tends to degrade towards baseline performance.", "labels": [], "entities": [{"text": "Backoff", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9960263967514038}, {"text": "Accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.6122953295707703}]}, {"text": "We determine the significance of differences between models with a \u03c7 2 test, applied to a 2\u00d72 contingency matrix containing the number of correct and incorrect answers.", "labels": [], "entities": []}, {"text": "Datapoints outside a model's coverage count half for each category, which corresponds exactly to the definition of Backoff Accuracy.", "labels": [], "entities": [{"text": "Backoff Accuracy", "start_pos": 115, "end_pos": 131, "type": "METRIC", "confidence": 0.7887410521507263}]}], "tableCaptions": []}