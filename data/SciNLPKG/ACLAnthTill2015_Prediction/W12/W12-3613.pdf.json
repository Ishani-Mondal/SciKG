{"title": [], "abstractContent": [{"text": "In this paper we present the results of a heuris-tic usability evaluation of three annotation tools (GATE, MMAX2 and UAM Corpus-Tool).", "labels": [], "entities": [{"text": "GATE", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.8735924959182739}, {"text": "MMAX2", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.8020559549331665}, {"text": "UAM Corpus-Tool", "start_pos": 117, "end_pos": 132, "type": "DATASET", "confidence": 0.8810698390007019}]}, {"text": "We describe typical usability problems from two categories: (1) general problems, which arise from a disregard of established best practices and guidelines for user interface (UI) design, and (2) more specific problems, which are closely related to the domain of linguistic annotation.", "labels": [], "entities": []}, {"text": "By discussing the domain-specific problems we hope to raise tool devel-opers' awareness for potential problem areas.", "labels": [], "entities": []}, {"text": "A set of 28 design recommendations, which describe generic solutions for the identified problems, points toward a structured and systematic collection of usability patterns for linguistic annotation tools.", "labels": [], "entities": []}], "introductionContent": [{"text": "To find valuable clues about annotation tools and the role of usability, we have reviewed the LAW proceedings from 2007-2011 1 (altogether 140 articles) systematically with regard to their main topics.", "labels": [], "entities": [{"text": "LAW proceedings from 2007-2011 1", "start_pos": 94, "end_pos": 126, "type": "DATASET", "confidence": 0.9102554440498352}]}, {"text": "As expected, most articles are concerned with linguistic corpus annotation scenarios, which are oftentimes realized by deploying automatic tools.", "labels": [], "entities": []}, {"text": "However, articles which use a manual or semi-automatic annotation approach are just as frequent.", "labels": [], "entities": []}, {"text": "Most manual annotation projects rely on annotation tools, which are either selected from the wide range of freely available tools, or crafted for the very project.", "labels": [], "entities": []}, {"text": "Although the usability of such tools, which is oftentimes paraphrased as ease-of-use or user-friendliness, is generally understood as an important factor to reduce time and effort for laborious annotation projects), a serious account on how to systematically test and engineer usability for annotation tools is largely missing. are amongst the few who evaluate the usability of a selection of tools in order to choose an adequate candidate for their annotation project.", "labels": [], "entities": [{"text": "ease-of-use", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.9715108871459961}]}, {"text": "In other respects, usability is only mentioned as a rather vague requirement that is (if at all) implemented according to the developer's personal assumption of what makes a usable tool (cf. e.g.).", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: in chapter 2 we show that usability is not some vague postulation, but actually a criterion that can be measured and systematically engineered.", "labels": [], "entities": []}, {"text": "Chapter 3 describes the testing method that has been applied to evaluate three annotation tools (GATE, MMAX2 and UAM CorpusTool) in order to reveal typical usability problems.", "labels": [], "entities": [{"text": "GATE", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.8390653729438782}, {"text": "MMAX2", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.765630841255188}, {"text": "UAM CorpusTool", "start_pos": 113, "end_pos": 127, "type": "DATASET", "confidence": 0.763653963804245}]}, {"text": "We discuss the results of the evaluation in chapter 4 and present usability recommendations for annotation tools in chapter 5.", "labels": [], "entities": []}, {"text": "These recommendations will help developers to design tools which are more usable than current implementations.", "labels": [], "entities": []}, {"text": "They can also be used as a basic usability checklist for annotators who have to choose from the wide variety of available tools.", "labels": [], "entities": []}, {"text": "Finally, the set of recommendations will serve as a starting point for further research concerning the usability of annotation tools, with the ultimate goal being to provide a wholesome collection of usability patterns for this 104 very domain.", "labels": [], "entities": []}, {"text": "Chapter 6 provides an outlook to the wider context of this particular study.", "labels": [], "entities": []}], "datasetContent": [{"text": "This subsection describes how the HW has been adopted to evaluate annotation tools.", "labels": [], "entities": []}, {"text": "Evaluators and prearrangements -For the evaluation of three exemplary annotation tools we chose three evaluators, with each of them testing each tool.", "labels": [], "entities": []}, {"text": "One of the three evaluators was a double-expert 3 , i.e. the evaluator is not only experienced in usabilitytesting, but also has experience in linguistic annotation and the use of annotation tools.", "labels": [], "entities": []}, {"text": "The other two evaluators are usability experts, with a basic background in linguistic annotation.", "labels": [], "entities": []}, {"text": "The double-expert thus had the additional function of making the usability experts aware of domain-and user-specific problems and requirements (cf..", "labels": [], "entities": []}, {"text": "A brief introductory text, which contained the essential contextual information, was provided for the other evaluators before they conducted the actual tests.", "labels": [], "entities": []}, {"text": "Additionally, the double-expert could be addressed during the first phase (CW) if any domain-specific problems kept the evaluators from solving their tasks.", "labels": [], "entities": []}, {"text": "The tasks were designed by the double-expert and pretested by two additional test persons before the actual HW session.", "labels": [], "entities": []}, {"text": "For means of feasibility we did not consider the special needs of multi-user annotation scenarios in this evaluation study.", "labels": [], "entities": []}, {"text": "We also simplified our test scenario by assuming that the schema designer and the actual annotator are the same person.", "labels": [], "entities": []}, {"text": "Large annotation projects, which involve many different annotators and schema designers at different skill levels, however imply additional requirements for annotation tools.", "labels": [], "entities": []}, {"text": "Such multi-user requirements are hard to test with expert-based evaluation approaches, but should be rather addressed by using empirical test methods (e.g. user observation or interviews).", "labels": [], "entities": []}, {"text": "System exploration (CW) -During the first phase of the evaluation the main steps and user comments were recorded as a screen capture with the corresponding audio track.", "labels": [], "entities": [{"text": "System exploration (CW)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8434704780578614}]}, {"text": "The main steps and important remarks were also written down by the double-expert, who acted as a passive observer.", "labels": [], "entities": []}, {"text": "After the evaluators had finished the first phase of the HW, the documented steps were quickly recapitulated by the observer.", "labels": [], "entities": []}, {"text": "Documentation of problems (HE) -In the second phase, the evaluators wrote down usability problems which they had discovered while solving the tasks from the first phase.", "labels": [], "entities": [{"text": "Documentation of problems (HE)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7749671439329783}]}, {"text": "During this phase, they were still allowed to use and explore the annotation tool.", "labels": [], "entities": []}, {"text": "The evaluators used a template for problem documentation, which provides fields for the name of the problem, the severity of the problem, and the violated heuristic(s).", "labels": [], "entities": []}, {"text": "The scale for the severity rating ranges from 1 (cosmetic problem) to 4 (usability catastrophe).", "labels": [], "entities": [{"text": "severity rating", "start_pos": 18, "end_pos": 33, "type": "METRIC", "confidence": 0.9686009883880615}]}, {"text": "Data analysis and clustering -At the end of the test sessions, all usability problems were analyzed by the double-expert.", "labels": [], "entities": [{"text": "Data analysis", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7599180340766907}]}, {"text": "The problems were aggregated if several evaluators described the same problem for one tool.", "labels": [], "entities": []}, {"text": "The problems were also clustered into thematic categories, which emerged during the analysis of the problems, and which are described in 107 more detail in the results section.", "labels": [], "entities": []}, {"text": "This section describes the results of the HW.", "labels": [], "entities": [{"text": "HW", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.8320377469062805}]}, {"text": "The first part views the results with focus on the vio-lated heuristics, and the second part focuses on more generic problem categories, which will be discussed in more detail in the next chapter.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Number of identified usability problems per tool.", "labels": [], "entities": []}, {"text": " Table 4: Number of general usability problems per tool  (G=GATE, M=MMAX2, U=UAM CorpusTool).", "labels": [], "entities": [{"text": "GATE", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.8602138161659241}, {"text": "MMAX2", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.904935359954834}, {"text": "UAM CorpusTool", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.6536004096269608}]}, {"text": " Table 5: Number of domain-specific annotation usabil- ity problems per tool (G=GATE, M=MMAX2, U=UAM  CorpusTool).", "labels": [], "entities": [{"text": "GATE", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.8330068588256836}, {"text": "MMAX2", "start_pos": 88, "end_pos": 93, "type": "DATASET", "confidence": 0.9102760553359985}, {"text": "UAM  CorpusTool", "start_pos": 97, "end_pos": 112, "type": "DATASET", "confidence": 0.7102060317993164}]}]}