{"title": [{"text": "Language Classification and Segmentation of Noisy Documents in Hebrew Scripts", "labels": [], "entities": [{"text": "Language Classification and Segmentation of Noisy Documents", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7386448425906045}]}], "abstractContent": [{"text": "Language classification is a preliminary step for most natural-language related processes.", "labels": [], "entities": [{"text": "Language classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7085619419813156}]}, {"text": "The significant quantity of multilingual documents poses a problem for traditional language classification schemes and requires segmentation of the document to monolingual sections.", "labels": [], "entities": []}, {"text": "This phenomenon is characteristic of classical and medieval Jewish literature, which frequently mixes Hebrew, Aramaic, Judeo-Arabic and other Hebrew-script languages.", "labels": [], "entities": []}, {"text": "We propose a method for classification and segmentation of multilingual texts in the Hebrew character set, using bigram statistics.", "labels": [], "entities": [{"text": "classification and segmentation of multilingual texts", "start_pos": 24, "end_pos": 77, "type": "TASK", "confidence": 0.714947352806727}]}, {"text": "For texts, such as the manuscripts found in the Cairo Genizah, we are also forced to deal with a significant level of noise in OCR-processed text.", "labels": [], "entities": []}], "introductionContent": [{"text": "The identification of the language in which a given testis written is a basic problem in naturallanguage processing and one of the more studied ones.", "labels": [], "entities": []}, {"text": "For some tasks, such as automatic cataloguing, it maybe used stand-alone, but, more often than not, it is just a preprocessing step for some other language-related task.", "labels": [], "entities": [{"text": "automatic cataloguing", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.5973637998104095}]}, {"text": "In some cases, even English and French, the identification of the language is trivial, due to non-identical character sets.", "labels": [], "entities": [{"text": "identification", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.9706449508666992}]}, {"text": "But this is not always the case.", "labels": [], "entities": []}, {"text": "When looking at Jewish religious documents, we often find a mixture of several languages, all with the same Hebrew character set.", "labels": [], "entities": []}, {"text": "Besides Hebrew, these include Aramaic, which was once the lingua franca in the Middle East, and Judeo-Arabic, which was used by Jews living allover the Arab world in medieval times.", "labels": [], "entities": []}, {"text": "Language classification has well-established methods with high success rates.", "labels": [], "entities": [{"text": "Language classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7294578850269318}]}, {"text": "In particular, character n-grams, which we dub n-chars, work well.", "labels": [], "entities": []}, {"text": "However, when we looked at recently digitized documents from the Cairo Genizah, we found that a large fraction contains segments in different languages, so a single language class is rather useless.", "labels": [], "entities": []}, {"text": "Instead, we need to identify monolingual segments and classify them.", "labels": [], "entities": []}, {"text": "Moreover, all that is available is the output of mediocre OCR of handwritten manuscripts that are themselves of poor quality and often seriously degraded.", "labels": [], "entities": []}, {"text": "This raises the additional challenge of dealing with significant noise in the text to be segmented and classified.", "labels": [], "entities": []}, {"text": "We describe a method for segmenting documents into monolingual sections using statistical analysis of the distribution of n-grams for each language.", "labels": [], "entities": []}, {"text": "In particular, we use cosine distance between character unigram and bigram distributions to classify each section and perform smoothing operations to increase accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9970076680183411}]}, {"text": "The algorithms were tested on artificially produced multilingual documents.", "labels": [], "entities": []}, {"text": "We also artificially introduced noise to simulate mistakes made in OCR.", "labels": [], "entities": []}, {"text": "These test documents are similar in length and language shifts to real Genizah texts, so similar results are expected for actual manuscripts.", "labels": [], "entities": []}], "datasetContent": [{"text": "We want to test the algorithm with well-defined parameters and evaluation factors.", "labels": [], "entities": []}, {"text": "So, we created artificially mixed documents, containing segments from pairs of different languages (Hebrew/Aramaic, which is hard, Hebrew/JudeoArabic, where classification is easy and segmentation is the main challenge, or a mix of all three).", "labels": [], "entities": []}, {"text": "The segments are produced using two parameters: The desired document length d and the average monolingual segment length k.", "labels": [], "entities": []}, {"text": "Obviously, \u00ed \u00b5\u00ed\u00b1\u0098 < \u00ed \u00b5\u00ed\u00b1\u0091.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b1\u0098", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.8812830050786337}]}, {"text": "We iteratively take a random number in the range and take a substring of that length from a corpus, rounded to whole words.", "labels": [], "entities": []}, {"text": "We cycle through the languages until the text is of size d.", "labels": [], "entities": []}, {"text": "The smaller k, the harder to segment.", "labels": [], "entities": []}, {"text": "Obviously splitting will not be perfect and we cannot expect to precisely split a document.", "labels": [], "entities": [{"text": "splitting", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9849855899810791}]}, {"text": "Given that, we want to establish some measures for the quality of the splitting result.", "labels": [], "entities": [{"text": "splitting", "start_pos": 70, "end_pos": 79, "type": "TASK", "confidence": 0.9673851728439331}]}, {"text": "We would like the measure to produce some kind of score to the algorithm output, using which we can indicate whether a certain feature or parameter in the algorithm improves it or not.", "labels": [], "entities": []}, {"text": "However, the result quality is not well defined since it is not clear what is more important: detecting the segment's boundaries accurately, classifying each segment correctly or even split the document to the exact number of segments.", "labels": [], "entities": []}, {"text": "For example, given along document in Hebrew with a small segment in Aramaic, is it better to return that it actually is along document in Hebrew with Aramaic segment but misidentify the segment's location or rather recognize the Aramaic segment perfectly but classify it as Judeo-Arabic.", "labels": [], "entities": []}, {"text": "There are several measures for evaluating text segmentation (.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7781772613525391}]}, {"text": "Correct word percentage -The most intuitive measure is simply measuring the percentage of the words classified correctly.", "labels": [], "entities": [{"text": "Correct word percentage", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.8991728822390238}]}, {"text": "Since the \"atomic\" block of the text is words (or sentences in some cases described further), which are certainly monolingual, this measure will resemble the algorithm accuracy pretty good for most cases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9003646969795227}]}, {"text": "It is however not enough, since in some cases it does not reflect the quality of the splitting.", "labels": [], "entities": []}, {"text": "Assume along Hebrew document with several short sentences in Aramaic.", "labels": [], "entities": []}, {"text": "If the Hebrew is 95% of the text, a result that classifies the whole text as Hebrew will get 95% but is pretty useless and we may prefer a result that identifies the Aramaic segments but errs on more words.", "labels": [], "entities": []}, {"text": "Segmentation error (SE) estimates the algorithm's sensitivity to language shifts.", "labels": [], "entities": [{"text": "Segmentation error (SE)", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.8263773798942566}]}, {"text": "It is the difference between the correct number and that returned by the algorithm, divided by correct number.", "labels": [], "entities": []}, {"text": "Obviously, SE is in the range.", "labels": [], "entities": [{"text": "SE", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9977899789810181}]}, {"text": "It will indeed resemble the problem previously described, since, if the entire document is classified as Hebrew, the SE score will be very low, as the actual number is much greater than 1.", "labels": [], "entities": [{"text": "SE score", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9900355041027069}]}, {"text": "Neighboring segments -The first thing we tested is the way a segment's classification is affected by neighboring segments.", "labels": [], "entities": []}, {"text": "We begin by checking if adding the distance of the closest segments enhances performance.", "labels": [], "entities": []}, {"text": "Define \u00ed \u00b5\u00ed\u00b1\u0086\u00ed \u00b5\u00ed\u00b1\u0090\u00ed \u00b5\u00ed\u00b1\u009c\u00ed \u00b5\u00ed\u00b1\u009f\u00ed \u00b5\u00ed\u00b1\u0092 !,!", "labels": [], "entities": []}, {"text": "= \u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u00a1 \u00ed \u00b5\u00ed\u00b1\u0099, \u00ed \u00b5\u00ed\u00b1\u0093 + \u00ed \u00b5\u00ed\u00b1\u008e \u00ed \u00b5\u00ed\u00b1\u0081\u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u00a1 !,!", "labels": [], "entities": []}, {"text": "1 + \u00ed \u00b5\u00ed\u00b1\u0081\u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u00a1 !,!", "labels": [], "entities": []}, {"text": "\u22121 . For the test we set a=0.4.", "labels": [], "entities": []}, {"text": "From, one can see that neighboring segments improve classification of short segments, while on shorter ones classification without the neighbors was superior.", "labels": [], "entities": []}, {"text": "It is not surprising that when using neighbors the splitting procedure tends to split the text to longer segments, which has good effect only if segments actually are longer.", "labels": [], "entities": []}, {"text": "We can also see from that the SE measure is now positive with k=100, which means the algorithm underestimates the number of segments even when each segment is 100 characters long.", "labels": [], "entities": [{"text": "SE measure", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9847394526004791}]}, {"text": "By further experiments, we can see that the a parameter is insignificant, and fix it at 0.3.", "labels": [], "entities": [{"text": "a", "start_pos": 44, "end_pos": 45, "type": "METRIC", "confidence": 0.982370138168335}]}, {"text": "As expected, looking at neighboring segments can often improve results.", "labels": [], "entities": []}, {"text": "The next question is if farther neighbors also do.", "labels": [], "entities": []}, {"text": "Let: \u00ed \u00b5\u00ed\u00b1\u0086\u00ed \u00b5\u00ed\u00b1\u0090\u00ed \u00b5\u00ed\u00b1\u009c\u00ed \u00b5\u00ed\u00b1\u009f\u00ed \u00b5\u00ed\u00b1\u0092 !,!", "labels": [], "entities": []}, {"text": "= \u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u00a1 \u00ed \u00b5\u00ed\u00b1\u0099, \u00ed \u00b5\u00ed\u00b1\u0093 + ! !", "labels": [], "entities": []}, {"text": "\u00ed \u00b5\u00ed\u00b1\u0081\u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u00a1 !,!", "labels": [], "entities": []}, {"text": "\u00ed \u00b5\u00ed\u00b1\u00981 + \u00ed \u00b5\u00ed\u00b1\u0081\u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u00a1 !,!", "labels": [], "entities": []}, {"text": "\u2212\u00ed \u00b5\u00ed\u00b1\u0098 . Parameter N stands for the longest distance of neighbors to consider in the score.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed", "start_pos": 1, "end_pos": 5, "type": "METRIC", "confidence": 0.8361045718193054}, {"text": "Parameter N", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.960059642791748}]}, {"text": "Parameter a is set to 0.3.", "labels": [], "entities": [{"text": "Parameter a", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9807121455669403}]}, {"text": "We see that increasing N does not have a significant impact on algorithm performance, and on shorter segment lengths performance drops with N.", "labels": [], "entities": []}, {"text": "We conclude that there is no advantage at looking at distant neighbors.", "labels": [], "entities": []}, {"text": "Post-processing -Another thing we testis the post-processing of the splitting results to refine the initial segment choice.", "labels": [], "entities": []}, {"text": "We try to move the transition point from the original position to a more accurate position using the technique described above.", "labels": [], "entities": []}, {"text": "We note is cannot affect the SE measure, since we only move the transition points without changing the classification.", "labels": [], "entities": [{"text": "SE measure", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9881058037281036}]}, {"text": "As shown in, it does improve the performance for all values of l.", "labels": [], "entities": []}, {"text": "Noise reduction -To test noise reduction, we artificially added noise, randomly replacing some letters with $.", "labels": [], "entities": [{"text": "Noise reduction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7368412166833878}, {"text": "noise reduction", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7294200956821442}]}, {"text": "Let P denote the desired noise rate and replace each letter independently with $ with probability P.", "labels": [], "entities": []}, {"text": "Since the replacements of character is mutually independent, we can expect a normal distribution of error positions, and the correction phase described above does not assume anything about the error creation process.", "labels": [], "entities": []}, {"text": "Error creation does not assign different probabilities for different characters in the text unlike natural OCR systems or other noisy processing.", "labels": [], "entities": [{"text": "Error creation", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8742411136627197}]}, {"text": "Not surprisingly, illustrates that the accuracy reduces as the error rate rises.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9996840953826904}, {"text": "error rate", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9635888636112213}]}, {"text": "However, it does not significantly drop even fora very high error rate, and obviously we cannot expect that the error reducing process will perform better then the algorithm performs on errorless text.", "labels": [], "entities": []}, {"text": "illustrates the performance of each method.", "labels": [], "entities": []}, {"text": "It looks like looking at most common nchars does not help, nor trying to correct the unrecognized character.", "labels": [], "entities": []}, {"text": "Ignoring the unrecognized character, using either bigrams or trigrams, or estimating the missing unrecognized bigram probability show the best and pretty similar results.", "labels": [], "entities": []}], "tableCaptions": []}