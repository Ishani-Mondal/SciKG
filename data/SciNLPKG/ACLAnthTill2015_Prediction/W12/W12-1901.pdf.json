{"title": [], "abstractContent": [{"text": "The frame-semantic parsing task is challenging for supervised techniques, even for those few languages where relatively large amounts of labeled data are available.", "labels": [], "entities": [{"text": "frame-semantic parsing", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7215103805065155}]}, {"text": "In this preliminary work, we consider unsupervised induction of frame-semantic representations.", "labels": [], "entities": []}, {"text": "An existing state-of-the-art Bayesian model for PropBank-style unsupervised semantic role induction (Titov and Klementiev, 2012) is extended to jointly induce semantic frames and their roles.", "labels": [], "entities": [{"text": "PropBank-style unsupervised semantic role induction", "start_pos": 48, "end_pos": 99, "type": "TASK", "confidence": 0.6448817491531372}]}, {"text": "We evaluate the model performance both quantitatively and qualitatively by comparing the induced representation against FrameNet annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Shallow representations of meaning, and semantic role labels in particular, have along history in linguistics.", "labels": [], "entities": []}, {"text": "In this paper we focus on frame-semantic representations: a semantic frame is a conceptual structure describing a situation (or an entity) and its participants (or its properties).", "labels": [], "entities": []}, {"text": "Participants and properties are associated with semantic roles (also called frame elements).", "labels": [], "entities": []}, {"text": "For example, following the FrameNet annotation guidelines), in the following sentences: the same semantic frame Apply Heat is evoked by verbs cook and sautee, and roles COOK and F OOD in the sentence (a) are filled by Mary and the broccoli, respectively.", "labels": [], "entities": [{"text": "Apply", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.9148075580596924}, {"text": "COOK", "start_pos": 169, "end_pos": 173, "type": "METRIC", "confidence": 0.9647948741912842}, {"text": "F OOD", "start_pos": 178, "end_pos": 183, "type": "METRIC", "confidence": 0.7792783975601196}]}, {"text": "Note that roles are specific to the frame, not to the individual lexical units (verbs cook and sautee, in the example).", "labels": [], "entities": []}, {"text": "1 Most approaches to predicting these representations, called semantic role labeling (SRL), have relied on large annotated datasets (.", "labels": [], "entities": [{"text": "predicting these representations", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.8709307710329691}, {"text": "semantic role labeling (SRL)", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.7673522333304087}]}, {"text": "By far, most of this work has focused on PropBank-style representations () where roles are defined for each individual verb, or even individual senses of a verb.", "labels": [], "entities": []}, {"text": "The only exceptions are modifiers and roles A0 and A1 which correspond to proto-agent (a doer, or initiator of the action) and proto-patient (an affected entity), respectively.", "labels": [], "entities": [{"text": "A0", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9509573578834534}, {"text": "A1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.8116024732589722}]}, {"text": "However, the SRL task is known to be especially hard for the FrameNetstyle representations fora number of reasons, including, the lack of cross-frame correspondence for most roles, fine-grain definitions of roles and frames in FrameNet, and relatively small amounts of statistically representative data.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 13, "end_pos": 21, "type": "TASK", "confidence": 0.9069253504276276}]}, {"text": "Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (), German), Spanish (Subirats, 2009) and Japanese ().", "labels": [], "entities": [{"text": "predicting FrameNet representations", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.7654473086198171}]}, {"text": "Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (  Lapata, 2011a;.", "labels": [], "entities": []}, {"text": "However, all these approaches have focused on PropBank-style representations.", "labels": [], "entities": []}, {"text": "This may seem somewhat unnatural as FrameNet representations, though arguably more powerful, are harder to learn in the supervised setting, harder to annotate, and annotated data is available fora considerably fewer languages.", "labels": [], "entities": []}, {"text": "This is the gap which we address in this preliminary study.", "labels": [], "entities": []}, {"text": "More specifically, we extend an existing stateof-the-art Bayesian model for unsupervised semantic role labeling and apply it to support FrameNetstyle semantics.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.683674176534017}]}, {"text": "In other words, our method jointly induces both frames and frame-specific semantic roles.", "labels": [], "entities": []}, {"text": "We experiment only with verbal predicates and evaluate the performance of the model with respect to some natural baselines.", "labels": [], "entities": []}, {"text": "Though the scores for frame induction are not high, we argue that this is primarily due to very high granularity of FrameNet frames which is hard to reproduce for unsupervised systems, as the implicit supervision signal is not capable of providing these distinctions.", "labels": [], "entities": [{"text": "frame induction", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7806636095046997}]}], "datasetContent": [{"text": "We cannot use supervised metrics to evaluate our models, since we do not have an alignment between gold labels and clusters induced in the unsupervised setup.", "labels": [], "entities": []}, {"text": "Instead, we use the standard purity (PU) and collocation (CO) metrics as well as their harmonic mean (F1) to measure the quality of the resulting clusters.", "labels": [], "entities": [{"text": "standard purity (PU) and collocation (CO)", "start_pos": 20, "end_pos": 61, "type": "METRIC", "confidence": 0.7589078634977341}, {"text": "harmonic mean (F1)", "start_pos": 87, "end_pos": 105, "type": "METRIC", "confidence": 0.8686439156532287}]}, {"text": "Purity measures the degree to which each cluster contains arguments (verbs) sharing the same gold role (gold frame) and collocation evaluates the degree to which arguments (verbs) with the same gold roles (gold frame) are assigned to a single cluster, see.", "labels": [], "entities": []}, {"text": "As in previous work, for role induction, the scores are first computed for individual predicates and then averaged with the weights proportional to the total number occurrences of roles for each predicate.", "labels": [], "entities": [{"text": "role induction", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.9211939871311188}]}, {"text": "Our model induced 128 multi-verb frames from the dataset.", "labels": [], "entities": []}, {"text": "Out of 78,039 predicate occurrences in the data, these correspond to 18,963 verb occurrences (or, approximately, 25%).", "labels": [], "entities": []}, {"text": "Some examples of the induced multi-verb frames are shown in.", "labels": [], "entities": []}, {"text": "As we can observe from the table, our model clusters semantically related verbs into a single frame, even though they may not correspond to the same gold frame in FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 163, "end_pos": 171, "type": "DATASET", "confidence": 0.9265421628952026}]}, {"text": "Consider, for example, the frame (ratify::sign::accede): the verbs are semantically related and hence they should go into a single frame, as they all denote a similar action.", "labels": [], "entities": []}, {"text": "Another result worth noting is that the model often clusters antonyms together as they are often used in similar context.", "labels": [], "entities": []}, {"text": "For example, consider the frame (cool::heat::warm), the verbs cool, heat and warm, all denote a change in temperature.", "labels": [], "entities": []}, {"text": "This agrees well with annotation in FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.9084683656692505}]}, {"text": "Similarly, we cluster sell and purchase together.", "labels": [], "entities": []}, {"text": "This contrasts with FrameNet annotation as FrameNet treats them not as antonyms but as different views on same situation and according to their guidelines, different frames are assigned to different views.", "labels": [], "entities": []}, {"text": "Often frames in FrameNet correspond to more fine-grained meanings of the verbs, as we can see in the example for (plait::braid::dye).", "labels": [], "entities": []}, {"text": "The three describe a similar activity involving hair but FrameNet gives them a finer distinction.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9199296236038208}]}, {"text": "Arguably, implicit supervision signal present in the unlabeled data is not sufficient to provide such fine-grained distinctions.", "labels": [], "entities": []}, {"text": "The model does not distinguish verb senses, i.e. it always assigns a single frame to each verb, so there is an upper bound on our clustering performance.", "labels": [], "entities": []}, {"text": "Now we turn to quantitative evaluation of both frame and role induction.", "labels": [], "entities": [{"text": "role induction", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.6918279230594635}]}, {"text": "In this section, we evaluate how well the induced frames correspond to the gold standard annotation.", "labels": [], "entities": []}, {"text": "Because of the lack of relevant previous work, we use only a trivial baseline which places each verb in a separate cluster (NoClustering).", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "As we can see from the results, our model achieves a small, but probably significant, improvement in the F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9986245632171631}]}, {"text": "Though the scores are fairly low, note that, as discussed in Section 4.4, the model is severely penalized even for inducing semantically plausible frames such as the frame (plait::braid::dye).", "labels": [], "entities": []}, {"text": "In this section, we evaluate how well the induced roles correspond to the gold standard annotation.", "labels": [], "entities": [{"text": "gold standard annotation", "start_pos": 74, "end_pos": 98, "type": "DATASET", "confidence": 0.9046379526456197}]}, {"text": "We use two baselines: one is the syntactic baseline SyntF, which simply clusters arguments according to the dependency rela-  tion to their head, as described in, and the other one is aversion of our model which does not attempt to cluster verbs and only induces roles (NoFrameInduction).", "labels": [], "entities": []}, {"text": "Note that the NoFrameInduction baseline is equivalent to the factored model of.", "labels": [], "entities": [{"text": "NoFrameInduction baseline", "start_pos": 14, "end_pos": 39, "type": "DATASET", "confidence": 0.8226698935031891}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "First, observe that both our full model and its simplified version NoFrameInduction significantly outperform the syntactic baseline.", "labels": [], "entities": [{"text": "NoFrameInduction", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.8943055272102356}]}, {"text": "It is important to note that the syntactic baseline is not trivial to beat in the unsupervised setting.", "labels": [], "entities": []}, {"text": "Though there is a minor improvement from inducing frames, it is small and may not be significant.", "labels": [], "entities": []}, {"text": "Another observation is that the absolute scores of all the systems, including the baselines, are significantly below the results reported in) on the CoNLL-08 version of PropBank in a comparable setting (auto parses, gold argument identification): 73.9 % and 77.9 % F1 for SyntF and NoFrameInduction, respectively.", "labels": [], "entities": [{"text": "CoNLL-08 version of PropBank", "start_pos": 149, "end_pos": 177, "type": "DATASET", "confidence": 0.9076576828956604}, {"text": "F1", "start_pos": 265, "end_pos": 267, "type": "METRIC", "confidence": 0.9996811151504517}]}, {"text": "We believe that the main reason for this discrepancy is the difference in the syntactic representations.", "labels": [], "entities": []}, {"text": "The CoNLL-08 dependencies include function tags (e.g., TMP, LOC), and, therefore, modifiers do not need to be predicted, whereas the Stanford syntactic dependencies do not provide this information and the model needs to induce it.", "labels": [], "entities": []}, {"text": "It is clear from these results, and also from the previous observation that only 25% of verb occurrences belong to multi-verb clusters, that the model does not induce sufficiently rich clustering of verbs.", "labels": [], "entities": []}, {"text": "Arguably, this is largely due to the relatively small size of FrameNet, as it may not provide enough evidence for clustering.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9185559153556824}]}, {"text": "Given that our method is quite efficient, a single experiment was taking around 8 hours on a single CPU, and the procedure is highly parallelizable, the next step would be to use a much larger and statistically representative corpus to induce the representations.", "labels": [], "entities": []}, {"text": "3 There is no well-established methodology for testing statistical significance when comparing two clustering methods.", "labels": [], "entities": []}, {"text": "Additional visual inspection suggest that the data is quite noisy primarily due to mistakes in parsing.", "labels": [], "entities": []}, {"text": "The large proportion of mistakes can probably be explained by the domain shift: the parser is trained on the WSJ newswire data and tested on more general BNC texts.", "labels": [], "entities": [{"text": "WSJ newswire data", "start_pos": 109, "end_pos": 126, "type": "DATASET", "confidence": 0.9828618367513021}, {"text": "BNC texts", "start_pos": 154, "end_pos": 163, "type": "DATASET", "confidence": 0.8825040459632874}]}], "tableCaptions": [{"text": " Table 2: Role labeling performance.", "labels": [], "entities": [{"text": "Role labeling", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8536717891693115}]}, {"text": " Table 3: Frame labeling performance.", "labels": [], "entities": [{"text": "Frame labeling", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8512706458568573}]}]}