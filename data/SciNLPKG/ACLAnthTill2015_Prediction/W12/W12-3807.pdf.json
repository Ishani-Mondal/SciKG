{"title": [{"text": "Statistical Modality Tagging from Rule-based Annotations and Crowdsourcing", "labels": [], "entities": [{"text": "Statistical Modality Tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.85084201892217}]}], "abstractContent": [{"text": "We explore training an automatic modality tagger.", "labels": [], "entities": [{"text": "automatic modality tagger", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7461383740107218}]}, {"text": "Modality is the attitude that a speaker might have toward an event or state.", "labels": [], "entities": []}, {"text": "One of the main hurdles for training a linguistic tag-ger is gathering training data.", "labels": [], "entities": []}, {"text": "This is particularly problematic for training a tagger for modality because modality triggers are sparse for the overwhelming majority of sentences.", "labels": [], "entities": []}, {"text": "We investigate an approach to automatically training a modality tagger where we first gathered sentences based on a high-recall simple rule-based modality tagger and then provided these sentences to Mechanical Turk annotators for further annotation.", "labels": [], "entities": []}, {"text": "We used the resulting set of training data to train a precise modality tagger using a multi-class SVM that delivers good performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modality is an extra-propositional component of meaning.", "labels": [], "entities": []}, {"text": "In John may go to NY, the basic proposition is John go to NY and the word may indicates modality.", "labels": [], "entities": [{"text": "John may go to NY", "start_pos": 3, "end_pos": 20, "type": "DATASET", "confidence": 0.738729727268219}]}, {"text": "Van Der Auwera and Ammann (2005) define core cases of modality: John must go to NY (epistemic necessity), John might go to NY (epistemic possibility), John has to leave now (deontic necessity) and John may leave now (deontic possibility).", "labels": [], "entities": []}, {"text": "Many semanticists (e.g.,,) define modality as quantification over possible worlds.", "labels": [], "entities": []}, {"text": "John might go means that there exist some possible worlds in which John goes.", "labels": [], "entities": []}, {"text": "Another view of modality relates more to a speakers attitude toward a proposition (e.g.).", "labels": [], "entities": []}, {"text": "Modality might be construed broadly to include several types of attitudes that a speaker wants to express towards an event, state or proposition.", "labels": [], "entities": []}, {"text": "Modality might indicate factivity, evidentiality, or sentiment ().", "labels": [], "entities": []}, {"text": "Factivity is related to whether the speaker wishes to convey his or her belief that the propositional content is true or not, i.e., whether it actually obtains in this world or not.", "labels": [], "entities": []}, {"text": "It distinguishes things that (the speaker believes) happened from things that he or she desires, plans, or considers merely probable.", "labels": [], "entities": []}, {"text": "Evidentiality deals with the source of information and may provide clues to the reliability of the information.", "labels": [], "entities": [{"text": "reliability", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.9502726793289185}]}, {"text": "Did the speaker have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence?", "labels": [], "entities": []}, {"text": "Sentiment deals with a speaker's positive or negative feelings toward an event, state, or proposition.", "labels": [], "entities": [{"text": "Sentiment deals with a speaker's positive or negative feelings toward an event, state, or proposition", "start_pos": 0, "end_pos": 101, "type": "Description", "confidence": 0.7440437591738172}]}, {"text": "In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (, and we leave other modalities to future work.", "labels": [], "entities": []}, {"text": "\u2022 Ability: can H do P?", "labels": [], "entities": [{"text": "Ability", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9676485061645508}]}, {"text": "\u2022 Effort: does H try to do P?", "labels": [], "entities": [{"text": "Effort", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9924947023391724}]}, {"text": "\u2022 Intention: does H intend P?", "labels": [], "entities": [{"text": "Intention", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9894570708274841}]}, {"text": "\u2022 Success: does H succeed in P?", "labels": [], "entities": [{"text": "Success", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9916251301765442}]}, {"text": "\u2022 Want: does H want P?", "labels": [], "entities": []}, {"text": "We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs).", "labels": [], "entities": [{"text": "training a modality tagger", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.7158171758055687}]}, {"text": "One of the main hurdles for training a linguistic tagger is gathering training data.", "labels": [], "entities": [{"text": "linguistic tagger", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.730997622013092}]}, {"text": "This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences.", "labels": [], "entities": [{"text": "modality tagger", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.7216025143861771}]}, {"text": "created a modality tagger by using a semiautomatic approach for creating rules fora rulebased tagger.", "labels": [], "entities": []}, {"text": "A pilot study revealed that it can boost recall well above the naturally occurring proportion of modality without annotated data but with only 60% precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9992659687995911}, {"text": "precision", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.998343825340271}]}, {"text": "We investigated an approach where we first gathered sentences based on a simple modality tagger and then provided these sentences to annotators for further annotation, The resulting annotated data also preserved the level of inter-annotator agreement for each example so that learning algorithms could take that into account during training.", "labels": [], "entities": []}, {"text": "Finally, the resulting set of annotations was used for training a modality tagger using SVMs, which gave a high precision indicating the success of this approach.", "labels": [], "entities": [{"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.991750180721283}]}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 discusses our procedure for gathering training data.", "labels": [], "entities": []}, {"text": "Section 4 discusses the machine learning setup and features used to train our modality tagger and presents experiments and results.", "labels": [], "entities": []}, {"text": "Section 5 concludes and discusses future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present experiments performed considering all the MTurk annotations where two annotators agreed and all the MTurk annotations where all three annotators agreed to be equally correct annotations.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.8858276605606079}, {"text": "MTurk", "start_pos": 128, "end_pos": 133, "type": "DATASET", "confidence": 0.8766623139381409}]}, {"text": "We present experiments applying differential weights for these annotations in Section 4.5.", "labels": [], "entities": []}, {"text": "We performed 4-fold cross validation (4FCV) on MTurk data in order to select the best feature set configuration \u03c6.", "labels": [], "entities": [{"text": "MTurk data", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.8939271867275238}]}, {"text": "The best feature set obtained was wordStem, P OS, whichM odal with a context width of 2.", "labels": [], "entities": []}, {"text": "For finding the best performing feature set -context width configuration, we did an exhaustive search on the feature space, pruning away features which were proven not useful by results at stages.", "labels": [], "entities": []}, {"text": "We also trained a model on the entire MTurk data using the best feature set \u03c6 and evaluated it against the Gold data.", "labels": [], "entities": [{"text": "MTurk data", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.9432621598243713}, {"text": "Gold data", "start_pos": 107, "end_pos": 116, "type": "DATASET", "confidence": 0.7941551208496094}]}, {"text": "The results obtained for each modality on gold evaluation are given in.", "labels": [], "entities": []}, {"text": "We attribute the lower performance on the Gold dataset to its difference from MTurk data.", "labels": [], "entities": [{"text": "Gold dataset", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.9811222553253174}, {"text": "MTurk data", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9352610111236572}]}, {"text": "MTurk data is entirely from email threads, whereas Gold data contained sentences from newswire, letters and blogs in addition to emails.", "labels": [], "entities": [{"text": "MTurk data", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.957606166601181}, {"text": "Gold data", "start_pos": 51, "end_pos": 60, "type": "DATASET", "confidence": 0.8913064002990723}]}, {"text": "Furthermore, the annotation is different.", "labels": [], "entities": []}, {"text": "Finally, the distribution of modalities in both datasets is very different.", "labels": [], "entities": []}, {"text": "For example, Ability modality was merely 6% of MTurk data compared to 48% in Gold data (see).", "labels": [], "entities": [{"text": "Ability", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9219686985015869}, {"text": "MTurk data", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.905140608549118}, {"text": "Gold data", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.8906973600387573}]}, {"text": "We obtained reasonable performances for Effort and Want modalities while the performance for other modalities was rather low.", "labels": [], "entities": []}, {"text": "Also, the Gold dataset contained only 8 instances of Success, none of which was recognized by the tagger resulting in a recall of 0%.", "labels": [], "entities": [{"text": "Gold dataset", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.8598407804965973}, {"text": "Success", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.982368528842926}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9983731508255005}]}, {"text": "Precision (and, accordingly, F Measure) for Success was considered \"not applicable\" (NA), as no such tag was assigned.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9925956130027771}, {"text": "F Measure)", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.947457492351532}, {"text": "Success", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.511362612247467}, {"text": "applicable\" (NA)", "start_pos": 72, "end_pos": 88, "type": "METRIC", "confidence": 0.702684760093689}]}, {"text": "Our MTurk data contains sentence for which at least two of the three Turkers agreed on the modality and the target of the modality.", "labels": [], "entities": [{"text": "MTurk data", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9167963564395905}]}, {"text": "In this section, we investigate the role of annotation confidence in training an automatic tagger.", "labels": [], "entities": []}, {"text": "The annotation confidence is denoted by whether an annotation was agreed by only two annotators or was unanimous.", "labels": [], "entities": [{"text": "annotation confidence", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.7704221606254578}]}, {"text": "We denote the set of sentences for which only two annotators agreed as Agr 2 and that for which all three annotators agreed as Agr 3 . We present four training setups.", "labels": [], "entities": [{"text": "Agr", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.8614314198493958}]}, {"text": "The first setup is T r23 where we train a model using both Agr 2 and Agr 3 with equal weights.", "labels": [], "entities": []}, {"text": "This is the setup we used for results presented in the Section 4.4.", "labels": [], "entities": []}, {"text": "Then, we have T r2 and T r3, where we train using only Agr 2 and Agr 3 respectively.", "labels": [], "entities": [{"text": "Agr", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.8628897666931152}]}, {"text": "Then, for T r23 W , we.", "labels": [], "entities": []}, {"text": "We also present the results of evaluating a tagger trained on the whole MTurk data for each setup against the Gold annotation in.", "labels": [], "entities": [{"text": "MTurk data", "start_pos": 72, "end_pos": 82, "type": "DATASET", "confidence": 0.9447965919971466}, {"text": "Gold", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.8654462099075317}]}, {"text": "The T r23 tested on both Agr 2 and Agr 3 presented in and T r23 tested on Gold data presented in correspond to the results presented in   One main observation is that including annotations of lower agreement, but still above a threshold (in our case, 66.7%), is definitely helpful.", "labels": [], "entities": [{"text": "Gold data", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.7072287350893021}]}, {"text": "T r23 outperformed both T r2 and T r3 in both recall and F-measure in all evaluations.", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9993537068367004}, {"text": "F-measure", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9817237854003906}]}, {"text": "Also, even when evaluating against only the high confident Agr 3 cases, T r2 gave a high gain in recall (10 .1 percentage points) over T r3, with only a 1.2 percentage point loss on precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9997606873512268}, {"text": "precision", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9987547397613525}]}, {"text": "We conjecture that this is because there are far more training instances in T r2 than in T r3 (674 vs 334), and that quantity beats quality.", "labels": [], "entities": []}, {"text": "Another important observation is the increase in performance by using varied costs for Agr 2 and Agr 3 examples (the T r23 W condition).", "labels": [], "entities": [{"text": "T r23 W condition", "start_pos": 117, "end_pos": 134, "type": "METRIC", "confidence": 0.906536191701889}]}, {"text": "Although it dropped the performance by 0.1 to 0.2 points in cross-validation F measure on the Enron corpora, it gained 2.1 points in Gold evaluation F measure.", "labels": [], "entities": [{"text": "F measure", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.8962668478488922}, {"text": "Enron corpora", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.9789188802242279}, {"text": "Gold evaluation F measure", "start_pos": 133, "end_pos": 158, "type": "METRIC", "confidence": 0.8953210264444351}]}, {"text": "These results seem to indicate that differential weighting based on annotator agreement might have more beneficial impact when training a model that will be applied to a wide range of genres than when training a model with genre-specific data for application to data from the same genre.", "labels": [], "entities": []}, {"text": "Put differently, using varied costs prevents genre over-fitting.", "labels": [], "entities": []}, {"text": "We don't have a full explanation for this difference in behavior yet.", "labels": [], "entities": []}, {"text": "We plan to explore this in future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Frequency of Modalities", "labels": [], "entities": []}, {"text": " Table 3: Per modality results for best feature set \u03c6 on  4-fold cross validation on MTurk data", "labels": [], "entities": [{"text": "MTurk data", "start_pos": 85, "end_pos": 95, "type": "DATASET", "confidence": 0.8965656459331512}]}, {"text": " Table 4: Per modality results for best feature set \u03c6 evalu- ated on Gold dataset", "labels": [], "entities": [{"text": "Gold dataset", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.8393733203411102}]}, {"text": " Table 5: Annotator Confidence Experiment Results; the best results per column are boldfaced", "labels": [], "entities": []}, {"text": " Table 5. We  also present the results of evaluating a tagger trained  on the whole MTurk data for each setup against the  Gold annotation in", "labels": [], "entities": [{"text": "MTurk data", "start_pos": 84, "end_pos": 94, "type": "DATASET", "confidence": 0.9413872957229614}]}, {"text": " Table 6. The T r23 tested on both  Agr 2 and Agr 3 presented in", "labels": [], "entities": []}, {"text": " Table 6: Annotator Confidence Experiment Results; the  best results per column are boldfaced", "labels": [], "entities": []}]}