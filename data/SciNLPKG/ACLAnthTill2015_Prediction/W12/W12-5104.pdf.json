{"title": [{"text": "Automatic index creation to support navigation in lexical graphs encoding part_of relations", "labels": [], "entities": [{"text": "index creation", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7716986835002899}]}], "abstractContent": [{"text": "We describe here the principles underlying the automatic creation of a semantic map to support navigation in a lexicon, our target group being authors (speakers, writers) rather than readers.", "labels": [], "entities": []}, {"text": "While machines can generally access information that it has stored, this does not always hold for people.", "labels": [], "entities": []}, {"text": "A speaker may very well know a word, yet still be (occasionally) unable to access it.", "labels": [], "entities": []}, {"text": "To help authors to overcome word-finding problems one could add to an existing electronic resource an index based on the (age-old) notion of association.", "labels": [], "entities": []}, {"text": "Since ideas or their expressive forms (words) are related, they may evoke each other (lemon-yellow), but the likelihood for doing so varies overtime and with the context.", "labels": [], "entities": []}, {"text": "For example, the word 'piano' may prime 'instrument' or 'weight', but which of the two gets evoked depends on the context: 'concert' vs. 'house moving'.", "labels": [], "entities": []}, {"text": "Given this dynamic aspect of the human brain, we should build the index automatically, computing the relation of terms and their weights on the fly.", "labels": [], "entities": []}, {"text": "This dynamic creation of the index could be done via a corpus.", "labels": [], "entities": []}, {"text": "This latter representing ideally the dictionary users' world knowledge, and the way how the prominence of words and ideas varies overtime.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the most vexing problems in speaking or writing is the fact that one has memorized, i.e. stored a given word, yet one fails to access it when needed.", "labels": [], "entities": []}, {"text": "This kind of search failure known as dysnomia or Tip of the Tongue-problem (TOT), occurs not only in language, but also in other activities of everyday life.", "labels": [], "entities": [{"text": "Tip of the Tongue-problem (TOT)", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.5627219506672451}]}, {"text": "It is basically a search-and index problem which we are reminded of when we look for something that exists in real world or our mind (keys, glasses, people's names), but which we are unable to locate, access or retrieve in time.", "labels": [], "entities": []}, {"text": "Word finding problems are generally dealt with via a lexicon.", "labels": [], "entities": [{"text": "Word finding problems", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8817406296730042}]}, {"text": "Obviously, readers and writers have different behaviors and expectations concerning input and output (target information).", "labels": [], "entities": []}, {"text": "While the decoder (listener/reader) provides the word s/he wants additional information for -(say, what is the meaning of 'rug', or what are its synonymes?),-the encoder (speaker/writer) provides the meaning, or meaning-related elements (for example, 'typical british sport') of the word for which s/he lacks the corresponding form (=> cricket).", "labels": [], "entities": []}, {"text": "Our concern here is more with the language producer, i.e. lexical access in language production, a task often neglected in lexicographical work.", "labels": [], "entities": []}, {"text": "Language producers typically start from meanings (concepts) or lexical items related to the target word: associations (strong + black + bitter + beverage + made_from beans => coffee).", "labels": [], "entities": []}, {"text": "Eventhough empirically well founded, concept-based search or access via associations is not well supported in current electronic dictionaries.", "labels": [], "entities": []}, {"text": "Actually, there are several problems to be addressed, let us mention only two: (a) the problem of input: how (i.e. in what terms) shall the user specify the meaning of the word whose form he is looking for?", "labels": [], "entities": []}, {"text": "-(say, 'name of the beverage the British fancy to take in the afternoon'),-and (b) the problem of navigation.", "labels": [], "entities": [{"text": "navigation", "start_pos": 98, "end_pos": 108, "type": "TASK", "confidence": 0.8799311518669128}]}, {"text": "How do you get from some input (source word), -say, 'huge animal, gray, trunk, ivory, Africa',-to the target word (elephant)?", "labels": [], "entities": []}, {"text": "Note that studies concerning the TOT-problem have shown over and over that people being in this state know a lot concerning the target word -meaning, origin, gender, number of syllables, etc.,-even if they cannot access its form ).", "labels": [], "entities": [{"text": "TOT-problem", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9104368090629578}]}], "datasetContent": [{"text": "We have tested our system for its ability to extract PT-WHRs by using the text collection of SemEval ().", "labels": [], "entities": []}, {"text": "The test corpus is POS-tagged and annotated in terms of WN senses.", "labels": [], "entities": []}, {"text": "The corpus has positive and negative semantic relations.", "labels": [], "entities": []}, {"text": "The corpus has positive and negative semantic relations.", "labels": [], "entities": []}, {"text": "The part-whole relations extracted by the system were validated by comparing them with the valid relations labeled in the test set answer key.", "labels": [], "entities": []}, {"text": "The format of the test set is described in the sample here below: \"Some sophisticated <e2>tables</e2> have three <e1>legs</e1>.\"", "labels": [], "entities": []}, {"text": "WordNet(e1) = \"n3\", WordNet(e2)=\"n2\"; Part-Whole(e1, e2) = \"true\" This format has been defined by Girju et al (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9606348276138306}, {"text": "WordNet", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.9533100724220276}]}, {"text": "Since this does not correspond to areal text format, we have changed the corpus accordingly, to obtain the following text: \"Some sophisticated tables have three legs\".", "labels": [], "entities": []}, {"text": "To evaluate the performance of our system we defined precision, recall, and F-measure metrics in the following way: Our system identified almost all of the present Component-Integral object part-whole relation pairs of the SemEval test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9991738200187683}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9980258941650391}, {"text": "F-measure", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9953176975250244}, {"text": "SemEval test set", "start_pos": 223, "end_pos": 239, "type": "DATASET", "confidence": 0.8020060857137045}]}, {"text": "Since these relations are both present and non-present in the Semeval training set and test set, we considered the present relations to evaluate the performance of our approach.", "labels": [], "entities": [{"text": "Semeval training set and test set", "start_pos": 62, "end_pos": 95, "type": "DATASET", "confidence": 0.6955269575119019}]}, {"text": "As the number of concepts having parts in different senses is very small in the SemEval test set, we have added some concepts from WN.", "labels": [], "entities": [{"text": "SemEval test set", "start_pos": 80, "end_pos": 96, "type": "DATASET", "confidence": 0.857661505540212}]}, {"text": "The resulting number of relation pairs accounts now for 20% of our test set.", "labels": [], "entities": []}, {"text": "80 % of this set contains negative examples coming either from the SemEval test set (all of them) or from our own.", "labels": [], "entities": [{"text": "SemEval test set", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.8011521995067596}]}, {"text": "We defined 'recall' as the percentage of correctly retrieved relations out of the correct relations available in the test set, while 'precision' is defined as the percentage of correctly retrieved relation out of retrieved relations.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9958963394165039}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9979221224784851}]}, {"text": "We obtained 95,2% for precision, 95% for recall and 95,1% for the F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9997928738594055}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9997344613075256}, {"text": "F-measure", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9901201725006104}]}, {"text": "The PT-WHRs extracted by the system were validated by comparing them with the valid relations labeled in the test set answer key.", "labels": [], "entities": [{"text": "PT-WHRs", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.982955813407898}]}, {"text": "The test set has answer key, so we manually counted correctly retrieved relations.: the similarity values of selected noun pairs All the encountered errors are hyponyms.", "labels": [], "entities": []}, {"text": "However, this does not imply that all the hyponyms in the test are incorrectly retrieved as part-whole relation.", "labels": [], "entities": []}, {"text": "Actually, only 12% of the hyponyms in the test set are incorrectly retrieved as part-whole relation.", "labels": [], "entities": []}, {"text": "It should also be noted that the majority (80%) of our test set relations are not part-whole relations.", "labels": [], "entities": []}, {"text": "Therefore, the probability of randomly selecting part-whole relation is 20/80 (0.25), showing the effectiveness of this approach for discriminating such relations.", "labels": [], "entities": []}, {"text": "We have also evaluated the performance of the system in determining the senses of a concept.", "labels": [], "entities": []}, {"text": "To do so we used the clustering technique described above.", "labels": [], "entities": []}, {"text": "Word forms expressing several senses have several clusters.", "labels": [], "entities": []}, {"text": "We evaluated the results against the gold standard of meronymic word senses taken from WN.", "labels": [], "entities": [{"text": "WN", "start_pos": 87, "end_pos": 89, "type": "DATASET", "confidence": 0.8954527974128723}]}, {"text": "Our clustering is based on the distance between the vectors of the parts of a given concept.", "labels": [], "entities": []}, {"text": "We defined precision as the percentage of words assigned to their actual WN meronymic senses out of total words assigned to output clusters.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9992542862892151}]}, {"text": "Recall is the ratio of words assigned to their actual WN meronymic senses' correct relations available in the test set.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9916002750396729}]}, {"text": "We have achieved 89% for precision, 86% for recall and 87, 47 % for the F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9998363256454468}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9997772574424744}, {"text": "F-measure", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9903280138969421}]}], "tableCaptions": [{"text": " Table 1: samples of the possible ranges of similarity values generated and their error rate", "labels": [], "entities": [{"text": "error rate", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9663845002651215}]}, {"text": " Table 2: the similarity values of selected noun pairs", "labels": [], "entities": []}]}