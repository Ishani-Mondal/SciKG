{"title": [{"text": "An Assessment of the Accuracy of Automatic Evaluation in Summarization", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9828231930732727}, {"text": "Summarization", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.8906337022781372}]}], "abstractContent": [{"text": "Automatic evaluation has greatly facilitated system development in summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9848883152008057}]}, {"text": "At the same time, the use of automatic evaluation has been viewed with mistrust by many, as its accuracy and correct application are not well understood.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9988497495651245}]}, {"text": "In this paper we provide an assessment of the automatic evaluations used for multi-document summarization of news.", "labels": [], "entities": [{"text": "multi-document summarization of news", "start_pos": 77, "end_pos": 113, "type": "TASK", "confidence": 0.7739348262548447}]}, {"text": "We outline our recommendations about how any evaluation, manual or automatic, should be used to find statistically significant differences between summarization systems.", "labels": [], "entities": [{"text": "summarization", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.9669895172119141}]}, {"text": "We identify the reference automatic evaluation metrics-ROUGE 1 and 2-that appear to best emulate human pyramid and responsiveness scores on four years of NIST evaluations.", "labels": [], "entities": []}, {"text": "We then demonstrate the accuracy of these metrics in reproducing human judgements about the relative content quality of pairs of systems and present an empirical assessment of the relationship between statistically significant differences between systems according to manual evaluations, and the difference according to automatic evaluations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9953881502151489}]}, {"text": "Finally, we present a case study of how new metrics should be compared to the reference evaluation, as we search for even more accurate automatic measures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic evaluation of content selection in summarization, particularly the ROUGE evaluation toolkit (, has been enthusiastically adopted by researchers since its introduction in 2003.", "labels": [], "entities": [{"text": "summarization", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9218073487281799}]}, {"text": "It is now standardly used to report results in publications; however we have a poor understanding of the accuracy of automatic evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9988872408866882}]}, {"text": "How often do we publish papers where we report an improvement according to automatic evaluation, but nevertheless, a standard manual evaluation would have led us to different conclusions?", "labels": [], "entities": []}, {"text": "In our work we directly address this question, and hope that our encouraging findings contribute to a better understanding of the strengths and shortcomings of automatic evaluation.", "labels": [], "entities": []}, {"text": "The aim of this paper is to give a better assessment of the automatic evaluation metrics for content selection standardly used in summarization research.", "labels": [], "entities": [{"text": "summarization research", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.9421722590923309}]}, {"text": "We perform our analyses on data from the 2008-2011 Text Analysis Conference (TAC) 1 organized by the National Institute of Standards and Technology (NIST).", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC) 1 organized by the National Institute of Standards and Technology (NIST)", "start_pos": 51, "end_pos": 154, "type": "TASK", "confidence": 0.721055943714945}]}, {"text": "We choose these datasets because in early evaluation initiatives, the protocol for manual evaluation changed from year to year in search of stable manual evaluation approaches.", "labels": [], "entities": []}, {"text": "Since 2008, however, the same evaluation protocol has been applied by NIST assessors and we consider it to be the model that automatic metrics need to emulate.", "labels": [], "entities": [{"text": "NIST", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.88579922914505}]}, {"text": "We start our discussion by briefly presenting the manual procedure for comparing systems (Section 2) and how these scores should be best used to identify significant differences between systems over a given test set (Section 3).", "labels": [], "entities": []}, {"text": "Then, we embark on our discussion of the accuracy of automatic evaluation and its ability to reproduce manual scoring.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9987947940826416}, {"text": "automatic evaluation", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.5476655960083008}]}, {"text": "To begin our analysis, we assess the accuracy of common variants of ROUGE on the TAC 2008-2011 datasets (Section 4.1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9994099140167236}, {"text": "ROUGE", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9536619782447815}, {"text": "TAC 2008-2011 datasets", "start_pos": 81, "end_pos": 103, "type": "DATASET", "confidence": 0.9692332943280538}]}, {"text": "There are two aspects of evaluation that we pay special attention to: Significant difference Ideally, all system comparisons should be performed using a test for sta-1 http://www.nist.gov/tac/ 1 tistical significance.", "labels": [], "entities": []}, {"text": "As both manual metrics and automatic metrics are noisy, a statistical hypothesis testis needed to estimate the probability that the differences observed are what would be expected if the systems are comparable in their performance.", "labels": [], "entities": []}, {"text": "When this probability is small (by convention 0.05 or less) we reject the null hypothesis that the systems' performance is comparable.", "labels": [], "entities": []}, {"text": "It is important to know if scoring a system via an automatic metric will lead to conclusions about the relative merits of two systems different from what one would have concluded on the basis of manual evaluation.", "labels": [], "entities": []}, {"text": "We report very encouraging results, showing that automatic metrics rarely contradict manual metrics, and some metrics never lead to contradictions.", "labels": [], "entities": []}, {"text": "For completeness, given that most papers do not report significance, we also compare the agreement between manual and automatic metrics without taking significance into account.", "labels": [], "entities": []}, {"text": "Type of comparison Established manual evaluations have two highly desirable properties: (1) they can tell apart good automatic systems from bad automatic systems and (2) they can differentiate automatic summaries from those produced by humans with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 253, "end_pos": 261, "type": "METRIC", "confidence": 0.9867852330207825}]}, {"text": "Obviously, choosing the better system in development cycles is key in eventually improving overall performance.", "labels": [], "entities": []}, {"text": "Being able to distinguish automatic from manual summaries is a general sanity test 2 that any evaluation adopted for wide use is expected to pass-it is useless to report system improvements when it appears that automatic methods are as good as human performance . As we will see, there is no single ROUGE variant that has both of these desirable properties.", "labels": [], "entities": []}, {"text": "Finally, in Section 5, we discuss ways to compare other automatic evaluation protocols with the refer-ence ROUGE metrics we have established.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.8546112179756165}]}, {"text": "We define standard tests for significance that would identify evaluations that are significantly more accurate than the current reference measures, thus warranting wider adoption for future system development and reporting of results.", "labels": [], "entities": []}, {"text": "As a case study we apply these to the TAC AESOP (Automatically Evaluating Summaries of Peers) task which called for the development of novel evaluation techniques that are more accurate than ROUGE evaluations.", "labels": [], "entities": [{"text": "TAC AESOP (Automatically Evaluating Summaries of Peers) task", "start_pos": 38, "end_pos": 98, "type": "TASK", "confidence": 0.7421727806329728}]}], "datasetContent": [{"text": "Before automatic evaluation methods are developed, it is necessary to establish a desirable manual evaluation which the automatic methods will need to reproduce.", "labels": [], "entities": []}, {"text": "The type of summarization task must also be precisely specified-single-or multi-document summarization, summarization of news, meetings, academic articles, etc.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8857737481594086}, {"text": "summarization of news, meetings, academic articles", "start_pos": 104, "end_pos": 154, "type": "TASK", "confidence": 0.8994404599070549}]}, {"text": "Saying that an automatic evaluation correlates highly with human judgement in general, is disturbingly incomplete, as the same automatic metric can predict some manual evaluation scores for some summarization tasks well, while giving poor correlation with other manual scores for certain tasks.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 195, "end_pos": 214, "type": "TASK", "confidence": 0.9064495861530304}]}, {"text": "In our work, we compare automatic metrics with the manual methods used at TAC: Pyramid and Responsiveness.", "labels": [], "entities": [{"text": "TAC", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.8024610877037048}]}, {"text": "These manual metrics primarily aim to assess if the content of the summary is appropriately chosen to include only important information.", "labels": [], "entities": []}, {"text": "They do not deal directly with the linguistic quality of the summary-how grammatical are the sentences or how well the information in the summary is organized.", "labels": [], "entities": []}, {"text": "Subsequently, in the experiments that we present in later sections, we do not address the assessment of automatic evaluations of linguistic quality, but instead analyze the performance of ROUGE and other related metrics that aim to score summary content.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 188, "end_pos": 193, "type": "METRIC", "confidence": 0.9450271129608154}]}, {"text": "The Pyramid evaluation () relies on multiple human-written gold-standard summaries for the input.", "labels": [], "entities": []}, {"text": "Annotators manually identify shared content across the gold-standards regardless of the specific phrasing used in each.", "labels": [], "entities": []}, {"text": "The pyramid score is based on the \"popularity\" of information in the gold-standards.", "labels": [], "entities": []}, {"text": "Information that is shared 2 across several human gold-standards is given higher weight when a summary is evaluated relative to the gold-standard.", "labels": [], "entities": []}, {"text": "Each evaluated summary is assigned a score which indicates what fraction of the most important information fora given summary size is expressed in the summary, where importance is determined by the overlap in content across the human gold-standards.", "labels": [], "entities": []}, {"text": "The Responsiveness metric is defined for queryfocused summarization, where the user's information need is clearly stated in a short paragraph.", "labels": [], "entities": [{"text": "summarization", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.7089290022850037}]}, {"text": "In this situation, the human assessors are presented with the user query and a summary, and are asked to assign a score that reflects to what extent the summary satisfies the user's information need.", "labels": [], "entities": []}, {"text": "There are no human gold-standards, and the linguistic quality of the summary is to some extent incorporated in the score, because information that is presented in a confusing manner may not be seen as relevant, while it could be interpreted by the assessor more easily in the presence of a human gold-standard.", "labels": [], "entities": []}, {"text": "Given that all standard automatic evaluation procedures compare a summary with a set of human gold-standards, it is reasonable to expect that they will be more accurate in reproducing results from Pyramid evaluation than results from Responsiveness judgements.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data in TAC 2008-2011 Summarization track.", "labels": [], "entities": [{"text": "TAC 2008-2011 Summarization track", "start_pos": 18, "end_pos": 51, "type": "DATASET", "confidence": 0.6959303691983223}]}, {"text": " Table 2: Average percentage agreement between ROUGE and manual metrics about significant differences on TAC  2008-2011 data. r1 = ROUGE-1, r2 = ROUGE-2, r4 = ROUGE-SU4, m = stemmed, s = stopwords removed; diff =  agreement on significant differences, no diff = agreement on lack of significant differences, contr = contradictions.", "labels": [], "entities": [{"text": "Average percentage agreement", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8552072842915853}, {"text": "TAC  2008-2011 data", "start_pos": 105, "end_pos": 124, "type": "DATASET", "confidence": 0.9491590658823649}]}]}