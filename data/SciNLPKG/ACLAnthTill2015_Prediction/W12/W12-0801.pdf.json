{"title": [{"text": "Preservation of Recognizability for Weighted Linear Extended Top-Down Tree Transducers *", "labels": [], "entities": []}], "abstractContent": [{"text": "An open question in [F \u00a8 UL\u00a8OPUL\u00a8 UL\u00a8OP, MALETTI, VOGLER: Weighted extended tree transducers.", "labels": [], "entities": [{"text": "F \u00a8 UL\u00a8OPUL\u00a8 UL\u00a8OP", "start_pos": 21, "end_pos": 39, "type": "METRIC", "confidence": 0.9388687014579773}, {"text": "MALETTI", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.7187036871910095}, {"text": "VOGLER", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.7557540535926819}]}, {"text": "Fundamenta Informaticae 111(2), 2011] asks whether weighted linear extended tree transducers preserve recogniz-ability in countably complete commuta-tive semirings.", "labels": [], "entities": []}, {"text": "In this contribution, the question is answered positively, which is achieved with a construction that utilizes inside weights.", "labels": [], "entities": []}, {"text": "Due to the completeness of the semiring, the inside weights always exist, but the construction is only effective if they can be effectively determined.", "labels": [], "entities": []}, {"text": "It is demonstrated how to achieve this in a number of important cases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntax-based statistical machine translation created renewed interest in tree automata and tree transducer theory).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6383532087008158}, {"text": "tree transducer theory", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.6479251682758331}]}, {"text": "In particular, it sparked research on extended top-down tree transducers (, which are top-down tree transducers in which the left-hand sides can contain several (or no) input symbols.", "labels": [], "entities": []}, {"text": "A recent contribution by investigates the theoretical properties of weighted extended tree transducers over countably complete and commutative semirings.", "labels": [], "entities": []}, {"text": "Such semirings permit sums of countably many summands, which still obey the usual associativity, commutativity, and distributivity laws.", "labels": [], "entities": []}, {"text": "We will use the same class of semirings.", "labels": [], "entities": []}, {"text": "* All authors were financially supported by the EMMY NOETHER project MA / 4959 / 1-1 of the German Research Foundation (DFG).", "labels": [], "entities": [{"text": "EMMY", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.8700170516967773}, {"text": "NOETHER project MA / 4959 / 1-1", "start_pos": 53, "end_pos": 84, "type": "DATASET", "confidence": 0.6187644047396523}, {"text": "German Research Foundation (DFG)", "start_pos": 92, "end_pos": 124, "type": "DATASET", "confidence": 0.8897354304790497}]}, {"text": "Input \u2192 Parser \u2192 TM \u2192 LM \u2192 Output Extended top-down tree transducers are used as translation models (TM) in syntax-based machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.6874508708715439}]}, {"text": "In the standard pipeline (see; LM is short for language model) the translation model is applied to the parses of the input sentence, which can be represented as a recognizable weighted forest.", "labels": [], "entities": []}, {"text": "In practice, only the best or the n-best parses are used, but in principle, we can use the recognizable weighted forest of all parses.", "labels": [], "entities": []}, {"text": "In either case, the translation model transforms the input trees into a weighted forest of translated output trees.", "labels": [], "entities": []}, {"text": "A class of transducers preserves recognizability if for every transducer of the class and each recognizable weighted forest, this weighted forest of translated output trees is again recognizable.", "labels": [], "entities": []}, {"text": "investigates which extended top-down tree transducers preserve recognizability under forward (i.e., the setting previously described) and backward application (i.e., the setting, in which we start with the output trees and apply the inverse of the translation model), but the question remained open for forward application of weighted linear extended top-down tree transducers [see for an overview of the existing results for forward application due to: Overview of the known results due to and our results in boxes.", "labels": [], "entities": []}, {"text": "weighted tree language \u03d5 such that M(\u03d5) [forward application] is not recognizable?", "labels": [], "entities": []}, {"text": "Or even harder, are there Sand M with the same properties such that M( 1) [ 1 is the weighted forest in which each tree has weight 1] is not recognizable?\"", "labels": [], "entities": []}, {"text": "In this contribution, we thus investigate preservation of recognizability (under forward application) for linear extended top-down tree transducers with regular look-ahead, which are equivalent to linear weighted extended tree transducers by.", "labels": [], "entities": []}, {"text": "We show that they always preserve recognizability, thus confirming the implicit hypothesis of.", "labels": [], "entities": []}, {"text": "The essential tool for our construction is the inside weight of the states of the weighted tree grammar representing the parses.", "labels": [], "entities": []}, {"text": "The inside weight of a state q is the sum of all weights of trees accepted in this state.", "labels": [], "entities": []}, {"text": "In our main construction (see Section 5) we first compose the input weighted tree grammar with the transducer (input restriction).", "labels": [], "entities": []}, {"text": "This is particularly simple since we just abuse the look-ahead of the initial rules.", "labels": [], "entities": []}, {"text": "Ina second step, we normalize the obtained transducer, which yields the standard product construction typically used for input restriction.", "labels": [], "entities": [{"text": "input restriction", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.7394106984138489}]}, {"text": "Finally, we project to the output by basically eliminating the left-hand sides.", "labels": [], "entities": []}, {"text": "In this step, the inside weights of states belonging to deleted subtrees are multiplied to the production weight.", "labels": [], "entities": []}, {"text": "Due to the completeness of the semiring, the inside weights always exist, but the infinite sums have to be computed effectively for the final step of the construction to be effective.", "labels": [], "entities": []}, {"text": "This problem is addressed in Section 6, where we show several methods to effectively compute or approximate the inside weights for all states of a weighted tree grammar.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}