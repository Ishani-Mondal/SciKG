{"title": [{"text": "An Ensemble Model of Word-based and Character-based Models for Japanese and Chinese Input Method", "labels": [], "entities": []}], "abstractContent": [{"text": "Since Japanese and Chinese languages have too many characters to be input directly using a standard keyboard, input methods for these languages that enable users to input the characters are required.", "labels": [], "entities": []}, {"text": "Recently, input methods based on statistical models have become popular because of their accuracy and ease of maintenance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9991156458854675}]}, {"text": "Most of them adopt word-based models because they utilize word-segmented corpora to train the models.", "labels": [], "entities": []}, {"text": "However, such word-based models suffer from unknown words because they cannot convert words correctly which are not in corpora.", "labels": [], "entities": []}, {"text": "To handle this problem, we propose a character-based model that enables input methods to convert unknown words by exploiting character-aligned corpora automatically generated by a monotonic alignment tool.", "labels": [], "entities": []}, {"text": "In addition to the character-based model, we propose an ensemble model of both character-based and word-based models to achieve higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9962559938430786}]}, {"text": "The ensemble model combines these two models by linear interpolation.", "labels": [], "entities": []}, {"text": "All of these models are based on joint source channel model to utilize rich context through higher order joint n-gram.", "labels": [], "entities": []}, {"text": "Experiments on Japanese and Chinese datasets showed that the character-based model performs reasonably and the ensemble model outperforms the word-based baseline model.", "labels": [], "entities": []}, {"text": "As a future work, the effectiveness of incorporating large raw data should be investigated.", "labels": [], "entities": []}], "introductionContent": [{"text": "There are more than 6,000 basic Kanji characters and 50 Hiragana/Katakana characters in Japanese language.", "labels": [], "entities": []}, {"text": "A Kanji character represents one meaning, while Hiragana/Katakana characters represent their sounds.", "labels": [], "entities": []}, {"text": "Therefore, it is difficult to input all kind of Japanese texts into computers or mobile phones by a standard keyboard which has only 100 keys.", "labels": [], "entities": []}, {"text": "In order to input Japanese texts, it is common to use input methods called Kana-Kanji conversion, which convert Hiragana characters into Kanji or mixed characters.", "labels": [], "entities": [{"text": "Kana-Kanji conversion", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.761905163526535}]}, {"text": "Since there are no spaces between words, most of Japanese input methods process texts sentence by sentence.", "labels": [], "entities": []}, {"text": "Chinese language has nearly the same problem.", "labels": [], "entities": []}, {"text": "There are more than 10,000 Hanzi characters in Chinese and Pinyin input methods are used to convert Roman characters into Chinese characters.", "labels": [], "entities": []}, {"text": "In these days, statistical models are used for such input methods to achieve high accuracy and automate parameter tuning ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.995648205280304}]}, {"text": "The statistical models are trained before actual conversion from corpora in each language.", "labels": [], "entities": []}, {"text": "Sentences in these corpora are segmented word byword and annotated to specify the words' pronunciation, whether manually or automatically.", "labels": [], "entities": []}, {"text": "Most of the models treat a word as anatomic unit; that means, they distinguish all words completely even if they share some characters in their strings.", "labels": [], "entities": []}, {"text": "In this paper, we call such approaches word-based models.", "labels": [], "entities": []}, {"text": "However, such word-based models suffer from unknown words in principle, because they cannot convert or even enumerate a word in the candidate list when the word is not contained in the corpora.", "labels": [], "entities": []}, {"text": "Instead of word-based models, we propose anew character-based model for input methods to avoid such a problem.", "labels": [], "entities": []}, {"text": "In addition, we also propose an ensemble model which is a combination of both word-based and character-based models to achieve higher accuracy and take advantages of both models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9984350800514221}]}, {"text": "The rest of this paper is organized as follows: section 2 introduces related work, section 3 proposes the models, section 4 describes experimental results, and section 5 summarizes this paper and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To confirm the effectiveness and properties of our models, we conducted experiments on Japanese and Chinese corpora in various situations.", "labels": [], "entities": []}, {"text": "We divided each corpus into 90% of training data and 10% of test data, trained our models and evaluated on test data.", "labels": [], "entities": []}, {"text": "The models are evaluated by comparing system output and gold standard; system output is produced by a decoder which is an implementation of Viterbi algorithm for higher order n-gram models 3 .  We adopt evaluation metrics based on the longest common sequence (LCS) between system output and gold standard following) and.", "labels": [], "entities": [{"text": "longest common sequence (LCS)", "start_pos": 235, "end_pos": 264, "type": "METRIC", "confidence": 0.7322617173194885}]}, {"text": "Here, N LC S is the length of the LCS, N SY S is the length of the system output sequence, N DAT is the length of gold standard.", "labels": [], "entities": [{"text": "DAT", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.7766556739807129}]}, {"text": "Note that LCS is not necessarily a continuous string contained in both string; that means, LCS can be concatenation of separated strings which are contained in both strings in order.", "labels": [], "entities": []}, {"text": "CER (Character Error Rate) and ACC (Sentence Accuracy) are also shown for convenience, but F-score is used as our main metric.", "labels": [], "entities": [{"text": "CER (Character Error Rate)", "start_pos": 0, "end_pos": 26, "type": "METRIC", "confidence": 0.7551742245753607}, {"text": "ACC (Sentence Accuracy)", "start_pos": 31, "end_pos": 54, "type": "METRIC", "confidence": 0.9249661922454834}, {"text": "F-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9933852553367615}]}, {"text": "show the summary of our experiments to compare three models; word-based, character-based and ensemble models.", "labels": [], "entities": []}, {"text": "The value of n is chosen to perform the best in terms of F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9963815212249756}]}], "tableCaptions": [{"text": " Table 2: Result for Japanese", "labels": [], "entities": []}, {"text": " Table 3: Result for Chinese with tone", "labels": [], "entities": []}, {"text": " Table 4: Result for Chinese without tone", "labels": [], "entities": [{"text": "Result", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.8635296821594238}]}, {"text": " Table 5: Cross Domain Analysis", "labels": [], "entities": [{"text": "Cross Domain Analysis", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7667725682258606}]}]}