{"title": [], "abstractContent": [{"text": "Fast retraining of word segmentation models is required for adapting to new resources or domains in NLP of many Asian languages without word delimiters.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.6991035789251328}]}, {"text": "The traditional tokenization model is efficient but inaccurate.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9741654396057129}]}, {"text": "This paper proposes a phrase-based model that factors sentence tokenization into phrase tokenizations, the dependencies of which are also taken into account.", "labels": [], "entities": []}, {"text": "The model has a good OOV recognition ability, which improves the overall performance significantly.", "labels": [], "entities": [{"text": "OOV recognition", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7547836303710938}]}, {"text": "The training is a linear time phrase extraction and MLE procedure, while the decoding is via dynamic programming based algorithms.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7048515975475311}, {"text": "MLE", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8352838754653931}]}], "introductionContent": [{"text": "In many Asian languages, including Chinese, a sentence is written as a character sequence without word delimiters, thus word segmentation remains a key research topic in language processing for these languages.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 120, "end_pos": 137, "type": "TASK", "confidence": 0.7041611075401306}]}, {"text": "Although many reports from evaluation tasks present quite positive results, a fundamental problem for real word applications is that most systems heavily depend on the data they were trained on.", "labels": [], "entities": []}, {"text": "In order to utilize increasingly available language resources such as user contributed annotations and web lexicon and/or to dynamically construct models for new domains, we have to either frequently re-build models or rely on techniques such as incremental learning and transfer learning, which are unsolved problems themselves.", "labels": [], "entities": []}, {"text": "In the case of frequent model re-building, the most efficient approach is the tokenization model (using the terminology in, in which the re-training is just the update of the dictionary and the segmentation is a greedy string matching procedure using the dictionary and some disambiguation heuristics, e.g. and.", "labels": [], "entities": []}, {"text": "An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as and, which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.", "labels": [], "entities": []}, {"text": "However, all the methods mentioned above are mostly based on the knowledge of in-vocabulary words and usually suffer from poor performance, as the out-of-vocabulary words (OOV) rather than segmentation ambiguities turnout to the dominant error source for word segmentation on real corpora.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 255, "end_pos": 272, "type": "TASK", "confidence": 0.7212093621492386}]}, {"text": "This fact has led to a shift of the research focus to modeling the roles of individual characters in the word formation process to tackle the OOV problem.", "labels": [], "entities": [{"text": "word formation process", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.8357662558555603}, {"text": "OOV problem", "start_pos": 142, "end_pos": 153, "type": "TASK", "confidence": 0.9092432856559753}]}, {"text": "proposes a character classification model, which classifies characters according to their positions in a word using the maximum entropy classifier. has further extended this model to its sequential form, i.e. sequence labeling, by adopting linear-chain conditional random fields (CRFs,).", "labels": [], "entities": [{"text": "character classification", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.7220810651779175}, {"text": "sequence labeling", "start_pos": 209, "end_pos": 226, "type": "TASK", "confidence": 0.6892593801021576}]}, {"text": "As it is capable of capturing the morphological behaviors of characters, the character classification model has significantly better performance in OOV recognition and overall segmentation accuracy, and has been the state-of-art since its introduction, suggested by the leading performances of systems based on it in recent international Chinese word segmentation bakeoffs).", "labels": [], "entities": [{"text": "character classification", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.7037973999977112}, {"text": "OOV recognition", "start_pos": 148, "end_pos": 163, "type": "TASK", "confidence": 0.8533867299556732}, {"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.914979100227356}, {"text": "word segmentation bakeoffs", "start_pos": 346, "end_pos": 372, "type": "TASK", "confidence": 0.7798375884691874}]}, {"text": "The tokenization model has advantages in simplicity and efficiency, as the basic operation in segmentation is string matching with linear time complexity to the sentence length and it only needs a dictionary thus requires no training as in the character classification model, which can easily have millions of features and require hundreds of iterations in the training phase.", "labels": [], "entities": [{"text": "character classification", "start_pos": 244, "end_pos": 268, "type": "TASK", "confidence": 0.7769794464111328}]}, {"text": "On the other hand, it has inferior performance, caused by its poor OOV induction ability.", "labels": [], "entities": [{"text": "OOV induction", "start_pos": 67, "end_pos": 80, "type": "METRIC", "confidence": 0.8451102674007416}]}, {"text": "This work proposes a framework called phrasebased tokenization as a generalization of the tokenization model to cope with its deficiencies in OOV recognition, while preserving its advantages of simplicity and efficiency, which are important for adaptive word segmentation.", "labels": [], "entities": [{"text": "OOV recognition", "start_pos": 142, "end_pos": 157, "type": "TASK", "confidence": 0.8763086795806885}, {"text": "adaptive word segmentation", "start_pos": 245, "end_pos": 271, "type": "TASK", "confidence": 0.6275616387526194}]}, {"text": "The segmentation hypothesis unit is extended from a word to a phrase, which is a character string of arbitrary length, i.e. combinations of partial and/or complete words.", "labels": [], "entities": []}, {"text": "And the statistics of different tokenizations of the same phrase are collected and used for parameters estimation, which leads to a linear time model construction procedure.", "labels": [], "entities": [{"text": "parameters estimation", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.6445993036031723}]}, {"text": "This extension makes hypothesis units capable of capturing richer context and describing morphological behavior of characters, which improves OOV recognition.", "labels": [], "entities": [{"text": "OOV recognition", "start_pos": 142, "end_pos": 157, "type": "TASK", "confidence": 0.9357500076293945}]}, {"text": "Moreover, overlapping hypothesis units can be combined once certain consistency conditions are satisfied, which avoids the unrealistic assumption of independence among the tokenizations of neighboring phrases.", "labels": [], "entities": []}, {"text": "Phrase-based tokenization decomposes the sentence tokenization into phrase tokenizations.", "labels": [], "entities": []}, {"text": "We use a graph called phrase tokenization lattice to represent all the hypotheses of phrase tokenization in a given sentence.", "labels": [], "entities": [{"text": "phrase tokenization", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.6804090291261673}]}, {"text": "Under such a formulation, tokenizing a sentence is transformed to the shortest path search problem on the graph, which can be efficiently solved by dynamic programming techniques similar to the algorithm.", "labels": [], "entities": [{"text": "tokenizing a sentence", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.8821407755215963}]}], "datasetContent": [{"text": "We use the training and testing sets from the second international Chinese word segmentation bakeoff, which are freely available and most widely used in evaluations.", "labels": [], "entities": []}, {"text": "There are two corpora in simplified Chinese provided by Peking University (PKU) and Microsoft Research (MSR) and two corpora in traditional Chinese provided by Academic Sinica (AS) and the City University of Hong Kong (CityU).", "labels": [], "entities": []}, {"text": "The experiments are conducted in a closed-test manner, in which no extra recourse other than the training corpora is used.", "labels": [], "entities": []}, {"text": "We use the same criteria and the official script for evaluation from the bakeoff, which measure the overall segmentation performance in terms of F-scores, and the OOV recognition capacity in terms of Roov.", "labels": [], "entities": [{"text": "bakeoff", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.8716222047805786}, {"text": "F-scores", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9870026111602783}, {"text": "OOV recognition", "start_pos": 163, "end_pos": 178, "type": "TASK", "confidence": 0.7171937823295593}, {"text": "Roov", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.9832202196121216}]}, {"text": "Precision is defined as the number of correctly segmented words divided by the total number of words in the segmentation result, where the correctness of the segmented words is determined by matching the segmentation with the gold standard test set.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9859765768051147}, {"text": "gold standard test set", "start_pos": 226, "end_pos": 248, "type": "DATASET", "confidence": 0.7805913090705872}]}, {"text": "Recall is defined as the number of correctly segmented words divided by the total number of words in the gold standard test set.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9629296660423279}, {"text": "gold standard test set", "start_pos": 105, "end_pos": 127, "type": "DATASET", "confidence": 0.7727079689502716}]}, {"text": "The evenly-weighted F-score is calculated by: Roov is the recall of all the OOV words.", "labels": [], "entities": [{"text": "F-score", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9824618697166443}, {"text": "Roov", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9978764057159424}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9990628361701965}]}, {"text": "And Riv is the recall of words that have occurred in the training corpus.", "labels": [], "entities": [{"text": "Riv", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8984919786453247}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9778463244438171}]}, {"text": "The evaluation in this experiment is done automatically using the script provided with the second bakeoffs data.", "labels": [], "entities": []}, {"text": "We have implemented both Algorithm 1 and Algorithm 2 in Python with some simplifications, e.g. only processing phrase up to the length of 10 characters, ignoring several important details such as pruning.", "labels": [], "entities": []}, {"text": "The performances are compared with the baseline algorithm maximum matching (MM), described in, and the best bakeoff results.", "labels": [], "entities": [{"text": "algorithm maximum matching (MM)", "start_pos": 48, "end_pos": 79, "type": "METRIC", "confidence": 0.7401214341322581}]}, {"text": "The F-score, Roov and Riv are summarized in, respectively.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9942386150360107}, {"text": "Roov", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.8131228089332581}]}, {"text": "All the algorithms have quite similar recall for the in-vocabulary words (Riv), but their Roov vary greatly, which leads to the differences in F-score.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9965662360191345}, {"text": "Roov", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.983359694480896}, {"text": "F-score", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.9947413206100464}]}, {"text": "In general both Algorithm 1 and Algorithm 2 improves OOV Recall significantly, compared with the baseline algorithm, maximum matching, which has barely any OOV recognition capacity.", "labels": [], "entities": [{"text": "OOV Recall", "start_pos": 53, "end_pos": 63, "type": "TASK", "confidence": 0.6132295578718185}, {"text": "OOV recognition", "start_pos": 156, "end_pos": 171, "type": "TASK", "confidence": 0.7326518595218658}]}, {"text": "This confirms the effectiveness of the proposed phrase-based model in modeling morphological behaviors of characters.", "labels": [], "entities": []}, {"text": "Moreover, Algorithm 2 works consistently better than Algorithm 1, which suggests the usefulness of its strategy of dealing with dependencies among phrase tokenizations.", "labels": [], "entities": []}, {"text": "Besides, the proposed method has the linear training and testing (when setting a maximum phrase length) time complexity, while the training complexity of CRF is the proportional to the feature numbers, which are often over millions.", "labels": [], "entities": []}, {"text": "Even with current prototype, our method takes only minutes to build the model, in contrast with several hours that CRF segmenter needs to train the model for the same corpus on the same machine.", "labels": [], "entities": [{"text": "CRF segmenter", "start_pos": 115, "end_pos": 128, "type": "TASK", "confidence": 0.5855914950370789}]}, {"text": "Admittedly, our model still underperforms the best systems in the bakeoff.", "labels": [], "entities": []}, {"text": "This maybe resulted from that 1) our system is still a prototype that ignores many minor issues and lack optimization and 2) as a generative model, our model may suffer more from the data sparseness problem, compared with discriminative models, such as CRF.", "labels": [], "entities": []}, {"text": "As mentioned earlier, the OOV recognition is the dominant factor that influences the overall accuracy.", "labels": [], "entities": [{"text": "OOV recognition", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.6892242133617401}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9984548091888428}]}, {"text": "Different from the mechanism of tokenization combination in our approach, state-ofart systems such as those based on MaxEnt or CRF, achieve OOV recognition basically in the same way as in-dictionary word recognition.", "labels": [], "entities": [{"text": "OOV recognition", "start_pos": 140, "end_pos": 155, "type": "TASK", "confidence": 0.8034767806529999}, {"text": "word recognition", "start_pos": 199, "end_pos": 215, "type": "TASK", "confidence": 0.7127291709184647}]}, {"text": "The segmentation is modeled as assigning labels to characters.", "labels": [], "entities": []}, {"text": "And the probability of the label assignment fora character token is mostly determined by its features, which are usually local contexts in the form of character co-occurrences.", "labels": [], "entities": []}, {"text": "There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.", "labels": [], "entities": [{"text": "OOV recognition", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.9078090786933899}]}, {"text": "For example, the Sproat et al.", "labels": [], "entities": [{"text": "Sproat et al.", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.9100692719221115}]}, {"text": "(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.", "labels": [], "entities": []}, {"text": "Another typical example is, which proposed context free grammar like rules together with a recursive bottom-up merge algorithm that merges possible morphemes after an initial segmentation using maximum matching.", "labels": [], "entities": []}, {"text": "It would be fairer to compare the OOV recognition performance of our approach with these methods, rather than maximum matching.", "labels": [], "entities": [{"text": "OOV recognition", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.8003357946872711}]}, {"text": "But most earlier works are not evaluated on standard bake-off corpora and the implementations are not openly available, so it is difficult to make direct comparisons.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The F-score over the bakeoff-2 data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9985617995262146}, {"text": "bakeoff-2 data", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.7378887832164764}]}, {"text": " Table 2. The Roov over the bakeoff-2 data.", "labels": [], "entities": [{"text": "bakeoff-2 data", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.8310103118419647}]}, {"text": " Table 3. The Riv over the bakeoff-2 data.", "labels": [], "entities": [{"text": "Riv", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.5861124992370605}, {"text": "bakeoff-2 data", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.8446030616760254}]}]}