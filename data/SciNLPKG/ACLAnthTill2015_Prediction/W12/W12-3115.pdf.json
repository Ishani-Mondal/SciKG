{"title": [{"text": "The UPC Submission to the WMT 2012 Shared Task on Quality Estimation", "labels": [], "entities": [{"text": "WMT 2012 Shared Task on Quality Estimation", "start_pos": 26, "end_pos": 68, "type": "TASK", "confidence": 0.645976219858442}]}], "abstractContent": [{"text": "In this paper, we describe the UPC system that participated in the WMT 2012 shared task on Quality Estimation for Machine Translation.", "labels": [], "entities": [{"text": "WMT 2012 shared task on Quality Estimation", "start_pos": 67, "end_pos": 109, "type": "TASK", "confidence": 0.580712731395449}, {"text": "Machine Translation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7128102630376816}]}, {"text": "Based on the empirical evidence that fluency-related features have a very high correlation with post-editing effort, we present a set of features for the assessment of quality estimation for machine translation designed around different kinds of n-gram language models, plus another set of features that model the quality of dependency parses automatically projected from source sentences to translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 191, "end_pos": 210, "type": "TASK", "confidence": 0.7654894590377808}, {"text": "dependency parses automatically projected from source sentences to translations", "start_pos": 325, "end_pos": 404, "type": "TASK", "confidence": 0.8266444537374709}]}, {"text": "We document the results obtained on the shared task dataset, obtained by combining the features that we designed with the baseline features provided by the task organizers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality Estimation (QE) for Machine Translations (MT) is the task concerned with the prediction of the quality of automatic translations in the absence of reference translations.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6457869589328766}, {"text": "Machine Translations (MT)", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.8619617700576783}]}, {"text": "The WMT 2012 shared task on QE for MT) required participants to score and rank a set of automatic English to Spanish translations output by a stateof-the-art phrase based machine translation system.", "labels": [], "entities": [{"text": "WMT 2012 shared task", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7035627216100693}, {"text": "MT)", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.6232579052448273}]}, {"text": "Task organizers provided a training dataset of 1, 832 source sentences, together with reference, automatic and post-edited translations, as well as human quality assessments for the automatic translations.", "labels": [], "entities": []}, {"text": "Postediting effort, i.e., the amount of editing required to produce an accurate translation, was selected as the quality criterion, with assessments ranging from 1 (extremely bad) to 5 (good as it is).", "labels": [], "entities": []}, {"text": "The organizers also provided a set of linguistic resources and processors to extract 17 global indicators of translation quality (baseline features) that participants could decide to employ for their models.", "labels": [], "entities": []}, {"text": "For the evaluation, these features are used to learn a baseline predictors for participants to compare against.", "labels": [], "entities": []}, {"text": "Systems participating in the evaluation are scored based on their ability to correctly rank the 422 test translations (using DeltaAvg and Spearman correlation) and/or to predict the human quality assessment for each translation (using Mean Average Error -MAE and Root Mean Squared Error -RMSE).", "labels": [], "entities": [{"text": "Mean Average Error -MAE", "start_pos": 235, "end_pos": 258, "type": "METRIC", "confidence": 0.9656481981277466}, {"text": "Root Mean Squared Error -RMSE", "start_pos": 263, "end_pos": 292, "type": "METRIC", "confidence": 0.8086582521597544}]}, {"text": "Our initial approach to the task consisted of several experiments in which we tried to identify common translation errors and correlate them with quality assessments.", "labels": [], "entities": []}, {"text": "However, we soon realized that simple regression models estimated on the baseline features resulted in more consistent predictors of translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.9549960494041443}]}, {"text": "For this reason, we eventually decided to focus on the design of a set of global indicators of translation quality to be combined with the strong features already computed by the baseline system.", "labels": [], "entities": [{"text": "translation", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.9537845849990845}]}, {"text": "An analysis of the Pearson correlation of the baseline features 1 with human quality assessments shows that the two strongest individual predictors of post-editing effort are the n-gram language model perplexities estimated on source and target sentences.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 19, "end_pos": 38, "type": "METRIC", "confidence": 0.9670943915843964}]}, {"text": "This evidence suggests that a reasonable approach to im- prove the accuracy of the baseline would be to concentrate on the estimation of other n-gram language models, possibly working at different levels of linguistic analysis and combining information coming from the source and the target sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9983429908752441}]}, {"text": "On top of that, we add another class of features that capture the quality of grammatical dependencies projected from source to target via automatic alignments, as they could provide clues about translation quality that may not be captured by sequential models.", "labels": [], "entities": []}, {"text": "The novel features that we incorporate are described in full detail in the next section; in Section 3 we describe the experimental setup and the resources that we employ, while in Section 4 we present the results of the evaluation; finally, in Section 5 we draw our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To extract the extended feature set we use an alignment model, a POS tagger and a dependency parser.", "labels": [], "entities": []}, {"text": "Concerning the former, we trained an unsupervised model with the Berkeley aligner , an implementation of the symmetric word-alignment model described by  SRILM toolkit , with order equal to 3 and KneserNey () smoothing.", "labels": [], "entities": []}, {"text": "As a learning framework we resort to Support Vector Regression (SVR) () and learn a linear separator using the SVMLight optimizer by . We represent feature values by means of their z-scores, i.e., the number of standard deviations that separate a value from the average of the feature distribution.", "labels": [], "entities": []}, {"text": "We carryout the system development via 5-fold cross evaluation on the 1,832 development sentences for which we have quality assessments.", "labels": [], "entities": []}, {"text": "In we show the absolute value of the Pearson correlation of the features used in our model, i.e., the 17 baseline features (BL/*), the 21 sequence (SEQ/*) and the 18 dependency (DEP/*) features, with the human quality assessments.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 37, "end_pos": 56, "type": "METRIC", "confidence": 0.9610493183135986}, {"text": "BL", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.9306046366691589}]}, {"text": "The more correlated features are in the top (left) part of the table.", "labels": [], "entities": []}, {"text": "At a first glance, we can see that 9 of the 10 features having highest correlation are already encoded by the baseline.", "labels": [], "entities": [{"text": "correlation", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.9671244025230408}]}, {"text": "We can also observe that DEP features show a higher correlation than SEQ features.", "labels": [], "entities": []}, {"text": "This evidence seems to contradict our initial expectations, but it can be easily ascribed to the limited size of the corpus used to estimate the ngram models (150K sentences).", "labels": [], "entities": []}, {"text": "This point is also confirmed by the fact that the three variants of the *PStop model (based on sequences of target stopwords interleaved by POS tags projected from the source sentence and, hence, on a very small vocabulary) are the three sequential models sporting the highest correlation.", "labels": [], "entities": []}, {"text": "Alas, the lack of lexical anchors makes them less useful as predictors of translation quality than BL/4 and BL/5.", "labels": [], "entities": []}, {"text": "Another interesting as-  pect is that DEP/C \u2212 features show higher correlation than DEP/C + . This is an expected behaviour, as being indicators of possible errors they are intended to have discriminative power with respect to the human assessments.", "labels": [], "entities": []}, {"text": "Finally, we can see that more than 50% of the included features, including five baseline features, have negligible (less than 0.1) correlation with the assessments.", "labels": [], "entities": []}, {"text": "Even though these features may not have predictive power per se, their combination maybe useful to learn more accurate models of quality.", "labels": [], "entities": []}, {"text": "9 shows a comparison of the baseline features against the extended feature set as the average DeltaAvg score and Mean Absolute Error (MAE) on the 10 most accurate development configurations.", "labels": [], "entities": [{"text": "DeltaAvg score", "start_pos": 94, "end_pos": 108, "type": "METRIC", "confidence": 0.8739621639251709}, {"text": "Mean Absolute Error (MAE)", "start_pos": 113, "end_pos": 138, "type": "METRIC", "confidence": 0.9405824542045593}]}, {"text": "In both cases, the extended feature set results in slightly more accurate models, even though the improvement is hardly significant.", "labels": [], "entities": []}, {"text": "shows the results of the official evaluation.", "labels": [], "entities": []}, {"text": "Our submission to the final evaluation (Official) was plagued by a bug that affected the values of all the baseline features on the test set.", "labels": [], "entities": []}, {"text": "As a consequence, the official performance of the model is extremely poor.", "labels": [], "entities": []}, {"text": "The row labeled Amended shows the results that we obtained after correcting the problem.", "labels": [], "entities": [{"text": "Amended", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9614656567573547}]}, {"text": "As we can see, on both tasks the baseline outperforms our model, even though the difference between the two is only marginal.", "labels": [], "entities": []}, {"text": "Ranking-wise, our official submission is last on the ranking task and last-but-one on the quality prediction task.", "labels": [], "entities": [{"text": "quality prediction task", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.7341790298620859}]}, {"text": "In contrast, the amended model shows very similar accuracy to the baseline, as the majority of the systems that took part in the evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9996023774147034}]}], "tableCaptions": [{"text": " Table 1: Pearson correlation (in absolute value) of the  baseline (BL) features and the extended feature set (SEQ  and DEP) with the quality assessments.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8841525614261627}]}, {"text": " Table 3: Official and amended evaluation on test data of  the extended feature sets.", "labels": [], "entities": []}]}