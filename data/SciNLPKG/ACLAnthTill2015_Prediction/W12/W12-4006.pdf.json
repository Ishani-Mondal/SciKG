{"title": [{"text": "Extracting Context-Rich Entailment Rules from Wikipedia Revision History", "labels": [], "entities": [{"text": "Extracting Context-Rich Entailment", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.9139954249064127}, {"text": "Wikipedia Revision", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.8691922426223755}]}], "abstractContent": [{"text": "Recent work on Textual Entailment has shown a crucial role of knowledge to support entail-ment inferences.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.766384482383728}, {"text": "entail-ment inferences", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.7637801170349121}]}, {"text": "However, it has also been demonstrated that currently available entail-ment rules are still far from being optimal.", "labels": [], "entities": []}, {"text": "We propose a methodology for the automatic acquisition of large scale context-rich entailment rules from Wikipedia revisions, taking advantage of the syntactic structure of entailment pairs to define the more appropriate linguistic constraints for the rule to be successfully applicable.", "labels": [], "entities": [{"text": "automatic acquisition of large scale context-rich entailment rules from Wikipedia revisions", "start_pos": 33, "end_pos": 124, "type": "TASK", "confidence": 0.6649857488545504}]}, {"text": "We report on rule acquisition experiments on Wikipedia, showing that it enables the creation of an innovative (i.e. acquired rules are not present in other available resources) and good quality rule repository.", "labels": [], "entities": [{"text": "rule acquisition", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7912489771842957}]}], "introductionContent": [{"text": "Entailment rules have been introduced to provide pieces of knowledge that may support entailment judgments ( ) with some degree of confidence.", "labels": [], "entities": []}, {"text": "More specifically, an entailment rule is defined () as a directional relation between two sides of a pattern, corresponding to text fragments with variables (typically phrases or parse sub-trees).", "labels": [], "entities": []}, {"text": "The left-hand side (LHS) of the pattern entails the right-hand side (RHS) of the same pattern under the same variable instantiation.", "labels": [], "entities": [{"text": "RHS", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9668858647346497}]}, {"text": "Given the Text-Hypothesis pair (T-H) in Example 1: brings to a TE system (aimed at recognizing that a particular target meaning can be inferred from different text variants in several NLP application, e.g. Question Answering or Information Extraction) the knowledge that the word hospital in Text can be aligned, or transformed, into the word medical institution in the Hypothesis, with a probability 0.8 that this operation preserves the entailment relation among T and H.", "labels": [], "entities": [{"text": "Question Answering or Information Extraction", "start_pos": 206, "end_pos": 250, "type": "TASK", "confidence": 0.8105207681655884}]}, {"text": "Similar considerations apply for more complex rules involving verbs, as: 2) LHS: X establish Y \u21d2 RHS: X create Y probability: 0.8 where the variables maybe instantiated by any textual element with a specified syntactic relation with the verb.", "labels": [], "entities": [{"text": "RHS: X create Y probability", "start_pos": 97, "end_pos": 124, "type": "METRIC", "confidence": 0.6311274270216624}]}, {"text": "Both kinds of rules are typically acquired either from structured sources (e.g. WordNet), or from unstructured sources according for instance to distributional properties (e.g. DIRT ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.9451964497566223}]}, {"text": "Entailment rules should typically be applied only in specific contexts, defined in () as relevant contexts.", "labels": [], "entities": []}, {"text": "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (,), but most do not.", "labels": [], "entities": [{"text": "paraphrase and entailment acquisition", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.7205642610788345}]}, {"text": "Because of alack of an adequate representation of the linguistic context in which the rules can be successfully applied, their concrete use reflects this limitation.", "labels": [], "entities": []}, {"text": "For instance, rule 2 (extracted from DIRT) fails if applied to \"The mathematician established the validity of the conjecture\", where the sense of establish is not a synonym of create (but of prove, demonstrate), decreasing system's precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 232, "end_pos": 241, "type": "METRIC", "confidence": 0.9972943663597107}]}, {"text": "Moreover, these rules often suffer from lack of directionality, and from low accuracy (i.e. the strength of association of the two sides of the rule is often weak, and not well defined).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9993587136268616}]}, {"text": "Such observations are also inline with the discussion on ablation tests carried out at the last RTE evaluation campaigns ( . Additional constraints specifying the variable types are therefore required to correctly instantiate them.", "labels": [], "entities": [{"text": "RTE evaluation", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.6236304640769958}]}, {"text": "In this work, we propose to take advantage of Collaboratively Constructed Semantic Resources (CSRs) (namely, Wikipedia) to mine information useful to context-rich entailment rule acquisition.", "labels": [], "entities": [{"text": "context-rich entailment rule acquisition", "start_pos": 150, "end_pos": 190, "type": "TASK", "confidence": 0.6877972558140755}]}, {"text": "More specifically, we take advantage of material obtained through Wikipedia revisions, which provides at the same time real textual variations from which we may extrapolate the relevant syntactic context, and several simplifications with respect to alternative resources.", "labels": [], "entities": []}, {"text": "We consider T-H pairs where T is a revision of a Wikipedia sentence and H is the original sentence, as the revision is considered more informative then the revised sentence.", "labels": [], "entities": []}, {"text": "We demonstrate the feasibility of the proposed approach for the acquisition of context-rich rules from Wikipedia revision pairs, focusing on two case studies, i.e. the acquisition of entailment rules for causality and for temporal expressions.", "labels": [], "entities": []}, {"text": "Both phenomena are highly frequent in TE pairs, and for both there are no available resources yet.", "labels": [], "entities": [{"text": "TE pairs", "start_pos": 38, "end_pos": 46, "type": "TASK", "confidence": 0.8778273463249207}]}, {"text": "The result of our experiments consists in a repository that can be used by TE systems, and that can be easily extended to entailment rules for other phenomena.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reports on previous work, highlighting the specificity of our work.", "labels": [], "entities": []}, {"text": "Section 3 motivates and describes the general principles underlying our acquisition methodology.", "labels": [], "entities": []}, {"text": "Section 4 describes in details the steps for context-rich rules acquisition from Wikipedia pairs.", "labels": [], "entities": [{"text": "context-rich rules acquisition", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.6159960627555847}]}, {"text": "Section 5 reports about the experiments on causality and temporal expressions and the obtained results.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes the paper and suggests directions for future improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the previous section, we described the steps carried out to acquire context-rich entailment rules from Wikipedia revisions.", "labels": [], "entities": []}, {"text": "To show the applicability of the adopted methodology, we have performed two experiments focusing, respectively, on entailment rules for causality and temporal expressions.", "labels": [], "entities": []}, {"text": "In particular, as case studies we chose two seeds: the conjunction because to derive rules for causality, and the preposition before for temporal expressions.", "labels": [], "entities": []}, {"text": "Accordingly, we extracted from set b only the pairs containing one of these two seeds (either in T or in H) and we built two separate data sets for our experiments.", "labels": [], "entities": []}, {"text": "We run the rule extraction algorithm, and then we filtered again the rules acquired, to collect only those containing one of the two seeds (either in the LHS or in the RHS).", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.785666435956955}, {"text": "RHS", "start_pos": 168, "end_pos": 171, "type": "DATASET", "confidence": 0.8309937715530396}]}, {"text": "This second filtering has been done because there could be pairs in which either because or before are present, but the differences in T and H do not concern those seeds.", "labels": [], "entities": []}, {"text": "The algorithm for rule expansion has then been applied to the selected rules to add the minimal context.", "labels": [], "entities": [{"text": "rule expansion", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.8563727736473083}]}, {"text": "The resulting rule for Example 3 is: To create entailment rules balancing highprecision with their recall (Section 3), when the words of the context added to the rule in Step 4 are identical we substitute them with their PoS.", "labels": [], "entities": [{"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9973341226577759}, {"text": "PoS", "start_pos": 221, "end_pos": 224, "type": "METRIC", "confidence": 0.852418065071106}]}, {"text": "For Example 3, the rule is generalized as follows: <rule ruleid=\"23\" docid=\"844\" pairid=\"15\"> <LHS> (PP The intuition underlying the generalization phase is to allow a more frequent application of the rule, while keeping some constraints on the allowed context.", "labels": [], "entities": [{"text": "LHS", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.971831202507019}]}, {"text": "The application of the rule from Example 3 is allowed if the subtrees below the seed words are the same (the rule can be applied in another T-H pair as, e.g. because of his status \u21d2 due to his status).", "labels": [], "entities": []}, {"text": "Contradictions (e.g. antonyms and semantic oppositions) are generally very infrequent, but in certain cases they can have high impact (one of the most frequent rule collected for temporal expression is before S \u21d2 after S).", "labels": [], "entities": []}, {"text": "For this reason, we used WordNet) to identify and filter antonyms out during the generalization phase.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.9409118294715881}, {"text": "generalization phase", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.8910436928272247}]}, {"text": "We also checked for awkward inconsistencies due to mistakes of the algorithm on noisy Wikipedia data (e.g. rules with the same seed word in both the LHS and the RHS), and we automatically filtered them out.", "labels": [], "entities": []}, {"text": "reports a sample of rules extracted for each seed word.", "labels": [], "entities": []}, {"text": "Statistics about the resulting data sets, i.e. the number of acquired rules both before and after the generalization phase are shown in.", "labels": [], "entities": []}, {"text": "Identical rules are collapsed into a unique one, but the value of their frequency is kept in the header of that rule.", "labels": [], "entities": []}, {"text": "Such index can then be used to estimate the correctness of the rule and, according to our intuition, the probability that the rule preserves the entailment relation.: Resulting sets of entailment rules  Due to the sparseness of the phenomena under consideration (i.e. causality and temporal expressions) in RTE data sets, evaluating the acquired rules on such data does not provide interesting results.", "labels": [], "entities": [{"text": "RTE data sets", "start_pos": 307, "end_pos": 320, "type": "DATASET", "confidence": 0.8502046664555868}]}, {"text": "For this reason, (following (,,)), we opted fora manual analysis of a sample of 100 rules per set, including all the rules whose frequency is \u22652, plus a random set of rules with frequency equal to 1.", "labels": [], "entities": []}, {"text": "Two annotators with skills in linguistics annotated such rules according It is difficult to compare our results with related work, since such phenomena are not covered by other resources.", "labels": [], "entities": []}, {"text": "The correct comparison would be with the subset of e.g. DIRT paraphrases dealing with causality and temporal relations, if any.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement has been calculated, counting when judges agree on the assigned value.", "labels": [], "entities": []}, {"text": "It amounts to 80% on the sample of rules for causality, and to 77% on the sample of rules for temporal expressions.", "labels": [], "entities": []}, {"text": "The highest inter-annotator agreement is for correct entailment rules, whereas the lowest agreement rates are for unknown and error judgments.", "labels": [], "entities": []}, {"text": "This is due to the fact that detecting correct rules is straightforward, while it is less clear whether to consider a wrong rule as well-formed but with an unknown judgment, or to consider it as not appropriate (i.e. error).", "labels": [], "entities": []}, {"text": "shows the outcomes of the analysis of the two sets of rules, as resulting after a reconciliation phase carried out by the annotators.", "labels": [], "entities": []}, {"text": "Such results, provided both for the whole samples: Accuracy (%) of the extracted sets of rules.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9672795534133911}]}], "tableCaptions": [{"text": " Table 2. Identical rules  are collapsed into a unique one, but the value of their  frequency is kept in the header of that rule. Such in- dex can then be used to estimate the correctness of  the rule and, according to our intuition, the probabil- ity that the rule preserves the entailment relation.", "labels": [], "entities": []}, {"text": " Table 2: Resulting sets of entailment rules", "labels": [], "entities": []}, {"text": " Table 3: Accuracy (%) of the extracted sets of rules.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990977048873901}]}]}