{"title": [{"text": "Semi-automatic Annotation of Chinese Word Structure", "labels": [], "entities": [{"text": "Annotation of Chinese Word", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.7776226997375488}]}], "abstractContent": [{"text": "Chinese word structure annotation is potentially useful for many NLP tasks, especially for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word structure annotation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6145191565155983}, {"text": "Chinese word segmentation", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.6190376877784729}]}, {"text": "Li and Zhou (2012) have presented an annotation for word structures in the Penn Chinese Treebank.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 75, "end_pos": 96, "type": "DATASET", "confidence": 0.9813696344693502}]}, {"text": "But they only consider words that have productive affixes, which covers 35% of word types in that corpus.", "labels": [], "entities": []}, {"text": "In this paper, we propose a linguistically inspired annotation that covers various morphological derivations of Chinese in a more general way, such that almost all multiple character words can be structurally analyzed.", "labels": [], "entities": []}, {"text": "As manual annotation is expensive, we propose a semi-supervised approach to automatic annotation, which combines the maximum entropy learning and the EM iteration for the Gaussian mixture model.", "labels": [], "entities": []}, {"text": "The proposed method has achieved an accuracy of 90% on the testing set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.999686598777771}]}], "introductionContent": [{"text": "In contrast to the pervasive success in creation and use of various language resources for corpus linguistics and natural language processing (NLP), Chinese word structure annotation has rarely been studied, although it is likely to be particularly useful to many NLP tasks, especially to Chinese word segmentation.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 114, "end_pos": 147, "type": "TASK", "confidence": 0.7740743656953176}, {"text": "Chinese word structure annotation", "start_pos": 149, "end_pos": 182, "type": "TASK", "confidence": 0.6531307250261307}, {"text": "Chinese word segmentation", "start_pos": 289, "end_pos": 314, "type": "TASK", "confidence": 0.6126174132029215}]}, {"text": "In this paper, we propose a semi-supervised approach to automatic annotation of Chinese word structures.", "labels": [], "entities": [{"text": "automatic annotation of Chinese word structures", "start_pos": 56, "end_pos": 103, "type": "TASK", "confidence": 0.6676393697659174}]}, {"text": "Li (2011) shows many problems in CWS, including wordhood, granularity of lexical units for different applications, as well as several other linguistic phenomena, such as the so-called separable words, and points out that they can only be solved with adequate knowledge of word structure.", "labels": [], "entities": []}, {"text": "Our motivation for creating such an annotation is to test the usefulness of morphological information for the Out-Of-Vocabulary word (OOV) detection, a major challenge in CWS.", "labels": [], "entities": [{"text": "Out-Of-Vocabulary word (OOV) detection", "start_pos": 110, "end_pos": 148, "type": "TASK", "confidence": 0.577257032195727}]}, {"text": "All state-of-the-art word segmenters () based on classification and sequence labeling () have to rely on using character n-grams as features.", "labels": [], "entities": [{"text": "word segmenters", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7386830598115921}]}, {"text": "Despite recent advances in model combination (, joint learning () and integration of supervised and unsupervised methods (), etc., an inherent problem with OOV words is that they are novel character combinations seldom occurring in a training corpus, giving machine learning methods little evidence for prediction.", "labels": [], "entities": []}, {"text": "Like other linguistic elements, the distribution of character n-grams also obeys law, indicating that exponentially more tokens have to occur before more distinct types are encountered.", "labels": [], "entities": []}, {"text": "In other words, we need an exponential growth of annotated corpora to offset the data sparseness problem ( , which is certainly expensive and impractical.", "labels": [], "entities": []}, {"text": "Morphology, on the other hand, offers a principled way to capture internal word structure and model the dynamic and productive word formation process for all words, including OOV ones.", "labels": [], "entities": [{"text": "word formation", "start_pos": 127, "end_pos": 141, "type": "TASK", "confidence": 0.7771754860877991}]}, {"text": "In this work, we will adhere to the conventional linguistic analysis of Chinese morphology).", "labels": [], "entities": []}, {"text": "Chinese words are known to be poor in inflections and rich in derivations, including compounding, affixation and abbreviation, among many others.", "labels": [], "entities": []}, {"text": "introduce an affixation annotation on the Penn Chinese Treebank version 6.0 (CTB,), which covers 35% word types.", "labels": [], "entities": [{"text": "Penn Chinese Treebank version 6.0 (CTB", "start_pos": 42, "end_pos": 80, "type": "DATASET", "confidence": 0.9745737399373736}]}, {"text": "The annotation to be addressed in this paper goes beyond affixation and explores fora general approach to accommodating more predominant processes including compounding.", "labels": [], "entities": []}, {"text": "Our linguistically inspired annotation scheme (Section 3) is based on part-of-speech (POS) like tags for both characters and words, together with syntactic and morphological rules to derive these tags.", "labels": [], "entities": []}, {"text": "In principle, our annotation covers most multiplecharacter words, except multi-char morphemes or binomes, such as \ud97b\udf59\ud97b\udf59 'grape'.", "labels": [], "entities": []}, {"text": "Manual annotation is expensive and inefficient.", "labels": [], "entities": []}, {"text": "To get around this problem, we propose a semi-supervised learning approach to automatic annotation of Chinese word structures, with a focus on two-character words.", "labels": [], "entities": [{"text": "automatic annotation of Chinese word structures", "start_pos": 78, "end_pos": 125, "type": "TASK", "confidence": 0.6737742225329081}]}, {"text": "This method combines the maximum entropy learning and the EM iteration for Gaussian mixture models (Section 5).", "labels": [], "entities": []}, {"text": "Our experiments show that it works significantly better than (1) two classic semi-supervised learning algorithms, self-training and co-training (Section 6), and (2) the supervised learning baseline (Section 4).", "labels": [], "entities": []}, {"text": "The accuracy of the 1-best assignment of char tags by our approach is 90%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999724805355072}]}, {"text": "It is expected that the probabilistic nature of this approach can lead to an even lower error rate in real applications.", "labels": [], "entities": [{"text": "error rate", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9543634355068207}]}, {"text": "To the best of our knowledge, this is the first attempt on wide-coverage semisupervised automatic annotation of Chinese word structures.", "labels": [], "entities": [{"text": "wide-coverage semisupervised automatic annotation of Chinese word structures", "start_pos": 59, "end_pos": 135, "type": "TASK", "confidence": 0.6042131148278713}]}], "datasetContent": [{"text": "We assume that (1) the word structures are independent and identically distributed variables and (2) automatic annotator's performance on samples of the complete set of two-character words, e.g. the manually annotated ones may reflect the performance on the complete set.", "labels": [], "entities": []}, {"text": "We randomly split the manual annotation into a training set and a testing set, of 500 and 100 words, respectively.", "labels": [], "entities": []}, {"text": "The performance of the model trained on the training set is measured by its accuracy on the testing set, which is calculated as follows: The average accuracy with 6-fold cross validation is 81%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9987747073173523}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9994463324546814}]}, {"text": "Note that the popular pair of metrics, precision and recall for binary classification does not apply for the evaluation of the collective result of multiple tags, as the original difference in denominators of the two metric formula no longer exists.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9995307922363281}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9989804625511169}]}, {"text": "We have tried other approaches to automatic annotation to compare with the proposed method.", "labels": [], "entities": []}, {"text": "Since our semi-supervised approach is a combination of supervised ME model and unsupervised GMM, two natural baselines would be the performance that could be achieved by applying two models independently, the former is 81% as shown in section 4.", "labels": [], "entities": []}], "tableCaptions": []}