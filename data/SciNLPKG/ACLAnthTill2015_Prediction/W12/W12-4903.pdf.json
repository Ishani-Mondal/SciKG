{"title": [{"text": "Robustness and processing difficulty models. A pilot study for eye-tracking data on the French Treebank", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.9852683246135712}]}], "abstractContent": [{"text": "We present in this paper a robust method for predicting reading times.", "labels": [], "entities": [{"text": "predicting reading", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8397358953952789}]}, {"text": "Robustness first comes from the conception of the difficulty model, which is based on a morpho-syntactic surprisal index.", "labels": [], "entities": []}, {"text": "This metric is not only a good predictor, as shown in the paper, but also intrinsically robust (because relying on POS-tagging instead of parsing).", "labels": [], "entities": []}, {"text": "Second, robustness also concerns data analysis: we propose to enlarge the scope of reading processing units by using syntactic chunks instead of words.", "labels": [], "entities": [{"text": "data analysis", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.7588888108730316}]}, {"text": "As a result, words with null reading time do not need any special treatment or filtering.", "labels": [], "entities": []}, {"text": "It appears that working at chunks scale smooths out the variability inherent to the different reader's strategy.", "labels": [], "entities": []}, {"text": "The pilot study presented in this paper applies this technique to anew resource we have built, enriching a French treebank with eye-tracking data and difficulty prediction measures.", "labels": [], "entities": [{"text": "French treebank", "start_pos": 107, "end_pos": 122, "type": "DATASET", "confidence": 0.954626053571701}, {"text": "difficulty prediction", "start_pos": 150, "end_pos": 171, "type": "TASK", "confidence": 0.7301052510738373}]}], "introductionContent": [{"text": "Eye-tracking data are now often used in the study of language complexity (e.g. difficulty metrics evaluation) as well as finer syntactic studies (e.g. relative complexity of alternative constructions).", "labels": [], "entities": []}, {"text": "However, only few resources exist, fora small number of languages.", "labels": [], "entities": []}, {"text": "We describe in this paper a pilot study aiming at developing a high-level resource enriching a treebank with physiological data and complexity measures.", "labels": [], "entities": []}, {"text": "This work have been done for French, with several objectives : (1) building anew large resource for French, freely available, associating syntactic information, eye-tracking data and difficulty prediction at different levels (tokens, chunks and phrases) (2) validating a difficulty model for French in the line of what has been done for other languages,) relying on a robust surprisal index described in ().", "labels": [], "entities": [{"text": "difficulty prediction", "start_pos": 183, "end_pos": 204, "type": "TASK", "confidence": 0.6650571227073669}]}, {"text": "This pilot study, on top of building anew resource, had important side-effects.", "labels": [], "entities": []}, {"text": "First, this work led us to examine carefully the question of data analysis.", "labels": [], "entities": [{"text": "data analysis", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7349469512701035}]}, {"text": "In particular, we found that working with larger units (syntactic chunks) instead of tokens makes it possible to take into consideration the entire set of data.", "labels": [], "entities": []}, {"text": "In other words, it is not anymore necessary to eliminate data that are usually considered for different reasons as problematic (tokens ending lines, before punctuations, etc.).", "labels": [], "entities": []}, {"text": "This result is important for several reasons.", "labels": [], "entities": []}, {"text": "First, it avoids the use of truncated data (which is problematic in a statistical point of view).", "labels": [], "entities": []}, {"text": "Second, it supports the hypothesis that chunks are not only functional, but can also be defined in linguistic terms by means of syntactic relation strength.", "labels": [], "entities": []}, {"text": "Another interesting result is the influence of the syntactic parameter on the global model: we show that (morpho)syntax has modest impact in comparison with word frequency and word length.", "labels": [], "entities": []}, {"text": "Finally, at the technical level, we have developed an entire experimental setup, facilitating data acquisition when using Tobii devices.", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.7281810790300369}]}, {"text": "Our environment proposes tools for the preparation of the experimental material (slide generation) as well as data post-processing (e.g. lines model detection).", "labels": [], "entities": [{"text": "slide generation", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.6978137642145157}, {"text": "lines model detection", "start_pos": 137, "end_pos": 158, "type": "TASK", "confidence": 0.6209886372089386}]}], "datasetContent": [{"text": "In the experiment, eliminates from the original corpus several data: first and last tokens of each line, token followed by a punctuation, region of 4 words with no fixations and words with zero value for FFD and FPD . Finally, this experiment retains a total of 200,684 data points, which means 20,068 tokens read by 10 subjects.", "labels": [], "entities": [{"text": "FFD", "start_pos": 204, "end_pos": 207, "type": "METRIC", "confidence": 0.8608783483505249}, {"text": "FPD", "start_pos": 212, "end_pos": 215, "type": "METRIC", "confidence": 0.9602277278900146}]}, {"text": "The results of this study show that unlexicalized surprisal can predict reading times, whereas the lexicalized formulation does not.", "labels": [], "entities": []}, {"text": "However, () pointed out recently that when using independent sentences, both lexicalized and unlexicalized surprisal measures are significant predictors of reading time (measures done with corpus of around 2,500 words and 54 participants).", "labels": [], "entities": []}, {"text": "These different studies focus on lexical and syntactic effects.", "labels": [], "entities": []}, {"text": "Ina complementary direction, () analyzed the influence of superficial lexical semantics on fixation duration.", "labels": [], "entities": [{"text": "fixation duration", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7415063977241516}]}, {"text": "( integrates this parameter into Surprisal.", "labels": [], "entities": [{"text": "Surprisal", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.8340916037559509}]}, {"text": "This work shows the effect of semantic costs in addition to syntactic surprisal for reading time prediction.", "labels": [], "entities": [{"text": "reading time prediction", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.6597284773985544}]}, {"text": "It also addresses in a specific way the question of modeling: experimental studies usually use linear mixed effect models, including random effects (e.g. participants characteristics) and fixed ones (e.g. word frequency).", "labels": [], "entities": []}, {"text": "In these approaches, many different parameters are brought together.", "labels": [], "entities": []}, {"text": "As authors pointed out, the use of a unique measure for predicting complexity is preferable than a set of factors, not only for simplicity, but also because it is difficult to analyze the effective contribution of a factor: one can evaluate whether adding it into a model improves it fits, but cannot explain the reasons.", "labels": [], "entities": [{"text": "predicting complexity", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.8707084059715271}]}, {"text": "As shown in the previous section, corpus used in the different experiments are very different in size and nature.) explicitly focuses on naturalistic data.", "labels": [], "entities": []}, {"text": "On the opposite, () relies on a very small corpus, but with large amount of subjects.", "labels": [], "entities": []}, {"text": "The following table presents the main features of the different corpora.", "labels": [], "entities": []}, {"text": "It mentions the number of token presented to the readers, the number of subjects participating to the experiment, the number of data points (roughly speaking fixation points) taken into account in the evaluation (after eliminating problematic data), the average number of tokens read by the subjects and taken into account after data filtering (data points are more or less the number of participants times the number of remaining tokens) and the experimental method.", "labels": [], "entities": []}, {"text": "For similar study on French, there exists only one resource (the French part of the Dundee corpus (), but which is not publically available.", "labels": [], "entities": [{"text": "Dundee corpus", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.9263200163841248}]}, {"text": "This situation leads us to the project to build anew large resource for French, associating syntactic information, eye-tracking data and difficulty prediction.", "labels": [], "entities": [{"text": "French", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.8305380344390869}, {"text": "difficulty prediction", "start_pos": 137, "end_pos": 158, "type": "TASK", "confidence": 0.6415209323167801}]}, {"text": "The pilot study presented hereafter has been realized in order to check the viability of the overall project.", "labels": [], "entities": []}, {"text": "One of our goal is to validate the experimental design.", "labels": [], "entities": []}, {"text": "Our pilot study consisted in acquiring eye-movement data for 13 subjects reading an extract of the French Treebank (herefater FTB, ().", "labels": [], "entities": [{"text": "French Treebank (herefater FTB", "start_pos": 99, "end_pos": 129, "type": "DATASET", "confidence": 0.7374991297721862}]}, {"text": "The FTB is a set of articles from the newspaper Le Monde.", "labels": [], "entities": [{"text": "The", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8702520132064819}, {"text": "FTB is a set of articles from the newspaper Le Monde", "start_pos": 4, "end_pos": 56, "type": "DATASET", "confidence": 0.8311980095776644}]}, {"text": "Most of these articles are in the economical field, which does not fit well with the idea of natural reading.", "labels": [], "entities": []}, {"text": "However, we selected from this corpus several extracts that seemed to us less technical in terms of semantic contents.", "labels": [], "entities": []}, {"text": "The eye-tracking device is a Tobii 60 Hz 3 . The selected subcorpus used in this experiment is made of 6 articles of variable length (from 3 to 6 minutes of reading time), each of them presented to the reader as a succession of slides.", "labels": [], "entities": []}, {"text": "Participants have to press a key to access to the next slide.", "labels": [], "entities": []}, {"text": "Once the key pressed, an empty frame with a target cursor indicating the position of the first line beginning the next slide is presented during three seconds, followed by the text slide.", "labels": [], "entities": []}, {"text": "A calibration of the Tobii machine is proposed before reading each article and a three minutes pause between articles has been observed, filled by an informal discussion with the experimenter about the content of the article.", "labels": [], "entities": [{"text": "Tobii machine", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.8798465430736542}]}, {"text": "The overall session last 45 minutes in average for each participant.", "labels": [], "entities": []}, {"text": "Each slide contains from 4 to 7 lines.", "labels": [], "entities": []}, {"text": "Sentences were constrained to appear on a single slide, and the text is not right justified, tokens too long to enter the current line are printed on the next line.", "labels": [], "entities": []}, {"text": "The text is printed on 800 \u00d7 600 pixels slides using an Arial font of size 18 with line spacing of size 26 pixels (an example is presented).", "labels": [], "entities": []}, {"text": "The participant is positioned at a 60 cm distance of the screen, which implies a 30 pixels precision on Tobii measurements or equivalently a two characters horizontal precision and half line spacing in vertical precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9869968891143799}, {"text": "Tobii measurements", "start_pos": 104, "end_pos": 122, "type": "DATASET", "confidence": 0.848635196685791}, {"text": "precision", "start_pos": 211, "end_pos": 220, "type": "METRIC", "confidence": 0.6878760457038879}]}, {"text": "The design of the experiment has been done thanks to a software we have developed (the generic designing software coming with Tobii being not suited fora full-text reading experiment).", "labels": [], "entities": []}, {"text": "Our system automatically generates the slides and associates to each word its size in pixels as well as its precise spatial location.", "labels": [], "entities": []}, {"text": "This renders straightforward the specification of each word (or set of words) as \"area-of-interest\" for the eye-tracking system.", "labels": [], "entities": []}, {"text": "The overall corpus is made of 80 slides, 198 sentences split on 549 lines, which contains 6, 572 tokens (5, 696 words and 876 punctuation marks), which comes to 75,077 data points (a reasonable size in comparison with existing resources, see previous section).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The slopes, standard errors and statistical significance for the variables entering the  linear fit.", "labels": [], "entities": [{"text": "standard errors", "start_pos": 22, "end_pos": 37, "type": "METRIC", "confidence": 0.9612657725811005}, {"text": "statistical significance", "start_pos": 42, "end_pos": 66, "type": "METRIC", "confidence": 0.8346261978149414}]}]}