{"title": [{"text": "Improving Sentence Completion in Dialogues with Multi-Modal Features", "labels": [], "entities": [{"text": "Improving Sentence Completion", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8946572740872701}]}], "abstractContent": [{"text": "With the aim of investigating how humans understand each other through language and gestures , this paper focuses on how people understand incomplete sentences.", "labels": [], "entities": []}, {"text": "We trained a system based on interrupted but resumed sentences , in order to find plausible completions for incomplete sentences.", "labels": [], "entities": []}, {"text": "Our promising results are based on multi-modal features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our project, called RoboHelper, focuses on developing an interface for elderly people to effectively communicate with robotic assistants that can help them perform Activities of Daily Living (ADLs), so that they can safely remain living in their home).", "labels": [], "entities": []}, {"text": "We are developing a multi-modal interface since people communicate with each other using a variety of verbal and non-verbal signals, including haptics, i.e., force exchange (as when one person hands a bowl to another person, and lets go only when s/he senses that the other is holding it).", "labels": [], "entities": []}, {"text": "We collected a medium size multi-modal human-human dialogue corpus, then processed and analyzed it.", "labels": [], "entities": []}, {"text": "We observed that a fair number of sentences are incomplete, namely, the speaker does not finish the utterance.", "labels": [], "entities": []}, {"text": "Because of that, we developed a core component of our multi-modal interface, a sentence completion system, trained on the set of interrupted but eventually completed sentences from our corpus.", "labels": [], "entities": [{"text": "sentence completion", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7163298577070236}]}, {"text": "In this paper, we will present the component of the system that predicts reasonable completion structures for an incomplete sentence.", "labels": [], "entities": []}, {"text": "Sentence completion has been addressed within information retrieval, to satisfy user's information needs ().", "labels": [], "entities": [{"text": "Sentence completion", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.957332968711853}, {"text": "information retrieval", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.7574346363544464}]}, {"text": "Completing sentences in human-human dialogue is more difficult than in written text.", "labels": [], "entities": [{"text": "Completing sentences in human-human dialogue", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7333138823509217}]}, {"text": "First, utterances maybe informal, ungrammatical or dis-fluent; second, people interrupt each other during conversations).", "labels": [], "entities": []}, {"text": "Additionally, the interaction is complex, as people spontaneously use hand gestures, body language and gaze besides spoken language.", "labels": [], "entities": []}, {"text": "As noticed by, during face-to-face interaction, the completion problem is not only an exclusively verbal phenomenon but \"an action embedded within a complex web of different meaning-making fields\".", "labels": [], "entities": []}, {"text": "Accordingly, among our features, we will include pointing gestures, and haptic-ostensive (H-O) actions, e.g., referring to an object by manipulating it in the real world.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe our data collection and multi-modal annotation.", "labels": [], "entities": []}, {"text": "In Section 3 we discuss how we generate our training data, and in Section 4 the model we train for sentence completion, and the results we obtain.", "labels": [], "entities": [{"text": "sentence completion", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.788697361946106}]}], "datasetContent": [{"text": "In contrast with other sentence completion systems that focus on text input, the dataset we use in this paper is a subset of the ELDERLY-AT-HOME corpus, a multi-modal corpus in the domain of elderly care, which includes collaborative human-human dialogues, pointing gestures and haptic-ostensive (H-O) actions.", "labels": [], "entities": [{"text": "sentence completion", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7525486648082733}, {"text": "ELDERLY-AT-HOME corpus", "start_pos": 129, "end_pos": 151, "type": "DATASET", "confidence": 0.7994686365127563}]}, {"text": "Our experiments were conducted in a fully functional apartment and included a helper (HEL) and an elderly person (ELD).", "labels": [], "entities": [{"text": "HEL", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9315482974052429}]}, {"text": "HEL helps ELD to complete several realistic tasks, such as putting on shoes, finding a pot, cooking pasta and setting the table for dinner.", "labels": [], "entities": [{"text": "HEL", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.930454432964325}, {"text": "ELD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7520666122436523}]}, {"text": "We used 7 web cameras to videotape the whole experiment, one microphone each to record the audio and one data glove each to collect haptics data.", "labels": [], "entities": []}, {"text": "We ran 20 realistic experiments in total, and then imported the videos and audios (in avi format), haptics data (in csv format) and transcribed utterances (in xml format) into Anvil) to build the multi-modal corpus.", "labels": [], "entities": [{"text": "Anvil", "start_pos": 176, "end_pos": 181, "type": "DATASET", "confidence": 0.9213022589683533}]}, {"text": "Among other annotations (for example Dialogue Acts) we have annotated these dialogues for Pointing gestures and H-O actions.", "labels": [], "entities": [{"text": "Pointing gestures", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.893665611743927}]}, {"text": "Due to the setting of our experiments, the targets of pointing gestures and H-O actions are real life objects, thus we designed a reference index system to annotate them.", "labels": [], "entities": []}, {"text": "We give pre-defined indices to targets which cannot be moved, such as cabinets, draws, and fridge.", "labels": [], "entities": []}, {"text": "We also assign runtime indices to targets which can be moved, like pots, glasses, and plates.", "labels": [], "entities": []}, {"text": "For example, \"Glass1\" refers to the first glass that appears in one experiment.", "labels": [], "entities": []}, {"text": "In our annotation, a \"Pointing\" gesture is defined as a hand gesture without any physical contact between human and objects.", "labels": [], "entities": []}, {"text": "Hand gestures with physical contact to objects are annotated as H-O actions.", "labels": [], "entities": []}, {"text": "H-O actions are further subdivided into 7 subtypes, including \"Holding\", \"Touching\",\"Open\" and \"Close\".", "labels": [], "entities": [{"text": "Close", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9625682234764099}]}, {"text": "In order to verify the reliability of our annotations, we double coded 15% of the pointing gestures and H-O actions.", "labels": [], "entities": [{"text": "reliability", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9602093696594238}]}, {"text": "Kappa values of 0.751 for pointing gestures, and of 0.703 for H-O actions, are considered acceptable, especially considering the complexity of these real life tasks.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9180639386177063}]}, {"text": "In this paper, we focus on specific sub-dialogues in the corpus, which we call interruptions.", "labels": [], "entities": []}, {"text": "An interruption can occur at any point in human-human dialogues: it happens when presumably the interrupter (ITR) thinks s/he has already understood what the speaker (SPK) means before listening to the entire sentence.", "labels": [], "entities": [{"text": "ITR) thinks s/he has already understood what the speaker (SPK) means before listening to the entire sentence", "start_pos": 109, "end_pos": 217, "type": "TASK", "confidence": 0.5475575124675577}]}, {"text": "By observing the data from our corpus, we conclude that there are generally three cases of interruptions.", "labels": [], "entities": []}, {"text": "First, the speaker (SPK) stops speaking and does not complete the sentence -these are the incomplete sentences whose completion a robot would need to infer.", "labels": [], "entities": []}, {"text": "In the second type of interruption, after being interrupted SPK continues with (a) few words, and then stops without finishing the whole sentence: hence, there is a short time overlap between two sentences (7 cases).", "labels": [], "entities": []}, {"text": "The third case occurs when the SPK ignores the ITR and finishes the entire sentence.", "labels": [], "entities": [{"text": "SPK ignores the ITR", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7397594004869461}]}, {"text": "In this case, the SPK and the ITR speak simultaneously.", "labels": [], "entities": [{"text": "ITR", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.75583416223526}]}, {"text": "The number of interruptions ranges from 1 to 37 in each experiment.", "labels": [], "entities": []}, {"text": "An excerpt from an interruption with a subsequent completion (an example of case 3) is shown below.", "labels": [], "entities": []}, {"text": "The interruption occurs at the start of the overlap between the two speakers, marked by < and >.", "labels": [], "entities": []}, {"text": "This example also includes annotations for pointing gestures and for H-O actions.", "labels": [], "entities": []}, {"text": "Elder: I need some glasses from < that cabinet >.", "labels": [], "entities": []}, {"text": "[Point (Elder, Cabinet1)] Helper: < From this > cabinet?", "labels": [], "entities": [{"text": "Elder, Cabinet1)] Helper", "start_pos": 8, "end_pos": 32, "type": "DATASET", "confidence": 0.825177538394928}]}, {"text": "[Point (Helper, Cabinet2)] Helper: Is this the glass you < 're looking for?", "labels": [], "entities": [{"text": "Cabinet2)] Helper", "start_pos": 16, "end_pos": 33, "type": "DATASET", "confidence": 0.7488487561543783}]}, {"text": "> [Touching (Helper, Glass1)] Elder: As concerns annotation for interruptions, it proceeds from identifying interrupted sentences to finding <interrupted sentences, candidate structure> pairs which will be used for generating grammatical completion for an incomplete sentence.", "labels": [], "entities": [{"text": "generating grammatical completion", "start_pos": 215, "end_pos": 248, "type": "TASK", "confidence": 0.6074327031771342}]}, {"text": "Each interrupted sentence is marked with two categories: incomplete form, from the start of the sentence to where it is interrupted, such as \"I need some glasses\"; complete form, from the start of a sentence to where the speaker stops, \"I need some glasses from that cabinet.\" shows distribution statistics for our ELDERLY-AT-HOME corpus.", "labels": [], "entities": [{"text": "ELDERLY-AT-HOME corpus", "start_pos": 315, "end_pos": 337, "type": "DATASET", "confidence": 0.8256167471408844}]}, {"text": "It contains a total of 4839 sentences, which in turn contain 7219 clauses.", "labels": [], "entities": []}, {"text": "320 sentences are incomplete in the sense of case 1 (after interruption SPK never completes his/her sentence); whereas 205 sentences are completed after interruption (cases 2 and 3).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Distribution of completion classifications", "labels": [], "entities": [{"text": "Distribution of completion classifications", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.8443345427513123}]}, {"text": " Table 4: Reasonable Structure Selection models", "labels": [], "entities": []}]}