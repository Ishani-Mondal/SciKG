{"title": [{"text": "Interactive Natural Language Query Construction for Report Generation *", "labels": [], "entities": [{"text": "Interactive Natural Language Query Construction", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.5871631145477295}, {"text": "Report Generation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7768921554088593}]}], "abstractContent": [{"text": "Question answering is an age old AI challenge.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9130288064479828}]}, {"text": "How we approach this challenge is determined by decisions regarding the linguistic and domain knowledge our system will need, the technical and business acumen of our users, the interface used to input questions , and the form in which we should present answers to a user's questions.", "labels": [], "entities": []}, {"text": "Our approach to question answering involves the interactive construction of natural language queries.", "labels": [], "entities": [{"text": "question answering", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9144631028175354}]}, {"text": "We describe and evaluate a question answering system that provides a point-and-click, web-based interface in conjunction with a semantic grammar to support user-controlled natural language question generation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8156743943691254}, {"text": "user-controlled natural language question generation", "start_pos": 156, "end_pos": 208, "type": "TASK", "confidence": 0.7306716561317443}]}, {"text": "A preliminary evaluation is performed using a selection of 12 questions based on the Adventure Works sample database.", "labels": [], "entities": [{"text": "Adventure Works sample database", "start_pos": 85, "end_pos": 116, "type": "DATASET", "confidence": 0.9211530089378357}]}], "introductionContent": [{"text": "There is along history of systems that allow users to pose questions in natural language to obtain appropriate responses from information systems.", "labels": [], "entities": []}, {"text": "Information systems safeguard a wealth of information, but traditional interfaces to these systems require relatively sophisticated technical know-how and do not always present results in the most useful or intuitive way for non-technical users.", "labels": [], "entities": []}, {"text": "Simply put, people and computers do not speak the same language.", "labels": [], "entities": []}, {"text": "The question answering challenge is thus the matter of developing a method that allows users with varying levels * This research was supported in part by a discovery grant from the Natural Sciences and Engineering Research Council of Canada.", "labels": [], "entities": [{"text": "question answering", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.891878604888916}]}, {"text": "The authors would also like to thank the referees for their insights and suggestions. of technical proficiency to ask questions using natural language and receive answers in an appropriate, intuitive format.", "labels": [], "entities": []}, {"text": "Using natural language to ask these questions maybe easy for users, but is challenging due to the ambiguity inherent in natural language anaylsis.", "labels": [], "entities": []}, {"text": "Proposals involving controlled natural language, such as), can deal with some of the challenges, but the task becomes more difficult when we seek to answer natural language questions in away that is domain portable.", "labels": [], "entities": []}, {"text": "Before we can attempt to design and implement a question answering system, we need to address several key issues.", "labels": [], "entities": [{"text": "question answering", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8562036454677582}]}, {"text": "First, we need to decide what knowledge our system needs.", "labels": [], "entities": []}, {"text": "Specifically, we must decide what linguistic knowledge is needed to properly interpret users' questions.", "labels": [], "entities": []}, {"text": "Then we need to consider what kind of domain-specific knowledge the system must have and how that knowledge will be stored and accessed.", "labels": [], "entities": []}, {"text": "We must address the challenges posed by users with varying levels of technical sophistication and domain knowledge.", "labels": [], "entities": []}, {"text": "The sophistication of the user and the environment in which the system is used will also affect how users will give input to the system.", "labels": [], "entities": []}, {"text": "Will we need to process text, speech, or will a simpler point-and-click interface be sufficient?", "labels": [], "entities": []}, {"text": "Finally, we must decide how to best answer the user's questions, whether it be by fetching pre-existing documents, dynamically generating structured database reports, or producing natural language sentences.", "labels": [], "entities": []}, {"text": "These five issues do not present us with a series of independent choices that are merely stylistic or cosmetic.", "labels": [], "entities": []}, {"text": "The stance we take regarding each of these issues strongly influences design decisions, ease of installation/configuration, and the end-user experience.", "labels": [], "entities": [{"text": "ease", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9847095608711243}]}, {"text": "Here we solve this problem in the context of ac-cessing information from a structured database -a natural language interface to a database (NLIDB) (.", "labels": [], "entities": []}, {"text": "However, instead of treating it as a natural language analysis problem, we will consider it as a task involving natural language generation (NLG) where users build natural language questions by making choices that add words and phrases.", "labels": [], "entities": [{"text": "natural language analysis", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.7237124045689901}, {"text": "natural language generation (NLG)", "start_pos": 112, "end_pos": 145, "type": "TASK", "confidence": 0.8279882172743479}]}, {"text": "Using our method, users construct queries in a menu driven manner to ask questions that are always unambiguous and easy for anyone to understand, getting answers in the form of interactive database reports (not textual reports) that are both immediate and consistent.", "labels": [], "entities": []}, {"text": "This approach retains the main advantage of traditional NLIDBs that allow input of a question in a free form text -the ability for the user to communicate with the information system in English.", "labels": [], "entities": []}, {"text": "There is no need for the user to master a computer query langauge such as SQL or MDX.", "labels": [], "entities": []}, {"text": "Many disadvantges of traditional free input NLIDBs are removed.", "labels": [], "entities": []}, {"text": "Traditional NLIDBs fail to analyze some questions and indicate so to the user, greatly decreasing the user's confidence in the system.", "labels": [], "entities": []}, {"text": "The problem is even worse when the NLIDB analyzes the question incorrectly and produces a wrong or unpexpected result.", "labels": [], "entities": []}, {"text": "In contrast, our system is able to answer every question correctly.", "labels": [], "entities": []}, {"text": "In traditional free input NLIDBs, the user can make grammatical or spelling mistakes that may lead to other errors.", "labels": [], "entities": []}, {"text": "Using a menu-based technique, the user is forced to input only valid and wellformed queries.", "labels": [], "entities": []}, {"text": "The complexity of the system is greatly reduced as the language that the system has to process is simple and unambiguous.", "labels": [], "entities": []}, {"text": "Portability to other domains is improved because there is no need for vocabulary that fully covers the domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "An evaluation of this kind of system requires an examination of three performance metrics: domain coverage, ease of use, and query efficiency.", "labels": [], "entities": [{"text": "ease", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9836344718933105}]}, {"text": "How well the system covers the target domain is crucially important.", "labels": [], "entities": []}, {"text": "In order to measure domain coverage, we need to determine how many answerable questions can actually be answered using the system.", "labels": [], "entities": []}, {"text": "We can answer this question in part by examining the user interface.", "labels": [], "entities": []}, {"text": "Does the interface restrict users' access to domain elements and relationships?", "labels": [], "entities": []}, {"text": "A more thorough assessment of domain coverage requires exten-sive user studies.", "labels": [], "entities": []}, {"text": "Ease of use is often thought of as a qualitative measure of performance, but a systematic, objective evaluation requires us to define a quantitative measure.", "labels": [], "entities": []}, {"text": "The primary action used to generate queries in our system is the \"click.\"", "labels": [], "entities": []}, {"text": "Users click on items to refine their queries, so the number of clicks required to generate queries seems like a reasonable starting point for evaluating ease of use.", "labels": [], "entities": []}, {"text": "The time it takes users to make those clicks is important.", "labels": [], "entities": []}, {"text": "A four-click query sounds efficient, but if it takes the user two minutes to figure out which four clicks need to be made, not much is gained.", "labels": [], "entities": []}, {"text": "It would be ideal if the number of clicks and the time needed to make those clicks grow proportionally.", "labels": [], "entities": []}, {"text": "That is, we do not want to penalize users who need to build longer queries.", "labels": [], "entities": []}, {"text": "Query efficiency is measured by the time between the user submitting a query and the system presenting the answer.", "labels": [], "entities": []}, {"text": "How long must a user wait while data is being fetched and the report generated?", "labels": [], "entities": []}, {"text": "Unlike ease of use, this is objectively measurable and easy to benchmark.", "labels": [], "entities": []}, {"text": "In our initial evaluation, we applied these metrics to a selection of 12 natural language questions about the data in the Adventure Works (Codeplex Open Source Community, 2008) database that could be answered by our natural language query construction system.", "labels": [], "entities": [{"text": "Adventure Works (Codeplex Open Source Community, 2008) database", "start_pos": 122, "end_pos": 185, "type": "DATASET", "confidence": 0.9386905215003274}, {"text": "natural language query construction", "start_pos": 216, "end_pos": 251, "type": "TASK", "confidence": 0.6843288838863373}]}, {"text": "These questions were generated by a user with prior exposure to the Adventure Works database but no prior exposure to the query construction software system or its design or algorithms, so the questions are not purposely fine-tuned to yield artificially optimal results.", "labels": [], "entities": [{"text": "Adventure Works database", "start_pos": 68, "end_pos": 92, "type": "DATASET", "confidence": 0.9003308018048605}]}, {"text": "Eight of these questions were directly answerable, while four were indirectly answerable.", "labels": [], "entities": []}, {"text": "For each of these questions, we measured the number of clicks required to generate the query string, the time it took to make the required clicks, and the time required to retrieve the needed records and generate a report.", "labels": [], "entities": []}, {"text": "The distinction between directly answerable and indirectly answerable questions deserves a short explanation.", "labels": [], "entities": []}, {"text": "A question is deemed directly answerable if the answer is the sole result returned in the report or if the answer is included in a group of results returned.", "labels": [], "entities": []}, {"text": "A question is deemed indirectly answerable if the report generated based on a related query can be used to calculate the answer or if the information relevant to the answer is a subset of the information returned.", "labels": [], "entities": []}, {"text": "So, the question What are the top 20 products based on internet sales was directly answerable through the constructed query Show products with one of 20 highest internet sales amount, while the question What is the averagefreight cost for internet orders over $1000 could only be answered Show internet freight cost for customers with more than 1000 dollars of internet sales amount and for each date.", "labels": [], "entities": []}, {"text": "We found that a user was able to construct natural language queries using between 2 and 6 clicks which required 10 and 57 seconds of elaspsed time for the construction process.", "labels": [], "entities": []}, {"text": "On average 3.3 clicks were required to create a query with an average time of 33 seconds, where the time grew in a linear manner based on the number of clicks.", "labels": [], "entities": []}, {"text": "Once a query was constructed, the average time to generate a report was 6.7 seconds with the vast majority of queries producing a report from the database system in 4 seconds or less.", "labels": [], "entities": []}, {"text": "The median values for query construction was 2.5 clicks, query construction was 31.5 seconds, and report generation was 4 seconds..", "labels": [], "entities": [{"text": "query construction", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8337258398532867}, {"text": "report generation", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.6335466802120209}]}], "tableCaptions": []}