{"title": [{"text": "NEU Systems in SIGHAN Bakeoff 2012", "labels": [], "entities": [{"text": "NEU Systems in SIGHAN Bakeoff 2012", "start_pos": 0, "end_pos": 34, "type": "DATASET", "confidence": 0.8169475694497427}]}], "abstractContent": [{"text": "This paper describes the methods used for the parsing the Sinica Treebank for the bakeoff task of SigHan 2012.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9805861711502075}, {"text": "Sinica Treebank", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.7908247113227844}, {"text": "SigHan 2012", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.7301525175571442}]}, {"text": "Based on the statistics of the training data and the experimental results, we show that the major difficulties in parsing the Sinica Treebank comes from both the data sparse problem caused by the fine-grained annotation and the tagging ambiguity.", "labels": [], "entities": [{"text": "parsing", "start_pos": 114, "end_pos": 121, "type": "TASK", "confidence": 0.9863508343696594}, {"text": "Sinica Treebank", "start_pos": 126, "end_pos": 141, "type": "DATASET", "confidence": 0.7061323523521423}]}], "introductionContent": [{"text": "Parsing has been a major interest of research in the NLP community.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9347862005233765}]}, {"text": "For the last two decades, statistical approaches to parsing achieved great success and parsing performance has been significantly improved.", "labels": [], "entities": [{"text": "parsing", "start_pos": 52, "end_pos": 59, "type": "TASK", "confidence": 0.9910209774971008}, {"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9753175973892212}]}, {"text": "One of the most important factors for developing accurate and robust statistical parsers for one language is the availability of large scale annotated Treebank in that language.", "labels": [], "entities": []}, {"text": "The availability of the Sinica Treebank provides such an opportunity for developing statistical parsers for traditional Chinese.", "labels": [], "entities": [{"text": "Sinica Treebank", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.8433069288730621}]}, {"text": "In this paper, we analyze the difficulties in parsing the Sinica Treebank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9875227808952332}, {"text": "Sinica Treebank", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.7772054672241211}]}, {"text": "By comparing the statistics between the Sinica Treebank and CTB we found that the fine-grained annotation schema adopted by the Sinica Treebank lead to more severe data sparse problem.", "labels": [], "entities": [{"text": "Sinica Treebank", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.9283140897750854}, {"text": "CTB", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.6275725364685059}, {"text": "Sinica Treebank", "start_pos": 128, "end_pos": 143, "type": "DATASET", "confidence": 0.8932967483997345}]}, {"text": "By inspecting the parsing results, we also found that a great portion of parsing errors is caused by tagging errors.", "labels": [], "entities": []}, {"text": "In particular, word classes such as Ng and Ncd are quite similar in their meaning.", "labels": [], "entities": []}, {"text": "However, the two tags yield quite different syntactic structures.", "labels": [], "entities": []}], "datasetContent": [{"text": "The initial models are trained using our training data without any treatments.", "labels": [], "entities": []}, {"text": "The parsing performances are listed in table 2.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9710767865180969}]}, {"text": "From table 2, we can see that the best parsing performance in terms of F1 score is 78.16, and the best tagging accuracy is 91.60.", "labels": [], "entities": [{"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9561049938201904}, {"text": "F1 score", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9613180756568909}, {"text": "tagging", "start_pos": 103, "end_pos": 110, "type": "TASK", "confidence": 0.9409798979759216}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9585732817649841}]}, {"text": "These numbers are far below that achieved on the Penn Chinese Treebank (5.1) even the average length of the sentences in CTB is longer than the Sinica Treebank and we assume that the Sinica Treebank suffers more from data sparse problem.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.9821889797846476}, {"text": "Sinica Treebank", "start_pos": 144, "end_pos": 159, "type": "DATASET", "confidence": 0.8741739988327026}, {"text": "Sinica Treebank", "start_pos": 183, "end_pos": 198, "type": "DATASET", "confidence": 0.8484417796134949}]}, {"text": "Interestingly, from table 2 we can see that the best parsing and tagging performance are both achieved at the 4-th split-merge round and after that parsing performance started to drop..", "labels": [], "entities": [{"text": "parsing", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.9701895117759705}]}, {"text": "Parsing performance on the development set.", "labels": [], "entities": []}, {"text": "#Split is the number of split-merge round WSJ Penn Treebank and the Penn Chinese Treebank, the best performance is achieved around the 6-th split-merge round.", "labels": [], "entities": [{"text": "Split", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.9597844481468201}, {"text": "WSJ Penn Treebank", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.8628746271133423}, {"text": "Penn Chinese Treebank", "start_pos": 68, "end_pos": 89, "type": "DATASET", "confidence": 0.9827629923820496}]}, {"text": "One should note that we do not argue the parsing performance of the Sinica Treebank and CTB are directly comparable.", "labels": [], "entities": [{"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9638192057609558}, {"text": "Sinica Treebank", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.9100456237792969}, {"text": "CTB", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.8844314217567444}]}, {"text": "However, we do believe that the difference between the statistics of the two Treebanks helps to identify some difficulties in parsing the Sinica Treebank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 126, "end_pos": 133, "type": "TASK", "confidence": 0.9837374687194824}, {"text": "Sinica Treebank", "start_pos": 138, "end_pos": 153, "type": "DATASET", "confidence": 0.8425578773021698}]}, {"text": "By comparing the statistics between the training set in this work and the training set of CTB, we found that the CTB contains more words, totally 536806 words, while less different word forms, 36922 word forms.", "labels": [], "entities": [{"text": "CTB", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.861993670463562}]}, {"text": "Moreover, CTB only contains 42 different POS tags which is less than a half of the POS tags of the Sinica Treebank.", "labels": [], "entities": [{"text": "CTB", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.8929653167724609}, {"text": "Sinica Treebank", "start_pos": 99, "end_pos": 114, "type": "DATASET", "confidence": 0.8740482330322266}]}, {"text": "These numbers demonstrate that parameters are more sufficiently estimated on CTB than on the Sinica Treebank.", "labels": [], "entities": [{"text": "CTB", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.9840328097343445}, {"text": "Sinica Treebank", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.9122391045093536}]}, {"text": "By inspecting the detail tree structures and labels in the Sinica Treebank, we found that the Sinica Treebank annotation is more fine-grained compare with that of CTB.", "labels": [], "entities": [{"text": "Sinica Treebank", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.9629389047622681}, {"text": "Sinica Treebank annotation", "start_pos": 94, "end_pos": 120, "type": "DATASET", "confidence": 0.9072388211886088}, {"text": "CTB", "start_pos": 163, "end_pos": 166, "type": "DATASET", "confidence": 0.976253092288971}]}, {"text": "For POS tags, all words are divided into 8 basic categories including nouns, verbs, prepositions...", "labels": [], "entities": [{"text": "POS tags", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.8385239243507385}]}, {"text": "Each category contains several sub-classes.", "labels": [], "entities": []}, {"text": "For nouns, person names are annotated as Nb and organizations are annotated as Nc while in CTB, these two types of nouns are all tagged as NR.", "labels": [], "entities": [{"text": "CTB", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9414492249488831}]}, {"text": "Moreover, some of the sub-classes are further distinguished with suffix such as VC.", "labels": [], "entities": [{"text": "VC", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9817073941230774}]}, {"text": "Non-terminals are annotated in a similar manner.", "labels": [], "entities": []}, {"text": "In Sinica Treebank, all non-terminals belong to one of the 7 basic classes including noun phrase, verb phrase, preposition phrase...", "labels": [], "entities": [{"text": "Sinica Treebank", "start_pos": 3, "end_pos": 18, "type": "DATASET", "confidence": 0.6815775781869888}]}, {"text": "Each of the class contains several sub-classes which might be further distinguished by some suffixes.", "labels": [], "entities": []}, {"text": "The Sinica Treebank annotation does make its labels carry more information.", "labels": [], "entities": [{"text": "Sinica Treebank annotation", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.847119927406311}]}, {"text": "However, the data sparse problem caused by the fine grained annotation prevents the Berkeley parser from learning a high performance model.", "labels": [], "entities": []}, {"text": "To examine the effect of decreasing the number of label types on parsing performance, we carried on another two experiments.", "labels": [], "entities": [{"text": "parsing", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.9788705110549927}]}, {"text": "In the first experiment, we removed all suffixes from the POS tags and non-terminal labels of Sinica Treebank.", "labels": [], "entities": [{"text": "POS tags and non-terminal labels of Sinica Treebank", "start_pos": 58, "end_pos": 109, "type": "DATASET", "confidence": 0.7103336453437805}]}, {"text": "For example, removing suffix from V_11 yields V and removing suffix from VC[+NEG] yields VC.", "labels": [], "entities": [{"text": "VC", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9169377684593201}]}, {"text": "After removing suffixes, the number of different POS tags decreased to 55.", "labels": [], "entities": []}, {"text": "For the second experiments, in addition to remove all suffixes, we also maps all non-terminal labels to one of the seven phrase labels including NP, VP, GP, PP, XP, DM and S. These labels are used to measure parsing performance by the official backoff task evaluation metrics.", "labels": [], "entities": [{"text": "parsing", "start_pos": 208, "end_pos": 215, "type": "TASK", "confidence": 0.9620386958122253}]}, {"text": "The mapping procedure is conducted according to the first letter of the non-terminal label of the Sinica Treebank.", "labels": [], "entities": [{"text": "Sinica Treebank", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.8852899372577667}]}, {"text": "That is, nonterminal labels with the first letter 'N' are all mapped to NP and labels with first letter 'V' are all mapped to VP \u2026.", "labels": [], "entities": []}, {"text": "Parsing performance with different label set Parsing performances are shown in table 3.", "labels": [], "entities": []}, {"text": "\"RAW\" denotes the performance achieved on Sinica Treebank without any treatment.", "labels": [], "entities": [{"text": "RAW", "start_pos": 1, "end_pos": 4, "type": "METRIC", "confidence": 0.9838176965713501}, {"text": "Sinica Treebank", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.7972699105739594}]}, {"text": "\"RMS\" denotes parsing performance achieved when label suffixes are removed.", "labels": [], "entities": [{"text": "RMS", "start_pos": 1, "end_pos": 4, "type": "METRIC", "confidence": 0.8645144104957581}, {"text": "parsing", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.9560264348983765}]}, {"text": "\"RMSM\" denotes parsing performance when both label suffixes are removed and non-terminals are mapped.", "labels": [], "entities": [{"text": "RMSM", "start_pos": 1, "end_pos": 5, "type": "METRIC", "confidence": 0.773926854133606}, {"text": "parsing", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.9629070162773132}]}, {"text": "For these settings, the best parsing performances in terms of F1 score are all achieved on the 4-th split-merge round and we omit the performance achieved on other rounds.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9498595595359802}, {"text": "F1 score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9807714223861694}]}, {"text": "From table 3, we can see that on the one hand, 'RMS' improves parsing performance about 0.35 F1 points.", "labels": [], "entities": [{"text": "RMS", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.6115924715995789}, {"text": "parsing", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.9606131315231323}, {"text": "F1", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9988195300102234}]}, {"text": "This demonstrates that removing suffix to reduce the number of POS tag and nonterminal labels does to some degree helpful.", "labels": [], "entities": []}, {"text": "On the other hand, aggressively maps non-terminal labels to only seven basic phrase labels hurts parsing performance dramatically.", "labels": [], "entities": []}, {"text": "Here, one may argue that these performances are not directly comparable since the gold development set for each setting are not annotated with the same label set.", "labels": [], "entities": []}, {"text": "That is, scores for \"RAW\" setting is calculated against the development set without any treatment while scores for \"RMS\" setting is calculated against the development set which non-terminal labels' suffix are removed.", "labels": [], "entities": []}, {"text": "For \"RMSM\", the gold development set only contains seven basic phrase labels.", "labels": [], "entities": [{"text": "RMSM\"", "start_pos": 5, "end_pos": 10, "type": "TASK", "confidence": 0.8631285130977631}]}, {"text": "To handle this issue, we also mapped the parsing results of \"RAW\" and \"RMS\" to the seven basic phrase labels and the performance are listed in table 4.", "labels": [], "entities": []}, {"text": "We see that \"RMS_B\" still yields the best performance.", "labels": [], "entities": [{"text": "RMS_B", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.6933584610621134}]}, {"text": "The last issue we examine is tagging accuracy on parsing performance.", "labels": [], "entities": [{"text": "tagging", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9738092422485352}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9586707949638367}, {"text": "parsing", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.9703457355499268}]}, {"text": "To see this, we use the model trained with \"RMS\" setting to parse the development set where sentences are assigned with gold standard POS tags.", "labels": [], "entities": []}, {"text": "The result is that parsing precision, recall and F1 boosted to 84.95, 84.44 and 84.69, respectively.", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9619890451431274}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.989120364189148}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9998107552528381}, {"text": "F1", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9998748302459717}]}, {"text": "These results illustrate that improving tagging accuracy can significantly boosting parsing performance on the Sinica Treebank.", "labels": [], "entities": [{"text": "tagging", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.9638882875442505}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9348031282424927}, {"text": "parsing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9709417819976807}, {"text": "Sinica Treebank", "start_pos": 111, "end_pos": 126, "type": "DATASET", "confidence": 0.9015113115310669}]}, {"text": "By inspecting the parsing errors which also evolve at least one tagging error, we found that one of the major sources of parsing errors is caused by Ncd-Ng ambiguity.", "labels": [], "entities": []}, {"text": "Parsing performance where nonterminal labels of the guess trees of \"RAW\" and \"RMS\" are mapped to seven basic phrase labels two POS tags denote position information such as \u5916 /'outside', \u4e2d /'in'.", "labels": [], "entities": []}, {"text": "For example \u6821\u5916 /'outside the school', \u5ead\u9662\u4e2d/'in the yard'.", "labels": [], "entities": []}, {"text": "However, the two tags show quite different syntactic behavior.", "labels": [], "entities": []}, {"text": "Ng always coupled with NP or VP and they together forms a GP while Ncd always comes after a NP or a sequence of nouns to form another NP as shown in Figure1.", "labels": [], "entities": []}, {"text": "Another major source of errors comes from noun-verb ambiguity which is also one of the most difficulty issues for tagging and parsing simplified Chinese.", "labels": [], "entities": [{"text": "tagging and parsing simplified Chinese", "start_pos": 114, "end_pos": 152, "type": "TASK", "confidence": 0.8063979625701905}]}, {"text": "Such tagging error would results in a NP incorrectly analyzed as a VP and vice versa.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Parsing performance on the develop- ment set. #Split is the number of split-merge  round", "labels": [], "entities": [{"text": "Split", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9878228902816772}]}, {"text": " Table 3. Parsing performance with different label  set", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.966658890247345}]}, {"text": " Table 4. Parsing performance where non- terminal labels of the guess trees of \"RAW\" and  \"RMS\" are mapped to seven basic phrase labels", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9543657898902893}]}]}