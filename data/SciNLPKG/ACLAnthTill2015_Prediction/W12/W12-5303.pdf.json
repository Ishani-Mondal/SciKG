{"title": [{"text": "Categorical Probability Proportion Difference (CPPD): A Feature Selection Method for Sentiment Classification", "labels": [], "entities": [{"text": "Categorical Probability Proportion Difference (CPPD)", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7602615186146328}, {"text": "Sentiment Classification", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.9735559523105621}]}], "abstractContent": [{"text": "Sentiment analysis is to extract the opinion of the user from of the text documents.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9535317718982697}]}, {"text": "Sentiment classification using machine learning methods face problem of handling huge number of unique terms in a feature vector for the classification.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9366823434829712}]}, {"text": "Thus it is required to eliminate the irrelevant and noisy terms from the feature vector.", "labels": [], "entities": []}, {"text": "Feature selection methods reduce the feature size by selecting prominent features for better classification.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8217456638813019}]}, {"text": "In this paper, anew feature selection method namely Probability Proportion Difference (PPD) is proposed which is based on the probability of belongingness of a term to a particular class.", "labels": [], "entities": []}, {"text": "It is capable of removing irrelevant terms from the feature vector.", "labels": [], "entities": []}, {"text": "Further, a Categorical Probability Proportion Difference (CPPD) feature selection method is proposed based on Probability Proportion Difference (PPD) and Categorical Proportion Difference (CPD).", "labels": [], "entities": [{"text": "Categorical Probability Proportion Difference (CPPD) feature selection", "start_pos": 11, "end_pos": 81, "type": "TASK", "confidence": 0.7572800185945299}]}, {"text": "CPPD feature selection method is able to select the features which are relevant and capable of discriminating the class.", "labels": [], "entities": [{"text": "CPPD feature selection", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6852576037247976}]}, {"text": "The performance of the proposed feature selection methods is compared with the CPD method and Information Gain (IG) method which has been identified as one of the best feature selection method for sentiment classification.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7590693831443787}, {"text": "sentiment classification", "start_pos": 197, "end_pos": 221, "type": "TASK", "confidence": 0.9489297270774841}]}, {"text": "Experimentation of proposed feature selection methods was performed on two standard datasets viz.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7457235157489777}]}, {"text": "movie review dataset and product review (i.e. book) dataset.", "labels": [], "entities": [{"text": "movie review dataset", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.777739961942037}]}, {"text": "Experimental results show that proposed CPPD feature selection method outperforms other feature selection method for sentiment classification.", "labels": [], "entities": [{"text": "CPPD feature selection", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7153042356173197}, {"text": "sentiment classification", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.9556751847267151}]}], "introductionContent": [{"text": "With the rapid growth of web technology, people now express their opinion, experience, attitude, feelings, and emotions on the web.", "labels": [], "entities": []}, {"text": "So, it has increased the demand of processing, organizing, and analyzing the web content to know the opinion of the users).", "labels": [], "entities": []}, {"text": "An automatic sentiment text classification means to identify the sentiment orientation of the text documents i.e. positive or negative.", "labels": [], "entities": [{"text": "sentiment text classification", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.7597491145133972}]}, {"text": "It is important for users as well as companies to know the opinion of users, for example review for electronic products like laptop, car, movies etc.", "labels": [], "entities": []}, {"text": "can be beneficial for users to take decision on which product to purchase and for companies to improve and market their products.", "labels": [], "entities": []}, {"text": "Various researchers have applied machine learning algorithms for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.9676481485366821}]}, {"text": "One of the major problems in sentiment classification is to deal with huge number of features used for describing text documents, which produces hurdles to machine learning methods in determining the sentiment orientation of the document.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.9660657644271851}]}, {"text": "Thus, it is required to select only prominent features which contribute majorly in the identification of sentiment of the document.", "labels": [], "entities": [{"text": "identification of sentiment of the document", "start_pos": 87, "end_pos": 130, "type": "TASK", "confidence": 0.8903084695339203}]}, {"text": "The aim of feature selection methods is to produce the reduced feature set which is capable of determining sentiment orientation of the document by eliminating irrelevant and noisy features.", "labels": [], "entities": [{"text": "determining sentiment orientation", "start_pos": 95, "end_pos": 128, "type": "TASK", "confidence": 0.6000606815020243}]}, {"text": "Various feature selection methods has been proposed for selecting predominating features for sentiment classification, for example Information Gain (IG), Mutual Information (MI), Chi square (CHI), Gain Ratio (GR), Document Frequency (DF) etc.).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.9415635168552399}, {"text": "Chi square (CHI)", "start_pos": 179, "end_pos": 195, "type": "METRIC", "confidence": 0.8582902669906616}]}, {"text": "In the proposed approach, feature selection methods are used for improving the performance of the machine learning method.", "labels": [], "entities": []}, {"text": "Initially, binary weighting scheme is used to represent the review documents, and then various feature selection methods are applied to reduce the feature set size.", "labels": [], "entities": []}, {"text": "Further, machine learning methods are applied to the reduced and prominent feature set.", "labels": [], "entities": []}, {"text": "Two new feature selection methods i.e. PPD and CPPD are proposed for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.9702995419502258}]}, {"text": "2. Compared the performance of proposed feature selection methods on two different standard datasets of different domains.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7131344527006149}]}, {"text": "The paper is organized as follows: A brief discussion of the related work is given in Section 2.", "labels": [], "entities": []}, {"text": "Feature selection methods used for sentiment classification are discussed in Section 3.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.707081139087677}, {"text": "sentiment classification", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.9810893833637238}]}, {"text": "Dataset, Experimental setup and results are discussed in Section 4.", "labels": [], "entities": [{"text": "Dataset", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7835209965705872}]}, {"text": "Finally, conclusions and future work is described.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of the most popular publically available standard movie review dataset is used to test the proposed feature selection methods).", "labels": [], "entities": [{"text": "movie review dataset", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.6640354792277018}, {"text": "feature selection", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.6877159029245377}]}, {"text": "This standard dataset, known as Cornell Movie Review Dataset is consisting of 2000 reviews that contain 1000 positive and 1000 negative labeled reviews.", "labels": [], "entities": [{"text": "Cornell Movie Review Dataset", "start_pos": 32, "end_pos": 60, "type": "DATASET", "confidence": 0.9724971503019333}]}, {"text": "In addition, product review dataset (book reviews) consisting amazon products reviews has also been used).", "labels": [], "entities": []}, {"text": "This dataset contains 1000 positive and 1000 negative labeled book reviews.", "labels": [], "entities": []}, {"text": "Documents are initially pre-processed as follows: (i) Negation handling, \"NOT_\" is added to every words occurring after the negation word (no, not, isn't, can't etc.) in the sentence.", "labels": [], "entities": [{"text": "Negation handling", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.8844278454780579}, {"text": "NOT", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9665843844413757}]}, {"text": "Since, a negation word inverts the sentiment of the sentence).", "labels": [], "entities": []}, {"text": "(ii) Terms which are occurring in less than 2 documents are removed from the feature set.", "labels": [], "entities": []}, {"text": "The feature vector generated after pre-processing is further used for the classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.9675624966621399}]}, {"text": "Binary weighting scheme is used for representing text since it has been proved the best method for sentiment classification).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.9353137314319611}]}, {"text": "Among various machine learning algorithms Support Vector Machine (SVM) and Na\u00efve Bayes (NB) classifiers are mostly used for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.9510383903980255}]}, {"text": "So, in our experiments, SVM and NB are used for classifying review documents into positive or negative class.", "labels": [], "entities": []}, {"text": "Evaluation of classification results is done by 10 folds cross validation.", "labels": [], "entities": []}, {"text": "Linear SVM and Na\u00efve Bayes are used for all the experiments with default setting in weka machine learning tool (WEKA).", "labels": [], "entities": [{"text": "weka machine learning tool (WEKA)", "start_pos": 84, "end_pos": 117, "type": "DATASET", "confidence": 0.8485411150114877}]}], "tableCaptions": [{"text": " Table 2. Unigram feature set without any feature selection method is taken as baseline accuracy.  It is observed from the experiments that all the feature selection methods improve the  performance of both the classifiers (SVM and NB) as compared to baseline performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.8435715436935425}]}]}