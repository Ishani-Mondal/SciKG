{"title": [{"text": "Combining the Best of Two Worlds: A Hybrid Approach to Multilingual Coreference Resolution", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe our system for the CoNLL-2012 shared task, which seeks to model corefer-ence in OntoNotes for English, Chinese, and Arabic.", "labels": [], "entities": []}, {"text": "We adopt a hybrid approach to coreference resolution, which combines the strengths of rule-based methods and learning-based methods.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.9712863266468048}]}, {"text": "Our official combined score overall three languages is 56.35.", "labels": [], "entities": []}, {"text": "In particular , our score on the Chinese test set is the best among the participating teams.", "labels": [], "entities": [{"text": "Chinese test set", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.9750643968582153}]}], "introductionContent": [{"text": "The CoNLL-2012 shared task extends last year's task on coreference resolution from a monolingual to a multilingual setting ().", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.968465268611908}]}, {"text": "Unlike the SemEval-2010 shared task on Coreference Resolution in Multiple Languages (, which focuses on coreference resolution in European languages, the CoNLL shared task is arguably more challenging: it focuses on three languages that come from very different language families, namely English, Chinese, and Arabic.", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.8928072452545166}, {"text": "coreference resolution", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.8975476622581482}]}, {"text": "We designed a system for resolving references in all three languages.", "labels": [], "entities": []}, {"text": "Specifically, we participated in four tracks: the closed track for all three languages, and the open track for Chinese.", "labels": [], "entities": []}, {"text": "In comparison to last year's participating systems, our resolver has two distinguishing characteristics.", "labels": [], "entities": [{"text": "resolver", "start_pos": 56, "end_pos": 64, "type": "TASK", "confidence": 0.9750961661338806}]}, {"text": "First, unlike last year's resolvers, which adopted either a rulebased method or a learning-based method, we adopt a hybrid approach to coreference resolution, attempting to combine the strengths of both methods.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.9504412114620209}]}, {"text": "Second, while last year's resolvers did not exploit genrespecific information, we optimize our system's parameters with respect to each genre.", "labels": [], "entities": []}, {"text": "Our decision to adopt a hybrid approach is motivated by the observation that rule-based methods and learning-based methods each have their unique strengths.", "labels": [], "entities": []}, {"text": "As shown by the Stanford coreference resolver), the winner of last year's shared task, many coreference relations in OntoNotes can be identified using a fairly small set of simple hand-crafted rules.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.6959129571914673}]}, {"text": "On the other hand, our prior work on machine learning for coreference resolution suggests that coreference-annotated data can be profitably exploited to (1) induce lexical features) and (2) optimize system parameters with respect to the desired coreference evaluation measure.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.9569990038871765}]}, {"text": "Our system employs a fairly standard architecture, performing mention detection prior to coreference resolution.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.794469028711319}, {"text": "coreference resolution", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.9555580019950867}]}, {"text": "As we will see, however, the parameters of these two components are optimized jointly with respect to the desired evaluation measure.", "labels": [], "entities": []}, {"text": "In the rest of this paper, we describe the mention detection component (Section 2) and the coreference resolution component (Section 3), show how their parameters are jointly optimized (Section 4), and present evaluation results on the development set and the official test set (Section 5).", "labels": [], "entities": [{"text": "mention detection", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.658438503742218}, {"text": "coreference resolution", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.9113101959228516}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Mention detection results on the development set  obtained prior to coreference resolution.", "labels": [], "entities": [{"text": "Mention detection", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7867316901683807}, {"text": "coreference resolution", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.9637883901596069}]}, {"text": " Table 3: Results on the development set with optimal parameter values.", "labels": [], "entities": []}, {"text": " Table 4: Official results on the test set.", "labels": [], "entities": []}, {"text": " Table 5: Supplementary results on the test set obtained using gold mention boundaries and predicted parse trees.", "labels": [], "entities": [{"text": "Supplementary", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.8593083024024963}]}, {"text": " Table 6: Supplementary results on the test set obtained using gold mentions and predicted parse trees.", "labels": [], "entities": [{"text": "Supplementary", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.8994142413139343}]}, {"text": " Table 7: Optimal parameter values.", "labels": [], "entities": []}]}