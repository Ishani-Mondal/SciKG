{"title": [{"text": "Parameter estimation under uncertainty with Simulated Annealing applied to an ant colony based probabilistic WSD algorithm", "labels": [], "entities": [{"text": "Parameter estimation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7833430171012878}, {"text": "Simulated Annealing", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.820156991481781}]}], "abstractContent": [{"text": "In this article we propose a method based on simulated annealing for the parameter estimation of probabilistic algorithms, where the solution provided by the algorithm can vary from execution to execution.", "labels": [], "entities": []}, {"text": "Such algorithms are often very interesting to solve complex combinatorial problems, yet they involve many parameters that can be difficult to estimate manually due to their randomized output.", "labels": [], "entities": []}, {"text": "We applied and evaluated a method for the parameter estimation of such algorithms and applied it for an Ant Colony Algorithm for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.583983302116394}]}, {"text": "For the evaluation, we used the Semeval 2007 Task 7 corpus.", "labels": [], "entities": [{"text": "Semeval 2007 Task 7 corpus", "start_pos": 32, "end_pos": 58, "type": "DATASET", "confidence": 0.8424370050430298}]}, {"text": "We split the corpus and took in turn one text as a training corpus and the four remaining texts as a test corpus.", "labels": [], "entities": []}, {"text": "We tuned the parameters with an increasing number of sentences from the training text in order to estimate the quantity of data necessary to obtain an efficient and general set of parameters.", "labels": [], "entities": []}, {"text": "We found that the results greatly depend on the nature of the text, even a very small amount of training sentences can lead to good results if the text has the right properties.", "labels": [], "entities": []}, {"text": "R\u00c9SUM\u00c9 (French) Estimation de param\u00e8tres \u00e0 base de Recuit Simul\u00e9 sous incertitude appliqu\u00e9e \u00e0 un algo-rithmes \u00e0 colonies de fourmis probabiliste.", "labels": [], "entities": [{"text": "R\u00c9SUM\u00c9", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6605909466743469}, {"text": "Estimation", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9246411323547363}]}, {"text": "Nous proposons une m\u00e9thode bas\u00e9e sur un Recuit Simul\u00e9 pour l'estimation de param\u00e8tres pour des algorithmes probabilistes o\u00f9 les solutions g\u00e9n\u00e9r\u00e9es varient.", "labels": [], "entities": []}, {"text": "Ces algorithmes sont souvent tr\u00e8s int\u00e9ressant pour la r\u00e9solution de probl\u00e8mes combinatoires complexes, mais ils requi\u00e8rent de nombreux param\u00e8tres pouvant \u00eatre difficiles \u00e0 estimer manuellement \u00e0 cause de la nature al\u00e9atoire des solutions.", "labels": [], "entities": []}, {"text": "Nous avons appliqu\u00e9sPlus sp\u00e9cifiquement, nous appliquons et \u00e9valuons cette m\u00e9thode \u00e0 pour estimer les param\u00e8tres de tels algorimes et l'appliquons \u00e0 un Algorithme \u00e0 Colonies de Fourmis pour la d\u00e9sambigu\u00efsation lexicale.", "labels": [], "entities": []}, {"text": "Pour l'\u00e9valuation, nous avons utilis\u00e9 le corpus de Semeval 2007 T\u00e2che 7.", "labels": [], "entities": [{"text": "corpus de Semeval 2007 T\u00e2che 7", "start_pos": 41, "end_pos": 71, "type": "DATASET", "confidence": 0.8368421594301859}]}, {"text": "Nous avons repectivement s\u00e9par\u00e9 un texte comme corpus d'entra\u00eenement et les quatre autres comme corpus de test.", "labels": [], "entities": []}, {"text": "Nous estimons les param\u00e8tres pour un nombre croissant de phrases pour d\u00e9terminer combien de donn\u00e9es sont n\u00e9cessaires pour obtenir un ensemble de valeurs de param\u00e8tres g\u00e9n\u00e9rales et efficaces.", "labels": [], "entities": []}, {"text": "Nous concluons que la qualit\u00e9 des r\u00e9sultats d\u00e9pends de la nature des textes.", "labels": [], "entities": []}, {"text": "M\u00eame des petites quantit\u00e9s de de phrases peuvent suffir \u00e0 obtenir de bons r\u00e9sultats, du moment que le texte \u00e0 les bonnes propri\u00e8t\u00e9s.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is a very difficult yet central task in, that pertains to the labelling of the words of a text with the senses that most closely match the context.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7991973012685776}]}, {"text": "Numerous approaches exist to tackle WSD, yet many of these methods have in common a sizeable complexity, reflected by a number of parameters that need to be tuned in order to obtain the best performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9868612885475159}]}, {"text": "Depending on the number of parameters, it can be very difficult fora human to directly estimate the optimal parameters.", "labels": [], "entities": []}, {"text": "Even then, it remains a question of guesswork with no guarantee of success.", "labels": [], "entities": []}, {"text": "This issue is all the more salient with algorithms and models that typically involve many parameters upon which the results greatly depend.", "labels": [], "entities": []}, {"text": "Such algorithms for WSD include Neural Networks (, Simulated Annealing (SA) (, Genetic Algorithms (GA) ( and Ant Colony Algorithms (ACA) (, and many other machine learning methods.", "labels": [], "entities": [{"text": "WSD", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9863183498382568}]}, {"text": "In such approaches, the values of the parameters are paramount.", "labels": [], "entities": []}, {"text": "They are what make the algorithms work for specific applications such as WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.856878936290741}]}, {"text": "Yet, the authors of the aforementioned articles give little insight as to how the parameters are determined.", "labels": [], "entities": []}, {"text": "They only provide the \"best\" values.", "labels": [], "entities": []}, {"text": "Although atrial and error approach is a good start when devising an algorithm or tailoring it to anew application, it makes the adaptation and the scalability of such systems an issue.", "labels": [], "entities": []}, {"text": "Naturally, for well known and understood algorithms such as Simulated Annealing, there are some theoretical criteria to select the parameters.", "labels": [], "entities": [{"text": "Simulated Annealing", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8690284788608551}]}, {"text": "However, the process remains to be repeated for every new setting and can be tedious to perform.", "labels": [], "entities": []}, {"text": "Furthermore, even with few parameters, evaluating all possible parameter configurations is a prohibitively long process.", "labels": [], "entities": []}, {"text": "For example, in the case of a stochastic algorithm with 10 parameters that have 20 possible values each, assuming the algorithm needs to be run 100 times (due to its stochastic nature) and that one execution takes on average 30 seconds, the time necessary to evaluate all parameter combinations is 20 \u00b7 100 \u00d7 30 = 3 \u00d7 10 15 s which is almost 1 billion years!", "labels": [], "entities": []}, {"text": "Thus, the advantages of considering automated or adaptive heuristic approaches to the estimation of parameters are clear.", "labels": [], "entities": []}, {"text": "In the case of a deterministic algorithm, many combinatorial optimisation methods can be used with little effort.", "labels": [], "entities": []}, {"text": "However, due to the complexity of WSD, many approaches are statistical or probabilistic in nature and the use classical combinatorial optimization heuristics becomes impossible.", "labels": [], "entities": [{"text": "WSD", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9540748596191406}]}, {"text": "Therefore, we adapt the classical simulated annealing algorithm to work for an uncertain objective function based on the ideas of, by using standard non-parametric statistical significance tests.", "labels": [], "entities": []}, {"text": "First, we present the tools and the metrics for the evaluation of WSD, followed by the description of the Ant Colony Algorithm considered in this article.", "labels": [], "entities": [{"text": "WSD", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.8536152243614197}]}, {"text": "Then, we briefly survey other approaches for parameter estimation under uncertainty and express the problem formally.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.6679145395755768}]}, {"text": "Subsequently, we present the general formulation or simulated annealing and different aspects pertaining to it and then present its adaptation to uncertain objective function.", "labels": [], "entities": []}, {"text": "We then present and analyse our experimental protocol and the results of the experiments.", "labels": [], "entities": []}, {"text": "Finally, we conclude on the results and draw some perspectives for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "The use of Precision and Recall constitutes the standard evaluation metrics for WSD systems.", "labels": [], "entities": [{"text": "Precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.990616500377655}, {"text": "Recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9436385631561279}, {"text": "WSD", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9754687547683716}]}, {"text": "Precision represents the number of correctly disambiguated words among all the attempted words, while Recall represents the number of correctly disambiguated words among all the ambiguous words in the document.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9426191449165344}, {"text": "Recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.8209342956542969}]}, {"text": "It is customary to compute the F1 score between Precision and Recall as 2\u00b7P\u00b7R  The principle used for the evaluation of the variant of simulated annealing considered for the estimation of parameters in an uncertain setting, was to consider the Semeval 2007 Task 7 annotated corpus.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9822031557559967}, {"text": "Precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9588614702224731}, {"text": "Recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9346221089363098}, {"text": "Semeval 2007 Task 7 annotated corpus", "start_pos": 244, "end_pos": 280, "type": "DATASET", "confidence": 0.7062382797400156}]}, {"text": "We split the corpus into a training and a test set in order to evaluate the parameter search with the Ant Colony Algorithm for WSD originally proposed in () and then further improved upon in ().", "labels": [], "entities": [{"text": "Ant", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9116582870483398}]}, {"text": "Furthermore, we want to determine exactly how much data was necessary to obtain a good set of parameter values.", "labels": [], "entities": []}, {"text": "Normally, in order to obtain a reliable training and thus parameters, it would be necessary to have a somewhat larger test set and to split it into several parts by randomly aggregating words to perform a k \u2212 fol d cross validation.", "labels": [], "entities": []}, {"text": "However, the Ant Colony algorithm is meant to work on a full text as it exploits its structure.", "labels": [], "entities": []}, {"text": "Notably, the order of the words and the continuity of sentences are very important.", "labels": [], "entities": [{"text": "continuity", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9898111820220947}]}, {"text": "Making cross-validation sets randomly would thus only create a very noisy training data.", "labels": [], "entities": []}, {"text": "Even just picking sentences at random still compromises the global coherence of the solutions found, and thus the parameters.", "labels": [], "entities": []}, {"text": "This is why we only considered a training on successively larger portions of our training corpus.", "labels": [], "entities": []}, {"text": "More specifically, the experiments were carried outwith 6,12,18,24,30 and 36 sentences in order from the first sentences to the full text in multiples of 6.", "labels": [], "entities": []}, {"text": "Furthermore, it is apparent that the nature of the training text itself has a big importance on the resulting parameters found.", "labels": [], "entities": []}, {"text": "It is necessary to use a text that is as general as possible in terms of the senses it uses and of its theme as well as lexically varied.", "labels": [], "entities": []}, {"text": "Even though, k \u2212 fol d cross validation cannot be applied directly in our experimental setting, we have decided to perform the experiment by taking, in turn, each text as a training corpus, with the remainder as a test corpus.", "labels": [], "entities": []}, {"text": "Thus, we may also study the effect of the text itself and its characteristics (type of text, average polysemy, etc).", "labels": [], "entities": []}, {"text": "The implementation of this uncertain simulated annealing, runs the ant colony algorithm and passes the results through to the scorer to obtain the F-score values.", "labels": [], "entities": [{"text": "F-score", "start_pos": 147, "end_pos": 154, "type": "METRIC", "confidence": 0.9972246885299683}]}, {"text": "For the stochastic loop, for every new candidate configuration, the ant colony algorithm is systematically run 50 times in order to guarantee a sufficient level of statistical significance.", "labels": [], "entities": []}, {"text": "The search was left to converge for each successive number of sentences and then, each resulting parameter values were tested over 100 executions of the ant colony algorithm on the test corpus in order to ascertain the quality of the parameters.", "labels": [], "entities": []}, {"text": "The results for each number of sentences are evaluated with relation to both classical lesk baseline and the baseline obtained with the parameters before the estimation (BL).", "labels": [], "entities": [{"text": "estimation (BL)", "start_pos": 158, "end_pos": 173, "type": "METRIC", "confidence": 0.92990542948246}]}, {"text": "We used standard statistical tests to guarantee the statistical significance of the comparisons.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Parameters of the Ant Colony Algorithm and their typical value-ranges", "labels": [], "entities": []}, {"text": " Table 3: Results in terms of the average F scor e and its standard deviation \u03c3 F s for a given set of  parameter values. Unless specified, the pairwise Tukey HSD test is significant with \u03b1 = 0.01.", "labels": [], "entities": [{"text": "standard deviation \u03c3 F s", "start_pos": 59, "end_pos": 83, "type": "METRIC", "confidence": 0.7871679067611694}, {"text": "Tukey HSD test", "start_pos": 153, "end_pos": 167, "type": "DATASET", "confidence": 0.5508162180582682}]}, {"text": " Table 4: Results in terms of the average F scor e and its standard deviation \u03c3 F s for the parameters  found with each number of sentences of training data.", "labels": [], "entities": [{"text": "F scor e", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.8664393226305643}, {"text": "standard deviation \u03c3 F s", "start_pos": 59, "end_pos": 83, "type": "METRIC", "confidence": 0.7573621988296508}]}]}