{"title": [], "abstractContent": [{"text": "We introduce the first fully automatic, fully semantic frame based MT evaluation metric, MEANT, that outperforms all other commonly used automatic metrics in correlating with human judgment on translation adequacy.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9222870171070099}, {"text": "MEANT", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9783187508583069}]}, {"text": "Recent work on HMEANT, which is a human metric, indicates that machine translation can be better evaluated via semantic frames than other evaluation paradigms, requiring only minimal effort from monolingual humans to annotate and align semantic frames in the reference and machine translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7794736325740814}]}, {"text": "We propose a surprisingly effective Occam's razor automation of HMEANT that combines standard shallow semantic parsing with a simple maximum weighted bipartite matching algorithm for aligning semantic frames.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.7602759897708893}]}, {"text": "The matching criterion is based on lexical similarity scoring of the semantic role fillers through a simple context vector model which can readily be trained using any publicly available large monolingual corpus.", "labels": [], "entities": []}, {"text": "Sentence level correlation analysis, following standard NIST MetricsMATR protocol, shows that this fully automated version of HMEANT achieves significantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR , PER, CDER, WER, or TER.", "labels": [], "entities": [{"text": "Sentence level correlation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8093762000401815}, {"text": "Kendall correlation", "start_pos": 163, "end_pos": 182, "type": "METRIC", "confidence": 0.9370656311511993}, {"text": "BLEU", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.9982469081878662}, {"text": "NIST", "start_pos": 224, "end_pos": 228, "type": "DATASET", "confidence": 0.9458188414573669}, {"text": "METEOR", "start_pos": 230, "end_pos": 236, "type": "METRIC", "confidence": 0.7303024530410767}, {"text": "PER", "start_pos": 239, "end_pos": 242, "type": "METRIC", "confidence": 0.9148845076560974}, {"text": "WER", "start_pos": 250, "end_pos": 253, "type": "METRIC", "confidence": 0.9103310704231262}, {"text": "TER", "start_pos": 258, "end_pos": 261, "type": "METRIC", "confidence": 0.9799553751945496}]}, {"text": "Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually.", "labels": [], "entities": [{"text": "semantic frame alignment", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.6497301459312439}]}, {"text": "Despite its high performance, fully automated MEANT is still able to preserve HMEANT's virtues of simplicity, repre-sentational transparency, and inexpensiveness.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 46, "end_pos": 51, "type": "TASK", "confidence": 0.4619869291782379}]}], "introductionContent": [{"text": "We introduce the first fully automatic semantic-framebased MT evaluation metric capable of outperforming all other commonly used automatic metrics like BLEU, NIST, METEOR, PER, CDER, WER, and TER for evaluating translation adequacy.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.91895592212677}, {"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9977676868438721}, {"text": "METEOR", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.9400253891944885}, {"text": "PER", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.9274192452430725}, {"text": "WER", "start_pos": 183, "end_pos": 186, "type": "METRIC", "confidence": 0.903160035610199}, {"text": "TER", "start_pos": 192, "end_pos": 195, "type": "METRIC", "confidence": 0.9936071634292603}]}, {"text": "This work, MEANT, can be seen as a fully automated version of HMEANT, which is a human metric, introduced by.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9779524207115173}, {"text": "HMEANT", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.7767187356948853}]}, {"text": "Despite its high performance, MEANT is still able to preserve HMEANT's virtues of Occam's razor simplicity, representational transparency, and inexpensiveness.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.7385022640228271}]}, {"text": "For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU), NIST), METEOR (Banerjee and), PER (), CDER (), WER (), and TER).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9654732048511505}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.998816967010498}, {"text": "NIST", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.6214709877967834}, {"text": "METEOR", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9965309500694275}, {"text": "PER", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9697051644325256}, {"text": "WER", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.9677855372428894}, {"text": "TER", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.9973582625389099}]}, {"text": "In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 202, "end_pos": 214, "type": "TASK", "confidence": 0.9107366800308228}]}, {"text": "Despite the fact that HMEANT, a human metric recently proposed by, was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly into SMT training methods, due to the non-automatic processes of (1) semantic parsing and (2) aligning semantic frames.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 241, "end_pos": 253, "type": "TASK", "confidence": 0.9190047681331635}, {"text": "semantic parsing", "start_pos": 305, "end_pos": 321, "type": "TASK", "confidence": 0.7224062085151672}]}, {"text": "In this paper we introduce an automatic metric in which both the semantic parsing and the alignment of semantic frames are fully automated.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7043401151895523}]}, {"text": "Our aim is to show that even with full automation, this new metric still outperforms all the previous automatic metrics mentioned, thus providing a foundation for future incorporation into the training of SMT to drive system improvements in providing more adequate translation output.", "labels": [], "entities": [{"text": "SMT", "start_pos": 205, "end_pos": 208, "type": "TASK", "confidence": 0.9906959533691406}]}, {"text": "N-gram oriented automatic MT evaluation metrics like BLEU perform well at capturing translation fluency, and ranking overall systems with respect to each other when their scores are averaged over entire documents or corpora.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.885016918182373}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9899750351905823}, {"text": "capturing translation fluency", "start_pos": 74, "end_pos": 103, "type": "TASK", "confidence": 0.7014313141504923}]}, {"text": "However, they do not fare so well in ranking translations of individual sentences.", "labels": [], "entities": [{"text": "ranking translations of individual sentences", "start_pos": 37, "end_pos": 81, "type": "TASK", "confidence": 0.8270896196365356}]}, {"text": "As MT systems improve, the n-gram based evaluation metrics have begun to show their limits.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9893359541893005}]}, {"text": "State-of-the-art MT systems are often able to output translations containing roughly the correct words, while failing to convey important aspects of the meaning of the input sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9811976552009583}]}, {"text": "Cases where BLEU strongly disagrees with human judgment of translation quality were reported in large scale MT evaluation tasks by and.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9945682287216187}, {"text": "translation quality", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7823887765407562}, {"text": "MT evaluation tasks", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.9270231326421102}]}, {"text": "Motivated by the goal of addressing the weaknesses of n-gram oriented automatic MT evaluation metrics at evaluating translation adequacy, the HMEANT metric assesses translation utility by matching the basic event structure-\"who did what to whom, when, where and why\")-representing the central meaning conveyed by sentences.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.904311329126358}]}, {"text": "As mentioned above, however, HMEANT requires humans to manually annotate semantic frames in the reference and machine translations, and then to align the semantic frames-making it difficult to incorporate HMEANT as an objective function in the MT system training, evaluating, and optimizing cycle.", "labels": [], "entities": [{"text": "MT system training", "start_pos": 244, "end_pos": 262, "type": "TASK", "confidence": 0.8757631182670593}]}, {"text": "We argue in this paper that both the human semantic parsing and the semantic frame alignment tasks performed within HMEANT can be successfully automated to produce a state-of-the-art automatic metric.", "labels": [], "entities": [{"text": "human semantic parsing", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.6574905117352804}, {"text": "semantic frame alignment", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.6282412111759186}]}, {"text": "Moreover, we show that the spirit of Occam's razor can be preserved even for the semantic frame alignment, by demonstrating the effectiveness of a simple maximum weighted bipartite matching algorithm based on the lexical similarity between semantic frames.", "labels": [], "entities": [{"text": "semantic frame alignment", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.6742111643155416}]}, {"text": "In addition, we show empirically that performing this semantic frame alignment automatically tends to be just as good as performing it manually.", "labels": [], "entities": [{"text": "semantic frame alignment", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.6412906547387441}]}, {"text": "Our results indicate that MEANT, the fully automatic version of HMEANT, achieves levels of correlation with human adequacy judgment (in our experiments, approximately 0.37) which significantly outperforms the commonly used automatic metrics BLEU, NIST, METEOR, PER, CDER, WER, and TER (in our experiments, ranging between 0.20 and 0.29).", "labels": [], "entities": [{"text": "MEANT", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9718577861785889}, {"text": "BLEU", "start_pos": 241, "end_pos": 245, "type": "METRIC", "confidence": 0.9001205563545227}, {"text": "METEOR", "start_pos": 253, "end_pos": 259, "type": "METRIC", "confidence": 0.8520758152008057}, {"text": "PER", "start_pos": 261, "end_pos": 264, "type": "METRIC", "confidence": 0.934440016746521}, {"text": "WER", "start_pos": 272, "end_pos": 275, "type": "METRIC", "confidence": 0.9678887724876404}, {"text": "TER", "start_pos": 281, "end_pos": 284, "type": "METRIC", "confidence": 0.9977801442146301}]}, {"text": "2 Related Work 2.1 Automatic lexical similarity based metrics BLEU () remains the most widely used MT evaluation metric despite the fact that a number of large scale meta-evaluations) report cases where it strongly disagrees with human judgments of translation accuracy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.8870871067047119}, {"text": "MT evaluation", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.8967556059360504}]}, {"text": "Other lexical similarity based automatic MT evaluation metrics, like NIST), ME-TEOR (), PER (), CDER (), WER (), and TER (), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumptionthat a good translation is one that shares the same lexical choices as the reference translationis not justified semantically.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.9179430305957794}, {"text": "NIST", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.8952707052230835}, {"text": "PER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9721588492393494}, {"text": "WER", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9847026467323303}, {"text": "TER", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9958810806274414}]}, {"text": "Lexical similarity does not adequately reflect similarity in meaning.", "labels": [], "entities": [{"text": "Lexical similarity", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8281281292438507}]}, {"text": "Generating a translation that contains roughly the correct words maybe necessary-but is far from sufficientto preserve the essence of the meaning.", "labels": [], "entities": []}, {"text": "We argue that a translation metric that reflects meaning similarity needs to be based on similarity of semantic structure, and not merely flat lexical similarity.", "labels": [], "entities": []}], "datasetContent": [{"text": "Like HMEANT, our guiding principle is that a good translation is one that is useful, in the sense that human readers may successfully understand at least the basic event structurewho did what to whom, when, where and why)representing the central meaning of the source utterances.", "labels": [], "entities": []}, {"text": "Whereas HMEANT measures this using a f-score of correctly translated semantic roles in MT output that are annotated and compared by monolingual human annotators, MEANT automates HMEANT as follows (the differences from HMEANT are italicized): 1.", "labels": [], "entities": [{"text": "MT output", "start_pos": 87, "end_pos": 96, "type": "TASK", "confidence": 0.8639799058437347}]}, {"text": "Apply an automatic shallow semantic parser on both the references and MT output.", "labels": [], "entities": []}, {"text": "2. Apply maximum weighted bipartite matching algorithm to align the semantic frames between the references and MT output by the lexical similarity of the predicates.", "labels": [], "entities": []}, {"text": "3. For each pair of aligned semantic frames, (a) Lexical similarity scores determine the similarity of the semantic role fillers.", "labels": [], "entities": []}, {"text": "(b) Apply maximum weighted bipartite matching algorithm to align the semantic role fillers between the reference and MT output according to their lexical similarity.", "labels": [], "entities": []}, {"text": "4. Compute the weighted f-score over the matching role labels of these aligned predicates and role fillers.", "labels": [], "entities": []}, {"text": "To avoid the danger of aligning a token in one segment to excessive numbers of tokens in the other segment, we adopt a variant of competitive linking by.", "labels": [], "entities": []}, {"text": "Competitive linking is a greedy best-first word alignment algorithm.", "labels": [], "entities": [{"text": "Competitive linking", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7168254852294922}, {"text": "greedy best-first word alignment", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.7034783810377121}]}, {"text": "The rest of the experimental setup is the same as that used in Section 4.", "labels": [], "entities": []}, {"text": "shows that, surprisingly, judging semantic role filler similarity using only the aligned tokens (selected by competitive linking word alignment algorithm) does not help the correlation with human adequacy judgment.", "labels": [], "entities": [{"text": "judging semantic role filler similarity", "start_pos": 26, "end_pos": 65, "type": "TASK", "confidence": 0.6884560286998749}]}, {"text": "This is surprising as, intuitively, using only the aligned tokens should avoid the introduction of noise in judging the similarity between semantic role fillers because it avoids adding in similarities for words within semantic role fillers whose meanings are not close to each other.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Automatic semantic frame alignment of the MT2 output from figure 2, along with the automatic lexical similarity scoring  on translation correctness for each argument.", "labels": [], "entities": [{"text": "semantic frame alignment", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6160882711410522}, {"text": "automatic lexical similarity scoring", "start_pos": 93, "end_pos": 129, "type": "METRIC", "confidence": 0.6091670766472816}]}, {"text": " Table 3: Sentence-level correlation with human adequacy judgment on GALE-A (training) and GALE-B (testing) comparing all  commonly used MT evaluation metrics against our proposed new fully automatic semantic frame based MT evaluation metric  integrated with various lexical similarity scores between semantic role fillers: (a) BLEU, (b) METEOR, (c) cosine similarity and  (d) MinMax with mutual information.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 137, "end_pos": 150, "type": "TASK", "confidence": 0.9079745411872864}, {"text": "MT evaluation", "start_pos": 221, "end_pos": 234, "type": "TASK", "confidence": 0.8743041157722473}, {"text": "BLEU", "start_pos": 328, "end_pos": 332, "type": "METRIC", "confidence": 0.9976415634155273}, {"text": "METEOR", "start_pos": 338, "end_pos": 344, "type": "METRIC", "confidence": 0.9393466711044312}]}, {"text": " Table 4: Sentence-level correlation with human adequacy judg- ment on GALE-A (training) and GALE-B (testing) for aligning  sematnic frame automatically and manually.", "labels": [], "entities": [{"text": "GALE-A", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.8909812569618225}, {"text": "GALE-B", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.719779372215271}]}, {"text": " Table 5: Sentence-level correlation with human adequacy judg- ments on GALE-A (training set) and GALE-B (testing set) for  aligning semantic frames using predicate only vs. using all se- mantic role fillers aggregated by (1) the linear average of the  lexical similarity vs. (2) the inverse of the sum of negative log  of the lexical similarity.", "labels": [], "entities": []}, {"text": " Table 6: Sentence-level correlation with human adequacy judg- ments on GALE-A (training set) and GALE-B (testing set) for  judging semantic role fillers similarity using pairwise tokens vs.  only aligned tokens.", "labels": [], "entities": [{"text": "judging semantic role fillers similarity", "start_pos": 124, "end_pos": 164, "type": "TASK", "confidence": 0.692558079957962}]}]}