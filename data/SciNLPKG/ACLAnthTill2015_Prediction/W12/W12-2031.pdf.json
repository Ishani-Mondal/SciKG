{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 263-271, KU Leuven at HOO-2012: A Hybrid Approach to Detection and Correction of Determiner and Preposition Errors in Non-native English Text", "labels": [], "entities": [{"text": "HOO-2012", "start_pos": 113, "end_pos": 121, "type": "DATASET", "confidence": 0.8464297652244568}]}], "abstractContent": [{"text": "In this paper we describe the technical implementation of our system that participated in the Helping Our Own 2012 Shared Task (HOO-2012).", "labels": [], "entities": [{"text": "Helping Our Own 2012 Shared Task (HOO-2012)", "start_pos": 94, "end_pos": 137, "type": "TASK", "confidence": 0.728533923625946}]}, {"text": "The system employs a number of preprocessing steps and machine learning classifiers for correction of determiner and preposition errors in non-native English texts.", "labels": [], "entities": []}, {"text": "We use maximum entropy classifiers trained on the provided HOO-2012 development data and a large high-quality English text collection.", "labels": [], "entities": [{"text": "HOO-2012 development data", "start_pos": 59, "end_pos": 84, "type": "DATASET", "confidence": 0.9397132396697998}]}, {"text": "The system proposes a number of highly-probable corrections, which are evaluated by a language model and compared with the original text.", "labels": [], "entities": []}, {"text": "A number of deterministic rules are used to increase the precision and recall of the system.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.999535083770752}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9990887641906738}]}, {"text": "Our system is ranked among the three best performing HOO-2012 systems with a precision of 31.15%, recall of 22.08% and F 1-score of 25.84% for correction of determiner and preposition errors combined.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9976300001144409}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9998205304145813}, {"text": "F 1-score", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9918086230754852}]}], "introductionContent": [{"text": "The Helping Our Own Challenge () is a shared task that was proposed to address automated error correction of non-native English texts.", "labels": [], "entities": [{"text": "automated error correction of non-native English texts", "start_pos": 79, "end_pos": 133, "type": "TASK", "confidence": 0.808198481798172}]}, {"text": "In particular, the Helping Our Own 2012 Shared Task (HOO-2012) ( ) focuses on determiners and prepositions as they are wellknown sources for errors produced by non-native English writers.", "labels": [], "entities": [{"text": "Helping Our Own 2012 Shared Task (HOO-2012)", "start_pos": 19, "end_pos": 62, "type": "TASK", "confidence": 0.7291823956701491}]}, {"text": "For instance, reported error rates of respectively 20% and 29%.", "labels": [], "entities": [{"text": "error rates", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9826214909553528}]}, {"text": "Determiners are in particular challenging because they depend on a large discourse context and world knowledge, and moreover, they simply do not exist in many languages, such as Slavic and South-East Asian languages ().", "labels": [], "entities": []}, {"text": "The use of prepositions in English is idiomatic and thus very difficult for learners of English.", "labels": [], "entities": []}, {"text": "On the one hand, prepositions connect noun phrases to other words in a sentence (e.g. .", "labels": [], "entities": []}, {"text": ". by bus), on the other hand, they can also be part of phrasal verbs such as carry on, hold on, etc.", "labels": [], "entities": []}, {"text": "In this paper we describe our system implementation and results in HOO-2012.", "labels": [], "entities": [{"text": "HOO-2012", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.8949447274208069}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives the task definition, errors addressed, data resources and evaluation criteria and metrics.", "labels": [], "entities": []}, {"text": "Section 3 shows some background and related work.", "labels": [], "entities": []}, {"text": "Section 4 gives the full system description, while Section 5 reports and discusses the results of the experiments.", "labels": [], "entities": []}, {"text": "Section 6 concludes with an error analysis and possible further improvements.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.9265390932559967}]}], "datasetContent": [{"text": "For evaluation in the HOO framework, a distinction is made between scores and measures.", "labels": [], "entities": [{"text": "HOO framework", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.859805166721344}]}, {"text": "The complete evaluation mechanism is described in detail in ) and on the HOO-2012 website.", "labels": [], "entities": [{"text": "HOO-2012 website", "start_pos": 73, "end_pos": 89, "type": "DATASET", "confidence": 0.9485171139240265}]}, {"text": "Scores Three different scores are used: 1.", "labels": [], "entities": []}, {"text": "Detection: does the system determine that an edit of the specified type is required at some point in the text?", "labels": [], "entities": []}, {"text": "2. Recognition: does the system correctly determine the extent of the source text that requires editing?", "labels": [], "entities": [{"text": "Recognition", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9597129225730896}]}, {"text": "3. Correction: does the system offer a correction that is identical to that provided in the gold standard?", "labels": [], "entities": [{"text": "Correction", "start_pos": 3, "end_pos": 13, "type": "METRIC", "confidence": 0.9631596207618713}, {"text": "correction", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9602308869361877}]}, {"text": "Measures For each score, three measures are calculated: precision (1), recall (2) and F -score (3).", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9996609687805176}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9980138540267944}, {"text": "F -score (3)", "start_pos": 86, "end_pos": 98, "type": "METRIC", "confidence": 0.9729854762554169}]}, {"text": "where tp is the number of true positives (the number of instances that are correctly found by the system), f p the number of false positives (the number of instances that are incorrectly found), and f n the number of false negatives (missing results).", "labels": [], "entities": []}, {"text": "where \u03b2 is used as a weight factor regulating the trade-off between recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9991822838783264}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9941826462745667}]}, {"text": "We use the balanced F -score, i.e. \u03b2 = 1, such that recall and precision are equally weighted.", "labels": [], "entities": [{"text": "F -score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9935372869173685}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.999387264251709}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9992388486862183}]}, {"text": "Combined We provide results on prepositions and determiners combined, and for each of these two subcategories separately.", "labels": [], "entities": []}, {"text": "We also report on each of the different error types separately.", "labels": [], "entities": []}, {"text": "In this section we describe the pre-evaluation experiments and the results of the final evaluation on the HOO-2012 test set.", "labels": [], "entities": [{"text": "HOO-2012 test set", "start_pos": 106, "end_pos": 123, "type": "DATASET", "confidence": 0.9609938263893127}]}, {"text": "shows the characteristics of the datasets used in the experiments.", "labels": [], "entities": []}, {"text": "In the course of system development, we split the files in the HOO development dataset into a training set (80%), a tuning set (10%) and a held-out test set (10%).", "labels": [], "entities": [{"text": "HOO development dataset", "start_pos": 63, "end_pos": 86, "type": "DATASET", "confidence": 0.8167249162991842}]}, {"text": "From the beginning it was clear that the provided development dataset alone was too small to address the automated error correction tasks by employing machine learning classification techniques.", "labels": [], "entities": [{"text": "automated error correction", "start_pos": 105, "end_pos": 131, "type": "TASK", "confidence": 0.6661835511525472}, {"text": "machine learning classification", "start_pos": 151, "end_pos": 182, "type": "TASK", "confidence": 0.6683194041252136}]}, {"text": "Additionally to that dataset, we used a set of Reuters news data and the Wikipedia corpus for training the classifiers.", "labels": [], "entities": [{"text": "Reuters news data", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9526845216751099}, {"text": "Wikipedia corpus", "start_pos": 73, "end_pos": 89, "type": "DATASET", "confidence": 0.943830668926239}]}, {"text": "Once the classification models had been derived, the system was evaluated on the tuning data and adjusted in order to increase the overall performance.", "labels": [], "entities": []}, {"text": "After that, the system was evaluated on the held-out test set for which the results are shown in: Correction results on held-out test set.", "labels": [], "entities": [{"text": "Correction", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9930154085159302}]}, {"text": "For the final evaluation, we retrained the models using the complete HOO development data (again, in addition to the Reuters and Wikipedia corpus mentioned above).", "labels": [], "entities": [{"text": "HOO development data", "start_pos": 69, "end_pos": 89, "type": "DATASET", "confidence": 0.8489096959431967}, {"text": "Reuters and Wikipedia corpus", "start_pos": 117, "end_pos": 145, "type": "DATASET", "confidence": 0.8591367453336716}]}, {"text": "The number of training instances are shown in  In the HOO framework, precision and recall are weighted equally.", "labels": [], "entities": [{"text": "HOO framework", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.8140318393707275}, {"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9996597766876221}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.999554455280304}]}, {"text": "However, in the domain of error correction for non-native writers, precision is probably more important because false positives can be very confusing and demotivating for learners of English.", "labels": [], "entities": [{"text": "error correction", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.6765955090522766}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9991071820259094}]}, {"text": "For this reason, we submitted two different runs which also gave us insights into the impact of the language model.", "labels": [], "entities": []}, {"text": "'Run 0' denotes the system excluding the language model and using lower thresholds, such that neither precision nor recall is favored in particular, while 'Run 1' focuses on precision by using the language model as a filter, and having higher thresholds.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9992921352386475}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9973141551017761}, {"text": "precision", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9970518350601196}]}, {"text": "Thus, we present the results for two different runs on the final HOO test set, both before and after manual revision (see Section 2.2).", "labels": [], "entities": [{"text": "HOO test set", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.883019208908081}]}, {"text": "presents the results for recognition and Table 6 those for correction.", "labels": [], "entities": [{"text": "recognition", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.9652373790740967}, {"text": "correction", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9545571208000183}]}, {"text": "The difficulty of the HOO 2012 Shared Task is reflected by rather low system performance levels ).", "labels": [], "entities": [{"text": "HOO 2012 Shared Task", "start_pos": 22, "end_pos": 42, "type": "DATASET", "confidence": 0.7368760704994202}]}, {"text": "Nonetheless, we observed some interesting patterns.", "labels": [], "entities": []}, {"text": "In terms of the overall system performance, our system achieved better results for determiner errors than for preposition errors.", "labels": [], "entities": []}, {"text": "With respect to determiners, missing determiners are handled best by our system, while unnecessary determiners and replacement errors are more difficult.", "labels": [], "entities": [{"text": "determiners", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.8626782894134521}]}, {"text": "Concerning prepositions, missing prepositions are found to be the most challenging.", "labels": [], "entities": []}, {"text": "This confirms the difficulty of choosing the right preposition due to the large number of possible alternatives, and their sometimes subtle differences in usage and meaning.", "labels": [], "entities": []}, {"text": "While 'Run 1' achieved a higher precision (at the cost of recall), 'Run 0' performed better in terms of overall performance (F 1 -score).", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.999371349811554}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9995711445808411}, {"text": "F 1 -score)", "start_pos": 125, "end_pos": 136, "type": "METRIC", "confidence": 0.9851177215576172}]}, {"text": "This result can be explained by the relative small size and limited tuning of the language model.", "labels": [], "entities": []}, {"text": "Moreover, it also shows that the use of the F 1 -score might not be the most informative evaluation metric in this context.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9768222719430923}]}], "tableCaptions": [{"text": " Table 1: Data error statistics.", "labels": [], "entities": []}, {"text": " Table 1. The table shows counts for the development  dataset ('Dev') and two versions of the gold stan- dard test data: the original version as derived from  the CUP-provided dataset ('Test A'), and a revised  version ('Test B') which was compiled in response  to requests for corrections from participating teams.  The datasets and the revision process are further ex- plained in", "labels": [], "entities": [{"text": "CUP-provided dataset", "start_pos": 163, "end_pos": 183, "type": "DATASET", "confidence": 0.9785425364971161}]}, {"text": " Table 3: Correction results on held-out test set.", "labels": [], "entities": [{"text": "Correction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9949988126754761}]}, {"text": " Table 4: Number of training instances used for the  ME models.", "labels": [], "entities": []}, {"text": " Table 5: Recognition results of the runs on the test set.", "labels": [], "entities": []}]}