{"title": [{"text": "Phrase Model Training for Statistical Machine Translation with Word Lattices of Preprocessing Alternatives", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.793048620223999}]}], "abstractContent": [{"text": "In statistical machine translation, word lattices are used to represent the ambiguities in the preprocessing of the source sentence, such as word segmentation for Chinese or morphological analysis for German.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6725668509801229}, {"text": "word segmentation", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.7083656042814255}]}, {"text": "Several approaches have been proposed to define the probability of different paths through the lattice with external tools like word segmenters, or by applying indicator features.", "labels": [], "entities": [{"text": "word segmenters", "start_pos": 128, "end_pos": 143, "type": "TASK", "confidence": 0.6982524245977402}]}, {"text": "We introduce a novel lattice design, which explicitly distinguishes between different preprocessing alternatives for the source sentence.", "labels": [], "entities": []}, {"text": "It allows us to make use of specific features for each preprocess-ing type and to lexicalize the choice of lattice path directly in the phrase translation model.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7199201285839081}]}, {"text": "We argue that forced alignment training can be used to learn lattice path and phrase translation model simultaneously.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7347608655691147}]}, {"text": "On the news-commentary portion of the German\u2192English WMT 2011 task we can show moderate improvements of up to 0.6% BLEU over a state-of-the-art baseline system.", "labels": [], "entities": [{"text": "German\u2192English WMT 2011 task", "start_pos": 38, "end_pos": 66, "type": "DATASET", "confidence": 0.7934074997901917}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9995428323745728}]}], "introductionContent": [{"text": "The application of statistical machine translation (SMT) to word lattice input was first introduced for the translation of speech recognition output.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 19, "end_pos": 56, "type": "TASK", "confidence": 0.7801943967739741}, {"text": "translation of speech recognition output", "start_pos": 108, "end_pos": 148, "type": "TASK", "confidence": 0.822798204421997}]}, {"text": "Rather than translating the single-best transcription, the speech recognition system encodes all possible transcriptions and their probabilities within a word lattice, which is then used as input for the machine translation system.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7081301212310791}, {"text": "machine translation", "start_pos": 204, "end_pos": 223, "type": "TASK", "confidence": 0.7467124760150909}]}, {"text": "Since then, several groups have adapted this approach to model ambiguities in representing the source language with lattices and were able to report improvements over their respective baselines.", "labels": [], "entities": []}, {"text": "The probabilities for different paths through the lattice are usually modeled by assigning probabilities to arcs as a byproduct of the lattice generation or by defining binary indicator features.", "labels": [], "entities": []}, {"text": "Applying the first method only makes sense if the lattice construction is based on a single, comprehensive probabilistic method, like a Chinese word segmentation model as is used by.", "labels": [], "entities": []}, {"text": "In applications like the one described by, where several different segmenters for Chinese are combined to create the lattice, this is not possible.", "labels": [], "entities": []}, {"text": "Also, our intuition suggests that simply defining indicator features for each of the segmenters may not be ideal, if we assume that there is not a single best segmenter, but rather that for different data instances a different one works best.", "labels": [], "entities": []}, {"text": "In this paper, we propose to model the lattice path implicitly within the phrase translation model.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7686590850353241}]}, {"text": "We introduce a novel lattice design, which explicitly distinguishes between different ways of preprocessing the source sentence.", "labels": [], "entities": []}, {"text": "It enables us to define specific binary features for each preprocessing type and to learn lexicalized lattice path probabilities and the phrase translation model simultaneously with a forced alignment training procedure.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.7386301457881927}]}, {"text": "To train the phrase translation model, most stateof-the-art SMT systems rely on heuristic phrase extraction from a word-aligned training corpus.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8529706597328186}, {"text": "SMT", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9605417251586914}, {"text": "phrase extraction", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7386722564697266}]}, {"text": "Using a modified version of the translation decoder to force-align the training data provides a more consistent way of training.", "labels": [], "entities": []}, {"text": "introduce a leave-one-out method which can overcome the over-fitting effects inherent to this training procedure ().", "labels": [], "entities": []}, {"text": "The authors report this to yield both a significantly smaller phrase table and higher translation quality than the heuristic phrase extraction.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7199313342571259}]}, {"text": "We argue that applying forced alignment training helps to exploit the full potential of word lattice translation.", "labels": [], "entities": [{"text": "word lattice translation", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.7622941931088766}]}, {"text": "The effects of the training on lattice input are analyzed on the news-commentary portion of the German\u2192English WMT 2011 task.", "labels": [], "entities": [{"text": "German\u2192English WMT 2011 task", "start_pos": 96, "end_pos": 124, "type": "DATASET", "confidence": 0.805963267882665}]}, {"text": "Our results show moderate improvements of up to 0.6% BLEU over the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9998480081558228}]}, {"text": "This paper is organized as follows: We will review related work in Section 2, describe the decoder in Section 3 and present our novel lattice design in Section 4.", "labels": [], "entities": []}, {"text": "The phrase training algorithm is introduced in Section 5, and Section 6 gives a detailed account of the experimental setup and discusses the results.", "labels": [], "entities": []}, {"text": "Finally, our findings are summarized in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are carried out on the newscommentary portion of the German\u2192English data provided for the EMNLP 2011 Sixth Workshop on Statistical Machine Translation (WMT 2011).", "labels": [], "entities": [{"text": "newscommentary portion of the German\u2192English data", "start_pos": 39, "end_pos": 88, "type": "DATASET", "confidence": 0.6713164933025837}, {"text": "EMNLP 2011 Sixth Workshop on Statistical Machine Translation (WMT 2011)", "start_pos": 106, "end_pos": 177, "type": "TASK", "confidence": 0.8215133398771286}]}, {"text": "* We use newstest2008 as development set and newstest2009 and newstest2010 as unseen test sets.", "labels": [], "entities": []}, {"text": "The word alignments are produced with GIZA++ (. To optimize the loglinear parameters, the Downhill-Simplex algorithm language model is a standard 4-gram LM with modified Kneser-Ney smoothing) produced with the SRILM toolkit.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7044887840747833}, {"text": "SRILM toolkit", "start_pos": 210, "end_pos": 223, "type": "DATASET", "confidence": 0.8875128328800201}]}, {"text": "It is trained on the full bilingual data and parts of the monolingual News crawl corpus provided for WMT 2011.", "labels": [], "entities": [{"text": "News crawl corpus", "start_pos": 70, "end_pos": 87, "type": "DATASET", "confidence": 0.8479640483856201}, {"text": "WMT 2011", "start_pos": 101, "end_pos": 109, "type": "DATASET", "confidence": 0.8708761632442474}]}, {"text": "Numbers are replaced with a single category symbol in a separate preprocessing step and we apply the long-range part-of-speech based reordering rules proposed by).", "labels": [], "entities": []}, {"text": "shows statistics for the bilingual training data and the development and test corpora for the three different German preprocessing alternatives.", "labels": [], "entities": []}, {"text": "It can be seen that both compound splitting and lemmatization reduce the vocabulary size and number of out-of-vocabulary (OOV) words.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7395226806402206}]}, {"text": "Results are measured in BLEU and TER), which are computed case-insensitively with a single reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9991135001182556}, {"text": "TER", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9966576099395752}]}, {"text": "To get an overview over the effects of the different preprocessing alternatives for the German source, we built three baseline systems, one for each preprocessing type.", "labels": [], "entities": []}, {"text": "The phrase tables are extracted heuristically in the standard way from the word-aligned training data.", "labels": [], "entities": []}, {"text": "Additionally, we performed phrase training for the compound split version of the data.", "labels": [], "entities": [{"text": "phrase training", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.8552950024604797}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "When moving from the Surface to the Compound layer, we observe improvements of up to 1.0% in BLEU and 1.1% in TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9990552067756653}, {"text": "TER", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9907974004745483}]}, {"text": "Reducing the morphological richness further (Lemma) leads to a clear performance drop.", "labels": [], "entities": [{"text": "Lemma)", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9768587946891785}]}, {"text": "Application of phrase training on the compound split data yields a small degradation in TER on all data sets and in BLEU on newstest2010.", "labels": [], "entities": [{"text": "TER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9991236329078674}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9979470372200012}, {"text": "newstest2010", "start_pos": 124, "end_pos": 136, "type": "DATASET", "confidence": 0.9622149467468262}]}, {"text": "We assume that this is due to the small size of the training data and its heterogeneity, which makes it hard for the decoder to find good phrase alignments.", "labels": [], "entities": [{"text": "phrase alignments", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.698044627904892}]}, {"text": "We generated both slim and full lattices for all data sets.", "labels": [], "entities": []}, {"text": "Similar to (, we concatenate the three training data sets and their word alignments to extract the phrases.", "labels": [], "entities": []}, {"text": "Note that this only produces single-layer phrases.", "labels": [], "entities": []}, {"text": "It can be seen in that without the application of layer features the slim lattice slightly outperforms the full lattice.", "labels": [], "entities": []}, {"text": "In-newstest2008 newstest2009 newstest2010 BLEU TER BLEU TER BLEU TER  and TER.", "labels": [], "entities": [{"text": "BLEU TER BLEU TER BLEU TER", "start_pos": 42, "end_pos": 68, "type": "METRIC", "confidence": 0.7356325487295786}, {"text": "TER", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9244248270988464}]}, {"text": "We evaluate performance of the baseline systems, one for each of the three different encodings, with both slim and full lattices using heuristic phrase extraction and with full lattices using forced alignment phrase model training (FA).", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.70199915766716}, {"text": "FA", "start_pos": 232, "end_pos": 234, "type": "METRIC", "confidence": 0.7484843134880066}]}, {"text": "All lattice systems are evaluated with and without layer features.", "labels": [], "entities": []}, {"text": "The best scores in each column are in boldface, statistically significant improvement over the Compounds baseline is marked with blue color.", "labels": [], "entities": [{"text": "Compounds baseline", "start_pos": 95, "end_pos": 113, "type": "DATASET", "confidence": 0.9044948816299438}]}, {"text": "troducing layer features boosts the performance for both lattice types.", "labels": [], "entities": []}, {"text": "However, the performance increase is considerably larger for the full lattice systems, which now outperform the slim lattice systems on newstest2009 and newstest2010.", "labels": [], "entities": [{"text": "newstest2009", "start_pos": 136, "end_pos": 148, "type": "DATASET", "confidence": 0.9539469480514526}, {"text": "newstest2010", "start_pos": 153, "end_pos": 165, "type": "DATASET", "confidence": 0.8955666422843933}]}, {"text": "Compared to the Compounds baseline, the full lattice system with layer features shows a small improvement of up to 0.4% BLEU on newstest2009 and newstest2010, but a degradation in TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9995642304420471}, {"text": "TER", "start_pos": 180, "end_pos": 183, "type": "METRIC", "confidence": 0.997505247592926}]}, {"text": "The experiments on phrase training are setup as follows.", "labels": [], "entities": [{"text": "phrase training", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.8894415199756622}]}, {"text": "The phrase table is initialized with the standard extraction and is identical to the one used for the experiments in Section 6.3.", "labels": [], "entities": []}, {"text": "The log-linear scaling factors used in training are the optimized parameters on the corresponding lattice, also taken from the experiments described in Section 6.3.", "labels": [], "entities": []}, {"text": "The forced alignment procedure was run for one iteration.", "labels": [], "entities": [{"text": "forced alignment", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.602857917547226}]}, {"text": "Further iterations were tested, but did not give any improvements.", "labels": [], "entities": []}, {"text": "The phrase training was performed on the full lattice design.", "labels": [], "entities": []}, {"text": "The reason for this is that we want the system to learn all possible phrases.", "labels": [], "entities": []}, {"text": "Even if there is no difference in wording between the layers in training, the additional phrases could be useful for unseen test data.", "labels": [], "entities": []}, {"text": "The training was performed both with and without layer features.", "labels": [], "entities": []}, {"text": "The resulting systems were also optimized with and without layer features, resulting in four different setups.", "labels": [], "entities": []}, {"text": "From the results in it is clear that phrase training without layer features does not have the desired effect.", "labels": [], "entities": []}, {"text": "Even if we apply layer features to the system trained without them, we do not reach the performance of the best standard lattice system.", "labels": [], "entities": []}, {"text": "We conclude that, without these indicator features, the standard lattice system does not produce good phrase alignments.", "labels": [], "entities": [{"text": "phrase alignments", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.6955333203077316}]}, {"text": "When the layer features are applied for both training and translation, we observe improvements of up to 0.2% in BLEU and 0.5% in TER over the corresponding standard lattice system.", "labels": [], "entities": [{"text": "translation", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.9662714004516602}, {"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9977745413780212}, {"text": "TER", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.9944909811019897}]}, {"text": "The gap between the systems with and without layer features is much smaller than for the heuristically trained lattices.", "labels": [], "entities": []}, {"text": "This indicates that our goal of encoding the best lattice path directly in the phrase model was at least partially achieved.", "labels": [], "entities": []}, {"text": "However, in order to exceed the performance of our state-of-the-art baseline on both measures, the layer features are still needed within the phrase training procedure and for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 176, "end_pos": 187, "type": "TASK", "confidence": 0.969079852104187}]}, {"text": "Al-source Das Warten hat gedauert mehr als NUM Minuten, was im Fall einer Stra\u00dfe, wo werden erwartet NUM Menschen, ist unverst\u00e4ndlich.", "labels": [], "entities": [{"text": "NUM Minuten", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.8867041170597076}]}, {"text": "reference The wait lasted more than NUM minutes, something incomprehensible fora race where you expect more than NUM people.", "labels": [], "entities": []}, {"text": "lattice (heuristic) The wait has taken more than NUM minutes, which in the case of a street, where NUM people are expected to be, can't understand it. lattice (FA) The wait has taken more than NUM minutes, which in the case of a street, where expected NUM people, is incomprehensible.", "labels": [], "entities": [{"text": "FA", "start_pos": 160, "end_pos": 162, "type": "METRIC", "confidence": 0.9496220350265503}]}, {"text": "together, our phrase trained lattice approach outperforms the state-of-the-art baseline on all three data sets by up to 0.6% BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9989132881164551}]}, {"text": "On newstest2009, this result is statistically significant with 95% confidence according to the bootstrap resampling method described by.", "labels": [], "entities": [{"text": "bootstrap resampling", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.6045242100954056}]}, {"text": "For a direct comparison between the heuristic and phrase-trained full lattice systems, we manually inspected the optimized log-linear parameter values for the layer features.", "labels": [], "entities": []}, {"text": "We observe that for the standard lattices, paths through the lemmatized layer are heavily penalized.", "labels": [], "entities": []}, {"text": "In the phrase trained lattice setup, the penalty is much smaller.", "labels": [], "entities": []}, {"text": "As a result, the number of words from the Lemma layer used for translation of the newstest2009 data set is increased by 49% from 1828 to 2715 words.", "labels": [], "entities": [{"text": "newstest2009 data set", "start_pos": 82, "end_pos": 103, "type": "DATASET", "confidence": 0.9120524525642395}]}, {"text": "However, a manual inspection of the translations reveals that the main improvement seems to come from a better choice of phrases from the Compound layer.", "labels": [], "entities": []}, {"text": "More specifically, the used phrases tend to be shorter -the average phrase length of Compound layer phrases is 1.5 words for both the baseline and the heuristic lattice system.", "labels": [], "entities": []}, {"text": "In the phrase trained lattice system, it is 1.3 words.", "labels": [], "entities": []}, {"text": "An example is given in.", "labels": [], "entities": []}, {"text": "We focus on the end of the sentence, where the heuristic system uses the rather disfluent phrase (ist unverst\u00e4ndlich.", "labels": [], "entities": []}, {"text": "# can't understand it.), whereas the forced alignment trained system applies the three phrases (ist # is), (unverst\u00e4ndlich # incomprehensible) and (. # .).", "labels": [], "entities": []}, {"text": "This effect can be explained by the leave-one-out procedure.", "labels": [], "entities": []}, {"text": "As lemmatized phrases usually map to several phrases in the other layers, their count is generally higher.", "labels": [], "entities": [{"text": "count", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.9635368585586548}]}, {"text": "Application of leave-one-out, which reduces the counts of all phrases extracted from the current sentence by a fixed value, therefore has a stronger penalizing effect on Surface and Compound layer phrases.", "labels": [], "entities": []}, {"text": "In the extreme case, phrases which are singletons in the Compound layer are unlikely to be used at all in training, if the corresponding phrase in the Lemma layer has a higher count.", "labels": [], "entities": []}, {"text": "While this rarely leads to the competing lemmatized phrases being used in free translation, it allows for shorter, more general phrases from the more expressive layers to be applied.", "labels": [], "entities": []}, {"text": "Indeed, the 'bad' phrase (ist unverst\u00e4ndlich. # can't understand it.) from the example in is a singleton.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus Statistics for the WMT 2011 news-commentary data, the development set (newstest2008) and  the two test sets (newstest2009, newstest2010). For the source side, three different preprocessing alternatives  are included: Surface, Compound and Lemma.", "labels": [], "entities": [{"text": "WMT 2011 news-commentary data", "start_pos": 36, "end_pos": 65, "type": "DATASET", "confidence": 0.9690014272928238}]}, {"text": " Table 2: Results on the German-English WMT 2011 data. Scores are computed case-insensitively for BLEU [%]", "labels": [], "entities": [{"text": "German-English WMT 2011 data", "start_pos": 25, "end_pos": 53, "type": "DATASET", "confidence": 0.8402587026357651}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9957460761070251}]}]}