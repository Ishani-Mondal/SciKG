{"title": [{"text": "Offline Sentence Processing Measures for testing Readability with Users", "labels": [], "entities": [{"text": "Offline Sentence Processing Measures", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6015161201357841}]}], "abstractContent": [{"text": "While there has been much work on computational models to predict readability based on the lexical, syntactic and discourse properties of a text, there are also interesting open questions about how computer generated text should be evaluated with target populations.", "labels": [], "entities": []}, {"text": "In this paper, we compare two offline methods for evaluating sentence quality, magnitude estimation of acceptability judgements and sentence recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9119654893875122}]}, {"text": "These methods differ in the extent to which they can differentiate between surface level fluency and deeper comprehension issues.", "labels": [], "entities": []}, {"text": "We find, most importantly, that the two correlate.", "labels": [], "entities": []}, {"text": "Magnitude estimation can be run on the web without supervision, and the results can be analysed automatically.", "labels": [], "entities": [{"text": "Magnitude estimation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7660886347293854}]}, {"text": "The sentence recall methodology is more resource intensive, but allows us to tease apart the fluency and comprehension issues that arise.", "labels": [], "entities": [{"text": "sentence recall", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7422747910022736}]}], "introductionContent": [{"text": "In Natural Language Generation, recent approaches to evaluation tend to consider either \"naturalness\" or \"usefulness\".", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6971679131189982}]}, {"text": "Following evaluation methodologies commonly used for machine translation and summarisation, there have been attempts to measure naturalness in NLG by comparison to human generated gold standards.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8071050643920898}, {"text": "summarisation", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.8352367281913757}]}, {"text": "This has particularly been the casein evaluating referring expressions, where the generated expression can be treated as a set of attributes and compared with human generated expressions (), but there have also been attempts at evaluating sentences this way.", "labels": [], "entities": []}, {"text": "For instance, generate sentences from a parsed analysis of an existing sentence, and evaluate by comparison to the original.", "labels": [], "entities": []}, {"text": "However, this approach has been criticised at many levels (see for example, or); for instance, because there are many good ways to realise a sentence, because typical NLG tasks do not come with reference sentences, and because fluency judgements in the monolingual case are more subtle than for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 295, "end_pos": 314, "type": "TASK", "confidence": 0.7761260271072388}]}, {"text": "Readability metrics, by comparison, do not rely on reference texts, and try to model the linguistic quality of a text based on features derived from the text.", "labels": [], "entities": []}, {"text": "This body of work ranges from the Flesch Metric, which is based on average word and sentence length, to more systematic evaluations of various lexical, syntactic and discourse characteristics of a text (cf., who assess readability of textual summaries).", "labels": [], "entities": [{"text": "Flesch Metric", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.811085432767868}]}, {"text": "Some researchers have also suggested measuring edit distance by using a human to revise a system generated text and quantifying the revisions made (.", "labels": [], "entities": []}, {"text": "This does away with the need for reference texts and is quite suited to expert domains such as medicine or weather forecasting, where a domain expert can easily correct system output.", "labels": [], "entities": [{"text": "weather forecasting", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7476251423358917}]}, {"text": "Analysis of these corrections can provide feedback on problematic content and style.", "labels": [], "entities": []}, {"text": "We have previously evaluated text reformulation applications by asking readers which version they prefer), or through the use of Likert scales) for measuring meaning preservation and grammaticality).", "labels": [], "entities": [{"text": "text reformulation", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.76720330119133}, {"text": "meaning preservation", "start_pos": 158, "end_pos": 178, "type": "TASK", "confidence": 0.7261941730976105}]}, {"text": "However, none of these approaches tell us very much about the comprehensibility of a text for an end reader.", "labels": [], "entities": []}, {"text": "To address this, there has been recent interest in task based evaluations.", "labels": [], "entities": []}, {"text": "Task based evaluations directly evaluate generated utterances for their utility to the hearer.", "labels": [], "entities": []}, {"text": "However, while for some generation areas like reference (, the real world evaluation task is obvious, it is less so for other generation tasks such as surface realisation or text-totext regeneration or paraphrase.", "labels": [], "entities": [{"text": "text-totext regeneration", "start_pos": 174, "end_pos": 198, "type": "TASK", "confidence": 0.7106385976076126}]}, {"text": "We are thus keen to investigate psycholinguistic methods for investigating sentence processing as an alternative to task based evaluations.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.6924499273300171}]}, {"text": "In the psycholinguistics literature, various offline and online techniques have been used to investigate sentence processing by readers.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7130835503339767}]}, {"text": "Online techniques (eye-tracking, neurophysiological, etc.) offer many advantages in studying how readers process a sentence.", "labels": [], "entities": []}, {"text": "But as these are difficult to setup and also resource intensive, we would prefer to evaluate NLG using offline techniques.", "labels": [], "entities": []}, {"text": "Some offline techniques, such as Cloze tests or question answering, require careful preparation of material (choice of texts and questions, and for Cloze, the words to leave out).", "labels": [], "entities": [{"text": "question answering", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8584795892238617}]}, {"text": "Other methods, such as magnitude estimation and sentence recall (cf. Sec 3 for details), are more straightforward to implement.", "labels": [], "entities": [{"text": "magnitude estimation", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.8892184197902679}, {"text": "sentence recall", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.6425328552722931}]}, {"text": "In this paper, we investigate magnitude estimation of acceptability judgements and delayed sentence recall in the context of an experiment investigating generation choices when realising causal relations.", "labels": [], "entities": []}, {"text": "Our goal is to study how useful these methods are for evaluating surface level fluency and deeper comprehensibility.", "labels": [], "entities": []}, {"text": "We are interested in whether they can distinguish between similar sentences, and whether they can be used to test hypotheses regarding the effect of common generation decisions such as information order and choice of discourse marker.", "labels": [], "entities": []}, {"text": "We briefly discuss the data in Section 2, before describing our experiments (Sections 3.1 and 3.2).", "labels": [], "entities": []}, {"text": "We finish with a discussion of their suitability for more general evaluation of NLG with target readers.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Average acceptability for the n th best formula- tion of each of the 144 sentences.", "labels": [], "entities": []}]}