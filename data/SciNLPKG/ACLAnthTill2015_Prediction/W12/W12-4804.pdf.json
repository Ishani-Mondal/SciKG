{"title": [{"text": "Using Collocations and K-means Clustering to Improve the N-pos Model for Japanese IME", "labels": [], "entities": [{"text": "IME", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.4317384958267212}]}], "abstractContent": [{"text": "Kana-Kanji conversion is known as one of the representative applications of Natural Language Processing (NLP) for the Japanese language.", "labels": [], "entities": [{"text": "Kana-Kanji conversion", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7704000174999237}]}, {"text": "The N-pos model, presenting the probability of a Kanji candidate sequence by the product of bi-gram Part-of-Speech (POS) probabilities and POS-to-word emission probabilities, has been successfully applied in a number of well-known Japanese Input Method Editor (IME) systems.", "labels": [], "entities": []}, {"text": "However, since N-pos model is an approximation of n-gram word-based language model, important word-to-word collocation information are lost during this compression and lead to a drop of the conversion accuracies.", "labels": [], "entities": []}, {"text": "In order to overcome this problem, we propose ways to improve current N-pos model.", "labels": [], "entities": []}, {"text": "One way is to append the high-frequency collocations and the other way is to subcategorize the huge POS sets to make them more representative.", "labels": [], "entities": []}, {"text": "Experiments on large-scale data verified our proposals.", "labels": [], "entities": []}], "introductionContent": [{"text": "In Japanese IME systems 1 , Kana-Kanji conversion is known as one of the representative applications of NLP.", "labels": [], "entities": [{"text": "Kana-Kanji conversion", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.8089584708213806}]}, {"text": "Unfortunately, numerous researchers have taken it for granted that current NLP technologies have already given a fully support to this task and there are few things left to be done as research topics.", "labels": [], "entities": []}, {"text": "However, as we go deeper to this \"trivial\" task, we recognize that converting from a romanized Hirakana sequence (i.e., users' input) into a mixture of Kana and Kanji sequence (i.e., users' expected output) is more difficult than it looks.", "labels": [], "entities": []}, {"text": "Concretely, we are facing a lot of NLP research topics such as Japanese word/chunk segmentation, POS tagging, n-best decoding, etc.", "labels": [], "entities": [{"text": "Japanese word/chunk segmentation", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.5511828601360321}, {"text": "POS tagging", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.8259941041469574}]}, {"text": "Existing algorithms dealing with these topics are challenged by the daily-updating and large-scale Web data.", "labels": [], "entities": []}, {"text": "Traditional n-gram word-level language model (short for \"word n-gram model\", hereafter) is good at ranking the Kanji candidates.", "labels": [], "entities": []}, {"text": "However, by using the large-scale Web data in tera-bytes (TB), even bi-gram word-level language model is too large 2 to fit the memories (for loading the model) and computing abilities (for decoding) of users' personal computers (PCs).", "labels": [], "entities": []}, {"text": "Dealing with this limitation, n-pos model () was proposed to make a compression 3 of the word n-gram model.", "labels": [], "entities": []}, {"text": "N-pos model takes POS tags (or word classes) as the latent variable and factorizes the probability of a Kanji candidate sequence into a product of POS-to-POS transition probabilities and POS-to-word emission probabilities.", "labels": [], "entities": []}, {"text": "This factorization makes n-pos model alike the well-known Hidden Markov Model (HMM).", "labels": [], "entities": []}, {"text": "Since the number of POS tags is far smaller than the number of word types in the training data, n-pos model can significantly smaller the final model without deteriorating the conversion accuracies too much.", "labels": [], "entities": []}, {"text": "Compared with word n-gram model, n-pos model is good for its small size for both storing and decoding.", "labels": [], "entities": []}, {"text": "However, the major disadvantage is that important word-level collocation information are not guaranteed to be kept in the model.", "labels": [], "entities": []}, {"text": "One direction to remedy the n-pos model is to find those lost word-level information and append it.", "labels": [], "entities": []}, {"text": "The other direction is to sub-categorize the original POS tags to make the entries under one POS tag contain as less homophonic words as possible.", "labels": [], "entities": []}, {"text": "These considerations yielded our proposals, first by appending collocations and second by sub-categorizing POS tags.", "labels": [], "entities": []}, {"text": "Experiments by making use of large-scale training data verified the effectiveness of our proposals.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section, we give the formal definition of n-pos model and explain its disadvantage by real examples.", "labels": [], "entities": []}, {"text": "In Section 3 we describe our proposed approaches.", "labels": [], "entities": []}, {"text": "Experiments in Section 4 testify our proposals.", "labels": [], "entities": []}, {"text": "We finally conclude this paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: The accuracies of appending collocations and K-means clustering. Here, w = word, s =  sentence.", "labels": [], "entities": []}]}