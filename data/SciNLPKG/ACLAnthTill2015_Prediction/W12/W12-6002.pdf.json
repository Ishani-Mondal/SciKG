{"title": [{"text": "Question Classification and Answering from Procedural Text in English", "labels": [], "entities": [{"text": "Question Classification and Answering from Procedural Text", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.799384845154626}]}], "abstractContent": [{"text": "Linguistic patterns reflect the regularities of Natural Language and the applicability of such linguistic patterns is acknowledged in several Natural Language Processing tasks.", "labels": [], "entities": []}, {"text": "Many question classification systems depend on patterns that are extracted from already framed questions.", "labels": [], "entities": [{"text": "question classification", "start_pos": 5, "end_pos": 28, "type": "TASK", "confidence": 0.73930823802948}]}, {"text": "In this paper, we have investigated possible question categories and question patterns for procedural text documents in English and proposed seven question classes.", "labels": [], "entities": []}, {"text": "More than six thousands questions of different domains, e.g., cooking recipes, electronics, home and maintenance, medical etc have been collected from Yahoo answers as experimentation corpus.", "labels": [], "entities": []}, {"text": "Annotators reached almost perfect agreement of 94.6% at kappa scale.", "labels": [], "entities": []}, {"text": "A procedural question answering system has been developed to verify the proposed question classes.", "labels": [], "entities": [{"text": "question answering", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.6996379345655441}]}, {"text": "The evaluation reveals that the proposed classes area good approach to deal with Question Answering for procedural text questions.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.8042346835136414}]}, {"text": "The procedural question answering system has achieved overall 95.08%, 86.95% and 90.84 precision, recall and F-measure value respectively.", "labels": [], "entities": [{"text": "procedural question answering", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.6718306342760721}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9927566647529602}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9993846416473389}, {"text": "F-measure value", "start_pos": 109, "end_pos": 124, "type": "METRIC", "confidence": 0.9782178103923798}]}], "introductionContent": [{"text": "Automated question answering (QA) has been a hot topic of research and development since the earliest AI applications.", "labels": [], "entities": [{"text": "Automated question answering (QA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8638115525245667}]}, {"text": "Many international question answering evaluation tracks have taken place at conferences and workshops, such as TREC 1 , CLEF 2 , and NTCIR 3 to improve question-answering systems.", "labels": [], "entities": [{"text": "question answering evaluation", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.9113850792249044}, {"text": "NTCIR", "start_pos": 133, "end_pos": 138, "type": "DATASET", "confidence": 0.7771050930023193}]}, {"text": "An important component of question answering systems is question classification.", "labels": [], "entities": [{"text": "question answering", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8946612775325775}, {"text": "question classification", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.8586122393608093}]}, {"text": "The task of a question classifier is to assign one or more class labels, depending on classification strategy, to a given question written in natural language.", "labels": [], "entities": []}, {"text": "For example, for the question \"What is the capital of India?\", the task of question classification is to assign label\"Location\"to this question.", "labels": [], "entities": [{"text": "question classification", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.7205489277839661}]}, {"text": "Since we predict the type of the answer, question classification is also referred as answer type prediction.", "labels": [], "entities": [{"text": "question classification", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7855774760246277}, {"text": "answer type prediction", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.7568135261535645}]}, {"text": "Common classification strategies include semantic categorization and surface patterns identification.", "labels": [], "entities": [{"text": "surface patterns identification", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.6764620145161947}]}, {"text": "Surface pattern identification methods classify questions to sets of word-based patterns.", "labels": [], "entities": [{"text": "Surface pattern identification", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6171031693617502}]}, {"text": "Answers are then extracted from retrieved documents using these patterns.", "labels": [], "entities": []}, {"text": "Without the help of external knowledge, surface pattern methods suffer from limited ability to exclude answers that are in irrelevant semantic classes, especially when using smaller or heterogeneous corpora.", "labels": [], "entities": []}, {"text": "The amount of supported classification types greatly influences the performance of QA systems.", "labels": [], "entities": []}, {"text": "Question classification has been studied by using different type of classifiers.", "labels": [], "entities": [{"text": "Question classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8361861109733582}]}, {"text": "Most of the successful studies on this task used support vector machines (SVM) ().", "labels": [], "entities": []}, {"text": "SVMs are very successful on high dimensional data since they are more efficient especially when the feature vectors are sparse.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9051697254180908}]}, {"text": "Question classification has also been done by Maximum Entropy models (), Sparse Network of Winnows (SNoW) () and language modeling.", "labels": [], "entities": [{"text": "Question classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8612255156040192}, {"text": "language modeling", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7584388256072998}]}, {"text": "As per Wikipedia (en.wikipedia.org), the term procedure is being used in diverse domains with different meanings- \u2022 Organization: A procedure is a document written to support a \"Policy Directive\".", "labels": [], "entities": []}, {"text": "\u2022 Medical: A procedure is a course of action intended to achieve a result in the care of persons with health problems.", "labels": [], "entities": []}, {"text": "\u2022 Mathematics and Computing: A procedure is a set of operations or calculations that accomplish some goal.", "labels": [], "entities": []}, {"text": "\u2022 Cooking: A procedure is a set of commands that show how to prepare or make something.", "labels": [], "entities": []}, {"text": "\u2022 Industry and Military: A procedure is a step-by-step instruction to achieve a desired result.", "labels": [], "entities": []}, {"text": "\u2022 Legal: A procedure is the law and rules used in the administration of justice in the court system.", "labels": [], "entities": []}, {"text": "\u2022 Computer science: A procedure is apart of a larger computer program that performs a specific task.", "labels": [], "entities": []}, {"text": "So, in general a procedure is a specified series of actions or operations or a set of commands which have to be executed in order to obtain a goal.", "labels": [], "entities": []}, {"text": "Less precisely speaking, the word 'procedure' can indicate a sequence of activities, task, steps, decisions, calculations and processes, that when undertaken in the sequence laid down produces the described results, product or outcome.", "labels": [], "entities": []}, {"text": "So, procedural texts consist of a sequence of instructions in order to reach a goal and range from apparently simple cooking recipes to large maintenance manuals.", "labels": [], "entities": []}, {"text": "They also include documents as diverse as teaching texts, medical notices, social behaviour recommendations, directions for use, assembly notices, do-it-yourself notices, itinerary guides, advice texts, savoir-faire guides etc ).", "labels": [], "entities": []}, {"text": "So, the questions of procedural text are as diverse as its range of diversity.", "labels": [], "entities": []}, {"text": "In our perspective, procedural questions will be of much growing interest to the non-technical as well as technical staff.", "labels": [], "entities": []}, {"text": "Statistics also showed that procedural questions is the second largest set of queries formed to web search engines after factoid questions (de).", "labels": [], "entities": []}, {"text": "This is confirmed by another detailed study carried out by).", "labels": [], "entities": []}, {"text": "While the first QA systems mainly dealt with factoid questions, a number of systems in the last decade have appeared with the aim of addressing non-factoid questions (E. M..", "labels": [], "entities": []}, {"text": "Procedural questions, sometimes called 'How-questions', are questions whose induced response is typically a fragment, more or less large, of a procedure, i.e., a set of coherent instructions designed to reach a goal.", "labels": [], "entities": [{"text": "Procedural questions, sometimes called 'How-questions', are questions whose induced response is typically a fragment, more or less large, of a procedure, i.e., a set of coherent instructions designed to reach a goal", "start_pos": 0, "end_pos": 215, "type": "Description", "confidence": 0.751525892317295}]}, {"text": "Answering procedural questions thus requires being able to extract well-formed text structure unlike factoid question and analyzing a procedural text requires a dedicated discourse analysis, e.g. by means of a grammar . Though less research has been conducted so far on other types of non-factoid QA, such as why-questions () and procedural (how-to) questions, during the last decade challenges of procedural text and argument extraction have been addressed.", "labels": [], "entities": [{"text": "argument extraction", "start_pos": 418, "end_pos": 437, "type": "TASK", "confidence": 0.7558689117431641}]}, {"text": "In this work, we have focused on question classification and answering from the procedural text in English and building a generic domain independent procedural question answering system.", "labels": [], "entities": [{"text": "question classification", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8609987497329712}, {"text": "question answering", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.747755229473114}]}, {"text": "The remainder of the paper is organized as follows: in the next section, we review the related works.", "labels": [], "entities": []}, {"text": "Corpus preparation and system description are elaborated in third section and fourth section respectively; Corpus for procedural text and evaluation are described in fifth section and sixth section respectively; and finally seventh section describes the conclusions of our study and outlines directions of our future work.", "labels": [], "entities": [{"text": "Corpus preparation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.750760018825531}]}], "datasetContent": [{"text": "The evaluation set composed of 1824 questions over four domains: cooking recipes, electronics, maintenance and medical procedure.", "labels": [], "entities": []}, {"text": "Though the test set is not very large, but it is sufficient for inductive evaluation.", "labels": [], "entities": []}, {"text": "We have used standard evaluation metrics precision, recall and F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9989224672317505}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9996652603149414}, {"text": "F-measure", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9972478747367859}]}, {"text": "show the statistics for cooking recipe, electronics, home and maintenance, and medical domains respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Procedure statistics for different domains", "labels": [], "entities": []}]}