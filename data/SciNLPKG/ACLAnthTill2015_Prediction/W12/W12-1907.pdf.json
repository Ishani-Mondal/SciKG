{"title": [{"text": "Unsupervised Part of Speech Inference with Particle Filters", "labels": [], "entities": [{"text": "Speech Inference", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.7029845267534256}]}], "abstractContent": [{"text": "As linguistic models incorporate more subtle nuances of language and its structure, standard inference techniques can fall behind.", "labels": [], "entities": []}, {"text": "Often , such models are tightly coupled such that they defy clever dynamic programming tricks.", "labels": [], "entities": []}, {"text": "However, Sequential Monte Carlo (SMC) approaches , i.e. particle filters, are well suited to approximating such models, resolving their multi-modal nature at the cost of generating additional samples.", "labels": [], "entities": [{"text": "Sequential Monte Carlo (SMC)", "start_pos": 9, "end_pos": 37, "type": "TASK", "confidence": 0.6432875643173853}]}, {"text": "We implement two particle filters, which jointly sample either sentences or word types, and incorporate them into a Gibbs sampler for part-of-speech (PoS) inference.", "labels": [], "entities": [{"text": "part-of-speech (PoS) inference", "start_pos": 134, "end_pos": 164, "type": "TASK", "confidence": 0.642779004573822}]}, {"text": "We analyze the behavior of the particle filters, and compare them to a block sentence sampler, a local token sampler, and a heuristic sampler, which constrains inference to a single PoS per word type.", "labels": [], "entities": []}, {"text": "Our findings show that particle filters can closely approximate a difficult or even intractable sampler quickly.", "labels": [], "entities": []}, {"text": "However, we found that high posterior likelihood do not necessarily correspond to better Many-to-One accuracy.", "labels": [], "entities": [{"text": "posterior likelihood", "start_pos": 28, "end_pos": 48, "type": "METRIC", "confidence": 0.8840648531913757}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9608191251754761}]}, {"text": "The results suggest that the approach has potential and more advanced particle filters are likely to lead to stronger performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern research is steadily revealing more of the subtle structure of natural language to create increasingly intricate models.", "labels": [], "entities": []}, {"text": "Many modern problems in computational linguistics require or benefit from modeling the long range correlations between latent variables, e.g. part of speech (PoS) induction, dependency parsing (, and coreference resolution.", "labels": [], "entities": [{"text": "part of speech (PoS) induction", "start_pos": 142, "end_pos": 172, "type": "TASK", "confidence": 0.6256437514509473}, {"text": "dependency parsing", "start_pos": 174, "end_pos": 192, "type": "TASK", "confidence": 0.8128690719604492}, {"text": "coreference resolution", "start_pos": 200, "end_pos": 222, "type": "TASK", "confidence": 0.9578654170036316}]}, {"text": "These correlations make inference difficult because they reflect the complicated effect variables have on each other in such tightly coupled models.", "labels": [], "entities": []}, {"text": "Sequential Monte Carlo (SMC) methods, like particle filters, are particularly well suited to estimating tightly coupled distributions (.", "labels": [], "entities": [{"text": "Sequential Monte Carlo (SMC)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7032093703746796}]}, {"text": "Particle filters sample sequences of latent variable assignments by concurrently generating several representative sequences consistent with a model's conditional dependencies.", "labels": [], "entities": []}, {"text": "The sequential nature of the sampling simplifies inference by ignoring ambiguous correlations with unsampled variables at the cost of sampling the sequence multiple times.", "labels": [], "entities": []}, {"text": "The few applications of particle filters in computational linguistics generally focus on the online nature of SMC ().", "labels": [], "entities": [{"text": "SMC", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9609522819519043}]}, {"text": "However, batch applications still benefit from the power of SMC to generate samples from tightly coupled distributions that would otherwise need to be approximated.", "labels": [], "entities": [{"text": "SMC", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9838623404502869}]}, {"text": "Furthermore, the time cost of the additional samples generated by SMC can be mitigated by generating them in parallel.", "labels": [], "entities": [{"text": "SMC", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9735225439071655}]}, {"text": "This report presents an initial approach to the integration of SMC and block sampling, sometimes reffered to as Particle Gibbs (PG) sampling (.", "labels": [], "entities": [{"text": "SMC", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9852368235588074}]}, {"text": "Unsupervised PoS induction serves as a motivating example for future extensions to other problems.", "labels": [], "entities": [{"text": "PoS induction", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.8374228775501251}]}, {"text": "Section 3 reviews the PYP-HMM model used for PoS inference.", "labels": [], "entities": [{"text": "PoS inference", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.8991011381149292}]}, {"text": "Section 4 explains the Sequential Importance Sampling (SIS) algorithm, a basic SMC method that generates samples for the 47 block sampler.", "labels": [], "entities": [{"text": "Sequential Importance Sampling (SIS)", "start_pos": 23, "end_pos": 59, "type": "TASK", "confidence": 0.8287872970104218}, {"text": "SMC", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9871113896369934}]}, {"text": "This approach yields two implementations: a simple sentence-based block sampler (4.1) and a more complicated type-based sampler (4.2).", "labels": [], "entities": []}, {"text": "Finally, section 5 evaluates both implementations on a variety of unsupervised PoS inference tasks, analyzing the behavior of the SMC inference and comparing them to state-of-the-art approaches.", "labels": [], "entities": [{"text": "SMC inference", "start_pos": 130, "end_pos": 143, "type": "TASK", "confidence": 0.9225879609584808}]}], "datasetContent": [{"text": "We take two approaches to evaluating the SMC based samplers.", "labels": [], "entities": [{"text": "SMC", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9834221005439758}]}, {"text": "The first approach is an analysis of the samplers as inference algorithms.", "labels": [], "entities": []}, {"text": "The samplers should tend to maximize the posterior likelihood of the model over iterations, eventually converging tothe mode.", "labels": [], "entities": []}, {"text": "Section 5.1 analyzes the particle filter based samplers with various numbers of particles in an effort to understand how they behave.", "labels": [], "entities": []}, {"text": "Then, section 5.2 evaluates each of the proposed approaches on PoS inference tasks from several languages.", "labels": [], "entities": [{"text": "PoS inference tasks", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8584147294362386}]}, {"text": "These results allow a practical comparison with other PoS inference approaches.", "labels": [], "entities": [{"text": "PoS inference", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.8486244976520538}]}], "tableCaptions": [{"text": " Table 1: Many-to-1 accuracies on CoNLL and Penn-Treebank Wall Street Journal corpora for sentence-(Sent) and  type-(Type) based filtering. The table lists the average M-1 accuracy measured according to the maximum marginal  tag assignments over 3 seperate runs after 200 iterations for the sent, type, 1HMM and 1HMM-LM samplers,  and 500 iterations for the HMM local sampler. The 1HMM-LM model has been shown to achieve state-of-the-art  unsupervised M-1 accuracies on these datasets. and thus represents the limit of unsupervised M-1 accuracy.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.9332073330879211}, {"text": "Penn-Treebank Wall Street Journal corpora", "start_pos": 44, "end_pos": 85, "type": "DATASET", "confidence": 0.9202999472618103}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9620359539985657}, {"text": "accuracy", "start_pos": 536, "end_pos": 544, "type": "METRIC", "confidence": 0.9736697673797607}]}]}