{"title": [{"text": "Detection of Implicit Citations for Sentiment Detection", "labels": [], "entities": [{"text": "Detection of Implicit Citations", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.857652559876442}, {"text": "Sentiment Detection", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.936542421579361}]}], "abstractContent": [{"text": "Sentiment analysis of citations in scientific papers is anew and interesting problem which can open up many exciting new applications in bibliometrics.", "labels": [], "entities": [{"text": "Sentiment analysis of citations in scientific papers", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.9352435895374843}]}, {"text": "Current research assumes that using just the citation sentence is enough for detecting sentiment.", "labels": [], "entities": [{"text": "detecting sentiment", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.9082024991512299}]}, {"text": "In this paper, we show that this approach misses much of the existing sentiment.", "labels": [], "entities": []}, {"text": "We present anew corpus in which all mentions of a cited paper have been annotated.", "labels": [], "entities": []}, {"text": "We explore methods to automatically identify these mentions and show that the inclusion of implicit citations in citation sentiment analysis improves the quality of the overall sentiment assignment.", "labels": [], "entities": [{"text": "citation sentiment analysis", "start_pos": 113, "end_pos": 140, "type": "TASK", "confidence": 0.7752955555915833}]}], "introductionContent": [{"text": "The idea of using citations as a source of information has been explored extensively in the field of bibliometrics, and more recently in the field of computational linguistics.", "labels": [], "entities": []}, {"text": "State-of-the-art citations identification mechanisms focus either on detecting explicit citations i.e. those that consist of either the author names and the year of publication or bracketed numbers only, or include a small sentence window around the explicit citation as input text.", "labels": [], "entities": [{"text": "citations identification", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8104478716850281}]}, {"text": "The assumption behind this approach is that all related mentions of the paper would be concentrated in the immediate vicinity of the anchor text.", "labels": [], "entities": []}, {"text": "However, this assumption does not generally hold true.", "labels": [], "entities": []}, {"text": "The phenomenon of trying to determine a citations's citation context has along tradition in library sciences, and its connection with coreference has been duely noted).", "labels": [], "entities": []}, {"text": "Consider, which illustrates atypical case.", "labels": [], "entities": []}, {"text": "While the first sentence cites the target paper explicitly using the name of the primary author along with the year of publication of the paper, the remaining sentences mentioning the same paper appear after a gap and contain an indirect and implicit reference to that paper.", "labels": [], "entities": []}, {"text": "These mentions occur two sentences after the formal citation in the form of anaphoric it and the lexical hook METEOR.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9566290974617004}]}, {"text": "Most current techniques, with the exception of, are notable to detect linguistic mentions of citations in such forms.", "labels": [], "entities": []}, {"text": "Ignoring such mentions and examining only the sentences contain-ing an explicit citation results in loss of information about the cited paper.", "labels": [], "entities": []}, {"text": "While this phenomenon is problematic for applications like scientific summarisation (, it has a particular relevance for citation sentiment detection.", "labels": [], "entities": [{"text": "scientific summarisation", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.5795010328292847}, {"text": "citation sentiment detection", "start_pos": 121, "end_pos": 149, "type": "TASK", "confidence": 0.9397767782211304}]}, {"text": "Citation sentiment detection is an attractive task.", "labels": [], "entities": [{"text": "Citation sentiment detection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8952803611755371}]}, {"text": "Availability of citation polarity information can help researchers in understanding the evolution of afield on the basis of research papers and their critiques.", "labels": [], "entities": []}, {"text": "It can also help expert researchers who are in the process of preparing opinion based summaries for survey papers by providing them with motivations behind as well as positive and negative comments about different approaches.", "labels": [], "entities": []}, {"text": "Current work on citation sentiment detection works under the assumption that the sentiment present in the citation sentence represents the true sentiment of the author towards the cited paper).", "labels": [], "entities": [{"text": "citation sentiment detection", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.9256175756454468}]}, {"text": "This assumption is so dominant because current citation identification methods ( can readily identify the citation sentence, whereas it is much harder to determine the relevant context.", "labels": [], "entities": [{"text": "citation identification", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.791194498538971}]}, {"text": "However, this assumption most certainly does not hold true when the citation context spans more than one sentence.", "labels": [], "entities": []}, {"text": "Concerning the sentiment aspect of the citation context from, we see that the citation sentence does not contain any sentiment towards the cited paper, whereas the following sentences act as a critique and list its shortcomings.", "labels": [], "entities": []}, {"text": "It is clear that criticism is the intended sentiment, but if the gold standard is defined by looking at the citation sentence in isolation, a significant amount of sentiment expressed in the text is lost.", "labels": [], "entities": []}, {"text": "Given that overall most citations in a text are neutral with respect to sentiment), this makes it even more important to recover what explicit sentiment there is in the article, wherever it is to be found.", "labels": [], "entities": []}, {"text": "In this paper, we examine methods to extract all opinionated sentences from research papers which mention a given paper in as many forms as we can identify, not just as explicit citations.", "labels": [], "entities": []}, {"text": "We present anew corpus in which all mentions of a cited paper have been manually annotated, and show that our annotation treatment increases citation sentiment coverage, particularly for negative sentiment.", "labels": [], "entities": []}, {"text": "We then explore methods to automatically identify all mentions of a paper in a supervised manner.", "labels": [], "entities": []}, {"text": "In particular, we consider the recognition of named approaches and acronyms.", "labels": [], "entities": []}, {"text": "Our overall system then classifies explicit and implicit mentions according to sentiment.", "labels": [], "entities": []}, {"text": "The results support the claim that including implicit citations in citation sentiment analysis improves the quality of the overall sentiment assignment.", "labels": [], "entities": [{"text": "citation sentiment analysis", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.7886146505673727}]}], "datasetContent": [{"text": "For the task of detecting all mentions of a citation, we merge the class labels of sentences mentioning a citation in any form (o n p).", "labels": [], "entities": [{"text": "detecting all mentions of a citation", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.7460285127162933}]}, {"text": "To make sure that the easily detectable explicit citations do not influence the results, we change the class label of all those sentences to x which contain the first author's name within a 4-word window of the year of publication.", "labels": [], "entities": []}, {"text": "Our dataset is skewed as there are many more objective sentences than subjective ones.", "labels": [], "entities": []}, {"text": "In such scenarios, average micro-F scores tend to be slightly higher as they area weighted measure.", "labels": [], "entities": []}, {"text": "To avoid this bias, we also report the macro-F scores.", "labels": [], "entities": []}, {"text": "Furthermore, to ensure there is enough data for training each class, we use 10-fold cross-validation) in all our experiments.", "labels": [], "entities": []}, {"text": "We represent each citation as a feature set in a Support Vector Machine (SVM) framework.", "labels": [], "entities": []}, {"text": "The corpus is processed using WEKA ( and the Weka LibSVM library (EL-Manzalawy and).", "labels": [], "entities": [{"text": "WEKA", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.8757376670837402}, {"text": "Weka LibSVM library", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.9655474424362183}]}, {"text": "For each i th sentence Si , we use the following binary features.", "labels": [], "entities": []}, {"text": "\u2022 S i\u22121 contains the last name of the primary author, followed by the year of publication within a four-word window.", "labels": [], "entities": []}, {"text": "This feature is meant to capture the fact that the sentence immediately after an explicit citation is more likely to continue talking about the same work.", "labels": [], "entities": []}, {"text": "\u2022 Si contains the last name of the primary author followed by the year of publication within a four-word window.", "labels": [], "entities": []}, {"text": "This feature should help in identifying sentences containing explicit citations.", "labels": [], "entities": [{"text": "identifying sentences containing explicit citations", "start_pos": 28, "end_pos": 79, "type": "TASK", "confidence": 0.8004844188690186}]}, {"text": "Since such sentences are easier to identify, including them in the evaluation metric would result in a false boost in the final score.", "labels": [], "entities": []}, {"text": "We have thus excluded all such sentences in our annotation and this feature should indicate a negative instance to the classifier.", "labels": [], "entities": []}, {"text": "\u2022 Si contains the last name of the primary author.", "labels": [], "entities": []}, {"text": "This feature captures sentences which contain a reference to tools and algorithms which have been named after their inventors, such as, \"One possible direction for future work is to compare the search-based approach of Collins and Roark with our DP-based approach.\"", "labels": [], "entities": []}, {"text": "It should also capture the mentions of methods and techniques used in the cited paper e.g., \"We show that our approach outperforms Turney's approach.\"", "labels": [], "entities": []}, {"text": "\u2022 Si contains an acronym used in an explicit citation.", "labels": [], "entities": []}, {"text": "Acronyms are taken to be capitalised words which are extracted from the vicinity of the cited author's last name using regular expressions.", "labels": [], "entities": []}, {"text": "For example, METEOR in is an acronym which is used in place of a formal citation to refer to the original paper in the rest of the citing paper.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9588472247123718}]}, {"text": "\u2022 Si contains a determiner followed by a work noun.", "labels": [], "entities": []}, {"text": "We use the following determiners D = {the, this, that, those, these, his, her, their, such, previous, other}.", "labels": [], "entities": []}, {"text": "The list of work nouns (technique, method, etc.) has been taken from.", "labels": [], "entities": []}, {"text": "This feature extracts a pattern which has been found to be useful for extracting citations in previous work.", "labels": [], "entities": [{"text": "extracting citations", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.8625648021697998}]}, {"text": "Such phrases usually signal a continuation of the topics related to citations in earlier sentences.", "labels": [], "entities": []}, {"text": "For example: \", and describe algorithms which do this.", "labels": [], "entities": []}, {"text": "However, the validity of these algorithms has not been tested by systematic comparisons with associations of human subjects.\"", "labels": [], "entities": []}, {"text": "\u2022 Si starts with a third person pronoun.", "labels": [], "entities": []}, {"text": "The feature also tries to capture the topic continuation after a citation.", "labels": [], "entities": []}, {"text": "Sentences starting with a pronoun (e.g. they, their, he, she, etc.) are more likely to describe the subject citation of the previous sentence in detail.", "labels": [], "entities": []}, {"text": "For example: \"Because Daume III (2007) views the adaptation as merely augmenting the feature space, each of his features has the same prior mean and variance, regardless of whether it is domain specific or independent.", "labels": [], "entities": [{"text": "prior mean and variance", "start_pos": 134, "end_pos": 157, "type": "METRIC", "confidence": 0.6559536010026932}]}, {"text": "He could have set these parameters differently, but he did not.\"", "labels": [], "entities": []}, {"text": "\u2022 Si starts with a connector.", "labels": [], "entities": []}, {"text": "This feature also focuses on detecting the topic continuity.", "labels": [], "entities": [{"text": "detecting the topic continuity", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.7189722061157227}]}, {"text": "Connectors have been shown to be effective in other context related works as well ().", "labels": [], "entities": []}, {"text": "A list of 23 connectors (e.g. however, although, moreover, etc.) has been compiled by examining the high frequency connectors from a separate set of papers from the same domain.", "labels": [], "entities": []}, {"text": "An example is: \"An additional consistent edge of a linearchain conditional random field (CRF) explicitly models the dependencies between distant occurrences of similar words).", "labels": [], "entities": []}, {"text": "However, this approach requires additional time complexity in inference/learning time and it is only suitable for representing constraints by enforcing label consistency.\"", "labels": [], "entities": []}, {"text": "\u2022 Si starts with a (sub)section heading.", "labels": [], "entities": []}, {"text": "\u2022 S i\u22121 starts with a (sub)section heading.", "labels": [], "entities": []}, {"text": "\u2022 S i+1 starts with a (sub)section heading.", "labels": [], "entities": []}, {"text": "The three features above area consequence of missing information about the paragraph and section boundaries in the used corpus.", "labels": [], "entities": []}, {"text": "Since the text extraction has been done automatically, the section headings are usually found to be merged with the text of the succeeding sentence.", "labels": [], "entities": [{"text": "text extraction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7012604922056198}]}, {"text": "For example, the text below merges the heading of section 4.2 with the next sentence.", "labels": [], "entities": []}, {"text": "\"4.2 METEOR vs. SIA SIA is designed to take the advantage of loose sequence-based metrics without losing word-level information.\"", "labels": [], "entities": [{"text": "METEOR", "start_pos": 5, "end_pos": 11, "type": "METRIC", "confidence": 0.9865797162055969}]}, {"text": "Start and end of such section boundaries can give us important information about the scope of a citation.", "labels": [], "entities": []}, {"text": "In order to exploit this information, we use regular expressions to detect if the sentences under review contains these merged section titles and headings.", "labels": [], "entities": []}, {"text": "\u2022 Si contains a citation other than the one under review.", "labels": [], "entities": []}, {"text": "It is more probable for the context of a citation to end when other citations are mentioned in a sentence, which is the motivation behind using this feature, which might contribute to the discriminating power of the classifier in conjunction with the presence of a citation in the previous sentence.", "labels": [], "entities": []}, {"text": "For example, in the extract below, the scope of the first citation is limited to the first sentence only.", "labels": [], "entities": []}, {"text": "\u2022 Si contains a lexical hook.", "labels": [], "entities": []}, {"text": "The lexical hooks feature identifies lexical substitutes for the citations.", "labels": [], "entities": []}, {"text": "We obtain these hooks by examining all explicit citation sentences to the cited paper and selecting the most frequent capitalized phrase in the vicinity of the author's last name.", "labels": [], "entities": []}, {"text": "The explicit citations come from all citing papers and not just the paper for which the features are being determined.", "labels": [], "entities": []}, {"text": "For example, the sentences below have been taken from two different papers and cite the same target paper (.", "labels": [], "entities": []}, {"text": "While the acronym HMM will be captured by the feature stated earlier, the word Xerox will be missed.", "labels": [], "entities": []}, {"text": "E95-1014: \"This text was part-of-speech tagged using the Xerox HMM tagger.\"", "labels": [], "entities": [{"text": "E95-1014", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9455190300941467}, {"text": "Xerox HMM tagger", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.8796625137329102}]}, {"text": "J97-3003: \"The Xerox tagger () comes with a set of rules that assign an unknown word a set of possible pos-tags (i.e. , POS-class) on the basis of its ending segment.\"", "labels": [], "entities": [{"text": "J97-3003", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9448542594909668}]}, {"text": "This 'domain level' feature makes it possible to extract the commonly used name fora technique which may have been missed by the acronym feature due to long term dependencies.", "labels": [], "entities": []}, {"text": "We also extrapolate the acronym for such phrases, e.g., in the example below, SCL would also be checked along with Structural Correspondence Learning.", "labels": [], "entities": []}, {"text": "\"The paper compares Structural Correspondence) with (various instances of) self-training) for the adaptation of a parse selection model to Wikipedia domains\" We also add n-grams of length 1 to 3 to this lexical feature set and compare the results obtained with an n-gram only baseline in: Comparison of F -scores for non-explicit citation detection.", "labels": [], "entities": [{"text": "F -scores", "start_pos": 303, "end_pos": 312, "type": "METRIC", "confidence": 0.9854170282681783}, {"text": "citation detection", "start_pos": 330, "end_pos": 348, "type": "TASK", "confidence": 0.767248272895813}]}, {"text": "By adding the new features listed above, the performance of our system increases by almost 8% over the n-gram baseline for the task of detecting citation mentions.", "labels": [], "entities": [{"text": "detecting citation mentions", "start_pos": 135, "end_pos": 162, "type": "TASK", "confidence": 0.857677181561788}]}, {"text": "Using the pairwise Wilcoxon rank-sum test at 0.05 significance level, we found that the difference between the baseline and our system is statistically significant 2 . While the micro-F score obtained is quite high, the individual class scores show that the task is hard and a better solution may require a deeper analysis of the context.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of classes.", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9732896089553833}]}, {"text": " Table 2: Comparison of F -scores for non-explicit  citation detection.", "labels": [], "entities": [{"text": "F -scores", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9796248475710551}, {"text": "non-explicit  citation detection", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.6611963311831156}]}, {"text": " Table 3: F -scores for citation sentiment detection.", "labels": [], "entities": [{"text": "F -scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9853520592053732}, {"text": "citation sentiment detection", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.9490634799003601}]}]}