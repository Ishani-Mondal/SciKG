{"title": [{"text": "Optimization and Sampling for NLP from a Unified Viewpoint", "labels": [], "entities": []}], "abstractContent": [{"text": "The OS* algorithm is a unified approach to exact optimization and sampling, based on incremental refinements of a functional upper bound, which combines ideas of adaptive rejection sampling and of A* optimization search.", "labels": [], "entities": [{"text": "exact optimization", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7854416966438293}]}, {"text": "We first give a detailed description of OS*.", "labels": [], "entities": []}, {"text": "We then explain how it can be applied to several NLP tasks, giving more details on two such applications: (i) decoding and sampling with a high-order HMM, and (ii) decoding and sampling with the intersection of a PCFG and a high-order LM.", "labels": [], "entities": []}], "introductionContent": [{"text": "Optimization and Sampling are usually seen as two completely separate tasks.", "labels": [], "entities": [{"text": "Sampling", "start_pos": 17, "end_pos": 25, "type": "TASK", "confidence": 0.9611626863479614}]}, {"text": "However, in NLP and many other domains, the primary objects of interest are often probability distributions over discrete or continuous spaces, for which both aspects are natural: in optimization, we are looking for the mode of the distribution, in sampling we would like to produce a statistically representative set of objects from the distribution.", "labels": [], "entities": []}, {"text": "The OS * algorithm approaches the two aspects from a unified viewpoint; it is a joint exact Optimization and Sampling algorithm that is inspired both by rejection sampling and by classical A * optimization (O S * ).", "labels": [], "entities": []}, {"text": "Common algorithms for sampling high-dimensional distributions are based on MCMC techniques, which are approximate in the sense that they produce valid samples only asymptotically.", "labels": [], "entities": []}, {"text": "By contrast, the elementary technique of Rejection Sampling directly produces exact samples, but, if applied naively to high-dimensional spaces, typically requires unacceptable time before producing a first sample.", "labels": [], "entities": [{"text": "Rejection Sampling", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9775085151195526}]}, {"text": "By contrast, OS * can be applied to high-dimensional spaces.", "labels": [], "entities": []}, {"text": "The main idea is to upper-bound the complex target distribution p by a simpler proposal distribution q, such that a dynamic programming (or another low-complexity) method can be applied to q in order to efficiently sample or maximize from it.", "labels": [], "entities": []}, {"text": "In the case of sampling, rejection sampling is then applied to q, and on a reject at point x, q is refined into a slightly more complex q in an adaptive way.", "labels": [], "entities": [{"text": "rejection sampling", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8577788174152374}]}, {"text": "This is done by using the evidence of the reject at x, which implies a gap between q(x) and p(x), to identify a (soft) constraint implicit in p which is not accounted for by q.", "labels": [], "entities": []}, {"text": "This constraint is then integrated into q to obtain q . The constraint which is integrated tends to be highly relevant and to increase the acceptance rate of the algorithm.", "labels": [], "entities": [{"text": "acceptance", "start_pos": 139, "end_pos": 149, "type": "METRIC", "confidence": 0.9485059976577759}]}, {"text": "By contrast, many constraints that are constitutive of pare never \"activated\" by sampling from q, because q never explores regions where they would become visible.", "labels": [], "entities": []}, {"text": "For example, anticipating our HMM experiments in section 3.1, there is little point in explicitly including in q a 5-gram constraint on a certain latent sequence in the HMM if this sequence is already unlikely at the bigram level: the bigram constraints present in the proposal q will ensure that this sequence will never (or very rarely) be explored by q.", "labels": [], "entities": []}, {"text": "The case of optimization is treated in exactly the same way as sampling.", "labels": [], "entities": []}, {"text": "Formally, this consists in moving from assessing proposals in terms of the L 1 norm to assessing them in terms of the L \u221e norm.", "labels": [], "entities": []}, {"text": "Typically, when a dynamic programming procedure is available for sampling (L 1 norm) with q, it is also available for maximizing from q (L \u221e norm), and the main difference between the two cases is then in the criteria for selecting among possible refinements.", "labels": [], "entities": []}, {"text": "Related work In order to improve the acceptance rate of rejection sampling, one has to lower the proposal q curve as much as possible while keeping it above the p curve.", "labels": [], "entities": [{"text": "rejection sampling", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7736904621124268}]}, {"text": "In order to do that, some authors, have proposed Adaptive Rejection Sampling (ARS) where, based on rejections, the q curve is updated to a lower curve q with a better acceptance rate.", "labels": [], "entities": [{"text": "Adaptive Rejection Sampling (ARS)", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.787887046734492}]}, {"text": "These techniques have predominantly been applied to continuous distributions on the one-dimensional real line, where convexity assumptions on the target distribution can be exploited to progressively approximate it tighter and tighter through upper bounds consisting of piecewise linear envelopes.", "labels": [], "entities": []}, {"text": "These sampling techniques have not been connected to optimization.", "labels": [], "entities": []}, {"text": "One can find a larger amount of related work in the optimization domain.", "labels": [], "entities": []}, {"text": "In an heuristic optimization context the two interesting, but apparently little known, papers, discuss a technique for decoding images based on high-order language models where upper-bounds are constructed in terms of simpler variable-order models.", "labels": [], "entities": []}, {"text": "Our application of OS * in section 3.1 to the problem of maximizing a high-order HMM is similar to their (also exact) technique; while this work seems to be the closest to ours, the authors do not attempt to generalize their approach to other optimization problems amenable to dynamic programming.", "labels": [], "entities": []}, {"text": "Among NLP applications, are another recent approach to exact optimization for sequence labelling that also has connections to our experiments in section 3.1, but differs in particular by using a less flexible refinement scheme than our variable-order n-grams.", "labels": [], "entities": [{"text": "exact optimization", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7382335066795349}]}, {"text": "In the NLP community again, there is currently a lot of interest for optimization methods that fall in the general category of \"coarse-to-fine\" techniques, which tries to guide the inference process towards promising regions that get incrementally tighter and tighter.", "labels": [], "entities": []}, {"text": "While most of this line of research concentrates on approximate optimization, some related approaches aim at finding an exact optimum.", "labels": [], "entities": [{"text": "approximate optimization", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.8756669759750366}]}, {"text": "Thus,] attempt to maximize a certain objective while respecting complex hard constraints, which they do by incrementally adding those constraints that are violated by the current optimum, using finite-state techniques.", "labels": [], "entities": []}, {"text": "[] have a similar goal, but address it by incrementally adding ILP (Integer Linear Programming) constraints.", "labels": [], "entities": []}, {"text": "Linear Programming techniques are also involved in the recent applications of Dual Decomposition to NLP, which can be applied to difficult combinations of easy problems, and often are able to find an exact optimum.", "labels": [], "entities": []}, {"text": "None of these optimization papers, to our knowledge, attempts to extend these techniques to sampling, in contrast to what we do.", "labels": [], "entities": []}, {"text": "Paper organization The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe the OS * algorithm, explain how it can be used for exact optimization and sampling, and show its connection to A * . In section 3, we first describe several NLP applications of the algorithm, then give more details on two such applications.", "labels": [], "entities": [{"text": "exact optimization", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7040776908397675}]}, {"text": "The first one is an application to optimization/sampling with high-order HMMs, where we also provide experimental results; the second one is a high-level description of its application to the generic problem of optimiza-tion/sampling with the intersection of a PCFG with a high-order language model, a problem which has recently attracted some attention in the community.", "labels": [], "entities": []}, {"text": "We finally conclude and indicate some perspectives.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: # of n-grams in our variable-order HMM.", "labels": [], "entities": []}]}