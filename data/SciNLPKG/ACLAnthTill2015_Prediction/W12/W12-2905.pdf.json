{"title": [], "abstractContent": [{"text": "Most icon-based augmentative and alternative communication (AAC) devices require users to formulate messages in syntactic order in order to produce syntactic utterances.", "labels": [], "entities": [{"text": "icon-based augmentative and alternative communication (AAC)", "start_pos": 5, "end_pos": 64, "type": "TASK", "confidence": 0.7108728215098381}]}, {"text": "Reliance on syntactic ordering, however, may not be appropriate for individuals with limited or emerging literacy skills.", "labels": [], "entities": [{"text": "syntactic ordering", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.728463888168335}]}, {"text": "Some of these users may benefit from unordered message formulation accompanied by automatic message expansion to generate syntactically correct messages.", "labels": [], "entities": []}, {"text": "Facilitating communication via unordered message formulation, however, requires new methods of prediction.", "labels": [], "entities": [{"text": "communication via unordered message formulation", "start_pos": 13, "end_pos": 60, "type": "TASK", "confidence": 0.7655392408370971}]}, {"text": "This paper describes a novel approach to word prediction using semantic grams, or \"sem-grams,\" which provide relational information about message components regardless of word order.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.8209976851940155}]}, {"text": "Performance of four word-level prediction algorithms, two based on sem-grams and two based on n-grams, were compared on a corpus of informal blogs.", "labels": [], "entities": [{"text": "word-level prediction", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7163399159908295}]}, {"text": "Results showed that sem-grams yield accurate word prediction , but lack prediction coverage.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.7657496333122253}]}, {"text": "Hybrid methods that combine n-gram and sem-gram approaches maybe viable for unordered prediction in AAC.", "labels": [], "entities": [{"text": "AAC", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.8229553699493408}]}], "introductionContent": [{"text": "Many individuals with severe speech impairments rely on augmentative and alternative communication (AAC) devices to convey their thoughts and desires.", "labels": [], "entities": []}, {"text": "Those with limited or emerging literacy skills may use icon-based systems, which often require that vocabulary items be selected in syntactic order to generate syntactically well-formed messages; however, selecting vocabulary items serially and in syntactic order can be physically and cognitively arduous depending on the icon organization scheme).", "labels": [], "entities": []}, {"text": "Moreover, AAC productions are often syntactically incomplete or incorrect (Van Balkom and Welle Donker-Gimbrere, 1996), perhaps for efficiency or due to limited linguistic abilities.", "labels": [], "entities": [{"text": "AAC", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9406489729881287}]}, {"text": "For many users, unordered vocabulary selection may alleviate the physical and cognitive demands of message formulation and shift the onus of generating syntactically complete and accurate messages onto the AAC device.", "labels": [], "entities": [{"text": "message formulation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7399239093065262}]}, {"text": "Although unordered message formulation schemes have been proposed () and techniques have been developed for expanding incomplete input (), prediction has not been incorporated.", "labels": [], "entities": [{"text": "message formulation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7428695559501648}, {"text": "prediction", "start_pos": 139, "end_pos": 149, "type": "TASK", "confidence": 0.9472289085388184}]}, {"text": "This paper presents an initial step toward text prediction from a set of unordered vocabulary selections.", "labels": [], "entities": [{"text": "text prediction", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7914469540119171}]}, {"text": "Rate enhancement is a commonly cited issue in AAC because aided message formulation rates are an order of magnitude slower than spoken interaction).", "labels": [], "entities": [{"text": "Rate enhancement", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6466599702835083}, {"text": "AAC", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9670332074165344}, {"text": "message formulation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.6950773000717163}]}, {"text": "Prediction is a common rate enhancement technique.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.7155946493148804}]}, {"text": "Text prediction for AAC has primarily focused on wellordered, syntactic input and has leveraged both semantic characteristics and variations of n-grams ().", "labels": [], "entities": [{"text": "Text prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8161691427230835}, {"text": "AAC", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.8498924970626831}]}, {"text": "For example, semantic networks and linguistic rules have been used to predict missing function words and to apply affixes to content).", "labels": [], "entities": []}, {"text": "The use of n-grams to predict text entry has been extensively studied at both the level of letters () and words ().", "labels": [], "entities": [{"text": "predict text entry", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7347829242547353}]}, {"text": "For example, memory based language models have been used to predict missing content words using trigrams).", "labels": [], "entities": []}, {"text": "Although some recent work has attempted to loosen syntactic requirements by including either left or right context, some directional context has historically been required.", "labels": [], "entities": []}, {"text": "Furthermore, word prediction approaches in AAC have typically been implemented for letter-by-letter message formulation (.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.845927357673645}, {"text": "AAC", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9523332118988037}, {"text": "letter-by-letter message formulation", "start_pos": 83, "end_pos": 119, "type": "TASK", "confidence": 0.7201330463091532}]}, {"text": "The current work is fundamentally novel in that: (1) no syntactic order is implied or required during either training or testing; and (2) the prediction is implemented at word level to accommodate icon-based interaction.", "labels": [], "entities": []}, {"text": "Previous work in information retrieval has explored relationships between words with regard to distance, grammatical purpose (), and semantic characteristics, particularly for retrieving highly relevant documents or passages.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7295737415552139}]}, {"text": "One study in this area resulted in an approach called sgrams, a generalization of n-grams, in which the distance between words directly affects the strength of their semantic relationship.", "labels": [], "entities": []}, {"text": "Another approach to predicting semantically related words is to use collocation to indicate topic changes within a moving window of fixed length.", "labels": [], "entities": [{"text": "predicting semantically related words", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.8651073127985001}]}, {"text": "Rather than relying on distance to indicate relationship strength, the current work combines frequency analysis with syntactic indications of semantic coherence.", "labels": [], "entities": []}], "datasetContent": [{"text": "Testing was conducted on 2,000 sentences that were randomly selected from the test corpus.", "labels": [], "entities": []}, {"text": "The same processing steps used during training were performed on the test sentences: stop words were removed, the remaining words were stemmed, and all stems not in the dictionary were filtered out.", "labels": [], "entities": []}, {"text": "To avoid run-on sentences and sentence boundary detection errors, all test sentences were also truncated to a maximum of 20 words.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.5977005561192831}]}, {"text": "The words in each test sentence were then shuffled and one word was removed at random and designated as the target word.", "labels": [], "entities": []}, {"text": "Each of the four algorithms were provided the shuffled words as input; as output, each algorithm attempted to identify the target word by generating a ranked list of candidates).", "labels": [], "entities": []}, {"text": "In addition to the shuffled multiset of input words, each algorithm required a seed list of candidate words.", "labels": [], "entities": []}, {"text": "Ideally, all known words in the corpus would be used as candidate words.", "labels": [], "entities": []}, {"text": "To constrain the computational requirements, the two algorithms based on n-grams (N1 and N2) were provided with the list of most frequently co-occurring words that appeared as n-grams with any of the multiset of input words, limited to the top 10 n-grams fora given input word.", "labels": [], "entities": []}, {"text": "Similarly, each sem-gram algorithm (S1 and S2) received a list of most frequently co-occurring words that appeared as sem-grams with any of the multiset of input words, limited to the top 10 sem-grams fora given input word.", "labels": [], "entities": []}, {"text": "With a limit of 19 input words (20 minus the target word), each algorithm received at most 190 unique candidate words to rank.", "labels": [], "entities": []}, {"text": "Two evaluation metrics were used to quantify the performance of each algorithm: (1) a boolean value that was true if the output list contained the target word in any position, indicating that the target word had been successfully predicted; (2) if the algorithm successfully predicted the target word, the algorithm received a positive integer score corresponding to the position of the target word in the output list, with lower scores indicating more accurate prediction.", "labels": [], "entities": []}, {"text": "For example, if an algorithm suggested the target word as the first item in its ranked list, it received a score of 1; if it suggested the target word as the second item in its ranked list, it received a score of 2.", "labels": [], "entities": []}, {"text": "For computational convenience, the output lists of each algorithm were truncated to the first 100 items; thus, if an algorithm's output list contained the target word in a position after 100, it was marked as failing to predict the target word.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Summary of Results", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.5650898218154907}]}, {"text": " Table 4: Prediction Coverage (%) and Average Scores by Sentence Length", "labels": [], "entities": [{"text": "Prediction Coverage", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8664277791976929}, {"text": "Average Scores", "start_pos": 38, "end_pos": 52, "type": "METRIC", "confidence": 0.9823507964611053}]}]}