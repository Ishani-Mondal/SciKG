{"title": [{"text": "Literary authorship attribution with phrase-structure fragments", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a method of authorship attribution and stylometry that exploits hierarchical information in phrase-structures.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.715882807970047}]}, {"text": "Contrary to much previous work in stylometry, we focus on content words rather than function words.", "labels": [], "entities": []}, {"text": "Texts are parsed to obtain phrase-structures, and compared with texts to be analyzed.", "labels": [], "entities": []}, {"text": "An efficient tree kernel method identifies common tree fragments among data of known authors and unknown texts.", "labels": [], "entities": []}, {"text": "These fragments are then used to identify authors and characterize their styles.", "labels": [], "entities": []}, {"text": "Our experiments show that the structural information from fragments provides complementary information to the baseline trigram model.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of authorship attribution (for an overview cf.) is typically performed with superficial features of texts such as sentence length, word frequencies, and use of punctuation & vocabulary.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.6699734181165695}]}, {"text": "While such methods attain high accuracies (e.g.,, the models make purely statistical decisions that are difficult to interpret.", "labels": [], "entities": []}, {"text": "To overcome this we could turn to higher-level patterns of texts, such as their syntactic structure.", "labels": [], "entities": []}, {"text": "Syntactic stylometry was first attempted by, who looked at the distribution of frequencies of grammar productions.", "labels": [], "entities": [{"text": "Syntactic stylometry", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8730550408363342}]}, {"text": "1 More recently, identified authors by deriving a probabilistic grammar for each author and picking the author grammar that can parse the unidentified A grammar production is a rewrite rule that generates a constituent.", "labels": [], "entities": []}, {"text": "text with the highest probability.", "labels": [], "entities": []}, {"text": "There is also work that looks at syntax on a more shallow level, such as, who work with partial parses; Wiersma et al.", "labels": [], "entities": []}, {"text": "(2011) looked at n-grams of part-of-speech (POS) tags, and Menon and Choi (2011) focussed on particular word frequencies such as those of 'stop words,' attaining accuracies well above 90% even in cross-domain tasks.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 162, "end_pos": 172, "type": "METRIC", "confidence": 0.9864700436592102}]}, {"text": "In this work we also aim to perform syntactic stylometry, but we analyze syntactic parse trees directly, instead of summarizing the data as a set of grammar productions or a probability measure.", "labels": [], "entities": []}, {"text": "The unit of comparison is tree fragments.", "labels": [], "entities": []}, {"text": "Our hypothesis is that the use of fragments can provide a more interpretable model compared to one that uses fine-grained surface features such as word tokens.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our data consist of a collection of novels from five authors.", "labels": [], "entities": []}, {"text": "We perform cross-validation on 4 works per author.", "labels": [], "entities": []}, {"text": "We evaluate on two different test sizes: 20 and 100 sentences.", "labels": [], "entities": []}, {"text": "We test with a total of 500 sentences per work, which gives 25 and 5 datapoints per work given these sizes.", "labels": [], "entities": []}, {"text": "As training sets only the works that are not tested on are presented to the model.", "labels": [], "entities": []}, {"text": "The training sets consist of 15,000 sentences taken from the remaining works.", "labels": [], "entities": []}, {"text": "Evaluating the model on these test sets took about half an hour on a machine with 16 cores, employing less than 100 MB of memory per process.", "labels": [], "entities": []}, {"text": "The similarity functions were explored on a development set, the results reported here are from a separate test set.", "labels": [], "entities": []}, {"text": "The authorship attribution results are in table 2.", "labels": [], "entities": []}, {"text": "It is interesting to note that even with three different translators, the work of Tolstoy can be successfully identified; i.e., the style of the author is modelled, not the translator's.", "labels": [], "entities": []}, {"text": "Gamon also classifies chunks of 20 sentences, but note that in his methodology data for training and testing includes sentences from the same work.", "labels": [], "entities": []}, {"text": "Recognizing the same work is easier because of recurring topics and character names.", "labels": [], "entities": []}, {"text": "uses opinion columns of 500-2,000 words, which amounts to 25-100 sentences, assuming an average sentence length of 20 words.", "labels": [], "entities": []}, {"text": "Most of the individual algorithms in score much lower than our method, when classifying among 5 possible authors like we do, while the accuracies are similar when many algorithms are combined into an ensemble.", "labels": [], "entities": []}, {"text": "Although the corpus of Grieve is carefully controlled to contain comparable texts written for the same audience, our task is not necessarily easier, because large differences within the works of an author can make classifying that author more challenging.", "labels": [], "entities": [{"text": "Grieve", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.7109978199005127}]}, {"text": "shows a confusion matrix when working with 20 sentences.", "labels": [], "entities": []}, {"text": "It is striking that the errors are relatively asymmetric: if A is often confused with B, it does not imply that B is often confused with A.", "labels": [], "entities": []}, {"text": "This appears to indicate that the similarity metric has a bias towards certain categories which could be removed with a more principled model.", "labels": [], "entities": []}, {"text": "Here are some examples of sentence-level and productive fragments that were found: It is likely that more sophisticated statistics, for example methods used for collocation detection, or general machine learning methods to select features such as support vector machines would allow to select only the most characteristic fragments.", "labels": [], "entities": [{"text": "collocation detection", "start_pos": 161, "end_pos": 182, "type": "TASK", "confidence": 0.7738318741321564}]}], "tableCaptions": [{"text": " Table 2: Accuracy in % for authorship attribution with test texts of 20 or 100 sentences.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994924068450928}]}, {"text": " Table 3: Confusion matrix when looking at 20 sentences  with trigrams and fragments combined. The rows are the  true authors, the columns the predictions of the model.", "labels": [], "entities": []}]}