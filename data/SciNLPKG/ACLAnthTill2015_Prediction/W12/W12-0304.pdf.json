{"title": [{"text": "Google Books N-gram Corpus used as a Grammar Checker", "labels": [], "entities": []}], "abstractContent": [{"text": "In this research we explore the possibility of using a large n-gram corpus (Google Books) to derive lexical transition probabilities from the frequency of word n-grams and then use them to check and suggest corrections in a target text without the need for grammar rules.", "labels": [], "entities": []}, {"text": "We conduct several experiments in Spanish, although our conclusions also reach other languages since the procedure is corpus-driven.", "labels": [], "entities": []}, {"text": "The paper reports on experiments involving different types of grammar errors, which are conducted to test different grammar-checking procedures , namely, spotting possible errors, deciding between different lexical possibilities and filling-in the blanks in a text.", "labels": [], "entities": [{"text": "spotting possible errors", "start_pos": 154, "end_pos": 178, "type": "TASK", "confidence": 0.8649213711420695}]}], "introductionContent": [{"text": "This paper discusses a series of early experiments on a methodology for the detection and correction of grammatical errors based on co-occurrence statistics using an extensive corpus of n-grams (Google Books, compiled by).", "labels": [], "entities": [{"text": "detection and correction of grammatical errors", "start_pos": 76, "end_pos": 122, "type": "TASK", "confidence": 0.8059284786383311}]}, {"text": "We start from two complementary assumptions: on the one hand, books are published accurately, that is to say, they usually go through different phases of revision and correction with high standards and thus a large proportion of these texts can be used as a reference corpus for inferring the grammar rules of a language.", "labels": [], "entities": []}, {"text": "On the other hand, we hypothesise that with a sufficiently large corpus a high percentage of the information about these rules can be extracted with word n-grams.", "labels": [], "entities": []}, {"text": "Thus, although there are still many grammatical errors that cannot be detected with this method, there is also another important group which can be identified and corrected successfully, as we will see in Section 4.", "labels": [], "entities": []}, {"text": "Grammatical errors are the most difficult and complex type of language errors, because grammar is made up of a very extensive number of rules and exceptions.", "labels": [], "entities": []}, {"text": "Furthermore, when grammar is observed in actual texts, the panorama becomes far more complicated, as the number of exceptions grows and the variety and complexity of syntactical structures increase to an extent that is not predicted by theoretical studies of grammar.", "labels": [], "entities": []}, {"text": "Grammar errors are extremely important, and the majority of them cannot be considered to be performance-based because it is the meaning of the text and therefore, the successor failure of communication, that is compromised.", "labels": [], "entities": []}, {"text": "To our knowledge, no grammar book or dictionary has yet provided a solution to all the problems a person may have when he or she writes and tries to follow the grammar rules of language.", "labels": [], "entities": []}, {"text": "Doubts that arise during the writing process are not always clearly associated to a lexical unit, or the writer is notable to detect such an association, and this makes it difficult to find the solution using a reference book.", "labels": [], "entities": []}, {"text": "In recent years, some advances have been made in the automatic detection of grammar mistakes (see Section 2).", "labels": [], "entities": [{"text": "automatic detection of grammar mistakes", "start_pos": 53, "end_pos": 92, "type": "TASK", "confidence": 0.8110514104366302}]}, {"text": "Effective rule-based methods have been reported, but at the cost of a very timeconsuming task and with an inherent lack of flexibility.", "labels": [], "entities": []}, {"text": "In contrast, statistical methods are easier and faster to implement, as well as being more flexible and adaptable.", "labels": [], "entities": []}, {"text": "The experiment we will describe in the following sections is the first part of a more extensive study.", "labels": [], "entities": []}, {"text": "Most probably, the logical step to follow in order to continue such a study will be a hybrid approach, based on both statistics and rules.", "labels": [], "entities": []}, {"text": "Hence, this paper aims to contribute to the statistical approach applied to grammar checking.", "labels": [], "entities": [{"text": "grammar checking", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.9341371655464172}]}, {"text": "The Google Books N-gram Corpus is a database of n-grams of sequences of up to 5 words and records the frequency distribution of each unit in each year from 1500 onwards.", "labels": [], "entities": [{"text": "Google Books N-gram Corpus", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.9458539485931396}]}, {"text": "The bulk of the corpus, however, starts from 1970, and that is the year we took as a starting point for the material that we used to compile our reference corpus.", "labels": [], "entities": []}, {"text": "The idea of using this database as a grammar checker is to analyse an input text and detect any sequence of words that cannot be found in the n-gram database (which only contains n-grams with frequency equal to or greater than 40) and, eventually, to replace a unit in the text with one that makes a frequent n-gram.", "labels": [], "entities": []}, {"text": "More specifically, we conduct four types of operations: accepting a text and spotting possible errors; inflecting a lemma into the appropriate form in a given context; filling-in the blanks in a text; and selecting, from a number of options, the most probable word form fora given context.", "labels": [], "entities": []}, {"text": "In order to evaluate the algorithm, we applied it to solve exercises from a Spanish grammar book and also tested the detection of errors in a corpus of real errors made by second language learners.", "labels": [], "entities": []}, {"text": "The paper is organised as follows: we first offer a brief description of related work, and then explain our methodology for each of the experiments.", "labels": [], "entities": []}, {"text": "In the next section, we show the evaluation of the results in comparison to the Microsoft Word grammar checker and, finally, we draw some conclusions and discuss lines of future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Summary of the results obtained by our algorithm in comparison to Word 2007", "labels": [], "entities": [{"text": "Word 2007", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.96473827958107}]}, {"text": " Table 2: Replication of the experiment with a corpus of non-native speakers (CEDEL2, Lozano, 2009)", "labels": [], "entities": [{"text": "CEDEL2, Lozano, 2009)", "start_pos": 78, "end_pos": 99, "type": "DATASET", "confidence": 0.9005710581938425}]}, {"text": " Table 3: Solution of the multiple choice exercise", "labels": [], "entities": []}, {"text": " Table 4: Results of the inflection exercise", "labels": [], "entities": []}, {"text": " Table 5: Results of the fill-in-the-blank exercise", "labels": [], "entities": []}]}