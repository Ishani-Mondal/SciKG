{"title": [{"text": "Learning Improved Reordering Models for Urdu, Farsi and Italian using SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.798085629940033}]}], "abstractContent": [{"text": "This paper presents some experiments which have been carried out as part of a shared task for the workshop \"Reordering for Statistical Machine Translation\" (RSMT, collocated with COLING 2012).", "labels": [], "entities": [{"text": "Statistical Machine Translation\" (RSMT, collocated with COLING 2012)", "start_pos": 123, "end_pos": 191, "type": "TASK", "confidence": 0.6718681578834852}]}, {"text": "The shared task objective is to learn reordering models by making use of a manually word-aligned, bilingual parallel data.", "labels": [], "entities": []}, {"text": "We view this task as that of a statistical machine translation (SMT) system which implicitly employ such models.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.7726748784383138}]}, {"text": "These models are obtained using empirical methods and machine learning techniques.", "labels": [], "entities": []}, {"text": "We have therefore used \"Moses\"; a state of the art SMT system to conduct experiments for the task at hand.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9916397929191589}]}, {"text": "The training and the development datasets used for the experiments have been provided by RSMT and we report our work on three pair of languages namely English-Urdu, English-Farsi and English-Italian.", "labels": [], "entities": [{"text": "RSMT", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.8074154257774353}]}], "introductionContent": [{"text": "The objective of the shared task is to learn reordering models by making use of humanannotated parallel data which is word aligned.", "labels": [], "entities": []}, {"text": "We have transformed this task into one of empirical machine translation where model parameters for the system are learnt using parallel training data and machine learning techniques.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7313943207263947}]}, {"text": "A statistical machine translation (SMT) system primarily relies on two models viz.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 2, "end_pos": 39, "type": "TASK", "confidence": 0.8209580332040787}]}, {"text": "the translation model (TM) and the language model (LM).", "labels": [], "entities": []}, {"text": "In essence, it involves learning mutual correspondences using bilingual parallel data and reducing divergences among the source-target language pair.", "labels": [], "entities": []}, {"text": "The alignment models which help establish such links, and the reordering models which help reduce the word order differences in the sourcetarget pair constitute apart of the TM and are an implicit part of such an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 213, "end_pos": 216, "type": "TASK", "confidence": 0.9918054342269897}]}, {"text": "Thus, our motivation to use the SMT system for the shared task comes from the alignment model GIZA++ and the basic reordering model (distance based distortion) employed in the Moses ( ) framework.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9928102493286133}]}, {"text": "Section 2 briefly explains the GIZA++ alignment model and Section 3 elaborates on the reordering model.", "labels": [], "entities": [{"text": "GIZA++ alignment", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.5875620245933533}]}, {"text": "Across many language pairs, the existing SMT systems are usually infested with alack of resources which leads to reduced annotations on the source and/or target side.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9877851009368896}]}, {"text": "Use of machine learning techniques, data preprocessing or other heuristics is mostly employed to overcome this lack of information and estimate good translation models.", "labels": [], "entities": []}, {"text": "However, the training data provided in this shared task has the necessary alignment information on both sides.", "labels": [], "entities": []}, {"text": "Availability of such information initially motivated us to use simple techniques of chunking based on source-target index information thereby modeling large distance word movements.", "labels": [], "entities": []}, {"text": "However, we failed in these initial experiments which resulted in even lesser scores than those trained on a phrase based baseline system.", "labels": [], "entities": []}, {"text": "Therefore, the experiments were planned with only a scope of 1.", "labels": [], "entities": []}, {"text": "training a baseline phrase based translation model; elaborated in Section 4. 2. training a factored translation model ( ) with linguistic annotations as factors; explained in Section 5.", "labels": [], "entities": []}, {"text": "The BLEU () score was the evaluation metric chosen to compare various results.", "labels": [], "entities": [{"text": "BLEU () score", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9611250758171082}]}, {"text": "The experiments and results are detailed in Section 6, followed by conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The focus of this task as mentioned above is to learn the alignment information from the training data.", "labels": [], "entities": []}, {"text": "Since the given data was in the CoNLL-X format, some preprocessing was done to obtain sentence aligned source and target data files for all language pairs.", "labels": [], "entities": []}, {"text": "A distinct pair of source and target files was created for sentences containing indices, surface word forms and part-of-speech (POS) forms.", "labels": [], "entities": []}, {"text": "In order to run trials on the phrase based and factored model, the data was split as per  A pair of trials was conducted with phrase based and factor based approach each with default parameters and tuned parameters.", "labels": [], "entities": []}, {"text": "The Moses default setting sets the distortion limit to 6.", "labels": [], "entities": [{"text": "distortion", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9844602942466736}]}, {"text": "Therefore, if no. of words skipped is greater than 6 the translation will be pruned.", "labels": [], "entities": [{"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.7757208347320557}]}, {"text": "This puts hard constraint and makes the model less suitable for more syntactically divergent languages like Urdu, Hindi, and Marathi etc.", "labels": [], "entities": []}, {"text": "According to the choice of parameters, the correct reordering is sometimes improbable for large scale reordering.", "labels": [], "entities": []}, {"text": "Thus, we have varied the distortion limits from 3 to 12 and observed the results for all trials.", "labels": [], "entities": [{"text": "distortion", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9749233722686768}]}, {"text": "Surface word form training was done for phrase based TM.", "labels": [], "entities": [{"text": "phrase based TM", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.6957107384999593}]}, {"text": "We trained this baseline system with the original source sentences and the target reordered sentences.", "labels": [], "entities": []}, {"text": "For the factor based TM, we used a training data containing the surface form word and a POS tag (as an additional factor).", "labels": [], "entities": []}, {"text": "Additionally, the training script included a mapping for translation-factors.", "labels": [], "entities": []}, {"text": "The results of the experiments on both approaches were evaluated for two test datasets.", "labels": [], "entities": []}, {"text": "The first test dataset (test1) was obtained from splitting the provided data (ref and the second test dataset (test2) is the same on which the task results were announced.", "labels": [], "entities": []}, {"text": "For BLEU evaluation and comparison, we requested the reference set for test2 from the RSMT organizers.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9720707535743713}]}, {"text": "The results without tuning and with default parameter settings of Moses are shown in  For tuning, the development data of 500 sentences was used.", "labels": [], "entities": []}, {"text": "We evaluated results for varying distortion limit values after tuning.", "labels": [], "entities": []}, {"text": "The motivation for this comes from the fact that the distance-based distortion model is placed as a weak model for highly divergent languages and our task is to learn reordering using alignments.", "labels": [], "entities": []}, {"text": "Hence, the evaluation results might inform about the extent of reordering expected by each language pair.", "labels": [], "entities": []}, {"text": "The evaluation results in the for each pair of languages have been plotted below against varying distortion limit values.", "labels": [], "entities": []}, {"text": "The dotted line in the plot represents phrase based values and the solid line represents the factor based values obtained after tuning.", "labels": [], "entities": []}, {"text": "For a consistent comparison of the test results with that of the system, the scores obtained on the test2 dataset are also plotted.", "labels": [], "entities": []}], "tableCaptions": []}