{"title": [{"text": "KRAKEN: N-ary Facts in Open Information Extraction", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8078373670578003}, {"text": "N-ary Facts in Open Information Extraction", "start_pos": 8, "end_pos": 50, "type": "TASK", "confidence": 0.5091909319162369}]}], "abstractContent": [{"text": "Current techniques for Open Information Extraction (OIE) focus on the extraction of binary facts and suffer significant quality loss for the task of extracting higher order N-ary facts.", "labels": [], "entities": [{"text": "Open Information Extraction (OIE)", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.7897256364425024}]}, {"text": "This quality loss may not only affect the correctness, but also the completeness of an extracted fact.", "labels": [], "entities": []}, {"text": "We present KRAKEN, an OIE system specifically designed to capture N-ary facts, as well as the results of an experimental study on extracting facts from Web text in which we examine the issue of fact completeness.", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.7888867259025574}]}, {"text": "Our preliminary experiments indicate that KRAKEN is a high precision OIE approach that captures more facts per sentence at greater completeness than existing OIE approaches , but is vulnerable to noisy and un-grammatical text.", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.7945109009742737}]}], "introductionContent": [{"text": "For the task of fact extraction from billions of Web pages the method of Open Information Extraction (OIE)) trains domainindependent extractors.", "labels": [], "entities": [{"text": "fact extraction from billions of Web pages", "start_pos": 16, "end_pos": 58, "type": "TASK", "confidence": 0.8601999112537929}, {"text": "Open Information Extraction (OIE))", "start_pos": 73, "end_pos": 107, "type": "TASK", "confidence": 0.7253509362538656}]}, {"text": "This important characteristic enables a potential application of OIE for even very large corpora, such as the Web.", "labels": [], "entities": []}, {"text": "Existing approaches for OIE, such as REVERB), WOE ( or WANDER-LUST) focus on the extraction of binary facts, e.g. facts that consist of only two arguments, as well as a fact phrase which denotes the nature of the relationship between the arguments.", "labels": [], "entities": [{"text": "OIE", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.4529707133769989}, {"text": "REVERB", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9766942262649536}, {"text": "WOE", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9340998530387878}, {"text": "WANDER-LUST", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9039645791053772}]}, {"text": "However, a recent analysis of OIE based on Semantic Role) revealed that N-ary facts (facts that connect more than two arguments) were present in 40% of surveyed English sentences.", "labels": [], "entities": [{"text": "OIE", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.6264992952346802}]}, {"text": "Worse, the analyses performed in and show that incorrect handling of N-ary facts leads to extraction errors, such as incomplete, uninformative or erroneous facts.", "labels": [], "entities": []}, {"text": "Our first example illustrates the case of a significant information loss: a) In the 2002 film Bubba Ho-tep, Elvis lives in a nursing home.", "labels": [], "entities": []}, {"text": "REVERB: LivesIn(Elvis, nursing home) In this case, the OIE system ignores the significant contextual information in the argument the 2002 film Bubba Ho-tep, which denotes the domain in which the fact LivesIn(Elvis, nursing home) is true.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9719528555870056}]}, {"text": "As a result, and by itself, the extracted fact is false.", "labels": [], "entities": []}, {"text": "The next example shows a binary fact from a sentence that de-facto expresses an N-ary fact.", "labels": [], "entities": []}, {"text": "Contrary to the previous example, the OIE systems extracted two binary facts that are not false, but incomplete, as the interaction between all three entities in this sentence can only be adequately modeled using an ternary fact.", "labels": [], "entities": []}, {"text": "The fact MovedIn for example misses an important aspect, namely the location Elvis moved to in 1948.", "labels": [], "entities": [{"text": "MovedIn", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.8285503387451172}]}, {"text": "Therefore, each of these two facts is an example of important, but not crucial information loss.", "labels": [], "entities": [{"text": "information loss", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.6991101950407028}]}, {"text": "Unfortunately, current OIE systems are not designed to capture the complete set of arguments for 52 each fact phrase within a sentence and to link arguments into an N-ary fact.", "labels": [], "entities": []}, {"text": "We view intra-sentence fact completeness as a major measure of data quality.", "labels": [], "entities": []}, {"text": "Following existing work from () complete factual data is a key for advanced data cleansing tasks, such as fact de-duplication, object resolution across N-ary facts, semantic fact interpretation and corpus wide fact aggregation.", "labels": [], "entities": [{"text": "data cleansing", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.83018359541893}, {"text": "object resolution", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.8147976398468018}, {"text": "semantic fact interpretation", "start_pos": 165, "end_pos": 193, "type": "TASK", "confidence": 0.6897798379262289}, {"text": "corpus wide fact aggregation", "start_pos": 198, "end_pos": 226, "type": "TASK", "confidence": 0.632169634103775}]}, {"text": "Therefore we argue that complete facts may serve a human reader or an advanced data cleansing approach as additional clue for interpreting and validating the fact.", "labels": [], "entities": [{"text": "data cleansing", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.8348709940910339}]}, {"text": "In order to investigate the need and feasibility for N-ary OIE we have performed the following, the results of which we present in this paper: 1.", "labels": [], "entities": []}, {"text": "We introduce the OIE system KRAKEN, which has been built specifically for capturing complete facts from sentences and is capable of extracing unary, binary and higher order N-ary facts.", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.408130019903183}]}, {"text": "2. We examine intra sentence fact correctness (true/false) and fact completeness for KRAKEN and REVERB on the corpus of).", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9827425479888916}, {"text": "REVERB", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9878342151641846}]}, {"text": "In the rest of the paper we review earlier work and outline KRAKEN, our method for extracting N-ary facts and contextual information.", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9776625037193298}, {"text": "extracting N-ary facts and contextual information", "start_pos": 83, "end_pos": 132, "type": "TASK", "confidence": 0.7694730361302694}]}, {"text": "Next, we describe our experiments and end with conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare REVERB, the state-of-the-art in binary fact extraction, with KRAKEN, in order to measure the effect of using N-ary fact extraction over purely binary extractors on overall precision and completeness.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9982520937919617}, {"text": "binary fact extraction", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6230239868164062}, {"text": "KRAKEN", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9966573715209961}, {"text": "precision", "start_pos": 183, "end_pos": 192, "type": "METRIC", "confidence": 0.9991118311882019}]}, {"text": "Additionally, we test in how far using an IE approach based on deep syntactic parsing can be used for sentences from the Web, which have a higher chance of being ungrammatical or noisy.", "labels": [], "entities": []}, {"text": "1 http://nlp.stanford.edu/software/dependencies  Data set: We use the data set from) which consists of 500 sentences sampled from the Web using Yahoo's random link service.", "labels": [], "entities": []}, {"text": "The sentences were labeled both with facts found with KRAKEN and the current version of REVERB.", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9901477694511414}, {"text": "REVERB", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9696971774101257}]}, {"text": "We then paired facts for the same sentence that overlap in at least one of the fact phrase words, in order to present to the judges two different versions of the same fact -often one binary (REVERB) and one Nary (KRAKEN).", "labels": [], "entities": [{"text": "REVERB", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.9910474419593811}, {"text": "Nary (KRAKEN)", "start_pos": 207, "end_pos": 220, "type": "METRIC", "confidence": 0.8898324370384216}]}, {"text": "Measurements/Instructions: Given a sentence and a fact (or fact-pair), we asked two human judges to label each fact as either 1) true and complete, 2) true and incomplete, or 3) false.", "labels": [], "entities": []}, {"text": "True and incomplete facts either lack contextual information in the form of arguments that were present in the sentence, or contain underspecified arguments, but are nevertheless valid statements in themselves (see our examples in Section 1   facts have been counted as true.", "labels": [], "entities": []}, {"text": "We distinguish them from true and complete facts that capture all relevant arguments as given by the sentence they were extracted from.", "labels": [], "entities": []}, {"text": "We measured an inter-annotator agreement of 87%, differently evaluated facts were discussed by the judges and resolved.", "labels": [], "entities": []}, {"text": "Most disagreement was caused by facts with underspecified arguments, labeled as false by one judge and as true and incomplete by the other.", "labels": [], "entities": []}, {"text": "KRAKEN extracts higher order N-ary facts.", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8788431286811829}]}, {"text": "show results for KRAKEN and REVERB.", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.8440499901771545}, {"text": "REVERB", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9781991839408875}]}, {"text": "We measured results for REVERB with different confidence thresholds.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.997051477432251}]}, {"text": "In all measurements, we observe a significantly higher number of true and complete facts for KRAKEN, as well as both a higher overall precision and number of facts extracted per sentence.", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.716313898563385}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9991558790206909}]}, {"text": "The completeness, measured as the ratio of complete facts overall true facts, is also significantly higher for KRAKEN.", "labels": [], "entities": [{"text": "completeness", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9659922122955322}, {"text": "KRAKEN", "start_pos": 111, "end_pos": 117, "type": "DATASET", "confidence": 0.6781641840934753}]}, {"text": "breaks down the fact arity.", "labels": [], "entities": []}, {"text": "KRAKEN performs particularly well for binary, ternary and 4-ary facts, which are also most common.", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7314892411231995}]}, {"text": "We conclude that even though our ruleset was generated on a different domain (Wikipedia text), it generalizes well to the Web domain.", "labels": [], "entities": []}, {"text": "Dependency parsing of Web text.", "labels": [], "entities": [{"text": "Dependency parsing of Web text", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8876841306686402}]}, {"text": "One major drawback of the settings we used is our (possibly too crude) heuristic for detecting erroneous dependency parses: We set KRAKEN to extract facts from all sentences in which the dependency parse does not contain the typed dependency dep, which indicates unclear grammatical relationships.", "labels": [], "entities": [{"text": "detecting erroneous dependency parses", "start_pos": 85, "end_pos": 122, "type": "TASK", "confidence": 0.5886887684464455}]}, {"text": "A total of 155 sentences -31% of the overall evaluation set -were skipped as a consequence.", "labels": [], "entities": []}, {"text": "Also, the elapsed time of the fact extraction process was more than one order of magnitude longer than REVERB, possibly limit- ing the ability of the system to scale to very large collections of documents.", "labels": [], "entities": [{"text": "fact extraction", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7982324361801147}, {"text": "REVERB", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9968955516815186}]}, {"text": "Measurements over different sentence lengths.", "labels": [], "entities": []}, {"text": "When limiting the maximum number of words allowed per sentence, we note modest gains in precision and losses incomplete positives in both systems, see.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9994105100631714}]}, {"text": "KRAKEN performs well even on long sentences, extracting more true and complete positives at a high precision.", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9365407824516296}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9740641117095947}]}, {"text": "Based on these observations, we reach the conclusion that given the 'right portion' of sentences from a collection such as the Web, our method for N-ary OIE can be very effective, extracting more complete facts with a high precision and fact-per-sentence rate.", "labels": [], "entities": [{"text": "precision", "start_pos": 223, "end_pos": 232, "type": "METRIC", "confidence": 0.9986640214920044}]}, {"text": "Sentences that are well suited for our algorithm must fulfill the following desiderata: 1) They are noise free and grammatically correct, so there is a high chance fora correct parse.", "labels": [], "entities": []}, {"text": "2) They are fact-rich, so that processing resources are wisely used.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The results of the comparative evaluation.  KRAKEN nearly doubles the amount of recognized com- plete and true facts.", "labels": [], "entities": [{"text": "KRAKEN", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.997616171836853}]}]}