{"title": [{"text": "Boosting the protein name recognition performance by bootstrapping on selected text", "labels": [], "entities": [{"text": "protein name recognition", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.6832571526368459}]}], "abstractContent": [{"text": "When only a small amount of manually annotated data is available, application of a boot-strapping method is often considered to compensate for the lack of sufcient training material fora machine-learning method.", "labels": [], "entities": []}, {"text": "The paper reports a series of experimental results of bootstrapping for protein name recognition.", "labels": [], "entities": [{"text": "protein name recognition", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.657327393690745}]}, {"text": "The results show that the performance changes signicantly according to the choice of text collection where the training samples to bootstrap, and that an improvement can be obtained only with a well chosen text collection .", "labels": [], "entities": []}], "introductionContent": [{"text": "While machine learning-based approaches are becoming more and more popular for the development of natural language processing (NLP) systems, corpora with annotation are regarded as a critical resource for the training process.", "labels": [], "entities": []}, {"text": "Nonetheless, the creation of corpus annotation is an expensive and timeconsuming work), and it is often the case that lack of sufcient annotation hinders the development of NLP systems.", "labels": [], "entities": []}, {"text": "Bootstrapping method (; Vlachos and) can be considered as away to automatically inate the amount of corpus annotation to complement the lack of sufcient annotation.", "labels": [], "entities": []}, {"text": "In this study, we report the experimental results on the effect of bootstrapping for the training of protein name recognizers, particularly in the situation when we have only a small amount of corpus annotations.", "labels": [], "entities": [{"text": "protein name recognizers", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.6450522541999817}]}, {"text": "In summary, we begin with a small corpus with manual annotation for protein names.", "labels": [], "entities": []}, {"text": "A named entity tagger trained on the small corpus is applied to a big collection of text, to obtain more annotation.", "labels": [], "entities": []}, {"text": "We hope the newly created annotation to be precise enough so that the training of a protein tagger can benet from the increased training material.", "labels": [], "entities": []}, {"text": "We assume that the accuracy of a bootstrapping method) depends on two factors: the accuracy of the bootstrap tagger itself and the similarity of the text to the original corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9984930753707886}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.999201238155365}]}, {"text": "While accuracy of the bootstrap tagger maybe maximized by nding the optimal parameters of the applied machine learning method, the choice of text where the original annotations will bootstrap may also be a critical factor for the success of the bootstrapping method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9992591738700867}]}, {"text": "Experimental results presented in this paper conrm that we can get a improvement by using a bootstrapping method with a well chosen collection of texts.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the two datasets used in this paper.", "labels": [], "entities": []}, {"text": "Following that, in Section 3, we briey introduce the experiments performed in our research.", "labels": [], "entities": []}, {"text": "The experimental results are demonstrated in Section 4.", "labels": [], "entities": []}, {"text": "The research is concluded in Section 5 and in the meanwhile, future work is discussed.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following experiments, the NERSuite ! , a named entity tagger based on Conditional Random Fields (CRFs) ( As mentioned in Section 2.1, the three subsets of Kazusa data are used for training, tuning and testing purposes, in turn.", "labels": [], "entities": [{"text": "NERSuite", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.7620015144348145}]}, {"text": "We experimented with all the six combinations.", "labels": [], "entities": []}, {"text": "Experiments were performed to compare three different strategies.", "labels": [], "entities": []}, {"text": "First, with the baseline strategy, the protein tagger is trained only on the Kazusa training set.", "labels": [], "entities": [{"text": "protein tagger", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.6989330500364304}, {"text": "Kazusa training set", "start_pos": 77, "end_pos": 96, "type": "DATASET", "confidence": 0.9436800877253214}]}, {"text": "The sigma value is optimized on the tuning set, and the performance is evaluated on the test set.", "labels": [], "entities": []}, {"text": "It is the most typical strategy particularly when it is believed there is a sufcient training material.", "labels": [], "entities": []}, {"text": "Second, with the bootstrapping strategy, the Kazusa training set is used as the seed data.", "labels": [], "entities": [{"text": "Kazusa training set", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.8831781148910522}]}, {"text": "A tagger for bootstrapping (bootstrap tagger, hereafter) is trained on the seed data, and applied to the BC2 data to bootstrap the training examples.", "labels": [], "entities": [{"text": "BC2 data", "start_pos": 105, "end_pos": 113, "type": "DATASET", "confidence": 0.926311731338501}]}, {"text": "Another protein tagger (application tagger) is then trained on the bootstrapped BC2 data together with the seed data.", "labels": [], "entities": [{"text": "BC2 data", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.8306859731674194}]}, {"text": "The Kazusa tuning set is used to optimize the two sigma values for the two protein taggers, and the performance is evaluated on the test set.", "labels": [], "entities": [{"text": "Kazusa tuning set", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.7019701997439066}]}, {"text": "With this strategy, we wish the bootstrapped examples complement the lack of sufcient training examples.", "labels": [], "entities": []}, {"text": "Third, the bootstrapping with sentence selection strategy is almost the same with the bootstrapping strategy, except that the second tagger is trained after the non-relevant sentences are ltered out from the BC2 data.", "labels": [], "entities": [{"text": "bootstrapping with sentence selection", "start_pos": 11, "end_pos": 48, "type": "TASK", "confidence": 0.6537591889500618}, {"text": "BC2 data", "start_pos": 208, "end_pos": 216, "type": "DATASET", "confidence": 0.8804321885108948}]}, {"text": "Here, non-relevant sentences mean those that are not tagged by the the bootstrap tagger.", "labels": [], "entities": []}, {"text": "With this strategy, we wish an improvement with the bootstrapping by removing noisy data.", "labels": [], "entities": []}, {"text": "annotation bootstrap only on a small portion of the BC2 data set, e.g., 1,103 vs. 15,000 sentences in the case of E1 (less than 10%), suggesting that a large portion of the data set maybe irrelevant to the original data set.", "labels": [], "entities": [{"text": "BC2 data set", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.926452100276947}]}, {"text": "The experimental results of all the six combinations are shown in.", "labels": [], "entities": []}, {"text": "The use of the three subsets, denoted by A, B, C, of the Kazusa data set for training, tuning and testing in each experiment is specied in \u0093training\u0094, \u0093tuning\u0094 and \u0093testing\u0094 columns.", "labels": [], "entities": [{"text": "Kazusa data set", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.946591854095459}]}, {"text": "The results of the baseline strategy that uses only the Kazusa data are shown in the \u0093baseline\u0094 column, whereas the results with the bootstrapping methods with and without sentence selection are shown in the last two columns.", "labels": [], "entities": [{"text": "Kazusa data", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.8801676034927368}]}, {"text": "As explained in Section 3, the sigma values are optimized using the tuning set for each experiment.", "labels": [], "entities": []}, {"text": "Note that for bootstrapping, we need two sigma values for the bootstrapping tagger and the application tagger.", "labels": [], "entities": []}, {"text": "The performance of named entity recognition is measured in terms of precision, recall and F-score.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.6278023223082224}, {"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9997181296348572}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9997401833534241}, {"text": "F-score", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9983738660812378}]}, {"text": "For matching criterion, in order to avoid underestimation, instead of the exact matching, system performance is evaluated under a soft matching, the overlapping matching criterion.", "labels": [], "entities": [{"text": "matching", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.9534900188446045}]}, {"text": "That is, if any part of the annotated protein/gene names is recognized by the NER tagger, we will regard that as a correct answer.", "labels": [], "entities": [{"text": "NER tagger", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.6178175210952759}]}], "tableCaptions": [{"text": " Table 1: The number of positive examples used in each  experiment. The \u0093BT\u0094 column shows the number of posi- tive examples obtained by the bootstrapping in the 15,000  BC2 sentences. In the last column, the gures in paren- theses are the number of the selected sentences.", "labels": [], "entities": [{"text": "\u0093", "start_pos": 72, "end_pos": 73, "type": "METRIC", "confidence": 0.903190553188324}, {"text": "BT", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.4526726305484772}]}, {"text": " Table 2: Experimental results of using the Kazusa and BC2 data (Precision/Recall/F-score). \u0093BT\u0094 and \u0093SS\u0094 represent  the bootstrapping and sentence selection strategies, respectively. The gures in square brackets are the sigma values  optimized in the experiments.", "labels": [], "entities": [{"text": "Kazusa and BC2 data", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.6159077435731888}, {"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.5038389563560486}, {"text": "\u0093", "start_pos": 92, "end_pos": 93, "type": "METRIC", "confidence": 0.9864704012870789}, {"text": "BT", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.5453923344612122}, {"text": "sentence selection", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.7110861092805862}]}]}