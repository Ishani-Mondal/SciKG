{"title": [{"text": "Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing", "labels": [], "entities": [{"text": "Constructing Parallel Corpora for Six Indian Languages", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.8095676898956299}]}], "abstractContent": [{"text": "Recent work has established the efficacy of Amazon's Mechanical Turk for constructing parallel corpora for machine translation research.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.9097826778888702}, {"text": "machine translation research", "start_pos": 107, "end_pos": 135, "type": "TASK", "confidence": 0.8709198633829752}]}, {"text": "We apply this to building a collection of parallel corpora between English and six languages from the Indian subcontinent: Bengali, Hindi, Malayalam, Tamil, Telugu, and Urdu.", "labels": [], "entities": []}, {"text": "These languages are low-resource, under-studied, and exhibit linguistic phenomena that are difficult for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7494474947452545}]}, {"text": "We conduct a variety of baseline experiments and analysis, and release the data to the community .", "labels": [], "entities": []}], "introductionContent": [{"text": "The quality of statistical machine translation (MT) systems is strongly related to the amount of parallel text available for the language pairs.", "labels": [], "entities": [{"text": "statistical machine translation (MT)", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.7879397968451182}]}, {"text": "However, most language pairs have little or no readily available bilingual training data available.", "labels": [], "entities": []}, {"text": "As a result, most contemporary MT research tends to opportunistically focus on language pairs with large amounts of parallel data.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9931938052177429}]}, {"text": "A consequence of this bias is that language exhibiting certain linguistic phenomena are underrepresented, including languages with complex morphology and languages with divergent word orderings.", "labels": [], "entities": []}, {"text": "In this paper, we describe our work gathering and refining document-level parallel corpora between English and each of six verb-final languages spoken on the Indian subcontinent: Bengali, Hindi, Malayalam, Tamil, Telugu, and Urdu.", "labels": [], "entities": []}, {"text": "This paper's contributions are as follows: \u2022 We apply an established protocol for using Amazon's Mechanical Turk (MTurk) to collect parallel data to train and evaluate translation systems for six Indian languages.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk (MTurk)", "start_pos": 88, "end_pos": 120, "type": "DATASET", "confidence": 0.8317895446504865}]}, {"text": "\u2022 We investigate the relative performance of syntactic translation models over hierarchical ones, showing that syntax results in higher BLEU scores inmost cases.", "labels": [], "entities": [{"text": "syntactic translation", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.6742238104343414}, {"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9993066787719727}]}, {"text": "\u2022 We explore the impact of training data quality on the quality of the resulting model.", "labels": [], "entities": []}, {"text": "\u2022 We release the corpora to the research community under the Creative Commons AttributionSharealike 3.0 Unported License (CC BY-SA 3.0).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present experiments on the collected data sets in order to quantify their performance.", "labels": [], "entities": []}, {"text": "The experiments aim to address the following questions: 1.", "labels": [], "entities": []}, {"text": "How well can we translate the test sets?", "labels": [], "entities": [{"text": "translate", "start_pos": 16, "end_pos": 25, "type": "TASK", "confidence": 0.9616233706474304}]}, {"text": "2. Do linguistically motivated translation models improve translation results?", "labels": [], "entities": []}, {"text": "3. What is the effect of data quality on model quality?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The 100 most viewed Hindi Wikipedia articles (titles translated to English using inter-language links and  Google translate and manually categorized). Entries in bold were present in the top 100 lists of at least four of the  Indian top 100 lists. Earth, India, World War II, and Wikipedia were in the top 100 lists of all six languages.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 290, "end_pos": 299, "type": "DATASET", "confidence": 0.9479069709777832}]}, {"text": " Table 3: Dictionary statistics. Entries is the number of  source-language types, while translations lists the num- ber of words or phrases they translated to (i.e., the num- ber of pairs in the dictionary). Controls for Hindi were  obtained using Google translate, the only one of these lan- guages that were available at the outset of this project.", "labels": [], "entities": []}, {"text": " Table 4: Data set sizes for each language pair: words in  the first row, parallel sentences in the second. (The dictio- naries contains short phrases in addition to words, which  accounts for the difference in dictionary word and line  counts.)", "labels": [], "entities": []}, {"text": " Table 5: BLEU scores translating into English (four ref- erences). BLEU scores are the mean of three MERT runs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987223744392395}, {"text": "ref- erences", "start_pos": 53, "end_pos": 65, "type": "METRIC", "confidence": 0.7433453599611918}, {"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9992039799690247}, {"text": "MERT", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9921906590461731}]}, {"text": " Table 7: BLEU scores translating into English on a quar- ter of the training data (plus dictionary), selected in two  ways: best (result of vote), and random. There is little  difference, suggesting quality control may not be terribly  important. We did not collect votes for Malayalam.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9978378415107727}, {"text": "Malayalam", "start_pos": 277, "end_pos": 286, "type": "DATASET", "confidence": 0.8955368995666504}]}]}