{"title": [{"text": "Direct Error Rate Minimization for Statistical Machine Translation", "labels": [], "entities": [{"text": "Direct Error Rate Minimization", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6337348222732544}, {"text": "Statistical Machine Translation", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.8576158086458842}]}], "abstractContent": [{"text": "Minimum error rate training is often the preferred method for optimizing parameters of statistical machine translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.6205128133296967}]}, {"text": "MERT minimizes error rate by using a surrogate representation of the search space, such as N-best lists or hypergraphs, which only offer an incomplete view of the search space.", "labels": [], "entities": [{"text": "error rate", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9394855499267578}]}, {"text": "In our work, we instead minimize error rate directly by integrating the decoder into the min-imizer.", "labels": [], "entities": [{"text": "error rate", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9577148258686066}]}, {"text": "This approach yields two benefits.", "labels": [], "entities": []}, {"text": "First, the function being optimized is the true error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.932088315486908}]}, {"text": "Second, it lets us optimize parameters of translations systems other than standard linear model features, such as distortion limit.", "labels": [], "entities": []}, {"text": "Since integrating the decoder into the minimizer is often too slow to be practical, we also exploit statistical significance tests to accelerate the search by quickly discarding un-promising models.", "labels": [], "entities": []}, {"text": "Experiments with a phrase-based system show that our approach is scal-able, and that optimizing the parameters that MERT cannot handle brings improvements to translation results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Minimum error rate training) is a common method for optimizing linear model parameters, which is an important part of building good machine translation systems.", "labels": [], "entities": [{"text": "Minimum error rate training", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.728530615568161}, {"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7380573153495789}]}, {"text": "MERT minimizes an arbitrary loss function, usually an evaluation metric such as BLEU () or TER () from a surrogate representation of the search space, such as the N -best candidate translations of a development set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.998777449131012}, {"text": "TER", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9833921194076538}]}, {"text": "Much of the recent work on minimum error rate training focused on improving the method by.", "labels": [], "entities": []}, {"text": "Recent efforts extended MERT to work on lattices () and hypergraphs (.", "labels": [], "entities": [{"text": "MERT", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.8798141479492188}]}, {"text": "Random restarts and random walks are commonly used to combat the fact the search space is highly non-convex, often with multiple minima.", "labels": [], "entities": []}, {"text": "Several problems still remain with MERT, three of which are addressed by this work.", "labels": [], "entities": [{"text": "MERT", "start_pos": 35, "end_pos": 39, "type": "TASK", "confidence": 0.48746711015701294}]}, {"text": "First, the Nbest error surface explored by MERT is generally not the same as the true error surface, which means that the error rate at an optimum 1 of the N -best error surface is not guaranteed to be any close to an optimum of the true error surface.", "labels": [], "entities": []}, {"text": "Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an error-prone decoder differs from the one of an exact decoder (.", "labels": [], "entities": [{"text": "SMT decoders", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9324976801872253}, {"text": "MERT", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.722777247428894}]}, {"text": "MERT calculates an envelope from candidate translations and assumes all translations on the envelope are reachable by the decoder, but these translations may become unreachable due to search errors.", "labels": [], "entities": []}, {"text": "Third, MERT is only used to tune linear model parameters, yet SMT systems have many free decoder parameters-such as distortion limit and beam size-that are not handled by MERT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9799527525901794}]}, {"text": "MERT does not provide a principled way to set these parameters.", "labels": [], "entities": [{"text": "MERT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5307224988937378}]}, {"text": "In order to overcome these issues, we explore the application of direct search methods to SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9941400289535522}]}, {"text": "To do this, we integrate the decoder and the evaluation metric inside the objective function, which takes source sentences and a set of weights as inputs, and outputs the evaluation score (e.g., BLEU score) computed on the decoded sentences.", "labels": [], "entities": [{"text": "BLEU score)", "start_pos": 195, "end_pos": 206, "type": "METRIC", "confidence": 0.9845891197522482}]}, {"text": "Since it is impractical to calculate derivatives of this function, we use derivative-free optimization methods such as the downhill simplex method) and Powell's method, which generally handle such difficult search conditions relatively well.", "labels": [], "entities": []}, {"text": "This approach confers several benefits over MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 44, "end_pos": 48, "type": "TASK", "confidence": 0.6854877471923828}]}, {"text": "First, the function being optimized is the true error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.932088315486908}]}, {"text": "Second, integrating the decoder inside the objective function forces the optimizer to account for possible search errors.", "labels": [], "entities": []}, {"text": "Third, contrary to MERT, our approach does not require input parameters to be those of a linear model, so our approach can tune a broader range of features, including non-linear and hidden-state parameters (e.g., distortion limit, beam size, and weight vector applied to future cost estimates).", "labels": [], "entities": [{"text": "MERT", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.4126335382461548}]}, {"text": "In this paper, we make direct search reasonably fast thanks to two speedup techniques.", "labels": [], "entities": []}, {"text": "First, we use a model selection acceleration technique called racing in conjunction with randomization tests () to avoid decoding the entire development set at each function evaluation.", "labels": [], "entities": [{"text": "model selection acceleration", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.7316769560178121}]}, {"text": "This approach discards the current model whenever performance on the translated subset of the development data is deemed significantly worse in comparison to the current best model.", "labels": [], "entities": []}, {"text": "Second, we store and re-use search graphs across function evaluations, which eliminates some of the redundancy of regenerating the same translations in different optimization steps.", "labels": [], "entities": []}, {"text": "Our experiments with a strong phrase-based translation system show that the direct search approach is an effective alternative to MERT.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6743535101413727}]}, {"text": "The speed of direct search is generally comparable to MERT, and translation accuracy is generally superior.", "labels": [], "entities": [{"text": "MERT", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.7302899956703186}, {"text": "translation", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9289184212684631}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9716222882270813}]}, {"text": "The non-linear and hidden-state features tuned in this work bring gains on three language pairs, with improvements ranging between 0.27 and 0.35 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9989684820175171}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Size of bitexts in number of sentence pairs.", "labels": [], "entities": [{"text": "Size of bitexts", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7869069774945577}]}]}