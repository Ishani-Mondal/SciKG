{"title": [{"text": "Towards Effective Use of Training Data in Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.8169011871019999}]}], "abstractContent": [{"text": "We report on findings of exploiting large data sets for translation modeling, language mod-eling and tuning for the development of competitive machine translation systems for eight language pairs.", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.9812363684177399}, {"text": "language mod-eling", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7163476198911667}, {"text": "machine translation", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.7581735849380493}]}], "introductionContent": [{"text": "We report on experiments carried out for the development of competitive systems on the datasets of the 2012 Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.6346509754657745}]}, {"text": "Our main focus was directed on the effective use of all the available training data during training of translation and language models and tuning.", "labels": [], "entities": [{"text": "translation and language models", "start_pos": 103, "end_pos": 134, "type": "TASK", "confidence": 0.8598811775445938}]}, {"text": "We use the open source machine translation system Moses ( and other standard open source tools, hence all our experiments are straightforwardly replicable . Compared to all single system submissions by participants of the workshop we achieved the best BLEU scores for four language pairs (es-en, en-es, cs-en, en-cs), the 2 nd best results for two language pairs (fr-en, de-en), as well as a 3 rd place (en-de) and a 5 th place (en-fr) for the remaining pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 252, "end_pos": 256, "type": "METRIC", "confidence": 0.9983839988708496}]}, {"text": "We improved upon this in the post-evaluation period for some of the language pairs by more systematically applying our methods.", "labels": [], "entities": []}, {"text": "During the development of our system, we saw most gains from using large corpora for translation model training, especially when using subsampling techniques for out-of-domain sets, using large corpora for language model training, and larger tuning sets.", "labels": [], "entities": [{"text": "translation model training", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.936642567316691}, {"text": "language model training", "start_pos": 206, "end_pos": 229, "type": "TASK", "confidence": 0.6919083197911581}]}, {"text": "We also observed mixed results with alternative tuning methods.", "labels": [], "entities": []}, {"text": "We also experimented with hierarchical models and semi-supervised training, but did not achieve any improvements.", "labels": [], "entities": []}, {"text": "We report all results in case-sensitive BLEU (mteval13a) on the newstest2011 test set).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9814574122428894}, {"text": "newstest2011 test set", "start_pos": 64, "end_pos": 85, "type": "DATASET", "confidence": 0.9787092606226603}]}, {"text": "Please also note that baseline scores vary throughout the paper, since different methods were investigated at different time points.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Gains from larger translation models: UN (about  300 million English words), GigaFrEn (about 550 million  English words).", "labels": [], "entities": [{"text": "UN", "start_pos": 48, "end_pos": 50, "type": "DATASET", "confidence": 0.7712105512619019}]}, {"text": " Table 2: Subsampling UN and GigaFrEn corpora using Model 1 and Moore-Lewis filtering, before and after word  alignment", "labels": [], "entities": [{"text": "word  alignment", "start_pos": 104, "end_pos": 119, "type": "TASK", "confidence": 0.6989750266075134}]}, {"text": " Table 3: Using the LDC Gigaword corpora to train larger  language models.", "labels": [], "entities": []}, {"text": " Table 4: Using a larger tuning set (7567 sentences) by  combining newstest 2008 to 2010.", "labels": [], "entities": [{"text": "newstest 2008", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.9610291719436646}]}, {"text": " Table 6: Hierarchical phrase models vs. baseline phrase- based models.", "labels": [], "entities": []}]}