{"title": [{"text": "Contingency and Comparison Relation Labeling and Structure Prediction in Chinese Sentences", "labels": [], "entities": [{"text": "Contingency and Comparison Relation Labeling", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6301110744476318}, {"text": "Structure Prediction", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.6978613883256912}]}], "abstractContent": [{"text": "Unlike in English, the sentence boundaries in Chinese are fuzzy and not well-defined.", "labels": [], "entities": []}, {"text": "As a result, Chinese sentences tend to belong and consist of complex discourse relations.", "labels": [], "entities": []}, {"text": "In this paper, we focus on two important relations, Contingency and Comparison, which occur often inside a sentence.", "labels": [], "entities": []}, {"text": "We construct a moderate-sized corpus for the investigation of intra-sentential relations and propose models to label the relation structure.", "labels": [], "entities": []}, {"text": "A learning based model is evaluated with various features.", "labels": [], "entities": []}, {"text": "Experimental results show our model achieves accuracies of 81.63% in the task of relation labeling and 74.8% in the task of relation structure prediction.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9936040043830872}, {"text": "relation labeling", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.8693320751190186}, {"text": "relation structure prediction", "start_pos": 124, "end_pos": 153, "type": "TASK", "confidence": 0.7597730954488119}]}], "introductionContent": [{"text": "Discourse relation labeling has attracted much attention in recent years due to its potential applications such as opinion mining, question answering, etc.", "labels": [], "entities": [{"text": "Discourse relation labeling", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8426832358042399}, {"text": "opinion mining", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.8640673458576202}, {"text": "question answering", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.9111735224723816}]}, {"text": "The release of the Penn Discourse Treebank ( has advanced the development of English discourse relation recognition (.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 19, "end_pos": 42, "type": "DATASET", "confidence": 0.9896870851516724}, {"text": "English discourse relation recognition", "start_pos": 77, "end_pos": 115, "type": "TASK", "confidence": 0.6527555659413338}]}, {"text": "For Chinese, a discourse corpus is not publicly available yet.", "labels": [], "entities": []}, {"text": "Thus, the research on Chinese discourse relation recognition is relatively rare.", "labels": [], "entities": [{"text": "Chinese discourse relation recognition", "start_pos": 22, "end_pos": 60, "type": "TASK", "confidence": 0.6985205784440041}]}, {"text": "Most notably, annotated discourse connectives in the Chinese Treebank.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.9573245048522949}]}, {"text": "Our previous work labeled four types of relations, including temporal, contingency, comparison and expansion, between two successive sentences, and reported an accuracy of 88.28% and an F-score of 62.88% ().", "labels": [], "entities": [{"text": "comparison and expansion", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.700961709022522}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.999619722366333}, {"text": "F-score", "start_pos": 186, "end_pos": 193, "type": "METRIC", "confidence": 0.9994718432426453}]}, {"text": "The major issue of our work is the determination of discourse boundaries.", "labels": [], "entities": [{"text": "determination of discourse boundaries", "start_pos": 35, "end_pos": 72, "type": "TASK", "confidence": 0.7661378383636475}]}, {"text": "Each Chinese sentence is always treated as one of the two arguments in their annotation and many instances of the Contingency and the Comparison remain uncaught.", "labels": [], "entities": []}, {"text": "As suggested by the Penn Discourse Treebank annotation guidelines, an argument is possibly some clauses in a sentence, a sentence, or several successive sentences.", "labels": [], "entities": [{"text": "Penn Discourse Treebank annotation", "start_pos": 20, "end_pos": 54, "type": "DATASET", "confidence": 0.9692394882440567}]}, {"text": "In Chinese, the Contingency and the Comparison relations are likely to occur within a sentence.", "labels": [], "entities": []}, {"text": "Thus, a lot of the Contingency relations and the Comparison relations are missing from annotation in the corpus used in our previous work, and the classification performance for these two relations, especially the Contingency relation, is especially poor (.", "labels": [], "entities": []}, {"text": "In contrast to Chinese inter-sentential discourse relation detection () and the study of English coherence evaluation (), this paper focuses on the Contingency relation and the Comparison relations that occur inside a sentence.", "labels": [], "entities": [{"text": "inter-sentential discourse relation detection", "start_pos": 23, "end_pos": 68, "type": "TASK", "confidence": 0.6339829489588737}]}, {"text": "In Chinese, the relations usually occur in the sentences which contain many clauses.", "labels": [], "entities": []}, {"text": "For example, two relations occur in sample (S1).", "labels": [], "entities": []}, {"text": "(S1) \u7ba1\u7406\u8655\u96d6\u5617\u8a66\u8981\u8b93\u9577\u671f\u4f86\u4f5c\u70ba\u5927\u53f0\u5317\u5f8c \u82b1 \u5712 \u7684 \u967d \u660e \u5c71 \u5340 \u66f4 \u56de \u6b78 \u81ea \u7136 (\"Although the management office tried to make the Yangmingshan area a more natural environment as the long-term garden of Taipei\")\uff0c\u4f46\u96a8\u8457\u9031\u4f11\u4e8c \u65e5 \u3001 \u7d93 \u6fdf \u74b0 \u5883 \u6539 \u5584 (\"But due to the two-day weekend and the improved economic conditions\")\uff0c \u904a\u5ba2\u5e36\u4f86\u505c\u8eca\u3001\u5783\u573e\u7b49\u9593\u63a5\u5f71\u97ff\u537b\u66f4\u70ba\u56b4\u91cd (\"The issues of tourists parking, garbage, and other indirect effects become more serious\")\u3002", "labels": [], "entities": []}, {"text": "In (S1), the long sentence consists of three clauses, and such a Chinese sentence is expressed as multiple short sentences in English.", "labels": [], "entities": []}, {"text": "shows that a Comparison relation occurs between the first clause and the last two clauses, and a Contingency relation occurs between the second clause and the third clause.", "labels": [], "entities": []}, {"text": "An explicit paired discourse marker \u96d6 (although) \u2026 \u4f46 (but) denotes a Comparison relation in (S1), where the first clause is the first argument of this relation, and the second and the third clauses are the second argument of this relation.", "labels": [], "entities": []}, {"text": "In addition, an implicit Contingency relation also occurs between the second and the third clauses.", "labels": [], "entities": []}, {"text": "The second clause is the cause argument of this Contingency relation, and the third clause is its effect.", "labels": [], "entities": []}, {"text": "It shows a nested relation, which makes relation labeling and relation structure determination challenging.", "labels": [], "entities": [{"text": "relation labeling", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7669597566127777}, {"text": "relation structure determination", "start_pos": 62, "end_pos": 94, "type": "TASK", "confidence": 0.7716607054074606}]}, {"text": "In Chinese, an explicit discourse marker does not always uniquely identify the existence of a particular discourse relation.", "labels": [], "entities": []}, {"text": "In sample (S2), a discourse marker \u800c \"moreover\" appears, but neither Contingency nor Comparison relation exists between the two clauses.", "labels": [], "entities": []}, {"text": "The discourse marker \u800c has many meanings.", "labels": [], "entities": []}, {"text": "Here, It has the meaning of \"and\" or \"moreover\", which indicates an Expansion relation.", "labels": [], "entities": [{"text": "Expansion", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.948469877243042}]}, {"text": "In other usages, it may have the meaning of \"but\" or \"however\", which indicates a Comparison relation.", "labels": [], "entities": []}, {"text": "(S2) \u800c\u5927\u9678\u7d93\u6fdf\u958b\u653e\uff11\uff10\u5e74\u4ee5\u4f86\uff0c\u5176\u9032\u6b65\u66f4 \u4ee4 \u4eba \u522e \u76ee \u76f8 \u770b \u3002 (\"Moreover, the progress of mainland is more impressive due to its economic openness for the last 10 years.\")", "labels": [], "entities": []}, {"text": "Note that the relation structure of a sentence cannot be exactly derived from the parse tree of the sentence.", "labels": [], "entities": []}, {"text": "Shown in is the structure of sample (S3) based on the syntactic tree generated by the Stanford parser.", "labels": [], "entities": []}, {"text": "However, it is clear that the correct structure of (S3) is the one shown in.", "labels": [], "entities": []}, {"text": "(S3) \u76ee\u524d\u96d6\u7136\u9084\u53ea\u80fd\u5728\u5716\u7247\u4e0a\u8b93\u5973\u6027\u9732\u9732\u81c9 (\"Although women only appear in the pictures\")\uff0c \u4f46 \u672a \u4f86 \u5973 \u6027 \u7684 \u8ca2 \u737b (\"The contribution of women\")\uff0c\u5c07\u662f\u6559\u79d1\u66f8\u53e6\u4e00\u500b\u8457\u58a8\u7684\u91cd\u9ede (\"Will be another major focus in textbooks in the future\")\u3002", "labels": [], "entities": []}, {"text": "This shows that the Stanford parser does not capture the information that the last two clauses form a unit, which in turn is one of the two arguments of a Comparison relation.", "labels": [], "entities": []}, {"text": "In this work, we investigate intra-sentential relation detection in Chinese.", "labels": [], "entities": [{"text": "intra-sentential relation detection", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.7060095071792603}]}, {"text": "Given a Chinese sentence, our model will predict if Contingency or Comparison relations exist, and determine their relation structure.", "labels": [], "entities": []}, {"text": "In Section 2, the development of a corpus annotated with Contingency and Comparison relations is presented.", "labels": [], "entities": []}, {"text": "The methods and the features are proposed in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, the experimental results are shown and discussed.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The corpus is based on the Sinica Treebank (.", "labels": [], "entities": [{"text": "Sinica Treebank", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.8773239552974701}]}, {"text": "A Total of 81 articles are randomly selected from the Sino and Travel sets.", "labels": [], "entities": [{"text": "Sino and Travel sets", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.8655775934457779}]}, {"text": "All the sentences that consist of two, three, and four clauses are extracted for relation and structure labeling by native Chinese speakers.", "labels": [], "entities": [{"text": "relation and structure labeling", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.8099734932184219}]}, {"text": "A web-based system is developed for annotation.", "labels": [], "entities": []}, {"text": "The annotation scheme is designed as follows.", "labels": [], "entities": []}, {"text": "An annotator first signs in to the annotation system, and a list of sentences that are assigned to the annotator are given.", "labels": [], "entities": []}, {"text": "The annotator labels the sentences one by one in the system.", "labels": [], "entities": []}, {"text": "A sentence is split into clauses along commas, and all of its feasible binary tree structures are shown in the interface.", "labels": [], "entities": []}, {"text": "The annotator decides if a Contingency/Comparison relation occurs in this sentence.", "labels": [], "entities": []}, {"text": "The sentence will be marked as \"Nil\" if no relation is found.", "labels": [], "entities": []}, {"text": "If there is at least one relation in this sentence, the annotator then chooses the best tree structure of the relations, and the second page is shown.", "labels": [], "entities": []}, {"text": "The previously chosen tree structure is presented again, and at this time the annotator has to assign a suitable relation type to each internal node of the tree structure.", "labels": [], "entities": []}, {"text": "The relation type includes Contingency \"\u56e0\u679c\", Comparison \"\u8f49\u6298\", and Nil.", "labels": [], "entities": []}, {"text": "For example, in sample (S4), its three internal nodes are annotated with three relation types as shown in.", "labels": [], "entities": []}, {"text": "(S4) \u5373\u4f7f\u6c92\u6709\u50b3\u627f\u7684\u4f7f\u547d\u611f (\"Even without the sense of mission of the heritage\")\uff0c\u70ba\u4e86\u5c0b\u6c42 \u66f4 \u597d \u7684 \u6cbb \u7642 \u65b9 \u5f0f (\"In order to seek better treatments\")\uff0c\u4e5f\u6703\u9a45\u4f7f\u9019\u4e9b\u91ab\u5b78\u5de5\u4f5c\u8005\u8de8\u8d8a\u9818 \u57df\u5340\u9694 (\"These medical workers will be driven crossing domain areas\")\uff0c\u53bb\u5c0b\u627e\u8cc7\u6e90 (\"To find resources\")\u3002", "labels": [], "entities": []}, {"text": "The number of feasible relation structures of a sentence maybe very large depending on the number of clauses.", "labels": [], "entities": []}, {"text": "For a sentence with n clauses, the number of its feasible structures is given as the recursive function f(n) as follows, and the number of its feasible relation structures is 3 !!!", "labels": [], "entities": []}, {"text": "\u00ed \u00b5\u00ed\u00b1\u0093 \u00ed \u00b5\u00ed\u00b1\u009b .  For a two-clause sentence, there are only one tree structure and three possible relation tags (Contingency, Comparison, and Nil) for the only one internal node, the root.", "labels": [], "entities": [{"text": "Nil", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.9586223363876343}]}, {"text": "For a three-clause sentence, there are two candidate tree structures and nine combinations of the relation tags.", "labels": [], "entities": []}, {"text": "For a four-clause sentence, there are five candidate tree structures and 27 combinations of the relation tags.", "labels": [], "entities": []}, {"text": "There are theoretically 3, 18, and 135 feasible relation structures for the two-, three-, and fourclause sentences, respectively, though only 49 types of relations structures are observed in the dataset.", "labels": [], "entities": []}, {"text": "Each sentence is shown to three annotators, and the majority is taken as the ground-truth.", "labels": [], "entities": []}, {"text": "The Fleiss-Kappa of the inter-annotator agreement is 0.44 (moderate agreement).", "labels": [], "entities": [{"text": "Fleiss-Kappa", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9924046397209167}]}, {"text": "A final decider is involved to break ties.", "labels": [], "entities": [{"text": "decider", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.8671433925628662}]}, {"text": "The statistics of our corpus are shown in.", "labels": [], "entities": []}, {"text": "The explicit data are those sentences which have at least one discourse marker.", "labels": [], "entities": []}, {"text": "The rest of the data are implicit.", "labels": [], "entities": []}, {"text": "A total of 11 explicit sentences which contain both Contingency and Comparison relations form complex sentence compositions.", "labels": [], "entities": []}, {"text": "The implicit samples are relatively rare.", "labels": [], "entities": []}, {"text": "C5.0 1 , and the support vector machine, SVMlight 2 , are applied.", "labels": [], "entities": [{"text": "C5.0", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9013658165931702}]}, {"text": "The linguistic features are the crucial part in the learning-based approaches.", "labels": [], "entities": []}, {"text": "Various features from different linguistic levels are evaluated in the experiments as shown below.", "labels": [], "entities": []}, {"text": "Word: The bags of words in each clause.", "labels": [], "entities": [{"text": "Word", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.709768533706665}]}, {"text": "The Stanford Chinese word segmenter 3 is applied to all the sentences to tokenize the Chinese words.", "labels": [], "entities": [{"text": "Stanford Chinese word segmenter", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.7846624702215195}]}, {"text": "In addition, the first word and the last word in each clause are extracted as distinguished features.", "labels": [], "entities": []}, {"text": "POS: The bags of parts of speech (POS) of the words in each clause are also taken as features.", "labels": [], "entities": []}, {"text": "All the sentences in the dataset are sent to the Stanford parser 4 that parses a sentence from a surface form into a syntactic tree, labels POS for each word, and generates all the dependencies among the words.", "labels": [], "entities": [{"text": "POS", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9194717407226562}]}, {"text": "In addition, the POS tags of the first word and the last word in each clause are extracted as distinguished features.", "labels": [], "entities": []}, {"text": "Length: Several length features are considered, including the number of clauses in the sentence and the number of words for each clause in the sentence.", "labels": [], "entities": [{"text": "Length", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9217585921287537}]}, {"text": "Connective: In English, some words/phrases called connectives are used as discourse markers.", "labels": [], "entities": []}, {"text": "For example, the phrase \"due to\" is atypical connective that indicates a Contingency relation, and the word \"however\" is a connective that indicates a Comparison relation.", "labels": [], "entities": []}, {"text": "Similar to the connectives in English, various words and word pair patterns are usually used as discourse markers in Chinese.", "labels": [], "entities": []}, {"text": "A dictionary that contains several types of discourse markers is used.", "labels": [], "entities": []}, {"text": "The statistics of the connective dictionary and samples are listed in.", "labels": [], "entities": []}, {"text": "An intra-sentential phrase pair indicates a relation which occurs only inside a sentence.", "labels": [], "entities": []}, {"text": "In other words, a relation occurs when the two phrases of an intra-sentential pair exist in the same sentence no matter whether they are in the same clause or not.", "labels": [], "entities": []}, {"text": "In contrast, an intersentential connective indicates a relation that can occur across neighboring sentences.", "labels": [], "entities": []}, {"text": "Some connectives belong to both intra-sentential and inter-sentential types.", "labels": [], "entities": []}, {"text": "Each connective in each clause is detected and marked with its corresponding type.", "labels": [], "entities": []}, {"text": "For example, the phrase \u76f8\u5c0d Dependency: The dependencies among all words in a sentence are used as features.", "labels": [], "entities": []}, {"text": "The Stanford parser generates dependency pairs from the sentence.", "labels": [], "entities": []}, {"text": "A dependency pair consists of two arguments, i.e., the governor and the dependent, and their types.", "labels": [], "entities": []}, {"text": "We are interested in those dependency pairs that are across two clauses.", "labels": [], "entities": []}, {"text": "That is, the two arguments of a pair are from different clauses.", "labels": [], "entities": []}, {"text": "In our assumption, the clauses have a closer connection if some dependencies occur between them.", "labels": [], "entities": []}, {"text": "All such dependency pairs and their types are extracted and counted.", "labels": [], "entities": []}, {"text": "Structure: Recent research work reported improved performance using syntactic information for English discourse relation detection.", "labels": [], "entities": [{"text": "English discourse relation detection", "start_pos": 94, "end_pos": 130, "type": "TASK", "confidence": 0.7281053811311722}]}, {"text": "In the work of, the categories of a tree node, its parent, its left sibling, and its right sibling are taken as features.", "labels": [], "entities": []}, {"text": "In the work of, the entire paragraph is parsed   as a syntactic tree, and three levels of tree expansions are extracted as structured syntactic features.", "labels": [], "entities": []}, {"text": "To capture syntactic structure, we get the syntactic tree for each sentence using the Stanford parser, and extract the sub-tree of the upper three levels, which represents the fundamental composition of this sentence.", "labels": [], "entities": []}, {"text": "In addition, all the paths from the root to each punctuation node in a sentence are extracted.", "labels": [], "entities": []}, {"text": "From the paths, the depth of each comma node is counted, and the common parent node of every adjacent clause is also extracted.", "labels": [], "entities": []}, {"text": "For example, the upper three level subtree of the syntactic tree of (S1) is shown in.", "labels": [], "entities": []}, {"text": "In addition, the sub-tree in the dotted line forms the structure of the punctuations in the (S1).", "labels": [], "entities": []}, {"text": "Polarity: A Comparison relation implies its two arguments are contrasting, and some contrasts are presented with different polarities in the two arguments.", "labels": [], "entities": []}, {"text": "For example, sample (S5) is a case of Comparison.", "labels": [], "entities": []}, {"text": "(S5) \u5118\u7ba1\u5929\u7136\u74b0\u5883\u5982\u6b64\u512a\u8d8a\uff0c\u4eba\u70ba\u7684\u4e0d\u5e78\u9084 \u662f\u53eb\u9ad8\u68c9\u5b50\u6c11\u4e0d\u5f97\u597d\u904e\uff0c\u904d\u5690\u6230\u4e82\u7684\u75db\u695a\u3002", "labels": [], "entities": []}, {"text": "(\"Despite such favorable natural environment, man-made disasters still make the Khmer people unfortunate to suffer from the pain of war.\")", "labels": [], "entities": []}, {"text": "The first clause in (S5) is positive (\"favorable natural environment\"), while the last two clauses are negative (\"unfortunate to suffer from the pain of war\").", "labels": [], "entities": []}, {"text": "Besides the connectives \u5118\u7ba1 \"despite\" and \u9084 \u662f \"still\", the opposing polarity values between the first and the last two clauses is also a strong clue to the existence of a Comparison relation.", "labels": [], "entities": []}, {"text": "In addition, the same polarity of the last two clauses is also a hint that no Comparison relation occurs between them.", "labels": [], "entities": []}, {"text": "To capture polarity information, we estimate the polarity of each clause and detect the negations from the clause.", "labels": [], "entities": []}, {"text": "The polarity score is areal number estimated by a sentiment dictionary-based algorithm.", "labels": [], "entities": []}, {"text": "For each clause, the polarity score, and the existence of negation are taken as features.", "labels": [], "entities": []}, {"text": "All the models in the experiments are evaluated by 5-fold cross-validation.", "labels": [], "entities": []}, {"text": "The metrics are accuracies and macro-averaged F-scores.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9593889117240906}, {"text": "F-scores", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.92463219165802}]}, {"text": "The t-test is used for significance testing.", "labels": [], "entities": [{"text": "significance testing", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.9328796863555908}]}, {"text": "We firstly examine our model for the task of two-way classification.", "labels": [], "entities": [{"text": "two-way classification", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.6411107033491135}]}, {"text": "In this task, binary classifiers are trained to predict the existence of Contingency and Comparison relations in a given sentence.", "labels": [], "entities": []}, {"text": "For meaningful comparison, a majority classifier is used as a baseline model, which always predicts the majority class.", "labels": [], "entities": []}, {"text": "In the dataset, 72.6% of the sentences involve neither Contingency nor Comparison.", "labels": [], "entities": []}, {"text": "Thus, the major class is \"Nil\", and the accuracy and the F-score of the baseline model is 72.6% and 42.06%, respectively.", "labels": [], "entities": [{"text": "Nil", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9855524897575378}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9998979568481445}, {"text": "F-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9995298385620117}]}, {"text": "The experimental results for the two-way classification task are shown in.", "labels": [], "entities": [{"text": "two-way classification task", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.678963859875997}]}, {"text": "In the table, the symbol \u2020 denotes the lowest accuracy which has a significant improvement over the baseline at p=0.05 for the two models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9996373653411865}]}, {"text": "The symbol \u2021 denotes the adding of a single feature yields a significant improvement for the model at p=0.005.", "labels": [], "entities": []}, {"text": "The performance of the decision tree and the SVM are similar in terms of accuracy and F-score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9996833801269531}, {"text": "F-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.997503936290741}]}, {"text": "Overall, the decision tree model achieves better accuracies.", "labels": [], "entities": []}, {"text": "In the two-way classification task, the decision tree model with only the Word feature achieves an accuracy of 76.75%, which is significantly better than the baseline at p=0.05.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9996792078018188}]}, {"text": "For both the decision tree and the SVM, Connective is the most useful feature: performance is significantly improved with the addition of Connective.", "labels": [], "entities": []}, {"text": "Besides the binary classification task, we extend our model to tackle the task of finer classification.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7012432366609573}]}, {"text": "In the second task, four-way classifiers are trained    to predict a given sentence with four classes: existence of Contingency relations only, existence of Comparison relations only, existence of Both relations, and Nil.", "labels": [], "entities": []}, {"text": "The experimental results of the four-way classification task are shown in.", "labels": [], "entities": [{"text": "four-way classification task", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.6716012954711914}]}, {"text": "Consistent with the results of the two-way classification task, the addition of Connective to the SVM yields a significant improvement at p=0.005.", "labels": [], "entities": [{"text": "classification task", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8155942261219025}]}, {"text": "The performance between the decision tree and the SVM is still similar, but the SVM achieves a slightly better F-score of 45.26% in comparison with the best F-score of 44.47% achieved by the decision tree.", "labels": [], "entities": [{"text": "SVM", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.7991516590118408}, {"text": "SVM", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.7914664149284363}, {"text": "F-score", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9993442893028259}, {"text": "F-score", "start_pos": 157, "end_pos": 164, "type": "METRIC", "confidence": 0.9975652694702148}]}, {"text": "We further extend our model to predict the full relation structure of a given sentence as shown in and.", "labels": [], "entities": []}, {"text": "This is a 49-way classification task because there are 49 types of the full relation structures in the dataset.", "labels": [], "entities": []}, {"text": "Not only as many as 49-ways, 72.6% of instances belong to the Nil relation, which yields an unbalanced classification problem.", "labels": [], "entities": []}, {"text": "The experimental results are shown in.", "labels": [], "entities": []}, {"text": "In the most challenging case, the SVM achieves a better F-score of 7.66% in comparison with the F-score of 4.90% achieved by the decision tree.", "labels": [], "entities": [{"text": "SVM", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.7040407061576843}, {"text": "F-score", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9996387958526611}, {"text": "F-score", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.9993376135826111}]}, {"text": "Connective is still the most helpful feature.", "labels": [], "entities": []}, {"text": "Comparing the F-scores of the SVM in the three tasks with the F-scores of the decision tree, it shows that the SVM performs better for predicting finer classes.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9850442409515381}, {"text": "F-scores", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9667885899543762}, {"text": "predicting finer classes", "start_pos": 135, "end_pos": 159, "type": "TASK", "confidence": 0.8716501593589783}]}], "tableCaptions": [{"text": " Table 1: Statistics of the dataset.", "labels": [], "entities": []}, {"text": " Table 3: Performance of the two-way classification.", "labels": [], "entities": []}, {"text": " Table 4: Performance of the four-way classification.", "labels": [], "entities": []}, {"text": " Table 5: Performance of the 49-way classification.", "labels": [], "entities": []}, {"text": " Table 6: Performances for explicit cases and implicit  cases.", "labels": [], "entities": []}, {"text": " Table 8. Each row  represents the samples in an actual class, while  each column of the matrix represents the samples  in a predicted class. The precision (P), recall (R),", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 146, "end_pos": 159, "type": "METRIC", "confidence": 0.9370355010032654}, {"text": "recall (R)", "start_pos": 161, "end_pos": 171, "type": "METRIC", "confidence": 0.9462352395057678}]}, {"text": " Table 7: Performances of clauses of different lengths.", "labels": [], "entities": []}, {"text": " Table 8: Confusion matrix of the best model in the 4- way classification.", "labels": [], "entities": [{"text": "Confusion", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9572516679763794}]}, {"text": " Table 9: Instances of the top ten useful features for the  decision tree model", "labels": [], "entities": []}, {"text": " Table 9. Word and Connective provide  useful information for the classification. Moreover,", "labels": [], "entities": [{"text": "classification", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.9606599807739258}]}]}