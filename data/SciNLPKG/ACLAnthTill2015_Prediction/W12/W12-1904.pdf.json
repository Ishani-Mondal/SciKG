{"title": [{"text": "Toward Tree Substitution Grammars with Latent Annotations", "labels": [], "entities": []}], "abstractContent": [{"text": "We provide a model that extends the split-merge framework of Petrov et al.", "labels": [], "entities": []}, {"text": "(2006) to jointly learn latent annotations and Tree Substitution Grammars (TSGs).", "labels": [], "entities": []}, {"text": "We then conduct a variety of experiments with this model, first inducing grammars on a portion of the Penn Treebank and the Korean Treebank 2.0, and next experimenting with grammar refinement from a single nonterminal and from the Universal Part of Speech tagset.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9960435330867767}, {"text": "Korean Treebank 2.0", "start_pos": 124, "end_pos": 143, "type": "DATASET", "confidence": 0.9484477440516154}]}, {"text": "We present qualitative analysis showing promising signs across all experiments that our combined approach successfully provides for greater flexibility in grammar induction within the structured guidance provided by the treebank, leveraging the complementary natures of these two approaches .", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 155, "end_pos": 172, "type": "TASK", "confidence": 0.8112466931343079}]}], "introductionContent": [{"text": "Context-free grammars (CFGs) area useful tool for describing the structure of language, modeling a variety of linguistic phenomena while still permitting efficient inference.", "labels": [], "entities": [{"text": "Context-free grammars (CFGs", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7098372206091881}]}, {"text": "However, it is widely acknowledged that CFGs employed in practice make unrealistic independence and structural assumptions, resulting in grammars that are overly permissive.", "labels": [], "entities": []}, {"text": "One successful approach has been to refine the nonterminals of grammars, first manually and later automatically ().", "labels": [], "entities": []}, {"text": "In addition to improving parsing accuracy, the automatically learned latent annotations of these latter approaches yield results that accord well with human intuitions, especially at the lexical or preterminal level (for example, separating demonstrative adjectives from definite articles under the DT tag).", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9754770398139954}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9181864857673645}, {"text": "separating demonstrative adjectives from definite articles under the DT tag", "start_pos": 230, "end_pos": 305, "type": "TASK", "confidence": 0.681979775428772}]}, {"text": "It is more difficult, though, to extend this analysis to higher-level nonterminals, where the long-distance interactions among latent annotations of internal nodes are subtle and difficult to trace.", "labels": [], "entities": []}, {"text": "In another line of work, many researchers have examined the use of formalisms with an extended domain of locality, where the basic grammatical units are arbitrary tree fragments instead of traditional depth-one context-free grammar productions.", "labels": [], "entities": []}, {"text": "In particular, Tree Substitution Grammars (TSGs) retain the context-free properties of CFGs (and thus the cubic-time inference) while at the same time allowing for the modeling of long distance dependencies.", "labels": [], "entities": [{"text": "Tree Substitution Grammars (TSGs", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.7741632461547852}]}, {"text": "Fragments from such grammars are intuitive, capturing exactly the sorts of phrasal-level properties (such as predicate-argument structure) that are not present in Treebank CFGs and which are difficult to model with latent annotations.", "labels": [], "entities": [{"text": "Treebank CFGs", "start_pos": 163, "end_pos": 176, "type": "DATASET", "confidence": 0.91519695520401}]}, {"text": "This paper is motivated by the complementarity of these approaches.", "labels": [], "entities": []}, {"text": "We present our progress in learning latent-variable TSGs in a joint approach that extends the split-merge framework of.", "labels": [], "entities": []}, {"text": "We present our current results on the Penn and Korean treebanks), demonstrating that we are able to learn fragments that draw on the strengths of both approaches.", "labels": [], "entities": [{"text": "Penn and Korean treebanks", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.8529411852359772}]}, {"text": "First, we investigate whether the tagset can be automatically derived after mapping all nonterminals to a single, coarse nonterminal.", "labels": [], "entities": []}, {"text": "Second, we begin with the mapping defined by the tagset, and investigate how closely the learned annotations resemble the original treebank.", "labels": [], "entities": []}, {"text": "Together with our TSG efforts, this work is aimed at increased flexibility in the grammar induction process, while retaining the use of Treebanks for structural guidance.", "labels": [], "entities": [{"text": "grammar induction process", "start_pos": 82, "end_pos": 107, "type": "TASK", "confidence": 0.8606321612993876}]}], "datasetContent": [{"text": "We perform a qualitative analysis of fragments learned on datasets for two languages: the Korean Treebank v2.0 (Han and) and a comparably-sized portion of the WSJ portion of the Penn Treebank (.", "labels": [], "entities": [{"text": "Korean Treebank v2.0", "start_pos": 90, "end_pos": 110, "type": "DATASET", "confidence": 0.9731731613477071}, {"text": "WSJ portion of the Penn Treebank", "start_pos": 159, "end_pos": 191, "type": "DATASET", "confidence": 0.876906951268514}]}, {"text": "The Korean Treebank (KTB) has predefined splits; to be comparable for our analysis, from the PTB we used \u00a72-3 for training and \u00a722 for validation (we refer to this as wsj2-3).", "labels": [], "entities": [{"text": "Korean Treebank (KTB)", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.9764188170433045}, {"text": "PTB", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.9772018194198608}]}, {"text": "As described in, although Korean presents its own challenges to grammar induction, the KTB yields additional difficulties by including a high occurrence of very flat rules (in 5K sentences, there are 13 NP rules with at least four righthand side NPs) and a coarser nonterminal set than that of the Penn Treebank.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7091173380613327}, {"text": "Penn Treebank", "start_pos": 298, "end_pos": 311, "type": "DATASET", "confidence": 0.9917188584804535}]}, {"text": "On both sets, we run for two iterations.", "labels": [], "entities": []}, {"text": "Recall that our algorithm is designed to induce a state-split TSG on a binarized tree; as neither dataset is binarized in native form we apply a left-branching binarization across all trees in both collections as a preprocessing step.", "labels": [], "entities": []}, {"text": "found different binarization methods to be inconsequential, and we have yet to observe significant impact of this binarization decision (this will be considered in more detail in future work).", "labels": [], "entities": []}, {"text": "(2011) provided a set of coarse, \"universal\" (as measured across 22 languages), part-of-speech tags.", "labels": [], "entities": []}, {"text": "We explore here the interaction of this tagset in our model on wsj2-3: call this modified version uwsj2-3, on which we run three iterations.", "labels": [], "entities": []}, {"text": "By further coarsening the PTB tags, we can ask questions such as: what is the refinement pattern?", "labels": [], "entities": [{"text": "PTB tags", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.8409481942653656}]}, {"text": "Can we identify linguistic phenomena in a different manner than we might without the universal tag set?", "labels": [], "entities": []}, {"text": "Then, as an extreme, we replace all POS tags with the same symbol \"X,\" to investigate what predicate/argument relationships can be derived: we call this set xwsj2-3 and run four times on it.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Top-three representatives for various refine-", "labels": [], "entities": []}]}