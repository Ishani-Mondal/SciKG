{"title": [{"text": "Dialogue Act Recognition using Reweighted Speaker Adaptation", "labels": [], "entities": [{"text": "Dialogue Act Recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5796265105406443}, {"text": "Reweighted Speaker Adaptation", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.5828655362129211}]}], "abstractContent": [{"text": "In this work we study the effectiveness of speaker adaptation for dialogue act recognition in multiparty meetings.", "labels": [], "entities": [{"text": "speaker adaptation", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7415759265422821}, {"text": "dialogue act recognition", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.7462467551231384}]}, {"text": "First, we analyze idiosyncracy in dialogue verbal acts by qualitatively studying the differences and conflicts among speakers and by quantitively comparing speaker-specific models.", "labels": [], "entities": []}, {"text": "Based on these observations, we propose anew approach for dialogue act recognition based on reweighted domain adaptation which effectively balance the influence of speaker specific and other speakers' data.", "labels": [], "entities": [{"text": "dialogue act recognition", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.7861597537994385}]}, {"text": "Our experiments on a real-world meeting dataset show that with even only 200 speaker-specific annotated dialogue acts, the performances on dialogue act recognition are significantly improved when compared to several baseline algorithms.", "labels": [], "entities": [{"text": "dialogue act recognition", "start_pos": 139, "end_pos": 163, "type": "TASK", "confidence": 0.6901236971219381}]}, {"text": "To our knowledge, this work is the first 1 to tackle this promising research direction of speaker adaptation for dialogue act recogntion.", "labels": [], "entities": [{"text": "speaker adaptation", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7904145121574402}, {"text": "dialogue act recogntion", "start_pos": 113, "end_pos": 136, "type": "TASK", "confidence": 0.7158273458480835}]}], "introductionContent": [{"text": "By representing a higher level intention of utterances during human conversation, dialogue act labels are being used to enrich the information provided by spoken words ().", "labels": [], "entities": []}, {"text": "Dialogue act recognition is a preliminary step towards deep dialogue understanding.", "labels": [], "entities": [{"text": "Dialogue act recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7670598228772482}]}, {"text": "It plays a key role in the design of dialogue systems.", "labels": [], "entities": []}, {"text": "Besides, find certain dialogue acts are important cues for detecting decisions in Multi-party dialogue.", "labels": [], "entities": []}, {"text": "In, dialogue acts are used as important features for flirt detection.", "labels": [], "entities": [{"text": "flirt detection", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.8740274012088776}]}, {"text": "Automatic dialogue act recognition is still an active research topic.", "labels": [], "entities": [{"text": "Automatic dialogue act recognition", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7778349220752716}]}, {"text": "The conventional approach is to train one generic classifier using a large corpus of annotated utterances.", "labels": [], "entities": []}, {"text": "One aspect that makes it so challenging is that people can express the same idea (or speech act) using a very different set of spoken words.", "labels": [], "entities": []}, {"text": "Even more, people can mean different things with the exact same spoken words.", "labels": [], "entities": []}, {"text": "These idiosyncratic differences in dialogue acts make the learning of generic classifiers extremely challenging.", "labels": [], "entities": []}, {"text": "Luckily, in many applications such as face-to-face meetings or tele-immersion, we have access to archives of previous interactions with the same participants.", "labels": [], "entities": []}, {"text": "From these archives, a small subset of spoken utterances can be efficiently annotated.", "labels": [], "entities": []}, {"text": "As we will later show in our experiments, even a small number of annotated utterances can make a significant difference.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew approach for dialogue act recognition based on reweighted domain adaptation which effectively balance the influence of speaker specific and other speakers' data.", "labels": [], "entities": [{"text": "dialogue act recognition", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.7839536070823669}]}, {"text": "By treating each speaker as one domain, we point out the connection between training speaker specific dialogue act classifier and supervised domain adaptation problem.", "labels": [], "entities": [{"text": "training speaker specific dialogue act classifier", "start_pos": 76, "end_pos": 125, "type": "TASK", "confidence": 0.6124205936988195}, {"text": "supervised domain adaptation", "start_pos": 130, "end_pos": 158, "type": "TASK", "confidence": 0.726038932800293}]}, {"text": "We analyze idiosyncracy in dialogue verbal acts by qualitatively studying the differences and conflicts among speakers and by quantitively comparing speaker-specific models.", "labels": [], "entities": []}, {"text": "We present an extensive set of experiments studying the effect of speaker adaptation on dialogue act recogntion in multi-party meetings using the ICSI-MRDA dataset).", "labels": [], "entities": [{"text": "speaker adaptation", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.6920207738876343}, {"text": "dialogue act recogntion", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.6314313312371572}, {"text": "ICSI-MRDA dataset", "start_pos": 146, "end_pos": 163, "type": "DATASET", "confidence": 0.9646892249584198}]}, {"text": "The following section presents related work on dialogue act recognition and domain adaptation.", "labels": [], "entities": [{"text": "dialogue act recognition", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.8651086290677389}, {"text": "domain adaptation", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7578967213630676}]}, {"text": "Section 3 describes the ICSI-MRDA) dataset which is used in all our experiments.", "labels": [], "entities": [{"text": "ICSI-MRDA) dataset", "start_pos": 24, "end_pos": 42, "type": "DATASET", "confidence": 0.8474372824033102}]}, {"text": "Section 4 analyze idiosyncracy in dialogue acts, both qualitatively and quantitatively.", "labels": [], "entities": []}, {"text": "Section 5 explains our reweighting-based speaker adaptation algorithm.", "labels": [], "entities": [{"text": "reweighting-based speaker adaptation", "start_pos": 23, "end_pos": 59, "type": "TASK", "confidence": 0.636584480603536}]}, {"text": "Section 6 contains all experiments to prove the applicability of speaker adaptation to dialogue act recognition.", "labels": [], "entities": [{"text": "speaker adaptation", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7636461555957794}, {"text": "dialogue act recognition", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.6805634299914042}]}, {"text": "Finally, inspired by the promising results, Section 8 describes some future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our goal is to get one model specifically adapted for each speaker.", "labels": [], "entities": []}, {"text": "We first describes 4 different approaches to be compared in the experiments, and section 6.2 explains our experimental methodology.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The 7 speakers from ICSI-MRDA dataset used  in our experiments. The table lists: the Speaker ID, orig- inal speaker tag, the type of meeting selected for this  speaker, the number of meetings this speaker participated  and the total number of dialogue acts by this speaker.", "labels": [], "entities": [{"text": "ICSI-MRDA dataset", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.9327444136142731}]}, {"text": " Table 2. Tag  proportion  Disruption  14.73%  Back Channel  10.20%  Floor Mechanism  12.40%  Question  7.20%  Statement  55.46%", "labels": [], "entities": [{"text": "Tag  proportion  Disruption", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.8832215070724487}, {"text": "Question", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.8998451232910156}]}, {"text": " Table 3: Average results among all 7 speakers when train  with different combinations of speaker specific data and  other speakers' data. The number of speaker specific data  is varied from 200, 500, 1000, 1500 to 2000.", "labels": [], "entities": []}]}