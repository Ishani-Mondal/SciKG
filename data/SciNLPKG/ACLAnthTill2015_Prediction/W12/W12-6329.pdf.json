{"title": [{"text": "Evaluation Reportof the third Chinese Parsing Evaluation: CIPS- SIGHAN-ParsEval-2012", "labels": [], "entities": [{"text": "Chinese Parsing Evaluation: CIPS- SIGHAN-ParsEval-2012", "start_pos": 30, "end_pos": 84, "type": "DATASET", "confidence": 0.5971284210681915}]}], "abstractContent": [{"text": "This paper gives the overview of the third Chinese parsing evaluation: CIPS-SIGHAN-ParsEval-2012, including its parsing sub-tasks, evaluation metrics, training and test data.", "labels": [], "entities": [{"text": "Chinese parsing evaluation", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.6155672868092855}, {"text": "CIPS-SIGHAN-ParsEval-2012", "start_pos": 71, "end_pos": 96, "type": "DATASET", "confidence": 0.8988462090492249}, {"text": "parsing", "start_pos": 112, "end_pos": 119, "type": "TASK", "confidence": 0.9634156227111816}]}, {"text": "The detailed evaluation results and simple discussions will be given to show the difficulties in Chinese syntactic parsing.", "labels": [], "entities": [{"text": "Chinese syntactic parsing", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.6260770161946615}]}], "introductionContent": [{"text": "The first and second Chinese parsing evaluations CIPS-ParsEval-2009( and CIPS-SIGHAN-ParsEval-2010 ( were held successfully in 2009 and 2010 respectively.", "labels": [], "entities": [{"text": "parsing evaluations CIPS-ParsEval-2009", "start_pos": 29, "end_pos": 67, "type": "TASK", "confidence": 0.5150783856709799}]}, {"text": "The evaluation results in the Chinese clause and sentence levels show that the complex sentence parsing is still a big challenge for the Chinese language.", "labels": [], "entities": [{"text": "complex sentence parsing", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.7546115318934122}]}, {"text": "This time we will focus on the sentence parsing task proposed by the second CIPS-SIGHANParsEval-2010 to dig out the detailed difficulties of Chinese complex sentence parsing in the respect of two typical sentence complexity schemes: event combination in the sentence level and concept composition in the clausal level.", "labels": [], "entities": [{"text": "sentence parsing", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7038193941116333}, {"text": "CIPS-SIGHANParsEval-2010", "start_pos": 76, "end_pos": 100, "type": "DATASET", "confidence": 0.9506589770317078}, {"text": "Chinese complex sentence parsing", "start_pos": 141, "end_pos": 173, "type": "TASK", "confidence": 0.5949432998895645}]}, {"text": "We will introduce anew lexicon-based Combinatory Categorical Grammar (CCG) () annotation scheme in the evaluation, and make a parallel comparison of the parser performance with the traditional Phrase Structure Grammar (PSG) used in the Tsinghua Chinese Treebank (TCT).", "labels": [], "entities": [{"text": "Tsinghua Chinese Treebank (TCT)", "start_pos": 236, "end_pos": 267, "type": "DATASET", "confidence": 0.7796907226244608}]}, {"text": "This evaluation includes two sub-tasks, i.e. PSG parsing evaluation and CCG parsing evaluation.", "labels": [], "entities": [{"text": "PSG parsing evaluation", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.8228783011436462}, {"text": "CCG parsing evaluation", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.6911562085151672}]}, {"text": "For each sub-task, there are two tracks.", "labels": [], "entities": []}, {"text": "One is the Close track in which model parameter estimation is conducted solely on the train data.", "labels": [], "entities": [{"text": "Close", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.8273525834083557}, {"text": "model parameter estimation", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.6061908006668091}]}, {"text": "The other is the Open track in which any datasets other than the given training data can be used to estimate model parameters.", "labels": [], "entities": []}, {"text": "We will set separated evaluation ranks for these two tracks.", "labels": [], "entities": []}, {"text": "In addition, we will evaluate following two kinds of methods separately in each track.", "labels": [], "entities": []}, {"text": "1) Single system: parsers that use a single parsing model to finish the parsing task.", "labels": [], "entities": []}, {"text": "2) System combination: participants are allowed to combine multiple models to improve the performance.", "labels": [], "entities": []}, {"text": "Collaborative decoding methods will be regarded as a combination method.", "labels": [], "entities": []}], "datasetContent": [{"text": "Input: A Chinese sentence with correct word segmentation annotation.", "labels": [], "entities": []}, {"text": "The word number is more than 2.", "labels": [], "entities": [{"text": "word number", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.813236802816391}]}, {"text": "The following is an example: \uf06c \u5c0f\u578b(small) \u6728\u6750(wood) \u52a0\u5de5\u573a(factory) \u5728(is) \u5fd9(busy) \u7740(-modality) \u5236\u4f5c(build) \u5404 (several) \u79cd (-classifier) \u6728 \u5236 \u54c1 (woodwork) \u3002(period) (A small wood factory is busy to build several woodworks.)", "labels": [], "entities": []}, {"text": "Parsing goal: Assign appropriate part-of-speech (POS) tags tothe words in the sentence and generate phrase structure tree for the sentence.", "labels": [], "entities": []}, {"text": "Output: The phrase structure tree with POS tags for the sentence.", "labels": [], "entities": [{"text": "Output", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.888080894947052}]}, {"text": "Input: Same with task 1.", "labels": [], "entities": [{"text": "Input", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8887890577316284}]}, {"text": "Parsing goal: Assign appropriate CCG category tags tothe wordsin the sentence and generate CCG derivation tree for the sentence.", "labels": [], "entities": []}, {"text": "Output: The CCG derivation tree with CCG category tags for the sentence.", "labels": [], "entities": [{"text": "Output", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9075492024421692}]}, {"text": "There are two parsing stages for the PSG and CCG parsers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.9686238765716553}]}, {"text": "One is the stage of syntactic category assignment, including POS tag and CCG category.", "labels": [], "entities": [{"text": "syntactic category assignment", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.6249092916647593}]}, {"text": "The other is the stage of parse tree generation, including PSG parsing tree and CCG derivation tree.", "labels": [], "entities": [{"text": "parse tree generation", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.9281284014383951}, {"text": "PSG parsing", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.7975547313690186}]}, {"text": "So we design two different sets of metrics for them.", "labels": [], "entities": []}, {"text": "Basic metrics are the syntactic category tagging precision (SC_P), recall (SC_R) and F1-score(SC_F1).", "labels": [], "entities": [{"text": "syntactic category tagging precision (SC_P)", "start_pos": 22, "end_pos": 65, "type": "METRIC", "confidence": 0.7560393247339461}, {"text": "recall (SC_R)", "start_pos": 67, "end_pos": 80, "type": "METRIC", "confidence": 0.9427658915519714}, {"text": "F1-score(SC_F1)", "start_pos": 85, "end_pos": 100, "type": "METRIC", "confidence": 0.9166298906008402}]}, {"text": "\uf06c SC_P= (#of correctly tagged words) /(# of automatically tagged words) * 100% \uf06c SC_R= (#of correctly tagged words) /(# of gold-standard words) * 100% \uf06c SC_F1= 2*SC_P*SC_R / (SC_P + SC_R) The correctly tagged words must have the same syntactic categories with the gold-standard ones.", "labels": [], "entities": []}, {"text": "To obtain detailed evaluation results for different syntactic categories, we can classify all tagged words into different sets and compute different SC_P, SC_R and SC_F1 for them.", "labels": [], "entities": [{"text": "F1", "start_pos": 167, "end_pos": 169, "type": "METRIC", "confidence": 0.8390369415283203}]}, {"text": "The classification condition is as follows.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9481636881828308}]}, {"text": "If (SC_Token_Ratio>=10%) then the syntactic tag will be one class with its SC tag, otherwise all other low-frequency SC-tagged words will be classified with a special class with Oth_SC tag.", "labels": [], "entities": []}, {"text": "Where, SC_Token_Ratio= (word token # of one special SC in the test set) / (word token # in the test set) * 100%.", "labels": [], "entities": []}, {"text": "Basic metrics are the labeled constituent precision (LC_P), recall (LC_R) and F1-score (LC_F1).", "labels": [], "entities": [{"text": "labeled constituent precision (LC_P)", "start_pos": 22, "end_pos": 58, "type": "METRIC", "confidence": 0.7751162685453892}, {"text": "recall (LC_R)", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9562215407689413}, {"text": "F1-score (LC_F1)", "start_pos": 78, "end_pos": 94, "type": "METRIC", "confidence": 0.9389777382214864}]}, {"text": "\uf06c LC_P = (#of correctly labeled constituents) /(# of automatically parsed constituents) * 100% \uf06c LC_R= (# of correctly labeled constituents) / (# of gold-standard constituents) * 100% \uf06c LC_F1= 2*LC_P*LC_R / (LC_P+LC_R) The correctly labeled constituents must have the same syntactic tags and left and right boundaries with the gold-standard ones.", "labels": [], "entities": [{"text": "\uf06c LC_R", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.6234094053506851}, {"text": "\uf06c LC_F1", "start_pos": 184, "end_pos": 191, "type": "METRIC", "confidence": 0.6247184500098228}]}, {"text": "To obtain detailed evaluation results for different syntactic constituents, we can classify them into 6 sets and compute different LC_P, LC_R and LC_F1 for them.", "labels": [], "entities": [{"text": "F1", "start_pos": 149, "end_pos": 151, "type": "METRIC", "confidence": 0.5376073122024536}]}, {"text": "(1) Clausal and phrasal constituents (2) Complex event constituents (3) Concept compound constituents (4) Single-node constituents (5) Complementary parsing constituents (6) All other constituents The classification is based on the syntactic constituent tags annotated in the automatically parsed results.", "labels": [], "entities": []}, {"text": "Please refer next section for more detailed information.", "labels": [], "entities": []}, {"text": "We compute the labeled F1-scores of the first four sets (Tot4_LC_F1) to obtain the final ranked scores for different proposed systems.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9888206124305725}, {"text": "Tot4_LC_F1)", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.6485833128293356}]}, {"text": "For comparison analysis, we also list the F1-scores of all six sets for ranking reference.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9988553524017334}]}, {"text": "To estimate the possible performance upper bound of the automatic parsers, we also design the following complementary metrics: (  We used the annotated sentences in the TCT version 1.0 (Zhou, 2004) as the basic resources and designed the following automatic transformation procedures to obtain the final training and test data for the two parsing tasks.", "labels": [], "entities": [{"text": "TCT version 1.0 (Zhou, 2004)", "start_pos": 169, "end_pos": 197, "type": "DATASET", "confidence": 0.8797636479139328}]}, {"text": "Firstly, we make binary for all TCT annotation trees 1 and obtain anew binarizated TCT version.", "labels": [], "entities": []}, {"text": "Two new grammatical relation tags RT and LT are added to describe the inserted dummy nodes with left and right punctuation combination structures.", "labels": [], "entities": [{"text": "LT", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9043969511985779}]}, {"text": "They can provide basic parsing tree structures for PSG and CCG parsing evaluations.", "labels": [], "entities": [{"text": "CCG parsing evaluations", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.6898748477300009}]}, {"text": "Secondly, we classify all TCT constituents into 6 sets, according to the syntactic constituent (SynC) and grammatical relation (GR) tags annotated in TCT 2 . 1. Clausal and phrasal constituents, if all the following two conditions are matched a) TCT GR tag \u2208{ZW, PO, DZ, ZZ, 1 TCT binarizationalgorithm and TCT2CCG tool were finished during the author visited Microsoft Research Asia (MSRA) in April, 2011.", "labels": [], "entities": [{"text": "Microsoft Research Asia (MSRA)", "start_pos": 360, "end_pos": 390, "type": "DATASET", "confidence": 0.8382994929949442}]}, {"text": "The visiting project was supported by the MSRA research foundation provided by Prof. Ming Zhou and Prof. Changning Huang.", "labels": [], "entities": [{"text": "MSRA research foundation", "start_pos": 42, "end_pos": 66, "type": "DATASET", "confidence": 0.9401888449986776}]}, {"text": "2 Please refer () for more detailed descriptions of these syntactic constituent and grammatical relation tags.", "labels": [], "entities": []}, {"text": "JY, FW, JB, AD} b) TCT Sync tag \u2208{dj, np, sp, tp, mp, vp, ap, dp, pp, mbar, bp} 2.", "labels": [], "entities": []}, {"text": "Complex event constituents, if one of the following conditions is matched.", "labels": [], "entities": []}, {"text": "a) TCT SynC tag=fj and TCT GR tag \u2208{BL, LG, DJ, YG, MD, TJ, JS, ZE, JZ, LS} b) TCT SynC tag=jq 3.", "labels": [], "entities": []}, {"text": "Concept compound constituents, if all the following two conditions are matched a) TCT GR tag \u2208{LH, LW, SX, CD, FZ, BC, SB} b) TCT Sync tag \u2208{np, vp, ap, bp, dp, mp, sp, tp, pp} 4.", "labels": [], "entities": [{"text": "TCT GR tag", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.8403177062670389}]}, {"text": "Single-node constituents, if TCT SynC tag=dlc 5.", "labels": [], "entities": []}, {"text": "Complementary parsing constituents, if TCT GR tag \u2208{RT, LT, XX} 6.", "labels": [], "entities": []}, {"text": "All other constituents They will provide basic information for detailed parsing tree evaluation metrics computation.", "labels": [], "entities": [{"text": "parsing tree evaluation metrics computation", "start_pos": 72, "end_pos": 115, "type": "TASK", "confidence": 0.8970005512237549}]}, {"text": "Finally, we build the evaluation data sets for two parsing tasks through the following approaches: 1.", "labels": [], "entities": [{"text": "parsing tasks", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9014285802841187}]}, {"text": "For PSG parsing evaluation, we automatically transform the TCT annotation data through: a) For the syntactic constituents belong to the above class 2-3 and 5-6, we retain the original TCT two tags; b) For the syntactic constituent belong to the above class 1-4, we only retain the original TCT SynC tags.", "labels": [], "entities": [{"text": "PSG parsing evaluation", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.939349353313446}]}, {"text": "2. For CCG parsing evaluation, we automatically transform the TCT annotation data into CCG format by using the TCT2CCG tool (Zhou, 2011).", "labels": [], "entities": [{"text": "CCG parsing evaluation", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.7898149887720743}, {"text": "TCT annotation data", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.6362563172976176}]}], "tableCaptions": [{"text": " Table 1 Basic statistics of the training and test data:  Average Sentence Length(ASL)=Word Sum/ Sent.  Sum)  Sent.  Sum", "labels": [], "entities": [{"text": "Average Sentence Length(ASL)=Word Sum", "start_pos": 58, "end_pos": 95, "type": "METRIC", "confidence": 0.889448769390583}]}, {"text": " Table 2 Different annotated constituents in the training and test set  Class 1 Class 2 Class 3 Class 4 Class 5 Class 6 Total  Training set 310394 24239 30719  2735  89836  316  458239  Test set  16617  1578  1224  53  4746  50  24268", "labels": [], "entities": [{"text": "Total  Training set 310394 24239 30719  2735  89836  316  458239  Test set  16617  1578  1224  53  4746  50  24268", "start_pos": 120, "end_pos": 234, "type": "DATASET", "confidence": 0.8133758447672191}]}, {"text": " Table 3 Participant information for ParsEval-2012  ID  Participants  Registered  Tasks", "labels": [], "entities": [{"text": "ParsEval-2012  ID  Participants  Registered  Tasks", "start_pos": 37, "end_pos": 87, "type": "TASK", "confidence": 0.4852083265781403}]}, {"text": " Table 4 Ranked results in the Open Track of the PSG parsing task", "labels": [], "entities": [{"text": "PSG parsing", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.7241322845220566}]}, {"text": " Table 5 Ranked results in the Close Track of the PSG parsing task", "labels": [], "entities": [{"text": "Close Track", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9168025851249695}, {"text": "PSG parsing", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.8325304090976715}]}, {"text": " Table 6 Evaluation results of the different classes in the Open Track (unlabeled constituents)", "labels": [], "entities": []}, {"text": " Table 7 Evaluation results of the different classes in the Open Track (labeled constituents)", "labels": [], "entities": []}, {"text": " Table 8 Evaluation results of the different classes in the Closed Track (Unlabeled constituents)", "labels": [], "entities": []}]}