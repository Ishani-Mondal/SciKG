{"title": [{"text": "Towards Probabilistic Acceptors and Transducers for Feature Structures", "labels": [], "entities": []}], "abstractContent": [{"text": "Weighted finite-state acceptors and transducers (Pereira and Riley, 1997) area critical technology for NLP and speech systems.", "labels": [], "entities": []}, {"text": "They flexibly capture many kinds of stateful left-to-right substitution, simple transducers can be composed into more complex ones, and they are EM-trainable.", "labels": [], "entities": []}, {"text": "They are unable to handle long-range syntactic movement, but tree acceptors and transducers address this weakness (Knight and Graehl, 2005).", "labels": [], "entities": []}, {"text": "Tree au-tomata have been profitably used in syntax-based MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9139859676361084}]}, {"text": "Still, strings and trees are both weak at representing linguistic structure involving semantics and reference (\"who did what to whom\").", "labels": [], "entities": []}, {"text": "Feature structures provide an attractive, well-studied, standard format (Shieber, 1986; Rounds and Kasper, 1986), which we can view computationally as directed acyclic graphs.", "labels": [], "entities": []}, {"text": "In this paper, we develop probabilistic acceptors and transducers for feature structures, demonstrate them on linguistic problems, and lay down a foundation for semantics-based MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 177, "end_pos": 179, "type": "TASK", "confidence": 0.8601346015930176}]}], "introductionContent": [{"text": "Weighted finite-state acceptors and transducers) provide a clean and practical knowledge representation for string-based speech and language problems.", "labels": [], "entities": []}, {"text": "Complex problems can be broken down into cascades of simple transducers, and generic algorithms (best path, composition, EM, etc) can be re-used across problems.", "labels": [], "entities": []}, {"text": "String automata only have limited memory and cannot handle complex transformations needed in machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.8260175704956054}]}, {"text": "Weighted tree acceptors and transducers) have proven valuable in these scenarios.", "labels": [], "entities": []}, {"text": "For example, systems that transduce source strings into target syntactic trees performed well in recent MT evaluations.", "labels": [], "entities": [{"text": "MT evaluations", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.9114541709423065}]}, {"text": "To build the next generation of language systems, we would like to represent and transform deeper linguistic structures, e.g., ones that explicitly capture semantic \"who does what to whom\" relationships, with syntactic sugar stripped away.", "labels": [], "entities": []}, {"text": "Feature structures area well-studied formalism for capturing natural language semantics; and provide overviews.", "labels": [], "entities": []}, {"text": "A feature structure is defined as a collection of unordered features, each of which has a value.", "labels": [], "entities": []}, {"text": "The value maybe anatomic symbol, or it may itself be another feature structure.", "labels": [], "entities": []}, {"text": "Furthermore, structures maybe re-entrant, which means that two feature paths may point to the same value.", "labels": [], "entities": []}, {"text": "shows a feature structure that captures the meaning of a sample sentence.", "labels": [], "entities": []}, {"text": "This semantic structure provides much more information than atypical parse, including semantic roles on both nouns and verbs.", "labels": [], "entities": []}, {"text": "Note how \"Pascale\" plays four different semantic roles, even though it appears only once overtly in the string.", "labels": [], "entities": []}, {"text": "The feature structure also makes clear which roles are unfilled (such as the agent of the charging), by omitting them.", "labels": [], "entities": []}, {"text": "For computational purposes, feature structures are often represented as rooted, directed acyclic graphs with edge and leaf labels.", "labels": [], "entities": []}, {"text": "While feature structures are widely used in handbuilt grammars, there has been no compelling proposal for weighted acceptors and transducers for  Devices: FSA = finite string automaton; ln-XTOPs = linear non-deleting extended top-down tree-to-string transducer; RTG = regular tree grammar.", "labels": [], "entities": [{"text": "FSA", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.93143230676651}]}, {"text": "string automata tree automata graph automata k-best . .", "labels": [], "entities": []}, {"text": "paths through a WFSA . .", "labels": [], "entities": [{"text": "WFSA", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.8119224309921265}]}, {"text": "trees in a weighted forest ? EM training Forward-backward EM ( Tree transducer EM training ( ? Determinization . .", "labels": [], "entities": []}, {"text": ". of weighted string acceptors . them.", "labels": [], "entities": []}, {"text": "Such automata would be of great use.", "labels": [], "entities": []}, {"text": "For example, a weighted graph acceptor could form the basis of a semantic language model, and a weighted graph-to-tree transducer could form the basis of a natural language understanding (NLU) or generation (NLG) system, depending on which direction it is employed.", "labels": [], "entities": []}, {"text": "Putting NLU and NLG together, we can also envision semantics-based MT systems (.", "labels": [], "entities": []}, {"text": "A similar approach has been taken by who incorporate LFG fstructures, which are deep syntax feature structures, into their (automatically acquired) transfer rules.", "labels": [], "entities": []}, {"text": "Feature structure graph acceptors and transducers could themselves be learned from semanticallyannotated data, and their weights trained by EM.", "labels": [], "entities": [{"text": "EM", "start_pos": 140, "end_pos": 142, "type": "DATASET", "confidence": 0.833704948425293}]}, {"text": "However, there is some distance to be traveled.", "labels": [], "entities": []}, {"text": "gives a snapshot of some efficient, generic algorithms for string automata (mainly developed in the last century), plus algorithms for tree automata (mainly developed in the last ten years).", "labels": [], "entities": []}, {"text": "These algorithms have been packaged in general-purpose software toolkits like AT&T FSM (), OpenFST (, and Tiburon ().", "labels": [], "entities": [{"text": "AT&T FSM", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.8034045100212097}, {"text": "OpenFST", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.8943570852279663}]}, {"text": "A research program for graphs should hold similar value.", "labels": [], "entities": []}, {"text": "Formal graph manipulation has, fortunately, received prior attention.", "labels": [], "entities": [{"text": "Formal graph manipulation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7639832695325216}]}, {"text": "A unification grammar can specify semantic mappings for strings, effectively capturing an infinite set of string/graph pairs.", "labels": [], "entities": []}, {"text": "But unification grammars seem too powerful to admit the efficient algorithms we desire in, and weighted versions are not popular.", "labels": [], "entities": [{"text": "unification grammars", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.9207476079463959}]}, {"text": "Hyperedge replacement grammars ( are another natural candidate for graph acceptors, and asynchronous hyperedge replacement grammar might serve as a graph transducer.", "labels": [], "entities": []}, {"text": "Finally, propose graph acceptor and graph-to-tree transducer formalisms for rooted directed acyclic graphs.", "labels": [], "entities": []}, {"text": "Their model has been extended to multi-rooted dags and arbitrary hypergraphs; however, these extensions seem too powerful for NLP.", "labels": [], "entities": []}, {"text": "Hence, we use the model of as a starting point for our definition, then we give a natural language example, followed by an initial set of generic algorithms for graph automata.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}