{"title": [{"text": "System Combination with Extra Alignment Information", "labels": [], "entities": [{"text": "Extra Alignment", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.6814746111631393}]}], "abstractContent": [{"text": "This paper provides the system description of the IHMM team of Dublin City University for our participation in the system combination task in the Second Workshop on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid MT (ML4HMT-12).", "labels": [], "entities": [{"text": "Dublin City University", "start_pos": 63, "end_pos": 85, "type": "DATASET", "confidence": 0.8698787490526835}, {"text": "Division of Labour in Hybrid MT (ML4HMT-12)", "start_pos": 218, "end_pos": 261, "type": "TASK", "confidence": 0.6951905025376214}]}, {"text": "Our work is based on a confusion network-based approach to system combination.", "labels": [], "entities": [{"text": "system combination", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.697101354598999}]}, {"text": "We propose anew method to build a confusion network for this: (1) incorporate extra alignment information extracted from given metadata, treating them assure alignments, into the results from IHMM, and (2) decode together with this information.", "labels": [], "entities": [{"text": "IHMM", "start_pos": 192, "end_pos": 196, "type": "DATASET", "confidence": 0.8963921666145325}]}, {"text": "We also heuristically set one of the system outputs as the default backbone.", "labels": [], "entities": []}, {"text": "Our results show that this backbone, which is the RBMT system output, achieves an 0.11% improvement in BLEU over the backbone chosen by TER, while the extra information we added in the decoding part does not improve the results.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.5578708648681641}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9993589520454407}, {"text": "TER", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.4739989638328552}]}], "introductionContent": [{"text": "This paper describes anew extension to our system combination module in Dublin City University for the participation in the system combination task in the ML4HMT-2012 workshop.", "labels": [], "entities": [{"text": "Dublin City University", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.9798961480458578}, {"text": "system combination task in the ML4HMT-2012 workshop", "start_pos": 124, "end_pos": 175, "type": "TASK", "confidence": 0.6699823183672768}]}, {"text": "We incorporate alignment meta information to the alignment module when building a confusion network.", "labels": [], "entities": []}, {"text": "Given multiple translation outputs, a system combination strategy aims at finding the best translations, either by choosing one sentence or generating anew translation from fragments originated from individual systems(.", "labels": [], "entities": []}, {"text": "Combination methods have been widely used in fields such as parsing) and speech recognition.", "labels": [], "entities": [{"text": "parsing", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9779654145240784}, {"text": "speech recognition", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.8812428712844849}]}, {"text": "In late the 90s, the speech recognition community produced a confusion network-based system combination approach, spreading instantly to SMT community as well.", "labels": [], "entities": [{"text": "speech recognition community", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.8324935833613077}, {"text": "SMT community", "start_pos": 137, "end_pos": 150, "type": "TASK", "confidence": 0.902184009552002}]}, {"text": "The traditional system combination approach employs confusion networks which are built by the monolingual alignment which induces sentence similarity.", "labels": [], "entities": []}, {"text": "Confusion networks are compact graph-based structures representing multiple hypothesises ().", "labels": [], "entities": []}, {"text": "It is noted that there are several generalized forms of confusion networks as well.", "labels": [], "entities": []}, {"text": "One is a lattice) and the other is a translation forest ().", "labels": [], "entities": []}, {"text": "The former employs lattices that can describe arbitrary mappings in hypothesis alignments.", "labels": [], "entities": []}, {"text": "A lattice is more general than a confusion network.", "labels": [], "entities": []}, {"text": "By contrast, a confusion forest exploits syntactic similarity between individual outputs.", "labels": [], "entities": []}, {"text": "Up to now, various state-of-the-art alignment methods have been developed including Indirect-HMM ( which is a statistical-model-based method, and TER which is a metric-based method which uses an edit distance.", "labels": [], "entities": [{"text": "TER", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9175718426704407}]}, {"text": "In this work we focus on the IHMM method.", "labels": [], "entities": [{"text": "IHMM", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.6602789759635925}]}, {"text": "The main problem of IHMM is that there are numerous one-to-many and one-to-null cases in the alignment results.", "labels": [], "entities": [{"text": "IHMM", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.5957616567611694}]}, {"text": "This alignment noise significantly affects the confusion network construction and the decoding process.", "labels": [], "entities": [{"text": "confusion network construction", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6387851933638254}]}, {"text": "In this work, in addition to the IHMM alignment, we also incorporate alignment meta information extracted from an RBMT system to help the decoding process.", "labels": [], "entities": [{"text": "IHMM alignment", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.706438422203064}]}, {"text": "The other crucial factor is the backbone selection which also affects the combination results.", "labels": [], "entities": []}, {"text": "The backbone determines the word order in the final output.", "labels": [], "entities": []}, {"text": "Backbone selection is often done by Minimum Bayes Risk (MBR) decoding which selects a hypothesis with minimum average distance among all hypotheses (.", "labels": [], "entities": [{"text": "Backbone selection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8315785825252533}, {"text": "Minimum Bayes Risk (MBR", "start_pos": 36, "end_pos": 59, "type": "METRIC", "confidence": 0.6877006351947784}]}, {"text": "In this work we heuristically choose an RBMT output as the backbone due to its (expected) overall grammatically well-formed output and better human evaluation results.", "labels": [], "entities": []}, {"text": "We report our results and provide a comparison with traditional confusion-network-based network approach.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: We will review the state-of-the-art system combination framework based on confusion networks in Section 2.", "labels": [], "entities": []}, {"text": "We describe our experimental setup, how we extract the alignment information from meta-data and how we use it in Section 3.", "labels": [], "entities": []}, {"text": "The results and analysis are also given in this section.", "labels": [], "entities": []}, {"text": "We draw conclusions in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we set \u03b1 0.1 in Equation 5, according to).", "labels": [], "entities": [{"text": "Equation", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9742334485054016}]}, {"text": "All development set and test set data are tokenized and lower cased.", "labels": [], "entities": []}, {"text": "We use mteval-v13.pl 1 , no-smoothing and case sensitive for the evaluation.", "labels": [], "entities": []}, {"text": "shows the result of using Lucy as a backbone and the result of changing \u03b8 \u03c8 on the development and test sets.", "labels": [], "entities": []}, {"text": "Note that \u03b8 \u03c8 = 1 stands for the case when there is no effect of this factor on the current path.", "labels": [], "entities": []}, {"text": "From, we see a slight decrease of quanlity when we increased the factor.", "labels": [], "entities": [{"text": "quanlity", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9734835624694824}]}, {"text": "But an interesting observation is that when we increased \u03b8 \u03c8 to 10 the result was not much affected.", "labels": [], "entities": []}, {"text": "We believe this is since the sure alignments which we extracted from the Lucy alignments were almost perfectly consistent with the alignment resulting from IHMM.", "labels": [], "entities": [{"text": "IHMM", "start_pos": 156, "end_pos": 160, "type": "DATASET", "confidence": 0.9589408040046692}]}, {"text": "The best path derived by IHMM included most of the sure alignments extracted from Lucy.", "labels": [], "entities": [{"text": "IHMM", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.9056152105331421}]}, {"text": "Devset Testset when \u03b8 \u03c8 = 1) with that of the TER backbone in.", "labels": [], "entities": [{"text": "Devset", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8853362798690796}, {"text": "TER backbone", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.9258117973804474}]}, {"text": "We can see that the Lucy backbone result was 0.11% better than that of TER.", "labels": [], "entities": [{"text": "Lucy backbone", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.8785264790058136}, {"text": "TER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.6169761419296265}]}, {"text": "This confirmed our assumption that Lucy would be a good backbone in system combination.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The Lucy backbone with tuning of \u03b8 \u03c8 .", "labels": [], "entities": [{"text": "Lucy backbone", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.971455454826355}, {"text": "tuning", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9533622860908508}]}, {"text": " Table 2: TER Backbone selection results.  We compared results with thos obtained by Lucy backbone (which are in the", "labels": [], "entities": [{"text": "TER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9515149593353271}, {"text": "Lucy backbone", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.9245612025260925}]}, {"text": " Table 2. We can see that the Lucy backbone result  was 0.11% better than that of TER. This confirmed our assumption that Lucy would be a good  backbone in system combination.", "labels": [], "entities": [{"text": "TER", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.8031715750694275}]}]}