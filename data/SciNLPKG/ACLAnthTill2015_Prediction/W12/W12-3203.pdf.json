{"title": [{"text": "Discovering Factions in the Computational Linguistics Community", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a joint probabilistic model of who cites whom in computational linguistics, and also of the words they use to do the citing.", "labels": [], "entities": []}, {"text": "The model reveals latent factions, or groups of individuals whom we expect to collaborate more closely within their faction, cite within the faction using language distinct from citation outside the faction, and be largely understandable through the language used when cited from without.", "labels": [], "entities": []}, {"text": "We conduct an exploratory data analysis on the ACL Anthology.", "labels": [], "entities": [{"text": "the ACL Anthology", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.7012355923652649}]}, {"text": "We extend the model to reveal changes in some authors' faction memberships overtime.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ACL Anthology presents an excellent dataset for studying both the language and the social connections in our evolving research field.", "labels": [], "entities": [{"text": "ACL Anthology", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.957703709602356}]}, {"text": "Extensive studies using techniques from the field of bibliometrics have been applied to this dataset (), quantifying the importance and impact factor of both authors and articles in the community.", "labels": [], "entities": []}, {"text": "Moreover, recent work has leveraged the availability of digitized publications to study trends and influences within the ACL community ( and to analyze academic collaborations.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, however, existing work has mainly pursued \"macroscopic\" investigations of the interaction of authors in collaboration, citation networks, or the textual content of whole papers.", "labels": [], "entities": []}, {"text": "We seek to complement these results with a \"microscopic\" investigation of authors' interactions by considering the individual sentences authors use to cite each other.", "labels": [], "entities": []}, {"text": "In this paper, we present a joint model of who cites whom in computational linguistics, and also of how they do the citing.", "labels": [], "entities": []}, {"text": "Central to this model is the idea of factions, or groups of individuals whom we expect to (i) collaborate more closely within their faction, (ii) cite within the faction using language distinct from citation outside the faction, (iii) be largely understandable through the language used when cited from without, and (iv) evolve overtime.", "labels": [], "entities": []}, {"text": "Factions can bethought of as \"communities,\" which are loosely defined in the literature on networks as subgraphs where internal connections are denser than external ones ().", "labels": [], "entities": []}, {"text": "The distinction here is that the strength of connections depends on a latent language model estimated from citation contexts.", "labels": [], "entities": []}, {"text": "This paper is an exploratory data analysis using a Bayesian generative model.", "labels": [], "entities": []}, {"text": "We aim both to discover meaningful factions in the ACL community and also to illustrate the use of a probabilistic model for such discovery.", "labels": [], "entities": []}, {"text": "As such, we do not present any objective evaluation of the model or make any claims that the factions optimally explain the research community.", "labels": [], "entities": []}, {"text": "Indeed, we suspect that reaching abroad consensus among community members about factions (i.e., a \"gold standard\") would be quite difficult, as any social community's factions are likely perceived very subjectively.", "labels": [], "entities": []}, {"text": "It is for this reason that a probabilistic generative model, in which all assumptions are made plain, is appropriate for the task.", "labels": [], "entities": []}, {"text": "We hope this analysis will prove useful in future empirical research on social communities (including scientific ones) and their use of language.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the ACL Anthology Network Corpus (), which currently contains 18,041 papers written by 12,777 authors.", "labels": [], "entities": [{"text": "ACL Anthology Network Corpus", "start_pos": 12, "end_pos": 40, "type": "DATASET", "confidence": 0.9566093534231186}]}, {"text": "These papers are published in the field of computational linguistics between 1965 and 2011.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.6958917677402496}]}, {"text": "2 Furthermore, the corpus provides bibliographic data such as authors of the papers and bibliographic references between each paper in the corpus.", "labels": [], "entities": []}, {"text": "We extracted sentences containing citations using regular expressions and linked them between authors with the help of metadata provided in the corpus.", "labels": [], "entities": []}, {"text": "We tokenized the extracted sentences and downcased them.", "labels": [], "entities": []}, {"text": "Words that are numeric, appear less than 20 times, or are in a stop word list are discarded.", "labels": [], "entities": []}, {"text": "For papers with multiple authors, we divided the word counts by the number of pairings between authors in both papers, assigning each word to each author-pair (i.e., a count of 1 nn if a paper with n authors cites a paper with n authors).", "labels": [], "entities": []}, {"text": "Due to the large number of authors, we only used the 500 most cited authors (within the corpus) who have published at least 5 papers.", "labels": [], "entities": []}, {"text": "Papers with no authors left are removed from the dataset.", "labels": [], "entities": []}, {"text": "As a result, we have 8,144 papers containing 80,776 citation sentences (31,659 citation pairs).", "labels": [], "entities": []}, {"text": "After text processing, there are 391,711 tokens and 3,037 word types.", "labels": [], "entities": []}, {"text": "In each iteration of the EM algorithm, we run the E-step Gibbs sampler for 300 iterations, discarding the first 100 samples for burn-in and collecting samples at every 3rd iteration to avoid autocorrelation.", "labels": [], "entities": []}, {"text": "At the M-step, we update our \u03b7 and \u03b1 using the samples collected.", "labels": [], "entities": []}, {"text": "We run the model for 100 EM iterations.", "labels": [], "entities": []}, {"text": "We fixed \u03bb = 5, \u03b3 same = (0.5, 1) and \u03b3 diff = (1, 0.5).", "labels": [], "entities": []}, {"text": "Our setting of \u03b3 reflects our prior beliefs that coauthors tend to be from the same faction.", "labels": [], "entities": []}], "tableCaptions": []}