{"title": [{"text": "ISCAS: A Cascaded Approach for CIPS-SIGHAN Micro-Blog Word Segmentation Bakeoff 2012 Track", "labels": [], "entities": [{"text": "CIPS-SIGHAN Micro-Blog Word Segmentation Bakeoff 2012", "start_pos": 31, "end_pos": 84, "type": "TASK", "confidence": 0.6779986272255579}]}], "abstractContent": [{"text": "The state-of-the-art Chinese word segmentation systems have achieved high performance on well-formed long document.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.6750073432922363}]}, {"text": "However, the segmentation for microblog is difficult due to the noise problem and the OOV problem.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9789258241653442}, {"text": "OOV", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.8769489526748657}]}, {"text": "In this paper, we present a Chinese Micro-Blog Segmentation system for the CIP-SIGHAN Word Segmentation Bakeoff 2012 track.", "labels": [], "entities": [{"text": "CIP-SIGHAN Word Segmentation Bakeoff 2012 track", "start_pos": 75, "end_pos": 122, "type": "DATASET", "confidence": 0.735560933748881}]}, {"text": "The proposed system adopts a cascaded approach which contains three steps, correspondingly the preprocessing, the word segmentation and the post-processing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.7373962104320526}]}, {"text": "In the preprocessing step, the noise which contains the special characters is processed and removed.", "labels": [], "entities": []}, {"text": "The remaining sentences are segmented in the second step.", "labels": [], "entities": []}, {"text": "Finally, we use the dictionary to detect the OOVs which are not correctly segmented.", "labels": [], "entities": []}, {"text": "The results show the competitive performance of our approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, Chinese word segmentation (CWS) has a large of progress on statistical methods ().", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.7650109926859537}]}, {"text": "For instance, character-based tagging method) achieves great success in the second International Chinese word segmentation).", "labels": [], "entities": [{"text": "character-based tagging", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.6427559405565262}, {"text": "International Chinese word segmentation", "start_pos": 83, "end_pos": 122, "type": "TASK", "confidence": 0.7992288768291473}]}, {"text": "And the state-of-the-art CRF-based systems have achieved great performance using the closed train set and test set.", "labels": [], "entities": []}, {"text": "However, the segmentation performance on the web document or on the open set is still low).", "labels": [], "entities": [{"text": "segmentation", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9664450883865356}]}, {"text": "Specifically, generated by different kinds of users in the daily life, the micro blogs are noisy and full of OOV ().", "labels": [], "entities": [{"text": "OOV", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9917863011360168}]}, {"text": "For example, for the brevity and the significance of labels, there are lots of emotion labels, URLs, abbreviations and special characters in the micro-blogs.", "labels": [], "entities": []}, {"text": "Otherwise, due to the social property of the micro blogs, there are lots of OOVs (including names of users, stars, locations and organizations), which make it a challenge task for the segmentation of micro blogs.", "labels": [], "entities": [{"text": "OOVs", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9452303051948547}]}, {"text": "In this paper, we propose a cascaded approach of Micro-Blog segmentation.", "labels": [], "entities": [{"text": "Micro-Blog segmentation", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.7705178260803223}]}, {"text": "Firstly, we use regex expressions to recognize the URLs, English words and Numbers.", "labels": [], "entities": []}, {"text": "Some special characters and punctuations are used to split the sentence into pieces.", "labels": [], "entities": []}, {"text": "Secondly, the generated components of the sentences are partitioned into smaller pieces which comprise the preliminary result using a segmentation system.", "labels": [], "entities": []}, {"text": "Finally, we leverage quantities of dictionaries of OOVs and idioms from the network to merge the words in order to handle the words which are segmented incorrectly.", "labels": [], "entities": []}, {"text": "Our system's final F1 score on the test set is 92.73%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9784354567527771}]}, {"text": "In the rest of this paper, the models and the method used in our tasks are presented in section 2.", "labels": [], "entities": []}, {"text": "The experiments and the results are described in section 3.", "labels": [], "entities": []}, {"text": "We will discuss the method in section 4.", "labels": [], "entities": []}, {"text": "Finally, we give the conclusions and make prospect in the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the CIPS-SIGHAN track, the train data set consists of 503 sentences.", "labels": [], "entities": [{"text": "CIPS-SIGHAN track", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.9413828551769257}]}, {"text": "And we mainly do experiments on train data set for evaluating the performance of our tokenizer because of the test data set has not been published.", "labels": [], "entities": []}, {"text": "There are three evaluation metrics used in this bake-off task: Precision (P), Recall (R) and F1, where F1 is calculated as 2RP/(R+P).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.946113184094429}, {"text": "Recall (R)", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9481359720230103}, {"text": "F1", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9983478784561157}, {"text": "F1", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9986079335212708}]}, {"text": "In this section, we evaluate our methods and discuss the result of each step.", "labels": [], "entities": []}, {"text": "In this task of micro blogs, our final results are showed in.", "labels": [], "entities": []}, {"text": "\"CS\" denotes the number of the correct sentences; \"PCS\" denotes the percentage of the correct sentences.", "labels": [], "entities": [{"text": "PCS\"", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9820611774921417}]}, {"text": "The first row is our result and the second row is the best result in this task.", "labels": [], "entities": []}, {"text": "indicates that our result (0.9273) of the test set is worse than our result on the train set (0.9341).", "labels": [], "entities": []}, {"text": "We believe this is because the resources are not sufficient for the test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results with and without preprocessing", "labels": [], "entities": []}, {"text": " Table 4: Results of the three tokenizers", "labels": [], "entities": [{"text": "tokenizers", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.9318758249282837}]}, {"text": " Table 5. \"CS\" denotes the number of the  correct sentences; \"PCS\" denotes the percentage of  the correct sentences. The first row is our result  and the second row is the best result in this task.", "labels": [], "entities": [{"text": "PCS\"", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9820612072944641}]}, {"text": " Table 5: Final Result of the Test Set", "labels": [], "entities": []}]}