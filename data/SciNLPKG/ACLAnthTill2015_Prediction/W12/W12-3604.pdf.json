{"title": [{"text": "Exploiting naive vs expert discourse annotations: an experiment using lexical cohesion to predict Elaboration / Entity-Elaboration confusions", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper brings a contribution to the field of discourse annotation of corpora.", "labels": [], "entities": [{"text": "discourse annotation of corpora", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.8083550781011581}]}, {"text": "Using ANN-ODIS, a french corpus annotated with discourse relations by naive and expert annota-tors, we focus on two of them, Elaboration and Entity-Elaboration.", "labels": [], "entities": []}, {"text": "These two very frequent relations are (a) often confused by naive annotators (b) difficult to detect automatically as their signalling is poorly studied.", "labels": [], "entities": []}, {"text": "We propose to use lexical cohesion to differentiate between them, and show that Elaboration is more cohesive than Entity-Elaboration.", "labels": [], "entities": []}, {"text": "We then integrate lexical cohesion cues in a classification experiment, obtaining highly satisfying results.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper brings a contribution to the field of corpus annotation at the discourse level.", "labels": [], "entities": [{"text": "corpus annotation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.6940732449293137}]}, {"text": "Discourse structure is based on coherence links existing between discourse units.", "labels": [], "entities": []}, {"text": "These links can be captured using the notion of discourse relations.", "labels": [], "entities": []}, {"text": "Handling and detecting elements of discourse structure is very challenging for Natural Language Processing.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.6423965692520142}]}, {"text": "Applications such as natural language generation (), automatic summarization), among others, could take advantage of discourse level information detection.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.6658578515052795}, {"text": "summarization", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.785081148147583}, {"text": "discourse level information detection", "start_pos": 117, "end_pos": 154, "type": "TASK", "confidence": 0.6416207551956177}]}, {"text": "In the current state of research, providing reliably annotated corpora at the discourse level is really groundbreaking and opens new possibilities of investigation in discourse studies.", "labels": [], "entities": []}, {"text": "The ANNODIS project) will provide the scientific community with access to such a corpus for French (see section 2).", "labels": [], "entities": [{"text": "ANNODIS project", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8537255823612213}]}, {"text": "It is the first ressource in French annotated with discourse relations.", "labels": [], "entities": []}, {"text": "Similar corpora have already been developped for English, including the Penn Discourse TreeBank (, the RST Tree Bank () or the Discor corpus (.", "labels": [], "entities": [{"text": "Penn Discourse TreeBank", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.9659436146418253}, {"text": "RST Tree Bank", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.9588832060496012}, {"text": "Discor corpus", "start_pos": 127, "end_pos": 140, "type": "DATASET", "confidence": 0.78750941157341}]}, {"text": "But ANN-ODIS has distinct characteristics.", "labels": [], "entities": [{"text": "ANN-ODIS", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.6939548254013062}]}, {"text": "For our concern, the main difference is the two-level annotation: first a pre-annotation done by naive annotators (called \"naive annotation\") and then a revised annotation done by expert annotators (called \"expert annotation\").", "labels": [], "entities": []}, {"text": "This allows investigation on the whole process of annotation.", "labels": [], "entities": []}, {"text": "In this paper, we focus on Elaboration and EntityElaboration, the two most frequent and frequently confused relations (see section 3).", "labels": [], "entities": []}, {"text": "We propose anew approach based on lexical cohesion cues to differentiate between these relations, and show its reliability using expert annotation (see section 4).", "labels": [], "entities": []}, {"text": "We integrate this approach in a machine learning experiment and highlight the improvement it brings (see section 5).", "labels": [], "entities": []}, {"text": "We show how the obtained classifier can be used to automatically improve naive annotation or to reduce the experts' workload.", "labels": [], "entities": []}], "datasetContent": [{"text": "From the ANNODIS corpus, we extracted all Elaboration and E-Elaboration relations according to the naive annotation.", "labels": [], "entities": [{"text": "ANNODIS corpus", "start_pos": 9, "end_pos": 23, "type": "DATASET", "confidence": 0.8703625798225403}]}, {"text": "We restricted this subset to relations having an Elaboration or E-Elaboration annotation in the expert annotation.", "labels": [], "entities": []}, {"text": "Indeed, we only defined cues for these two relations; considering other relations would require specifying markers for them.", "labels": [], "entities": []}, {"text": "Then, for each < S a , Sb > couple, we computed the attributes listed in.", "labels": [], "entities": []}, {"text": "Thus, we considered: \u2022 lexical cohesion cues described in section 4.2 (N and Sc); \u2022 linguistic features presented in section 3.3 (rel, app, ger and bra): these features were detected using patterns based on the part-of-speech annotation of the segments; \u2022 structural features regarding the two segments: is Sb embedded in S a ? (emb) How many words are therein the two segments?", "labels": [], "entities": []}, {"text": "(w Sa , w Sb and w tot ) Are they simple segments or complex segments?", "labels": [], "entities": []}, {"text": "(s Sa , s Sb and s tot ).", "labels": [], "entities": []}, {"text": "We then processed the data produced using the machine learning software Weka ().", "labels": [], "entities": []}, {"text": "More specifically, we used Weka's implementation of the Random Forest classifier).", "labels": [], "entities": [{"text": "Random Forest classifier", "start_pos": 56, "end_pos": 80, "type": "DATASET", "confidence": 0.8714867830276489}]}, {"text": "In the following sections, we present our results (section 5.2) and discuss the way they could be exploited in an annotation campaign (section 5.3).", "labels": [], "entities": []}, {"text": "In order to evaluate the impact of the different attributes used in the classifier (see), we repeated the classification experiment, using a single attributes category at a time.", "labels": [], "entities": []}, {"text": "The results are summarized in  0.3% gain.", "labels": [], "entities": []}, {"text": "As expected, lexical cohesion cues bring a noticeable improvement (+2.9%).", "labels": [], "entities": []}, {"text": "Moreover, this improvement is stronger than the one brought by all linguistic features combined (+2.3%).", "labels": [], "entities": []}, {"text": "This confirms the importance of lexical cohesion to differentiate between Elaboration and E-Elaboration.", "labels": [], "entities": []}, {"text": "The synergy between the attributes categories is highlighted by the gain brought by the combination of all attributes, significantly higher than the sum of individual gains.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Expert annotations for E-Elaborations and Elab- orations in naive annotation", "labels": [], "entities": []}, {"text": " Table 2: Comparison between Elaboration and E- Elaboration lexical cohesion", "labels": [], "entities": []}, {"text": " Table 4: Confusion matrix for naive annotation", "labels": [], "entities": []}, {"text": " Table 5: Confusion matrix for naive-aided automatic an- notation", "labels": [], "entities": [{"text": "naive-aided automatic an- notation", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.601972234249115}]}, {"text": " Table 7.  Thus, only 8.2% of the accepted annotations are er-", "labels": [], "entities": []}, {"text": " Table 8: Confusion matrix for naive e-elab second-look  setup", "labels": [], "entities": []}]}