{"title": [{"text": "WFST-based Grapheme-to-Phoneme Conversion: Open Source Tools for Alignment, Model-Building and Decoding", "labels": [], "entities": [{"text": "WFST-based Grapheme-to-Phoneme Conversion", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7582957545916239}]}], "abstractContent": [{"text": "This paper introduces anew open source, WFST-based toolkit for Grapheme-to-Phoneme conversion.", "labels": [], "entities": [{"text": "Grapheme-to-Phoneme conversion", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.7997637093067169}]}, {"text": "The toolkit is efficient, accurate and currently supports a range of features including EM sequence alignment and several decoding techniques novel in the context of G2P.", "labels": [], "entities": [{"text": "EM sequence alignment", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.6995585362116495}]}, {"text": "Experimental results show that a combination RNNLM system outperforms all previous reported results on several standard G2P test sets.", "labels": [], "entities": [{"text": "G2P test sets", "start_pos": 120, "end_pos": 133, "type": "DATASET", "confidence": 0.7236772179603577}]}, {"text": "Preliminary experiments applying Lattice Minimum Bayes-Risk decoding to G2P conversion are also provided.", "labels": [], "entities": []}, {"text": "The toolkit is implemented using OpenFst.", "labels": [], "entities": [{"text": "OpenFst", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9483916163444519}]}], "introductionContent": [{"text": "Grapheme-to-Phoneme (G2P) conversion is an important problem related to Natural Language Processing, Speech Recognition and Spoken Dialog Systems development.", "labels": [], "entities": [{"text": "Grapheme-to-Phoneme (G2P) conversion", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5819901645183563}, {"text": "Speech Recognition", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7927394211292267}, {"text": "Spoken Dialog Systems development", "start_pos": 124, "end_pos": 157, "type": "TASK", "confidence": 0.8892632275819778}]}, {"text": "The primary goal of G2P conversion is to accurately predict the pronunciation of a novel input word given only the spelling.", "labels": [], "entities": [{"text": "G2P conversion", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.7595793306827545}]}, {"text": "For example, we would like to be able to predict, PHOENIX \u2192 /f in I k s/ given only the input spelling and a G2P model or set of rules.", "labels": [], "entities": [{"text": "PHOENIX", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9741727113723755}]}, {"text": "This problem is straightforward for some languages like Spanish or Italian, where pronunciation rules are consistent.", "labels": [], "entities": []}, {"text": "For languages like English and French however, inconsistent conventions make the problem much more challenging.", "labels": [], "entities": []}, {"text": "In this paper we present a fully data-driven, state-of-the-art, open-source toolkit for G2P conversion, Phonetisaurus.", "labels": [], "entities": [{"text": "G2P conversion", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.7443786859512329}]}, {"text": "It includes a novel modified Expectation-Maximization (EM)-driven G2P sequence alignment algorithm, support for jointsequence language models, and several decoding solutions.", "labels": [], "entities": [{"text": "G2P sequence alignment", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.5119430621465048}]}, {"text": "The paper also provides preliminary investigations of the applicability of Lattice Minimum Bayes-Risk (LMBR) decoding and Nbest rescoring with a Recurrent Neural Network Language Model (RNNLM) to G2P conversion.", "labels": [], "entities": [{"text": "Lattice Minimum Bayes-Risk (LMBR", "start_pos": 75, "end_pos": 107, "type": "METRIC", "confidence": 0.8025169372558594}]}, {"text": "The Weighted Finite-State Transducer (WFST) framework is used throughout, and the open source implementation relies on OpenFst.", "labels": [], "entities": [{"text": "Weighted Finite-State Transducer (WFST)", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.6787602404753367}, {"text": "OpenFst", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9459808468818665}]}, {"text": "Experimental results are provided illustrating the speed and accuracy of the proposed system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9990615248680115}]}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides background, Section 3 outlines the alignment approach, Section 4 describes the joint-sequence LM.", "labels": [], "entities": [{"text": "alignment", "start_pos": 54, "end_pos": 63, "type": "TASK", "confidence": 0.9766517281532288}, {"text": "joint-sequence LM", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.5129943042993546}]}, {"text": "Section 5 describes decoding approaches.", "labels": [], "entities": []}, {"text": "Section 6 discusses preliminary experiments, Section 7 provides simple usage commands and Section 8 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experimental evaluations were conducted utilizing three standard G2P test sets.", "labels": [], "entities": [{"text": "G2P test sets", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.7977084219455719}]}, {"text": "These included replications of the NetTalk, CMUdict, and OALD English language dictionary evaluations described in detail in.", "labels": [], "entities": [{"text": "NetTalk", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.952170729637146}, {"text": "OALD English language dictionary evaluations", "start_pos": 57, "end_pos": 101, "type": "DATASET", "confidence": 0.7908405661582947}]}, {"text": "Results comparing various configuration of the proposed toolkit to the joint sequence model Sequitur and an alternative discriminative training toolkit direcTL+ are described in Table 1.", "labels": [], "entities": []}, {"text": "Here m2m-P indicates the proposed toolkit using the alignment algorithm from, m2m-fst-P indicates the alternative FST-based alignment algorithm, and rnnlm-P indicates the use of RNNLM Nbest reranking.", "labels": [], "entities": [{"text": "FST-based alignment", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.61321160197258}]}], "tableCaptions": [{"text": " Table 1: Comparison of G2P WA(%) for previous sys- tems and variations of the proposed toolkit.", "labels": [], "entities": []}, {"text": " Table 3: LMBR decoding Word Accuracy (WA) and  Phoneme Accuracy (PA) for order N=1-6.", "labels": [], "entities": [{"text": "Phoneme Accuracy (PA)", "start_pos": 48, "end_pos": 69, "type": "METRIC", "confidence": 0.7137225508689881}]}]}