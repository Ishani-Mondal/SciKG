{"title": [], "abstractContent": [{"text": "In this paper, we look at the problem of robust detection of a very productive class of Asian style emoticons, known as facemarks or kao-moji.", "labels": [], "entities": []}, {"text": "We demonstrate the frequency and productivity of these sequences in social media such as Twitter.", "labels": [], "entities": []}, {"text": "Previous approaches to detection and analysis of kaomoji have placed limits on the range of phenomena that could be detected with their method, and have looked at largely monolingual evaluation sets (e.g., Japanese blogs).", "labels": [], "entities": [{"text": "detection and analysis of kaomoji", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.7858279466629028}]}, {"text": "We find that these emoticons occur broadly in many languages, hence our approach is language agnostic.", "labels": [], "entities": []}, {"text": "Rather than relying on regular expressions over a prede-fined set of likely tokens, we build weighted context-free grammars that reward graphical affinity and symmetry within whatever symbols are used to construct the emoticon.", "labels": [], "entities": []}], "introductionContent": [{"text": "Informal text genres, such as email, SMS or social media messages, lack some of the modes used in spoken language to communicate affect -prosody or laughter, for example.", "labels": [], "entities": []}, {"text": "Affect can be provided within such genres through the use of text formatting (e.g., capitalization for emphasis) or through the use of extra-linguistic sequences such as the widely used smiling, winking ;) emoticon.", "labels": [], "entities": [{"text": "Affect", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8932912945747375}]}, {"text": "These sorts of vertical face representations via ASCII punctuation sequences are widely used in European languages, but in Asian informal text genres another class of emoticons is popular, involving a broader symbol set and with a horizontal facial orientation.", "labels": [], "entities": []}, {"text": "These go by the name of facemarks or kaomoji.", "labels": [], "entities": []}, {"text": "(\u00b4\ud97b\udf59\u03c9\ud97b\udf59`) 57825858454433792 <--uses a fun swirly Tamil This class of emoticon is far more varied and productive than the sideways European style emoticons, and even lists of on the order often thousand emoticons will fail to coverall instances in even a modest sized sample of text.", "labels": [], "entities": []}, {"text": "This relative productivity is due to several factors, including the horizontal orientation, which allows for more flexibility in configuring features both within the face and surrounding the face (e.g., arms) than the vertical orientation.", "labels": [], "entities": []}, {"text": "Another important factor underlying kaomoji productivity is historical in nature.", "labels": [], "entities": []}, {"text": "kaomoji were developed and popularized in Japan and other Asian countries whose scripts have always required multibyte character encodings, and whose users of electronic communication systems have significant experience working with characters beyond those found in the standard ASCII set.", "labels": [], "entities": []}, {"text": "Linguistic symbols from various scripts can be appropriated into the kaomoji for their resemblence to facial features, such as a winking eye, and authors of kaomoji sometimes use advanced Unicode techniques to decorate glyphs with elaborate combinations of diacritic marks.", "labels": [], "entities": []}, {"text": "For example, the kao-moji in the top righthand corner of, includes an Arabic letter, and Thai vowel diacritics.", "labels": [], "entities": []}, {"text": "Accurate detection of these tokens -and other common sequences of extra-linguistic symbol sequences -is important for normalization of social media text for downstream applications.", "labels": [], "entities": [{"text": "normalization of social media text", "start_pos": 118, "end_pos": 152, "type": "TASK", "confidence": 0.8468214511871338}]}, {"text": "At the most basic level, the complex and unpredictable combinations of characters found within many kaomoji (often including punctuation and whitespace, as well as irregularly-used Unicode combining characters) can seriously confound sentence and word segmentation algorithms that attempt to operate on kaomoji-rich text; since segmentation is typically the first step in any text processing pipeline, issues here can cause a wide variety of problems downstream.", "labels": [], "entities": [{"text": "sentence and word segmentation", "start_pos": 234, "end_pos": 264, "type": "TASK", "confidence": 0.7212903946638107}]}, {"text": "Accurately removing or normalizing such sequences before attempting segmentation can ensure that existing NLP tools are able to effectively work with and analyze kaomojiincluding text.", "labels": [], "entities": []}, {"text": "At a higher level, the inclusion of a particular kaomoji in a text represents a conscious decision on the part of the text's author, and fully interpreting the text necessarily involves a degree of interpretation of the kaomoji that they chose to include.", "labels": [], "entities": []}, {"text": "European-style emoticons form a relatively closed set and are often fairly straightforward to interpret (both in terms of computational, as well as human, effort); kaomoji, on the other hand, are far more diverse, and interpretation is rarely simple.", "labels": [], "entities": [{"text": "interpretation", "start_pos": 218, "end_pos": 232, "type": "TASK", "confidence": 0.9652321934700012}]}, {"text": "In this paper, we present preliminary work on defining robust models for detecting kaomoji in social media text.", "labels": [], "entities": [{"text": "detecting kaomoji in social media text", "start_pos": 73, "end_pos": 111, "type": "TASK", "confidence": 0.728262335062027}]}, {"text": "Prior work on detecting and classifying these extra-linguistic sequences has relied on the presence of fixed attested patterns (see discussion in Section 2) for detection, and regular expressions for segmentation.", "labels": [], "entities": []}, {"text": "While such approaches can capture the most common kaomoji and simple variants of them, the productive and creative nature of the phenomenon results in a non-negligible out-ofvocabulary problem.", "labels": [], "entities": []}, {"text": "In this paper, we approach the problem by examining a broader class of possible sequences (see Section 4.2) for symmetry using a robust probabilistic context-free grammar with rule probabilities proportional to the symmetry or affinity of matched terminal items in the rule.", "labels": [], "entities": []}, {"text": "Our PCFG is robust in the sense that every candidate sequence is guaranteed to have a valid parse.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8924094438552856}]}, {"text": "We use the resulting Viterbi best parse to provide a score to the candidate sequence -reranking our high recall list to achieve, via thresholds, high precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9914658069610596}, {"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9976170659065247}]}, {"text": "In addition, we investigate unsupervised model adaptation, by incorporating Viterbi-best parses from a small set of attested kaomoji scraped from websites; and inducing grammars with a larger non-terminal set corresponding to regions of the face.", "labels": [], "entities": []}, {"text": "We present bootstrapping experiments for deriving highly functional, language independent models for detecting kaomoji in text, on multilingual Twitter data.", "labels": [], "entities": []}, {"text": "Our approach can be used as part of a stand-alone detection model, or as input into semiautomatic kaomoji lexicon development.", "labels": [], "entities": []}, {"text": "Before describing our approach, we will first present prior work on this class of emoticon.", "labels": [], "entities": []}], "datasetContent": [{"text": "Using the candidate extraction methodology described in section 4.2, we extracted 1.6 million distinct candidates from our corpus of 80 million Twitter messages (candidates often appeared in multiple messages).", "labels": [], "entities": [{"text": "candidate extraction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8052173852920532}]}, {"text": "These candidates included genuine emoticons, as well as extended strings of punctuation and other \"noisy\" chunks of text.", "labels": [], "entities": []}, {"text": "Genuine kaomoji were often picked up with some amount of leading or trailing punctuation, for example: \" ..\\(\u00b4\ud97b\udf59`)/ \"; other times, kaomoji beginning with linguistic characters were truncated: (^\u02db*)\u0283 . We provided these candidates to our parser under four different conditions, each one producing 1.5 million parse trees: the single non-terminal approach described in section 4.3 or the enhanced multiple non-terminal approach described in section 4.4, both with and without training via the Maximum A Posteriori approach described in section 4.4.", "labels": [], "entities": []}, {"text": "Using the weighted-inside-score method described in section 4.3, we produced a ranked list of candidate emoticons from each condition's output.", "labels": [], "entities": []}, {"text": "\"Well-scoring\" candidates were ones for which the parser was able to construct a low-cost parse.", "labels": [], "entities": []}, {"text": "We evaluated our approach in two ways.", "labels": [], "entities": []}, {"text": "The first way examined precision-how many of the bestscoring candidate sequences actually contained kaomoji?", "labels": [], "entities": [{"text": "precision-how", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9939464926719666}]}, {"text": "Manually reviewing all 1.6 million candidates was not feasible, so we evaluated this aspect of our system's performance on a small subset of its output.", "labels": [], "entities": []}, {"text": "Computational considerations forced us to process our large corpus in parallel, meaning that our set of 1.6 million candidate kaomoji was already partitioned into 160 sets of \u224810,000 candidates each.", "labels": [], "entities": []}, {"text": "We manually reviewed the top 1,000 sorted results from one of these partitions, and flagged any entries that did not contain or consist of a face-like kaomoji.", "labels": [], "entities": []}, {"text": "The results of each condition are presented in table 3.", "labels": [], "entities": []}, {"text": "The second evaluation approach we will examine looks at how our method compares with the trigram-based approach described by (as described by).", "labels": [], "entities": []}, {"text": "We trained both smoothed and unsmoothed language models 5 on the \"just faces\" sub-corpus used for the A Posteriori grammar enhancement, and computed perplexity measurements for the same set \u224810,000 candidates used previously.", "labels": [], "entities": [{"text": "A Posteriori grammar enhancement", "start_pos": 102, "end_pos": 134, "type": "TASK", "confidence": 0.6267716586589813}]}, {"text": "presents these results; clearly, a smoothed trigram model can achieve good results.", "labels": [], "entities": []}, {"text": "The unsmoothed model at first glance seems to have performed very well; note, however, that only approximately 600 (out of nearly 10,000) candidates were \"matched\" by the unsmoothed model (i.e., they did not contain any OOV symbols and therefore had finite perplexity scores), yielding a very small but high-precision set of emoticons.", "labels": [], "entities": []}, {"text": "Looking at precision, the model-based approaches outperformed our grammar approach.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9974627494812012}]}, {"text": "It  should be noted, however, that the trigram approach was much less tolerant of certain non-standard formulations involving novel characters or irregular formulations ( (\u02d8!\u02d8) are examples of kaomoji that our grammar-based approach ranked more highly than did the trigram approach).", "labels": [], "entities": []}, {"text": "The two approaches also had different failure profiles.", "labels": [], "entities": []}, {"text": "The grammar approach's false positives tended to be symmetrical sequences of punctuation, whereas the language models' were more variable.", "labels": [], "entities": []}, {"text": "Were we to review a larger selection of candidates, we believe that the structure-capturing nature of the grammar approach would enable it to outperform the more simplistic approach.", "labels": [], "entities": []}, {"text": "We also attempted a hybrid \"mixed\" approach in which we used the language models to re-rank the top 1,000 \"best\" candidates from our parser's output.", "labels": [], "entities": []}, {"text": "This generally resulted in improved performance, and for some conditions the improvement was substantial.", "labels": [], "entities": []}, {"text": "Future work will explore this approach in greater detail and over larger amounts of data.", "labels": [], "entities": []}], "tableCaptions": []}