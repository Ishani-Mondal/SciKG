{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 281-288, NAIST at the HOO 2012 Shared Task", "labels": [], "entities": [{"text": "NAIST at the HOO 2012 Shared Task", "start_pos": 100, "end_pos": 133, "type": "DATASET", "confidence": 0.9174373405320304}]}], "abstractContent": [{"text": "This paper describes the Nara Institute of Science and Technology (NAIST) error correction system in the Helping Our Own (HOO) 2012 Shared Task.", "labels": [], "entities": [{"text": "Nara Institute of Science and Technology (NAIST) error correction", "start_pos": 25, "end_pos": 90, "type": "TASK", "confidence": 0.891391163522547}]}, {"text": "Our system targets preposition and determiner errors with spelling correction as a pre-processing step.", "labels": [], "entities": []}, {"text": "The result shows that spelling correction improves the Detection, Correction, and Recognition F-scores for preposition errors.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.5488542914390564}, {"text": "Detection", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9818302989006042}, {"text": "Correction", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.8172783851623535}, {"text": "Recognition F-scores", "start_pos": 82, "end_pos": 102, "type": "METRIC", "confidence": 0.912041038274765}]}, {"text": "With regard to preposition error correction, F-scores were not improved when using the training set with correction of all but preposition errors.", "labels": [], "entities": [{"text": "preposition error correction", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.6537551979223887}, {"text": "F-scores", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9986525177955627}]}, {"text": "As for determiner error correction, there was an improvement when the constituent parser was trained with a concatenation of treebank and modified treebank where all the articles appearing as the first word of an NP were removed.", "labels": [], "entities": [{"text": "determiner error correction", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.8088928858439127}]}, {"text": "Our system ranked third in preposition and fourth in determiner error corrections.", "labels": [], "entities": [{"text": "determiner error corrections", "start_pos": 53, "end_pos": 81, "type": "METRIC", "confidence": 0.7133355538050333}]}], "introductionContent": [{"text": "Researchers in natural language processing have focused recently on automatic grammatical error detection and correction for English as a Second Language (ESL) learners' writing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6705687840779623}, {"text": "automatic grammatical error detection", "start_pos": 68, "end_pos": 105, "type": "TASK", "confidence": 0.5716905668377876}, {"text": "English as a Second Language (ESL) learners' writing", "start_pos": 125, "end_pos": 177, "type": "TASK", "confidence": 0.6510108470916748}]}, {"text": "There have been a lot of papers on these challenging tasks, and remarkably, an independent session for grammatical error correction took place in the ACL-2011.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 103, "end_pos": 131, "type": "TASK", "confidence": 0.7865166266759237}, {"text": "ACL-2011", "start_pos": 150, "end_pos": 158, "type": "DATASET", "confidence": 0.5283434987068176}]}, {"text": "The Helping Our Own (HOO) shared task) is proposed for improving the quality of ESL learners' writing, and a pilot run with six teams was held in 2011.", "labels": [], "entities": [{"text": "ESL learners' writing", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.6861296693483988}]}, {"text": "The HOO 2012 shared task focuses on the correction of preposition and determiner errors.", "labels": [], "entities": [{"text": "HOO 2012 shared task", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7779760509729385}, {"text": "correction of preposition and determiner errors", "start_pos": 40, "end_pos": 87, "type": "TASK", "confidence": 0.6995844741662344}]}, {"text": "There has been a lot of work on correcting preposition and determiner errors, where discriminative models such as Maximum Entropy and Averaged Perceptron) and/or probablistic language models are generally used.", "labels": [], "entities": [{"text": "correcting preposition and determiner", "start_pos": 32, "end_pos": 69, "type": "TASK", "confidence": 0.8307118564844131}]}, {"text": "In addition, it is pointed out that spelling and punctuation errors often disturb grammatical error correction.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 82, "end_pos": 110, "type": "TASK", "confidence": 0.6531628668308258}]}, {"text": "In fact, some teams reported in the HOO 2011 that they corrected spelling and punctuation errors before correcting grammatical errors).", "labels": [], "entities": [{"text": "HOO 2011", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.7776542603969574}]}, {"text": "Our strategy for HOO 2012 follows the above procedure.", "labels": [], "entities": [{"text": "HOO 2012", "start_pos": 17, "end_pos": 25, "type": "TASK", "confidence": 0.8778290450572968}]}, {"text": "In other words, we correct spelling errors at the beginning, and then train classifiers for correcting preposition and determiner errors.", "labels": [], "entities": []}, {"text": "The result shows our system achieved 24.42% (thirdranked) in F-score for preposition error correction, 29.81% (fourth-ranked) for determiners, and 27.12% (fourth-ranked) for their combined.", "labels": [], "entities": [{"text": "F-score", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9993321299552917}, {"text": "preposition error correction", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.5744663377602895}]}, {"text": "In this report, we describe our system architecture and the experimental results.", "labels": [], "entities": []}, {"text": "Sections 2 to 4 describe the system for correcting spelling, preposition, and determiner errors.", "labels": [], "entities": [{"text": "correcting spelling, preposition, and determiner errors", "start_pos": 40, "end_pos": 95, "type": "TASK", "confidence": 0.7625439614057541}]}, {"text": "Section 5 shows the experimental design and results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Previously undisclosed data extracted from the CLC-FCE dataset was provided as a test set by the HOO organizers.", "labels": [], "entities": [{"text": "CLC-FCE dataset", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9500167667865753}, {"text": "HOO", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.6984765529632568}]}, {"text": "The test set includes 100 essays and each contains 180.1 word tokens on average.", "labels": [], "entities": []}, {"text": "We defined eight distinct configurations based on our subsystem parameters.", "labels": [], "entities": []}, {"text": "The official task evaluation uses three metrics (Detection, Recognition, and Correction), and three measures Precision, Recall, and F-score were computed    each metric.", "labels": [], "entities": [{"text": "Detection", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9579907655715942}, {"text": "Correction", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9784483909606934}, {"text": "Precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9984509944915771}, {"text": "Recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9644954800605774}, {"text": "F-score", "start_pos": 132, "end_pos": 139, "type": "METRIC", "confidence": 0.9993249177932739}]}, {"text": "to show the overall results of our systems.", "labels": [], "entities": []}, {"text": "In terms of the effect of pre-processing, spelling correction improved the F-score of Detection, Correction, and Recognition for preposition errors after revision, whereas there were fluctuations in other conditions.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8328234851360321}, {"text": "F-score", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9993084669113159}, {"text": "Detection", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9344639182090759}, {"text": "Correction", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.6205283403396606}, {"text": "Recognition", "start_pos": 113, "end_pos": 124, "type": "METRIC", "confidence": 0.7126784920692444}]}, {"text": "This maybe because there were a few spelling errors corrected in the test set.", "labels": [], "entities": [{"text": "spelling errors", "start_pos": 36, "end_pos": 51, "type": "METRIC", "confidence": 0.9166478216648102}]}, {"text": "Another reason why no stable improvement was found in determiner error correction is because spelling correction often produces nouns that affect the determiner error detection and correction more sensitively than prepositions.", "labels": [], "entities": [{"text": "determiner error correction", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.8660598794619242}, {"text": "spelling correction", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8514829874038696}, {"text": "determiner error detection and correction", "start_pos": 150, "end_pos": 191, "type": "TASK", "confidence": 0.7685328722000122}]}, {"text": "For example, a misspelled word *freewho / free who was corrected as freezer.", "labels": [], "entities": []}, {"text": "This type of error may have increased false positives.", "labels": [], "entities": []}, {"text": "The example *National Filharmony / the National Philharmony was corrected as National Fleming, where the proper noun Fleming does not need a determiner and this type of error increased false negatives.", "labels": [], "entities": [{"text": "National Filharmony / the National Philharmony", "start_pos": 13, "end_pos": 59, "type": "DATASET", "confidence": 0.701973150173823}, {"text": "National Fleming", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.8810206055641174}]}, {"text": "As for preposition error correction, the classifier performed better when it was trained with the \"original\" set rather than the error-corrected (all but preposition errors) \"gold\" set.", "labels": [], "entities": [{"text": "preposition error correction", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.681217203537623}]}, {"text": "The reason for this is that the gold set is trained with the test set that contains correcttext.org/hoo2012/eval.html There was one spelling correction per document in average.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.5624452531337738}]}, {"text": "several types of errors which the original CLC-FCE dataset also contains.", "labels": [], "entities": [{"text": "CLC-FCE dataset", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.938303679227829}]}, {"text": "Therefore, the \"original\" classifier is more optimised and suitable for the test set than the \"gold\" one.", "labels": [], "entities": []}, {"text": "For determiner error correction, the \"mixed\" model improved precision and F-score in the additional experiments.", "labels": [], "entities": [{"text": "determiner error correction", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.7701329092184702}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9997233748435974}, {"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9992461204528809}]}], "tableCaptions": [{"text": " Table 4: Result for preposition and determiner errors combined before revisions.", "labels": [], "entities": [{"text": "Result", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9244933724403381}]}, {"text": " Table 5: Result for preposition errors before revisions.", "labels": [], "entities": []}, {"text": " Table 6: Result for determiner errors before revisions.", "labels": [], "entities": [{"text": "Result", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.7981404662132263}]}, {"text": " Table 7: Result for preposition and determiner errors combined after revisions.", "labels": [], "entities": [{"text": "Result", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9439510703086853}]}, {"text": " Table 8: Result for preposition errors after revisions.", "labels": [], "entities": []}, {"text": " Table 9: Result for determiner errors after revisions.", "labels": [], "entities": [{"text": "Result", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8325084447860718}, {"text": "determiner errors", "start_pos": 21, "end_pos": 38, "type": "METRIC", "confidence": 0.6550450623035431}]}, {"text": " Table 10: Result of additional experiments for determiner errors after revisions.", "labels": [], "entities": [{"text": "Result", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.8732611536979675}]}]}