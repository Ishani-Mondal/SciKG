{"title": [{"text": "Learning Preferences for Referring Expression Generation: Effects of Domain, Language and Algorithm", "labels": [], "entities": [{"text": "Referring Expression Generation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.9259507656097412}]}], "abstractContent": [{"text": "One important subtask of Referring Expression Generation (REG) algorithms is to select the attributes in a definite description fora given object.", "labels": [], "entities": [{"text": "Referring Expression Generation (REG)", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.8528753022352854}]}, {"text": "In this paper, we study how much training data is required for algorithms to do this properly.", "labels": [], "entities": []}, {"text": "We compare two REG algorithms in terms of their performance: the classic Incremental Algorithm and the more recent Graph algorithm.", "labels": [], "entities": []}, {"text": "Both rely on a notion of preferred attributes that can be learned from human descriptions.", "labels": [], "entities": []}, {"text": "In our experiments, preferences are learned from training sets that vary in size, in two domains and languages.", "labels": [], "entities": []}, {"text": "The results show that depending on the algorithm and the complexity of the domain, training on a handful of descriptions can already lead to a performance that is not significantly different from training on a much larger data set.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most practical NLG systems include a dedicated module for Referring Expression Generation (REG) in one form or another ().", "labels": [], "entities": [{"text": "Referring Expression Generation (REG)", "start_pos": 58, "end_pos": 95, "type": "TASK", "confidence": 0.8066014150778452}]}, {"text": "One central problem a REG module needs to address is deciding on the contents of a description.", "labels": [], "entities": []}, {"text": "Jordan and, for example, studied humanproduced descriptions in a furniture scenario, and found that speakers can refer to a target in many different ways (\"the yellow rug\", \"the $150 rug\", etc.).", "labels": [], "entities": []}, {"text": "The question, then, is how speakers decide which attributes to include in a description, and how this decision process can be modeled in a REG algorithm.", "labels": [], "entities": []}, {"text": "When we focus on the generation of distinguishing descriptions (which is often done in REG), it is usually assumed that some attributes are more preferred than others: when trying to identify a chair, for example, its colour is probably more helpful than its size.", "labels": [], "entities": [{"text": "generation of distinguishing descriptions", "start_pos": 21, "end_pos": 62, "type": "TASK", "confidence": 0.7969744503498077}]}, {"text": "It is precisely this intuition of preferred attributes which is incorporated in the Incremental Algorithm (, arguably one of the most influential REG algorithms to date.", "labels": [], "entities": []}, {"text": "The Incremental Algorithm (IA) assumes the existence of a complete, ordered list of preferred attributes.", "labels": [], "entities": [{"text": "Incremental Algorithm (IA)", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6511162757873535}]}, {"text": "The algorithm basically iterates through this list, adding an attribute (e.g., COLOUR) to the description under construction if its value (e.g., yellow) helps ruling out one or more of the remaining distractors.", "labels": [], "entities": [{"text": "COLOUR", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9956529140472412}]}, {"text": "Even though the IA is exceptional in that it relies on a complete ordering of attributes, most current REG algorithms make use of preferences in someway.", "labels": [], "entities": [{"text": "IA", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9740999937057495}]}, {"text": "The graph-based REG algorithm (, for example, models preferences in terms of costs, where cheaper is more preferred.", "labels": [], "entities": []}, {"text": "Contrary to the IA, the graph-based algorithm assumes that preferences operate at the level of attribute-value pairs (or properties) rather than at the level of attributes; in this way it becomes possible to prefer a straightforward size (large) over a subtle colour (mauve, taupe).", "labels": [], "entities": [{"text": "IA", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.5063523054122925}]}, {"text": "Moreover, the graphbased algorithm looks for the cheapest overall description, and may opt fora description with a single, relatively dispreferred property (\"the man with the blue eyes\") when the alternative would be to combine many, relatively preferred properties (\"the large, balding man with the bow tie and the striped tuxedo\").", "labels": [], "entities": []}, {"text": "This flexibility is arguably one of the reasons why the graph-based REG approach works well: it was the best performing system in the most recent REG Challenge (.", "labels": [], "entities": []}, {"text": "But where do the preferences used in the algorithms come from?", "labels": [], "entities": []}, {"text": "Dale and Reiter point out that preferences are domain dependent, and that determining them fora given domain is essentially an empirical question.", "labels": [], "entities": []}, {"text": "Unfortunately, they do not specify how this particular empirical question should be answered.", "labels": [], "entities": []}, {"text": "The general preference for colour oversize is experimentally well-established, but for most other cases experimental data are not readily available.", "labels": [], "entities": []}, {"text": "An alternative would be to look at human data, preferably in a \"semantically transparent\" corpus), that is: a corpus that contains the attributes and values of all domain objects, together with the attribute-value pairs actually included in a target reference.", "labels": [], "entities": []}, {"text": "Such corpora are typically collected using human participants, who are asked to produce referring expressions for targets in controlled visual scenes.", "labels": [], "entities": []}, {"text": "One example is the TUNA corpus, which is a publicly available data set containing 2280 human-produced descriptions in total, and which formed the basis of various REG Challenges.", "labels": [], "entities": [{"text": "TUNA corpus", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.906454473733902}, {"text": "REG Challenges", "start_pos": 163, "end_pos": 177, "type": "TASK", "confidence": 0.8621698021888733}]}, {"text": "Clearly, building a corpus such as TUNA is a time consuming and labour intensive exercise, so it will not be surprising that only a handful of such corpora exists (and often only for English).", "labels": [], "entities": []}, {"text": "This raises an important question: how many human-produced references are needed to make a good estimate of which attributes and properties are preferred?", "labels": [], "entities": []}, {"text": "Do we really need hundreds of instances, or is it conceivable that a few of them (collected in a semantically transparent way) will do?", "labels": [], "entities": []}, {"text": "This is not an easy matter, since various factors might play a role: from which data set are example references sampled, what are the domains of interest, and, perhaps most importantly, which REG algorithm is considered?", "labels": [], "entities": []}, {"text": "In this paper, we address these questions by systematically training two REG algorithms (the Incremental Algorithm and the graph-based REG algorithm) on sets of human-produced descriptions of increasing size and evaluating them on a held-out test set; we do this for two different domains (people and furniture descriptions) and two data sets in two different languages (TUNA and D-TUNA, the Dutch version of TUNA).", "labels": [], "entities": []}, {"text": "That size of the training set may have an impact on the performance of a REG algorithm was already suggested by, who used the English TUNA corpus to determine preferences (costs) for the graph-based algorithm using a similar learning curve set-up as we use here.", "labels": [], "entities": [{"text": "English TUNA corpus", "start_pos": 126, "end_pos": 145, "type": "DATASET", "confidence": 0.7252912521362305}]}, {"text": "However, the current paper expands on in three major ways.", "labels": [], "entities": []}, {"text": "Firstly, and most importantly, where Theune et al. reported results for only one algorithm (the graph-based one), we directly compare the performance of the graph-based algorithm and the Incremental Algorithm (something which, somewhat surprisingly, has not been done before).", "labels": [], "entities": []}, {"text": "Secondly, we test whether these algorithms perform differently in two different languages (English and Dutch), and thirdly, we use eight training set sizes, which is more than the six set sizes that were used by Theune et al.", "labels": [], "entities": []}, {"text": "Below we first explain in more detail which algorithms (Section 2) and corpora (Section 3) we used for our experiments.", "labels": [], "entities": []}, {"text": "Then we describe how we derived costs and orders from subsets of these corpora (Section 4), and report the results of our experiments focusing on effects of domain, language and size of the training set (Section 5).", "labels": [], "entities": []}, {"text": "We end with a discussion and conclusion (Section 6), where we also compare the performance of the IA trained on small set sizes with that of the classical Full Brevity and Greedy algorithms).", "labels": [], "entities": [{"text": "Brevity", "start_pos": 160, "end_pos": 167, "type": "METRIC", "confidence": 0.6892231702804565}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance for each set size in the furniture  domain. For sizes 1 to 50, means over five sets are given.  The full sets are 165 English and 160 Dutch descriptions.  Note that the scores of the IA for the English sets of sizes  1 to 30 were also reported in Theune et al. (2011).", "labels": [], "entities": [{"text": "IA", "start_pos": 205, "end_pos": 207, "type": "METRIC", "confidence": 0.8534111380577087}]}, {"text": " Table 3: Performance for each set size in the people do- main. For sizes 1 to 50, means over five sets are given.  The full sets are 136 English and 160 Dutch descriptions.  Note that the scores of the IA for the English sets of sizes  1 to 30 were also reported in Theune et al. (2011).", "labels": [], "entities": [{"text": "IA", "start_pos": 203, "end_pos": 205, "type": "METRIC", "confidence": 0.9015301465988159}]}]}