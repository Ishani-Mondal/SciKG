{"title": [{"text": "CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes", "labels": [], "entities": [{"text": "CoNLL-2012", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8238797783851624}, {"text": "Modeling Multilingual Unrestricted Coreference", "start_pos": 24, "end_pos": 70, "type": "TASK", "confidence": 0.7900915294885635}]}], "abstractContent": [{"text": "The CoNLL-2012 shared task involved predicting coreference in English, Chinese, and Arabic, using the final version, v5.0, of the OntoNotes corpus.", "labels": [], "entities": [{"text": "predicting coreference", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.7761274576187134}, {"text": "OntoNotes corpus", "start_pos": 130, "end_pos": 146, "type": "DATASET", "confidence": 0.896380215883255}]}, {"text": "It was a follow-on to the English-only task organized in 2011.", "labels": [], "entities": []}, {"text": "Until the creation of the OntoNotes corpus, resources in this sub-field of language processing were limited to noun phrase coreference, often on a restricted set of entities, such as the ACE entities.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.8184805512428284}, {"text": "noun phrase coreference", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.6463624934355418}]}, {"text": "OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types, and covers multiple languages.", "labels": [], "entities": []}, {"text": "OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8240494132041931}]}, {"text": "This paper describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information , evaluation criteria, and presents and discusses the results achieved by the participating systems.", "labels": [], "entities": []}, {"text": "The task of coreference has had a complex evaluation history.", "labels": [], "entities": [{"text": "coreference", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.9658222794532776}]}, {"text": "Potentially many evaluation conditions, have, in the past, made it difficult to judge the improvement in new algorithms over previously reported results.", "labels": [], "entities": []}, {"text": "Having a standard test set and standard evaluation parameters, all based on a resource that provides multiple integrated annotation layers (syntactic parses, semantic roles, word senses, named entities and coreference) and in multiple languages could support joint modeling and help ground and energize ongoing research in the task of entity and event coreference.", "labels": [], "entities": []}], "introductionContent": [{"text": "The importance of coreference resolution for the entity/event detection task, namely identifying all mentions of entities and events in text and clustering them into equivalence classes, has been well recognized in the natural language processing community.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.9251162707805634}, {"text": "entity/event detection task", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.7449718713760376}]}, {"text": "Early work on corpus-based coreference resolution dates back to the mid-90s by where they experimented with decision trees and hand-written rules.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.9115805923938751}]}, {"text": "Corpora to support supervised learning of this task date back to the Message Understanding Conferences (MUC).", "labels": [], "entities": [{"text": "Message Understanding Conferences (MUC)", "start_pos": 69, "end_pos": 108, "type": "TASK", "confidence": 0.8209266861279806}]}, {"text": "The de facto standard datasets for current coreference studies are the MUC and the ACE 1 () corpora.", "labels": [], "entities": [{"text": "MUC", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.9359018206596375}, {"text": "ACE 1 () corpora", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.8592601120471954}]}, {"text": "These corpora were tagged with coreferring entities in the form of noun phrases in the text.", "labels": [], "entities": []}, {"text": "The MUC corpora coverall noun phrases in text but are relatively small in size.", "labels": [], "entities": [{"text": "MUC corpora", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8168127536773682}]}, {"text": "The ACE corpora, on the other hand, cover much more data, but the annotation is restricted to a small subset of entities.", "labels": [], "entities": [{"text": "ACE corpora", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9161186218261719}]}, {"text": "Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it is a problem that requires world knowledge to solve and word knowledge is hard to define, and partly owing to the lack of substantial annotated data.", "labels": [], "entities": [{"text": "Automatic identification of coreferring entities and events in text", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.8633468084865146}]}, {"text": "Aside from the fact that resolving coreference in text is simply a very hard problem, there have been other hindrances that further contributed to the slow progress in this area: (i) Smaller sized corpora such as MUC which covered coreference across all noun phrases.", "labels": [], "entities": []}, {"text": "Corpora such as ACE which are larger in size, but cover a smaller set of entities; and (ii) low consistency in existing corpora annotated with coreference -in terms of inter-annotator agreement (ITA)) -owing to attempts at covering multiple coreference phenomena that are not equally annotatable with high agreement which likely lessened the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and used by a classifier to generate better predictive models.", "labels": [], "entities": [{"text": "consistency", "start_pos": 96, "end_pos": 107, "type": "METRIC", "confidence": 0.9948315024375916}]}, {"text": "The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past ().", "labels": [], "entities": []}, {"text": "There is a growing consensus that in order to take language understanding applications such as question answering or distillation to the next level, we need more consistent annotation for larger amounts of broad coverage data to train better automatic models for entity and event detection.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7276919633150101}, {"text": "question answering", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.8286221027374268}, {"text": "entity and event detection", "start_pos": 263, "end_pos": 289, "type": "TASK", "confidence": 0.6244215667247772}]}, {"text": "(iii) Complex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios.", "labels": [], "entities": []}, {"text": "This has made it hard to gauge the improvement in algorithms over the years (, or to determine which particular areas require further attention.", "labels": [], "entities": []}, {"text": "Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task.", "labels": [], "entities": []}, {"text": "It can seem to be a very hard problem () or one that is relatively easy (.", "labels": [], "entities": []}, {"text": "(iv) the knowledge bottleneck which has been a well-accepted ceiling that has kept the progress in this task at bay.", "labels": [], "entities": []}, {"text": "These issues suggest that the following steps might take the community in the right direction towards improving the state of the art in coreference resolution: (i) Create a large corpus with high interannotator agreement possibly by restricting the coreference annotating to phenomena that can be annotated with high consistency, and covering an unrestricted set of entities and events; and (ii) Create a standard evaluation scenario with an official evaluation setup, and possibly several ablation settings to capture the range of performance.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.9493264555931091}]}, {"text": "This can then be used as a standard benchmark by the research community.", "labels": [], "entities": []}, {"text": "(iii) Continue to improve learning algorithms that better incorporate world knowledge and jointly incorporate information from other layers of syntactic and semantic annotation to improve the state of the art.", "labels": [], "entities": []}, {"text": "One of the many goals of the OntoNotes project 2 ( was to explore whether it could fill this void and help push the progress further -not only in coreference, but with the various layers of semantics that it tries to capture.", "labels": [], "entities": []}, {"text": "As one of its layers, it has created a corpus for general anaphoric coreference that covers entities and events not limited to noun phrases or a subset of entity types.", "labels": [], "entities": [{"text": "general anaphoric coreference", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.6638107299804688}]}, {"text": "The coreference layer in OntoNotes constitutes just one part of a multilayered, integrated annotation of shallow semantic structures in text with high inter-annotator agreement.", "labels": [], "entities": []}, {"text": "This addresses the first issue.", "labels": [], "entities": []}, {"text": "In the language processing community, the field of speech recognition probably has the longest history of shared evaluations held primary by NIST 3).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7567260563373566}]}, {"text": "In the past decade machine translation has been a topic of shared evaluations also by NIST . There are many syntactic and semantic processing tasks that are not quite amenable to such continued evaluation efforts.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.823955625295639}]}, {"text": "The CoNLL shared tasks over the past 15 years have filled that gap, helping establish benchmarks and advance the state of the art in various sub-fields within NLP.", "labels": [], "entities": []}, {"text": "The importance of shared tasks is now in full display in the domain of clinical NLP) and recently a coreference task was organized as part of the i2b2 workshop ().", "labels": [], "entities": []}, {"text": "The computational learning community is also witnessing a shift towards joint inference based evaluations, with the two previous CoNLL tasks () devoted to joint learning of syntactic and semantic dependencies.", "labels": [], "entities": []}, {"text": "A SemEval-2010 coreference task () was the first attempt to address the second issue.", "labels": [], "entities": [{"text": "SemEval-2010 coreference task", "start_pos": 2, "end_pos": 31, "type": "TASK", "confidence": 0.7342894275983175}]}, {"text": "It included six different Indo-European languagesCatalan, Dutch, English, German, Italian, and Spanish.", "labels": [], "entities": []}, {"text": "Among other corpora, a small subset (\u223c120K) of English portion of OntoNotes was used for this purpose.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.6127188205718994}]}, {"text": "However, the lack of a strong participation prevented the organizers from reaching any firm conclusions.", "labels": [], "entities": []}, {"text": "The CoNLL-2011 shared task was another attempt to address the second issue.", "labels": [], "entities": [{"text": "CoNLL-2011 shared task", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8342763582865397}]}, {"text": "It was well received, but the shared task was only limited to the English portion of OntoNotes.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 85, "end_pos": 94, "type": "DATASET", "confidence": 0.6599898934364319}]}, {"text": "In addition, the coreference portion of OntoNotes did not have a concrete baseline prior to the 2011 evaluation, thereby making it challenging for participants to gauge the performance of their algorithms in the absence of established state of the art on this flavor of annotation.", "labels": [], "entities": []}, {"text": "The closest comparison was to the results reported by on the newswire portion of OntoNotes.", "labels": [], "entities": [{"text": "newswire portion of OntoNotes", "start_pos": 61, "end_pos": 90, "type": "DATASET", "confidence": 0.8251596689224243}]}, {"text": "Since the corpus also covers two other languages from completely different language families, Chinese and Arabic, it provided a great opportunity to have a follow-on task in 2012 covering all three languages.", "labels": [], "entities": []}, {"text": "As we will see later, peculiarities of each of these languages had to be considered in creating the evaluation framework.", "labels": [], "entities": []}, {"text": "The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.9574441313743591}, {"text": "MUC corpora", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.9388526678085327}]}, {"text": "Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward.", "labels": [], "entities": [{"text": "language processing", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7298141121864319}, {"text": "coreference resolution", "start_pos": 157, "end_pos": 179, "type": "TASK", "confidence": 0.9616132080554962}]}, {"text": "Researchers have continued to find novel ways of exploiting ontologies such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.9681915640830994}]}, {"text": "Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited.", "labels": [], "entities": []}, {"text": "Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia ().", "labels": [], "entities": []}, {"text": "More recently researchers have used graph based algorithms) rather than pair-wise classifications.", "labels": [], "entities": []}, {"text": "For a detailed survey of the progress in this field, we refer the reader to a recent article and a tutorial dedicated to this subject.", "labels": [], "entities": []}, {"text": "In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs' distance, etc.", "labels": [], "entities": []}, {"text": "Further research to reduce the knowledge gap is essential to take coreference resolution techniques to the next level.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.9593842625617981}]}, {"text": "The rest of the paper is organized as follows: Section 2 presents an overview of the OntoNotes corpus.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.7953872978687286}]}, {"text": "Section 3 describes the range of phenomena annotated in OntoNotes, and language-specific issues.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.8362265229225159}]}, {"text": "Section 4 describes the shared task data and the evaluation parameters, with Section 4.4.2 examining the performance of the state-of-the-art tools on all/most intermediate layers of annotation.", "labels": [], "entities": []}, {"text": "Section 5 describes the participants in the task.", "labels": [], "entities": []}, {"text": "Section 6 briefly compares the approaches taken by various participating systems.", "labels": [], "entities": []}, {"text": "Section 7 presents the system results with some analysis.", "labels": [], "entities": []}, {"text": "Section 8 compares the performance of the systems on the a subset of the Engish test set that corresponds with the test set used for the CoNLL-2011 evaluation.", "labels": [], "entities": [{"text": "Engish test set", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.9463064273198446}, {"text": "CoNLL-2011 evaluation", "start_pos": 137, "end_pos": 158, "type": "DATASET", "confidence": 0.8910067081451416}]}, {"text": "Section 9 draws some conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The primary evaluation comprises the closed and open tracks where predicted information is provided on all layers of the test set other than coreference.", "labels": [], "entities": []}, {"text": "As mentioned earlier, we provide gold lemma and vocalization information for Arabic, and we use gold standard treebank segmentation for all three languages.", "labels": [], "entities": []}, {"text": "The predicted part of speech for Arabic area mapped down version of the richer gold version present in the treebank  In addition to the option of selecting between the primary closed or the open tracks, the participants also had an option to run their systems in the following ablation settings.", "labels": [], "entities": []}, {"text": "Gold Mention Boundaries (GB) In this case, we provided all possible correct mention boundaries in the test data.", "labels": [], "entities": [{"text": "Gold Mention Boundaries (GB)", "start_pos": 0, "end_pos": 28, "type": "METRIC", "confidence": 0.8947541912396749}]}, {"text": "This essentially entails all NPs, and PRPs in the data extracted from the gold parse trees, as well as the mentions that do not align with any parse constituent, for example, non-existent constituents in the predicted parse owing to errors, some named entities, etc.", "labels": [], "entities": []}, {"text": "Gold Mentions (GM) In this dataset, we provided only and all the correct mentions for the test sets, thereby reducing the task to one of pure mention clustering, and eliminating the task of mention detection and anaphoricity determination 8 . These also include potential spans that do not align with any constituent in the predicted parse tree.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 190, "end_pos": 207, "type": "TASK", "confidence": 0.7178066223859787}]}, {"text": "Gold Parses (GS) In this case, for each language, we replaced the predicted parses in the closed track data with manual, gold parses.", "labels": [], "entities": []}, {"text": "This section describes the evaluation criteria used for the shared task.", "labels": [], "entities": []}, {"text": "Unlike propositions, word sense and named entities, where it is simply a matter of counting the correct answers, or for parsing, where there is an established metric, evaluating the accuracy of coreference continues to be contentious.", "labels": [], "entities": [{"text": "parsing", "start_pos": 120, "end_pos": 127, "type": "TASK", "confidence": 0.9683899283409119}, {"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9952196478843689}]}, {"text": "Various alternative metrics have been proposed, as mentioned below, which weight different features of a proposed coreference pattern differently.", "labels": [], "entities": []}, {"text": "The choice is not clear in part because the value of a particular set of coreference predictions is integrally tied to the consuming application.", "labels": [], "entities": []}, {"text": "A further issue in defining a coreference metric concerns the granularity of the mentions, and how closely the predicted mentions are required to match those in the gold standard fora coreference prediction to be counted as correct.", "labels": [], "entities": []}, {"text": "Our evaluation criterion was in part driven by the OntoNotes data structures.", "labels": [], "entities": [{"text": "OntoNotes data structures", "start_pos": 51, "end_pos": 76, "type": "DATASET", "confidence": 0.9405549963315328}]}, {"text": "OntoNotes coreference makes the distinction between identity coreference and appositive coreference, treating the latter separately.", "labels": [], "entities": []}, {"text": "Thus we evaluated systems only on the identity coreference task, which links all categories of entities and events together into equivalent classes.", "labels": [], "entities": []}, {"text": "The situation with mentions for OntoNotes is also different than it was for MUC or ACE.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.8181300163269043}, {"text": "MUC or ACE", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.6738932530085245}]}, {"text": "OntoNotes data does not explicitly identify the minimum extents of an entity mention, but it does include handtagged syntactic parses.", "labels": [], "entities": [{"text": "OntoNotes data", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.7956505119800568}]}, {"text": "Thus for the official evaluation, we decided to use the exact spans of mentions for determining correctness.", "labels": [], "entities": []}, {"text": "The NP boundaries for the test data were pre-extracted from the handtagged Treebank for annotation, and events triggered by verb phrases were tagged using the verbs themselves.", "labels": [], "entities": [{"text": "handtagged Treebank", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.7250868380069733}]}, {"text": "This choice means that scores for the CoNLL-2012 coreference task are likely to be lower than for coreference evaluations based on MUC, or ACE data, where an approximate match is often allowed based on the specified head of the mentions.", "labels": [], "entities": [{"text": "CoNLL-2012 coreference task", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.5372973183790842}, {"text": "MUC", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.7426310181617737}, {"text": "ACE data", "start_pos": 139, "end_pos": 147, "type": "DATASET", "confidence": 0.8384537994861603}]}, {"text": "In order to determine the best performing system in the shared task, we needed to associate a single number with each system.", "labels": [], "entities": []}, {"text": "This could have been one of the metrics above, or some combination of more than one of them.", "labels": [], "entities": []}, {"text": "The choice was not simple, and after having consulted various researchers in the field, we came to a conclusion that each metric had its pros and cons and there is no silver bullet.", "labels": [], "entities": []}, {"text": "Therefore we settled on the MELA metric proposed by, which takes a weighted average of three metrics: MUC, B-CUBED, and CEAF.", "labels": [], "entities": [{"text": "MELA", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.598706066608429}, {"text": "MUC", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.6071534156799316}, {"text": "B-CUBED", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.9798381328582764}, {"text": "CEAF", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.7515879273414612}]}, {"text": "The rationale for the combination is that each of the three metrics represents a different, important dimension.", "labels": [], "entities": []}, {"text": "The MUC measure is based on links.", "labels": [], "entities": [{"text": "MUC measure", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7811623811721802}]}, {"text": "The B-CUBED is based on mentions, and the CEAF is based on entities.", "labels": [], "entities": [{"text": "B-CUBED", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9937586784362793}, {"text": "CEAF", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.6638712286949158}]}, {"text": "We decided to use the entity based CEAF e instead of mention based CEAF m . For a given end application, a weighted average of the three might be optimal, but since we don't have a particular end task in mind, we decided to use the unweighted mean of the three metrics as the score on which the winning system was judged.", "labels": [], "entities": []}, {"text": "This still leaves us with a score for each language.", "labels": [], "entities": []}, {"text": "We wanted to encourage researchers to run their systems on all three languages.", "labels": [], "entities": []}, {"text": "Therefore, we decided to compute the final official score that would determine the winning submission as the average of the MELA metric across all the three languages.", "labels": [], "entities": [{"text": "MELA metric", "start_pos": 124, "end_pos": 135, "type": "DATASET", "confidence": 0.5691782683134079}]}, {"text": "We decided to give a MELA score of zero to every language that a particular group did not run its system on.", "labels": [], "entities": [{"text": "MELA score", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9591334462165833}]}], "tableCaptions": [{"text": " Table 1: Inter Annotator (A1 and A2) and Adjudicator  (ADJ) agreement for the Coreference Layer in OntoNotes  measured in terms of the MUC score.", "labels": [], "entities": [{"text": "Inter Annotator (A1 and A2) and Adjudicator  (ADJ) agreement", "start_pos": 10, "end_pos": 70, "type": "METRIC", "confidence": 0.8172857807232783}, {"text": "MUC score", "start_pos": 136, "end_pos": 145, "type": "DATASET", "confidence": 0.7622360289096832}]}, {"text": " Table 3: Number of documents in the OntoNotes v5.0 data, and some comparison with the MUC and ACE data sets. The  numbers in parenthesis for the OntoNotes corpus indicate the total number of parts that correspond to the documents.  Each part was considered a separate document for evaluation purposes.", "labels": [], "entities": [{"text": "OntoNotes v5.0 data", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.8808419505755106}, {"text": "MUC and ACE data sets", "start_pos": 87, "end_pos": 108, "type": "DATASET", "confidence": 0.8055927515029907}, {"text": "OntoNotes corpus", "start_pos": 146, "end_pos": 162, "type": "DATASET", "confidence": 0.8566188812255859}]}, {"text": " Table 4: Distribution of mentions in the data by their syn- tactic category.", "labels": [], "entities": []}, {"text": " Table 5: Number of entities, links and mentions in the  OntoNotes v5.0 data.", "labels": [], "entities": [{"text": "OntoNotes v5.0 data", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.9206518530845642}]}, {"text": " Table 7: Number of senses defined for English, Chinese  and Arabic in the OntoNotes v5.0 corpus.", "labels": [], "entities": [{"text": "OntoNotes v5.0 corpus", "start_pos": 75, "end_pos": 96, "type": "DATASET", "confidence": 0.8623436490694681}]}, {"text": " Table 6: Parser performance on the CoNLL-2012 test set.", "labels": [], "entities": [{"text": "Parser", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9223465323448181}, {"text": "CoNLL-2012 test set", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.9836833079655966}]}, {"text": " Table 8: Word sense performance over both verbs and nouns in the CoNLL-2012 test set.", "labels": [], "entities": [{"text": "CoNLL-2012 test set", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.9499516487121582}]}, {"text": " Table 9: Performance on the propositions and framesets in the CoNLL-2012 test set.", "labels": [], "entities": [{"text": "CoNLL-2012 test set", "start_pos": 63, "end_pos": 82, "type": "DATASET", "confidence": 0.9688160419464111}]}, {"text": " Table 11: Named Entity performance on the English subset of the CoNLL-2012 test set.", "labels": [], "entities": [{"text": "English subset of the CoNLL-2012 test set", "start_pos": 43, "end_pos": 84, "type": "DATASET", "confidence": 0.8347082563808986}]}, {"text": " Table 13: Participation by country.", "labels": [], "entities": []}, {"text": " Table 14: Participation across languages and tracks.", "labels": [], "entities": []}, {"text": " Table 17: Performance on primary open and closed tracks using all predicted information.", "labels": [], "entities": []}, {"text": " Table 18: Performance on supplementary open and closed tracks using all predicted information, given gold mention  boundaries.", "labels": [], "entities": []}, {"text": " Table 19: Performance on supplementary open and closed tracks using all predicted information, given gold mentions.", "labels": [], "entities": []}, {"text": " Table 23: Per genre performance for fernandes on the closed, official and supplementary evaluations.", "labels": [], "entities": []}]}