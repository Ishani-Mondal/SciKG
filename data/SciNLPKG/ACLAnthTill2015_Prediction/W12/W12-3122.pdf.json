{"title": [{"text": "Match without a Referee: Evaluating MT Adequacy without Reference Translations", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.7238458395004272}, {"text": "Translations", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.7723053097724915}]}], "abstractContent": [{"text": "We address two challenges for automatic machine translation evaluation: a) avoiding the use of reference translations, and b) focusing on adequacy estimation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.8028250932693481}]}, {"text": "From an economic perspective, getting rid of costly hand-crafted reference translations (a) permits to alleviate the main bottleneck in MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 136, "end_pos": 149, "type": "TASK", "confidence": 0.9554649591445923}]}, {"text": "From a system evaluation perspective, pushing semantics into MT (b) is a necessity in order to complement the shallow methods currently used overcoming their limitations.", "labels": [], "entities": []}, {"text": "Casting the problem as a cross-lingual textual entail-ment application, we experiment with different benchmarks and evaluation settings.", "labels": [], "entities": []}, {"text": "Our method shows high correlation with human judgements and good results on all datasets without relying on reference translations.", "labels": [], "entities": []}], "introductionContent": [{"text": "While syntactically informed modelling for statistical MT is an active field of research that has recently gained major attention from the MT community, work on integrating semantic models of adequacy into MT is still at preliminary stages.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.7740048170089722}, {"text": "MT", "start_pos": 139, "end_pos": 141, "type": "TASK", "confidence": 0.979478120803833}, {"text": "MT", "start_pos": 206, "end_pos": 208, "type": "TASK", "confidence": 0.9403313994407654}]}, {"text": "This situation holds not only for system development (most current methods disregard semantic information, in favour of statistical models of words distribution), but also for system evaluation.", "labels": [], "entities": [{"text": "system development", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7792101502418518}, {"text": "system evaluation", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.7359043657779694}]}, {"text": "To realize its full potential, however, MT is now in the need of semanticaware techniques, capable of complementing frequency counts with meaning representations.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9866346120834351}]}, {"text": "In order to integrate semantics more deeply into MT technology, in this paper we focus on the evaluation dimension.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9863870143890381}]}, {"text": "Restricting our investigation to some of the more pressing issues emerging from this area of research, we provide two main contributions.", "labels": [], "entities": []}, {"text": "1. An automatic evaluation method that avoids the use of reference translations.", "labels": [], "entities": []}, {"text": "Most current metrics are based on comparisons between automatic translations and human references, and reward lexical similarity at the n-gram level (e.g. BLEU (), NIST), METEOR (Banerjee and), TER).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9966993927955627}, {"text": "METEOR", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9601978063583374}, {"text": "TER", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.9787449836730957}]}, {"text": "Due to the variability of natural languages in terms of possible ways to express the same meaning, reliable lexical similarity metrics depend on the availability of multiple hand-crafted (costly) realizations of the same source sentence in the target language.", "labels": [], "entities": []}, {"text": "Our approach aims to avoid this bottleneck by adapting cross-lingual semantic inference capabilities and judging a translation only given the source sentence.", "labels": [], "entities": [{"text": "judging a translation", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.6267416874567667}]}, {"text": "2. A method for evaluating translation adequacy.", "labels": [], "entities": [{"text": "translation", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.8841336369514465}]}, {"text": "Most current solutions do not consistently reward translation adequacy (semantic equivalence between source sentence and target translation).", "labels": [], "entities": []}, {"text": "The scarce integration of semantic information in MT, specifically at the multilingual level, led to MT systems that are \"illiterate\" in terms of semantics and meaning.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.957650899887085}, {"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.9592568278312683}]}, {"text": "Moreover, current metrics are often difficult to interpret.", "labels": [], "entities": []}, {"text": "In contrast, our method targets the adequacy dimension, producing easily interpretable results (e.g. judgements in a 4-point scale).", "labels": [], "entities": []}, {"text": "Our approach builds on recent advances in cross-lingual textual entailment (CLTE) recognition, which provides a natural framework to address MT adequacy evaluation.", "labels": [], "entities": [{"text": "cross-lingual textual entailment (CLTE) recognition", "start_pos": 42, "end_pos": 93, "type": "TASK", "confidence": 0.756325785602842}, {"text": "MT adequacy evaluation", "start_pos": 141, "end_pos": 163, "type": "TASK", "confidence": 0.9099829196929932}]}, {"text": "In particular, we approach the problem as an application of CLTE where bi-directional entailment between source and target is considered as evidence of translation adequacy.", "labels": [], "entities": []}, {"text": "Besides avoiding the use of references, the proposed solution differs from most previous methods which typically rely on surface-level features, often extracted from the source or the target sentence taken in isolation.", "labels": [], "entities": []}, {"text": "Although some of these features might correlate well with adequacy, they capture semantic equivalence only indirectly, and at the level of a probabilistic prediction.", "labels": [], "entities": []}, {"text": "Focusing on a combination of surface, syntactic and semantic features, extracted from both source and target (e.g. \"sourcetarget length ratio\", \"dependency relations in common\"), our approach leads to informed adequacy judgements derived from the actual observation of a translation given the source sentence.", "labels": [], "entities": [{"text": "sourcetarget length ratio", "start_pos": 116, "end_pos": 141, "type": "METRIC", "confidence": 0.6517256995042165}]}], "datasetContent": [{"text": "We address adequacy evaluation by adapting crosslingual textual entailment recognition as away to measure to what extent a source sentence and its automatic translation are semantically similar.", "labels": [], "entities": [{"text": "adequacy evaluation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.6779744327068329}, {"text": "crosslingual textual entailment recognition", "start_pos": 43, "end_pos": 86, "type": "TASK", "confidence": 0.6625841408967972}]}, {"text": "CLTE has been proposed by as an extension of textual entailment () that consists in deciding, given a text T and a hypothesis H in different languages, if the meaning of H can be inferred from the meaning of T.", "labels": [], "entities": []}, {"text": "The main motivation in approaching adequacy evaluation using CLTE is that an adequate translation and the source text should convey the same meaning.", "labels": [], "entities": [{"text": "adequacy evaluation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7070502638816833}]}, {"text": "In terms of entailment, this means that an adequate MT output and the source sentence should entail each other (bi-directional entailment).", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9691397547721863}]}, {"text": "Los-ing or altering part of the meaning conveyed by the source sentence (i.e. having more, or different information in one of the two sides) will change the entailment direction and, consequently, the adequacy judgement.", "labels": [], "entities": []}, {"text": "Framed in this way, CLTE-based adequacy evaluation methods can be designed to distinguish meaning-preserving variations from true divergence, regardless of reference translations.", "labels": [], "entities": []}, {"text": "Similarly to many monolingual TE approaches, CLTE solutions proposed so far adopt supervised learning methods, with features that measure to what extent the hypotheses can be mapped into the texts.", "labels": [], "entities": []}, {"text": "The underlying assumption is that the probability of entailment is proportional to the number of words in H that can be mapped to words in T ().", "labels": [], "entities": []}, {"text": "Such mapping can be carried out at different word representation levels (e.g. tokens, lemmas, stems), possibly with the support of lexical knowledge in order to cross the language barrier between T and H (e.g. dictionaries, phrase tables).", "labels": [], "entities": []}, {"text": "Under the same assumption, since in the adequacy evaluation framework the entailment relation should hold in both directions, the mapping is performed both from the source to the target and vice-versa, building on features extracted from both sentences.", "labels": [], "entities": []}, {"text": "Moreover, to improve over previous CLTE methods and boost MT adequacy evaluation performance, we explore the joint contribution of a number of lexical, syntactic and semantic features ().", "labels": [], "entities": [{"text": "MT adequacy evaluation", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.8462044397989908}]}, {"text": "Concerning the features used, it's worth observing that the cost of implementing our approach (in terms of required resources and linguistic processors), and the need of reference translations are intrinsically different bottlenecks for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 237, "end_pos": 239, "type": "TASK", "confidence": 0.9915627241134644}]}, {"text": "While the limited availability of processing tools for some language pairs is a \"temporary\" bottleneck, the acquisition of multiple references is a \"permanent\" one.", "labels": [], "entities": []}, {"text": "The former cost is reducing overtime due to the progress in NLP research; the latter represents a fixed cost that has to be eliminated.", "labels": [], "entities": [{"text": "overtime", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9960523843765259}]}, {"text": "Similar considerations hold regarding the need of annotated data to develop our supervised learning approach.", "labels": [], "entities": []}, {"text": "Concerning this, the cost of labelling source-target pairs with adequacy judgments is significantly lower compared to the creation of multiple references.", "labels": [], "entities": []}, {"text": "Datasets with manual evaluation of MT output have been made available through a number of shared evaluation tasks.", "labels": [], "entities": [{"text": "MT output", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.8890524208545685}]}, {"text": "However, most of these datasets are not specifically annotated for adequacy measurement purposes, and the available adequacy judgements are limited to few hundred sentences for some language pairs.", "labels": [], "entities": []}, {"text": "Moreover, most datasets are created by comparing reference translations with MT systems' output, disregarding the input sentences.", "labels": [], "entities": []}, {"text": "Such judgements are hence biased towards the reference.", "labels": [], "entities": []}, {"text": "Furthermore, the inter-annotator agreement is often low . In light of these limitations, most of the available datasets are per se not fully suitable for adequacy evaluation methods based on supervised learning, nor to provide stable and meaningful results.", "labels": [], "entities": []}, {"text": "To partially cope with these problems, our experiments have been carried out over two different datasets: \u2022 16K: 16.000 English-Spanish pairs, with Spanish translations produced by multiple MT systems, annotated by professional translators with quality scores in a 4-point scale ().", "labels": [], "entities": []}, {"text": "\u2022 WMT07: 703 English-Spanish pairs derived from MT systems' output, with explicit adequacy judgements on a 5-point scale.", "labels": [], "entities": [{"text": "WMT07", "start_pos": 2, "end_pos": 7, "type": "DATASET", "confidence": 0.8432444930076599}]}, {"text": "The two datasets present complementary advantages and disadvantages.", "labels": [], "entities": []}, {"text": "On the one hand, although it is not annotated to explicitly capture meaning-related aspects of MT output, the quality oriented dataset has the main advantage of being large enough for supervised approaches.", "labels": [], "entities": [{"text": "MT output", "start_pos": 95, "end_pos": 104, "type": "TASK", "confidence": 0.927692323923111}]}, {"text": "Moreover, it should allow to check the effectiveness of our feature set in estimating adequacy as a latent aspect of the more general notion of MT output quality.", "labels": [], "entities": [{"text": "MT output", "start_pos": 144, "end_pos": 153, "type": "TASK", "confidence": 0.8877092003822327}]}, {"text": "On the other hand, the smaller dataset is less suitable for supervised learning, but represents an appropriate benchmark for MT adequacy evaluation.", "labels": [], "entities": [{"text": "MT adequacy evaluation", "start_pos": 125, "end_pos": 147, "type": "TASK", "confidence": 0.87298583984375}]}, {"text": "We grouped the quality scores in the 4-point scale into two classes, where scores {1,2} are considered as \"bad\" or \"inadequate\", while {3,4} are taken as \"good\" or \"adequate\".", "labels": [], "entities": []}, {"text": "We carried out learning and classification using different sets of features with 10-fold cross validation.", "labels": [], "entities": []}, {"text": "We also compared our accuracy with the MjC baseline, and calculated the improvement of each model (\u2206) against it.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9996563196182251}, {"text": "MjC baseline", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.8508248925209045}]}, {"text": "The results reported in demonstrate that the accuracy of our models is always significantly superior to the MjC baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9995391368865967}, {"text": "MjC baseline", "start_pos": 108, "end_pos": 120, "type": "DATASET", "confidence": 0.7591152191162109}]}, {"text": "Moreover, also in this case there is a steady improvement using syntactic and semantic features over the results obtained by surface form features.", "labels": [], "entities": []}, {"text": "Additionally, it is worth mentioning that the best model improvement over the baseline (\u2206) is much higher (about 18%) than the improvement reported in () over the same dataset (about 8%), considering the average score obtained with their data distribution.", "labels": [], "entities": []}, {"text": "This confirms the effectiveness of our CLTE approach also in classifying \"good\" and \"bad\" translations.", "labels": [], "entities": [{"text": "classifying \"good\" and \"bad\" translations", "start_pos": 61, "end_pos": 102, "type": "TASK", "confidence": 0.6508009334405264}]}, {"text": "We mapped the 5-point scale adequacy scores into two classes, with {1,2,3} judgements assigned to the \"inadequate\" class, and {4,5} judgements assigned to the \"adequate\" class.", "labels": [], "entities": []}, {"text": "The main motivation for this distribution was to separate the examples in away that adequate translations are substantially acceptable, while inadequate translations present evident meaning discrepancies with the source.", "labels": [], "entities": []}, {"text": "The results reported in show that the accuracy of the binary classifiers to distinguish between \"adequate\" and \"inadequate\" classes was significantly superior (up to about 15%) to the MjC baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9992619156837463}, {"text": "MjC baseline", "start_pos": 184, "end_pos": 196, "type": "DATASET", "confidence": 0.7879482209682465}]}, {"text": "We also notice that surface form features have a significant contribution to deal with the adequacy-oriented dataset, while the gain obtained using syntactic and semantic features (2%) is lower than the improvement observed in the 16K dataset.", "labels": [], "entities": [{"text": "16K dataset", "start_pos": 231, "end_pos": 242, "type": "DATASET", "confidence": 0.8719219267368317}]}, {"text": "This might be due to the more unbalanced distribution of the classes which: i) leads to a high baseline, and ii) together with the small size of the WMT07 dataset, makes supervised learning more challenging.", "labels": [], "entities": [{"text": "WMT07 dataset", "start_pos": 149, "end_pos": 162, "type": "DATASET", "confidence": 0.9789514243602753}]}, {"text": "Finally, the improvement of all models (\u2206) over the MjC baseline is much higher than the gain reported in (Specia et al., 2011) over their adequacyoriented dataset (around 2%).", "labels": [], "entities": [{"text": "MjC baseline", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.8509341180324554}]}], "tableCaptions": [{"text": " Table 2: Pearson's correlation between SVM regression and human adequacy annotation over WMT07.", "labels": [], "entities": [{"text": "WMT07", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.8734806776046753}]}, {"text": " Table 3: Multi-class classification accuracy of the quality/adequacy scores.", "labels": [], "entities": [{"text": "Multi-class classification", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.5736644864082336}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9829167723655701}]}, {"text": " Table 4: Accuracy of the binary classification into \"good\" or \"adequate\", and \"bad\" or \"inadequate\".", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9872270226478577}]}]}