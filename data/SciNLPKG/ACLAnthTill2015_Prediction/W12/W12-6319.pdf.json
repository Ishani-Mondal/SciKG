{"title": [{"text": "A MMSM-based Hybrid Method for Chinese MicroBlog Word Seg- mentation", "labels": [], "entities": [{"text": "Chinese MicroBlog Word Seg- mentation", "start_pos": 31, "end_pos": 68, "type": "DATASET", "confidence": 0.8336207667986552}]}], "abstractContent": [{"text": "After years of researches, Chinese word seg-mentation has achieved quite high precisions for formal style text.", "labels": [], "entities": [{"text": "precisions", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9964485168457031}]}, {"text": "However, the performance of segmentation is not so satisfying for MicroBlog corpora.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.9768950939178467}, {"text": "MicroBlog corpora", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.8723068535327911}]}, {"text": "In this paper we describe a scheme for Chinese word segmentation for, MicroBlog which integrates the character-based and word-based information in the directed graph generated by MMSM model.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6184307932853699}, {"text": "MicroBlog", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.953974723815918}]}, {"text": "Word-level information is effective for analysis of known words, while character-level information is useful for analysis of unknown words.", "labels": [], "entities": []}, {"text": "A multi-chain unequal states CRF model is proposed.", "labels": [], "entities": []}, {"text": "The proposed multi-chain unequal states CRF has two state chains with unequal states which can recognize the POS tag simultaneously.", "labels": [], "entities": []}, {"text": "The hybrid model was effective and adopted in real-world system.", "labels": [], "entities": []}], "introductionContent": [{"text": "MicroBlog is an emerging application in the Web 2.0 era.", "labels": [], "entities": [{"text": "MicroBlog", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9601311087608337}]}, {"text": "On MicroBlog websites, users are able to post short messages less than a certain length, e.g., English or Chinese characters, to communicate and share information with each other.", "labels": [], "entities": []}, {"text": "After obtaining cleaned messages fora given user, we perform word segmentation for messages.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7061508744955063}]}, {"text": "In this paper, we use the system developed by Affective Computing and Natural Language Processing Group in Hefei University of Technology.", "labels": [], "entities": [{"text": "Hefei University of Technology", "start_pos": 107, "end_pos": 137, "type": "DATASET", "confidence": 0.9533648788928986}]}, {"text": "The system performs word segmentation and POS tagging simultaneously using a word lattice based re-ranking method proposed by Sun et al..", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7943457961082458}, {"text": "POS tagging", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.8228024244308472}]}, {"text": "Microblogs contain many out-of-vocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "To address the OOV problem, we also maintain a large up-to-date external vocabulary for word segmentation and POS tagging.", "labels": [], "entities": [{"text": "OOV", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.6877151727676392}, {"text": "word segmentation", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7573763430118561}, {"text": "POS tagging", "start_pos": 110, "end_pos": 121, "type": "TASK", "confidence": 0.8472610712051392}]}, {"text": "To keep the vocabulary up-to-date, we import new words from two sources.", "labels": [], "entities": []}, {"text": "The first is the Sogou New Word Dictionary which is updated weekly, and the second is the Sina Popular Word List, which is updated daily.", "labels": [], "entities": [{"text": "Sogou New Word Dictionary", "start_pos": 17, "end_pos": 42, "type": "DATASET", "confidence": 0.8736688047647476}, {"text": "Sina Popular Word List", "start_pos": 90, "end_pos": 112, "type": "DATASET", "confidence": 0.8457687199115753}]}, {"text": "The hybrid model for Chinese MicroBlog morphological analysis includes Chinese word segmentation, unknown word recognition and POS tagging.", "labels": [], "entities": [{"text": "Chinese MicroBlog morphological analysis", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.7191342264413834}, {"text": "Chinese word segmentation", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.5535684327284495}, {"text": "unknown word recognition", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.6312009890874227}, {"text": "POS tagging", "start_pos": 127, "end_pos": 138, "type": "TASK", "confidence": 0.7774747014045715}]}, {"text": "The foundation of the model is a directed segmentation graph based on the maximum matching and second-maximum matching (MMSM) model.", "labels": [], "entities": []}, {"text": "Based on a known words system dictionary trained from the corpus, the MMSM model tries to build a directed graph with the candidate words and their parts-of-speech.", "labels": [], "entities": []}, {"text": "In the directed graph, the character-level information and wordlevel information are combined, the HMM model is used to process the known words (words in system dictionary) using the word-level information; the proposed multi-chain unequal states CRF model is adopted to process the unknown words and their parts-of-speech using characterlevel information.", "labels": [], "entities": []}, {"text": "Meanwhile, for the unknown word, which is the main difficulty in Chinese morphological analysis, both the word boundary and the parts-of-speech of the unknown words are unknown.", "labels": [], "entities": []}, {"text": "A multi-chain unequal states (MUS) CRF model is proposed hereto process the unknown word segmentation and POS tagging.", "labels": [], "entities": [{"text": "unknown word segmentation", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.6463147401809692}, {"text": "POS tagging", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.7648640275001526}]}, {"text": "The proposed multi-chain CRF model has multi states chains for multi tasks.", "labels": [], "entities": []}, {"text": "In our system, we adopted two states chains in which one states chain for the unknown words recognition and the other states chain for the unknown words POS tagging.", "labels": [], "entities": [{"text": "unknown words recognition", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.7175678213437399}, {"text": "POS tagging", "start_pos": 153, "end_pos": 164, "type": "TASK", "confidence": 0.6677095293998718}]}, {"text": "The proposed MUS CRF model recognizes the unknown words from the sentence together with their POSs in one step, without using two separate linear-chain CRF models.", "labels": [], "entities": [{"text": "MUS CRF", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.6744145750999451}]}, {"text": "The unknown words with their part-of-speech recognized by the multi-chain are added into the directed graph as candidates.", "labels": [], "entities": []}, {"text": "With the directed segmentation graph and the proposed multi-chain CRF, the word-level information and character-level information are combined, Chinese word segmentation, unknown word recognition and POS tagging can be accomplished simultaneously.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 144, "end_pos": 169, "type": "TASK", "confidence": 0.5857694447040558}, {"text": "unknown word recognition", "start_pos": 171, "end_pos": 195, "type": "TASK", "confidence": 0.6187546650568644}, {"text": "POS tagging", "start_pos": 200, "end_pos": 211, "type": "TASK", "confidence": 0.803890734910965}]}], "datasetContent": [{"text": "We trained the hybrid model on the PKU2002 corpus, the PKU2002 corpus have 12 months corpus of Peoples' Daily News of year 2002 that have been annotated.", "labels": [], "entities": [{"text": "PKU2002 corpus", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.9811684787273407}, {"text": "PKU2002 corpus have 12 months corpus of Peoples' Daily News of year 2002", "start_pos": 55, "end_pos": 127, "type": "DATASET", "confidence": 0.8775257789171659}]}, {"text": "As the corpus are different from MicroBlog, so the final test result are not quite satisfying.", "labels": [], "entities": [{"text": "MicroBlog", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.9610089659690857}]}, {"text": "The evaluation tools and standards for SIGHAN6 are adopted in the experiments.", "labels": [], "entities": [{"text": "SIGHAN6", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.48153525590896606}]}, {"text": "We present the results of our experiments in recall, precision and F-measure, which are defined in the equations below, as usual in such experiments.", "labels": [], "entities": [{"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9989293217658997}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9994031190872192}, {"text": "F-measure", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.998216450214386}]}, {"text": "First the hybrid model was tested by using different size of training corpus with the same outer lexical dictionary (with the maximum length of word of five).", "labels": [], "entities": []}, {"text": "The test corpus in our experiment is randomly selected 500KB raw corpus from the PKU corpus except the training corpus.", "labels": [], "entities": [{"text": "PKU corpus", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.8809393644332886}]}, {"text": "The result is shown in.", "labels": [], "entities": []}, {"text": "Chinese word segmentation result by using different size of training corpus.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5972845554351807}]}, {"text": "In the experiments, as the size of training corpus increases, the training cost increases exponentially.", "labels": [], "entities": []}, {"text": "It costs too much memories and time to train the model on four months corpus, so we only tested on one month, two months and three months corpus.", "labels": [], "entities": []}, {"text": "We can see as the size of training corpus increases, the F-score of our model increases simultaneously.", "labels": [], "entities": [{"text": "F-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9986649751663208}]}, {"text": "We also tested the model using different outer dictionary.", "labels": [], "entities": []}, {"text": "We adopted two different outer dictionaries, the maximum length of word in one dictionary is 4(DIC4), and the other is 5(DIC5).", "labels": [], "entities": []}, {"text": "The first dictionary has about 100,000 words.", "labels": [], "entities": []}, {"text": "The other has more than 300,000 words.", "labels": [], "entities": []}, {"text": "The words in the dictionary are collected from the internet using our internet crawler.", "labels": [], "entities": []}, {"text": "The training corpus in this experiment is the three months training corpus.", "labels": [], "entities": []}, {"text": "The test corpus is randomly selected 500KB raw corpus.", "labels": [], "entities": []}, {"text": "The result is shown in the following Outer.", "labels": [], "entities": []}, {"text": "Chinese word segmentation result by using different outer dictionary The result of DIC5 is much better than the DIC4 because of the increasing of the maximum length of the word in the dictionary and the size of the dictionary.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5540670653184255}]}, {"text": "We tested our POS tagging result using two training corpus.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.8504525721073151}]}, {"text": "In the first experiment we trained one month corpus and in the second we trained two months corpus.", "labels": [], "entities": []}, {"text": "The test corpus is randomly selected 500KB raw corpus.", "labels": [], "entities": []}, {"text": "The result of POS tagging is in.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.7090183198451996}]}, {"text": "The A in means total accuracy of POS tagging.", "labels": [], "entities": [{"text": "A", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9955571293830872}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9979155659675598}, {"text": "POS tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.8169491291046143}]}, {"text": "The IV-R means the POS tagging recall of in-vocabulary words.", "labels": [], "entities": [{"text": "POS tagging recall of in-vocabulary words", "start_pos": 19, "end_pos": 60, "type": "TASK", "confidence": 0.7506509919961294}]}, {"text": "The OOV-R means the POS tagging recall of out-of-vocabulary words.", "labels": [], "entities": [{"text": "OOV-R", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9656000733375549}, {"text": "POS tagging recall of out-of-vocabulary words", "start_pos": 20, "end_pos": 65, "type": "TASK", "confidence": 0.7657059729099274}]}, {"text": "The MT-R means POS tagging recall of multi-tag words..", "labels": [], "entities": [{"text": "MT-R", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.4122028350830078}, {"text": "POS tagging recall of multi-tag words.", "start_pos": 15, "end_pos": 53, "type": "TASK", "confidence": 0.7246727248032888}]}, {"text": "POS tagging result by using different size of training corpus.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7856940031051636}]}], "tableCaptions": [{"text": " Table 1. Each  character in the sentence is assigned a tag from  the 6-tag label set to mark their position in a  word.", "labels": [], "entities": []}, {"text": " Table 3. Chinese word segmentation result by  using different size of training corpus.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.5887856284777323}]}, {"text": " Table 4. Chinese word segmentation result by  using different outer dictionary", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.5700670878092448}]}, {"text": " Table 5. POS tagging result by using different  size of training corpus.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7718526422977448}]}, {"text": " Table 6. Chinese word segmentation result of  SIGHAN2007", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.5233784665664037}, {"text": "SIGHAN2007", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.46182847023010254}]}, {"text": " Table 7. POS tagging result of SIGHAN 2007", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6844448745250702}, {"text": "SIGHAN 2007", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.6688190847635269}]}]}