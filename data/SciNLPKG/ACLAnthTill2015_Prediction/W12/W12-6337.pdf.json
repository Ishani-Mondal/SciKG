{"title": [{"text": "Adapting Multilingual Parsing Models to Sinica Treebank", "labels": [], "entities": [{"text": "Adapting Multilingual Parsing Models", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8856841027736664}, {"text": "Sinica Treebank", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.6367244273424149}]}], "abstractContent": [{"text": "This paper presents our work for participation in the 2012 CIPS-SIGHAN shared task of Traditional Chinese Parsing.", "labels": [], "entities": [{"text": "2012 CIPS-SIGHAN shared task of Traditional Chinese Parsing", "start_pos": 54, "end_pos": 113, "type": "TASK", "confidence": 0.6276910938322544}]}, {"text": "We have adopted two multilingual parsing models-a factored model (Stanford Parser) and an unlexicalized model (Berkeley Parser) for parsing the Sinica Treebank.", "labels": [], "entities": [{"text": "Sinica Treebank", "start_pos": 144, "end_pos": 159, "type": "DATASET", "confidence": 0.83155158162117}]}, {"text": "This paper also proposes anew Chinese unknown word model and integrates it into the Berkeley Parser.", "labels": [], "entities": [{"text": "Berkeley Parser", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.9295795857906342}]}, {"text": "Our experiment gives the first result of adapting existing multilingual parsing models to the Sinica Treebank and shows that the parsing accuracy can be improved by our suggested approach.", "labels": [], "entities": [{"text": "Sinica Treebank", "start_pos": 94, "end_pos": 109, "type": "DATASET", "confidence": 0.9369597434997559}, {"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9676516652107239}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.8971340656280518}]}], "introductionContent": [{"text": "Work in syntactic parsing has developed substantial advanced Probabilistic Context-Free Grammar (PCFG) models).", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7728477120399475}]}, {"text": "The syntactic structures of English sentences can be well analyzed by utilizing these models.", "labels": [], "entities": []}, {"text": "The highest traditional PAR-SEVAL F1 accuracy evaluation reported on English Parsing have already reached 92.4%, which is very acceptable.", "labels": [], "entities": [{"text": "PAR-SEVAL F1 accuracy evaluation", "start_pos": 24, "end_pos": 56, "type": "METRIC", "confidence": 0.7918644845485687}, {"text": "English Parsing", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.5530081540346146}]}, {"text": "However, parsing Chinese still a tough task.", "labels": [], "entities": [{"text": "parsing Chinese", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.8342932164669037}]}, {"text": "Chinese varies from English in many linguistic aspects.", "labels": [], "entities": []}, {"text": "That makes a big difference between the Chinese syntactic trees' structures and the English ones.", "labels": [], "entities": []}, {"text": "For example, the Chinese syntactic tree is constructed flatter than the English one (.", "labels": [], "entities": []}, {"text": "In this paper, we present our solution for the 2012 CIPS-SIGHAN shared task of Traditional Chinese parsing.", "labels": [], "entities": [{"text": "2012 CIPS-SIGHAN shared task", "start_pos": 47, "end_pos": 75, "type": "DATASET", "confidence": 0.7096202671527863}, {"text": "Traditional Chinese parsing", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.6644939879576365}]}, {"text": "We exploit two existing powerful parsing models -the factored model (Stanford Parser) and the unlexicalized model, which have already shown their effectiveness in English, and adapt it to our task with necessary modification.", "labels": [], "entities": [{"text": "Stanford Parser)", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.8670574029286703}]}, {"text": "First, in order to make use of Stanford Parser, we try to build ahead propagation table of Traditional Chinese for the adaptation of the specific Traditional Chinese Corpus -Sinica Treebank ().", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.920310378074646}, {"text": "Traditional Chinese Corpus -Sinica Treebank", "start_pos": 146, "end_pos": 189, "type": "DATASET", "confidence": 0.8896398444970449}]}, {"text": "Second, we propose anew Chinese unknown word model to estimate the word emission probability, to improve the Traditional Chinese parsing performance for the Berkeley Parser.", "labels": [], "entities": [{"text": "word emission probability", "start_pos": 67, "end_pos": 92, "type": "METRIC", "confidence": 0.6076704462369283}, {"text": "Berkeley Parser", "start_pos": 157, "end_pos": 172, "type": "DATASET", "confidence": 0.9066148698329926}]}], "datasetContent": [{"text": "In our experiment, we divide the Sinica Treebank in 3 parts following the traditional supervised parsing experimental protocol: training (first 80%), development (second 10%) and test (remaining 10%).", "labels": [], "entities": [{"text": "Sinica Treebank", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.8504860103130341}]}, {"text": "We systematically report the result with treebank transformed.", "labels": [], "entities": []}, {"text": "Namely, we preprocess the treebank in order to turn each tree into the same format 6 as in Penn Treebank since As suggested, the geometric average is better than arithmetic average, but we do not testify it in this paper due to tight schedule.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.9917649924755096}, {"text": "arithmetic average", "start_pos": 162, "end_pos": 180, "type": "METRIC", "confidence": 0.9460663497447968}]}, {"text": "All the Semantic Role Labels are eliminated.", "labels": [], "entities": []}, {"text": "mentioned constituency parsers only accept this format.", "labels": [], "entities": []}, {"text": "We use the standard labeled bracketed PARSE-VAL metric for constituency evaluation, all the phrasal tags will betaken into account.", "labels": [], "entities": [{"text": "constituency evaluation", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.9120469987392426}]}, {"text": "Besides, we also report the POS accuracy.", "labels": [], "entities": [{"text": "POS", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9822618961334229}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8869574069976807}]}, {"text": "For better description, we name the basic version of Stanford parser as Stanford-BA and the modified version with the Traditional Chinese head propagation table as Stanford-MOD.", "labels": [], "entities": []}, {"text": "While Berkeley-BA and Berkeley-MOD represent for the basic Berkeley parser and the intensive one respectively.", "labels": [], "entities": []}, {"text": "gives their performance on parsing Traditional Chinese.", "labels": [], "entities": [{"text": "parsing Traditional Chinese", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.788968582948049}]}, {"text": "Coming to a comparing among these two parsers, Berkeley parser has better overall performance.", "labels": [], "entities": []}, {"text": "The basic version of Berkeley parser, Berkeley-BA, beat Stanford-BA in 4.4%, scored 45.20% and 49.60% F1 respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.999832272529602}]}, {"text": "For each model, our modification for adaptation also makes an improvement.", "labels": [], "entities": []}, {"text": "After deploying the specific head propagation table, we got 2.12% and 0.2% improvement in constituent accuracy and POS accuracy respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9474974274635315}, {"text": "POS", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9641208052635193}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.6812261939048767}]}, {"text": "While the Berkeley-MOD benefits from the new Chinese Unknown word model, the constituent F1 and POS accuracy reach to 50.42% and 74.02% respectively 8 .", "labels": [], "entities": [{"text": "F1", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9632159471511841}, {"text": "POS", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9720274806022644}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8582093715667725}]}], "tableCaptions": [{"text": " Table 1: Previous Work on Parsing Chinese", "labels": [], "entities": [{"text": "Parsing Chinese", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.9300276935100555}]}]}