{"title": [{"text": "PRHLT Submission to the WMT12 Quality Estimation Task", "labels": [], "entities": [{"text": "PRHLT Submission", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5623826384544373}, {"text": "WMT12 Quality Estimation", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.653545081615448}]}], "abstractContent": [{"text": "This is a description of the submissions made by the pattern recognition and human language technology group (PRHLT) of the Uni-versitat Polit\u00e8cnica deVa\u00ec encia to the quality estimation task of the seventh workshop on statistical machine translation (WMT12).", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7421285212039948}, {"text": "statistical machine translation (WMT12)", "start_pos": 219, "end_pos": 258, "type": "TASK", "confidence": 0.7098486572504044}]}, {"text": "We focus on two different issues: how to effectively combine subsequence-level features into sentence-level features, and how to select the most adequate subset of features.", "labels": [], "entities": []}, {"text": "Results showed that an adequate selection of a subset of highly discriminative features can improve efficiency and performance of the quality estimation system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality estimation (QE) () is a topic of increasing interest in machine translation.", "labels": [], "entities": [{"text": "Quality estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7583996534347535}, {"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8261669278144836}]}, {"text": "It aims at providing a quality indicator for unseen translations at various granularity levels.", "labels": [], "entities": []}, {"text": "Different from MT evaluation, QE do not rely on reference translations and is generally addressed using machine learning techniques to predict quality scores.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9648315906524658}, {"text": "QE", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.8239718079566956}]}, {"text": "Our main focus in this article is in the combination of subsequence features into sentence features, and in the selection of a subset of relevant features to improve performance and efficiency.", "labels": [], "entities": []}, {"text": "Section 2 describes the features and the learning algorithm used in the experiments.", "labels": [], "entities": []}, {"text": "Section 3 describe two different approaches implemented to select the best-performing subset of features.", "labels": [], "entities": []}, {"text": "Section 4 displays the results of the experimentation intended to determine the optimal setup to train our final submission.", "labels": [], "entities": []}, {"text": "Finally, section 5 summarizes the submission and discusses the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "After establishing the optimal training setup, we now show the official evaluation results for our submissions.", "labels": [], "entities": []}, {"text": "shows the performance of the various participants in the ranking (delta average) and scoring (MAE) tasks.", "labels": [], "entities": [{"text": "delta average) and scoring (MAE)", "start_pos": 66, "end_pos": 98, "type": "METRIC", "confidence": 0.7922575175762177}]}, {"text": "Surprisingly our submissions yielded a slightly worse result than the baseline features.", "labels": [], "entities": []}, {"text": "However, given the large improvements over the baseline system obtained in the pre-submission experiments, we expected to obtain similar improvements over Baseline in test.", "labels": [], "entities": []}, {"text": "We considered two possible explanations for this counterintuitive result.", "labels": [], "entities": []}, {"text": "First, a possibly divergence between the underlying distributions of the training and test data.", "labels": [], "entities": []}, {"text": "To investigate this possibility, we stud-ied the distributions of feature values in the training and test data.", "labels": [], "entities": []}, {"text": "deviation for the first 15 features used in our final submissions (similar results are obtained for all the 222 features).", "labels": [], "entities": []}, {"text": "We can observe that feature values in training and test data follow a similar distribution, although test values tend to be slightly lower than training values.", "labels": [], "entities": []}, {"text": "A second plausible explanation is the small amount of training data (only 1832 samples).", "labels": [], "entities": []}, {"text": "Limited data favors simpler systems that can train its few free parameters more accurately.", "labels": [], "entities": []}, {"text": "This is the case of the Baseline system that was trained using only 11 features, in comparison with the 222 features used in our submissions.", "labels": [], "entities": []}, {"text": "Since the training and test data seem to have been generated following the same underlying distribution, we hypothesize that the limited training data is the main explanation for the poor test performance of our submissions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Best official evaluation results on each task of  the different participating teams. Results for our submis- sions are displayed in bold. Baseline results in italics.", "labels": [], "entities": []}]}