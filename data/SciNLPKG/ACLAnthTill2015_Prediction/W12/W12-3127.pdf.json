{"title": [{"text": "Using Categorial Grammar to Label Translation Rules", "labels": [], "entities": [{"text": "Label Translation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7265219986438751}]}], "abstractContent": [{"text": "Adding syntactic labels to synchronous context-free translation rules can improve performance, but labeling with phrase structure constituents, as in GHKM (Galley et al., 2004), excludes potentially useful translation rules.", "labels": [], "entities": [{"text": "GHKM", "start_pos": 150, "end_pos": 154, "type": "DATASET", "confidence": 0.8856477737426758}]}, {"text": "SAMT (Zollmann and Venugopal, 2006) introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar.", "labels": [], "entities": []}, {"text": "We introduce a labeling scheme based on categorial grammar, which allows syntactic labeling of many rules with a minimal , well-motivated label set.", "labels": [], "entities": []}, {"text": "We show that our labeling scheme performs comparably to SAMT on an Urdu-English translation task, yet the label set is an order of magnitude smaller, and translation is twice as fast.", "labels": [], "entities": [{"text": "Urdu-English translation task", "start_pos": 67, "end_pos": 96, "type": "TASK", "confidence": 0.6762855251630148}]}], "introductionContent": [{"text": "The Hiero model of popularized the usage of synchronous context-free grammars (SCFGs) for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7373818755149841}]}, {"text": "SCFGs model translation as a process of isomorphic syntactic derivation in the source and target language.", "labels": [], "entities": []}, {"text": "But the Hiero model is formally, not linguistically syntactic.", "labels": [], "entities": []}, {"text": "Its derivation trees use only a single non-terminal label X, carrying no linguistic information.", "labels": [], "entities": []}, {"text": "X \u2192 maison ; house We can add syntactic information to the SCFG rules by parsing the parallel training data and projecting parse tree labels onto the spans they yield and their translations.", "labels": [], "entities": []}, {"text": "For example, if house was parsed as a noun, we could rewrite Rule 1 as N \u2192 maison ; house But we quickly run into trouble: how should we label a rule that translates pour l'\u00b4 etablissement de into for the establishment of?", "labels": [], "entities": []}, {"text": "There is no phrase structure constituent that corresponds to this English fragment.", "labels": [], "entities": []}, {"text": "This raises a model design question: what label do we assign to spans that are natural translations of each other, but have no natural labeling under a syntactic parse?", "labels": [], "entities": []}, {"text": "One possibility would be to discard such translations from our model as implausible.", "labels": [], "entities": []}, {"text": "However, such non-compositional translations are important in translation), and they have been repeatedly shown to improve translation performance (.", "labels": [], "entities": [{"text": "translation", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.9838305711746216}]}, {"text": "Syntax-Augmented Machine Translation (SAMT;) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the concatenation of a noun phrase with a preposition on either side.", "labels": [], "entities": [{"text": "Syntax-Augmented Machine Translation (SAMT", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7348007440567017}, {"text": "phrase structure parse", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.7352645397186279}]}, {"text": "While descriptive, this label is unsatisfying as a concise description of linguistic function, fitting uneasily alongside more natural labels in the phrase structure formalism.", "labels": [], "entities": []}, {"text": "SAMT introduces many thousands of such labels, most of which are seen very few times.", "labels": [], "entities": [{"text": "SAMT", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.6825207471847534}]}, {"text": "While these heuristics are effective (, they inflate grammar size, hamper effective parameter estimation due to feature sparsity, and slow translation speed.", "labels": [], "entities": []}, {"text": "Our objective is to find a syntactic formalism that enables us to label most translation rules without relying on heuristics.", "labels": [], "entities": []}, {"text": "Ideally, the label should be small in order to improve feature estimation and reduce translation time.", "labels": [], "entities": [{"text": "feature estimation", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.617708221077919}]}, {"text": "Furthering an insight that informs SAMT, we show that combinatory categorial grammar (CCG) satisfies these requirements.", "labels": [], "entities": [{"text": "SAMT", "start_pos": 35, "end_pos": 39, "type": "TASK", "confidence": 0.9570807814598083}]}, {"text": "Under CCG, for the establishment of is labeled with ((S\\NP)\\(S\\NP))/NP.", "labels": [], "entities": []}, {"text": "This seems complex, but it describes exactly how the fragment should combine with other English words to create a complete sentence in a linguistically meaningful way.", "labels": [], "entities": []}, {"text": "We show that CCG is a viable formalism to add syntax to SCFG-based translation.", "labels": [], "entities": [{"text": "SCFG-based translation", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7524937987327576}]}, {"text": "\u2022 We introduce two models for labeling SCFG rules.", "labels": [], "entities": [{"text": "labeling SCFG", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.8058295249938965}]}, {"text": "One uses labels from a 1-best CCG parse tree of training data; the second uses the top labels in each cell of a CCG parse chart.", "labels": [], "entities": [{"text": "CCG parse tree of training data", "start_pos": 30, "end_pos": 61, "type": "DATASET", "confidence": 0.6726430157820383}]}, {"text": "\u2022 We show that using 1-best parses performs as well as a syntactic model using phrase structure derivations.", "labels": [], "entities": []}, {"text": "\u2022 We show that using chart cell labels performs almost as well than SAMT, but the nonterminal label set is an order of magnitude smaller and translation is twice as fast.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the scripts included with the Moses MT toolkit ( ) to tokenize and normalize the English data.", "labels": [], "entities": [{"text": "Moses MT toolkit", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.8375262022018433}]}, {"text": "We used a tokenizer and normalizer developed at the SCALE 2009 workshop () to preprocess the Urdu data.", "labels": [], "entities": [{"text": "SCALE 2009 workshop", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.6032487154006958}]}, {"text": "We used GIZA++) to perform word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7681303322315216}]}, {"text": "For phrase structure parses of the English data, we used the Berkeley parser.", "labels": [], "entities": [{"text": "phrase structure parses", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8029024998346964}]}, {"text": "For CCG parses, and for reading labels out of a parse chart, we used the C&C parser.", "labels": [], "entities": [{"text": "CCG parses", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.5059784948825836}]}, {"text": "After aligning and parsing the training data, we used the Thrax grammar extractor) to extract all of the translation grammars.", "labels": [], "entities": []}, {"text": "We used the same feature set in all the translation grammars.", "labels": [], "entities": [{"text": "translation grammars", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.9224951863288879}]}, {"text": "This includes, for each rule C \u2192 f ; e, relative-frequency estimates of the probabil-  ities p(f |A), p(f |e), p(f |e, A), p(e|A), p(e|f ), and p(e|f, A).", "labels": [], "entities": []}, {"text": "The feature set also includes lexical weighting for rules as defined by and various binary features as well as counters for the number of unaligned words in each rule.", "labels": [], "entities": []}, {"text": "To train the feature weights we used the Z-MERT implementation of the Minimum Error-Rate Training algorithm.", "labels": [], "entities": []}, {"text": "To decode the test sets, we used the Joshua machine translation decoder).", "labels": [], "entities": []}, {"text": "The language model is a 5-gram LM trained on English GigaWord Fourth Edition.", "labels": [], "entities": [{"text": "English GigaWord Fourth Edition", "start_pos": 45, "end_pos": 76, "type": "DATASET", "confidence": 0.7818229645490646}]}, {"text": "We measure machine translation performance using the BLEU metric ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7577880024909973}, {"text": "BLEU metric", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9738990664482117}]}, {"text": "We also report the translation time for the test set in seconds per sentence.", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9326558709144592}]}, {"text": "These results are shown in.", "labels": [], "entities": []}, {"text": "All of the syntactic labeling schemes show an improvement over the Hiero model.", "labels": [], "entities": []}, {"text": "Indeed, they all fall in the range of approximately 27-28 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.998065173625946}]}, {"text": "We can see that the 1-best derivation CCG model performs slightly better than the phrase structure model, and the CCG parse chart model performs a little better than that.", "labels": [], "entities": []}, {"text": "SAMT has the highest BLEU score.", "labels": [], "entities": [{"text": "SAMT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8314061760902405}, {"text": "BLEU score", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9675514996051788}]}, {"text": "The models with a larger number of rules perform better; this supports our assertion that we shouldn't throwaway too many rules.", "labels": [], "entities": []}, {"text": "When it comes to translation time, the three smaller models (Hiero, phrase structure syntax, and CCG 1-best derivations) are significantly faster than the two larger ones.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9710416197776794}, {"text": "Hiero", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.7377943396568298}]}, {"text": "However, even though the CCG parse chart model is almost 4 the size of SAMT in terms of number of rules, it doesn't take 4 of the", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of translation rules and non- terminal labels in an Urdu-English grammar under  various models.", "labels": [], "entities": []}, {"text": " Table 3: Results of translation experiments on  Urdu-English. Higher BLEU scores are better.  BLEU's brevity penalty is reported in parentheses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9988082647323608}, {"text": "BLEU's brevity penalty", "start_pos": 95, "end_pos": 117, "type": "METRIC", "confidence": 0.8388611376285553}]}]}