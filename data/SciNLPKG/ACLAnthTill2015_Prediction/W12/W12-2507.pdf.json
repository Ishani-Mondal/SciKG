{"title": [{"text": "Workshop on Computational Linguistics for Literature, pages 54-58, Mining wisdom", "labels": [], "entities": []}], "abstractContent": [{"text": "Simple text classification algorithms perform remarkably well when used for detecting famous quotes in literary or philosophical text, with f-scores approaching 95%.", "labels": [], "entities": [{"text": "text classification", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7197268158197403}, {"text": "detecting famous quotes in literary or philosophical text", "start_pos": 76, "end_pos": 133, "type": "TASK", "confidence": 0.8579222783446312}, {"text": "f-scores", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9709451794624329}]}, {"text": "We compare the task to topic classification, polarity classification and authorship attribution.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.8177326619625092}, {"text": "polarity classification", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.7131032347679138}]}], "introductionContent": [{"text": "Mark Twain famously said that 'the difference between the right word and the almost-right word is the difference between lightning and a lightning bug.'", "labels": [], "entities": []}, {"text": "Twain's quote is also about the importance of quotes.", "labels": [], "entities": []}, {"text": "A great quote can come in handy when you are looking to inspire people, make them laugh or persuade people to believe in a particular point of view.", "labels": [], "entities": []}, {"text": "Quotes are emblems that serve to remind us of philosophical or political stand-points, world views, perspectives that comfort or entertain us.", "labels": [], "entities": []}, {"text": "Famous quotes such as 'Cogito ergo sum' (Descartes) and 'God is dead' (Nietzsche) occur millions of times on the Internet.", "labels": [], "entities": []}, {"text": "The importance of quotes has motivated publishing houses to create and publish large collections of quotes.", "labels": [], "entities": []}, {"text": "In this process, the editor typically spends years reading philosophy books, literature, and interviews to find good quotes, but this process is both expensive and cumbersome.", "labels": [], "entities": []}, {"text": "In this paper, we consider the possibility of automatically learning what is a good quote, and what is not.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each data point is represented as a binary bag-ofwords -or bag-of-n-grams, really.", "labels": [], "entities": []}, {"text": "Our initial hypothesis was to include stop words and keep infor-mation about case (capital letters).", "labels": [], "entities": []}, {"text": "Stop words are extremely important to distinguish between literary styles, and we speculated that quotes can be distinguished from ordinary text in part by their style.", "labels": [], "entities": []}, {"text": "We also speculated that there would be a tendency to capitalize some words in quotes, e.g. 'God', 'the Other', or 'the World'.", "labels": [], "entities": []}, {"text": "Finally, we hypothesized that including more context would be beneficial.", "labels": [], "entities": []}, {"text": "Our intuition was that sometimes larger chunks such as 'He who' may indicate that a sentence is a quote without the component words being indicative of that in anyway.", "labels": [], "entities": []}, {"text": "To evaluate these hypotheses we considered a logistic regression classifier over bag-of-word representations of the quotes and our neutral sentences.", "labels": [], "entities": []}, {"text": "We used a publicly available implementation 3 of limited memory L-BFGS to find the weights that maximize the log-likelihood of the training data: where w \u00b7 x is the dot product of weights and binary features in the usual way.", "labels": [], "entities": []}, {"text": "We prefer logistic regression over Naive Bayes, since logistic regression is more resistant to possible dependencies between variables.", "labels": [], "entities": []}, {"text": "The conditional likelihood maximization in logistic regression will adjust its parameters to maximize the fit even when the resulting parameters are inconsistent with the Naive Bayes assumption.", "labels": [], "entities": []}, {"text": "Finally, logistic regression is less sensitive to parameter tuning than SVMs, so to avoid expensive parameter optimization we settled for logistic regression.", "labels": [], "entities": []}, {"text": "To test the importance of case, we did experiments with and without lowercasing of all words.", "labels": [], "entities": []}, {"text": "To test the importance of stop words, we did experiments where stop words had been removed from the texts in advance.", "labels": [], "entities": []}, {"text": "We also considered models with bigrams and trigrams to test the impact of bigger units of text (context).", "labels": [], "entities": []}, {"text": "Finally, we varied the size of the dataset to obtain a learning curve suggesting how our model would perform in the limit.", "labels": [], "entities": []}], "tableCaptions": []}