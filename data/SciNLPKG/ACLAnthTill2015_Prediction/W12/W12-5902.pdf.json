{"title": [{"text": "Report of the Shared Task on Learning Reordering from Word Alignments at RSMT 2012", "labels": [], "entities": [{"text": "Shared Task on Learning Reordering from Word Alignments at RSMT", "start_pos": 14, "end_pos": 77, "type": "TASK", "confidence": 0.7797517836093902}]}], "abstractContent": [{"text": "Several studies have shown that the task of reordering source sentences to match the target order is crucial to improve the performance of Statistical Machine Translation, especially when the source and target languages have significantly divergent grammatical structures.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 139, "end_pos": 170, "type": "TASK", "confidence": 0.8925295869509379}]}, {"text": "In fact, it is now become a standard practice to include reordering as a pre-processing step or as an integrated module (within the decoder).", "labels": [], "entities": []}, {"text": "However, despite the importance of this sub-task, there is no forum dedicated for its evaluation.", "labels": [], "entities": []}, {"text": "The objective of this Shared Task was to provide a common benchmarking platform to evaluate state of the art approaches for reordering.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The output reorderings were evaluated using two metrics: \u2022 BLEU (): In the past decade, BLEU has been the most widely used metric for MT evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9989006519317627}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9985499978065491}, {"text": "MT evaluation", "start_pos": 134, "end_pos": 147, "type": "TASK", "confidence": 0.958740770816803}]}, {"text": "BLEU compares N-grams in the output translation and the reference translation(s), and uses a \"brevity penalty\" to prevent outputs that are accurate in terms of N-gram match, but too short.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9677318930625916}]}, {"text": "For reordering, we use the BLEU metric by comparing candidate reorderings with the reference reorderings that we create from the hand-alignments.", "labels": [], "entities": [{"text": "reordering", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9670715928077698}, {"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.99801105260849}]}, {"text": "BLEU is calculated as: where, N = 4(unigrams, bigrams, trigrams, and 4-grams are matched) r = length of reference reordering c = length of candidate reordering and where C runs over the entire set of candidate reorderings, and Count clip returns the number of n-grams that match in the reference reordering.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9877393841743469}, {"text": "length", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.970995306968689}, {"text": "length", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9525559544563293}, {"text": "Count clip", "start_pos": 227, "end_pos": 237, "type": "METRIC", "confidence": 0.9672490060329437}]}, {"text": "\u2022 LRscore (Birch and Osborne, 2010): LRscore was introduced a couple of years ago as a metric to directly measure reordering performance.", "labels": [], "entities": []}, {"text": "LRscore uses a distance score in conjunction with BLEU to help evaluate the word order of MT outputs better.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9371331930160522}, {"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9974422454833984}, {"text": "MT outputs", "start_pos": 90, "end_pos": 100, "type": "TASK", "confidence": 0.9149622321128845}]}, {"text": "Experiments show that this combined metric correlates better with human judgments than BLEU alone (Birch and Osborne, 2010).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9987198114395142}]}, {"text": "Since we do not need a lexical metric, we use only the distance metric from LRscore.", "labels": [], "entities": []}, {"text": "We will evaluate reordering distance using the following two scores: -Hamming distance: This measures the number of disagreements between two permutations: This measures the minimum number of transpositions of two adjacent symbols necessary to transform one permutation into another: , where z ij = 1 if \u03c0(i) < \u03c0(j) and \u03c1(i) > \u03c1(j) These two distance metrics will be combined with a brevity penalty (as defined in the description of BLEU above).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 433, "end_pos": 437, "type": "METRIC", "confidence": 0.9050374031066895}]}, {"text": "Links to these evaluation scripts are provided on the workshop webpage 1 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: mBLEU scores of different systems that participated in the Shared Task.", "labels": [], "entities": []}, {"text": " Table 3: LR Hamming scores of different systems that participated in the Shared Task.", "labels": [], "entities": [{"text": "LR Hamming", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.5602684020996094}]}, {"text": " Table 4: LR Kendall scores of different systems that participated in the Shared Task.", "labels": [], "entities": [{"text": "LR Kendall scores", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7403913338979086}]}]}