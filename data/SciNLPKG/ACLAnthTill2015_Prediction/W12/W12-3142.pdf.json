{"title": [], "abstractContent": [{"text": "This paper describes the UPM system for the Spanish-English translation task at the NAACL 2012 workshop on statistical machine translation.", "labels": [], "entities": [{"text": "UPM", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.948477566242218}, {"text": "Spanish-English translation task at the NAACL 2012 workshop on statistical machine translation", "start_pos": 44, "end_pos": 138, "type": "TASK", "confidence": 0.6003066872557005}]}, {"text": "This system is based on Mo-ses.", "labels": [], "entities": []}, {"text": "We have used all available free corpora, cleaning and deleting some repetitions.", "labels": [], "entities": []}, {"text": "In this paper, we also propose a technique for selecting the sentences for tuning the system.", "labels": [], "entities": []}, {"text": "This technique is based on the similarity with the sentences to translate.", "labels": [], "entities": []}, {"text": "With our approach, we improve the BLEU score from 28.37% to 28.57%.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.962141364812851}]}, {"text": "And as a result of the WMT12 challenge we have obtained a 31.80% BLEU with the 2012 test set.", "labels": [], "entities": [{"text": "WMT12 challenge", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.5589594393968582}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9997263550758362}, {"text": "2012 test set", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.8190386692682902}]}, {"text": "Finally, we explain different experiments that we have carried out after the competition.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Speech Technology Group at the Technical University of Madrid has participated in the seventh workshop on statistical machine translation in the Spanish-English translation task.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 110, "end_pos": 141, "type": "TASK", "confidence": 0.6601635118325552}, {"text": "Spanish-English translation task", "start_pos": 149, "end_pos": 181, "type": "TASK", "confidence": 0.7892096141974131}]}, {"text": "Our submission is based on the state-of-the-art SMT toolkit Moses ( . Firstly, we have proved different corpora for training the system: cleaning the whole corpus and deleting some repetitions in order to have a better performance of the translation model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9520241022109985}]}, {"text": "There are several related works on filtering the training corpus by removing noisy data that use a similarity measure based on the alignment score or based on sentences length (.", "labels": [], "entities": []}, {"text": "In this paper, we also propose a technique for selecting the most appropriate sentences for tuning the system, based on the similarity with the Spanish sentences to translate.", "labels": [], "entities": []}, {"text": "This technique is an update of the technique proposed by our group in the last WMT11 challenge ().", "labels": [], "entities": [{"text": "WMT11 challenge", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.7586104869842529}]}, {"text": "There are other works related to select the development set () that combine different development sets in order to find the more similar one with test set.", "labels": [], "entities": []}, {"text": "There are also works related to select sentences, but for training instead of tuning, based on the similarity with the source test sentences.", "labels": [], "entities": []}, {"text": "Some of them are based on transductive learning: semisupervised methods for the effective use of monolingual data from the source language in order to improve translation quality; methods using instance selection with feature decay algorithms (; or using TF-IDF algorithm (.", "labels": [], "entities": []}, {"text": "There are also works based on selecting training material with active learning: using language model adaptation); or perplexity-based methods (.", "labels": [], "entities": []}, {"text": "In this work, we have used the proposed selection method only for tuning.", "labels": [], "entities": [{"text": "tuning", "start_pos": 66, "end_pos": 72, "type": "TASK", "confidence": 0.9547648429870605}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Next section overviews the system.", "labels": [], "entities": []}, {"text": "Section 3 describes the used corpora.", "labels": [], "entities": []}, {"text": "Section 4 explains the experiments carried out before the competition.", "labels": [], "entities": []}, {"text": "Section 5 describes the sentences selection technique for tuning.", "labels": [], "entities": [{"text": "sentences selection", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7604252099990845}]}, {"text": "Section 6 summarizes the results: before the WMT12 challenge, the corresponding to the competition and the last experiments.", "labels": [], "entities": [{"text": "WMT12 challenge", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.603493332862854}]}, {"text": "Finally, section 7 shows the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the system development, only the free corpora distributed in the NAACL 2012 translation task has been used, so any researcher can validate these experiments easily.", "labels": [], "entities": [{"text": "NAACL 2012 translation task", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.7815191000699997}]}, {"text": "In order to train the translation model, we used the union of the Europarl corpus, the United Nations Organization (UNO) corpus and the News Commentary corpus.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9624815583229065}, {"text": "Europarl corpus", "start_pos": 66, "end_pos": 81, "type": "DATASET", "confidence": 0.9946272671222687}, {"text": "United Nations Organization (UNO) corpus", "start_pos": 87, "end_pos": 127, "type": "DATASET", "confidence": 0.5012572620596204}, {"text": "News Commentary corpus", "start_pos": 136, "end_pos": 158, "type": "DATASET", "confidence": 0.8442334532737732}]}, {"text": "A 5-gram language model was built joining the following monolingual corpora: Europarl, News commentary, United Nations and News Crawl.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9499670267105103}, {"text": "United Nations", "start_pos": 104, "end_pos": 118, "type": "DATASET", "confidence": 0.8654330372810364}]}, {"text": "We have not used the Gigaword corpus.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.9605042040348053}]}, {"text": "In order to tune the model weights, the 2010 and 2011 test set were used for development.", "labels": [], "entities": [{"text": "2010 and 2011 test set", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.7678786873817444}]}, {"text": "We did not use the complete set, but a sentences selection in order to improve the tuning process.", "labels": [], "entities": []}, {"text": "This selection will be explained in section 5.", "labels": [], "entities": []}, {"text": "The main characteristics of the corpora are shown in.", "labels": [], "entities": []}, {"text": "All the parallel corpora has been cleaned with clean-corpus-n.perl, lowercased with lowercase.perl and tokenized with tokenizer.perl.", "labels": [], "entities": []}, {"text": "All these tools can be also free downloaded from http://www.statmt.org/wmt12/.", "labels": [], "entities": []}, {"text": "We observed that the parallel corpora, specially the UNO corpus, have many repeated sentences.", "labels": [], "entities": [{"text": "UNO corpus", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.9483721852302551}]}, {"text": "We noted that these repetitions can cause a bad training.", "labels": [], "entities": [{"text": "repetitions", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.908661425113678}]}, {"text": "So, after cleaning the parallel corpora with the clean-corpus-n.perl tool, we eliminated all repetitions that appear more than 3 times in the parallel corpus.: Size of the corpora used in our experiments  Several experiments were carried out by using different number of sentences, as it is shown in.", "labels": [], "entities": []}, {"text": "In these experiments, we used the 2010 test set for tuning (news-test2010) and the 2011 test set for test (news-test2011).", "labels": [], "entities": []}, {"text": "And a 5-gram language model was built with the IRSTLM tool.", "labels": [], "entities": []}, {"text": "For evaluating the performance of the translation system, the BLEU (BiLingual Evaluation Understudy) metric Europarl has been computed using the NIST tool (mteval.pl) ().", "labels": [], "entities": [{"text": "BLEU (BiLingual Evaluation Understudy) metric", "start_pos": 62, "end_pos": 107, "type": "METRIC", "confidence": 0.8730306029319763}, {"text": "Europarl", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.6864851117134094}]}, {"text": "Firstly, we checked the contribution of UNO corpus in the final result.", "labels": [], "entities": [{"text": "UNO corpus", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.9498269557952881}]}, {"text": "As it is shown in, the results improve when we add the UNO corpus, although this difference is small compared to the increasing of number of sentences: with 1,643,597 sentences we have a 28.24% BLEU and if we add around other 8 million sentences more, the BLEU score only increase 0.13 points (28.37%).: Previous experiments using newstest2010 for tuning and news-test2011 as test set We observed that UNO corpus have a lot of repeated sentences.", "labels": [], "entities": [{"text": "UNO corpus", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.9140366315841675}, {"text": "BLEU", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.998626708984375}, {"text": "BLEU", "start_pos": 256, "end_pos": 260, "type": "METRIC", "confidence": 0.9991862177848816}, {"text": "UNO corpus", "start_pos": 402, "end_pos": 412, "type": "DATASET", "confidence": 0.899695098400116}]}, {"text": "So, we decided to remove repetitions in the whole corpus.", "labels": [], "entities": []}, {"text": "With this action, we aimed to keep the UNO sentences that let us to improve the BLEU score and, on the other hand, to delete the sentences that do not contribute in anyway, reducing the training time.", "labels": [], "entities": [{"text": "UNO sentences", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.8010386526584625}, {"text": "BLEU score", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9818765819072723}]}, {"text": "We did some experiments deleting repetitions: allowing 5 repetitions, 3 repetitions and, finally, 1 repetition (no repetitions).", "labels": [], "entities": [{"text": "repetitions", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.973518431186676}]}, {"text": "shows how the results improve deleting more than 3 repetitions.", "labels": [], "entities": []}, {"text": "So, finally, we improved the BLEU score from 23.24% without UNO corpus to 28.37% adding the UNO and to 28.47% deleting all sentences repeated more than 3 times.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9802549779415131}, {"text": "UNO corpus", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.8363338112831116}, {"text": "UNO", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.7373677492141724}]}, {"text": "We have carried out other experiments with the 2012 test set: factored models, Minimum Bayes Risk Decoding (MBR) and other sets for tuning.", "labels": [], "entities": [{"text": "2012 test set", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9504767457644144}, {"text": "Minimum Bayes Risk Decoding (MBR)", "start_pos": 79, "end_pos": 112, "type": "METRIC", "confidence": 0.806087063891547}]}, {"text": "However, they did not finish before the competition deadline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of the corpora used in our experi- ments", "labels": [], "entities": []}, {"text": " Table 2: Previous experiments using news- test2010 for tuning and news-test2011 as test set", "labels": [], "entities": []}, {"text": " Table 3: Results with different number of devel- opment sentences", "labels": [], "entities": []}, {"text": " Table 4: Final results of the translation system", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9735801219940186}]}, {"text": " Table 5: Freeling analyzer output", "labels": [], "entities": [{"text": "Freeling analyzer", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7255717217922211}]}, {"text": " Table 6: Results of the experiments after competi- tion", "labels": [], "entities": []}]}