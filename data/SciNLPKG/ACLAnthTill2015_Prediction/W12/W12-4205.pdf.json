{"title": [{"text": "Using Parallel Features in Parsing of Machine-Translated Sentences for Correction of Grammatical Errors *", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we present two dependency parser training methods appropriate for parsing outputs of statistical machine translation (SMT), which pose problems to standard parsers due to their frequent ungrammatical-ity.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.6546857059001923}, {"text": "parsing outputs of statistical machine translation (SMT)", "start_pos": 81, "end_pos": 137, "type": "TASK", "confidence": 0.7903622190157572}]}, {"text": "We adapt the MST parser by exploiting additional features from the source language, and by introducing artificial grammatical errors in the parser training data, so that the training sentences resemble SMT output.", "labels": [], "entities": [{"text": "MST parser", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.83022540807724}, {"text": "SMT", "start_pos": 202, "end_pos": 205, "type": "TASK", "confidence": 0.9709034562110901}]}, {"text": "We evaluate the modified parser on DEP-FIX, a system that improves English-Czech SMT outputs using automatic rule-based corrections of grammatical mistakes which requires parsed SMT output sentences as its input.", "labels": [], "entities": [{"text": "DEP-FIX", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9033766388893127}, {"text": "SMT outputs", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.9018527269363403}, {"text": "SMT output sentences", "start_pos": 178, "end_pos": 198, "type": "TASK", "confidence": 0.8452960650126139}]}, {"text": "Both parser modifications led to improvements in BLEU score; their combination was evaluated manually, showing a statistically significant improvement of the translation quality.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9792831540107727}]}], "introductionContent": [{"text": "The machine translation (MT) quality is on a steady rise, with mostly statistical systems (SMT) dominating the area).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8464370012283325}]}, {"text": "Most MT systems do not employ structural linguistic knowledge and even the stateof-the-art MT solutions are unable to avoid making serious grammatical errors in the output, which often leads to unintelligibility or to a risk of misinterpretations of the text by a reader.", "labels": [], "entities": [{"text": "MT", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9788933396339417}, {"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9566561579704285}]}, {"text": "This problem is particularly apparent in target languages with rich morphological inflection, such as Czech.", "labels": [], "entities": []}, {"text": "As Czech often conveys the relations between individual words using morphological agreement instead of word order, together with the word order itself being relatively free, choosing the correct inflection becomes crucial.", "labels": [], "entities": []}, {"text": "Since the output of phrase-based SMT shows frequent inflection errors (even in adjacent words) due to each word belonging to a different phrase, a possible way to address the grammaticality problem is a combination of statistical and structural approach, such as SMT output post-editing.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.7765647172927856}, {"text": "SMT output post-editing", "start_pos": 263, "end_pos": 286, "type": "TASK", "confidence": 0.8622695008913676}]}, {"text": "In this paper, we focus on improving SMT output parsing quality, as rule-based post-editing systems rely heavily on the quality of SMT output analysis.", "labels": [], "entities": [{"text": "SMT output parsing", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.9315933187802633}, {"text": "SMT output analysis", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.9405800302823385}]}, {"text": "Parsers trained on gold standard parse trees often fail to produce the expected result when applied to SMT output with grammatical errors.", "labels": [], "entities": [{"text": "SMT output", "start_pos": 103, "end_pos": 113, "type": "TASK", "confidence": 0.891842782497406}]}, {"text": "This is partly caused by the fact that when parsing highly inflected free word-order languages the parsers have to rely on morphological agreement, which, as stated above, is often erroneous in SMT output.", "labels": [], "entities": [{"text": "parsing highly inflected free word-order languages", "start_pos": 44, "end_pos": 94, "type": "TASK", "confidence": 0.8626754581928253}, {"text": "SMT output", "start_pos": 194, "end_pos": 204, "type": "TASK", "confidence": 0.9161504209041595}]}, {"text": "Training a parser specifically by creating a manually annotated treebank of MT systems' outputs would be very expensive, and the application of such treebank to other MT systems than the ones used for its generation would be problematic.", "labels": [], "entities": []}, {"text": "We address this issue by two methods of increasing the quality of SMT output parsing: \u2022 a different application of previous works on bitext parsing -exploiting additional features from the source language (Section 3), and \u2022 introducing artificial grammatical errors in the target language parser training data, so that the sentences resemble the SMT output in some ways.", "labels": [], "entities": [{"text": "SMT output parsing", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9534370104471842}, {"text": "bitext parsing", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.5854598581790924}, {"text": "SMT", "start_pos": 346, "end_pos": 349, "type": "TASK", "confidence": 0.9383082389831543}]}, {"text": "This technique is, to our knowledge, novel with regards to its application to SMT and the statistical error model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9970554113388062}]}, {"text": "We test these two techniques on English-Czech MT outputs using our own reimplementation of the MST parser () named RUR 1 parser. and evaluate their contribution to the SMT post-editing quality of the DEPFIX system), which we outline in Section 5.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.895253449678421}, {"text": "RUR", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9167481660842896}, {"text": "SMT post-editing", "start_pos": 168, "end_pos": 184, "type": "TASK", "confidence": 0.8644427359104156}, {"text": "DEPFIX", "start_pos": 200, "end_pos": 206, "type": "DATASET", "confidence": 0.8625612258911133}]}, {"text": "We describe the experiments carried out and present the most important results in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 then concludes the paper and indicates more possibilities of further improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate RUR parser indirectly by using it in the DEPFIX system and measuring the performance of the whole system.", "labels": [], "entities": [{"text": "RUR parser", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.7258343398571014}, {"text": "DEPFIX system", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9160871207714081}]}, {"text": "This approach has been chosen instead of direct evaluation of the SMT output parse trees, as the task of finding a correct parse tree of a possibly grammatically incorrect sentence is not well defined and considerably difficult to do.", "labels": [], "entities": [{"text": "SMT output parse", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.9264951944351196}]}, {"text": "We used WMT10, WMT11 and WMT12 English to Czech translation test sets, newssyscombtest2010, newssyscombtest2011 and newstest2012, 8 (denoted as WMT10, WMT11 and 8 http://www.statmt.org/wmt10, WMT12) for the automatic evaluation.", "labels": [], "entities": [{"text": "WMT10", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.9698522686958313}, {"text": "WMT11", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.8951650857925415}, {"text": "WMT12 English to Czech translation test sets", "start_pos": 25, "end_pos": 69, "type": "DATASET", "confidence": 0.8411028214863369}, {"text": "WMT10", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.9611917734146118}, {"text": "WMT11", "start_pos": 151, "end_pos": 156, "type": "DATASET", "confidence": 0.8708609342575073}, {"text": "WMT12", "start_pos": 192, "end_pos": 197, "type": "DATASET", "confidence": 0.9064732789993286}]}, {"text": "The data sets include the source (English) text, its reference translation and translations produced by several MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.8718351721763611}]}, {"text": "We used the outputs of three SMT systems: GOOGLE, 9 UEDIN () and BOJAR.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9704411625862122}, {"text": "GOOGLE", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.6955440044403076}, {"text": "UEDIN", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.8931294083595276}, {"text": "BOJAR", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9949009418487549}]}, {"text": "For the manual evaluation, two sets of 1000 randomly selected sentences from WMT11 and from WMT12 translated by GOOGLE were used.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.9783446788787842}, {"text": "WMT12", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.9069783091545105}, {"text": "GOOGLE", "start_pos": 112, "end_pos": 118, "type": "DATASET", "confidence": 0.6211109757423401}]}, {"text": "We have computed 95% confidence intervals on 1000 bootstrap samples, which showed that the BLEU score of RUR+WORS+PARA was significantly higher than that of MCD and RUR parser in 4 and 3 cases, respectively (results where RUR+WORS+PARA achieved a significantly higher score are marked with '*').", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9837923645973206}, {"text": "RUR+WORS+PARA", "start_pos": 105, "end_pos": 118, "type": "METRIC", "confidence": 0.7684134244918823}]}, {"text": "On the other hand, the score of neither RUR+WORS+PARA nor RUR+WORS and RUR+PARA was ever significantly lower than the score of MCD or RUR parser.", "labels": [], "entities": [{"text": "RUR+WORS+PARA", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.7356411218643188}, {"text": "PARA", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.6527883410453796}]}, {"text": "This leads us to believe that the two proposed methods are able to produce slightly better SMT output parsing results.: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and processed by DEPFIX.", "labels": [], "entities": [{"text": "SMT output parsing", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.9380036989847819}, {"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9988524913787842}, {"text": "SMT", "start_pos": 178, "end_pos": 181, "type": "TASK", "confidence": 0.976743221282959}, {"text": "BOJAR", "start_pos": 200, "end_pos": 205, "type": "METRIC", "confidence": 0.9653595685958862}, {"text": "GOOGLE", "start_pos": 207, "end_pos": 213, "type": "METRIC", "confidence": 0.7906002402305603}, {"text": "WMT10", "start_pos": 235, "end_pos": 240, "type": "DATASET", "confidence": 0.9628634452819824}, {"text": "WMT11", "start_pos": 242, "end_pos": 247, "type": "DATASET", "confidence": 0.5899654030799866}, {"text": "WMT12 test sets", "start_pos": 252, "end_pos": 267, "type": "DATASET", "confidence": 0.8621782064437866}, {"text": "SMT output parsed", "start_pos": 278, "end_pos": 295, "type": "TASK", "confidence": 0.9318340420722961}, {"text": "DEPFIX", "start_pos": 338, "end_pos": 344, "type": "DATASET", "confidence": 0.8921395540237427}]}, {"text": "The score of RUR+WORS+PARA is significantly higher at 95% confidence level than the scores marked with '*' on the same data.", "labels": [], "entities": [{"text": "RUR+WORS+", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.8226286470890045}, {"text": "PARA", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.7242276668548584}]}, {"text": "Performance of RUR+WORS+PARA setup was manually evaluated by doing a pairwise comparison with other setups -SMT output, MCD and RUR parser.", "labels": [], "entities": [{"text": "PARA", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.632612943649292}, {"text": "SMT output", "start_pos": 108, "end_pos": 118, "type": "TASK", "confidence": 0.829370766878128}]}, {"text": "The evaluation was performed on both the WMT11) and WMT12) test set.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.9157240390777588}, {"text": "WMT12) test set", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9149205535650253}]}, {"text": "1000 sentences from the output of the GOOGLE system were randomly selected and processed by DEPFIX, using the aforementioned SMT output parsers.", "labels": [], "entities": [{"text": "GOOGLE system", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.8289839029312134}, {"text": "DEPFIX", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.8877766728401184}, {"text": "SMT output parsers", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.8659343719482422}]}, {"text": "The annotators then compared the translation quality of the individual variants in differing sentences, selecting the better variant from a pair or declaring two variants \"same quality\" (indefinite).", "labels": [], "entities": []}, {"text": "They were also provided with the source sentence and a reference translation.", "labels": [], "entities": []}, {"text": "The evaluation was done as a blind test, with the sentences randomly shuffled.", "labels": [], "entities": []}, {"text": "The WMT11 test set was evaluated by two independent annotators.", "labels": [], "entities": [{"text": "WMT11 test set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9322573939959208}]}, {"text": "(The WMT12 test set was evaluated by one annotator only.)", "labels": [], "entities": [{"text": "WMT12 test set", "start_pos": 5, "end_pos": 19, "type": "DATASET", "confidence": 0.9192503889401754}]}, {"text": "The inter-annotator agreement and Cohen's kappa coefficient (Cohen and others, 1960), shown in, were computed both including all annotations (\"with indefs\"), and disregarding sentences whereat least one of the annotators marked the difference as indefinite (\"without indefs\") -we believe a disagreement in choosing the better translation to be more severe than a disagreement in deciding whether the difference in quality of the translations allows to mark one as being better.", "labels": [], "entities": []}, {"text": "For both of the test sets, RUR+WORS+PARA significantly outperforms both MCD and RUR baseline, confirming that a combination of the proposed modifications of the parser lead to its better performance.", "labels": [], "entities": [{"text": "RUR+WORS+", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.7817903012037277}, {"text": "PARA", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.5632704496383667}, {"text": "RUR baseline", "start_pos": 80, "end_pos": 92, "type": "METRIC", "confidence": 0.9610410630702972}]}, {"text": "confirmed by a one-sided pairwise t-test, with the following differences ranking: RUR+WORS+PARA better = 1, baseline better = -1, indefinite = 0.", "labels": [], "entities": [{"text": "RUR+WORS+PARA better", "start_pos": 82, "end_pos": 102, "type": "METRIC", "confidence": 0.8195203046003977}]}], "tableCaptions": [{"text": " Table 2: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and  UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and  processed by DEPFIX. The score of RUR+WORS+PARA is significantly higher at 95% confidence level than the scores  marked with '*' on the same data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9982940554618835}, {"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9911764860153198}, {"text": "BOJAR", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.9644904136657715}, {"text": "GOOGLE", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.6982123851776123}, {"text": "WMT10", "start_pos": 126, "end_pos": 131, "type": "DATASET", "confidence": 0.9762173295021057}, {"text": "WMT11", "start_pos": 133, "end_pos": 138, "type": "DATASET", "confidence": 0.6717631816864014}, {"text": "WMT12 test sets", "start_pos": 143, "end_pos": 158, "type": "DATASET", "confidence": 0.8806924422581991}, {"text": "SMT output parsed", "start_pos": 169, "end_pos": 186, "type": "TASK", "confidence": 0.9204923510551453}, {"text": "DEPFIX", "start_pos": 230, "end_pos": 236, "type": "DATASET", "confidence": 0.8538298606872559}, {"text": "RUR+WORS+", "start_pos": 251, "end_pos": 260, "type": "METRIC", "confidence": 0.7809655964374542}, {"text": "PARA", "start_pos": 260, "end_pos": 264, "type": "METRIC", "confidence": 0.570342481136322}]}, {"text": " Table 4: Manual comparison of RUR+WORS+PARA with various baselines, on 1000 sentences from WMT11 data set  translated by GOOGLE, evaluated by two independent annotators.", "labels": [], "entities": [{"text": "RUR+WORS+", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.7505194246768951}, {"text": "PARA", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.5620927214622498}, {"text": "WMT11 data set  translated", "start_pos": 92, "end_pos": 118, "type": "DATASET", "confidence": 0.9783580750226974}, {"text": "GOOGLE", "start_pos": 122, "end_pos": 128, "type": "DATASET", "confidence": 0.6039602160453796}]}, {"text": " Table 5: Manual comparison of RUR+WORS+PARA with various baselines, on 1000 sentences from WMT12 data set  translated by GOOGLE.", "labels": [], "entities": [{"text": "RUR+WORS+", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.7548534125089645}, {"text": "PARA", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.5396565198898315}, {"text": "WMT12 data set  translated", "start_pos": 92, "end_pos": 118, "type": "DATASET", "confidence": 0.9779091328382492}, {"text": "GOOGLE", "start_pos": 122, "end_pos": 128, "type": "DATASET", "confidence": 0.7992779016494751}]}]}