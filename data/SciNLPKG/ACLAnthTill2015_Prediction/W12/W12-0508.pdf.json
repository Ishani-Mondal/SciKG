{"title": [{"text": "A Joint Named Entity Recognition and Entity Linking System", "labels": [], "entities": [{"text": "Joint Named Entity Recognition and Entity Linking", "start_pos": 2, "end_pos": 51, "type": "TASK", "confidence": 0.5538174935749599}]}], "abstractContent": [{"text": "We present a joint system for named entity recognition (NER) and entity linking (EL), allowing for named entities mentions extracted from textual data to be matched to uniquely identifiable entities.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.8395118316014608}, {"text": "entity linking (EL)", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8255027770996094}]}, {"text": "Our approach relies on combined NER modules which transfer the disambiguation step to the EL component, where referential knowledge about entities can be used to select a correct entity reading.", "labels": [], "entities": []}, {"text": "Hybridation is a main feature of our system, as we have performed experiments combining two types of NER, based respectively on symbolic and statistical techniques.", "labels": [], "entities": []}, {"text": "Furthermore, the statistical EL module relies on entity knowledge acquired over a large news corpus using a simple rule-base disambiguation tool.", "labels": [], "entities": []}, {"text": "An implementation of our system is described, along with experiments and evaluation results on French news wires.", "labels": [], "entities": [{"text": "French news wires", "start_pos": 95, "end_pos": 112, "type": "DATASET", "confidence": 0.9726837873458862}]}, {"text": "Linking accuracy reaches up to 87%, and the NER F-score up to 83%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9619603753089905}, {"text": "NER", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.5298802852630615}, {"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.751858651638031}]}], "introductionContent": [], "datasetContent": [{"text": "We use a gold corpus of 96 AFP news items intended for both NER and EL purposes: the manual annotation includes mention boundaries as well as an entity identifier for each mention, corresponding to an Aleda entry when present or the normalized name of the entity otherwise.", "labels": [], "entities": [{"text": "AFP news items", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.8601816296577454}]}, {"text": "This allows for the model learning to take into account cases of out-of-base entities.", "labels": [], "entities": []}, {"text": "This corpus contains 1,476 mentions, 437 distinct Aleda's entries and 173 entities absent from Aleda.", "labels": [], "entities": [{"text": "Aleda", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.9066676497459412}]}, {"text": "All news items in this corpus are dated In order for the model to learn from cases of not-an-entity, the training examples were augmented with false matches from the NER step, associated with this special candidate and the positive class prediction, while other possible candidates were associated with the negative class.", "labels": [], "entities": []}, {"text": "Using a 10-fold cross-validation, we used this corpus for both training and evaluation of our joint NER and EL system.", "labels": [], "entities": [{"text": "NER", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.7626937031745911}]}, {"text": "It should be observed that the learning step concerns the ranking of candidates fora given mention and context, while the final purpose of our system is the ranking of multiple readings of sentences, which takes place after the application of our ranking model for mention candidates.", "labels": [], "entities": []}, {"text": "Thus our system is evaluated according to its ability to choose the right reading, considering both NER recall and precision and EL accuracy, and not only the latter.", "labels": [], "entities": [{"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9022138118743896}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9988910555839539}, {"text": "EL", "start_pos": 129, "end_pos": 131, "type": "METRIC", "confidence": 0.9976761937141418}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.7338436841964722}]}], "tableCaptions": [{"text": " Table 1: Structure of Entities Entries and Variants in Aleda", "labels": [], "entities": [{"text": "Aleda", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.5884730815887451}]}, {"text": " Table 3: Joint NER and EL results. Each EL accuracy covers a different set of correctly detected mentions", "labels": [], "entities": [{"text": "NER", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.7418836355209351}, {"text": "EL", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.8733788132667542}, {"text": "EL accuracy", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.6852923631668091}]}]}