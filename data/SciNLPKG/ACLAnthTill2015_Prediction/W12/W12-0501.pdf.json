{"title": [{"text": "Experiments on Hybrid Corpus-Based Sentiment Lexicon Acquisition", "labels": [], "entities": [{"text": "Sentiment Lexicon Acquisition", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.8015568057696024}]}], "abstractContent": [{"text": "Numerous sentiment analysis applications make usage of a sentiment lexicon.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.9255509078502655}]}, {"text": "In this paper we present experiments on hybrid sentiment lexicon acquisition.", "labels": [], "entities": [{"text": "hybrid sentiment lexicon acquisition", "start_pos": 40, "end_pos": 76, "type": "TASK", "confidence": 0.7942063212394714}]}, {"text": "The approach is corpus-based and thus suitable for languages lacking general dictionary-based resources.", "labels": [], "entities": []}, {"text": "The approach is a hybrid two-step process that combines semi-supervised graph-based algorithms and supervised models.", "labels": [], "entities": []}, {"text": "We evaluate the performance on three tasks that capture different aspects of a sentiment lexicon: polarity ranking task, polarity regression task, and sentiment classification task.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.8615765869617462}]}, {"text": "Extensive evaluation shows that the results are comparable to those of a well-known sentiment lexicon SentiWordNet on the polarity ranking task.", "labels": [], "entities": []}, {"text": "On the sentiment classification task, the results are also comparable to SentiWordNet when restricted to monosen-timous (all senses carry the same sentiment) words.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 7, "end_pos": 36, "type": "TASK", "confidence": 0.9221585591634115}]}, {"text": "This is satisfactory, given the absence of explicit semantic relations between words in the corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowing someone's attitude towards events, entities, and phenomena can be very important in various areas of human activity.", "labels": [], "entities": []}, {"text": "Sentiment analysis is an area of computational linguistics that aims to recognize the subjectivity and attitude expressed in natural language texts.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9240357279777527}]}, {"text": "Applications of sentiment analysis are numerous, including sentiment-based document classification (), opinion-oriented information extraction (, and question answering (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.959128201007843}, {"text": "sentiment-based document classification", "start_pos": 59, "end_pos": 98, "type": "TASK", "confidence": 0.7263962427775065}, {"text": "opinion-oriented information extraction", "start_pos": 103, "end_pos": 142, "type": "TASK", "confidence": 0.6225748856862386}, {"text": "question answering", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.8670042157173157}]}, {"text": "Sentiment analysis combines subjectivity analysis and polarity analysis.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9223440885543823}]}, {"text": "Subjectivity analysis answers whether the text unit is subjective or neutral, while polarity analysis determines whether a subjective text unit is positive or negative.", "labels": [], "entities": []}, {"text": "The majority of research approaches) see subjectivity and polarity as categorical terms (i.e., classification problems).", "labels": [], "entities": []}, {"text": "Intuitively, not all words express the sentiment with the same intensity.", "labels": [], "entities": []}, {"text": "Accordingly, there has been some research effort in assessing subjectivity and polarity as graded values ().", "labels": [], "entities": []}, {"text": "Most of the work on sentence or document level sentiment makes usage of sentiment annotated lexicon providing subjectivity and polarity information for individual words.", "labels": [], "entities": [{"text": "sentence or document level sentiment", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.6904400944709778}]}, {"text": "In this paper we present a hybrid approach for automated acquisition of sentiment lexicon.", "labels": [], "entities": [{"text": "automated acquisition of sentiment lexicon", "start_pos": 47, "end_pos": 89, "type": "TASK", "confidence": 0.789871346950531}]}, {"text": "The method is language independent and corpusbased and therefore suitable for languages lacking general lexical resources such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.9528427124023438}]}, {"text": "The two-step hybrid process combines semi-supervised graph-based algorithms and supervised learning models.", "labels": [], "entities": []}, {"text": "We consider three different tasks, each capturing different aspect of a sentiment lexicon: 1.", "labels": [], "entities": []}, {"text": "Polarity ranking task -determine the relative rankings of words, i.e., order lexicon items descendingly by positivity and negativity; 2.", "labels": [], "entities": []}, {"text": "Polarity regression task -assign each word absolute scores (between 0 and 1) for positivity and negativity; word into one of the three sentiment classes (positive, negative, or neutral).", "labels": [], "entities": [{"text": "Polarity regression task", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6882516145706177}]}, {"text": "Accordingly, we evaluate our method using three different measures -one to evaluate the quality of the ordering by positivity and negativity, other to evaluate the absolute sentiment scores assigned to each corpus word, and another to evaluate the classification performance.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we present the related work on sentiment lexicon acquisition.", "labels": [], "entities": [{"text": "sentiment lexicon acquisition", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.8772996465365092}]}, {"text": "Section 3 discusses the semi-supervised step of the hybrid approach.", "labels": [], "entities": []}, {"text": "In Section 4 we explain the supervised step in more detail.", "labels": [], "entities": []}, {"text": "In Section 5 the experimental setup, the evaluation procedure, and the results of the approach are discussed.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper and outlines future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "All the experiments were performed on the excerpt of the New York Times corpus, containing 434,494 articles.", "labels": [], "entities": [{"text": "excerpt of the New York Times corpus", "start_pos": 42, "end_pos": 78, "type": "DATASET", "confidence": 0.8292889850480216}]}, {"text": "The corpus was preprocessed (tokenized, lemmatized, and POS tagged) and only the content lemmas (nouns, verbs, adjectives, and adverbs) occurring at least 80 times in the corpus were considered.", "labels": [], "entities": []}, {"text": "Lemmas occurring less than 80 were mainly named entities or their derivatives.", "labels": [], "entities": []}, {"text": "The final sentiment lexicon consists of 41,359 lemmas annotated with positivity and negativity scores and sentiment class.", "labels": [], "entities": []}, {"text": "The semi-supervised step was designed to propagate sentiment properties of the labeled words, ordering the words according to their positivity or negativity.", "labels": [], "entities": []}, {"text": "Therefore, we decided to use the evaluation metric that measures the quality of the ranking in ordered lists, Kendall \u03c4 distance.", "labels": [], "entities": [{"text": "Kendall \u03c4 distance", "start_pos": 110, "end_pos": 128, "type": "METRIC", "confidence": 0.7824056148529053}]}, {"text": "The performance of the semi-supervised graphbased methods was evaluated both on the Micro-WN(Op)-1 and Micro-WN(Op)-0 sets.", "labels": [], "entities": [{"text": "Micro-WN", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.92925626039505}, {"text": "Micro-WN(Op)-0 sets", "start_pos": 103, "end_pos": 122, "type": "DATASET", "confidence": 0.8533164262771606}]}, {"text": "In order to be able to compare our results to SentiWordNet (, the de facto standard sentiment lexicon for English, we use the p-normalized Kendall \u03c4 distance between the rankings generated by our semi-supervised graph-based methods and the gold standard rankings.", "labels": [], "entities": [{"text": "p-normalized Kendall \u03c4 distance", "start_pos": 126, "end_pos": 157, "type": "METRIC", "confidence": 0.7155926302075386}]}, {"text": "presents the results for each of the methods used to build the sentiment graph and for both randomwalk algorithms.", "labels": [], "entities": []}, {"text": "The results were obtained by evaluating the relative rankings of words against the Micro-WN(Op)-1 as gold standard.", "labels": [], "entities": [{"text": "Micro-WN(Op)-1", "start_pos": 83, "end_pos": 97, "type": "DATASET", "confidence": 0.8241629242897034}]}, {"text": "For comparison, the p-normalized Kendall \u03c4 scores for SentiWordNet 1.0 and SentiWordNet 3.0 are extracted from (.", "labels": [], "entities": [{"text": "p-normalized Kendall \u03c4", "start_pos": 20, "end_pos": 42, "type": "METRIC", "confidence": 0.6334695021311442}]}, {"text": "Rankings for the negative scores are consistently better across all methods and algorithms.", "labels": [], "entities": []}, {"text": "We believe that the negative rankings are better for two reasons.", "labels": [], "entities": []}, {"text": "Firstly, the corpus contains many more articles describing negative events such as wars and accidents than the articles describing positive events such as celebrations and victories.", "labels": [], "entities": []}, {"text": "In short, the distribution of articles is significantly skewed towards \"negative\" events.", "labels": [], "entities": []}, {"text": "Secondly, the lemma new, which was included in the positive seed set, occurs in the corpus very frequently as apart of named entity collocations such as \"New York\" and \"New Jersey\" in which it does not reflect its dominant sense.", "labels": [], "entities": []}, {"text": "The harmonic function label propagation generally outperforms the PageRank algorithm.", "labels": [], "entities": [{"text": "harmonic function label propagation", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.6120384559035301}]}, {"text": "The best performance on the Micro-WN(Op)-0 set was 0.380 for the positive ranking and 0.270 for the negative ranking, showing that the performance deteriorates when polysemy is present.", "labels": [], "entities": [{"text": "Micro-WN(Op)-0 set", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.8303322593371073}]}, {"text": "However, the drop in performance, especially for the negative ranking, is not substantial.", "labels": [], "entities": []}, {"text": "Our best method (graph built based on PMI of corpus words used in combination with harmonic function label propagation) outperforms SentiWordNet 1.0 and performs slightly worse than SentiWordNet 3.0 for both positive and negative rankings.", "labels": [], "entities": [{"text": "harmonic function label propagation", "start_pos": 83, "end_pos": 118, "type": "TASK", "confidence": 0.7142941206693649}]}, {"text": "Supervised step deals with the polarity regression task and the sentiment classification task.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.9234551191329956}]}, {"text": "Polarity regression maps the \"virtual\" sentiment scores obtained on graphs to the absolute sentiment scores (on a scale from 0 to 1).", "labels": [], "entities": []}, {"text": "The regression was performed twice: once for the positive scores and once for the negative scores.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the polarity regression against the Micro-WN(Op)-0 gold standard in terms of root mean square error (RMSE).", "labels": [], "entities": [{"text": "Micro-WN(Op)-0 gold standard", "start_pos": 67, "end_pos": 95, "type": "DATASET", "confidence": 0.8842565502439227}, {"text": "root mean square error (RMSE)", "start_pos": 108, "end_pos": 137, "type": "METRIC", "confidence": 0.8696200677326748}]}, {"text": "We used the average of the labeled polarity scores (positive and negative) of all monosentimous words in Micro-WN(Op)-1 as a baseline for this task.", "labels": [], "entities": []}, {"text": "Sentiment classification uses the scores obtained on graphs as features in order to assign each word with one of the three sentiment labels (positive, negative, and neutral).", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9460233449935913}]}, {"text": "The classification performance is evaluated in terms of micro-F1 measure.", "labels": [], "entities": []}, {"text": "The labels for the classification are assigned according to the positivity and negativity scores (the label neutral is assigned if Obj (s) = 1\u2212Pos(s)\u2212Neg(s) is larger than both Pos(s) and Neg(s)).", "labels": [], "entities": [{"text": "Obj", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9021104574203491}]}, {"text": "The majority class predictor was used as a baseline for the classification task.", "labels": [], "entities": [{"text": "classification task", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.9022344350814819}]}, {"text": "Due to the small size of the labeled sets (e.g., 225 for Micro-WN(Op)-1) we performed the 10 \u00d7 10 CV evaluation (10 cross-validation trials, each on randomly permuted data) both for regression and classification.", "labels": [], "entities": []}, {"text": "For comparison, we evaluated the SentiWordNet in the same way -we averaged the SentiWordNet scores for all the senses of monosentimous words from the Micro-WN(Op)-1.", "labels": [], "entities": []}, {"text": "Although the semi-supervised step itself was not designed to deal with polarity regression task and sentiment classification task, we decided to evaluate the results gained from graphs on these tasks as well.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 100, "end_pos": 129, "type": "TASK", "confidence": 0.9010929862658182}]}, {"text": "This gives us an insight to how much the supervised step adds in terms of performance.", "labels": [], "entities": []}, {"text": "The positivity and negativity scores obtained from graphs were directly evaluated on the regression task measuring the RMSE against the gold standard.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.4186405539512634}]}, {"text": "Classification labels were deter-mined by comparing the positive rank of the word against the negative rank of the word.", "labels": [], "entities": []}, {"text": "The word was classified as neutral if the absolute difference between its positive and negative rank was below the given treshold t.", "labels": [], "entities": []}, {"text": "Empirically determined optimal value of the treshold wast = 1000.", "labels": [], "entities": [{"text": "wast", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.6160975098609924}]}, {"text": "we present the results of the hybrid method on both the regression (for both positive and negative scores) and classification tasks compared with the performance of the SentiWordNet and the baselines.", "labels": [], "entities": []}, {"text": "Additionally, we present the results obtained using only the semi-supervised step.", "labels": [], "entities": []}, {"text": "On both the regression and classification task our method outperforms the baseline.", "labels": [], "entities": []}, {"text": "The performance is comparable to SentiWordNet on the sentiment classification task.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.9588641921679179}]}, {"text": "However, the performance of our corpus-based approach is significantly lower than SentiWordNet on the polarity regression task -a more detailed analysis is required to determine the cause of this.", "labels": [], "entities": []}, {"text": "The hybrid approach performs significantly better than the semi-supervised method alone, confirming the importance of the supervised step.", "labels": [], "entities": []}, {"text": "Models trained on the Micro-WN(Op)-1 were applied on the set of words from the Micro-WN(Op)-0 not present in the Micro-WN(Op)-1 (i.e., the difference between the two sets) in order to test the performance on non-monosentimous words.", "labels": [], "entities": [{"text": "Micro-WN(Op)-1", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.8575033068656921}, {"text": "Micro-WN(Op)-0", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.8461641907691956}]}, {"text": "The obtained results on this set are, surprisingly, slightly better (positivity regression -0.337; negativity regression -0.313; and classification -57.55%).", "labels": [], "entities": [{"text": "classification", "start_pos": 133, "end_pos": 147, "type": "METRIC", "confidence": 0.9554421305656433}]}, {"text": "This is most likely due to the fact that, although not all senses have the same sentiment, most of them have similar sentiment, which is often also the sentiment of the dominant sense in the corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The results on the polarity ranking task", "labels": [], "entities": [{"text": "polarity ranking", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7828615307807922}]}]}