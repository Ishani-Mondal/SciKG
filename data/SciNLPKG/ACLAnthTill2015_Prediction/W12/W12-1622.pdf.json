{"title": [{"text": "Global Features for Shallow Discourse Parsing", "labels": [], "entities": [{"text": "Shallow Discourse Parsing", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.5676682492097219}]}], "abstractContent": [{"text": "A coherently related group of sentences maybe referred to as a discourse.", "labels": [], "entities": []}, {"text": "In this paper we address the problem of parsing coherence relations as defined in the Penn Discourse Tree Bank (PDTB).", "labels": [], "entities": [{"text": "parsing coherence relations", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.8753336071968079}, {"text": "Penn Discourse Tree Bank (PDTB)", "start_pos": 86, "end_pos": 117, "type": "DATASET", "confidence": 0.9601894787379673}]}, {"text": "A good model for discourse structure analysis needs to account both for local dependencies at the token-level and for global dependencies and statistics.", "labels": [], "entities": [{"text": "discourse structure analysis", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.7267544865608215}]}, {"text": "We present techniques on using inter-sentential or sentence-level (global), data-driven, non-grammatical features in the task of parsing discourse.", "labels": [], "entities": [{"text": "parsing discourse", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.8953968286514282}]}, {"text": "The parser model follows up previous approach based on using token-level (local) features with conditional random fields for shallow discourse parsing, which is lacking in structural knowledge of discourse.", "labels": [], "entities": [{"text": "shallow discourse parsing", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.7436927159627279}]}, {"text": "The parser adopts a two-stage approach where first the local constraints are applied and then global constraints are used on a reduced weighted search space (n-best).", "labels": [], "entities": []}, {"text": "In the latter stage we experiment with different rerankers trained on the first stage n-best parses, which are generated using lexico-syntactic local features.", "labels": [], "entities": []}, {"text": "The two-stage parser yields significant improvements over the best performing model of discourse parser on the PDTB corpus.", "labels": [], "entities": [{"text": "discourse parser", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.7265920042991638}, {"text": "PDTB corpus", "start_pos": 111, "end_pos": 122, "type": "DATASET", "confidence": 0.9587397277355194}]}], "introductionContent": [{"text": "There are relevant studies on the impact of global and local features on the models for natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.668554445107778}]}, {"text": "In this work we address a similar problem in the context of discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7010052055120468}]}, {"text": "Although a good number of the papers in this area heavily rely on local classifiers (), there are still some important works using global and local informations together to form a model of discourse ().", "labels": [], "entities": []}, {"text": "One of the main issues is the basis of the choice between a global or local or a joint model for discourse parsing: it all depends on the criteria to be able to capture maximum amount of information inside the discourse model.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.6914861053228378}]}, {"text": "The policy for discourse segmentation plays a big role to formulate the maximizing criteria ().", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7083687633275986}]}, {"text": "We study in the literature that defining a discourse segment is mostly a data-driven process: some argue for prosodic units, some for intentional structure and some for clause-like structures.", "labels": [], "entities": []}, {"text": "We work with PDTB 2.0 annotation framework, therefore use a clause-like structure.", "labels": [], "entities": []}, {"text": "empirically showed that at the sentence level, there is a strong correlation between syntax and discourse, found the same.", "labels": [], "entities": []}, {"text": "Since the discourse structure may span over multiple sentences, intersentential features are needed to improve the performance of a discourse parser.", "labels": [], "entities": []}, {"text": "Linguistic theory suggests that a core argument frame (i.e. a pair of the Arg1 and the Arg2 connected with one and only one connective) is a joint structure, with strong dependencies between arguments (.", "labels": [], "entities": []}, {"text": "Following this, also injected some structurelevel information through the token-level features, for eg. the previous sentence feature.", "labels": [], "entities": []}, {"text": "Still there is a room for improvement with more structurelevel information to that discourse model; though it is cost-intensive to modify this discourse model.", "labels": [], "entities": []}, {"text": "Therefore in this paper we re-use the model () and optimize the current loss function adding the global features through reranking of the single-best model.", "labels": [], "entities": []}, {"text": "Reranking has been a popular technique applied in a variety of comparable NLP problems including parsing), semantic role labeling (, NP Bracketing), NER), opinion expression detection), now we employ this technique in the area of discourse parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 97, "end_pos": 104, "type": "TASK", "confidence": 0.9769964814186096}, {"text": "semantic role labeling", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.6681788861751556}, {"text": "opinion expression detection", "start_pos": 155, "end_pos": 183, "type": "TASK", "confidence": 0.7703516085942587}, {"text": "discourse parsing", "start_pos": 230, "end_pos": 247, "type": "TASK", "confidence": 0.7086895555257797}]}, {"text": "In the next sections, we detail on the backgrounds and motivations of this work, before this we also add a short discussion on PDTB (Penn Discourse TreeBank), i.e. the data we used to train the system.", "labels": [], "entities": [{"text": "Penn Discourse TreeBank)", "start_pos": 133, "end_pos": 157, "type": "DATASET", "confidence": 0.886124461889267}]}, {"text": "Then we proceed to the reranking approaches and results sections after describing our global feature set.", "labels": [], "entities": []}, {"text": "Finally we state and analyze the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use PennDiscourse TreeBank ( and Penn TreeBank () data through this entire work.", "labels": [], "entities": [{"text": "PennDiscourse TreeBank", "start_pos": 7, "end_pos": 29, "type": "DATASET", "confidence": 0.9883820414543152}, {"text": "Penn TreeBank () data", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.9270169287919998}]}, {"text": "We keep the split of data as follows: 02 \u2212 22 folders of PDTB (& PTB) are used for training, 23 \u2212 24 folders of the same are used for testing; remaining 00-01 folders are meant for development split, it is used only to study the impact of feature (cf. 5).", "labels": [], "entities": [{"text": "PDTB (& PTB", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.7189850608507792}]}, {"text": "We prepare the n-best outputs of sentences from the base system (cf. 2.1).", "labels": [], "entities": []}, {"text": "The training data is prepared from the input of n-best lists of the train split, using a oracle module, which generates kbest oracle lists from the n-best single outputs.", "labels": [], "entities": []}, {"text": "We procure k-best lists from oracle using the evaluator module (see section 4.2), ordered by the highest to the lowest probability score.", "labels": [], "entities": []}, {"text": "Each of the list of the k-best list is a 5-sentence discourse window.", "labels": [], "entities": []}, {"text": "We prepare the test data given the n-best lists of the test split.", "labels": [], "entities": []}, {"text": "We obtain k-best list for testing, prepared with the module described in section 2.1.", "labels": [], "entities": []}, {"text": "We re-integrate the sentences connected with the same discourse connective id into the 5-sentence discourse window keeping the connective-bearing sentence in the middle.", "labels": [], "entities": []}, {"text": "This re-integration done using a priority queue in the style of.", "labels": [], "entities": []}, {"text": "Each of the list from the k-best list are ordered by the highest to the lowest score with sum of the log of posterior probabilities of each sentence in the n-best list.", "labels": [], "entities": []}, {"text": "Therefore, in short, the n-best list is the list of sentence-level analyses whereas the k-best list is the list of 5-sentence discourse window-level analyses.", "labels": [], "entities": []}, {"text": "Baseline: we consider the performance of the single-best output from the base implementation (cf. 2.1) as the baseline.", "labels": [], "entities": []}, {"text": "We present our results using precision, recall and F1 measures.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9996768236160278}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9992696642875671}, {"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.999736487865448}]}, {"text": "Following, we use three scoring schemes: exact, intersection (or partial), and overlap scoring.", "labels": [], "entities": [{"text": "exact", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.991748571395874}, {"text": "overlap scoring", "start_pos": 79, "end_pos": 94, "type": "METRIC", "confidence": 0.9391027390956879}]}, {"text": "In the exact scoring scheme, a span extracted by the system is counted as correct if its extent exactly coincides with one in the gold standard.", "labels": [], "entities": [{"text": "correct", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9609130024909973}]}, {"text": "We also include two other scoring schemes to have a rough approximation of the argument spans.", "labels": [], "entities": []}, {"text": "In the overlap scheme, an expression is counted as correctly detected if it overlaps with a gold standard argument.", "labels": [], "entities": []}, {"text": "The intersection scheme assigns a score between 0 and 1 for every predicted span based on how much it overlaps with a gold standard span, so unlike the other two schemes it will reward close matches.", "labels": [], "entities": []}, {"text": "We observe that reranking with global features improved the F1 scores for Arg1 significantly, although for Arg2 the improvement is insignificant . Since inmost of the cases the Arg2 is syntactically bound with the connective, it is obvious that lexico-syntactically motivated local features help the classification of Arg2.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9809301495552063}]}, {"text": "On the other hand, the classification of Arg1 is considerably dependent on non-grammatical, hand-crafted rule generated features.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.5730728507041931}]}, {"text": "If we compare to our reranking classification results of Arg1 with that one without previous sentence feature in then we observe that the global and globally motivated structural feature improved the classification of Arg1 by more than 10 points.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Exact Match Results for four classifiers. Baseline", "labels": [], "entities": []}, {"text": " Table 4: Partial Match Results for four classifiers. Baseline", "labels": [], "entities": []}, {"text": " Table 5: Oracle and reranker performance as a function of", "labels": [], "entities": []}, {"text": " Table 6: Oracle and reranker performance as a function of", "labels": [], "entities": []}, {"text": " Table 7: Inter-sentential Reranked Arg1 Results.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.6281112432479858}]}, {"text": " Table 8: Exact Match Results for Arg1 through Incremen-", "labels": [], "entities": [{"text": "Arg1", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.7302067279815674}]}]}