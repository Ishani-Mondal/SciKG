{"title": [], "abstractContent": [{"text": "The focus of our workshop was to use parallel corpora for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.8369514644145966}]}, {"text": "Recent experimentation has shown that the performance of SMT systems varies greatly with the source language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9937257170677185}]}, {"text": "In this workshop we encouraged researchers to investigate ways to improve the performance of SMT systems for diverse languages, including morphologically more complex languages, languages with partial free word order, and low-resource languages.", "labels": [], "entities": [{"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9928267002105713}]}, {"text": "Prior to the workshop, in addition to soliciting relevant papers for review and possible presentation, we conducted three shared tasks: a translation task, a quality estimation task, and a task to test automatic evaluation metrics.", "labels": [], "entities": [{"text": "translation task", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.8841112852096558}]}, {"text": "The results of the shared tasks were announced at the workshop, and these proceedings also include an overview paper for the shared tasks that summarizes the results, as well as provides information about the data used and any procedures that were followed in conducting or scoring the task.", "labels": [], "entities": []}, {"text": "In addition, there are short papers from each participating team that describe their underlying system in greater detail.", "labels": [], "entities": []}, {"text": "Like in previous years, we have received afar larger number of submission than we could accept for presentation.", "labels": [], "entities": []}, {"text": "This year we have received 45 full paper submissions and 39 shared task submissions.", "labels": [], "entities": []}, {"text": "In total WMT-2012 featured 20 full paper oral presentations and 39 shared task poster presentations.", "labels": [], "entities": []}], "introductionContent": [{"text": "The program committee decided to award the WMT 5-year Retrospective Best Paper Award to: Like last year's best paper award winner, Lavie and Agarwal's publication was a short paper describing the authors' submission to one of the WMT shared tasks.", "labels": [], "entities": [{"text": "WMT 5-year Retrospective Best Paper Award", "start_pos": 43, "end_pos": 84, "type": "DATASET", "confidence": 0.7000485062599182}, {"text": "WMT shared tasks", "start_pos": 230, "end_pos": 246, "type": "TASK", "confidence": 0.686775008837382}]}, {"text": "WMT07 introduced anew shared task to evaluate the quality of automatic metrics for machine translation quality by comparing the metrics' rankings to human rankings of MT systems.", "labels": [], "entities": [{"text": "WMT07", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9659218192100525}, {"text": "machine translation quality", "start_pos": 83, "end_pos": 110, "type": "TASK", "confidence": 0.8331642746925354}]}, {"text": "In the shared task, METEOR demonstrated higher correlation than BLEU (the de facto standard) across a variety of human evaluation measures, including adequacy and fluency, ranking the translations of whole sentences, and ranking the translation of smaller constituents within sentences.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.7670618295669556}, {"text": "correlation", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9654313325881958}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.998879611492157}]}, {"text": "The program committee members who selected Lavie and Agarwal's paper pointed out that METEOR is the only metric that has managed to compete with BLEU for attention in the MT world without a major funder backing the metric.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9050929546356201}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9944502115249634}, {"text": "MT world", "start_pos": 171, "end_pos": 179, "type": "TASK", "confidence": 0.8698082864284515}]}, {"text": "They pointed out that TER and HTER have also become prominent, but it is not clear whether that would have happened without backing from DARPA.", "labels": [], "entities": [{"text": "TER", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9411352872848511}, {"text": "HTER", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.6548776626586914}]}, {"text": "Furthermore, METEOR has contributed substantially to improving the assessment of the quality of MT systems by showing the importance of word similarity beyond surface form.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.5452824234962463}, {"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9882099628448486}]}, {"text": "In many ways this paper represents the ideals of the WMT workshops.", "labels": [], "entities": [{"text": "WMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.8132222890853882}]}, {"text": "It introduced a novel approach to the automatic evaluation of machine translation and demonstrated the metric's value empirically by comparing it to other state-of-the-art metrics on a public data set.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7487468719482422}]}], "datasetContent": [], "tableCaptions": []}