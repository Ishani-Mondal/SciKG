{"title": [{"text": "Measuring the Influence of Long Range Dependencies with Neural Network Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "In spite of their well known limitations, most notably their use of very local contexts , n-gram language models remain an essential component of many Natural Language Processing applications, such as Automatic Speech Recognition or Statistical Machine Translation.", "labels": [], "entities": [{"text": "Automatic Speech Recognition", "start_pos": 201, "end_pos": 229, "type": "TASK", "confidence": 0.6651118298371633}, {"text": "Statistical Machine Translation", "start_pos": 233, "end_pos": 264, "type": "TASK", "confidence": 0.8014694452285767}]}, {"text": "This paper investigates the potential of language models using larger context windows comprising up to the 9 previous words.", "labels": [], "entities": []}, {"text": "This study is made possible by the development of several novel Neural Network Language Model architectures, which can easily fare with such large context windows.", "labels": [], "entities": []}, {"text": "We experimentally observed that extending the context size yields clear gains in terms of perplexity and that the n-gram assumption is statistically reasonable as long as n is sufficiently high, and that efforts should be focused on improving the estimation procedures for such large models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conventional n-gram Language Models (LMs) area cornerstone of modern language modeling for Natural Language Processing (NLP) systems such as statistical machine translation (SMT) and Automatic Speech Recognition (ASR).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 141, "end_pos": 178, "type": "TASK", "confidence": 0.7839480340480804}, {"text": "Automatic Speech Recognition (ASR)", "start_pos": 183, "end_pos": 217, "type": "TASK", "confidence": 0.7875482042630514}]}, {"text": "After more than two decades of experimenting with these models in a variety of languages, genres, datasets and applications, the vexing conclusion is that these models are very difficult to improve upon.", "labels": [], "entities": []}, {"text": "Many variants of the simple n-gram model have been discussed in the literature; yet, very few of these variants have shown to deliver consistent performance gains.", "labels": [], "entities": []}, {"text": "Among these, smoothing techniques, such as Good-Turing, Witten-Bell and Kneser-Ney smoothing schemes (see) for an empirical overview and () fora Bayesian interpretation) are used to compute estimates for the probability of unseen events, which are needed to achieve state-of-the-art performance in large-scale settings.", "labels": [], "entities": []}, {"text": "This is because, even when using the simplifying n-gram assumption, maximum likelihood estimates remain unreliable and tend to overerestimate the probability of those rare n-grams that are actually observed, while the remaining lots receive a too small (null) probability.", "labels": [], "entities": []}, {"text": "One of the most successful alternative to date is to use distributed word representations ( to estimate the n-gram models.", "labels": [], "entities": []}, {"text": "In this approach, the discrete representation of the vocabulary, where each word is associated with an arbitrary index, is replaced with a continuous representation, where words that are distributionally similar are represented as neighbors.", "labels": [], "entities": []}, {"text": "This turns n-gram distributions into smooth functions of the word representation.", "labels": [], "entities": []}, {"text": "These representations and the associated estimates are jointly computed using a multi-layer neural network architecture.", "labels": [], "entities": []}, {"text": "The use of neural-networks language models was originally introduced in) and successfully applied to largescale speech recognition ( and machine translation tasks . Following these initial successes, the neural approach has recently been extended in several promising ways).", "labels": [], "entities": [{"text": "largescale speech recognition", "start_pos": 101, "end_pos": 130, "type": "TASK", "confidence": 0.5964978933334351}, {"text": "machine translation tasks", "start_pos": 137, "end_pos": 162, "type": "TASK", "confidence": 0.7946493526299795}]}, {"text": "neural network language models (NNLMs) that has often been overlooked is the ability of the latter to fare with extended contexts (; in comparison, standard n-gram LMs rarely use values of n above n = 4 or 5, mainly because of data sparsity issues and the lack of generalization of the standard estimates, notwithstanding the complexity of the computations incurred by the smoothing procedures (see however for an attempt to build very large models with a simple smoothing scheme).", "labels": [], "entities": []}, {"text": "The recent attempts of to resuscitate recurrent neural network architectures goes one step further in that direction, as a recurrent network simulates an unbounded history size, whereby the memory of all the previous words accumulates in the form of activation patterns on the hidden layer.", "labels": [], "entities": []}, {"text": "Significant improvements in ASR using these models were reported in).", "labels": [], "entities": [{"text": "ASR", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9904313683509827}]}, {"text": "It must however be emphasized that the use of a recurrent structure implies an increased complexity of the training and inference procedures, as compared to a standard feedforward network.", "labels": [], "entities": []}, {"text": "This means that this approach cannot handle large training corpora as easily as n-gram models, which makes it difficult to perform a fair comparison between these two architectures and to assess the real benefits of using very large contexts.", "labels": [], "entities": []}, {"text": "The contribution is this paper is two-fold.", "labels": [], "entities": []}, {"text": "We first analyze the results of various NNLMs to assess whether long range dependencies are efficient in language modeling, considering history sizes ranging from 3 words to an unbounded number of words (recurrent architecture).", "labels": [], "entities": []}, {"text": "A by-product of this study is a slightly modified version of n-gram SOUL model () that aims at quantitatively estimating the influence of context words both in terms of their position and their part-of-speech information.", "labels": [], "entities": []}, {"text": "The experimental set-up is based on a large scale machine translation task.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.7833564678827921}]}, {"text": "We then propose ahead to head comparison between the feed-forward and recurrent NNLMs.", "labels": [], "entities": [{"text": "NNLMs", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.8528812527656555}]}, {"text": "To make this comparison fair, we introduce an extension of the SOUL model that approximates the recurrent architecture with a limited history.", "labels": [], "entities": []}, {"text": "While this extension achieves performance that are similar to the recurrent model on small datasets, the associated training procedure can benefit from all the speed-ups and tricks of standard feedforward NNLM (mini-batch and resampling), which make it able to handle large training corpora.", "labels": [], "entities": []}, {"text": "Furthermore, we show that this approximation can also be effectively used to bootstrap the training of a \"true\" recurrent architecture.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first recollect, in Section 2, the basics of NNLMs architectures.", "labels": [], "entities": []}, {"text": "We then describe, in Section 3, a number of ways to speedup training for our \"pseudorecurrent\" model.", "labels": [], "entities": []}, {"text": "We finally report, in Section 4, various experimental results aimed at measuring the impact of large contexts, first in terms of perplexity, then on a realistic English to French translation task.", "labels": [], "entities": [{"text": "English to French translation task", "start_pos": 161, "end_pos": 195, "type": "TASK", "confidence": 0.7128139793872833}]}], "datasetContent": [{"text": "We now turn to the experimental part, starting with a description of the experimental setup.", "labels": [], "entities": []}, {"text": "We will then present an attempt to quantify the relative importance of history words, followed by ahead to head comparison of the various NNLM architectures discussed in the previous sections.", "labels": [], "entities": []}, {"text": "The tasks considered in our experiments are derived from the shared translation track of WMT 2011 (translation from English to French).", "labels": [], "entities": [{"text": "WMT 2011", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.8736934065818787}]}, {"text": "We only provide here a short overview of the task; all the necessary details regarding this evaluation campaign are available on the official Web site 3 and our system is described in ).", "labels": [], "entities": []}, {"text": "Simply note that our parallel training data includes a large Web corpus, referred to as the GigaWord parallel corpus.", "labels": [], "entities": [{"text": "GigaWord parallel corpus", "start_pos": 92, "end_pos": 116, "type": "DATASET", "confidence": 0.8672930399576823}]}, {"text": "After various preprocessing and filtering steps, the total amount of training data is approximately 12 million sentence pairs for the bilingual part, and about 2.5 billion of words for the monolingual part.", "labels": [], "entities": []}, {"text": "To built the target language models, the monolingual corpus was first split into several sub-parts based on date and genre information.", "labels": [], "entities": []}, {"text": "For each of these sub-corpora, a standard 4-gram LM was then estimated with interpolated Kneser-Ney smoothing).", "labels": [], "entities": [{"text": "LM", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.8828367590904236}]}, {"text": "All models were created without any pruning nor cutoff.", "labels": [], "entities": []}, {"text": "The baseline back-off n-gram LM was finally built as a linear combination of several these models, where the interpolation coefficients are chosen so as to minimize the perplexity of a development set.", "labels": [], "entities": []}, {"text": "All NNLMs are trained following the prescriptions of, and they all share the same inner structure: the dimension of the projection word space is 500; the size of two hidden layers are respectively 1000 and 500; the short-list contains 2000 words; and the non-linearity is introduced with the sigmoid function.", "labels": [], "entities": []}, {"text": "For the recurrent model, the parameter that limits the back-propagation of errors through time is set to 9 (see) for details).", "labels": [], "entities": []}, {"text": "This parameter can be considered to play a role that is similar to the history size in our pseudo-recurrent n-gram model: a value of 9 in the recurrent setting is equivalent ton = 10.", "labels": [], "entities": []}, {"text": "All NNLMs are trained with the following resampling strategy: 75% of in-domain data) and 25% of the other data.", "labels": [], "entities": [{"text": "NNLMs", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.8737233877182007}]}, {"text": "At each epoch, the parameters are updated using approximately 50 millions words for the last training step and about 140 millions words for the previous ones.", "labels": [], "entities": []}, {"text": "The integration of NNLMs for large SMT tasks is far from easy, given the computational cost of computing n-gram probabilities, a task that is performed repeatedly during the search of the best translation.", "labels": [], "entities": [{"text": "SMT tasks", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.9385335147380829}]}, {"text": "Our solution was to resort to a two-pass approach: the first pass uses a conventional back-off n-gram model to produce a list of the k most likely translations; in the second pass, the NNLMs probability of each hypothesis is computed and the k-best list is accordingly reordered.", "labels": [], "entities": []}, {"text": "The NNLM weights are optimized as the other feature weights using Minimum Error Rate Training (MERT).", "labels": [], "entities": [{"text": "NNLM", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8485865592956543}, {"text": "Minimum Error Rate Training (MERT)", "start_pos": 66, "end_pos": 100, "type": "METRIC", "confidence": 0.8276825802666801}]}, {"text": "For all our experiments, we used the value k = 300.", "labels": [], "entities": []}, {"text": "To clarify the impact of the language model order in translation performance, we considered three different ways to use NNLMs.", "labels": [], "entities": []}, {"text": "In the first setting, the NNLM is used alone and all the scores provided by the MT system are ignored.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.9093354940414429}, {"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9416916370391846}]}, {"text": "In the second setting (replace), the NNLM score replaces the score of the standard back-off LM.", "labels": [], "entities": [{"text": "NNLM score", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.7443155646324158}]}, {"text": "Finally, the score of the NNLM can be added in the linear combination (add).", "labels": [], "entities": [{"text": "NNLM", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.9015816450119019}]}, {"text": "In the last two settings, the weights used for: Results for the English to French task obtained with the baseline system and with various NNLMs.", "labels": [], "entities": [{"text": "NNLMs", "start_pos": 138, "end_pos": 143, "type": "DATASET", "confidence": 0.931840181350708}]}, {"text": "Perplexity is computed on newstest2009-2011 while BLEU is on the test set (newstest2010).", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9801385402679443}, {"text": "newstest2009-2011", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.9633786678314209}, {"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9988190531730652}]}, {"text": "n-best reranking are re-tuned with MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9619511961936951}]}, {"text": "summarizes the BLEU scores obtained on the newstest2010 test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9987674951553345}, {"text": "newstest2010 test set", "start_pos": 43, "end_pos": 64, "type": "DATASET", "confidence": 0.9822802543640137}]}, {"text": "BLEU improvements are observed with feed-forward NNLMs using a value of n = 8 with respect to the baseline (n = 4).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9903201460838318}]}, {"text": "Further increase from 8 to 10 only provides a very small BLEU improvement.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9991344809532166}]}, {"text": "These results strengthen the assumption made in Section 3.3: there seem to be very little information in remote words (above n = 7-8).", "labels": [], "entities": []}, {"text": "It is also interesting to see that the 4-gram NNLM achieves a comparable perplexity to the conventional 4-gram model, yet delivers a small BLEU increase in the alone condition.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.8656067252159119}, {"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9994125366210938}]}, {"text": "Surprisingly 4 , on this task, recurrent models seem to be comparable with 8-gram NNLMs.", "labels": [], "entities": []}, {"text": "The reason maybe the deep architecture of recurrent model that makes it hard to be trained in a large scale task.", "labels": [], "entities": []}, {"text": "With the recurrent-like n-gram model described in Section 2.1.2, it is feasible to train a recurrent model on a large task.", "labels": [], "entities": []}, {"text": "With 10% of perplexity reduction as compared to a backoff model, its yields comparable performances as reported in.", "labels": [], "entities": [{"text": "perplexity reduction", "start_pos": 12, "end_pos": 32, "type": "METRIC", "confidence": 0.8843891620635986}]}, {"text": "To the best of our knowledge, it is the first recurrent NNLM trained on a such large dataset (2.5 billion words) in a reasonable time (about 11 days).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for the English to French task obtained  with the baseline system and with various NNLMs. Per- plexity is computed on newstest2009-2011 while BLEU is  on the test set (newstest2010).", "labels": [], "entities": [{"text": "NNLMs", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.9289035797119141}, {"text": "Per- plexity", "start_pos": 108, "end_pos": 120, "type": "METRIC", "confidence": 0.9550268650054932}, {"text": "BLEU", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9987941980361938}]}]}