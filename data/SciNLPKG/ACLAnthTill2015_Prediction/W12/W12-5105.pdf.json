{"title": [{"text": "Modeling Word Meaning: Distributional Semantics and the Corpus Quality-Quantity Trade-Off", "labels": [], "entities": [{"text": "Modeling Word Meaning", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8545441031455994}]}], "abstractContent": [{"text": "Dictionaries constructed using distributional models of lexical semantics have a wide range of applications in NLP and in the modeling of linguistic cognition.", "labels": [], "entities": [{"text": "NLP", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9404215216636658}]}, {"text": "However when constructing such a model, we are faced with range of corpora to choose from.", "labels": [], "entities": []}, {"text": "Often there is a choice between small carefully constructed corpora of well-edited text, and very large heterogeneous collections harvested automatically from the web.", "labels": [], "entities": []}, {"text": "There may also be differences in the distribution of genres and registers in such corpora.", "labels": [], "entities": []}, {"text": "In this paper we examine these trade-offs by constructing a simple SVD-reduced word-collocate model, using four English corpora: the Google Web 5-gram collection, the Google Book 5-gram collection, the English Wikipedia, and collection of short social messages harvested from Twitter.", "labels": [], "entities": [{"text": "Google Web 5-gram collection", "start_pos": 133, "end_pos": 161, "type": "DATASET", "confidence": 0.766769215464592}, {"text": "Google Book 5-gram collection", "start_pos": 167, "end_pos": 196, "type": "DATASET", "confidence": 0.8750466108322144}]}, {"text": "Since these models need to encode semantics in away that approximates the mental lexicon, we evaluate the felicity of the resulting semantic representations using a set of behavioral and neural-activity benchmarks that depend on word-similarity.", "labels": [], "entities": []}, {"text": "We find that the quality of the input text has a very strong effect on the performance of the output model, and that a corpus of high quality at a small size can outperform a corpus of poor quality that is many orders of magnitude larger.", "labels": [], "entities": []}, {"text": "We also explore the semantic closeness of the models using their mutual information overlap to interpret the similarity of corpus texts.", "labels": [], "entities": []}, {"text": "1 Introduction Distributional semantic models (DSM) or distributional similarity models (Landauer, 1997) are unsupervised models based on the assertion that the meaning of a word can be inferred to some extent based on its distribution in the text.", "labels": [], "entities": []}, {"text": "They are high dimensional vector space representations that encode the semantics of words learnt from a statistical analysis of the context they appear in.", "labels": [], "entities": []}, {"text": "Word level dictionaries constructed using DSMs find use in many computational linguistics and cognitive science applications (Leacock, 1993; Bellegarda, 2000; Mitchell, 2008).", "labels": [], "entities": []}, {"text": "To build these models in a given language, there is typically a choice among several source corpora.", "labels": [], "entities": []}, {"text": "Often there is a choice between small well-curated text of good composition, and very large easy to collect text that is of inferior composition.", "labels": [], "entities": []}, {"text": "In addition, there are choices along the dimensions of language style, genre and register.", "labels": [], "entities": []}, {"text": "What kind of a corpus is most representative of a person's language experience?", "labels": [], "entities": []}, {"text": "Is colloquial text more preferable than the formal variety?", "labels": [], "entities": []}, {"text": "How does corpus size affect the model learnt?", "labels": [], "entities": []}, {"text": "In this paper we attempt to identify the trade-off between source corpus size and quality, measured based on their performance in modeling the mental lexicon.", "labels": [], "entities": []}, {"text": "Multiple behavioural and neurosemantic tests are used for this evaluation.", "labels": [], "entities": []}, {"text": "As additional explorations, we study the effects of dimensionality on model performance, and the mutual similarity byword categories among models derived from various corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional semantic models (DSM) or distributional similarity models) are unsupervised models based on the assertion that the meaning of a word can be inferred to some extent based on its distribution in the text.", "labels": [], "entities": []}, {"text": "They are high dimensional vector space representations that encode the semantics of words learnt from a statistical analysis of the context they appear in.", "labels": [], "entities": []}, {"text": "Word level dictionaries constructed using DSMs find use in many computational linguistics and cognitive science applications.", "labels": [], "entities": []}, {"text": "To build these models in a given language, there is typically a choice among several source corpora.", "labels": [], "entities": []}, {"text": "Often there is a choice between small well-curated text of good composition, and very large easy to collect text that is of inferior composition.", "labels": [], "entities": []}, {"text": "In addition, there are choices along the dimensions of language style, genre and register.", "labels": [], "entities": []}, {"text": "What kind of a corpus is most representative of a person's language experience?", "labels": [], "entities": []}, {"text": "Is colloquial text more preferable than the formal variety?", "labels": [], "entities": []}, {"text": "How does corpus size affect the model learnt?", "labels": [], "entities": []}, {"text": "In this paper we attempt to identify the trade-off between source corpus size and quality, measured based on their performance in modeling the mental lexicon.", "labels": [], "entities": []}, {"text": "Multiple behavioural and neurosemantic tests are used for this evaluation.", "labels": [], "entities": []}, {"text": "As additional explorations, we study the effects of dimensionality on model performance, and the mutual similarity byword categories among models derived from various corpora.", "labels": [], "entities": []}, {"text": "There is ample literature analysing the effects of feature types, normalization, dimensionality, pruning, among other factors, on distributional semantics (.", "labels": [], "entities": []}, {"text": "Quantitative and qualitative comparison of corpora based on the surface text has been performed as well.", "labels": [], "entities": []}, {"text": "But, to our knowledge, there is not any systematic analysis of the effect of the corpus quality on distributional semantics.", "labels": [], "entities": []}, {"text": "Authors have expressed that it is not adequate to explore the effects of size on model quality, it is important to analyse the effects of corpus quality as well.", "labels": [], "entities": []}, {"text": "Although a wide range of corpora have been used to build DSMs, variation in modeling parameters, processing techniques and evaluation metrics used by the authors makes a direct comparison of corpus quality unfeasible.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 57, "end_pos": 61, "type": "TASK", "confidence": 0.9425710439682007}]}, {"text": "In this paper, we build simple SVD-reduced word-collocate models using four English corpora that differ considerably in quality, size and composition.", "labels": [], "entities": []}, {"text": "We employ simple word co-occurrence based models rather than the more complex ones (such as dependency or document models) because it is possible to build word-collocate models for most languages and corpora that are available.", "labels": [], "entities": []}, {"text": "More importantly, the goal of the paper is to arrive at general reliable performance trends to address the quality-quantity trade off, and not to obtain the very best performance possible.", "labels": [], "entities": []}, {"text": "Thus, we employ more generic, commonly used methods and evaluation metrics in our experiments.", "labels": [], "entities": []}, {"text": "We find that the quality of the input text has a very strong effect on the performance of the output model, and that a corpus of high quality at a small size can outperform a corpus of poor quality that is many orders of magnitude larger.", "labels": [], "entities": []}, {"text": "And, we also explore the reasons for the relative performance of different corpora, in terms of the mutual similarity of the semantic spaces described by their corresponding models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}