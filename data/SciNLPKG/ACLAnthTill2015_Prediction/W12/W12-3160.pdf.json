{"title": [{"text": "Optimization Strategies for Online Large-Margin Learning in Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.755103349685669}]}], "abstractContent": [{"text": "The introduction of large-margin based dis-criminative methods for optimizing statistical machine translation systems in recent years has allowed exploration into many new types of features for the translation process.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.6148297985394796}, {"text": "translation process", "start_pos": 198, "end_pos": 217, "type": "TASK", "confidence": 0.9071178436279297}]}, {"text": "By removing the limitation on the number of parameters which can be optimized, these methods have allowed integrating millions of sparse features.", "labels": [], "entities": []}, {"text": "However, these methods have not yet met with widespread adoption.", "labels": [], "entities": []}, {"text": "This maybe partly due to the perceived complexity of implementation, and partly due to the lack of standard methodology for applying these methods to MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 150, "end_pos": 152, "type": "TASK", "confidence": 0.9798722863197327}]}, {"text": "This papers aims to shed light on large-margin learning for MT, explicitly presenting the simple passive-aggressive algorithm which underlies many previous approaches , with direct application to MT, and empirically comparing several widespread optimization strategies.", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9889413714408875}, {"text": "MT", "start_pos": 196, "end_pos": 198, "type": "TASK", "confidence": 0.9725455641746521}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) systems represent knowledge sources in the form of features, and rely on parameters, or weights, on each feature, to score alternative translations.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8089938958485922}]}, {"text": "As in all statistical models, these parameters need to be learned from the data.", "labels": [], "entities": []}, {"text": "In recent years, there has been a growing trend of moving away from discriminative training using batch log-linear optimization, with MinimumError Rate Training (MERT) being the principle method, to online linear optimization.", "labels": [], "entities": [{"text": "online linear optimization", "start_pos": 199, "end_pos": 225, "type": "TASK", "confidence": 0.6347328424453735}]}, {"text": "The major motivation for this has been that while MERT is able to efficiently optimize a small number of parameters directly toward an external evaluation metric, such as BLEU (, it has been shown that its performance can be erratic, and it is unable to scale to a large set of features).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9985830783843994}]}, {"text": "Furthermore, it is designed for batch learning, which maybe prohibitive or undesirable in certain scenarios, for instance if we have a large tuning set.", "labels": [], "entities": [{"text": "batch learning", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7690430879592896}]}, {"text": "One or both of these limitations have led to recent introduction of alternative optimization strategies, such as minimum-risk),), Structured SVM, and RAM-PION (, which are batch learners, and online large-margin structured learning (.", "labels": [], "entities": []}, {"text": "A popular method of large-margin optimization is the margin-infused relaxed algorithm (MIRA)), which has been shown to perform well for machine translation, as well as other structured prediction tasks, such as parsing.).", "labels": [], "entities": [{"text": "margin-infused relaxed algorithm (MIRA))", "start_pos": 53, "end_pos": 93, "type": "METRIC", "confidence": 0.7810186247030894}, {"text": "machine translation", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7909433841705322}, {"text": "parsing.", "start_pos": 211, "end_pos": 219, "type": "TASK", "confidence": 0.967700183391571}]}, {"text": "This is an attractive method because we have a simple analytical solution for the optimization problem at each step, which reduces to dual coordinate descent when using 1-best MIRA.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.6732251048088074}]}, {"text": "It is also quite easy to implement, as will be shown below.", "labels": [], "entities": []}, {"text": "Despite the proven success of MIRA-based largemargin optimization for both small and large numbers of features, these methods have not yielded wide adoption in the community.", "labels": [], "entities": [{"text": "MIRA-based largemargin optimization", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.8825283050537109}]}, {"text": "Part of the reason for this is a perception that these methods are complicated to implement, which has been cited as motivation for other.", "labels": [], "entities": []}, {"text": "Furthermore, there is a di-vergence between the standard application of these methods in machine learning, and our application in machine translation (, wherein machine learning there are usually clear correct outputs and no latent structures.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.8129152655601501}]}, {"text": "As a consequence of the above, there is alack of standard practices for large-margin learning for MT, which has resulted in numerous different implementations of MIRA-based optimizers, which further add to the confusion.", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9816422462463379}]}, {"text": "This paper aims to shed light on practical concerns with online large margin training.", "labels": [], "entities": []}, {"text": "Specifically, our contribution is first, to present the MIRA passive-aggressive update, which underlies all MIRA-based training, with an eye to application in MT.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.42654451727867126}, {"text": "MIRA-based training", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.5661280751228333}, {"text": "MT", "start_pos": 159, "end_pos": 161, "type": "TASK", "confidence": 0.9636298418045044}]}, {"text": "Then, we empirically compare several widespread as well as novel optimization strategies for large-margin training on Czech-to-English (csen) and French-to-English (fr-en) translation.", "labels": [], "entities": []}, {"text": "Analyzing the findings, we recommend an optimization strategy which should ensure convergence and stability.", "labels": [], "entities": [{"text": "convergence", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.8987835049629211}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Oracle score for model 1-best (baseline) and for  k-best of size 500, 50k, and 100k on NT08", "labels": [], "entities": [{"text": "NT08", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.9508756399154663}]}, {"text": " Table 3: Results with different strategies on cs-en transla- tion. MERT baseline is 18.4 for NT09 and 19.7 for NT10", "labels": [], "entities": [{"text": "MERT baseline", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.9621742367744446}, {"text": "NT09", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.8972044587135315}, {"text": "NT10", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.7191009521484375}]}, {"text": " Table 4: Results with different strategies on fr-en transla- tion. MERT baseline is 24.2 for NT09 and 26 for NT10", "labels": [], "entities": [{"text": "MERT baseline", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.9581066370010376}, {"text": "NT09", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.9067001938819885}, {"text": "NT10", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.7171533703804016}]}, {"text": " Table 5: Results on cs-en and fr-en with extended feature  set.", "labels": [], "entities": []}]}