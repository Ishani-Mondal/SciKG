{"title": [], "abstractContent": [{"text": "In this paper, we focus on improving part-of-speech (POS) tagging for Urdu by using existing tools and data for the language.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.6518267035484314}]}, {"text": "In our experiments, we use Humayoun's morphological analyzer, the POS tagging module of an Urdu Shallow Parser and our own SVM Tool tag-ger trained on CRULP manually annotated data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.6163348406553268}]}, {"text": "We convert the output of the taggers to a common format and more importantly unify their tagsets.", "labels": [], "entities": []}, {"text": "On an independent test set, our tagger outperforms the other tools by far.", "labels": [], "entities": []}, {"text": "We gain some further improvement by implementing a voting strategy that allows us to consider not only our tagger but also include suggestions by the other tools.", "labels": [], "entities": []}, {"text": "The final tagger reaches the accuracy of 87.98%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9996565580368042}]}], "introductionContent": [{"text": "Urdu belongs to the Indo-Aryan language family, a subclass of the Indo-European languages.", "labels": [], "entities": []}, {"text": "Urdu is the official language of Pakistan and one of the 23 official languages (including English) spoken in India.", "labels": [], "entities": []}, {"text": "It is the native language of at least 65.6 million speakers with another 40 million or more who speak it as a second language 1 . Urdu has borrowed its writing script from Persian, which is a modified form of the Arabic script, the Urdu script is thus called Perso-Arabic.", "labels": [], "entities": []}, {"text": "Urdu is written from right to left with numbers written from left to right.", "labels": [], "entities": []}, {"text": "The morphology of Urdu is similar to other Indo-European languages, e.g. by having concatenative inflective morphological system.", "labels": [], "entities": []}, {"text": "Urdu is a low-resource language with respect to even the core processing tasks like POS tagging or morphological analysis.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.8095625340938568}]}, {"text": "Existing taggers for Urdu do not reach sufficient coverage and accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.995010495185852}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9974009990692139}]}, {"text": "In this paper, we demonstrate how an ensemble of available tools and data can be joined to achieve a better performance.", "labels": [], "entities": []}, {"text": "First, we convert the output of the existing morphological tools to a common representation and more importantly, we unify the different tagsets.", "labels": [], "entities": []}, {"text": "Second, we train and evaluate anew tagger on the available annotated data (in our unified tagset) and finally, we implement and evaluate a \"voting\" scheme that combines the outputs of all available taggers.", "labels": [], "entities": []}], "datasetContent": [{"text": "We establish SVM tagger's (individual) accuracy as the baseline for our voting experiments.", "labels": [], "entities": [{"text": "SVM tagger", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.8516356647014618}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.929656982421875}]}, {"text": "SVM-Tag-1, 2, and 3 are our voting setups where we used the top 1, 2, or 3 options from SVM before normalizing their probabilities to sum to the one vote of SVM.", "labels": [], "entities": [{"text": "SVM-Tag-1", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.918217658996582}, {"text": "SVM", "start_pos": 157, "end_pos": 160, "type": "DATASET", "confidence": 0.8937079906463623}]}, {"text": "The final ambiguity resolution strategy is indicated in the column label.", "labels": [], "entities": [{"text": "ambiguity resolution", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7133008241653442}]}, {"text": "\"Voted Only\" means that the ambiguous final output is produced, which has no chance to score well in comparison with the fully disambiguated test set.", "labels": [], "entities": []}, {"text": "A preliminary analysis of SVM Tag-2 and 3 voted output revealed that we make errors in cases where SVM tagger predicts only one tag (so this tag gets the vote of 1) but it is still not selected because it is considered less probable by the remaining two taggers.", "labels": [], "entities": []}, {"text": "For such cases, i.e. when SVM had the chance to express its uncertainty but still decided unambiguously, we give it a preference.", "labels": [], "entities": []}, {"text": "As indicated in the lines labeled \"SVM Preferred If Sure\", this gives again a little improvement.", "labels": [], "entities": [{"text": "SVM Preferred If Sure", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.4714522659778595}]}], "tableCaptions": [{"text": " Table 1 summarizes the data we use in our experiments. We see that 1315 of the test  tokens (about 15% of the test set) are never seen in the training data.", "labels": [], "entities": []}, {"text": " Table 5: Gradual improvements of the baseline tagger: modified training data (MOD), closed- class words (CCW), cardinals (CD), and two new features for the SVM Tool (Best)", "labels": [], "entities": []}, {"text": " Table 10: Accuracy of test corpus after Voting and applying fall back option.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9976649284362793}]}]}