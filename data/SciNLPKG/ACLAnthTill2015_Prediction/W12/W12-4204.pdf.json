{"title": [{"text": "Towards a Predicate-Argument Evaluation for MT", "labels": [], "entities": [{"text": "Predicate-Argument Evaluation", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.8412632346153259}, {"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9878794550895691}]}], "abstractContent": [{"text": "HMEANT (Lo and Wu, 2011a) is a manual MT evaluation technique that focuses on predicate-argument structure of the sentence.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.9647403061389923}]}, {"text": "We relate HMEANT to an established linguistic theory, highlighting the possibilities of reusing existing knowledge and resources for interpreting and automating HMEANT.", "labels": [], "entities": [{"text": "interpreting and automating HMEANT", "start_pos": 133, "end_pos": 167, "type": "TASK", "confidence": 0.6613336354494095}]}, {"text": "We apply HMEANT to anew language, Czech in particular, by evaluating a set of English-to-Czech MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9086216688156128}]}, {"text": "HMEANT proves to correlate with manual rankings at the sentence level better than a range of automatic met-rics.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8730074167251587}]}, {"text": "However, the main contribution of this paper is the identification of several issues of HMEANT annotation and our proposal on how to resolve them.", "labels": [], "entities": [{"text": "HMEANT annotation", "start_pos": 88, "end_pos": 105, "type": "DATASET", "confidence": 0.7017804682254791}]}], "introductionContent": [{"text": "Manual evaluation of machine translation output is a tricky enterprise.", "labels": [], "entities": [{"text": "Manual evaluation of machine translation output", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.6576894819736481}]}, {"text": "It has been long recognized that different evaluation techniques lead to different outcomes, e.g. mention an evaluation carried out in 1972 where the very same Russian-to-English MT outputs were scored 4.5 out of the maximum 5 points by prospective users of the system but only 1 out of 5 by teachers of English.", "labels": [], "entities": [{"text": "MT", "start_pos": 179, "end_pos": 181, "type": "TASK", "confidence": 0.8100389838218689}]}, {"text": "Throughout the years, many techniques were explored with more or less of a success.", "labels": [], "entities": []}, {"text": "The two-scale scoring for adequacy and fluency used in NIST evaluation has been abandoned by some evaluation campaigns, most notably the WMT shared task series, see through . Since 2008, WMT uses a simple relative ranking of MT outputs as its primary manual evaluation technique: the annotator is presented with up to 5 MT outputs fora given input sentence and the task is to rank them from best to worst (ties allowed) on whatever criteria he or she deems appropriate.", "labels": [], "entities": [{"text": "NIST evaluation", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.6001025438308716}, {"text": "WMT shared task series", "start_pos": 137, "end_pos": 159, "type": "DATASET", "confidence": 0.8238551169633865}, {"text": "WMT", "start_pos": 187, "end_pos": 190, "type": "DATASET", "confidence": 0.7769293785095215}, {"text": "MT outputs", "start_pos": 225, "end_pos": 235, "type": "TASK", "confidence": 0.8607900142669678}]}, {"text": "While this single-scale relative ranking is perhaps faster to annotate and reaches a higher inter-and intra-annotator agreement than the (absolute) fluency and adequacy, the technique and its evaluation are still far from satisfactory.", "labels": [], "entities": []}, {"text": "observe several discrepancies in the interpretation of the rankings, partly due to the high load on human annotators (the comparison of several long sentences at once, among other issues) but partly also due to technicalities of the calculation.", "labels": [], "entities": []}, {"text": "Lo and Wu (2011a) present an interesting evaluation technique called MEANT (or HMEANT if carried out by humans), the core of which lies in assessing whether the key elements in the predicateargument structure of the sentence have been preserved.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9962228536605835}]}, {"text": "In other words, lay annotators are checking, if they recognize who did what to whom, when, where and why from the MT outputs and whether the respective role fillers convey the same meaning as in the reference translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 114, "end_pos": 116, "type": "TASK", "confidence": 0.8663533926010132}]}, {"text": "HMEANT has been shown to correlate reasonably well with manual adequacy and ranking evaluations.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9022799730300903}]}, {"text": "It is relatively fast and should lend itself to full automatization.", "labels": [], "entities": []}, {"text": "On the other hand, HMEANT was so far tested only on translation into English and with just three competing MT systems.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.5594736933708191}, {"text": "translation", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9649714827537537}, {"text": "MT", "start_pos": 107, "end_pos": 109, "type": "TASK", "confidence": 0.9444774389266968}]}, {"text": "In this work, we extend the application of HMEANT to evaluating MT into Czech, a morphologically rich language with relatively free word order.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.8788214921951294}]}, {"text": "The paper is structured as follows: Section 2 presents the technical details of HMEANT and relates HMEANT to an established linguistic theory that underlies the Prague dependency treebanks) and several other works.", "labels": [], "entities": [{"text": "Prague dependency treebanks", "start_pos": 161, "end_pos": 188, "type": "DATASET", "confidence": 0.8543478846549988}]}, {"text": "We also suggest possible benefits of this coupling such as the reuse of tools.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the setup and results of our HMEANT experiment.", "labels": [], "entities": []}, {"text": "Since this is the first time HMEANT is applied to anew language, Section 4 constitutes the main contribution of this work.", "labels": [], "entities": []}, {"text": "We point out at several problems of HMEANT and propose a remedy, the empirical evaluation of which however remains for future work.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.5585905313491821}]}, {"text": "Section 5 concludes our observations.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this first study, we selected 50 sentences from the English-to-Czech WMT12 manual evaluation.", "labels": [], "entities": [{"text": "WMT12 manual evaluation", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.8190736174583435}]}, {"text": "The sentences were chosen to overlap with the standard WMT ranking procedure (see Section 3.1) as much as possible.", "labels": [], "entities": [{"text": "WMT ranking", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.8645953238010406}]}, {"text": "In total, 13 MT systems participated in this translation direction.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9767302870750427}, {"text": "translation direction", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.9727661907672882}]}, {"text": "We allocated 14 annotators (one annotator for the SRL of the reference) so that nobody saw the same sentence translated by more systems.", "labels": [], "entities": []}, {"text": "The hypotheses were shuffled so every annotator got samples from all systems as well as the reference.", "labels": [], "entities": []}, {"text": "Unfortunately, time constraints and the large number of MT systems prevented us from collecting overlapping annotations, so we cannot evaluate inter-annotator agreement.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9599208235740662}]}, {"text": "Following Lo and Wu (2011a) and CallisonBurch et al.", "labels": [], "entities": []}, {"text": "(2012), we report Kendall's \u03c4 rank correlation coefficients for sentence-level rankings as provided by a range of automatic metrics and our HMEANT.", "labels": [], "entities": [{"text": "\u03c4 rank correlation", "start_pos": 28, "end_pos": 46, "type": "METRIC", "confidence": 0.6991016864776611}]}, {"text": "The gold standard are the manual WMT rankings.", "labels": [], "entities": [{"text": "WMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.911803126335144}]}, {"text": "We see that HMEANT achieves a better correlation than all the tested automatic metrics, although in absolute terms, the correlation is not very high.", "labels": [], "entities": [{"text": "HMEANT", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9604138731956482}, {"text": "correlation", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9604215621948242}, {"text": "correlation", "start_pos": 120, "end_pos": 131, "type": "METRIC", "confidence": 0.9665074348449707}]}, {"text": "Lo and Wu (2011b) report \u03c4 for HMEANT of up to 0.49 and Lo and Wu (2011a) observe \u03c4 in the range 0.33 to 0.43.", "labels": [], "entities": [{"text": "\u03c4", "start_pos": 25, "end_pos": 26, "type": "METRIC", "confidence": 0.9915090799331665}, {"text": "HMEANT", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9899365901947021}]}, {"text": "These figures are not comparable to our result for several reasons: we evaluated 13 and not just 3 MT systems, the gold standard for us are overall system rankings, not just adequacy judgments as for, and we evaluate translation to Czech, not English.", "labels": [], "entities": [{"text": "translation", "start_pos": 217, "end_pos": 228, "type": "TASK", "confidence": 0.9590527415275574}]}, {"text": "report \u03c4 for several automatic metrics on the whole WMT12 English-to-Czech dataset, the best of which correlates at \u03c4 = 0.18.", "labels": [], "entities": [{"text": "\u03c4", "start_pos": 7, "end_pos": 8, "type": "METRIC", "confidence": 0.9702388048171997}, {"text": "WMT12 English-to-Czech dataset", "start_pos": 52, "end_pos": 82, "type": "DATASET", "confidence": 0.940316637357076}]}, {"text": "The only common metric is METEOR and it reaches 0.16 on the whole WMT12 set.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9945281147956848}, {"text": "WMT12 set", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.9798361361026764}]}, {"text": "In line with our observation, Czech-to-English correlations reported by are higher: the best metric achieves 0.28 and averages 0.25 across four source languages.", "labels": [], "entities": []}, {"text": "The overall low sentence-level correlation of our HMEANT and WMT12 rankings is obviously caused to some extent by the problems we identified, see Section 4 below.", "labels": [], "entities": [{"text": "WMT12 rankings", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.7281081974506378}]}, {"text": "On the other hand, it is quite possible that the WMT-style rankings taken as the gold standard are of a disputable quality themselves, see Section 3.1 or the detailed report on interannotator agreement and along discussion on interpreting the rankings in.", "labels": [], "entities": []}, {"text": "Last but not least, it is likely that HMEANT and manual ranking simply measure different properties of MT outputs.", "labels": [], "entities": [{"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.980699360370636}]}, {"text": "The Kendall's \u03c4 is thus not an ultimate meta-evaluation metric for us.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Kendall's \u03c4 for sentence-level correlation with  human rankings.", "labels": [], "entities": []}, {"text": " Table 2: WMT12 system-level ranking results in three different evaluation regimes evaluated either on all sentences  or just the 50 sentences that were subject to our HMEANT annotation. The table is sorted along the first column and  the symbol \"\" in other columns marks items out of sequence.", "labels": [], "entities": [{"text": "WMT12 system-level ranking", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.5484791994094849}]}]}