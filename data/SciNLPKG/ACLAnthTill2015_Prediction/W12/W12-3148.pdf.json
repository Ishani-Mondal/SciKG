{"title": [{"text": "Forms Wanted: Training SMT on Monolingual Data. Abstract at Machine Translation and Morphologically-Rich Lan-guages", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9002761840820312}, {"text": "Abstract", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.974881112575531}, {"text": "Machine Translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7241999208927155}]}], "abstractContent": [{"text": "We provide a few insights on data selection for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8187749981880188}]}, {"text": "We evaluate the quality of the new CzEng 1.0, a parallel data source used in WMT12.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.8473663926124573}]}, {"text": "We describe a simple technique for reducing out-of-vocabulary rate after phrase extraction.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7375417202711105}]}, {"text": "We discuss the benefits of tuning towards multiple reference translations for English-Czech language pair.", "labels": [], "entities": []}, {"text": "We introduce a novel approach to data selection by full-text indexing and search: we select sentences similar to the test set from a large monolingual corpus and explore several options of incorporating them in a machine translation system.", "labels": [], "entities": [{"text": "data selection", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.6949061900377274}, {"text": "full-text indexing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.6759867519140244}]}, {"text": "We show that this method can improve translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9786852598190308}]}, {"text": "Finally, we describe our submitted system CU-TAMCH-BOJ.", "labels": [], "entities": [{"text": "CU-TAMCH-BOJ", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.8114689588546753}]}], "introductionContent": [{"text": "Selecting suitable data is important in all stages of creating an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9958947896957397}]}, {"text": "For training, the data size plays an essential role, but the data should also be as clean as possible.", "labels": [], "entities": []}, {"text": "The new CzEng 1.0 was prepared with the emphasis on data quality and we evaluate it against the previous version to show whether the effect for MT is positive.", "labels": [], "entities": [{"text": "CzEng 1.0", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.8860649764537811}, {"text": "MT", "start_pos": 144, "end_pos": 146, "type": "TASK", "confidence": 0.9807379841804504}]}, {"text": "Out-of-vocabulary rate is another problem related to data selection.", "labels": [], "entities": []}, {"text": "We present a simple technique to reduce it by including words that became spurious OOVs during phrase extraction.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7326659858226776}]}, {"text": "Another topic we explore is to use multiple references for tuning to make the procedure more robust as suggested by.", "labels": [], "entities": []}, {"text": "We evaluate this approach for translating from English into Czech.", "labels": [], "entities": [{"text": "translating from English into Czech", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.8436564326286315}]}, {"text": "The main focus of our paper however lies in presenting a method for data selection using full-text search.", "labels": [], "entities": [{"text": "data selection", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.764717310667038}]}, {"text": "We index a large monolingual corpus and then extract sentences from it that are similar to the input sentences.", "labels": [], "entities": []}, {"text": "We use these sentences in several ways: to create anew language model, anew phrase table and a tuning set.", "labels": [], "entities": []}, {"text": "The method can be seen as a kind of domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7438445389270782}]}, {"text": "We show that it contributes positively to translation quality and we provide a thorough evaluation.", "labels": [], "entities": [{"text": "translation", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9767882823944092}]}], "datasetContent": [{"text": "Domain adaptation is widely recognized as a technique which can significantly improve translation quality ().", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7979523539543152}, {"text": "translation", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.9604232907295227}]}, {"text": "In our experiments we tried to select sentences close to the source side of the test set and use them to improve the final translation.", "labels": [], "entities": []}, {"text": "The parallel data used in this section are only small: the news section of CzEng 1.0 (197k sentence pairs, 4.2M Czech words, 4.8M English words).", "labels": [], "entities": [{"text": "news section of CzEng 1.0", "start_pos": 59, "end_pos": 84, "type": "DATASET", "confidence": 0.6571324944496155}]}, {"text": "We tuned the models on WMT09 test set and evaluated on WMT11 test set.", "labels": [], "entities": [{"text": "WMT09 test set", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.9928645690282186}, {"text": "WMT11 test set", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.9949940244356791}]}, {"text": "The techniques examined here rely on a large monolingual corpus to select data from.", "labels": [], "entities": []}, {"text": "We used all the monolingual data provided by the organizers of WMT11 (18.3M sentences, 316M words).", "labels": [], "entities": [{"text": "WMT11", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.895218014717102}]}, {"text": "According to the results, using Lucene improves translation performance already in the case when only three sentences are selected for each translated sentence.", "labels": [], "entities": [{"text": "translation", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.9611977338790894}]}, {"text": "Our results are further supported by the contrastive setup that used a language model created from a random selection of the same number of sentences-the translation quality even slightly degraded.", "labels": [], "entities": []}, {"text": "On the other hand, adding more sentences to language model further improves results and the best result is achieved when the language model is created using the whole monolingual corpus.", "labels": [], "entities": []}, {"text": "This could have two reasons: Too good domain match.", "labels": [], "entities": []}, {"text": "The domain of the whole monolingual corpus is too close to the test corpus.", "labels": [], "entities": []}, {"text": "Adding the whole monolingual corpus is thus the best option.", "labels": [], "entities": []}, {"text": "For more diverse monolingual data, some domain-aware subsampling like our approach is likely to actually help.", "labels": [], "entities": []}, {"text": "Our queries to Lucene represent sentences as simple bags of words.", "labels": [], "entities": []}, {"text": "Lucene prefers less frequent words and the structure of the sentence is therefore often ignored.", "labels": [], "entities": []}, {"text": "For example it prefers to retrieve sentences with the same proper name rather than sentences with similar phrases or longer expressions.", "labels": [], "entities": []}, {"text": "This may not be the best option for language modelling.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8440271615982056}]}, {"text": "Our method can thus be useful mainly in the case when the data available are too large to be processed as a whole.", "labels": [], "entities": []}, {"text": "It can also highly reduce the computation power and time necessary to achieve good translation quality: the result achieved using the language model created via Lucene for 1000 selected sentences is not significantly worse than the result achieved using the whole monolingual corpus but the required data are 5 times smaller.: Results of experiments with Lucene, translation model applied.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of CzEng 0.9 and 1.0.", "labels": [], "entities": [{"text": "CzEng 0.9", "start_pos": 24, "end_pos": 33, "type": "DATASET", "confidence": 0.8918138742446899}]}, {"text": " Table 2: Source-side phrase table OOV.", "labels": [], "entities": [{"text": "OOV", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.8784714937210083}]}, {"text": " Table 3: BLEU scores on WMT12 test set when tuning  on WMT11 test set towards one or more references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995527863502502}, {"text": "WMT12 test set", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9864837527275085}, {"text": "WMT11 test set", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9854466319084167}]}, {"text": " Table 5: Results of experiments with Lucene, translation model applied.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.893856942653656}, {"text": "translation", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9692802429199219}]}]}