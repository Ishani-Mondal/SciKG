{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 233-241, Precision Isn't Everything: A Hybrid Approach to Grammatical Error Detection", "labels": [], "entities": [{"text": "Grammatical Error Detection", "start_pos": 149, "end_pos": 176, "type": "TASK", "confidence": 0.7920180161794027}]}], "abstractContent": [{"text": "Some grammatical error detection methods, including the ones currently used by the Educational Testing Service's e-rater system (At-tali and Burstein, 2006), are tuned for precision because of the perceived high cost of false positives (i.e., marking fluent En-glish as ungrammatical).", "labels": [], "entities": [{"text": "grammatical error detection", "start_pos": 5, "end_pos": 32, "type": "TASK", "confidence": 0.614209403594335}, {"text": "precision", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9982215762138367}]}, {"text": "Precision, however, is not optimal for all tasks, particularly the HOO 2012 Shared Task on grammatical errors , which uses F-score for evaluation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9892374277114868}, {"text": "HOO 2012 Shared Task on grammatical errors", "start_pos": 67, "end_pos": 109, "type": "TASK", "confidence": 0.6940656219209943}, {"text": "F-score", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.9923535585403442}]}, {"text": "In this paper, we extend e-rater's preposition and de-terminer error detection modules with a large-scale n-gram method (Bergsma et al., 2009) that complements the existing rule-based and classifier-based methods.", "labels": [], "entities": [{"text": "de-terminer error detection", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6569956640402476}]}, {"text": "On the HOO 2012 Shared Task, the hybrid method performed better than its component methods in terms of F-score, and it was competitive with submissions from other HOO 2012 participants.", "labels": [], "entities": [{"text": "HOO 2012 Shared Task", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.9131606072187424}, {"text": "F-score", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.996705949306488}]}], "introductionContent": [{"text": "The detection of grammatical errors is a challenging problem that, arguably, requires the use of both linguistic knowledge (e.g., in the form of rules or complex features) and large corpora for statistical learning.", "labels": [], "entities": [{"text": "detection of grammatical errors", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8724851459264755}]}, {"text": "Additionally, grammatical error detection can be applied in various scenarios (e.g., automated essay scoring, writing assistance, language learning), many of which may benefit from task-specific adaptation or tuning.", "labels": [], "entities": [{"text": "grammatical error detection", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.7399798035621643}, {"text": "essay scoring", "start_pos": 95, "end_pos": 108, "type": "TASK", "confidence": 0.6767013221979141}]}, {"text": "For example, one might want to take a different approach when detecting errors for the purpose of providing feedback than when detecting errors to evaluate the quality of writing in an essay.", "labels": [], "entities": []}, {"text": "Thus, it seems desirable to take a flexible approach to grammatical error detection that incorporates multiple, complementary techniques.", "labels": [], "entities": [{"text": "grammatical error detection", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.7611141204833984}]}, {"text": "In this paper, we extend the preposition and determiner error detection modules currently used in the Educational Testing Service's e-rater automated essay scoring system () for the HOO 2012 Shared Task on grammatical errors ( \u00a72).", "labels": [], "entities": [{"text": "determiner error detection", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.5825760463873545}, {"text": "HOO 2012 Shared Task on grammatical errors", "start_pos": 182, "end_pos": 224, "type": "TASK", "confidence": 0.6329933490071978}]}, {"text": "We refer to this set of modules from e-rater as our \"base system\" ( \u00a73).", "labels": [], "entities": []}, {"text": "While the base system uses statistical methods to learn models of grammatical English, it also leverages substantial amounts of linguistic knowledge in the form of various hand-coded filters and complex syntactic features.", "labels": [], "entities": []}, {"text": "The base system is also tuned for high precision at the expense of recall in order to avoid a high rate of potentially costly false positives (i.e., frequent marking of correct English sentences as ungrammatical).", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9986802935600281}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9990195035934448}]}, {"text": "We apply the pre-existing base system without modifications but complement it with a large-scale n-gram method ( \u00a75) based on work by.", "labels": [], "entities": []}, {"text": "The n-gram method employs very little linguistic knowledge and instead relies almost exclusively upon corpus statistics.", "labels": [], "entities": []}, {"text": "We also tune the resulting hybrid system with labeled training data in order to maximize the primary evaluation metric used in the HOO 2012 Shared Task: balanced F-score, or F 1 ( \u00a76).", "labels": [], "entities": [{"text": "HOO 2012 Shared Task", "start_pos": 131, "end_pos": 151, "type": "DATASET", "confidence": 0.7951423972845078}, {"text": "F-score", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.9453474879264832}, {"text": "F 1", "start_pos": 174, "end_pos": 177, "type": "METRIC", "confidence": 0.9801884293556213}]}, {"text": "We find that the tuned hybrid system improves upon the recall and F-score of the base system.", "labels": [], "entities": [{"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9998101592063904}, {"text": "F-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9958807229995728}]}, {"text": "Also, in the HOO 2012 Shared Task, the hybrid system achieved results that were competitive with other submitted grammatical error detection systems ( \u00a77).", "labels": [], "entities": [{"text": "HOO 2012 Shared Task", "start_pos": 13, "end_pos": 33, "type": "DATASET", "confidence": 0.7369823157787323}, {"text": "grammatical error detection", "start_pos": 113, "end_pos": 140, "type": "TASK", "confidence": 0.6730958620707194}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Precision, recall, and F-score for the combined  preposition and determiner error detection subtask for  various methods, before participant-suggested revisions  to the gold standard were applied. All values are percent- ages. Official run numbers are shown in the \"run\" col- umn. The \"n-gram\" run was not part of our official sub- mission. For comparison, \"UI\" is the submission, from  another team, that achieved the highest detection F-score  in the HOO 2012 Shared Task.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9983181953430176}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9971543550491333}, {"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9992215633392334}, {"text": "determiner error detection", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.580528457959493}, {"text": "HOO 2012 Shared Task", "start_pos": 463, "end_pos": 483, "type": "DATASET", "confidence": 0.787348598241806}]}]}