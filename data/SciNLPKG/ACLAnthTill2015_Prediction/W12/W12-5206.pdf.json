{"title": [{"text": "A New DOP Model for Phrase-structure Parsing of Persian Sentences", "labels": [], "entities": [{"text": "Phrase-structure Parsing of Persian Sentences", "start_pos": 20, "end_pos": 65, "type": "TASK", "confidence": 0.8952679991722107}]}], "abstractContent": [{"text": "In this paper we employ a most recent approach to Data Oriented Parsing (DOP), which has named Double-Dop, for Persian sentences.", "labels": [], "entities": [{"text": "Data Oriented Parsing (DOP)", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.8253105084101359}]}, {"text": "Like other DOP models, Double-Dop parser utilizes syntactic fragments of arbitrary size from a treebank to analyse new sentences, but it extracts a restricted yet representative subset of fragments.", "labels": [], "entities": []}, {"text": "It uses only those which are encountered at least twice.", "labels": [], "entities": []}, {"text": "The accuracy of Double-DOP is well within the range of state-of-the-art parsers currently used in other NLP-tasks, while offering the additional benefits of a simple generative probability model and an explicit representation of grammatical constructions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995546936988831}]}, {"text": "Heretofore there isn't any standard parser for Persian language and this work try to employ Double-Dop Method for parsing Persian sentences.", "labels": [], "entities": [{"text": "parsing Persian sentences", "start_pos": 114, "end_pos": 139, "type": "TASK", "confidence": 0.8725436329841614}]}], "introductionContent": [{"text": "The Data-Oriented Parsing (DOP) framework, is a famous and wide-coverage parsing method which was first proposed by) and formalized by Rens Bod.", "labels": [], "entities": [{"text": "wide-coverage parsing", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.585505872964859}]}, {"text": "Its underlying assumption is that human perception of language based on previous language experiences rather than abstract grammar rules.", "labels": [], "entities": []}, {"text": "In the most prominent DOP variants, certain subtrees (called fragments) of variable size, are extracted from the parse trees of the treebank during the training process.", "labels": [], "entities": []}, {"text": "These fragments are assigned weights between 0 and 1.", "labels": [], "entities": []}, {"text": "Fragments can be recombined to assign parse trees to new sentences.", "labels": [], "entities": []}, {"text": "The first implementation of DOP, DOP1, and its developed versions(e.g.) aimed at extracting all subtrees of all trees in the treebank.", "labels": [], "entities": []}, {"text": "The total number of constructions, however, is prohibitively large for nontrivial treebanks: it grows exponentially with the length of the sentences, yielding the astronomically large number of approximately 10 48 for section 2-21 of the Penn WSJ corpus.", "labels": [], "entities": [{"text": "Penn WSJ corpus", "start_pos": 238, "end_pos": 253, "type": "DATASET", "confidence": 0.9796506563822428}]}, {"text": "Later DOP models have used the Goodman transformation)to obtain a compact representation of all fragments in the treebank.", "labels": [], "entities": []}, {"text": "The transformation was defined for some versions of DOP to an equivalent PCFG-based model, with the number of rules extracted from each parse tree being linear in the size of the trees.", "labels": [], "entities": []}, {"text": "Bod has argued for the Goodman transform as the solution to the computational challenges of DOP (e.g.,); it is important to realize, however, that the resulting grammars are still very large: WSJ sections 2-21 yield about7.8 \u00d7 10 6 rules in the basic version of Goodman's transform.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 192, "end_pos": 195, "type": "DATASET", "confidence": 0.8655146956443787}]}, {"text": "Moreover, the transformed grammars differ from untransformed DOP grammars in that larger fragments are no longer explicitly represented.", "labels": [], "entities": []}, {"text": "This way, an attractive feature of DOP, viz.", "labels": [], "entities": []}, {"text": "the explicit representation of the 'productive units' of language, is lost.", "labels": [], "entities": []}, {"text": "In this paper we use a novel DOP model(Double-DOP) in which we extract a restricted yet representative subset of fragments: those recurring at least twice in the treebank(Sangati and Zuidema 2011).", "labels": [], "entities": []}, {"text": "The accuracy of Double-DOP is well within the range of state-of-the-art parsers currently used in other NLP-tasks, while offering the additional benefits of a simple generative probability model and an explicit representation of grammatical constructions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995546936988831}]}, {"text": "This model reduces the number of extracted fragments from the astronomical 10 48 to around 10 6 . The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In section 2 we describe Formal Specification of DOP model in general and Double-DOP model in detail, which we will use for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 124, "end_pos": 131, "type": "TASK", "confidence": 0.9720584750175476}]}, {"text": "In section 3 we illustrate the Implementation phase and the difficulties of Persian sentences parsing which we are encountered and finally we come to conclusion.", "labels": [], "entities": [{"text": "Implementation phase", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.891820639371872}, {"text": "Persian sentences parsing", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.6325714985529581}]}], "datasetContent": [{"text": "In order to build and test our Double-DOP model we employ the Persian treebank-PTB(Ghayoomi 2012).", "labels": [], "entities": [{"text": "Persian treebank-PTB(Ghayoomi 2012)", "start_pos": 62, "end_pos": 97, "type": "DATASET", "confidence": 0.9088907539844513}]}, {"text": "Preparing the treebank: We start with some pre-processing of the treebank, following standard parsing methods.", "labels": [], "entities": []}, {"text": "We named this step preparing the treebank and performed following tasks in this step in order: first we divide the treebank into two sections of train and test and assign about 1000 sentences for train and 200 sentences for test.", "labels": [], "entities": []}, {"text": "Next we have removed all empty nodes, functional tags, semantic tags, traces and also punctuations from the treebank.", "labels": [], "entities": []}, {"text": "After that we apply binarization procedure to training pars trees of treebank.", "labels": [], "entities": []}, {"text": "Binarization is particularly important for generative models like DOP and PCFGs, and was essential for the success of the model, where all the children of an internal node are produced at once.", "labels": [], "entities": [{"text": "Binarization", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8838022351264954}]}, {"text": "Binarization, in fact, provides away to generalize flat rules, by splitting it in multiple generation steps.", "labels": [], "entities": []}, {"text": "Double-DOP model creators claim that, on an unbinarized treebank, the model performed rather poorly because of the abundance of flat rules.", "labels": [], "entities": []}, {"text": "However, our current model uses a strict left binarization as in).", "labels": [], "entities": []}], "tableCaptions": []}