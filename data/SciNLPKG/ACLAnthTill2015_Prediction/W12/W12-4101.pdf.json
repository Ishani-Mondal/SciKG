{"title": [{"text": "A New Parametric Estimation Method for Graph-based Clustering", "labels": [], "entities": []}], "abstractContent": [{"text": "Relational clustering has received much attention from researchers in the last decade.", "labels": [], "entities": [{"text": "Relational clustering", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9538326859474182}]}, {"text": "In this paper we present a parametric method that employs a combination of both hard and soft clustering.", "labels": [], "entities": []}, {"text": "Based on the corresponding Markov chain of an affinity matrix, we simulate a probability distribution on the states by defining a conditional probability for each subpopulation of states.", "labels": [], "entities": []}, {"text": "This probabilistic model would enable us to use expectation maximization for parameter estimation.", "labels": [], "entities": []}, {"text": "The effectiveness of the proposed approach is demonstrated on several real datasets against spectral clustering methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Clustering methods based on pairwise similarity of data points have received much attention in machine learning circles and have been shown to be effective on a variety of tasks ().", "labels": [], "entities": []}, {"text": "Apart from pure relational data e.g. Biological networks), Social Networks, these methods can also be applied to none relational data them e.g. text), image, where the edges indicate the affinity of the data points in the dataset.", "labels": [], "entities": []}, {"text": "Relational clustering has been addressed from different perspectives e.g. spectral learning, random walks (), trace maximization () and probabilistic models.", "labels": [], "entities": [{"text": "Relational clustering", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6832994818687439}, {"text": "spectral learning", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.8053263425827026}]}, {"text": "Some works have proposed frameworks fora unified view of different approaches.", "labels": [], "entities": []}, {"text": "In (Meila and Shi 2000) a random walk view of the spectral clustering algorithm in was presented.", "labels": [], "entities": []}, {"text": "By selecting an appropriate kernel, kernel k-means and spectral clustering are also proved to be equivalent).", "labels": [], "entities": []}, {"text": "As shown in) the basic idea behind most methods are somehow optimizing the normalized cut objective function.", "labels": [], "entities": []}, {"text": "We propose anew perspective on relational clustering where we use the corresponding Markov chain of a similarity graph to iteratively cluster the nodes.", "labels": [], "entities": [{"text": "relational clustering", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.7889787256717682}]}, {"text": "Starting from a random distribution of nodes in groups and given the transition probabilities of the Markov chain, we use expectation maximization (EM) to estimate the membership of nodes in each group to eventually find the best partitioning.", "labels": [], "entities": [{"text": "expectation maximization (EM)", "start_pos": 122, "end_pos": 151, "type": "METRIC", "confidence": 0.8709538817405701}]}, {"text": "After a brief review of the literature in section 2, we present our clustering algorithm in detail (section 3) and report experiments and evaluation (section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "We use datasets provided in (  Since the ground truth for the datasets we have used is available, we evaluate the clustering results against the labels using three measures: cluster purity (Purity), normalized mutual information (NMI), and Rand index (RI).", "labels": [], "entities": [{"text": "cluster purity (Purity)", "start_pos": 174, "end_pos": 197, "type": "METRIC", "confidence": 0.8533725261688232}, {"text": "normalized mutual information (NMI)", "start_pos": 199, "end_pos": 234, "type": "METRIC", "confidence": 0.6563137720028559}, {"text": "Rand index (RI)", "start_pos": 240, "end_pos": 255, "type": "METRIC", "confidence": 0.9818633317947387}]}, {"text": "All three metrics are used to guarantee a more comprehensive evaluation of clustering results (for example, NMI takes into account cluster size distribution, which is disregarded by Purity).", "labels": [], "entities": []}, {"text": "We refer the reader to for details regarding all these measures.", "labels": [], "entities": []}, {"text": "In order to find the most likely result, each algorithm is run 100 times and the average in each criterion is reported.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. HSC  scores highest on all text datasets, on all three  evaluation metrics and just well on social network  data. The main reason for the effectiveness of HSC  is in its use of both local and global structure of the  graph. While the conditional probability  () () = ; )  looks at the immediate", "labels": [], "entities": []}]}