{"title": [{"text": "Towards Mediating Shared Perceptual Basis in Situated Dialogue", "labels": [], "entities": [{"text": "Mediating Shared Perceptual Basis in Situated Dialogue", "start_pos": 8, "end_pos": 62, "type": "TASK", "confidence": 0.7564105476651873}]}], "abstractContent": [{"text": "To enable effective referential grounding in situated human robot dialogue, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities.", "labels": [], "entities": [{"text": "referential grounding", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.8657870292663574}]}, {"text": "In particular , we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references.", "labels": [], "entities": []}, {"text": "Our empirical results have shown that, even when computer vision algorithms produce many errors (e.g. 84.7% of the objects in the environment are mis-recognized), our approach can still achieve 66% accuracy in referential grounding.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9992008805274963}, {"text": "referential grounding", "start_pos": 210, "end_pos": 231, "type": "TASK", "confidence": 0.8984118700027466}]}, {"text": "These results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction.", "labels": [], "entities": []}], "introductionContent": [{"text": "To support natural interaction between a human and a robot, technology enabling human robot dialogue has become increasingly important.", "labels": [], "entities": []}, {"text": "Human robot dialogue often involves objects and their identities in the environment.", "labels": [], "entities": [{"text": "Human robot dialogue", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.645620067914327}]}, {"text": "One critical problem is interpretation and grounding of references -a process to establish mutual understanding between conversation partners about intended references.", "labels": [], "entities": [{"text": "interpretation and grounding of references -a process to establish mutual understanding between conversation partners about intended references", "start_pos": 24, "end_pos": 167, "type": "Description", "confidence": 0.7655370947387483}]}, {"text": "The robot needs to identify referents in the environment that are specified by its human partner and the partner needs to recognize that the intended referents are correctly understood.", "labels": [], "entities": []}, {"text": "It is critical for the robot and its partner to quickly and reliably reach the mutual acceptance of references before conversation can move forward.", "labels": [], "entities": []}, {"text": "Despite recent progress (, interpreting and grounding references remains a very challenging problem.", "labels": [], "entities": [{"text": "interpreting", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.9808678030967712}]}, {"text": "In situated interaction, although a robot and its human partner are co-present in a shared environment, they have significantly mismatched perceptual capabilities (e.g., recognizing objects in the surroundings).", "labels": [], "entities": []}, {"text": "Their knowledge and representation of the shared world are significantly different.", "labels": [], "entities": []}, {"text": "When a shared perceptual basis is missing, grounding references to the environment will be difficult.", "labels": [], "entities": []}, {"text": "Therefore, a foremost question is to understand how partners with mismatched perceptual capabilities mediate shared basis to achieve referential grounding.", "labels": [], "entities": [{"text": "referential grounding", "start_pos": 133, "end_pos": 154, "type": "TASK", "confidence": 0.8761631548404694}]}, {"text": "To address this problem, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities.", "labels": [], "entities": []}, {"text": "In particular, we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references.", "labels": [], "entities": []}, {"text": "Our empirical results have shown that, even when the perception of the environment by computer vision algorithms has a high error rate (84.7% of the objects are mis-recognized), our approach can still correctly ground those misrecognized objects with 66% accuracy.", "labels": [], "entities": [{"text": "error rate", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9527766406536102}, {"text": "accuracy", "start_pos": 255, "end_pos": 263, "type": "METRIC", "confidence": 0.9979223608970642}]}, {"text": "The results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solu-tion to mediate shared perceptual basis for referential grounding in situated interaction.", "labels": [], "entities": []}, {"text": "In the following sections, we first describe an empirical study based on a virtual environment to examine how partners mediate their mismatched visual perceptual basis.", "labels": [], "entities": []}, {"text": "We then provide details about our graph matching based approach and its evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "The setup of our experimental system is shown in.", "labels": [], "entities": []}, {"text": "In the experiment, two human partners (a director and a matcher) collaborate on an object naming task.", "labels": [], "entities": [{"text": "object naming task", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7871775925159454}]}, {"text": "The mismatched perceptual capabilities between partners are simulated by different versions of an image shown to them: the director looks at an original image, while the matcher looks at an impoverished version of the original image.", "labels": [], "entities": []}, {"text": "The original image (the one on the left in) was created by randomly selecting images of daily-life items (office supplies, fruits, etc.) from an image database and randomly positioning them onto a background.", "labels": [], "entities": []}, {"text": "To create the impoverished im-age (the one on the right in), we applied standard Computer Vision (CV) algorithms to process the original image and then create an abstract representation based on the outputs from the CV algorithms.", "labels": [], "entities": []}, {"text": "More specifically, the original image was fed into a segmentation \u2192 feature extraction \u2192 recognition pipeline of CV algorithms.", "labels": [], "entities": [{"text": "segmentation \u2192 feature extraction \u2192 recognition", "start_pos": 53, "end_pos": 100, "type": "TASK", "confidence": 0.7232620219389597}]}, {"text": "First, the OTSU algorithm was used for image segmentation.", "labels": [], "entities": [{"text": "image segmentation", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8240271210670471}]}, {"text": "Then visual features such as color and shape were extracted from the segmented regions ().", "labels": [], "entities": []}, {"text": "Finally, object recognition was done by searching the nearest neighbor (in the shape-feature vector space) from a knowledge base of \"known\" objects.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.9237203299999237}]}, {"text": "The impoverished image was then created based on the CV algorithms' outputs.", "labels": [], "entities": []}, {"text": "For example, if an object in the original image was recognized as a pear, an abstract illustration of pear would be displayed in the impoverished image at the same position.", "labels": [], "entities": []}, {"text": "Other features suchlike color and size of the object were also extracted from the original image and assigned to the illustration in the impoverished image.", "labels": [], "entities": []}, {"text": "In the naming task, the director's goal is to communicate the \"secret names\" of some randomly selected objects (i.e., target objects) in his/her image to the matcher, so that the matcher would know which object has what name.", "labels": [], "entities": []}, {"text": "As shown in, those secret names are displayed only on the director's screen but not the matcher's.", "labels": [], "entities": []}, {"text": "Once the matcher believes that he/she correctly acquires the name of an target object, he/she will record the name by mouseclicking on the target and repeating the name.", "labels": [], "entities": []}, {"text": "A task is considered complete when the matcher has recorded the names of all the target objects.", "labels": [], "entities": []}], "tableCaptions": []}