{"title": [{"text": "Describing Video Contents in Natural Language", "labels": [], "entities": [{"text": "Describing Video Contents in Natural Language", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8500410119692484}]}], "abstractContent": [{"text": "This contribution addresses generation of natural language descriptions for human actions , behaviour and their relations with other objects observed in video streams.", "labels": [], "entities": []}, {"text": "The work starts with implementation of conventional image processing techniques to extract high level features from video.", "labels": [], "entities": []}, {"text": "These features are converted into natural language descriptions using context free grammar.", "labels": [], "entities": []}, {"text": "Although feature extraction processes are erroneous at various levels, we explore approaches to putting them together to produce a coherent description.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7279535233974457}]}, {"text": "Evaluation is made by calculating ROUGE scores between human annotated and machine generated descriptions.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9819843173027039}]}, {"text": "Further we introduce a task based evaluation by human subjects which provides qualitative evaluation of generated descriptions.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years video has established its dominance in communication and has become an integrated part of our everyday life ranging from hand-held videos to broadcast news video (from unstructured to highly structured).", "labels": [], "entities": []}, {"text": "There is a need for formalising video semantics to help users gain useful and refined information relevant to their demands and requirements.", "labels": [], "entities": []}, {"text": "Human language is a natural way of communication.", "labels": [], "entities": []}, {"text": "Useful entities extracted from videos and their inter-relations can be presented by natural language in a syntactically and semantically correct formulation.", "labels": [], "entities": []}, {"text": "While literature relating to object recognition (, human action recognition (, and emotion detection ( are moving towards maturity, automatic description of visual scenes is still in its infancy.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8217063844203949}, {"text": "human action recognition", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.657287210226059}, {"text": "emotion detection", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7277711629867554}, {"text": "automatic description of visual scenes", "start_pos": 132, "end_pos": 170, "type": "TASK", "confidence": 0.7608471691608429}]}, {"text": "Most studies in video retrieval have been based on keywords).", "labels": [], "entities": [{"text": "video retrieval", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.6382600516080856}]}, {"text": "An interesting extension to a keyword based scheme is natural language textual description of video streams.", "labels": [], "entities": [{"text": "natural language textual description of video streams", "start_pos": 54, "end_pos": 107, "type": "TASK", "confidence": 0.7309028591428485}]}, {"text": "They are more human friendly.", "labels": [], "entities": []}, {"text": "They can clarify context between keywords by capturing their relations.", "labels": [], "entities": []}, {"text": "Descriptions can guide generation of video summaries by converting a video to natural language.", "labels": [], "entities": [{"text": "generation of video summaries", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.8539153784513474}]}, {"text": "They can provide basis for creating a multimedia repository for video analysis, retrieval and summarisation tasks.", "labels": [], "entities": [{"text": "summarisation tasks", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8550751805305481}]}, {"text": "presented a method for describing human activities in videos based on a concept hierarchy of actions.", "labels": [], "entities": []}, {"text": "They described head, hands and body movements using natural language.", "labels": [], "entities": []}, {"text": "For a traffic control application, Nagel (2004) investigated automatic visual surveillance systems where human behaviour was presented by scenarios, consisting of predefined sequences of events.", "labels": [], "entities": [{"text": "traffic control", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.7895847260951996}]}, {"text": "The scenario was evaluated and automatically translated into a text by analysing the visual contents overtime, and deciding on the most suitable event.", "labels": [], "entities": []}, {"text": "introduced a framework for semantic annotation of visual events in three steps; image parsing, event inference and language generation.", "labels": [], "entities": [{"text": "semantic annotation of visual events", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.7715986430644989}, {"text": "image parsing", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.7284436225891113}, {"text": "language generation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7503881454467773}]}, {"text": "Instead of humans and their specific activities, they focused on object detection, their inter-relations and events that were present in videos.", "labels": [], "entities": [{"text": "object detection", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.730649396777153}]}, {"text": "performed human identification and scene modelling manually and focused on human behaviour description for crosswalk scenes.", "labels": [], "entities": [{"text": "human identification", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7951912581920624}, {"text": "scene modelling", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.6834422498941422}, {"text": "human behaviour description", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.6797665556271871}]}, {"text": "introduced their work on video to text description which is dependent on the significant amount of annotated data, a requirement that is avoided in this paper.", "labels": [], "entities": [{"text": "text description", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.6962991058826447}]}, {"text": "presented a framework for static images to textual descriptions where they contained to image with up to two objects.", "labels": [], "entities": []}, {"text": "In contrast, this paper presents a work on video streams, handling not only objects but also other features such as actions, age, gender and emotions.", "labels": [], "entities": []}, {"text": "The study presented in this paper is concerned with production of natural language description for visual scenes in a time series using a bottomup approach.", "labels": [], "entities": [{"text": "production of natural language description", "start_pos": 52, "end_pos": 94, "type": "TASK", "confidence": 0.6922706723213196}]}, {"text": "Initially high level features (HLFs) are identified in video frames.", "labels": [], "entities": []}, {"text": "They maybe 'keywords', such as a particular object and its position/moves, used fora semantic indexing task in video retrieval.", "labels": [], "entities": []}, {"text": "Spatial relations between HLFs are important when explaining the semantics of visual scene.", "labels": [], "entities": []}, {"text": "Extracted HLFs are then presented by syntactically and semantically correct expressions using a template based approach.", "labels": [], "entities": []}, {"text": "Image processing techniques are far from perfect; there can be many missing, misidentified and erroneously extracted HLFs.", "labels": [], "entities": [{"text": "Image processing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8893245160579681}]}, {"text": "We present scenarios to overcome these shortcomings and to generate coherent natural descriptions.", "labels": [], "entities": []}, {"text": "The approach is evaluated using video segments drafted manually from the TREC video dataset.", "labels": [], "entities": [{"text": "TREC video dataset", "start_pos": 73, "end_pos": 91, "type": "DATASET", "confidence": 0.9531212647755941}]}, {"text": "ROUGE scores is calculated between human annotated and machine generated descriptions.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9579695463180542}]}, {"text": "A task based evaluation is performed by human subjects, providing qualitative evaluation of generated descriptions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset was manually created from a subset of rushes and HLF extraction task videos in.", "labels": [], "entities": [{"text": "HLF extraction task", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7903662522633871}]}, {"text": "It consists of 140 segments, with each segment containing one camera shot, spanning 10 to 30 seconds in length.", "labels": [], "entities": []}, {"text": "There are 20 video segments for each of the seven categories: Action: Human can be seen performing some action (e.g., sit, walk) Closeup: Facial expressions/emotions can be seen (e.g., happy, sad) News: Anchor/reporter maybe seen; particular scene settings (e.g., weatherboard in the background) Meeting: Multiple humans are seen interacting; presence of objects such as chairs and a 13 human subjects individually annotated these videos in one to seven short sentences.", "labels": [], "entities": []}, {"text": "They are referred to as hand annotations in the rest of this paper.", "labels": [], "entities": []}, {"text": "Difficulty in evaluating natural language descriptions stems from the fact that it is not a simple task to define the criteria.", "labels": [], "entities": [{"text": "evaluating natural language descriptions", "start_pos": 14, "end_pos": 54, "type": "TASK", "confidence": 0.615523524582386}]}, {"text": "We adopted ROUGE, widely used for evaluating automatic summarisation, to calculate the overlap between machine generated and hand annotations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9944317936897278}, {"text": "summarisation", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.8275197148323059}]}, {"text": "shows the results where higher ROUGE score indicates closer match between them.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.989643931388855}]}, {"text": "In overall scores were not very high, demonstrating the fact that humans have different observations and interests while watching the same video.", "labels": [], "entities": []}, {"text": "Descriptions were often subjective, de-: ROUGE scores between machine generated descriptions (reference) and 13 hand annotations (model).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9915258288383484}]}, {"text": "ROUGE 1-3 shows n-gram overlap similarity between reference and model descriptions.", "labels": [], "entities": []}, {"text": "ROUGE-L is based on longest common subsequence (LCS).", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.68941730260849}, {"text": "longest common subsequence (LCS)", "start_pos": 20, "end_pos": 52, "type": "METRIC", "confidence": 0.7476536631584167}]}, {"text": "ROUGE-W is for weighted LCS.", "labels": [], "entities": [{"text": "ROUGE-W", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9705470204353333}]}, {"text": "ROUGE-S skips bigram co-occurrence without gap length.", "labels": [], "entities": [{"text": "ROUGE-S", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9284955263137817}]}, {"text": "ROUGE-SU shows results for skip bigram co-occurrence with unigrams.", "labels": [], "entities": [{"text": "ROUGE-SU", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9333860278129578}]}, {"text": "pendent on one's perception and understanding, that might have been affected by their educational and professional background, personal interests and experiences.", "labels": [], "entities": []}, {"text": "Nevertheless ROUGE scores were not hopelessly low for machine generated descriptions; Closeup, Action and News videos had higher scores because of presence of humans with well defined actions and emotions.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9901275634765625}]}, {"text": "Indoor/Outdoor videos show the poorest results due to the limited capability of image processing techniques.", "labels": [], "entities": []}, {"text": "Similar to human in the loop evaluation (Nwogu et al., 2011), a task based evaluation was performed to make qualitative evaluation of the generated descriptions.", "labels": [], "entities": []}, {"text": "Given a machine generated description, human subjects were instructed to find a corresponding video stream out of 10 candidate videos having the same theme (e.g., a description of a Closeup against 10 Closeup videos).", "labels": [], "entities": []}, {"text": "Once a choice was made, each subject was provided with the correct video stream and a questionnaire.", "labels": [], "entities": []}, {"text": "The first question was how well the description explained the actual video, rating from 'explained completely', 'satisfactorily', 'fairly', 'poorly', or 'does not explain'.", "labels": [], "entities": []}, {"text": "The second question was concerned with the ranking of usefulness for including various visual contents (e.g., human, objects, their moves, their relations, background) in the description.", "labels": [], "entities": []}, {"text": "Seven human subjects conducted this evaluation searching a corresponding video for each often machine generated descriptions.", "labels": [], "entities": []}, {"text": "They did not involve creation of the dataset, hence they saw these videos for the first time.", "labels": [], "entities": []}, {"text": "On average, they were able to identify correct videos for 53% 6 of It is interesting to note the correct identification rate descriptions.", "labels": [], "entities": []}, {"text": "They rated 68%, 48%, and 40% of descriptions explained the actual video 'fairly', 'satisfactorily', and 'completely'.", "labels": [], "entities": []}, {"text": "Because multiple videos might have very similar text descriptions, it was worth testing meaningfulness of descriptions for choosing the corresponding video.", "labels": [], "entities": []}, {"text": "Finally, usefulness of visual contents had mix results.", "labels": [], "entities": []}, {"text": "For about 84% of descriptions, subjects were able to identify videos based on information related to humans, their actions, emotions and interactions with other objects.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Confusion tables for (a) human detection and  (b) gender identification. Columns show the ground  truth, and rows indicate the automatic recognition re- sults. The human detection task is biased towards exis- tence of human, while in the gender identification pres- ence of male and female are roughly balanced.", "labels": [], "entities": [{"text": "human detection", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7715983092784882}, {"text": "gender identification", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7649983763694763}, {"text": "human detection", "start_pos": 174, "end_pos": 189, "type": "TASK", "confidence": 0.7053154557943344}]}, {"text": " Table 2: Confusion table for human action recogni- tion. Columns show the ground truth, and rows indi- cate the automatic recognition results. Some actions  (e.g., 'standing') were more commonly seen than oth- ers (e.g., 'waving').", "labels": [], "entities": []}, {"text": " Table 3: Confusion table for human emotion recogni- tion. Columns show the ground truth, and rows indi- cate the automatic recognition results.", "labels": [], "entities": []}, {"text": " Table 4: ROUGE scores between machine generated descriptions (reference) and 13 hand annotations (model).  ROUGE 1-3 shows n-gram overlap similarity between reference and model descriptions. ROUGE-L is based on  longest common subsequence (LCS). ROUGE-W is for weighted LCS. ROUGE-S skips bigram co-occurrence  without gap length. ROUGE-SU shows results for skip bigram co-occurrence with unigrams.", "labels": [], "entities": []}]}