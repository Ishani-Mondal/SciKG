{"title": [{"text": "Analysing the Effect of Out-of-Domain Data on SMT Systems", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9876077771186829}]}], "abstractContent": [{"text": "In statistical machine translation (SMT), it is known that performance declines when the training data is in a different domain from the test data.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8050123353799185}]}, {"text": "Nevertheless, it is frequently necessary to supplement scarce in-domain training data with out-of-domain data.", "labels": [], "entities": []}, {"text": "In this paper , we first try to relate the effect of the out-of-domain data on translation performance to measures of corpus similarity, then we separately analyse the effect of adding the out-of-domain data at different parts of the training pipeline (alignment, phrase extraction, and phrase scoring).", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 264, "end_pos": 281, "type": "TASK", "confidence": 0.8326698839664459}, {"text": "phrase scoring", "start_pos": 287, "end_pos": 301, "type": "TASK", "confidence": 0.7842278480529785}]}, {"text": "Through experiments in 2 domains and 8 language pairs it is shown that the out-of-domain data improves coverage and translation of rare words, but may degrade the translation quality for more common words.", "labels": [], "entities": [{"text": "translation of rare words", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.8242570012807846}]}], "introductionContent": [{"text": "In statistical machine translation (SMT), domain adaptation can bethought of as the problem of training a system on data mainly drawn from one domain (e.g. parliamentary proceedings) and trying to maximise its performance on a different domain (e.g. news).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8314498066902161}, {"text": "domain adaptation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7463634610176086}]}, {"text": "There is likely to be some parallel data similar to the test data, but as such data is expensive to create, it tends to be scarce.", "labels": [], "entities": []}, {"text": "The concept of \"domain\" is rarely given a precise definition, but it is normally understood that data from the same domain is in some sense similar (for example in the words and grammatical constructions used) and data from different domains shows less similarities.", "labels": [], "entities": []}, {"text": "Data from the same domain as the test set is usually referred to as in-domain and data from a different domain is referred to as out-of-domain.", "labels": [], "entities": []}, {"text": "The aim of this paper is to shed some light on what domain actually is, and why it matters.", "labels": [], "entities": []}, {"text": "The fact that a mismatch between training and test data domains reduces translation performance has been observed in previous studies, and will be confirmed here for multiple data sets and languages, but the question of why domain matters for performance has not been fully addressed in the literature.", "labels": [], "entities": []}, {"text": "Experiments in this paper will be conducted on phrasebased machine translation (PBMT) systems, but similar conclusions are likely to apply to other types of SMT systems.", "labels": [], "entities": [{"text": "phrasebased machine translation (PBMT)", "start_pos": 47, "end_pos": 85, "type": "TASK", "confidence": 0.7737742563088735}, {"text": "SMT", "start_pos": 157, "end_pos": 160, "type": "TASK", "confidence": 0.9878385066986084}]}, {"text": "Furthermore, we will mainly be concerned with the effect of domain on the translation model, since it depends on parallel data which is more likely to be in short supply than monolingual data, and domain adaptation for language modelling has been more thoroughly studied.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 197, "end_pos": 214, "type": "TASK", "confidence": 0.718543291091919}, {"text": "language modelling", "start_pos": 219, "end_pos": 237, "type": "TASK", "confidence": 0.7123887985944748}]}, {"text": "The effect of a shift of domain in the parallel data is complicated by the fact that training a translation model is a multi-stage process.", "labels": [], "entities": []}, {"text": "First the parallel data is word-aligned, normally using the IBM models (, then phrases are extracted using some heuristics () and scored using a maximum likelihood estimate.", "labels": [], "entities": [{"text": "IBM models", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.9263994693756104}]}, {"text": "Since the effect of domain maybe felt at the alignment stage, the extraction stage, or the scoring stage, we have designed experiments to try to tease these apart.", "labels": [], "entities": []}, {"text": "Experiments comparing the effect of domain at the alignment stage with the extraction and scoring stages have already been presented by), so we focus more on the differences between extraction and scoring.", "labels": [], "entities": []}, {"text": "In other words, we examine whether adding more data (in or out-of domain) helps improve coverage of the phrase table, or helps improve the scoring of phrases.", "labels": [], "entities": []}, {"text": "A further question that we wish to address is whether adding out-of-domain parallel data affects words with different frequencies to different degrees.", "labels": [], "entities": []}, {"text": "For example, a large out-of-domain data set may improve the translation of rare words by providing better coverage, but degrade translation of more common words by providing erroneous out-ofdomain translations.", "labels": [], "entities": [{"text": "translation of rare words", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.8340761214494705}]}, {"text": "In fact, the evidence presented in Section 3.5 will show a much clearer effect on low frequency words than on medium or high frequency words, but the total token count of these low frequency words is still small, so they don't necessarily have much effect on overall measures of translation quality.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 279, "end_pos": 298, "type": "TASK", "confidence": 0.8266092240810394}]}, {"text": "In summary, the main contributions of this paper are: \u2022 It presents experiments on 8 language pairs and 2 domains showing the effect on BLEU of adding out-of-domain data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9929060935974121}]}, {"text": "\u2022 It provides evidence that the difference between in and out-of domain translation performance is correlated with differences in word distribution and out-of-vocabulary rates.", "labels": [], "entities": []}, {"text": "\u2022 It develops a method for separating the effects of phrase extraction and scoring, showing that good coverage is nearly always more important than good scoring, and that out-of-domain data can adversely affect phrase scores.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.806742399930954}]}, {"text": "\u2022 It shows that adding out-of-domain data clearly improves translation of rare words, but may have a small negative effect on more common words.", "labels": [], "entities": [{"text": "translation of rare words", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.8829054236412048}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Summary of the data sets used, with ap- proximate sentence counts", "labels": [], "entities": []}]}