{"title": [{"text": "Assigning Deep Lexical Types Using Structured Classifier Features for Grammatical Dependencies", "labels": [], "entities": [{"text": "Assigning Deep Lexical Types", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8601671159267426}]}], "abstractContent": [{"text": "Deep linguistic grammars are able to provide rich and highly complex grammatical representations of sentences, capturing, for instance, long-distance dependencies and returning a semantic representation.", "labels": [], "entities": []}, {"text": "These grammars lack robustness in the sense that they do not gracefully handle words missing from their lexicon.", "labels": [], "entities": []}, {"text": "Several approaches have been explored to handle this problem, many of which consist in pre-annotating the input to the grammar with shallow processing machine-learning tools.", "labels": [], "entities": []}, {"text": "Most of these tools, however, use features based on a fixed window of context, such as n-grams.", "labels": [], "entities": []}, {"text": "We investigate whether the use of features that encode discrete structures, namely grammatical dependencies , can improve the performance of a machine learning classifier that assigns deep lexical types.", "labels": [], "entities": []}, {"text": "In this paper we report on the design and evaluation of this classifier.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parsing is one of the fundamental tasks in Natural Language Processing and a critical step in many applications.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9321157336235046}, {"text": "Natural Language Processing", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6519050200780233}]}, {"text": "Many of the most commonly used parsers rely on probabilistic approaches.", "labels": [], "entities": []}, {"text": "These parsers are obtained through data-driven approaches, by inferring a probabilistic language model over a dataset of annotated sentences.", "labels": [], "entities": []}, {"text": "Though these parsers always produce some analysis of their input sentences, they do not go into deep linguistic analysis.", "labels": [], "entities": []}, {"text": "Deep grammars, also referred to as precision grammars, seek to make explicit information about highly detailed linguistic phenomena and produce complex grammatical representations for their input sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9765353798866272}]}, {"text": "For instance, they are able to capture long-distance dependencies and produce the semantic representation of a sentence.", "labels": [], "entities": []}, {"text": "Although there is a great variety of parsing methods (see) for an overview), all CKY-based algorithms require a lexical look-up initialization step that, for each word in the input, returns all its possible categories.", "labels": [], "entities": []}, {"text": "From this it follows that if any of the words in a sentence is not present in the lexicon-an outof-vocabulary (OOV) word-a full parse of that sentence is impossible to obtain.", "labels": [], "entities": []}, {"text": "Given that novelty is one of the defining characteristics of natural languages, unknown words will eventually occur.", "labels": [], "entities": []}, {"text": "Hence, being able to handle OOV words is of paramount importance if one wishes to use a grammar to analyze unrestricted texts.", "labels": [], "entities": []}, {"text": "Another important issue is that of lexical ambiguity.", "labels": [], "entities": []}, {"text": "That is, words that may bear more than one lexical category.", "labels": [], "entities": []}, {"text": "The combinatorial explosion of lexical and syntactic ambiguity may hinder parsing due to increased requirements in terms of parsing time and memory usage.", "labels": [], "entities": [{"text": "parsing", "start_pos": 74, "end_pos": 81, "type": "TASK", "confidence": 0.9781126379966736}]}, {"text": "Thus, even if there were no OOV words in the input, being able to assign syntactic categories to words prior to parsing maybe desirable for efficiency reasons.", "labels": [], "entities": []}, {"text": "For the shallower parsing approaches, such as plain constituency parsing, it suffices to determine the part-of-speech of words, so pre-processing the input with a POS tagger is a common and effective way to tackle either of these problems.", "labels": [], "entities": [{"text": "plain constituency parsing", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6004903415838877}]}, {"text": "However, the linguistic information contained in the lexicon of a deep grammar is much more fine-grained, including, in particular, the subcategorization frame (SCF) of the word, which further constraints what can betaken as a well-formed sentence by imposing several restrictions on co-occurring expressions.", "labels": [], "entities": []}, {"text": "Thus, what fora plain POS tagger corresponds to a single category is often expanded into hundreds of different distinctions, and hence tags, when at the level of detail required by a deep grammar.", "labels": [], "entities": []}, {"text": "For instance, the particular grammar we will be using for the study reported in this paper-a grammar following the HPSG framework-has in its current version a lexicon with roughly 160 types for verbs and nearly 200 types for common nouns.", "labels": [], "entities": [{"text": "HPSG framework-has", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.9488357901573181}]}, {"text": "While the deep grammar may proceed with the analysis knowing only the base POS category of a word, it does so at the cost of vastly increased ambiguity 1 which may even allow the grammar to accept ungrammatical sentences as valid.", "labels": [], "entities": []}, {"text": "This has lead to research that specifically targets annotating words with a tagset suitable for deep grammars.", "labels": [], "entities": []}, {"text": "Current approaches tend to use shallow features with limited context (e.g. n-grams).", "labels": [], "entities": []}, {"text": "However, given that the SCF is one of the most relevant pieces of information that is associated with a word in the lexicon of a deep grammar, one would expect that features describing the inter-word dependencies in a sentence would be highly discriminative and help to accurately assign lexical types.", "labels": [], "entities": []}, {"text": "Accordingly, in this paper we investigate the use of structured features that encode grammatical dependencies in a machine-learning classifier and how it compares with state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "Our study targets Portuguese, a Romance language with a rich morphology, in particular in what concerns verb inflection (see for instance, () fora detailed account of Portuguese grammar and (  for an assessment of the issues raised by verbal ambiguity).", "labels": [], "entities": []}, {"text": "Paper outline: Section 2 provides an overview of related work, with a focus on supertagging, and introduces tree kernels as away of handling structured classifier features.", "labels": [], "entities": []}, {"text": "Section 3 introduces the particular deep grammar that is used in this work and how it supports the creation of the corpus that provides the data for training and evaluation of the classifier.", "labels": [], "entities": []}, {"text": "The classifier itself, and the features it uses, are described in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 covers empirical evaluation and comparison with other approaches.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes with some final remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "The deep linguistic grammar used in this study is LXGram, a hand-built HPSG grammar for Portuguese (.", "labels": [], "entities": []}, {"text": "We used this grammar to support the annotation of a corpus.", "labels": [], "entities": []}, {"text": "That is, the grammar is used to provide the set of possible analyses fora sentence (the parse forest).", "labels": [], "entities": []}, {"text": "Human annotators then perform manual disambiguation by picking the correct analysis from among all those that form the parse forest.", "labels": [], "entities": []}, {"text": "This grammar-supported approach to corpus annotation ensures that the various linguistic annotation layers-morphological, syntactic and semantic-are consistent.", "labels": [], "entities": []}, {"text": "The corpus that was used is composed mostly by a subset of the sentences in CETEMP\u00fablico, a corpus of plain text excerpts from the P\u00fablico newspaper.", "labels": [], "entities": [{"text": "CETEMP\u00fablico, a corpus of plain text excerpts from the P\u00fablico newspaper", "start_pos": 76, "end_pos": 148, "type": "DATASET", "confidence": 0.6904628078142802}]}, {"text": "After running LXGram and manually disambiguating the parse forests, we were left with a dataset consisting of 5,422 sentences annotated with all the linguistic information provided by LXGram.", "labels": [], "entities": [{"text": "LXGram", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.964735746383667}, {"text": "LXGram", "start_pos": 184, "end_pos": 190, "type": "DATASET", "confidence": 0.9651212692260742}]}, {"text": "The following evaluation results were obtained following a standard 10-fold cross-validation approach, where the folds were taken from a random shuffle of the sentences in the corpus.", "labels": [], "entities": []}, {"text": "We compare the performance of our tree kernel (TK) approach with two other automatic annotators, TnT) and SVMTool ().", "labels": [], "entities": []}, {"text": "TnT is a statistical POS tagger, well known for its efficiency-in terms of training and tagging speed-and for achieving state-of-the-art results despite having a quite simple underlying TnT was used as a supertagger in, where it achieved the best results for this task, and is thus a good representative for this approach to supertagging.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.7608720660209656}]}, {"text": "We run it out-of-the-box using the default settings.", "labels": [], "entities": []}, {"text": "SVMTool is another statistical sequential tagger which, as the name indicates, is based on support-vector machines.", "labels": [], "entities": [{"text": "SVMTool", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9468156695365906}, {"text": "statistical sequential tagger", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.6285755137602488}]}, {"text": "It is extremely flexible in allowing to define which features should be used in the model (e.g. size of word window, number of POS bigrams, etc.) and the tagging strategy (left to right, bidirectional, number of passes, etc).", "labels": [], "entities": []}, {"text": "In fact, due to this flexibility, it is described as being a tagger generator.", "labels": [], "entities": []}, {"text": "It beat TnT in a POS tagging task (), so we use it in the current paper to evaluate whether that lead is kept in a supertagging task.", "labels": [], "entities": [{"text": "TnT", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.8882094621658325}, {"text": "POS tagging task", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.8038035233815511}]}, {"text": "We used the simplest settings, \"M0 LR\", which uses Model 0 in a left to right tagging direction.", "labels": [], "entities": []}, {"text": "The type distribution in the dataset is highly skewed.", "labels": [], "entities": []}, {"text": "For instance, from the number of common noun types that occur in this corpus, the two most frequent ones are enough to account for 57% of all the common noun tokens.", "labels": [], "entities": []}, {"text": "Such skewed category distributions are usually a problematic issue for machine-learning approaches since the number of instances of the more rare categories is too small to properly estimate the parameters of the model.", "labels": [], "entities": []}, {"text": "For many types there are not enough instances in the dataset to train a classifier.", "labels": [], "entities": []}, {"text": "Hence, the evaluation that follows is done only for the most frequent types.", "labels": [], "entities": []}, {"text": "For instance, top-10 means picking the 10 most frequent types in the corpus, training one-vsone classifiers for those types, and evaluating only over tokens with one of those types.", "labels": [], "entities": []}, {"text": "In addition, we show only the evaluation results of verb types, for which SCF information is more varied and relevant.", "labels": [], "entities": []}, {"text": "show the accuracy results for each tool over the top-10, top-20 and top-30 most frequent verb types.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9996700286865234}]}, {"text": "Comparing both sequential supertaggers, one finds that SVMTool is consistently better than TnT, which is in accordance with the results for POS tagging reported in ().", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 140, "end_pos": 151, "type": "TASK", "confidence": 0.7856009006500244}]}, {"text": "Our TK approach beats both supertaggers when looking at the top-10 verb types, but falls behind as soon as the number of types under consideration increases.", "labels": [], "entities": []}, {"text": "This seems to point towards data-sparseness issues, an hypothesis we test by automatically extending the dataset, as discussed next.", "labels": [], "entities": []}, {"text": "The extended datasets were created by taking additional sentences from the P\u00fablico newspaper, as well as sentences from the Portuguese Wikipedia and from the Folha de S\u00e3o Paulo newspaper, preprocessing them with a POS tagger, and running them through LXGram.", "labels": [], "entities": [{"text": "P\u00fablico newspaper", "start_pos": 75, "end_pos": 92, "type": "DATASET", "confidence": 0.9619780480861664}, {"text": "Folha de S\u00e3o Paulo newspaper", "start_pos": 158, "end_pos": 186, "type": "DATASET", "confidence": 0.8032829403877259}]}, {"text": "Such an approach is only made possible because LXGram, like many other modern HPSG grammars, includes a stochastic disambiguation module that automatically chooses the most likely analysis among all those returned in the parse forest, instead of requiring a manual choice by a human annotator (.", "labels": [], "entities": []}, {"text": "The authors do not provide a complete evaluation of this disambiguation module.", "labels": [], "entities": []}, {"text": "Instead, they perform a manual evaluation of a sample of 50 sentences that indicates that this module picks the correct reading in 40% of the cases.", "labels": [], "entities": []}, {"text": "If this ratio is kept, 60% of the sentences in the extended datasets will have an analysis that is, in someway, the wrong analysis, though it is not clear how this translates into errors in the lexical types that end up being assigned to the tokens.", "labels": [], "entities": []}, {"text": "For instance, when faced with the rather common case of PP-attachment ambiguity, the disambiguation module may choose the wrong attachment, which will count as being a wrong analysis though most lexical types assigned to the words in the sentence maybe correct.", "labels": [], "entities": []}, {"text": "To evaluate this, we tested the disambiguation module over the base dataset, where we know what the correct parses are, and found that the grammar picks the correct parse in 44% of the cases.", "labels": [], "entities": []}, {"text": "If we just look at whether the lexical types are correct, the dataset sentences tokens unique oov  grammar picks a sentence with fully correct types in 68% of the cases.", "labels": [], "entities": []}, {"text": "LXGram displayed a coverage of roughly 30%, and allowed us to build progressively larger datasets as more data was added.", "labels": [], "entities": [{"text": "LXGram", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9386558532714844}]}, {"text": "The cumulative sizes of the resulting datasets are shown in.", "labels": [], "entities": []}, {"text": "The shows the ratio of OOV words, which was determined by taking the average of the ratio for each of the 10 folds (i.e. words that occur in a fold but not in any of the other 9 folds).", "labels": [], "entities": [{"text": "OOV", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9144283533096313}]}, {"text": "We can now evaluate the tools over the four progressively larger datasets and plot their learning curves.", "labels": [], "entities": []}, {"text": "In the following Figures, the errors bars represent a 95% confidence interval.", "labels": [], "entities": []}, {"text": "All learning curves in the following Figures tell a somewhat similar story.", "labels": [], "entities": []}, {"text": "The lead that SVMTool has over TnT when looking only at the base corpus is kept in the extended corpora.", "labels": [], "entities": [{"text": "SVMTool", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.7895515561103821}]}, {"text": "Both sequential supertaggers only start to benefit from the increased dataset at the final stage, when sentences from Folha de S\u00e3o Paulo are added.", "labels": [], "entities": [{"text": "Folha de S\u00e3o Paulo", "start_pos": 118, "end_pos": 136, "type": "DATASET", "confidence": 0.9386021345853806}]}, {"text": "Before that stage the added data seems to be slightly detrimental to them, possibly due to them being sensitive to noise in the automatically generated data.", "labels": [], "entities": []}, {"text": "The learning curves give credence to the hypothesis put forward earlier that our TK approach was being adversely affected by data-sparseness issues when classifying a greater number of verb types, and that it has much to gain by an increase in the amount of training data.", "labels": [], "entities": []}, {"text": "For the top-10 verb types, for which there is enough data in the base dataset, TK starts ahead from the outset and significantly increases its margin over the two supertaggers.", "labels": [], "entities": [{"text": "TK", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.8881914615631104}, {"text": "margin", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9399368166923523}]}, {"text": "For the top-20 and top-30 verb types, TK starts behind but its accuracy raises quickly as more data are added, ending slightly ahead of SVMTool when running over the largest dataset.", "labels": [], "entities": [{"text": "TK", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9084409475326538}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.999448835849762}]}], "tableCaptions": [{"text": " Table 1: Accuracy over frequent verb types", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9978183507919312}]}, {"text": " Table 2: Cumulative size of datasets", "labels": [], "entities": []}, {"text": " Table 3: MaltParser labeled accuracy", "labels": [], "entities": []}, {"text": " Table 4: SVM-TK classifier accuracy over gold and predicted features", "labels": [], "entities": [{"text": "SVM-TK classifier", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7918925881385803}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9549524784088135}]}]}