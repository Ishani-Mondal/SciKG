{"title": [{"text": "BioPOS: Biologically Inspired Algorithms for POS Tagging Ana Paula Silva 1 Ar l ind o Silva 1 Ir eneRod r i gues", "labels": [], "entities": [{"text": "POS Tagging Ana Paula", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.8952177166938782}, {"text": "Ar l ind o Silva 1 Ir eneRod r i gues", "start_pos": 75, "end_pos": 112, "type": "METRIC", "confidence": 0.8380853913047097}]}], "abstractContent": [{"text": "In this paper we present anew biologically inspired approach to the part-of-speech tagging problem, based on particle swarm optimization.", "labels": [], "entities": [{"text": "part-of-speech tagging problem", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.8039105335871378}, {"text": "particle swarm optimization", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.6697892745335897}]}, {"text": "As far as we know this is the first attempt of solving this problem using swarm intelligence.", "labels": [], "entities": []}, {"text": "We divided the part-of-speech problem into two subproblems.", "labels": [], "entities": []}, {"text": "The first concerns the way of automatically extracting disambiguation rules from an annotated corpus.", "labels": [], "entities": [{"text": "automatically extracting disambiguation rules from an annotated corpus", "start_pos": 30, "end_pos": 100, "type": "TASK", "confidence": 0.7865479215979576}]}, {"text": "The second is related with how to apply these rules to perform the automatic tagging.", "labels": [], "entities": []}, {"text": "We tackled both problems with particle swarm optimization.", "labels": [], "entities": [{"text": "particle swarm optimization", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.7463733355204264}]}, {"text": "We tested our approach using two different corpora of English language and also a Portuguese corpus.", "labels": [], "entities": []}, {"text": "The accuracy obtained on both languages is comparable to the best results previously published, including other evolutionary approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999502420425415}]}], "introductionContent": [{"text": "The part-of-speech (POS) tagging is a fundamental step for the execution of other natural language processing (NLP) tasks, like phrase chunking, parsing, named entity recognition, machine translation, information retrieval, speech recognition, etc.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6438860833644867}, {"text": "phrase chunking", "start_pos": 128, "end_pos": 143, "type": "TASK", "confidence": 0.7545318603515625}, {"text": "parsing", "start_pos": 145, "end_pos": 152, "type": "TASK", "confidence": 0.8390328288078308}, {"text": "named entity recognition", "start_pos": 154, "end_pos": 178, "type": "TASK", "confidence": 0.5580199062824249}, {"text": "machine translation", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.7793619632720947}, {"text": "information retrieval", "start_pos": 201, "end_pos": 222, "type": "TASK", "confidence": 0.7942298650741577}, {"text": "speech recognition", "start_pos": 224, "end_pos": 242, "type": "TASK", "confidence": 0.7880429625511169}]}, {"text": "It is the process of classifying words according to the roles they assume in a sentence.", "labels": [], "entities": []}, {"text": "The task is not straightforward because words inmost languages can assume different roles in a sentence, depending on how they are used.", "labels": [], "entities": []}, {"text": "Those roles are normally designated by part-of-speech tags or word classes, such as nouns, verbs, adjectives and adverbs.", "labels": [], "entities": []}, {"text": "The role of a word in a sentence is determined by it's surrounding words (context).", "labels": [], "entities": []}, {"text": "For instance, the word fish can assume the function of a verb, \"Men like to fish.\", or a noun, \"I like smoked fish\", depending on how we choose to use it on a sentence.", "labels": [], "entities": []}, {"text": "This means that in order to assign to each word of a sentence its correct tag, we have to consider the context in which each word appears.", "labels": [], "entities": []}, {"text": "However, each of the words belonging to a word's context can also be used in different ways, and that means that in order to solve the problem we need some type of disambiguation mechanism.", "labels": [], "entities": []}, {"text": "Traditionally, there are two groups of methods used to tackle this task, with respect to the information model used.", "labels": [], "entities": []}, {"text": "The first group is based on statistical data concerning the different context possibilities fora word (stochastic taggers)), while the second group is based on rules that capture the language properties and are used to improve tagging accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 235, "end_pos": 243, "type": "METRIC", "confidence": 0.8872132897377014}]}, {"text": "The simplest stochastic tagger, called unigram tagger, takes only into account the word itself.", "labels": [], "entities": [{"text": "unigram tagger", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.6824997663497925}]}, {"text": "It assigns the tag that is most likely for one particular token.", "labels": [], "entities": []}, {"text": "The tagger works like a simple lookup tagger, assigning to each word the most common tag for that word in the training corpus.", "labels": [], "entities": []}, {"text": "To do that, the learning process just counts, for each word, the number of times it appears with each of the possible tags.", "labels": [], "entities": []}, {"text": "An n-gram tagger is a generalization of a unigram tagger, whose context is the current word together with the part-of-speech tags of the n-1 preceding tokens.", "labels": [], "entities": []}, {"text": "In this case, the training step saves, for each possible tag, the number of times it appears in every different context presented on the training corpus.", "labels": [], "entities": []}, {"text": "Since the surrounding words can also have various possibilities of classification, it is necessary to use a statistical model that allows the selection of the best choices for marking the entire sequence, according to the model.", "labels": [], "entities": []}, {"text": "Most of the stochastic taggers are based on hidden Markov models, and, because of that, a word's context consists only in the tags of the words that precede it.", "labels": [], "entities": []}, {"text": "However, more recently, other taggers have emerged that, although based on stochastic information, have used different models that make possible to consider different context shapes.", "labels": [], "entities": []}, {"text": "One of the most popular taggers based on rules is the one proposed by.", "labels": [], "entities": []}, {"text": "Brill's rules are usually called transformation rules.", "labels": [], "entities": []}, {"text": "The system can be divided into two main components: a list of transformation rules patterns for error correction, and a learning system.", "labels": [], "entities": [{"text": "error correction", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.6670227646827698}]}, {"text": "The transformation patterns are handmade and provided to the learning algorithm, which will instantiate and order them.", "labels": [], "entities": []}, {"text": "The search is made in a greedy fashion.", "labels": [], "entities": []}, {"text": "The result is an ordered set of transformation rules, which is then used to perform the tagging.", "labels": [], "entities": []}, {"text": "These rules are meant to correct mistakes in a pre-tagged text, usually achieved by a baseline system that marks each word with its most common tag.", "labels": [], "entities": []}, {"text": "They are applied in a iterative way until no rule can be fired.", "labels": [], "entities": []}, {"text": "As already observed, the only information an n-gram tagger considers from prior context is the tags, even though words themselves might be a useful source of information.", "labels": [], "entities": []}, {"text": "It is simply impractical for n-gram models to be conditioned by the context words themselves (and not only their tags).", "labels": [], "entities": []}, {"text": "On the other hand, Brill's approach allows the inclusion of other type of information besides the context.", "labels": [], "entities": []}, {"text": "In fact, the author also used the learning algorithm to achieve a set of lexicalized transformation rules, that includes not only the tags but the words themselves.", "labels": [], "entities": []}, {"text": "There are also some other aspects that can be used to determine a word's category beside it's context in a sentence).", "labels": [], "entities": []}, {"text": "The internal structure of a word may give useful clues as to the word's class.", "labels": [], "entities": []}, {"text": "For example, in the English language, -ness is a suffix that combines with an adjective to produce a noun, e.g., happy \u2192 happiness, ill \u2192 illness.", "labels": [], "entities": []}, {"text": "Therefore, if we encounter a word that ends in -ness, it is very likely to be a noun.", "labels": [], "entities": []}, {"text": "Similarly, -ing is a suffix that is most commonly associated with gerunds, like walking, talking, thinking, listening.", "labels": [], "entities": []}, {"text": "We also might guess that any word ending in -ed is the past participle of a verb, and any word ending with 's is a possessive noun.", "labels": [], "entities": []}, {"text": "More recently, several evolutionary approaches have been proposed to solve the tagging problem.", "labels": [], "entities": [{"text": "tagging problem", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.950844019651413}]}, {"text": "These approaches can also be divided by the type of information used to solve the problem, statistical information), and rule-based information).", "labels": [], "entities": []}, {"text": "Although there is a substantial amount of work about POS tagging applied to the English language, there are, comparatively, a small number of attempts to approach the problem using the Portuguese language.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.913313627243042}]}, {"text": "One of the main reasons is the limited number of resources that exist for it.", "labels": [], "entities": []}, {"text": "Nevertheless, we can find some work where several of the existing techniques used to solve the POS tagging are tested on the Portuguese language: transformation based learning (), Markov models (), entropy guided transformation learning (Nogueira Dos).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.6864338219165802}]}, {"text": "One of the problems of the stochastic taggers, is that they tend to be dependent of the domain in which they are trained.", "labels": [], "entities": [{"text": "stochastic taggers", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.6314720213413239}]}, {"text": "Also, the information used is in the form of probabilistic values, which are less comprehensible than data presented in the form of rules, and only contemplates context information.", "labels": [], "entities": []}, {"text": "In this work, we investigate the possibility of extracting, from an annotated corpus, a set of rules similar to classification rules, that could be used to help the decision process needed to perform the tagging.", "labels": [], "entities": []}, {"text": "With our approach, we hope to be able to learn rules that prove to be more generic than the information used by the stochastic taggers, so that the tagger performs well in different domains.", "labels": [], "entities": []}, {"text": "We also intend to include in these rules, together with the context information, other type of data, namely some aspects related with the word morphology.", "labels": [], "entities": []}, {"text": "To accomplish our goals, we chose one technology that has already proven to be successful in data mining tasks, in particular in the discovery of classification rules () -the particle swarm optimization (PSO) algorithm.", "labels": [], "entities": [{"text": "data mining", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.7457680106163025}, {"text": "particle swarm optimization (PSO)", "start_pos": 175, "end_pos": 208, "type": "TASK", "confidence": 0.7907241135835648}]}, {"text": "We also investigate the possibility of using a discrete PSO algorithm to perform the tagging, using the disambiguation rules to guide the evolution of the swarm.", "labels": [], "entities": [{"text": "tagging", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.9656733274459839}]}, {"text": "The problem of searching for the best tag assignments can be seen as a combinatorial optimization problem, where a solution is evaluated with the help of the disambiguation rules previously learned.", "labels": [], "entities": []}, {"text": "We decided to test the application of swarm intelligence to this problem, since other population based algorithms, in particular genetic algorithms, have been successfully applied to many combinatorial optimization tasks.", "labels": [], "entities": []}, {"text": "In order to evaluate the success of our approach we tested it on two different languages: English and Portuguese.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe the two evolutionary approaches to the POS tagging problem that we think are the most closely related to the work reported in this paper.", "labels": [], "entities": [{"text": "POS tagging problem", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.9275856614112854}]}, {"text": "In section 3, we present the general idea behind particle swarm optimization.", "labels": [], "entities": [{"text": "particle swarm optimization", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.7746357123057047}]}, {"text": "The particle swarm optimization algorithm for disambiguation rules discovery is outlined in section 4 and the POS-Tagger in section 5.", "labels": [], "entities": [{"text": "particle swarm optimization", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.6962048014005026}, {"text": "disambiguation rules discovery", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.8381765882174174}, {"text": "POS-Tagger", "start_pos": 110, "end_pos": 120, "type": "DATASET", "confidence": 0.8272095322608948}]}, {"text": "The experimental results are presented in section 6, and finally, in section 7, we make some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "Rules must be evaluated during the training process in order to establish points of reference for the training algorithm.", "labels": [], "entities": []}, {"text": "The rule evaluation function must not only consider instances correctly classified, but also the ones left to classify and the wrongly classified ones.", "labels": [], "entities": []}, {"text": "The formula used to evaluate a rule, and therefore to set its quality, is expressed in equation 4.", "labels": [], "entities": [{"text": "quality", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9606224894523621}]}, {"text": "This formula penalizes a particle that represents a rule that ignores the first six attributes, which are related with the word's context, forcing it to assume a more desirable form.", "labels": [], "entities": []}, {"text": "The others are evaluated by the well known F \u03b2 -measure.", "labels": [], "entities": [{"text": "F \u03b2 -measure", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.9653062224388123}]}, {"text": "The F \u03b2 -measure can be interpreted as a weighted average of precision and recall.", "labels": [], "entities": [{"text": "F \u03b2 -measure", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.976838618516922}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9993065595626831}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9973194003105164}]}, {"text": "We used \u03b2 = 0.09, which means we put more emphasis on precision than recall.", "labels": [], "entities": [{"text": "\u03b2", "start_pos": 8, "end_pos": 9, "type": "METRIC", "confidence": 0.9646753072738647}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9993439316749573}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9978234767913818}]}, {"text": "We developed our system in Python and used the resources available on the NLTK (Natural Language Toolkit ) package in our experiences.", "labels": [], "entities": []}, {"text": "The NLTK package provides, for the English language, among others, the Brown corpus and a sample of 10% of the Wall Street Journal (WSJ) corpus of the Penn Treebank.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.9844953119754791}, {"text": "Wall Street Journal (WSJ) corpus of the Penn Treebank", "start_pos": 111, "end_pos": 164, "type": "DATASET", "confidence": 0.9282840707085349}]}, {"text": "These corpora are the most frequently used to test taggers' performances on the English language and the ones used in the approaches we mentioned earlier.", "labels": [], "entities": []}, {"text": "The Mac-Morpho corpus is also available for the Portuguese language.", "labels": [], "entities": [{"text": "Mac-Morpho corpus", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.927129328250885}]}, {"text": "The quality of a particle is given by the tagging quality of the full input sentence.", "labels": [], "entities": []}, {"text": "To evaluate the tagging of the sentence, we use the disambiguated rules to measure the quality of each instance extracted from the sentence.", "labels": [], "entities": []}, {"text": "The quality of the overall tagging is given by the sum of the evaluation results for each instance.", "labels": [], "entities": []}, {"text": "Therefore, the overall tagging evaluation should also consider these instances.", "labels": [], "entities": [{"text": "tagging", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9744570851325989}]}, {"text": "We tested our PSO-Tagger on two different languages: English and Portuguese.", "labels": [], "entities": []}, {"text": "For the English language we carryout experiments on two different corpora: in a test set of the WSJ corpus of the Penn Treebank made of 8300 tokens, and on a test set of 22562 tokens of the Brown corpus.", "labels": [], "entities": [{"text": "WSJ corpus of the Penn Treebank", "start_pos": 96, "end_pos": 127, "type": "DATASET", "confidence": 0.9304944376150767}, {"text": "Brown corpus", "start_pos": 190, "end_pos": 202, "type": "DATASET", "confidence": 0.9604602456092834}]}, {"text": "In both cases the POS-Tagger received as input the set of disambiguation rules learned from the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 96, "end_pos": 108, "type": "DATASET", "confidence": 0.8693747222423553}]}, {"text": "For the Portuguese language, we tested the tagger on a test set of the Mac-Morpho corpus with 21466 tokens.", "labels": [], "entities": [{"text": "Mac-Morpho corpus", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.9158485233783722}]}, {"text": "In this case the algorithm received as input the set of disambiguation rules learned from the Mac-Morpho training set.", "labels": [], "entities": []}, {"text": "We ran the algorithm 20 times with a swarm of 10 and 20 particles during 50 and 100 generations for the three corpora.", "labels": [], "entities": []}, {"text": "The results achieved are shown in table 1.", "labels": [], "entities": []}, {"text": "As we can see, the best average accuracy on the WSJ corpus was 96.91% and the best average accuracy on the Brown corpus was 96.72%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9947351217269897}, {"text": "WSJ corpus", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.9731893539428711}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9845372438430786}, {"text": "Brown corpus", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.9843731820583344}]}, {"text": "The best results on the Brown corpus were achieved with a swarm of 20 particles over 50 generations.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.961465984582901}]}, {"text": "A maximum accuracy of 96.75% was found.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997246861457825}]}, {"text": "A swarm of 20 particles over 100 generations gave the best results for the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.923914909362793}]}, {"text": "In this case, the best tagging found had an accuracy of 97.04%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9996993541717529}]}, {"text": "For the Portuguese corpus, the best average accuracy obtained was 96.83%, when the POStagger was executed with 10 particles over 100 generations.", "labels": [], "entities": [{"text": "Portuguese corpus", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.8983825743198395}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.997753918170929}]}, {"text": "The best accuracy, 96.89%, was found during a run of the algorithm with 20 particles over 50 generations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9996002316474915}]}, {"text": "Both results allow us to conclude that the PSO-Tagger usually finds a solution very quickly.", "labels": [], "entities": []}, {"text": "Like we said before, this algorithm works well with small swarms and tends to converge fast to a solution.", "labels": [], "entities": []}, {"text": "presents the best results achieved by the approaches we mentioned earlier, along with the best ones achieved by the PSO-Tagger.", "labels": [], "entities": [{"text": "PSO-Tagger", "start_pos": 116, "end_pos": 126, "type": "DATASET", "confidence": 0.8617583513259888}]}, {"text": "Observing table 2, we can see that the PSO-Tagger accuracy is very promising, since it is among the best values presented.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.8921300768852234}]}, {"text": "Naturally, the difficulty level of the tagging task depends on the number of ambiguous words of the sentence we want to tag.", "labels": [], "entities": [{"text": "tagging task", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9182011485099792}]}, {"text": "Although it is possible to construct sentences in which every word is ambiguous: Results achieved on the English and Portuguese corpora by the PSO-Tagger after 20 runs with a swarm of size 10 and 20, during 50 and 100 generations.", "labels": [], "entities": [{"text": "PSO-Tagger", "start_pos": 143, "end_pos": 153, "type": "DATASET", "confidence": 0.8878982663154602}]}, {"text": "explain the considerable low number of particles and generations needed to achieve a solution.", "labels": [], "entities": []}, {"text": "We could argue that in those conditions the use of a PSO algorithm is unnecessary, and that a exhaustive search could be applied to solve the problem.", "labels": [], "entities": []}, {"text": "However, we cannot ignore the worst case scenario, where, like we see above, all the words, or a large majority of the words, on a very long sentence maybe ambiguous.", "labels": [], "entities": []}, {"text": "Furthermore, we observed that the sentence average size of the Brown corpus is of 20.25 tokens, with a maximum of 180.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.9734170734882355}]}, {"text": "The largest number of ambiguous words on a sentence belonging to this corpus is 68.", "labels": [], "entities": []}, {"text": "Even for the smallest degree of ambiguity, with only two possible tags for each word, we have a search space of 2 68 , which fully justifies the use of a global search algorithm such as a PSO.", "labels": [], "entities": []}, {"text": "The results achieved show that there are no significant differences on the accuracy obtained by the tagger on the two test sets of the English language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9992702603340149}]}, {"text": "At this point, it is important to emphasize that the disambiguation rules used on the tagger were extracted from a subset (different from the test set used in this experiments) of the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 184, "end_pos": 196, "type": "DATASET", "confidence": 0.8602008819580078}]}, {"text": "Which bring us to the conclusion that the learned rules are generic enough to be used on different corpora, and are not domain dependent.", "labels": [], "entities": []}, {"text": "The results achieved for the Portuguese language showed that our strategy also performed well for other languages different from English.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results achieved on the English and Portuguese corpora by the PSO-Tagger after 20  runs with a swarm of size 10 and 20, during 50 and 100 generations.", "labels": [], "entities": [{"text": "English and Portuguese corpora", "start_pos": 34, "end_pos": 64, "type": "DATASET", "confidence": 0.7091173678636551}]}, {"text": " Table 2: Results achieved by the PSO-Tagger on corpora of English and Portuguese language,  along with the results achieved by the approaches more similar to the one presented here. The  ETL-Tagger are the one presented in (Nogueira Dos Santos et al., 2008)", "labels": [], "entities": []}]}