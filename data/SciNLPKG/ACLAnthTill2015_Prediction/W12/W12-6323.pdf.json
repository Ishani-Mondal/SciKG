{"title": [{"text": "A Template Based Hybrid Model for Chinese Personal Name Disam- biguation", "labels": [], "entities": [{"text": "Chinese Personal Name Disam- biguation", "start_pos": 34, "end_pos": 72, "type": "TASK", "confidence": 0.7031994462013245}]}], "abstractContent": [{"text": "This paper proposes a template based hybrid model for Chinese Personal Name Disambiguation (CPND).", "labels": [], "entities": [{"text": "Chinese Personal Name Disambiguation (CPND)", "start_pos": 54, "end_pos": 97, "type": "TASK", "confidence": 0.7312318980693817}]}, {"text": "The template makes use of the features of personal role such as discriminating personal name (nickname, stage name), together with the specific context of most frequent words, personal name nearest words named entities, date and time that are effective for this disambiguation task, as well as surrounding context of nominal, verbal and adjectival constituents.", "labels": [], "entities": []}, {"text": "The construction of the templates is automatically derived from the articles that maximizes the deviation of different categories of personal names.", "labels": [], "entities": []}, {"text": "The extraction algorithm of keyword features based on the distribution of unlabeled data is also proposed in this paper for this challenging task.", "labels": [], "entities": []}, {"text": "In addition, an augmented similarity measure for the CPND model has been designed to calculate the similarity between a standard template and an unlabeled text.", "labels": [], "entities": []}, {"text": "The final evaluation reveals that the proposed model can achieve the F-measure of 75.75% on the test data.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9997040629386902}]}], "introductionContent": [{"text": "The We participated in the CIPS-SIGHAN Joint Conference on Chinese Language Processing and focus on task 2: Chinese Personal Name Disambiguation.", "labels": [], "entities": [{"text": "Chinese Language Processing", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6727637648582458}, {"text": "Chinese Personal Name Disambiguation", "start_pos": 108, "end_pos": 144, "type": "TASK", "confidence": 0.5578552857041359}]}, {"text": "This task is a little different from 2010 SIGHAN task 3 . It has given a short description of a certain personal name (here we call this standard classes), and each unlabeled text may belong to three main categories which respectively area standard class, OUT class and OTHER class.", "labels": [], "entities": [{"text": "OUT", "start_pos": 256, "end_pos": 259, "type": "METRIC", "confidence": 0.9714148044586182}, {"text": "OTHER", "start_pos": 270, "end_pos": 275, "type": "METRIC", "confidence": 0.8767382502555847}]}, {"text": "This task is a little more challenging than 2010 SIGHAN Bake-off task 3, because this task has given us a standard class which usually has less information than an unlabeled text.", "labels": [], "entities": [{"text": "SIGHAN Bake-off task 3", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.759926438331604}]}, {"text": "This task is very similar to a text clustering problem.", "labels": [], "entities": [{"text": "text clustering", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7992365658283234}]}, {"text": "Usually most people will use some clustering algorithm, like Xiamen University and Dalian University () in 2010 SIGHAN Bake-off task3, both of them used Hierarchical Agglomerative Clustering (HAC) algorithm) to do the clustering.", "labels": [], "entities": [{"text": "SIGHAN Bake-off task3", "start_pos": 112, "end_pos": 133, "type": "DATASET", "confidence": 0.6147419611612955}]}, {"text": "As a conclusion, the most dissimilar in SIGHAN 2010 task3 is that they used different feature set.", "labels": [], "entities": [{"text": "SIGHAN 2010 task3", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.766381561756134}]}, {"text": "For this task, we have a referenced standard class; the clustering for this standard class may not have a good effect.", "labels": [], "entities": []}, {"text": "The shortage for this clustering algorithm is that the text must be large enough for this algorithm to extract useful feature, and more importantly the clustering algorithm is very time consuming and highly rely on the feature set.", "labels": [], "entities": []}, {"text": "This feature set will add much human effort inside, such as the university name selection, gender selection, job title selection, work experience selection.", "labels": [], "entities": [{"text": "university name selection", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.5397050678730011}, {"text": "gender selection", "start_pos": 91, "end_pos": 107, "type": "TASK", "confidence": 0.6809145510196686}, {"text": "job title selection", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.6482394436995188}, {"text": "work experience selection", "start_pos": 130, "end_pos": 155, "type": "TASK", "confidence": 0.6192823648452759}]}, {"text": "For this specific task these information may not be enough to distinguish standard classes.", "labels": [], "entities": []}, {"text": "Because two standard classes many have some common features.", "labels": [], "entities": []}, {"text": "That is the last we want to see.", "labels": [], "entities": []}, {"text": "Therefore we design a similarity formula to handle the clustering time consuming problem.", "labels": [], "entities": []}, {"text": "We pruned most unnecessary calculation.", "labels": [], "entities": []}, {"text": "For example, we first calculate the unlabeled text's keyword similarity to each standard class; then further calculate good feature similarity if there is more than one standard has the same keyword similarity.", "labels": [], "entities": []}, {"text": "For feature selection, we also design an algorithm to extract the most discriminating words.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7730742394924164}]}, {"text": "This original idea of this algorithm is to extract the primitive name or used name.", "labels": [], "entities": []}, {"text": "However this personal name information is limited, so we try to use other information as our text feature.", "labels": [], "entities": []}, {"text": "Here we proposed a word distribution concept.", "labels": [], "entities": [{"text": "word distribution", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.8164418041706085}]}, {"text": "This word distribution concept refers to the distribution in the whole unlabeled texts.", "labels": [], "entities": []}, {"text": "We suppose there is a group of existing words in the standard classes that their sum of distribution is close to 1.", "labels": [], "entities": []}, {"text": "Since the classification has OTHER and OUT class, we set the expected sum of all distribution is 0.75.", "labels": [], "entities": [{"text": "OTHER", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9932957291603088}, {"text": "OUT", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9957115650177002}]}, {"text": "So we suppose the total OTHER and OUT unlabeled texts are less than 25% of entire texts.", "labels": [], "entities": [{"text": "OTHER", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.8348824977874756}, {"text": "OUT", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.760971188545227}]}, {"text": "The following sections include Keyword extraction, named entity recognition, model construction, similarity calculation, OUT class solution and other issues.", "labels": [], "entities": [{"text": "Keyword extraction", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.8788745701313019}, {"text": "named entity recognition", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.6763669848442078}, {"text": "model construction", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7919431626796722}, {"text": "similarity calculation", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.6622933894395828}, {"text": "OUT class solution", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.756815493106842}]}, {"text": "Then we will show the evaluation and conclusion.", "labels": [], "entities": [{"text": "conclusion", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9518826007843018}]}], "datasetContent": [{"text": "We followed the formula given by the organizers to calculate the precision rate, recall rate and FB1 . We directly list the best test result based on the given so called train set  We only get overall score, not in detail.", "labels": [], "entities": [{"text": "precision rate", "start_pos": 65, "end_pos": 79, "type": "METRIC", "confidence": 0.9832359254360199}, {"text": "recall rate", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.9912131428718567}, {"text": "FB1", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9980751276016235}]}, {"text": "All these data show that our recall rate is obviously larger than the precision rate.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.9893493056297302}, {"text": "precision rate", "start_pos": 70, "end_pos": 84, "type": "METRIC", "confidence": 0.98750901222229}]}, {"text": "Which means our system is better at detecting the OUT and the OTHER class.", "labels": [], "entities": [{"text": "OUT", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9034911394119263}, {"text": "OTHER", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.8299404382705688}]}], "tableCaptions": [{"text": " Table 3: The evaluation of the training data", "labels": [], "entities": []}]}