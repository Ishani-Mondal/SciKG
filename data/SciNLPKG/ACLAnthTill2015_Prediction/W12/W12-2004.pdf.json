{"title": [{"text": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 33-43, Modeling coherence in ESOL learner texts", "labels": [], "entities": []}], "abstractContent": [{"text": "To date, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence in the noisy domain of learner texts.", "labels": [], "entities": []}, {"text": "We present the first systematic analysis of several methods for assessing coherence under the framework of automated assessment (AA) of learner free-text responses.", "labels": [], "entities": []}, {"text": "We examine the predictive power of different coherence models by measuring the effect on performance when combined with an AA system that achieves competitive results, but does not use discourse coherence features, which are also strong indicators of a learner's level of attainment.", "labels": [], "entities": []}, {"text": "Additionally, we identify new techniques that outperform previously developed ones and improve on the best published result for AA on a publically-available dataset of En-glish learner free-text examination scripts.", "labels": [], "entities": [{"text": "AA", "start_pos": 128, "end_pos": 130, "type": "TASK", "confidence": 0.9532502293586731}]}], "introductionContent": [{"text": "Automated assessment (hereafter AA) systems of English learner text assign grades based on textual features which attempt to balance evidence of writing competence against evidence of performance errors.", "labels": [], "entities": []}, {"text": "Previous work has mostly treated AA as a supervised text classification or regression task.", "labels": [], "entities": [{"text": "AA", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9777405261993408}, {"text": "text classification", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.6888085454702377}]}, {"text": "A number of techniques have been investigated, including cosine similarity of feature vectors), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (, and generative machine learning models) as well as discriminative ones).", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA)", "start_pos": 160, "end_pos": 190, "type": "TASK", "confidence": 0.7495327840248743}, {"text": "generative machine learning", "start_pos": 198, "end_pos": 225, "type": "TASK", "confidence": 0.8478177587191263}]}, {"text": "As multiple factors influence the linguistic quality of texts, such systems exploit features that correspond to different properties of texts, such as grammar, style, vocabulary usage, topic similarity, and discourse coherence and cohesion.", "labels": [], "entities": []}, {"text": "Cohesion refers to the use of explicit linguistic cohesive devices (e.g., anaphora, lexical semantic relatedness, discourse markers, etc.) within a text that can signal primarily suprasentential discourse relations between textual units.", "labels": [], "entities": []}, {"text": "Cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit linguistic cues.", "labels": [], "entities": []}, {"text": "Coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages.", "labels": [], "entities": []}, {"text": "There is a large body of work that has investigated a number of different coherence models on news texts (e.g.,,, and).", "labels": [], "entities": []}, {"text": "Recently, presented a detailed survey of current techniques in coherence analysis of extractive summaries.", "labels": [], "entities": [{"text": "coherence analysis of extractive summaries", "start_pos": 63, "end_pos": 105, "type": "TASK", "confidence": 0.7960098147392273}]}, {"text": "To date, however, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence and cohesion in the noisy domain of learner texts, where spelling and grammatical errors are common.", "labels": [], "entities": []}, {"text": "Coherence quality is typically present in marking criteria for evaluating learner texts, and it is iden-tified by examiners as a determinant of the overall score.", "labels": [], "entities": []}, {"text": "Thus we expect that adding a coherence metric to the feature set of an AA system would better reflect the evaluation performed by examiners and improve performance.", "labels": [], "entities": []}, {"text": "The goal of the experiments presented in this paper is to measure the effect a number of (previously-developed and new) coherence models have on performance when combined with an AA system that achieves competitive results, but does not use discourse coherence features.", "labels": [], "entities": []}, {"text": "Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in.", "labels": [], "entities": []}], "datasetContent": [{"text": "We examine the predictive power of a number of different coherence models by measuring the effect on performance when combined with an AA system that achieves state-of-the-art results, but does not use discourse coherence features.", "labels": [], "entities": []}, {"text": "Specifically, we describe a number of different experiments improving on the AA system presented in; AA is treated as a rank preference supervised learning problem and ranking Support Vector Machines (SVMs) are used to explicitly model the grade relationships between scripts.", "labels": [], "entities": [{"text": "AA", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.978325366973877}]}, {"text": "This system uses a number of different linguistic features that achieve good performance on the AA task.", "labels": [], "entities": [{"text": "AA task", "start_pos": 96, "end_pos": 103, "type": "TASK", "confidence": 0.8567281663417816}]}, {"text": "However, these features only focus on lexical and grammatical properties, as well as errors within individual sentences, ignoring discourse coherence, which is also present in marking criteria for evaluating learner texts, as well as a strong indicator of a writer's understanding of a language.", "labels": [], "entities": []}, {"text": "Also, in, experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts.", "labels": [], "entities": []}, {"text": "The results showed that the model is vulnerable to input where individually high-scoring sentences are randomly ordered within a text.", "labels": [], "entities": []}, {"text": "Failing to identify such pathological cases makes AA systems vulnerable to subversion by writers who understand something of its workings, thus posing a threat to their validity.", "labels": [], "entities": []}, {"text": "For example, an examinee might learn by rote a set of well-formed sentences and reproduce these in an exam in the knowledge that an AA system is not checking for prompt relevance or coherence 1 .  We use the First Certificate in English (FCE) ESOL examination scripts 2 (upper-intermediate level assessment) described in detail in, extracted from the Cambridge Learner Corpus 3 (CLC).", "labels": [], "entities": [{"text": "First Certificate in English (FCE) ESOL examination scripts", "start_pos": 208, "end_pos": 267, "type": "DATASET", "confidence": 0.5215295553207397}, {"text": "Cambridge Learner Corpus 3 (CLC)", "start_pos": 351, "end_pos": 383, "type": "DATASET", "confidence": 0.9570234077317374}]}, {"text": "The dataset consists of 1,238 texts between 200 and 400 words produced by 1,238 distinct learners in response to two different prompts.", "labels": [], "entities": []}, {"text": "An overall mark has been assigned in the range 1-40.", "labels": [], "entities": [{"text": "overall mark", "start_pos": 3, "end_pos": 15, "type": "METRIC", "confidence": 0.7707142531871796}]}, {"text": "For all experiments, we use a series of 5-fold cross-validation runs on 1,141 texts from the examination year 2000 to evaluate performance as well as generalization of numerous models.", "labels": [], "entities": []}, {"text": "Moreover, we identify the best model on year 2000 and we also test it on 97 texts from the examination year 2001, previously used in to report the best published results.", "labels": [], "entities": []}, {"text": "Validating the results on a different examination year tests generalization to some prompts not used in 2000, and also allows us to test correlation between examiners and the AA system.", "labels": [], "entities": [{"text": "AA", "start_pos": 175, "end_pos": 177, "type": "METRIC", "confidence": 0.923576831817627}]}, {"text": "Again, we treat AA as a rank preference learning problem and use SVMs, utilizing the SVM light package, to facilitate comparison with Yannakoudakis et al.", "labels": [], "entities": [{"text": "AA", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.8436605334281921}]}, {"text": "We examine the predictive power of each of the coherence models/features described in Section 4 by measuring the effect on performance when combined with an AA system that achieves state-of-theart results on the FCE dataset, but does not use discourse coherence features.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 212, "end_pos": 223, "type": "DATASET", "confidence": 0.9761657118797302}]}, {"text": "In particular, we use the system described in as our baseline AA system.", "labels": [], "entities": [{"text": "AA", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.9622875452041626}]}, {"text": "Discourse coherence is a strong indicator of thorough knowledge of a second language and thus we expect coherence features to further improve performance of AA systems.", "labels": [], "entities": []}, {"text": "We evaluate the grade predictions of our models against the gold standard grades in the dataset using Pearson's product-moment correlation coeffi-cient (r) and Spearman's rank correlation coefficient (\u03c1) as is standard in AA research (.", "labels": [], "entities": [{"text": "Pearson's product-moment correlation coeffi-cient (r)", "start_pos": 102, "end_pos": 155, "type": "METRIC", "confidence": 0.7665813937783241}, {"text": "rank correlation coefficient (\u03c1)", "start_pos": 171, "end_pos": 203, "type": "METRIC", "confidence": 0.8893041908740997}, {"text": "AA", "start_pos": 222, "end_pos": 224, "type": "TASK", "confidence": 0.9145541191101074}]}, {"text": "gives results obtained by augmenting the baseline model with each of the coherence features described above.", "labels": [], "entities": []}, {"text": "In each of these experiments, we perform 5-fold cross-validation 17 using all 1,141 texts from the exam year 2000 (see Section 3).", "labels": [], "entities": []}, {"text": "Most of the resulting models have minimal effect on performance . However, word length, ISA, LOWBOW lex , and the IBM model POS f derived models all improve performance, while larger differences are observed in r.", "labels": [], "entities": [{"text": "LOWBOW", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9592457413673401}]}, {"text": "The highest performance -0.675 and 0.678 -is obtained with ISA, while the second best feature is word length.", "labels": [], "entities": []}, {"text": "The entity-grid, the pronoun model and the discourse-new model do not improve on the baseline.", "labels": [], "entities": []}, {"text": "Although these models have been successfully used as components in state-of-the-art systems for discriminating coherent from incoherent news documents, and the entity-grid model has also been successfully applied to learner text (, they seem to have minimal impact on performance, while the discourse-new model decreases \u03c1 by 0.01.", "labels": [], "entities": []}, {"text": "On the other hand, LOWBOW lex and LOWBOW POS give an increase in performance, which confirms our hypothesis that local histograms are useful.", "labels": [], "entities": [{"text": "LOWBOW", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9111228585243225}, {"text": "LOWBOW POS", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.7898002564907074}]}, {"text": "Also, the former seems to perform slightly better than the latter.", "labels": [], "entities": []}, {"text": "Our adapted version of the IBM model -IBM model POS -performs better than its lexicalized version, which does not have an impact on performance, while larger differences are observed in r.", "labels": [], "entities": []}, {"text": "Additionally, the increase in performance is larger than the one obtained with the entity-grid, pronoun or discourse-new model.", "labels": [], "entities": []}, {"text": "The forward version of IBM model POS seems to perform slightly better than the backward one, while the results are comparable to LOWBOW POS and outperformed by LOWBOW lex . The rest of the models do not perform as well; the number of pronouns or discourse connectives gives low results, while lemma and POS cosine similarity between adjacent sentences are also We compute mean values of correlation coefficients by first applying the r-to-Z Fisher transformation, and then using the Fisher weighted mean correlation coefficient.", "labels": [], "entities": [{"text": "Fisher weighted mean correlation coefficient", "start_pos": 483, "end_pos": 527, "type": "METRIC", "confidence": 0.6596279084682465}]}, {"text": "Significance tests in averaged correlations are omitted as variable estimates are produced, whose variance is hard to be estimated unbiasedly.", "labels": [], "entities": []}, {"text": "have shown that combining the entity-grid with the pronoun, discourse-new and lexicalized IBM models gives state-of-the-art results for discriminating news documents and their random permutations.", "labels": [], "entities": []}, {"text": "We also combine these models and assess their performance under the AA framework.", "labels": [], "entities": [{"text": "AA framework", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.8394991755485535}]}, {"text": "Row 16 of shows that the combination does not give an improvement over the individual models.", "labels": [], "entities": []}, {"text": "Moreover, combining all feature classes together in row 17 does not yield higher results than those obtained with ISA, while \u03c1 is no better than the baseline.", "labels": [], "entities": []}, {"text": "In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in to report results of the final best system.", "labels": [], "entities": []}, {"text": "Validating the model on a different exam year also shows us the extent to which it generalizes between years.", "labels": [], "entities": []}, {"text": "The published correlations on this dataset are 0.741 and 0.773 rand \u03c1 respectively.", "labels": [], "entities": [{"text": "rand \u03c1", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9577219486236572}]}, {"text": "Adding ISA on top of the previous system significantly improves the Calculated using one-tailed tests for the difference between r \u03c1 Baseline 0.741 0.773 ISA 0.749 0.790 published results on the 2001 texts, getting closer to the upper-bound.", "labels": [], "entities": [{"text": "Calculated", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.907217264175415}]}, {"text": "The upper-bound on this dataset 20 is 0.796 and 0.792 rand \u03c1 respectively, calculated by taking the average correlation between the FCE grades and the ones provided by 4 senior ESOL examiners . also presents the average correlation between our extended AA system's predicted grades and the 4 examiners' grades, in addition to the original FCE grades from the dataset.", "labels": [], "entities": [{"text": "rand \u03c1", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9720706343650818}, {"text": "ESOL", "start_pos": 177, "end_pos": 181, "type": "DATASET", "confidence": 0.8938838243484497}]}, {"text": "Again, our extended model improves over the baseline.", "labels": [], "entities": []}, {"text": "Finally, we explore the utility of our best model for assessing the publically available 'outlier' texts used in.", "labels": [], "entities": []}, {"text": "The previous AA system is unable to downgrade appropriately 'outlier' scripts containing individually high-scoring sentences with poor overall coherence, created by randomly ordering a set of highly-marked texts.", "labels": [], "entities": []}, {"text": "To test our best system, we train an SVM rank preference model with the ISA-derived coherence feature, which can explicitly capture such sequential trends.", "labels": [], "entities": []}, {"text": "A generic model for flagging putative 'outlier' texts -whose predicted score is lower than a predefined threshold -for manual checking might be used as the first stage of a deployed AA system.", "labels": [], "entities": [{"text": "flagging putative 'outlier' texts", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.8138669629891714}]}, {"text": "The ISA model improves rand \u03c1 by 0.320 and 0.463 respectively for predicting a score on this type of 'outlier' texts and their original version).", "labels": [], "entities": [{"text": "rand \u03c1", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.8993367254734039}]}], "tableCaptions": [{"text": " Table 1: 5-fold cross-validation performance on texts  from year 2000 when adding different coherence features  on top of the baseline AA system.", "labels": [], "entities": [{"text": "AA", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.8940777778625488}]}, {"text": " Table 3: Average correlation between the AA model, the  FCE dataset grades, and 4 examiners on the exam scripts  from year 2000.", "labels": [], "entities": [{"text": "AA", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9983842372894287}, {"text": "FCE dataset grades", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.9110841155052185}]}, {"text": " Table 4: Performance of the ISA AA model on outliers.", "labels": [], "entities": [{"text": "ISA AA", "start_pos": 29, "end_pos": 35, "type": "TASK", "confidence": 0.7024520933628082}]}]}