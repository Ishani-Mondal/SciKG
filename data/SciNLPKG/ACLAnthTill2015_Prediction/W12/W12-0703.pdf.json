{"title": [{"text": "Sweeping through the Topic Space: Bad luck? Roll again!", "labels": [], "entities": [{"text": "Roll", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9588893055915833}]}], "abstractContent": [{"text": "Topic Models (TM) such as Latent Dirich-let Allocation (LDA) are increasingly used in Natural Language Processing applications.", "labels": [], "entities": []}, {"text": "At this, the model parameters and the influence of randomized sampling and inference are rarely examined-usually, the recommendations from the original papers are adopted.", "labels": [], "entities": []}, {"text": "In this paper, we examine the parameter space of LDA topic models with respect to the application of Text Segmentation (TS), specifically targeting error rates and their variance across different runs.", "labels": [], "entities": [{"text": "Text Segmentation (TS)", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.7767487525939941}]}, {"text": "We find that the recommended settings result in error rates far from optimal for our application.", "labels": [], "entities": [{"text": "error", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.9557927846908569}]}, {"text": "We show substantial variance in the results for different runs of model estimation and inference, and give recommendations for increasing the robust-ness and stability of topic models.", "labels": [], "entities": []}, {"text": "Running the inference step several times and selecting the last topic ID assigned per token, shows considerable improvements.", "labels": [], "entities": []}, {"text": "Similar improvements are achieved with the mode method: We store all assigned topic IDs during each inference iteration step and select the most frequent topic ID assigned to each word.", "labels": [], "entities": []}, {"text": "These recommendations do not only apply to TS, but are generic enough to transfer to other applications.", "labels": [], "entities": [{"text": "TS", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.8950915932655334}]}], "introductionContent": [{"text": "With the rise of topic models such as pLSI) or LDA () in Natural Language Processing (NLP), an increasing number of works in the field use topic models to map terms from a high-dimensional word space to a lower-dimensional semantic space.", "labels": [], "entities": []}, {"text": "TMs are 'the new Latent Semantic Analysis' (LSA),, and it has been shown that generative models like pLSI and LDA not only have a better mathematical foundation rooted in probability theory, but also outperform LSA in document retrieval and classification, e.g..", "labels": [], "entities": [{"text": "Latent Semantic Analysis' (LSA)", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.7761457065741221}, {"text": "document retrieval and classification", "start_pos": 218, "end_pos": 255, "type": "TASK", "confidence": 0.6900080293416977}]}, {"text": "To estimate the model parameters in LDA, the exact computation that was straightforward in LSA (matrix factorization) is replaced by a randomized Monte-Carlo sampling procedure (e.g. variational Bayes or Gibbs sampling).", "labels": [], "entities": []}, {"text": "Aside from the main parameter, the number of topics or dimensions, surprisingly little attention has been spent to understand the interactions of hyperparameters, the number of sampling iterations in model estimation and interference, and the stability of topic assignments across runs using different random seeds.", "labels": [], "entities": []}, {"text": "While progress in the field of topic modeling is mainly made by adjusting prior distributions (e.g. (), or defining more complex model mixtures, it seems unclear whether improvements, reached on intrinsic measures like perplexity or on application-based evaluations, are due to an improved model structure or could originate from sub-optimal parameter settings or literally 'bad luck' due to the randomized nature of the sampling process.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7672702074050903}]}, {"text": "In this paper, we address these issues by systematically sweeping the parameter space.", "labels": [], "entities": []}, {"text": "For this, we pick LDA since it is the most commonly used TM in the field of NLP.", "labels": [], "entities": []}, {"text": "To evaluate the contribution of the TM, we choose the task of TS: this task has received considerable interest from the NLP community, standard datasets and evaluation measures are available for testing, and it has been shown that this task considerably benefits from the use of TMs, see (.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: In the next section, we present related work regarding text segmentation using topic models and topic model parameter evaluations.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7281585782766342}]}, {"text": "Section 3 defines the TopicTiling text segmentation algorithm, which is a simplified version of TextTiling, and makes direct use of topic assignments.", "labels": [], "entities": [{"text": "TopicTiling text segmentation", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.7804561058680216}]}, {"text": "Its simplicity allows us to observe direct consequences of LDA parameter settings.", "labels": [], "entities": []}, {"text": "Further, we describe the experimental setup, our applicationbased evaluation methodology including the data set and the LDA parameters we vary in Section 4.", "labels": [], "entities": []}, {"text": "Results of our experiments in Section 5 indicate that a) there is an optimal range for the number of topics, b) there is considerable variance in performance for different runs for both model estimation and inference, c) increasing the number of sampling iterations stabilizes average performance but does not make TMs more robust, but d) combining the output of several independent sampling runs does, and additionally leads to large error rate reductions.", "labels": [], "entities": [{"text": "model estimation", "start_pos": 186, "end_pos": 202, "type": "TASK", "confidence": 0.7070454359054565}]}, {"text": "Similar results are obtained by e) the mode method with less computational costs using the most frequent topic ID that is assigned during different inference iteration steps.", "labels": [], "entities": []}, {"text": "In the conclusion, we give recommendations to add stability and robustness for TMs: aside from optimization of the hyperparameters, we recommend combining the topic assignments of different inference iterations, and/or of different independent inference runs.", "labels": [], "entities": [{"text": "TMs", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9583467841148376}]}], "datasetContent": [{"text": "For topic modeling, we use the widely applied LDA (, This model uses a training corpus of documents to create document-topic and topic-word distributions and is parameterized by the number of topics T as well as by two hyperparameters.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9228747487068176}]}, {"text": "To generate a document, the topic proportions are drawn using a Dirichlet distribution with hyperparameter \u03b1.", "labels": [], "entities": []}, {"text": "Adjacent for each word w a topic z dw is chosen according to a multinomial distribution using hyperparameter \u03b2 z dw . The model is estimated using m iterations of Gibbs sampling.", "labels": [], "entities": []}, {"text": "Unseen documents can be annotated with an existing topic model using Bayesian inference methods.", "labels": [], "entities": []}, {"text": "At this, Gibbs sampling with i iterations is used to estimate the topic ID for each word, given the topics of the other words in the same sentential unit.", "labels": [], "entities": []}, {"text": "After inference, every word in every sentence receives a topic ID, which is the sole information that is used by the TopicTiling algorithm to determine the segmentation.", "labels": [], "entities": []}, {"text": "We use the GibbsLDA implementation by for all our experiments.", "labels": [], "entities": [{"text": "GibbsLDA", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.8689376711845398}]}, {"text": "The article of compares LDA with pLSI and Mixture Unigram models using the perplexity of the model.", "labels": [], "entities": []}, {"text": "Ina collaborative filtering evaluation for different numbers of topics they observe that using too many topics leads to overfitting and to worse results.", "labels": [], "entities": []}, {"text": "In the field of topic model evaluations, use a corpus of abstracts published between 1991 and 2001 and evaluate model perplexity.", "labels": [], "entities": [{"text": "topic model evaluations", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.6475067933400472}]}, {"text": "For this particular corpus, they achieve the lowest perplexity using 300 topics.", "labels": [], "entities": []}, {"text": "Furthermore, they compare different sampling methods and show that the perplexity converges faster with Gibbs sampling than with expectation propagation and variational Bayes.", "labels": [], "entities": []}, {"text": "On a small artificial testset, small variations in perplexity across different runs were observed in early sampling iterations, but all runs converged to the same limit.", "labels": [], "entities": []}, {"text": "In topic models are evaluated with symmetric and asymmetric hyperparameters based on the perplexity.", "labels": [], "entities": []}, {"text": "They observe a benefit using asymmetric parameters for \u03b1, but cannot show improvement with asymmetric priors for \u03b2.", "labels": [], "entities": []}, {"text": "As dataset the Choi dataset) is used.", "labels": [], "entities": [{"text": "Choi dataset", "start_pos": 15, "end_pos": 27, "type": "DATASET", "confidence": 0.9121714532375336}]}, {"text": "This dataset is an artificially generated corpus that consists of 700 documents.", "labels": [], "entities": []}, {"text": "Each document consists of 10 segments and each segment has 3-11 sentences extracted from a document of the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.9635320901870728}]}, {"text": "For the first setup, we perform a 10-fold Cross Validation (CV) for estimating the TM (estimating on 630 documents at a time), for the other setups we use 600 documents for TM estimation and the remaining 100 documents for testing.", "labels": [], "entities": [{"text": "Cross Validation (CV)", "start_pos": 42, "end_pos": 63, "type": "METRIC", "confidence": 0.7767597913742066}, {"text": "TM estimation", "start_pos": 173, "end_pos": 186, "type": "TASK", "confidence": 0.9217526018619537}]}, {"text": "While we aim to neglect using the same documents for training and testing, it is not guaranteed that all testing data is unseen, since the same source sentences can find their way in several artificially crafted 'documents'.", "labels": [], "entities": []}, {"text": "This problem, however, applies for all evaluations on this dataset that use any kind of training, be it LDA models in or TF-IDF values in.", "labels": [], "entities": []}, {"text": "For the evaluation of the Topic Model in combination of Text Segmentation, we use the P k measure (), which is a standard measure for error rates in the field of TS.", "labels": [], "entities": [{"text": "Text Segmentation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7045422047376633}, {"text": "TS", "start_pos": 162, "end_pos": 164, "type": "TASK", "confidence": 0.9239843487739563}]}, {"text": "This measure compares the gold standard segmentation with the output of the algorithm.", "labels": [], "entities": []}, {"text": "AP k value of 0 indicates a perfect segmentation, the averaged state of the art on the Choi Dataset is P k = 0.0275 (.", "labels": [], "entities": [{"text": "AP k", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9447013735771179}, {"text": "Choi Dataset", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.980702817440033}]}, {"text": "To assess the robustness of the TM, we sweep over varying configurations of the LDA model, and plot the results using Box-and-Whiskers plots: the box indicates the quartiles and the whiskers are maximal 1.5 times of the Interquartile Range (IQR) or equal to the data point that is no greater to the 1.5 IQR.", "labels": [], "entities": [{"text": "TM", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9412797689437866}, {"text": "Interquartile Range (IQR)", "start_pos": 220, "end_pos": 245, "type": "METRIC", "confidence": 0.8954610586166382}]}, {"text": "The following parameters are subject to our exploration: \u2022 T : Number of topics used in the LDA model.", "labels": [], "entities": [{"text": "T", "start_pos": 59, "end_pos": 60, "type": "METRIC", "confidence": 0.9979999661445618}]}, {"text": "Common values vary between 50 and 500.", "labels": [], "entities": []}, {"text": "\u2022 \u03b1 : Hyperparameter that regulates the sparseness topic-per-document distribution.", "labels": [], "entities": []}, {"text": "Lower values result in documents being represented by fewer topics).", "labels": [], "entities": []}, {"text": "Recommended: \u03b1 = 50/T ( \u2022 \u03b2 : Reducing \u03b2 increases the sparsity of topics, by assigning fewer terms to each topic, which is correlated to how related words need to be, to be assigned to a topic.", "labels": [], "entities": [{"text": "T", "start_pos": 20, "end_pos": 21, "type": "METRIC", "confidence": 0.9883503913879395}]}, {"text": "Recommended: \u03b2 = {0.1, 0.01} ( \u2022 m Model estimation iterations.", "labels": [], "entities": []}, {"text": "Recommended / common settings: m = 500\u22125000 ( \u2022 i Inference iterations.", "labels": [], "entities": []}, {"text": "Recommended / common settings: 100 (Phan and Nguyen, 2007) \u2022 d Mode of topic assignments.", "labels": [], "entities": []}, {"text": "At each inference iteration step, a topic ID is assigned to each word within a document (represented as a sentence in our application).", "labels": [], "entities": []}, {"text": "With this option, we count these topic assignments for each single word in each iteration.", "labels": [], "entities": []}, {"text": "After all i inference iterations, the most frequent topic ID is chosen for each word in a document.", "labels": [], "entities": []}, {"text": "\u2022 r Number of inference runs: We repeat the inference r times and assign the most frequently assigned topic per word at the final inference run for the segmentation algorithm.", "labels": [], "entities": []}, {"text": "High r values might reduce fluctuations due to the randomized process and lead to a more stable word-to-topic assignment.", "labels": [], "entities": []}, {"text": "All introduced parameters parameterize the TM.", "labels": [], "entities": []}, {"text": "We are not aware of any research that has used several inference runs rand the mode of topic assignments d to increase stability and varying TM parameters in combinations with measures other then perplexity.", "labels": [], "entities": []}], "tableCaptions": []}