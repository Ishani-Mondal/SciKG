{"title": [{"text": "Content Selection From Semantic Web Data", "labels": [], "entities": []}], "abstractContent": [{"text": "So far, there has been little success in Natural Language Generation in coming up with general models of the content selection process.", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6380370755990347}, {"text": "content selection process", "start_pos": 109, "end_pos": 134, "type": "TASK", "confidence": 0.8111828962961832}]}, {"text": "Nonetheless, there has been some work on content selection that employ Machine learning or heuristic search.", "labels": [], "entities": [{"text": "content selection", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7406169772148132}]}, {"text": "On the other side, there is a clear tendency in NLG towards the use of resources encoded in standard Semantic Web representation formats.", "labels": [], "entities": []}, {"text": "For these reasons, we believe that time has come to propose an initial challenge on content selection from Semantic Web data.", "labels": [], "entities": []}, {"text": "In this paper, we briefly outline the idea and plan for the execution of this task.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The participants will be asked to participate in a preliminary task consisting in marking which triples are included in the text given a subset of the paired corpus (the size of the subset still has to be decided).", "labels": [], "entities": []}, {"text": "This task could be supported by some automatic anchoring techniques such as used in).", "labels": [], "entities": []}, {"text": "The objectives of the task are threefold: (1) to provide all participants with a common set of \"correct answers\" to be exploited in their approach, (2) to familiarize the participants with the nature of the contents, their semantics and the texts, and (3) to provide the task with a ceiling for the evaluation, i.e. inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Annotation guidelines will be needed to ensure that all participants follow the same procedure when annotating texts.", "labels": [], "entities": []}, {"text": "For this purpose, an early document will be produced detailing the procedure together with examples and descriptions of relevant problems such as ambiguities in the annotation.", "labels": [], "entities": []}, {"text": "The guidelines will be improved in multiple stages of annotation and revision with the goal of maximizing inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Each executable is run against the test corpus and the selected triples evaluated against the gold triple selection set.", "labels": [], "entities": []}, {"text": "Since this is formally a relatively simple task of selecting a subset of a given set, we will use for evaluation standard precision, recall and F measures.", "labels": [], "entities": [{"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9955982565879822}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9993336796760559}, {"text": "F", "start_pos": 144, "end_pos": 145, "type": "METRIC", "confidence": 0.9959867596626282}]}, {"text": "In addition, other appropriate metrics will be explored-for instance, certain metrics for extractive summarisation (which is to some extent a similar task).", "labels": [], "entities": [{"text": "extractive summarisation", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.6384086310863495}]}, {"text": "The organizers will explore whether it will be feasible to select and annotate some test examples from a different corpus and have the systems evaluated on these as a separate task.", "labels": [], "entities": []}, {"text": "presents the different tasks, protagonists and the schedule involved in the organization of the challenge.", "labels": [], "entities": []}, {"text": "The challenge proper will take place between November 2012 and May/June 2013.", "labels": [], "entities": []}, {"text": "Once all participants have submitted their executable to solve the task, the evaluation set will be processed.", "labels": [], "entities": []}, {"text": "If timing is tight, however, this could be done whilst the participants are still working on the task or extra effort (for instance, from the organizers) could be brought in.", "labels": [], "entities": []}, {"text": "A subset of the data is randomly selected and annotated with the selected triples by the participants.", "labels": [], "entities": []}, {"text": "This two-stage approach to triple selection annotation is proposed in order to avoid any bias on the evaluation data.", "labels": [], "entities": [{"text": "triple selection annotation", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.841799239317576}]}], "tableCaptions": []}