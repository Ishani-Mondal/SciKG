{"title": [{"text": "Towards a Surface Realization-Oriented Corpus Annotation", "labels": [], "entities": [{"text": "Surface Realization-Oriented Corpus Annotation", "start_pos": 10, "end_pos": 56, "type": "TASK", "confidence": 0.7762216478586197}]}], "abstractContent": [{"text": "Until recently, deep stochastic surface realization has been hindered by the lack of semantically annotated corpora.", "labels": [], "entities": [{"text": "deep stochastic surface realization", "start_pos": 16, "end_pos": 51, "type": "TASK", "confidence": 0.7099215984344482}]}, {"text": "This is about to change.", "labels": [], "entities": []}, {"text": "Such corpora are increasingly available , e.g., in the context of CoNLL shared tasks.", "labels": [], "entities": [{"text": "CoNLL shared tasks", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.5160148541132609}]}, {"text": "However, recent experiments with CoNLL 2009 corpora show that these popular resources, which serve well for other applications , may not do so for generation.", "labels": [], "entities": [{"text": "CoNLL 2009 corpora", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.9299822251001993}]}, {"text": "The attempts to adapt them for generation resulted so far in a better performance of the realizers, but not yet in a genuinely semantic generation-oriented annotation schema.", "labels": [], "entities": []}, {"text": "Our goal is to initiate a debate on how a generation suitable annotation schema should be defined.", "labels": [], "entities": []}, {"text": "We define some general principles of a semantic generation-oriented annotation and propose an annotation schema that is based on these principles.", "labels": [], "entities": []}, {"text": "Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7671139240264893}]}, {"text": "While in the early days of stochastic generation, annotations produced for other applications were used (), the poor results obtained, e.g., by ) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.", "labels": [], "entities": [{"text": "stochastic generation", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.7111036628484726}, {"text": "CoNLL 2009 corpora", "start_pos": 164, "end_pos": 182, "type": "DATASET", "confidence": 0.9467849334081014}]}, {"text": "This has also been acknowledged in the run-up to the surface realization challenge, where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (, i.e.,), which served as the reference treebank, into a more \"generation friendly\" annotation.", "labels": [], "entities": [{"text": "CoNLL 2008 corpora", "start_pos": 184, "end_pos": 202, "type": "DATASET", "confidence": 0.9552897016207377}]}, {"text": "However, all of the available annotations are to a certain extent still syntactic.", "labels": [], "entities": []}, {"text": "Even PropBank and its generation-oriented variants contain a significant number of syntactic features.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 5, "end_pos": 13, "type": "DATASET", "confidence": 0.9596851468086243}]}, {"text": "Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule.", "labels": [], "entities": []}, {"text": "(,),, and () start from deeper structures: Walker et al. and Stent et al. from deep-syntactic structures, and Wong and Mooney and Mairesse et al. from higher order predicate logic structures.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, Trained on the original ConLL 2009 corpora, ( )'s SVM-based generator reached a BLEU score of 0.12 for Chinese, 0.18 for English, 0.11 for German and 0.14 for Spanish.", "labels": [], "entities": [{"text": "ConLL 2009 corpora", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.9522518515586853}, {"text": "BLEU score", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.9816005527973175}]}, {"text": "Joining the unconnected parts of the sentence annotations to connected trees (as required by a stochastic realizer) improved the performance to a BLEU score of 0.69 for Chinese, 0.66 for English, 0.61 for German and 0.68 for Spanish.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.9829416275024414}]}, {"text": "none of them uses corpora annotated with the structures from which they start.", "labels": [], "entities": []}, {"text": "To deep stochastic generation, the use of hybrid models is not an option and training a realizer on syntactically-biased annotations is highly problematic in the case of data-to-text NLG, which starts from numeric time series or conceptual or semantic structures: the syntactic features will be simply not available in the input structures at the moment of application.", "labels": [], "entities": []}, {"text": "Therefore, it is crucial to define a theoretically sound semantic annotation that is still good in practical terms.", "labels": [], "entities": []}, {"text": "Our goal is thus to discuss some general principles of a semantic generation-oriented annotation schema and offer a first evaluation of its possible impact on stochastic generation.", "labels": [], "entities": []}, {"text": "Section2 details what kind of information is available respectively not available during data-to-text generation.", "labels": [], "entities": []}, {"text": "Section 3 states some general principles that constrain an adequate semantic representation, while Section 4 formally defines their well-formedness.", "labels": [], "entities": []}, {"text": "Section 5 reports then on the experiments made with the proposed annotation, and Section6 offers some conclusions.", "labels": [], "entities": [{"text": "Section6", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.8992234468460083}]}], "datasetContent": [{"text": "Obviously, the removal of syntactic features from a given standard annotation, with the goal to obtain an increasingly more semantic annotation, can only be accepted if the quality of (deep) stochastic generation does not unacceptably decrease.", "labels": [], "entities": []}, {"text": "To assess this aspect, we converted automatically the PropBank annotation of the WSJ journal as used in the CoNLL shared task 2009 into an annotation that complies with all of the principles sketched above for deep statistical generation and trained 's generator on this new annotation.", "labels": [], "entities": [{"text": "PropBank annotation of the WSJ journal", "start_pos": 54, "end_pos": 92, "type": "DATASET", "confidence": 0.8675168951352438}, {"text": "CoNLL shared task 2009", "start_pos": 108, "end_pos": 130, "type": "DATASET", "confidence": 0.8733676671981812}, {"text": "statistical generation", "start_pos": 215, "end_pos": 237, "type": "TASK", "confidence": 0.6690627485513687}]}, {"text": "For our experiments, we used the usual training, development and test data split of the WSJ corpus;  The resulting BLEU score of our experiment was 0.64, which is comparable with the accuracy reported in ) (namely, 0.659), who used an annotation that still contained all functional nodes (such that their generation task was considerably more syntactic and thus more straightforward).", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.978166937828064}, {"text": "BLEU score", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9778503775596619}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9972718358039856}]}, {"text": "To assess furthermore whether the automatically converted PropBank already offers some advantages to other applications than generation, we used it in a semantic role labeling (SRL) experiment with)'s parser.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.9080449938774109}, {"text": "semantic role labeling (SRL)", "start_pos": 153, "end_pos": 181, "type": "TASK", "confidence": 0.7641216913859049}]}, {"text": "The achieved overall accuracy is 0.818, with all analysis stages (including the predicate identification stage) being automatic, which is a rather competitive In the original CoNLL SRL setting with Oracle reading, an accuracy of 0.856 is achieved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9990575909614563}, {"text": "predicate identification", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.6596152633428574}, {"text": "CoNLL SRL setting", "start_pos": 175, "end_pos": 192, "type": "DATASET", "confidence": 0.8508368929227194}, {"text": "accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.998979389667511}]}, {"text": "Another telling comparison can be made between the outcomes of the First Surface Realization Shared Task (, in which two different input representations were given to the competing teams: a shallow representation and a deep representation.", "labels": [], "entities": [{"text": "First Surface Realization Shared Task", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.7101463079452515}]}, {"text": "The shallow structures were unordered syntactic dependency trees, with all the tokens of the sentence, and the deep structures were predicateargument graphs with some nodes removed (see Section 2).", "labels": [], "entities": []}, {"text": "Although the performance of shallow generators was higher than the performance of the deep generators (the StuMaBa shallow generator) obtained a BLEU score of 0.89, as opposed to 0.79 of the StuMaBa deep gen- erator), the difference is not as striking as one would expect.", "labels": [], "entities": [{"text": "StuMaBa shallow generator", "start_pos": 107, "end_pos": 132, "type": "DATASET", "confidence": 0.9023922085762024}, {"text": "BLEU score", "start_pos": 145, "end_pos": 155, "type": "METRIC", "confidence": 0.987403154373169}, {"text": "StuMaBa deep gen- erator", "start_pos": 191, "end_pos": 215, "type": "TASK", "confidence": 0.7675189971923828}]}], "tableCaptions": [{"text": " Table 1: Data split of the used data in the WSJ Corpus", "labels": [], "entities": [{"text": "Data split", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.5924423784017563}, {"text": "WSJ", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.924971878528595}]}]}