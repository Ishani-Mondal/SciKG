{"title": [{"text": "The Heterogeneity Principle in Evaluation Measures for Automatic Summarization *", "labels": [], "entities": [{"text": "Automatic Summarization", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.6917476952075958}]}], "abstractContent": [{"text": "The development of summarization systems requires reliable similarity (evaluation) measures that compare system outputs with human references.", "labels": [], "entities": [{"text": "summarization", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.9837546944618225}]}, {"text": "A reliable measure should have correspondence with human judgements.", "labels": [], "entities": []}, {"text": "However, the reliability of measures depends on the test collection in which the measure is meta-evaluated; for this reason, it has not yet been possible to reliably establish which are the best evaluation measures for automatic summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 229, "end_pos": 242, "type": "TASK", "confidence": 0.88771653175354}]}, {"text": "In this paper, we propose an unsupervised method called Heterogeneity-Based Ranking (HBR) that combines summa-rization evaluation measures without requiring human assessments.", "labels": [], "entities": []}, {"text": "Our empirical results indicate that HBR achieves a similar correspondence with human assessments than the best single measure for every observed corpus.", "labels": [], "entities": []}, {"text": "In addition, HBR results are more robust across topics than single measures.", "labels": [], "entities": [{"text": "HBR", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.8150699734687805}]}], "introductionContent": [{"text": "In general, automatic evaluation metrics for summarization are similarity measures that compare system outputs with human references.", "labels": [], "entities": [{"text": "summarization", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.988699734210968}]}, {"text": "The typical development cycle of a summarization system begins with selecting the most predictive metric.", "labels": [], "entities": [{"text": "summarization", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.9626356363296509}]}, {"text": "For this, evaluation metrics are compared to each other in terms * This work has been partially funded by the Madrid government, grant MA2VICMR (S-2009/TIC-1542), the Spanish Government, grant Holopedia (TIN2010-21128-C02-01) and the European Community's Seventh Framework Programme) under grant agreement nr. 288024 (LiMoSINe project). of correlation with human judgements.", "labels": [], "entities": [{"text": "MA2VICMR", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.8443117737770081}]}, {"text": "The second step consists of tuning the summarization system (typically in several iterations) in order to maximize the scores according to the selected evaluation measure.", "labels": [], "entities": [{"text": "summarization", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9738556742668152}]}, {"text": "There is a wide set of available measures beyond the standard ROUGE: for instance, those comparing basic linguistic elements (), dependency triples or convolution kernels () which reported some reliability improvement with respect to ROUGE in terms of correlation with human judgements.", "labels": [], "entities": []}, {"text": "However, in practice ROUGE is still the preferred metric of choice.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9969421029090881}]}, {"text": "The main reason is that the superiority of a measure with respect to other is not easy to demonstrate: the variability of results across corpora, reference judgements (Pyramid vs responsiveness) and correlation criteria (system vs. summary level) is substantial.", "labels": [], "entities": []}, {"text": "In the absence of a clear quality criterion, the de-facto standard is usually the most reasonable choice.", "labels": [], "entities": []}, {"text": "In this paper we rethink the development cycle of summarization systems.", "labels": [], "entities": [{"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.977807343006134}]}, {"text": "Given that the best measure changes across evaluation scenarios, we propose using multiple automatic evaluation measures, together with an unsupervised method to combine measures called Heterogeneity Based Ranking (HBR).", "labels": [], "entities": []}, {"text": "This method is grounded on the general Heterogeneity property proposed in), which states that the more a measure set is heterogeneous, the more a score increase according to all the measures simultaneously is reliable.", "labels": [], "entities": [{"text": "Heterogeneity", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.9837990999221802}]}, {"text": "In brief, the HBR method consists of computing the heterogeneity of measures for which a 36 system-produced summary improves each of the rest of summaries in comparison.", "labels": [], "entities": []}, {"text": "Our empirical results indicate that HBR achieves a similar correspondence with human assessments than the best single measure for every observed corpus.", "labels": [], "entities": []}, {"text": "In addition, HBR results are more robust across topics than single measures.", "labels": [], "entities": [{"text": "HBR", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.8150699734687805}]}], "datasetContent": [{"text": "The traditional way of meta-evaluating measures consists of computing the Pearson correlation between measure scores and quality human assessments.", "labels": [], "entities": [{"text": "Pearson correlation between measure scores", "start_pos": 74, "end_pos": 116, "type": "METRIC", "confidence": 0.8962467670440674}]}, {"text": "But the main goal of automatic evaluation metrics is not exactly to predict the real quality of systems; rather than this, their core mission is detecting system outputs that improve the baseline system in each development cycle.", "labels": [], "entities": []}, {"text": "Therefore, the issue is to what extent a quality increase between two system outputs is reflected by the output ranking produced by the measure.", "labels": [], "entities": []}, {"text": "According to this perspective, we propose metaevaluating measures in terms of an extended version of AUC (Area Under the Curve).", "labels": [], "entities": []}, {"text": "AUC can be seen as the probability of observing a score increase when observing areal quality increase between two system outputs).", "labels": [], "entities": [{"text": "AUC", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9277549386024475}]}, {"text": "In order to customize this measure to our scenario, two special cases must be handled: (i) For cases in which both summaries obtain the same value, we assume that the measure rewards each instance with equal probability.", "labels": [], "entities": []}, {"text": "That is, if x(s) = x(s ),P (x(s) > x(s )|Q(s) > Q(s )) = 1 2 . (ii) Given that in the AS evaluation scenarios there are multiple quality levels, we still apply the same probabilistic AUC definition, considering pairs of summaries in which one of them achieves more quality than the other according to human assessors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test collections from 2005 and 2006 DUC evaluation campaigns used in our experiments.", "labels": [], "entities": []}]}