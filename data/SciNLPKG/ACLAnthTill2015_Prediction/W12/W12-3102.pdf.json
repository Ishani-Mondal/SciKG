{"title": [{"text": "Findings of the 2012 Workshop on Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.7303360303243002}]}], "abstractContent": [{"text": "This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality.", "labels": [], "entities": [{"text": "WMT12 shared tasks", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.5101959307988485}, {"text": "translation task", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.8969109058380127}, {"text": "machine translation evaluation metrics", "start_pos": 105, "end_pos": 143, "type": "TASK", "confidence": 0.8552061170339584}]}, {"text": "We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7372808456420898}]}, {"text": "We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics.", "labels": [], "entities": []}, {"text": "We introduced anew quality estimation task this year, and evaluated submissions from 11 teams.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at NAACL 2012.", "labels": [], "entities": [{"text": "statistical Machine Translation (WMT)", "start_pos": 71, "end_pos": 108, "type": "TASK", "confidence": 0.7863364766041437}, {"text": "NAACL 2012", "start_pos": 128, "end_pos": 138, "type": "DATASET", "confidence": 0.9040652513504028}]}, {"text": "This workshop builds on six previous WMT workshops ().", "labels": [], "entities": [{"text": "WMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.6963502764701843}]}, {"text": "In the past, the workshops have featured a number of shared tasks: a translation task between English and other languages, a task for automatic evaluation metrics to predict human judgments of translation quality, and a system combination task to get better translation quality by combining the outputs of multiple translation systems.", "labels": [], "entities": [{"text": "translation task between English and other languages", "start_pos": 69, "end_pos": 121, "type": "TASK", "confidence": 0.8167964901242938}]}, {"text": "This year we discontinued the system combination task, and introduced anew task in its place: \u2022 Quality estimation task -Structured prediction tasks like MT are difficult, but the difficulty is not uniform across all input types.", "labels": [], "entities": [{"text": "Structured prediction tasks", "start_pos": 121, "end_pos": 148, "type": "TASK", "confidence": 0.7917352318763733}, {"text": "MT", "start_pos": 154, "end_pos": 156, "type": "TASK", "confidence": 0.936983585357666}]}, {"text": "It would thus be useful to have some measure of confidence in the quality of the output, which has potential usefulness in a range of settings, such as deciding whether output needs human post-editing or selecting the best translation from outputs from a number of systems.", "labels": [], "entities": []}, {"text": "This shared task focused on sentence-level estimation, and challenged participants to rate the quality of sentences produced by a standard Moses translation system on an EnglishSpanish news corpus in one of two tasks: ranking and scoring.", "labels": [], "entities": [{"text": "sentence-level estimation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.6585189551115036}, {"text": "EnglishSpanish news corpus", "start_pos": 170, "end_pos": 196, "type": "DATASET", "confidence": 0.8819198211034139}]}, {"text": "Predictions were scored against a blind test set manually annotated with relevant quality judgments.", "labels": [], "entities": []}, {"text": "The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation.", "labels": [], "entities": [{"text": "WMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8729994297027588}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8300664722919464}, {"text": "machine translation", "start_pos": 227, "end_pos": 246, "type": "TASK", "confidence": 0.8182332217693329}]}, {"text": "As with previous workshops, all of the data, translations, and collected human judgments are publicly available.", "labels": [], "entities": []}, {"text": "We hope these datasets form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation or automatic prediction of translation quality.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.6872292260328928}, {"text": "system combination", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.6946515887975693}, {"text": "automatic prediction of translation", "start_pos": 147, "end_pos": 182, "type": "TASK", "confidence": 0.6007324382662773}]}], "datasetContent": [{"text": "As with past workshops, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores.", "labels": [], "entities": []}, {"text": "It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality.", "labels": [], "entities": []}, {"text": "Therefore, we define the manual evaluation to be primary, and Figure 1: Statistics for the training and test sets used in the translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 126, "end_pos": 142, "type": "TASK", "confidence": 0.9289312958717346}]}, {"text": "The number of words and the number of distinct words (case-insensitive) is based on the provided tokenizer.: Participants in the shared translation task.", "labels": [], "entities": [{"text": "shared translation task", "start_pos": 129, "end_pos": 152, "type": "TASK", "confidence": 0.6929133137067159}]}, {"text": "Not all teams participated in all language pairs.", "labels": [], "entities": []}, {"text": "The translations from the commercial, online, and rule-based systems were crawled by us, not submitted by the respective companies, and are therefore anonymized.", "labels": [], "entities": []}, {"text": "Anonymized identifiers were chosen so as to correspond with the WMT11 systems.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.9302505850791931}]}, {"text": "use the human judgments to validate automatic metrics.", "labels": [], "entities": []}, {"text": "Manual evaluation is time consuming, and it requires a large effort to conduct on the scale of our workshop.", "labels": [], "entities": [{"text": "Manual evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6070550680160522}]}, {"text": "We distributed the workload across a number of people, beginning with shared-task participants and interested volunteers.", "labels": [], "entities": []}, {"text": "This year, we also opened up the evaluation to non-expert annotators hired on Amazon Mechanical Turk).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 78, "end_pos": 100, "type": "DATASET", "confidence": 0.9628776510556539}]}, {"text": "To ensure that the Turkers provided high quality annotations, we used controls constructed from the machine translation ranking tasks from prior years.", "labels": [], "entities": [{"text": "machine translation ranking", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.7438458402951559}]}, {"text": "Control items were selected such that there was high agreement across the system developers who completed that item.", "labels": [], "entities": []}, {"text": "In all, there were 229 people who participated in the manual evaluation, with 91 workers putting in more than an hour's worth of effort, and 21 putting in more than four hours.", "labels": [], "entities": []}, {"text": "After filtering Turker rankings against the controls to discard Turkers who fell below a threshold level of agreement on the control questions, there was a collective total of 336 hours of usable labor.", "labels": [], "entities": []}, {"text": "This is similar to the total of 361 hours of labor collected for WMT11.", "labels": [], "entities": [{"text": "WMT11", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.8285171985626221}]}, {"text": "We asked annotators to evaluate system outputs by ranking translated sentences relative to each other.", "labels": [], "entities": []}, {"text": "This was our official determinant of translation quality.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8743048310279846}]}, {"text": "The total number of judgments collected for each of the language pairs is given in Table 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: A summary of the WMT12 ranking task, show- ing the number of systems and number of labels (rank- ings) collected for each of the language translation tasks.", "labels": [], "entities": [{"text": "WMT12 ranking task", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.6502531369527181}, {"text": "number of labels (rank- ings) collected", "start_pos": 83, "end_pos": 122, "type": "METRIC", "confidence": 0.6825626459386613}, {"text": "language translation tasks", "start_pos": 139, "end_pos": 165, "type": "TASK", "confidence": 0.7577005128065745}]}, {"text": " Table 3: Inter-and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11  rows contain the results from the European languages individual systems task", "labels": [], "entities": [{"text": "WMT12 manual evaluation", "start_pos": 60, "end_pos": 83, "type": "DATASET", "confidence": 0.8254132866859436}, {"text": "WMT11  rows", "start_pos": 105, "end_pos": 116, "type": "DATASET", "confidence": 0.8744083940982819}]}, {"text": " Table 5: Overall ranking with different methods (English-German)", "labels": [], "entities": []}, {"text": " Table 7: System-level Spearman's rho correlation of the  automatic evaluation metrics with the human judgments  for translation into English, ordered by average absolute  value.", "labels": [], "entities": []}, {"text": " Table 8: System-level Spearman's rho correlation of the  automatic evaluation metrics with the human judgments  for translation out of English, ordered by average abso- lute value.", "labels": [], "entities": [{"text": "translation out of English", "start_pos": 117, "end_pos": 143, "type": "TASK", "confidence": 0.8799277245998383}, {"text": "abso- lute value", "start_pos": 164, "end_pos": 180, "type": "METRIC", "confidence": 0.9302484691143036}]}, {"text": " Table 10: Segment-level Kendall's tau correlation of the  automatic evaluation metrics with the human judgments  for translation out of English, ordered by average corre- lation.", "labels": [], "entities": [{"text": "translation out of English", "start_pos": 118, "end_pos": 144, "type": "TASK", "confidence": 0.890419065952301}, {"text": "corre- lation", "start_pos": 165, "end_pos": 178, "type": "METRIC", "confidence": 0.9676229556401571}]}, {"text": " Table 13: Official results for the scoring subtask of the WMT12 Quality Evaluation shared-task. The winning submis- sion is indicated by a \u2022 (the difference with respect to the other submissions is statistically significant at p = 0.05).", "labels": [], "entities": [{"text": "WMT12 Quality Evaluation shared-task", "start_pos": 59, "end_pos": 95, "type": "DATASET", "confidence": 0.7427799850702286}]}, {"text": " Table 14: Head to head comparison for Czech-English systems", "labels": [], "entities": []}, {"text": " Table 15: Head to head comparison for English-Czech systems", "labels": [], "entities": []}, {"text": " Table 16: Head to head comparison for English-French systems", "labels": [], "entities": []}, {"text": " Table 17: Head to head comparison for English-German systems", "labels": [], "entities": []}, {"text": " Table 18: Head to head comparison for English-Spanish systems", "labels": [], "entities": []}, {"text": " Table 19: Head to head comparison for French-English systems", "labels": [], "entities": []}, {"text": " Table 20: Head to head comparison for German-English systems", "labels": [], "entities": []}, {"text": " Table 21: Head to head comparison for Spanish-English systems", "labels": [], "entities": []}, {"text": " Table 33: Automatic evaluation metric scores for systems in the WMT12 English-Czech News Task", "labels": [], "entities": [{"text": "WMT12 English-Czech News Task", "start_pos": 65, "end_pos": 94, "type": "DATASET", "confidence": 0.9604626297950745}]}, {"text": " Table 35: Automatic evaluation metric scores for systems in the WMT12 English-French News Task", "labels": [], "entities": [{"text": "WMT12 English-French News Task", "start_pos": 65, "end_pos": 95, "type": "DATASET", "confidence": 0.9558559060096741}]}]}