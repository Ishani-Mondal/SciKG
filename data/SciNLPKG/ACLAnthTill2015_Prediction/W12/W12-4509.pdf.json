{"title": [{"text": "UBIU for Multilingual Coreference Resolution in OntoNotes", "labels": [], "entities": [{"text": "UBIU", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7466718554496765}, {"text": "Multilingual Coreference Resolution", "start_pos": 9, "end_pos": 44, "type": "TASK", "confidence": 0.6821894844373068}]}], "abstractContent": [{"text": "The current work presents the participation of UBIU (Zhekova and K\u00fcbler, 2010) in the CoNLL-2012 Shared Task: Model-ing Multilingual Unrestricted Coreference in OntoNotes (Pradhan et al., 2012).", "labels": [], "entities": [{"text": "UBIU", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.8474856019020081}]}, {"text": "Our system deals with all three languages: Arabic, Chi-nese and English.", "labels": [], "entities": []}, {"text": "The system results show that UBIU works reliably across all three languages , reaching an average score of 40.57 for Arabic, 46.12 for Chinese, and 48.70 for En-glish.", "labels": [], "entities": []}, {"text": "For Arabic and Chinese, the system produces high precision, while for English, precision and recall are balanced, which leads to the highest results across languages.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9991067051887512}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9995296001434326}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9994229078292847}]}], "introductionContent": [{"text": "Multilingual coreference resolution has been gaining considerable interest among researchers in recent years.", "labels": [], "entities": [{"text": "Multilingual coreference resolution", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7760616540908813}]}, {"text": "Yet, only a very small number of systems target coreference resolution (CR) for more than one language.", "labels": [], "entities": [{"text": "coreference resolution (CR)", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.8257745862007141}]}, {"text": "A first attempt at gaining insight into the comparability of systems on different languages was accomplished in the SemEval-2010 Task 1: Coreference Resolution in Multiple Languages (.", "labels": [], "entities": [{"text": "SemEval-2010 Task 1", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7695929010709127}, {"text": "Coreference Resolution", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.8498278558254242}]}, {"text": "Six systems participated in that task, UBIU) among them.", "labels": [], "entities": []}, {"text": "However, since systems participated across the various languages rather irregularly, reported that the data points were too few to allow fora proper comparison between different approaches.", "labels": [], "entities": []}, {"text": "Further significant issues concerned system portability across the various languages and the respective language tuning, the influence of the quantity and quality of diverse linguistic annotations as well as the performance and behavior of various evaluation metrics.", "labels": [], "entities": []}, {"text": "The CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in) targeted unrestricted CR, which aims at identifying nominal coreference but also event coreference, within an English data set from the OntoNotes corpus.", "labels": [], "entities": [{"text": "English data set from the OntoNotes corpus", "start_pos": 176, "end_pos": 218, "type": "DATASET", "confidence": 0.82497581413814}]}, {"text": "Not surprisingly, attempting to include such event mentions had a detrimental effect on overall accuracy, and the best performing systems (e.g.,) did not attempt event anaphora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.998948872089386}]}, {"text": "The current shared task extends the task definition to three different languages (Arabic, Chinese and English), which can prove challenging for rule-based approaches such as the best performing system from 2011 ().", "labels": [], "entities": []}, {"text": "In the current paper, we present UBIU, a memorybased coreference resolution system, and its results in the CoNLL-2012 Shared Task.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.8138061463832855}]}, {"text": "We give an overview of UBIU in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we present the system results, after which Section 4 lays out some conclusive remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of the final system evaluation are presented in.", "labels": [], "entities": []}, {"text": "Comparing the results for mention detection (MD) on the development set (see, which shows MD before the resolution step) and the final test set (, showing MD after resolution and the deletion of singletons), we encounter a reversal of precision and recall tendencies (even though the results are not fully comparable since they are based on different data sets).", "labels": [], "entities": [{"text": "mention detection", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.6537634134292603}, {"text": "precision", "start_pos": 235, "end_pos": 244, "type": "METRIC", "confidence": 0.999056875705719}, {"text": "recall", "start_pos": 249, "end_pos": 255, "type": "METRIC", "confidence": 0.9964011907577515}]}, {"text": "This is due to the fact that during mention detection, we aim for high recall, and after coreference resolution, all mentions identified as singletons by the system are excluded from the answer set.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7331225574016571}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9991545677185059}, {"text": "coreference resolution", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.8567545115947723}]}, {"text": "Thus mentions that are coreferent in the key set but wrongly classified in the answer set are removed, leading to a decrease in recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9985620379447937}]}, {"text": "With regard to MD precision, a considerable increase is recorded, showing that the majority of the mentions that the system indicates as coreferent have the correct mention spans.", "labels": [], "entities": [{"text": "MD", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.8745823502540588}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.6189001202583313}]}, {"text": "Additionally, the problem of selecting the correct span (as described in Section 2) is another factor that has a considerable effect on precision at that stage -mentions that were accurately attached to the correct coreference chain are not considered if their span is not identical to the span of their counterparts in the key set.", "labels": [], "entities": [{"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9989011287689209}]}, {"text": "Automatic Mention Detection In the first part in, we show the system scores for UBIU's performance when no mention information is provided in the data.", "labels": [], "entities": [{"text": "Automatic Mention Detection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6067297061284384}, {"text": "UBIU", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.7439094185829163}]}, {"text": "We report both gold (using gold linguistic annotations) and auto (using automatically annotated data) settings.", "labels": [], "entities": []}, {"text": "A comparison of the results shows that there are only minor differences between them with gold outperforming auto apart from Arabic for which there is a drop of 3.75 points in the gold setting.", "labels": [], "entities": []}, {"text": "However, the small difference between all results shows that the quality of the automatic annotation is good enough fora CR system and that further improvements in the quality of the linguistic information will not necessarily improve CR.", "labels": [], "entities": [{"text": "CR", "start_pos": 235, "end_pos": 237, "type": "METRIC", "confidence": 0.9956606030464172}]}, {"text": "If we compare results across languages, we see that Arabic has the lowest results.", "labels": [], "entities": []}, {"text": "One of the reasons for this decreased performance can be found in the NP-rich syntactic structure of Arabic.", "labels": [], "entities": []}, {"text": "This leads to a high number of identified mentions and in combination with the longer sentence length to a higher number of training/test instances.", "labels": [], "entities": []}, {"text": "Another reason for the drop in performance for Arabic can be found in the lack of annotations expected by our system (named entities and predicted arguments) that were not provided by the task due to time constraints and the accuracy of the annotations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.9982802867889404}]}, {"text": "Further, Arabic is a morphologically rich language for which only the simplified standard POS tags were provided and not the gold standard ones that contain much richer and thus more helpful morphology information.", "labels": [], "entities": []}, {"text": "The results for Chinese and English are relatively close.", "labels": [], "entities": []}, {"text": "We can also see that the CEAF E results are extremely close, with a difference of less than 1%.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.6205623745918274}, {"text": "E", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.4675769805908203}]}, {"text": "MUC, in contrast, shows the largest differences with more than 30% between Arabic and English in the gold setting.", "labels": [], "entities": [{"text": "MUC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9800028800964355}]}, {"text": "It is also noteworthy that the results for English show a balance between precision and recall while both Arabic and Chinese favor precision over recall in terms of mention detection, MUC, and B 3 . The reasons for this difference between languages need to be investigated further.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9991738200187683}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9950346946716309}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9976108074188232}, {"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9891701340675354}, {"text": "mention detection", "start_pos": 165, "end_pos": 182, "type": "TASK", "confidence": 0.5929897725582123}, {"text": "MUC", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.9561161994934082}, {"text": "B 3", "start_pos": 193, "end_pos": 196, "type": "METRIC", "confidence": 0.9921185970306396}]}], "tableCaptions": [{"text": " Table 1: Mention detection (development set).", "labels": [], "entities": [{"text": "Mention detection", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.909214437007904}]}, {"text": " Table 2: The scores for the short example in (1).", "labels": [], "entities": []}, {"text": " Table 3: The features used by the singleton classifier.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation of using (+SC) or not (-SC) the sin- gleton classifier in UBIU on the development set.", "labels": [], "entities": [{"text": "UBIU", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.8882591128349304}]}, {"text": " Table 5: The features used by the coreference classifier.  91", "labels": [], "entities": []}, {"text": " Table 6: UBIU system performance in the shared task.", "labels": [], "entities": []}]}