{"title": [{"text": "Shared Task Proposal: Syntactic Paraphrase Ranking", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe anew shared task on syntactic paraphrase ranking that is intended to run in conjunction with the main surface realization shared task.", "labels": [], "entities": []}, {"text": "Taking advantage of the human judgments collected to evaluate the surface realizations produced by competing systems, the task is to automatically rank these realizations-viewed as syntactic paraphrases-in away that agrees with the human judgments as often as possible.", "labels": [], "entities": []}, {"text": "The task is designed to appeal to developers of surface realization systems as well as machine translation evaluation metrics: for surface realization systems, the task sidesteps the thorny issue of converting inputs to a common representation; for MT evaluation metrics, the task provides a challenging framework for advancing automatic evaluation, as many of the paraphrases are expected to be of high quality, differing only in subtle syntactic choices.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 87, "end_pos": 117, "type": "TASK", "confidence": 0.8444523612658182}, {"text": "MT evaluation metrics", "start_pos": 249, "end_pos": 270, "type": "TASK", "confidence": 0.9149786631266276}]}], "introductionContent": [{"text": "For the first surface realization shared task, the organizers considered running a follow-on task for evaluating automatic evaluation metrics-along the lines of similar meta-evaluations carried out for machine translation in recent years-though it was deferred for lack of time.", "labels": [], "entities": [{"text": "surface realization shared task", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.7591286078095436}, {"text": "machine translation", "start_pos": 202, "end_pos": 221, "type": "TASK", "confidence": 0.7170959860086441}]}, {"text": "For the second surface realization shared task, we propose to generalize this metrics meta-evaluation task to also usefully encompass realization ranking, where the various realizations generated fora given input in the main task are viewed as syntactic paraphrases of the original corpus sentence.", "labels": [], "entities": []}, {"text": "The syntactic paraphrasing shared task comprises three tracks, described in the next section; in each case, the task is to automatically reproduce the relative preference judgments gathered during the human evaluation of the surface realization main task.", "labels": [], "entities": []}, {"text": "As explained further below, developers of realization systems that can generate and optionally rank multiple outputs fora given input will be encouraged to participate in the task, which will test the system's ability to produce acceptable paraphrases and/or to rank competing realizations.", "labels": [], "entities": []}, {"text": "The objectives of the shared task are as follows: broaden participation We expect developers of automatic quality metrics in the MT community to be interested in the proposed task, which is anticipated to be both more focused (with lexical choice largely excluded) and more challenging than in the MT case, given the generally high level of quality in realization results: as realization quality increases, the metrics' task becomes more difficult, since the paraphrases of a given sentence often involve subtle differences between acceptable and unacceptable variation.", "labels": [], "entities": [{"text": "MT community", "start_pos": 129, "end_pos": 141, "type": "TASK", "confidence": 0.8731822371482849}]}, {"text": "In an earlier study of the utility of automatic metrics with Penn Treebank (PTB) surface realization data (, we observed moderate correlations between the most popular metrics and human judgments, though lower than the levels seen with MT data.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) surface realization data", "start_pos": 61, "end_pos": 105, "type": "DATASET", "confidence": 0.9458179697394371}, {"text": "MT data", "start_pos": 236, "end_pos": 243, "type": "DATASET", "confidence": 0.8013637065887451}]}, {"text": "promote reuse of human judgments The task is intended to test the effectiveness of realization ranking models in away that reuses human judgments, making it possible to carryout re-: Additional inputs for the three realization tracks producible system comparisons.", "labels": [], "entities": []}, {"text": "mitigate input conversion issues Realizer evaluations have typically focused on single-best outputs, where the depth and specificity of system inputs has a large impact on quality, making comparative evaluation difficult.", "labels": [], "entities": [{"text": "input conversion", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7343690693378448}, {"text": "Realizer evaluations", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.9309163093566895}]}, {"text": "While the surface realization shared task seeks to address this issue by developing common ground input representations, to date it has proved to be difficult to adapt existing systems to work with these inputs.", "labels": [], "entities": []}, {"text": "By focusing on ranking paraphrases that are distinct from the reference sentence, the proposed task may provide away to mitigate these issues, as discussed below.", "labels": [], "entities": []}], "datasetContent": [{"text": "We propose three tracks for the task, going from pure realization ranking to metrics meta-evaluation, with a hybrid casein the middle.", "labels": [], "entities": []}, {"text": "For all three tracks, the input is a set of pairs of syntactic paraphrases (distinct from the reference sentence), and the output is the preferred member of each pair, where the goal is to match the human judgments of relative preference.", "labels": [], "entities": []}, {"text": "The tracks differ in the additional inputs that systems may use in determining which member of each pair is preferred (see).", "labels": [], "entities": []}, {"text": "In the realization ranking track, the task is to rank order the paraphrases fora given sentence, without having access to the reference sentence, using a realization ranking model.", "labels": [], "entities": []}, {"text": "To do so, each system is allowed to use its own \"native\" inputs derived from the Penn Treebank and PTB-based resources.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9933758974075317}, {"text": "PTB-based", "start_pos": 99, "end_pos": 108, "type": "DATASET", "confidence": 0.5677141547203064}]}, {"text": "To the extent that a system's statistical ranking model can be used to assign a score to any possible realization, the ranking task can be accomplished by simply ranking the realizations by model score.", "labels": [], "entities": []}, {"text": "As such, following this strategy, the task is one of analysis by synthesis.", "labels": [], "entities": []}, {"text": "For non-statistical realizers, or ones that cannot assign a score to any possible realization, there is an alternative strategy available, namely to automatically approximate HTER.", "labels": [], "entities": []}, {"text": "demonstrate that the human-targeted translation edit rate (HTER) represents a reliable and easily interpretable method of evaluating MT output.", "labels": [], "entities": [{"text": "human-targeted translation edit rate (HTER)", "start_pos": 21, "end_pos": 64, "type": "METRIC", "confidence": 0.7286716188703265}, {"text": "MT output", "start_pos": 133, "end_pos": 142, "type": "TASK", "confidence": 0.9135883748531342}]}, {"text": "With this method, a human annotator produces a targeted reference sentence which is as close as possible to the MT hypothesis while being fully acceptable; from the targeted reference, the TER score then represents a normalized post-edit score, which has been shown to correlate with human ratings at least as well as more complex competing metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.9574575424194336}, {"text": "TER score", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9705665409564972}]}, {"text": "As Madnani (2010) points out, generated paraphrases of the reference sentence can be used to approximate HTER scoring, as the closest acceptable paraphrase of a reference sentence should correspond to the version of the MT hypothesis with minimal changes to make it acceptable.", "labels": [], "entities": [{"text": "HTER scoring", "start_pos": 105, "end_pos": 117, "type": "METRIC", "confidence": 0.6754535287618637}, {"text": "MT", "start_pos": 220, "end_pos": 222, "type": "TASK", "confidence": 0.9190617203712463}]}, {"text": "Indeed, in the limit, it should be possible to use a system that can enumerate all and only the acceptable paraphrases of a reference sentence to fully implement HTER scoring.", "labels": [], "entities": [{"text": "HTER scoring", "start_pos": 162, "end_pos": 174, "type": "TASK", "confidence": 0.6397576332092285}]}, {"text": "Naturally, it is possible to combine the analysisby-synthesis and approximating HTER strategies.", "labels": [], "entities": []}, {"text": "One particularly simple way to do so is to (1) use an n-best list of realizations with normalized scores, (2) find the realization with the minimum TER score for each paraphrase to rank, then (3) combine the realizer's model score with the TER score, e.g. just by subtraction (weights for the combination could also be optimized using machine learning).", "labels": [], "entities": [{"text": "TER", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.9838058948516846}, {"text": "TER", "start_pos": 240, "end_pos": 243, "type": "METRIC", "confidence": 0.9924827218055725}]}, {"text": "Regarding the issue of whether fair comparisons can be made when each system is allowed to use its own PTB-derived \"native\" input, note that it is unclear whether using shallow, specific inputs is necessarily advantageous for ranking a range of possible realizations, all distinct from the reference sentence: in the limit, a realizer input that completely specifies the reference sentence (and no other variants) is of no help at all, as in this case the approximating HTER strategy reduces to just doing TER scoring against the reference sentence.", "labels": [], "entities": [{"text": "TER scoring", "start_pos": 506, "end_pos": 517, "type": "METRIC", "confidence": 0.9550442695617676}]}, {"text": "Turning now to the metrics meta-evaluation track, here the the task is to rank order a set of realizations fora given sentence, starting with the reference sentence and nothing else.", "labels": [], "entities": []}, {"text": "In principle, it should be possible to use any MT metric for this task off-the-shelf.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9567309021949768}]}, {"text": "It should also be possible for realization systems to participate in this track, if they can be paired with a parser that produces inputs for the realizer, or a parser whose outputs can be converted to realizer inputs.", "labels": [], "entities": []}, {"text": "To do so, strategies employed in the realization ranking track can be combined with ones that make use of the reference sentence.", "labels": [], "entities": []}, {"text": "Finally, between these two tracks is a hybrid track, where one is allowed to substitute automatic parses with gold parses.", "labels": [], "entities": []}, {"text": "This track can be viewed as providing away to estimate an upper bound on approaches that pay attention to how well a sentence expresses an intended meaning, while also arguably representing the most sensible way to automatically evaluate outputs in a data-to-text setting, where intended meanings can be reliably represented.", "labels": [], "entities": []}, {"text": "In this section, we present two pilot experiments intended to demonstrate the feasibility of the task.", "labels": [], "entities": []}, {"text": "The experiments use the human judgments collected in Espinosa et al.'s (2010) study, which consist of adequacy and fluency ratings from two judges fora variety of realizations for PTB Section 00.", "labels": [], "entities": [{"text": "PTB Section 00", "start_pos": 180, "end_pos": 194, "type": "DATASET", "confidence": 0.7528794805208842}]}, {"text": "The realizations in the corpus were generated using several OpenCCG realization ranking models) and using the XLE symbolic realizer with subsequent n-gram ranking (paraphrases involving WordNet substitutions were excluded).", "labels": [], "entities": []}, {"text": "For comparison purposes, three well-known metrics (BLEU, METEOR and TER) were tested, along with three OpenCCG ranking models: (I) a generative baseline model, incorporating three n-gram models as well as generative model; (II) a model additionally incorporating a slew of discriminative features, extending White & Rajkumar's model with dependency ordering features; and (III) a model adding one additional feature for minimizing dependency length.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9984661340713501}, {"text": "METEOR", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9815686345100403}, {"text": "TER", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9965257048606873}]}, {"text": "Note that Models II and III are very similar, usually yielding the same single-best output, though occasionally differing in important ways; by contrast, both models represent a substantial refinement of Model I. The two experiments investigate different strategies for approaching the hybrid task.", "labels": [], "entities": []}, {"text": "The first experiment investigates the approximating-HTER strategy (with an analysis-by-synthesis component) using a 20-best list.", "labels": [], "entities": []}, {"text": "For simplicity, edit rate (edit distance normalized by the number of words in the reference sentence) was used to find the realization in the 20-best list that was closest to the paraphrase to be ranked.", "labels": [], "entities": [{"text": "edit rate", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9354134500026703}]}, {"text": "The score for the paraphrase was then calculated by normalizing the realizer model score for the closest realization (linearly interpolating using the min and max scores across all 20-best lists), subtracting the edit rate, and adding in the metric score, for each of BLEU, METEOR and TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 268, "end_pos": 272, "type": "METRIC", "confidence": 0.9983417987823486}, {"text": "METEOR", "start_pos": 274, "end_pos": 280, "type": "METRIC", "confidence": 0.9629716873168945}, {"text": "TER", "start_pos": 285, "end_pos": 288, "type": "METRIC", "confidence": 0.9689222574234009}]}, {"text": "Since edit rate is less reliable than TER, as it overly penalizes phrasal shifts, the metric score was used alone in cases where the edit rate exceeded 0.5.", "labels": [], "entities": [{"text": "edit rate", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.8498360812664032}, {"text": "TER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9950810670852661}, {"text": "metric score", "start_pos": 86, "end_pos": 98, "type": "METRIC", "confidence": 0.9726015329360962}]}, {"text": "The results of the first experiment appear in Table 2.", "labels": [], "entities": []}, {"text": "Human judgments were combined by averaging the summed adequacy and fluency ratings from each judge.", "labels": [], "entities": []}, {"text": "Excluding exact match realizations, 2838 pairs of realizations with distinct combined scores (from approximately 250 sentences) were used to judge ranking accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.975898265838623}]}, {"text": "Here, BLEU substantially outperforms METEOR and TER, and combining Models I-III with BLEU does not yield significant differences in ranking accuracy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9983362555503845}, {"text": "METEOR", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9425413608551025}, {"text": "TER", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9933391213417053}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9964892864227295}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.976504921913147}]}, {"text": "Note, however, that using TER scores rather than edit rate, and optimizing the way the model scores are combined with the TER score and BLEU score, could perhaps yield significant improvements.", "labels": [], "entities": [{"text": "TER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.8998719453811646}, {"text": "TER score", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9793536961078644}, {"text": "BLEU score", "start_pos": 136, "end_pos": 146, "type": "METRIC", "confidence": 0.9718208611011505}]}, {"text": "With ME-TEOR and TER, combining the model score, edit rate and metric score in the simplest way does yield highly significant improvements.", "labels": [], "entities": [{"text": "ME-TEOR", "start_pos": 5, "end_pos": 12, "type": "METRIC", "confidence": 0.5907244682312012}, {"text": "TER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9670549631118774}, {"text": "edit rate", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.8741675019264221}, {"text": "metric score", "start_pos": 63, "end_pos": 75, "type": "METRIC", "confidence": 0.9410305321216583}]}, {"text": "With the ME-TEOR combination, Model II achieves a highly significant improvement over Model I, though in other cases, only trends are observed across models.", "labels": [], "entities": []}, {"text": "The second experiment investigates the analysisby-synthesis strategy more directly.", "labels": [], "entities": []}, {"text": "Here, the realizer's search was guided to reproduce each paraphrase where possible, with model scores then calculated where an exact match could be achieved.", "labels": [], "entities": []}, {"text": "The results appear in for 474 pairs with differing combined human judgments.", "labels": [], "entities": []}, {"text": "In the subsequent columns, we see that METEOR and TER are only performing at chance (50%) on these particular ranking cases, while adding the model scores and metric scores does much better, with Model III plus TER performing the best overall, as might have been expected.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9871634244918823}, {"text": "TER", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9921303391456604}]}, {"text": "Even with BLEU, which performs decently on its own, adding in the model scores achieves substantial (and highly significant) gains.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9954913258552551}]}], "tableCaptions": [{"text": " Table 2: Pairwise accuracy percentage on reproducing human judgments of relative adequacy plus fluency of syntactic  paraphrases, using n-best realizations from three OpenCCG ranking models and minimum edit rate in combination  with MT metrics (significance: * for p < 0.1, ** for p < 0.05, *** for p < 0.01 in comparison to MT metric, using  McNemar's test; similarly for number of daggers in comparison to model in previous row)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9850665330886841}, {"text": "McNemar's test", "start_pos": 344, "end_pos": 358, "type": "DATASET", "confidence": 0.8513209621111552}]}, {"text": " Table 3: Pairwise accuracy percentage on reproducing human judgments of relative adequacy plus fluency of syntactic  paraphrases, using exact targeted realizations from three OpenCCG ranking models and minimum edit rate in com- bination with MT metrics (significance: * for p < 0.1, ** for p < 0.05, *** for p < 0.01 in comparison to MT metric,  using McNemar's test; similarly for number of daggers in comparison to model in previous row)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9878147840499878}, {"text": "McNemar's test", "start_pos": 353, "end_pos": 367, "type": "DATASET", "confidence": 0.8494416276613871}]}]}