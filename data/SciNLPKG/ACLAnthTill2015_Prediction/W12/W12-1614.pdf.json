{"title": [{"text": "Improving Implicit Discourse Relation Recognition Through Feature Set Optimization", "labels": [], "entities": [{"text": "Improving Implicit Discourse Relation Recognition", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.9255216360092163}]}], "abstractContent": [{"text": "We provide a systematic study of previously proposed features for implicit discourse relation identification, identifying new feature combinations that optimize F 1-score.", "labels": [], "entities": [{"text": "implicit discourse relation identification", "start_pos": 66, "end_pos": 108, "type": "TASK", "confidence": 0.7104473784565926}]}, {"text": "The resulting classifiers achieve the best F 1-scores to date for the four top-level discourse relation classes of the Penn Discourse Tree Bank: COMPARISON, CONTINGENCY, EXPANSION , and TEMPORAL.", "labels": [], "entities": [{"text": "F 1-scores", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9260866641998291}, {"text": "Penn Discourse Tree Bank", "start_pos": 119, "end_pos": 143, "type": "DATASET", "confidence": 0.9718414545059204}, {"text": "TEMPORAL", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.8801151514053345}]}, {"text": "We further identify factors for feature extraction that can have a major impact on performance and determine that some features originally proposed for the task no longer provide performance gains in light of more powerful, recently discovered features.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7777776122093201}]}, {"text": "Our results constitute anew set of baselines for future studies of implicit discourse relation identification.", "labels": [], "entities": [{"text": "implicit discourse relation identification", "start_pos": 67, "end_pos": 109, "type": "TASK", "confidence": 0.6233970075845718}]}], "introductionContent": [{"text": "The ability to recognize the discourse relations that exist between arbitrary text spans is crucial for understanding a given text.", "labels": [], "entities": []}, {"text": "Indeed, a number of natural language processing (NLP) applications rely on it -e.g., question answering, text summarization, and textual entailment.", "labels": [], "entities": [{"text": "question answering", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.8788985311985016}, {"text": "text summarization", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7495758831501007}, {"text": "textual entailment", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.7269727438688278}]}, {"text": "Fortunately, explicit discourse relations -discourse relations marked by explicit connectives -have been shown to be easily identified by automatic means: each such connective is generally strongly coupled with a particular relation.", "labels": [], "entities": []}, {"text": "The connective \"because\", for example, serves as a prominent cue for the CONTIN-GENCY relation.", "labels": [], "entities": []}, {"text": "The identification of implicit discourse relations -where such connectives are absent -is much harder.", "labels": [], "entities": [{"text": "identification of implicit discourse relations", "start_pos": 4, "end_pos": 50, "type": "TASK", "confidence": 0.7835780382156372}]}, {"text": "It has been the subject of much recent research since the release of the Penn Discourse Treebank 2.0 (PDTB) (, which annotates relations between adjacent text spans in Wall Street Journal (WSJ) articles, while clearly distinguishing implicit from explicit discourse relations.", "labels": [], "entities": [{"text": "Penn Discourse Treebank 2.0 (PDTB)", "start_pos": 73, "end_pos": 107, "type": "DATASET", "confidence": 0.9352745584079197}, {"text": "Wall Street Journal (WSJ) articles", "start_pos": 168, "end_pos": 202, "type": "DATASET", "confidence": 0.8040584581238883}]}, {"text": "Recent studies, for example, explored the utility of various classes of features for the task, including linguistically informed features, context, constituent and dependency parse features, and features that encode entity information or rely on language models (.", "labels": [], "entities": []}, {"text": "To date, however, there has not been a systematic study of combinations of these features for implicit discourse relation identification.", "labels": [], "entities": [{"text": "implicit discourse relation identification", "start_pos": 94, "end_pos": 136, "type": "TASK", "confidence": 0.6410448178648949}]}, {"text": "In addition, the results of existing studies are often difficult to compare because of differences in data set creation, feature set choice, or experimental methodology.", "labels": [], "entities": []}, {"text": "This paper provides a systematic study of previously proposed features for implicit discourse relation identification and identifies feature combinations that optimize F 1 -score using forward selection.", "labels": [], "entities": [{"text": "implicit discourse relation identification", "start_pos": 75, "end_pos": 117, "type": "TASK", "confidence": 0.695056177675724}, {"text": "F 1 -score", "start_pos": 168, "end_pos": 178, "type": "METRIC", "confidence": 0.9536461532115936}]}, {"text": "We report the performance of our binary (one vs. rest) classifiers on the PDTB data set for its four top-level discourse relation classes: COMPARISON, CONTINGENCY, EXPANSION, and TEMPORAL.", "labels": [], "entities": [{"text": "PDTB data set", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9818043311436971}, {"text": "TEMPORAL", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.8792413473129272}]}, {"text": "In each case, the resulting classifiers achieve the best F 1 -scores for the PDTB to date.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.9875111877918243}, {"text": "PDTB", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8822966814041138}]}, {"text": "We Research on implicit discourse relation recognition prior to the release of the PDTB instead relied on synthetic data created by removing explicit connectives from explicit discourse relation instances (), but the trained classifiers do not perform as well on real-world data further identify factors for feature extraction that can have a major impact performance, including stemming and lexicon look-up.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 15, "end_pos": 54, "type": "TASK", "confidence": 0.6097598373889923}, {"text": "feature extraction", "start_pos": 308, "end_pos": 326, "type": "TASK", "confidence": 0.7577244639396667}]}, {"text": "Finally, by documenting an easily replicable experimental methodology and making public the code for feature extraction 2 , we hope to provide anew set of baselines for future studies of implicit discourse relation identification.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7076976150274277}, {"text": "implicit discourse relation identification", "start_pos": 187, "end_pos": 229, "type": "TASK", "confidence": 0.6076156571507454}]}], "datasetContent": [{"text": "We aim to identify the optimal subsets of the aforementioned features for each of the four top-level PDTB discourse relation senses: COMPARISON, CONTINGENCY, EXPANSION, and TEMPORAL.", "labels": [], "entities": [{"text": "TEMPORAL", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9244388937950134}]}, {"text": "In order to provide a meaningful comparison with existing work, we carefully follow the experiment setup of, the origin of the majority of the features under consideration: First, sections 0-2 and 21-22 of the PDTB are used as the validation and test set, respectively.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 210, "end_pos": 214, "type": "DATASET", "confidence": 0.9034091234207153}]}, {"text": "Then, we randomly down-sample sections 2-20 to construct training sets for each of the classifiers, where each set has the same number of positive and negative instances with respect to the target relation.", "labels": [], "entities": []}, {"text": "Since the composition of the corresponding training set has a noticeable impact on the classifier performance we select a down-sampled training set for each classifier through cross validation.", "labels": [], "entities": []}, {"text": "All instances of non-explicit relation senses are used; the ENTREL type is considered as having the EXPAN-SION sense.", "labels": [], "entities": []}, {"text": "Second, Naive Bayes is used not only to duplicate the setting, but also because it equaled or outperformed other learning algorithms, such as SVM and MaxEnt, in preliminary experiments, while requiring a significantly shorter training time.", "labels": [], "entities": []}, {"text": "Prior to the feature selection experiments, the best preprocessing methods for feature extraction are determined through cross validation.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7905395925045013}, {"text": "feature extraction", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.773585855960846}]}, {"text": "We consider simple lowercasing, Porter Stemming, PTB-style tokenization 8 , and hand-crafted rules for matching tokens to entries in the polarity and General Inquirer lexicons.", "labels": [], "entities": [{"text": "General Inquirer lexicons", "start_pos": 150, "end_pos": 175, "type": "DATASET", "confidence": 0.8044587969779968}]}, {"text": "Then, feature selection is performed via forward selection, in which we start with the single bestperforming feature and, in each iteration, add the feature that improves the F 1 -score the most, until no significant improvement can be made.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.6791162490844727}, {"text": "F 1 -score", "start_pos": 175, "end_pos": 185, "type": "METRIC", "confidence": 0.948093593120575}]}, {"text": "Once the 6 Some prior work uses a different experimental setting.", "labels": [], "entities": []}, {"text": "For instance, only considers two of the nonexplicit relations, namely Implicit and NoRel.", "labels": [], "entities": []}, {"text": "We use classifiers from the nltk package).", "labels": [], "entities": []}, {"text": "optimal feature set for each relation sense is determined by testing on the validation set, we retrain each classifier using the entire training set and report final performance on the test set.", "labels": [], "entities": []}, {"text": "indicates the performance achieved by employing the feature set found to be optimal for each relation sense via forward selection, along with the performance of the individual features that constitute the ideal subset.", "labels": [], "entities": []}, {"text": "The two bottom rows show the results reported in two previous papers with the most similar experiment methodology as ours.", "labels": [], "entities": []}, {"text": "The notable efficacy of the production rules feature, yielding the best or the second best result across all relation senses w.r.t. both F 1 -score and accuracy, confirms the finding of.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.9822311848402023}, {"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9978306889533997}]}, {"text": "In contrast to their work, however, combining existing features enhances the performance.", "labels": [], "entities": []}, {"text": "Below, we discuss the primary observations gleaned from the experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of Classifier Performance. 4-way classifiers have been tested as well, but their performance is not  as good as that of the binary classifiers shown here. One major difference is that it is harder to balance the number of  instances across all the classes when training 4-way classifiers.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9475284814834595}]}]}