{"title": [{"text": "Statistical Input Method based on a Phrase Class n-gram Model", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a method to construct a phrase class n-gram model for Kana-Kanji Conversion by combining phrase and class methods.", "labels": [], "entities": [{"text": "Kana-Kanji Conversion", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.7510927021503448}]}, {"text": "We use a word-pronunciation pair as the basic prediction unit of the language model.", "labels": [], "entities": []}, {"text": "We compared the conversion accuracy and model size of a phrase class bi-gram model constructed by our method to a tri-gram model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9507694244384766}]}, {"text": "The conversion accuracy was measured by F measure and model size was measured by the vocabulary size and the number of non-zero frequency entries.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9249654412269592}, {"text": "F measure", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9885597229003906}]}, {"text": "The F measure of our phrase class bi-gram model was 90.41%, while that of a word-pronunciation pair tri-gram model was 90.21%.", "labels": [], "entities": [{"text": "F measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9875939190387726}]}, {"text": "In addition, the vocabulary size and the number of non-zero frequency entries in the phrase class bi-gram model were 5,550 and 206,978 respectively, while those of the tri-gram model were 22,801 and 645,996 respectively.", "labels": [], "entities": []}, {"text": "Thus our method makes a smaller, more accurate language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Japanese input methods are an essential technology for Japanese computing.", "labels": [], "entities": []}, {"text": "Japanese has over 6,000 characters, which is much larger than the number of keys in a keyboard.", "labels": [], "entities": []}, {"text": "It is impossible to map each Japanese character to a key.", "labels": [], "entities": []}, {"text": "So an alternate input method for Japanese characters is needed.", "labels": [], "entities": []}, {"text": "This is done by inputting a pronunciation sequence and converting it to an output sequence of words.", "labels": [], "entities": []}, {"text": "Here, the input sequence of pronunciation is a sequence of kana characters and the output sequence of words is a mixture of kana and kanji characters.", "labels": [], "entities": []}, {"text": "So this conversion is called Kana-Kanji Conversion (KKC).", "labels": [], "entities": [{"text": "Kana-Kanji Conversion", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7653858661651611}]}, {"text": "The noisy channel model approach has been successfully applied to input methods).", "labels": [], "entities": []}, {"text": "In KKC, a word sequence is predicted from a pronunciation sequence.", "labels": [], "entities": []}, {"text": "The system is composed of two modules: a language model, which measures the likelihood of a word sequence in the language and a word-pronunciation model, which describes a relationship between a word sequence and a pronunciation sequence.", "labels": [], "entities": []}, {"text": "Thus, the conversion accuracy of KKC depends on the language model and the word-pronunciation model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9053153991699219}]}, {"text": "The language model is, however, more important since it describes the context and it is larger in size than the word-pronunciation model.", "labels": [], "entities": []}, {"text": "We focus on how to improve the language model for KKC.", "labels": [], "entities": []}, {"text": "An n-gram model is generally used for many tasks.", "labels": [], "entities": []}, {"text": "In KKC, considering the need for the conversion speed and the size of the language model, bi-gram models are often used.", "labels": [], "entities": []}, {"text": "However, bi-gram models cannot refer to along history.", "labels": [], "entities": []}, {"text": "A tri-gram model, which is also popular for many tasks, can refer a longer history but the size of tri-gram models is larger than that of bi-gram models.", "labels": [], "entities": []}, {"text": "There have been many attempts at improving language models.", "labels": [], "entities": []}, {"text": "A class n-gram model (, which groups words of similar behavior into a single class, and a phrase n-gram model) which replaces some word sequences by single tokens, are known to be practical in speech recognition community.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.7904167771339417}]}, {"text": "In this paper, we propose a method to construct a smaller, more accurate language model for KKC.", "labels": [], "entities": []}, {"text": "It is often thought that accurate models are larger and that small models are less accurate.", "labels": [], "entities": []}, {"text": "However, we successfully built a smaller, more accurate language model.", "labels": [], "entities": []}, {"text": "This is done by combining phrase and class methods.", "labels": [], "entities": []}, {"text": "First, we collect phrases and construct a phrase sequence corpus.", "labels": [], "entities": []}, {"text": "By changing the prediction unit from a word to a word sequence, the model can use a longer history.", "labels": [], "entities": []}, {"text": "Then we perform word clustering to restrict the growth of the model size.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7611164450645447}]}, {"text": "As a result, we obtain a phrase class n-gram model.", "labels": [], "entities": []}, {"text": "This model is small and expected to be accurate because it uses a history as long as those used by higher order n-gram models.", "labels": [], "entities": []}, {"text": "In order to test the effectiveness of our method, we compared the conversion accuracy and the model size of the phrase class bi-gram model constructed by our method to other language models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.880441427230835}]}, {"text": "We used a word-pronunciation pair as the basic prediction unit of the language models.", "labels": [], "entities": []}, {"text": "The conversion accuracy is measured by F measure, which is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9712771773338318}, {"text": "F measure", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9930562674999237}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9994544386863708}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9959483742713928}]}, {"text": "The F measure of our phrase class bi-gram model was 90.41%, while that of a word-pronunciation pair tri-gram model was 90.21%.", "labels": [], "entities": [{"text": "F measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9875939190387726}]}, {"text": "In addition, the vocabulary size and the number of non-zero frequency entries in the phrase class bi-gram model were 5,550 and 206,978 respectively, while those of the tri-gram model were 22,801 and 645,996 respectively.", "labels": [], "entities": []}, {"text": "These results show that our method of combining phrase and class methods makes a smaller, more accurate language model for KKC.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to test the effectiveness of combining the phrase and class methods, we constructed phrase class bi-gram using our method and compared the conversion accuracy and the model size of our model to other language models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.8964087963104248}]}, {"text": "In this section we show the experimental results and discuss them.", "labels": [], "entities": []}, {"text": "We used the core corpus of the Balanced Corpus of Contemporary Written Japanese (BCCWJ) for the experiments.", "labels": [], "entities": [{"text": "Balanced Corpus of Contemporary Written Japanese (BCCWJ)", "start_pos": 31, "end_pos": 87, "type": "DATASET", "confidence": 0.8078726165824466}]}, {"text": "The BCCWJ is composed of two corpora, the BCCWJ-CORE corpus and the BCCWJ-NON-CORE corpus, and we used the BCCWJ-CORE.", "labels": [], "entities": [{"text": "BCCWJ", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.9703996181488037}, {"text": "BCCWJ-CORE corpus", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.9693847894668579}, {"text": "BCCWJ-NON-CORE corpus", "start_pos": 68, "end_pos": 89, "type": "DATASET", "confidence": 0.9624371826648712}, {"text": "BCCWJ-CORE", "start_pos": 107, "end_pos": 117, "type": "DATASET", "confidence": 0.9806837439537048}]}, {"text": "A sentence of the BCCWJ-CORE corpus is a sequence of pairs of a word and its pronunciation as shown in.", "labels": [], "entities": [{"text": "BCCWJ-CORE corpus", "start_pos": 18, "end_pos": 35, "type": "DATASET", "confidence": 0.960334450006485}]}, {"text": "The word segmentation and pronunciation tagging were done manually.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7477068901062012}, {"text": "pronunciation tagging", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.8239730894565582}]}, {"text": "We split the BCCWJ-CORE into two parts: one is for training language models and the other is for the test of them.", "labels": [], "entities": [{"text": "BCCWJ-CORE", "start_pos": 13, "end_pos": 23, "type": "DATASET", "confidence": 0.9225531816482544}]}, {"text": "shows the specifications of the corpora.", "labels": [], "entities": []}, {"text": "We constructed language models and a vocabulary from the training corpus of the BCCWJ.", "labels": [], "entities": [{"text": "BCCWJ", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.9092897176742554}]}, {"text": "The vocabulary was constructed according to) and ().", "labels": [], "entities": []}, {"text": "The following is a list of language models we compared.", "labels": [], "entities": []}, {"text": "We performed another more practical experiment.", "labels": [], "entities": []}, {"text": "In this experiment, we also constructed a phrase class bi-gram model and a word-pronunciation pair tri-gram model.", "labels": [], "entities": []}, {"text": "We used the training corpus that has about 360,000 sentences mainly from BCCWJ-NON-CORE.", "labels": [], "entities": [{"text": "BCCWJ-NON-CORE", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.9749532341957092}]}, {"text": "The results are in table 4 and 5.", "labels": [], "entities": []}, {"text": "We see that the phrase class bi-gram model is smaller and it has the comparable conversion accuracy as the word-pronunciation pair tri-gram model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.8551937937736511}]}], "tableCaptions": []}