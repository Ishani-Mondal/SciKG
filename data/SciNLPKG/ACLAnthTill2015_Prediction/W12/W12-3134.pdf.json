{"title": [{"text": "Joshua 4.0: Packing, PRO, and Paraphrases", "labels": [], "entities": [{"text": "PRO", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9026853442192078}, {"text": "Paraphrases", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.5086631774902344}]}], "abstractContent": [{"text": "We present Joshua 4.0, the newest version of our open-source decoder for parsing-based statistical machine translation.", "labels": [], "entities": [{"text": "parsing-based statistical machine translation", "start_pos": 73, "end_pos": 118, "type": "TASK", "confidence": 0.8804282993078232}]}, {"text": "The main contributions in this release are the introduction of a compact grammar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization , J-PRO.", "labels": [], "entities": []}, {"text": "We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases.", "labels": [], "entities": [{"text": "Thrax SCFG grammar extractor", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6275457814335823}, {"text": "pivot-based extraction of syntactically informed sentential paraphrases", "start_pos": 72, "end_pos": 143, "type": "TASK", "confidence": 0.7960441878863743}]}], "introductionContent": [{"text": "Joshua is an open-source toolkit 1 for parsing-based statistical machine translation of human languages.", "labels": [], "entities": [{"text": "parsing-based statistical machine translation of human languages", "start_pos": 39, "end_pos": 103, "type": "TASK", "confidence": 0.8995628782681057}]}, {"text": "The original version of Joshua () was a reimplementation of the Python-based Hiero machine translation system.", "labels": [], "entities": [{"text": "Hiero machine translation", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.7514676650365194}]}, {"text": "It was later extended to support grammars with rich syntactic labels ().", "labels": [], "entities": []}, {"text": "More recent efforts introduced the Thrax module, an extensible Hadoopbased extraction toolkit for synchronous contextfree grammars.", "labels": [], "entities": []}, {"text": "In this paper we describe a set of recent extensions to the Joshua system.", "labels": [], "entities": []}, {"text": "We present anew compact grammar representation format that leverages sparse features, quantization, and data redundancies to store grammars in a dense binary format.", "labels": [], "entities": []}, {"text": "This allows for both near-instantaneous start-up times and decoding with extremely large grammars.", "labels": [], "entities": []}, {"text": "In Section 2 we outline our packed grammar format and 1 joshua-decoder.org present experimental results regarding its impact on decoding speed, memory use and translation quality.", "labels": [], "entities": []}, {"text": "Additionally, we present Joshua's implementation of the pairwise ranking optimization) approach to translation model tuning.", "labels": [], "entities": [{"text": "translation model tuning", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.9391086300214132}]}, {"text": "J-PRO, like Z-MERT, makes it easy to implement new metrics and comes with both a built-in perceptron classifier and out-of-the-box support for widely used binary classifiers such as.", "labels": [], "entities": []}, {"text": "We describe our implementation in Section 3, presenting experimental results on performance, classifier convergence, and tuning speed.", "labels": [], "entities": []}, {"text": "Finally, we introduce the inclusion of bilingual pivoting-based paraphrase extraction into Thrax, Joshua's grammar extractor.", "labels": [], "entities": [{"text": "bilingual pivoting-based paraphrase extraction", "start_pos": 39, "end_pos": 85, "type": "TASK", "confidence": 0.6019739881157875}]}, {"text": "Thrax's paraphrase extraction mode is simple to use, and yields state-ofthe-art syntactically informed sentential paraphrases ( ).", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.8039346039295197}]}, {"text": "The full feature set of) is supported for paraphrase grammars.", "labels": [], "entities": [{"text": "paraphrase grammars", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8733876645565033}]}, {"text": "An easily configured feature-level pruning mechanism allows to keep the paraphrase grammar size manageable.", "labels": [], "entities": []}, {"text": "Section 4 presents details on our paraphrase extraction module.", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.9158487617969513}]}], "datasetContent": [{"text": "We did our experiments using J-PRO on the NIST Chinese-English data, and BLEU score was used as the quality metric for experiments reported in this section.", "labels": [], "entities": [{"text": "NIST Chinese-English data", "start_pos": 42, "end_pos": 67, "type": "DATASET", "confidence": 0.9589400688807169}, {"text": "BLEU score", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9833793044090271}]}, {"text": "The experimental settings are as the following: Datasets: MT03 dataset (998 sentences) as development set for parameter tuning, MT04 (1788 sentences) and MT05 (1082 sentences) as test sets.", "labels": [], "entities": [{"text": "MT03 dataset", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9689111411571503}, {"text": "MT04", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.9372929930686951}, {"text": "MT05", "start_pos": 154, "end_pos": 158, "type": "DATASET", "confidence": 0.9249242544174194}]}, {"text": "Features: Dense feature set include the 10 regular features used in the Hiero system; Sparse feature set We also experimented with other metrics including TER, METEOR and TER-BLEU.", "labels": [], "entities": [{"text": "Hiero system", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.9365665316581726}, {"text": "Sparse", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9144655466079712}, {"text": "TER", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.9969101548194885}, {"text": "METEOR", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9883600473403931}, {"text": "TER-BLEU", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9702240824699402}]}, {"text": "Similar trends as reported in this section were observed.", "labels": [], "entities": []}, {"text": "These results are omitted here due to limited space.", "labels": [], "entities": []}, {"text": "includes 1016 target-side rule POS bi-gram features as used in ().", "labels": [], "entities": []}, {"text": "Classifiers: Perceptron, MegaM and Maximum entropy.", "labels": [], "entities": []}, {"text": "PRO parameters: \u0393 = 8000 (number of candidate pairs sampled uniformly from the n-best list), \u03b1 = 1 (sample acceptance probability), \u039e = 50 (number of top candidates to be added to the training set).", "labels": [], "entities": [{"text": "PRO", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.49715444445610046}]}, {"text": "shows the BLEU score curves on the development and test sets as a function of iterations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9738323092460632}]}, {"text": "The upper and lower rows correspond to the results trained with 10 dense features and 1026 dense+sparse features respectively.", "labels": [], "entities": []}, {"text": "We intentionally selected very bad initial parameter vectors to verify the robustness of the algorithm.", "labels": [], "entities": []}, {"text": "It can be seen that with each iteration, the BLEU score increases monotonically on both development and test sets, and begins to converge after a few iterations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9864439070224762}]}, {"text": "When only 10 features are involved, all classifiers give almost the same performance.", "labels": [], "entities": []}, {"text": "However, when scaled to over a thousand features, the maximum entropy classifier becomes unstable and the curve fluctuates significantly.", "labels": [], "entities": []}, {"text": "In this situation MegaM behaves well, but the J-PRO built-in perceptron gives the most robust performance.", "labels": [], "entities": []}, {"text": "compares the results of running Z-MERT and J-PRO.", "labels": [], "entities": []}, {"text": "Since MERT is notable to handle numerous sparse features, we only report results for the 10-feature setup.", "labels": [], "entities": []}, {"text": "The scores for both setups are quite close to each other, with Z-MERT doing slightly better on the development set but J-PRO yielding slightly better performance on the test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Decoding-time memory use for the packed  grammar versus the standard grammar format. Even  without lossy quantization the packed grammar rep- resentation yields significant savings in memory  consumption. Adding 8-bit quantization for the real- valued features in the grammar reduces even large  syntactic grammars to a manageable size.", "labels": [], "entities": []}, {"text": " Table 2: Comparison between the results given by Z-MERT and J-PRO (trained with 10 features).", "labels": [], "entities": []}, {"text": " Table 5: Large paraphrase grammars extracted from EuroParl data using Thrax. The sentence and word  counts refer to the English side of the bitexts used.", "labels": [], "entities": [{"text": "EuroParl data", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9916855096817017}]}]}