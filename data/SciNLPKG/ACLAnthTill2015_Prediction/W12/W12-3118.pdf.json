{"title": [{"text": "The SDL Language Weaver Systems in the WMT12 Quality Estimation Shared Task", "labels": [], "entities": [{"text": "SDL Language Weaver", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8490015665690104}, {"text": "WMT12 Quality Estimation Shared", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.7509937882423401}]}], "abstractContent": [{"text": "We present in this paper the system submissions of the SDL Language Weaver team in the WMT 2012 Quality Estimation shared-task.", "labels": [], "entities": [{"text": "SDL Language Weaver", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7597425977389017}, {"text": "WMT 2012 Quality Estimation shared-task", "start_pos": 87, "end_pos": 126, "type": "TASK", "confidence": 0.6328608751296997}]}, {"text": "Our MT quality-prediction systems use machine learning techniques (M5P regression-tree and SVM-regression models) and a feature-selection algorithm that has been designed to directly optimize towards the official metrics used in this shared-task.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9829660654067993}]}, {"text": "The resulting submissions placed 1st (the M5P model) and 2nd (the SVM model), respectively , on both the Ranking task and the Scoring task, out of 11 participating teams.", "labels": [], "entities": [{"text": "Scoring task", "start_pos": 126, "end_pos": 138, "type": "TASK", "confidence": 0.8622211515903473}]}], "introductionContent": [{"text": "The WMT 2012 Quality Estimation shared-task focused on automatic methods for estimating machine translation output quality at run-time (sentence-level estimation).", "labels": [], "entities": [{"text": "WMT 2012 Quality Estimation shared-task", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.6434517562389374}, {"text": "estimating machine translation output", "start_pos": 77, "end_pos": 114, "type": "TASK", "confidence": 0.7442952767014503}]}, {"text": "Different from MT evaluation metrics, quality prediction (QP) systems do not rely on reference translations and are generally built using machine learning techniques to estimate quality scores (.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.91573166847229}, {"text": "quality prediction (QP)", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.8376265406608582}]}, {"text": "Some interesting uses of sentence-level MT quality prediction are the following: decide whether a given translation is good enough for publishing asis, or inform monolingual (target-language) readers whether or not they can rely on a translation; filter out sentences that are not good enough for post-editing by professional translators; select the best translation among options from multiple MT systems, etc.", "labels": [], "entities": [{"text": "MT quality prediction", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.9107054670651754}]}, {"text": "This shared-task focused on estimating the quality of English to Spanish automatic translations.", "labels": [], "entities": [{"text": "English to Spanish automatic translations", "start_pos": 54, "end_pos": 95, "type": "TASK", "confidence": 0.6142965793609619}]}, {"text": "The training set distributed for the shared task comprised of 1, 832 English sentences taken from the news domain and their Spanish translations.", "labels": [], "entities": []}, {"text": "The translations were produced by the Moses SMT system () trained on Europarl data.", "labels": [], "entities": [{"text": "translations", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9602717161178589}, {"text": "Moses SMT system", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.8232990503311157}, {"text": "Europarl data", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.9933409988880157}]}, {"text": "Translations also had a quality score derived from an average of three human judgements of Post-Editing effort using a 1-5 scale (1 for worse-quality/most-effort, and 5 for best-quality/least-effort).", "labels": [], "entities": [{"text": "Translations", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9405987858772278}]}, {"text": "Submissions were evaluated using a blind official test set of 422 sentences produced in the same fashion as the training set.", "labels": [], "entities": []}, {"text": "Two sub-tasks were considered: (i) scoring translations using the 1-5 quality scores (Scoring), and (ii) ranking translations from best to worse (Ranking).", "labels": [], "entities": []}, {"text": "The official metrics used for the Ranking task were DeltaAvg (measuring how valuable a proposed ranking is from the perspective of extrinsic values associated with the test entries, in this case post-editing effort on a 1-5 scale; for instance, a DeltaAvg of 0.5 means that the top-ranked quantiles have +0.5 better quality on average compared to the entire set), as well as the Spearman ranking correlation.", "labels": [], "entities": [{"text": "DeltaAvg", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9906514883041382}]}, {"text": "For the Scoring task the metrics were Mean-Absolute-Error (MAE) and Root Mean Squared Error (RMSE).", "labels": [], "entities": [{"text": "Scoring task", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.9184673428535461}, {"text": "Mean-Absolute-Error (MAE)", "start_pos": 38, "end_pos": 63, "type": "METRIC", "confidence": 0.9662192016839981}, {"text": "Root Mean Squared Error (RMSE)", "start_pos": 68, "end_pos": 98, "type": "METRIC", "confidence": 0.896002641745976}]}, {"text": "The interested reader is referred to) for detailed descriptions of both the data and the evaluation metrics used in the shared-task.", "labels": [], "entities": []}, {"text": "The SDL Language Weaver team participated with two submissions based on M5P and SVM regression models in both the Ranking and the Scoring tasks.", "labels": [], "entities": [{"text": "SDL Language Weaver", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7014344533284506}]}, {"text": "The models were trained and used to predict Post-Editing-effort scores.", "labels": [], "entities": []}, {"text": "These scores were used as-such for the Scoring task, and also used to generate sentence rankings for the Ranking task by simply (reverse) sorting the predicted scores.", "labels": [], "entities": [{"text": "Scoring task", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9266741871833801}]}, {"text": "The submissions of the SDL Language Weaver team placed 1st (the M5P model) and 2nd (the SVM model) on both the Ranking task (out of 17 entries) and the Scoring task (out of 19 entries).", "labels": [], "entities": [{"text": "SDL Language Weaver", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7899110714594523}, {"text": "Scoring task", "start_pos": 152, "end_pos": 164, "type": "TASK", "confidence": 0.9060086011886597}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance of the Baseline Features using M5P and SVR models on the test set.", "labels": [], "entities": []}, {"text": " Table 2: Performance of the Moses-based Features with an M5P model on the test set.", "labels": [], "entities": []}, {"text": " Table 3: M5P-model performance for different feature-function sets (15-FFs \u2208 42-FFs; 14-FFs \u2208 42-FFs).", "labels": [], "entities": []}, {"text": " Table 5: SVR-model performance for dev and test sets.", "labels": [], "entities": []}]}