{"title": [], "abstractContent": [], "introductionContent": [{"text": "Data-driven tools for syntactic parsing have been successfully developed and applied to a large number of languages.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.8985085487365723}]}, {"text": "The existing data-driven syntactic parsers are based on phrase structure, dependency structure, or specific linguistic theories such as HPSG, LFG or CCG.", "labels": [], "entities": []}, {"text": "Dependencybased representations have become more widely used in the recent decade, as the approach seems better suited than phrase structure representations for languages with free or flexible word order (.", "labels": [], "entities": []}, {"text": "Dependency parsing, in addition, has been shown to be useful in language technology applications, such as machine translation and information extraction, when detecting the underlying syntactic pattern of a sentence, because of their transparent encoding of predicate-argument structure ().", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7986350953578949}, {"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.8202885389328003}, {"text": "information extraction", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.7700429260730743}, {"text": "detecting the underlying syntactic pattern of a sentence", "start_pos": 159, "end_pos": 215, "type": "TASK", "confidence": 0.7316390834748745}]}, {"text": "This paper presents the adaptation and evaluation of two dependency parsers,) and MSTParser () for Persian.", "labels": [], "entities": []}, {"text": "The parsers are trained on a syntactically annotated corpus developed for Persian; the Uppsala PErsian Dependency Treebank (UPEDT) ().", "labels": [], "entities": [{"text": "Uppsala PErsian Dependency Treebank (UPEDT)", "start_pos": 87, "end_pos": 130, "type": "DATASET", "confidence": 0.9231517655508858}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly describes the structure and the characteristics of the Persian language, followed by a description of the dependency structure and the functional annotation of the Persian dependency treebank on which the data-driven parsers are trained.", "labels": [], "entities": []}, {"text": "A short description of MaltParser and MSTParser ends the section.", "labels": [], "entities": [{"text": "MaltParser", "start_pos": 23, "end_pos": 33, "type": "DATASET", "confidence": 0.9009204506874084}, {"text": "MSTParser", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.937124490737915}]}, {"text": "Section 3 introduces the design of our experiments and Section 4 presents the results of the evaluation covering the results of MaltParser and MSTParser, as well as an error analysis for the developed parsers.", "labels": [], "entities": [{"text": "MaltParser", "start_pos": 128, "end_pos": 138, "type": "DATASET", "confidence": 0.9158439040184021}, {"text": "MSTParser", "start_pos": 143, "end_pos": 152, "type": "DATASET", "confidence": 0.9078589677810669}]}, {"text": "Finally, Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Labeled and unlabeled attachment score, and label accuracy including punctuation of  MSTParser in the model selection with different feature settings (for explanation of the features  see Table2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.8076527714729309}, {"text": "MSTParser", "start_pos": 95, "end_pos": 104, "type": "DATASET", "confidence": 0.8087937235832214}]}, {"text": " Table 4: Labeled and unlabeled attachment score, and label accuracy score including punctua- tion in the model assessment with different feature settings.", "labels": [], "entities": [{"text": "label accuracy score", "start_pos": 54, "end_pos": 74, "type": "METRIC", "confidence": 0.812116821606954}]}, {"text": " Table 5: Labeled and unlabeled attachment score, and label accuracy score ignoring punctuation  in the model assessment with different feature settings.", "labels": [], "entities": [{"text": "label accuracy score", "start_pos": 54, "end_pos": 74, "type": "METRIC", "confidence": 0.7872987985610962}]}]}