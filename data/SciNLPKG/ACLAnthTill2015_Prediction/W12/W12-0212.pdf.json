{"title": [{"text": "Estimating and visualizing language similarities using weighted alignment and force-directed graph layout", "labels": [], "entities": []}], "abstractContent": [{"text": "The paper reports several studies about quantifying language similarity via pho-netic alignment of core vocabulary items (taken from Wichman's Automated Similarity Judgement Program data base).", "labels": [], "entities": [{"text": "Automated Similarity Judgement Program data base", "start_pos": 143, "end_pos": 191, "type": "TASK", "confidence": 0.7562522937854131}]}, {"text": "It turns out that weighted alignment according to the Needleman-Wunsch algorithm yields best results.", "labels": [], "entities": []}, {"text": "For visualization and data exploration purposes , we used an implementation of the Fruchterman-Reingold algorithm, aversion of force directed graph layout.", "labels": [], "entities": [{"text": "data exploration", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7675182819366455}]}, {"text": "This software projects large amounts of data points to a two-or three-dimensional structure in such away that groups of mutually similar items form spatial clusters.", "labels": [], "entities": []}, {"text": "The exploratory studies conducted along these ways lead to suggestive results that provide evidence for historical relationships beyond the traditionally recognized language families.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Automated Similarity Judgment Program () is a collection of 40-item Swadesh lists from more than 5,000 languages.", "labels": [], "entities": [{"text": "Similarity Judgment", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7671213150024414}]}, {"text": "The vocabulary items are all given in a uniform, if coarse-grained, phonetic transcription.", "labels": [], "entities": []}, {"text": "In this project, we explore various ways to compute the pairwise similarities of these languages based on sequence alignment of translation pairs.", "labels": [], "entities": []}, {"text": "As the 40 concepts that are covered in the database are usually thought to be resistant against borrowing, these similarities provide information about genetic relationships between languages.", "labels": [], "entities": []}, {"text": "To visualize and explore the emerging patterns, we make use of Force Directed Graph Layout.", "labels": [], "entities": []}, {"text": "More specifically, we use the CLANS 1 implementation of the Fruchterman-Reingold algorithm (.", "labels": [], "entities": []}, {"text": "This algorithm takes a similarity matrix as input.", "labels": [], "entities": []}, {"text": "Each data point is treated as a physical particle.", "labels": [], "entities": []}, {"text": "There is a repelling force between any two particles -you may think of the particles as electrically charged with the same polarity.", "labels": [], "entities": []}, {"text": "Similarities are treated as attracting forces, with a strength that is positively related to the similarity between the corresponding data points.", "labels": [], "entities": []}, {"text": "All data points are arranged in a two-or threedimensional space.", "labels": [], "entities": []}, {"text": "The algorithm simulates the movement of the particles along the resulting force vector and will eventually converge towards an energy minimum.", "labels": [], "entities": []}, {"text": "In the final state, groups of mutually similar data items form spatial clusters, and the distance between such clusters provides information about their cumulative similarity.", "labels": [], "entities": []}, {"text": "This approach has proven useful in bioinformatics, for instance to study the evolutionary history of protein sequences.", "labels": [], "entities": []}, {"text": "Unlike more commonly used methods like SplitsTree (or other phylogenetic tree algorithms), CLANS does not assume an underlying tree structure; neither does it compute a hypothetical phylogenetic tree or network.", "labels": [], "entities": []}, {"text": "The authors of this software package, Tancred Frickey and Andrei Lupas, argue that this approach is advantageous especially in situations were a large amount of low-quality data are available: \"An alternative approach is the visualization of all-against-all pairwise similarities.", "labels": [], "entities": []}, {"text": "This method can handle unrefined, unaligned data, including non-homologous sequences.", "labels": [], "entities": []}, {"text": "Unlike phylogenetic reconstruction it becomes more accurate with an increasing number of sequences, as the larger number of pairwise relationships average out the spurious matches that are the crux of simpler pairwise similaritybased analyses.\"", "labels": [], "entities": [{"text": "phylogenetic reconstruction", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.8689875602722168}]}, {"text": "This paper investigates two issues, that are related to the two topics of the workshop respectively: \u2022 Which similarity measures over language pairs based on the ASJP data are apt to supply information about genetic relationships between languages?", "labels": [], "entities": [{"text": "ASJP data", "start_pos": 162, "end_pos": 171, "type": "DATASET", "confidence": 0.7715681493282318}]}, {"text": "\u2022 What are the advantages and disadvantages of a visualization method such as CLANS, as compared to the more commonly used phylogenetic tree algorithms, when applied to large scale language comparison?", "labels": [], "entities": []}, {"text": "2 Comparing similarity measures 2.1 The LDND distance measure Ina distance measure is defined that is based on the Levenshtein distance (= edit distance) between words from the two languages to be compared.", "labels": [], "entities": [{"text": "LDND distance measure Ina distance measure", "start_pos": 40, "end_pos": 82, "type": "METRIC", "confidence": 0.9106019735336304}, {"text": "Levenshtein distance (= edit distance)", "start_pos": 115, "end_pos": 153, "type": "METRIC", "confidence": 0.7957014540831248}]}, {"text": "Suppose two languages, L1 and L2, are to be compared.", "labels": [], "entities": []}, {"text": "Ina first step, the normalized Levenshtein distances between all word pairs from L1 and L2 are computed.", "labels": [], "entities": []}, {"text": "(Ideally this should be 40 word pairs, but some data are missing in the data base.)", "labels": [], "entities": []}, {"text": "This measure is defined as The normalization term ensures that word length does not affect the distance measure.", "labels": [], "entities": []}, {"text": "If L1 and L2 have small sound inventories with a large overlap (which is frequently the case for tonal languages), the distances between words from L1 and L2 will below for non-cognates because of the high probability of chance similarities.", "labels": [], "entities": []}, {"text": "If L1 and L2 have large sound inventories with little overlap, the distance between non-cognates will below in comparison.", "labels": [], "entities": []}, {"text": "To correct for this effect, normalize the distance between two synonymous words from L1 and L2 by defining the normalized Levenshtein distance by the average distance between all words from L1 and L2 that are non synonymous (39 \u00d7 40 = 1, 560 pairs if no data are missing).", "labels": [], "entities": []}, {"text": "The NDLD distance between L1 and L2 is defined as the average doubly normalized Levenshtein distance between synonymous word pairs from L1 and L2.", "labels": [], "entities": []}, {"text": "(LDND is a distance measure rather than a similarity measure, but it is straightforward to transform the one type of measure into the other.)", "labels": [], "entities": []}, {"text": "In the remainder of this section, I will propose an improvement of LDND in two aspects: \u2022 using weighted sequence alignment based on phonetic similarity, and \u2022 correcting for the variance of alignments using an information theoretic distance measure.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}