{"title": [{"text": "A Tagging-style Reordering Model for Phrase-based SMT Human Language Technology and Pattern Recognition Group", "labels": [], "entities": [{"text": "Tagging-style Reordering", "start_pos": 2, "end_pos": 26, "type": "TASK", "confidence": 0.9187331795692444}, {"text": "SMT Human Language Technology", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.8148652613162994}, {"text": "Pattern Recognition", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7527188956737518}]}], "abstractContent": [{"text": "For current statistical machine translation system, reordering is still a major problem for language pairs like Chinese-English, where the source and target language have significant word order differences.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.6237301727135977}]}, {"text": "In this paper we propose a novel tagging-style reordering model.", "labels": [], "entities": []}, {"text": "Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task.", "labels": [], "entities": [{"text": "sequence labeling problem", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.7273815274238586}, {"text": "tagging task", "start_pos": 83, "end_pos": 95, "type": "TASK", "confidence": 0.9062084555625916}]}, {"text": "For the given source sentence, we assign each source token a label which contains the reordering information for that token.", "labels": [], "entities": []}, {"text": "We also design an unaligned word tag so that the unaligned word phenomenon is automatically covered in the proposed model.", "labels": [], "entities": []}, {"text": "Our reordering model is conditioned on the whole source sentence.", "labels": [], "entities": []}, {"text": "Hence it is able to catch long dependencies in the source sentence.", "labels": [], "entities": []}, {"text": "The decoder makes use of the tagging information as soft constraints so that in the test phase (during translation) our model is very efficient.", "labels": [], "entities": []}, {"text": "The model training on large scale tasks requests notably amounts of computational resources.", "labels": [], "entities": []}, {"text": "We carried out experiments on five Chinese-English NIST tasks trained with BOLT data.", "labels": [], "entities": [{"text": "BOLT", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9175223708152771}]}, {"text": "Results show that our model improves the baseline system by 0.98 BLEU 1.21 TER on average.", "labels": [], "entities": [{"text": "BLEU 1.21", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9710640907287598}, {"text": "TER", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.653708279132843}]}], "introductionContent": [{"text": "The systematic word order difference between two languages pose a challenge for current statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 88, "end_pos": 125, "type": "TASK", "confidence": 0.7776002436876297}]}, {"text": "The system has to decide in which order to translate the given source words.", "labels": [], "entities": []}, {"text": "This problem is known as the reordering problem.", "labels": [], "entities": []}, {"text": "As shown in, if arbitrary reordering is allowed, the search problem is NP-hard.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel tagging style reordering model.", "labels": [], "entities": []}, {"text": "Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task.", "labels": [], "entities": [{"text": "sequence labeling problem", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.7273815274238586}, {"text": "tagging task", "start_pos": 83, "end_pos": 95, "type": "TASK", "confidence": 0.9062084555625916}]}, {"text": "For a given source sentence, we assign each source token a label which contains the reordering information for that token.", "labels": [], "entities": []}, {"text": "We also design an unaligned word tag so that the unaligned word phenomenon is automatically covered in the proposed model.", "labels": [], "entities": []}, {"text": "Our model is conditioned on the whole source sentence.", "labels": [], "entities": []}, {"text": "Hence it is able to capture the long dependencies in the source sentence.", "labels": [], "entities": []}, {"text": "We choose the conditional random fields (CRFs) approach for the tagging model.", "labels": [], "entities": [{"text": "tagging", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.970628023147583}]}, {"text": "Although utilizing CRFs on large scale task requests a notable amount of computational resources, the decoder makes use of the tagging information as soft constraints.", "labels": [], "entities": []}, {"text": "Therefore, the training procedure of our model is computationally expensive while in the test phase (during translation) our model is very efficient.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 reviews the related work for solving the reordering problem.", "labels": [], "entities": []}, {"text": "Section 3 introduces the basement of this research: the principle of statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.7300384640693665}]}, {"text": "Section 4 describes the proposed model.", "labels": [], "entities": []}, {"text": "Section 5 provides the experimental configuration and results.", "labels": [], "entities": []}, {"text": "Conclusion will be given in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the baseline setup, the CRFs training results and translation experimental results.", "labels": [], "entities": []}, {"text": "Our baseline is a phrase-based decoder, which includes the following models: an n-gram targetside language model (LM), a phrase translation model and a word-based lexicon model.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7917583584785461}]}, {"text": "The latter two models are used for both directions: p( f |e) and p(e| f ).", "labels": [], "entities": []}, {"text": "Additionally we use phrase count features, word and phrase penalty.", "labels": [], "entities": []}, {"text": "The reordering model for the baseline system is the distance-based jump model which uses linear distance.", "labels": [], "entities": []}, {"text": "This model does not have hard limit.", "labels": [], "entities": []}, {"text": "We list the important information regarding the experimental setup below.", "labels": [], "entities": []}, {"text": "All those conditions have been kept same in this work.", "labels": [], "entities": []}, {"text": "\u2022 lowercased training data from the BOLT task alignment trained with GIZA++ \u2022 development corpus: NIST06 test corpora: NIST02 03 04 05 and 08 \u2022 5-gram LM (1 694 412 027 running words) trained by SRILM toolkit) with modified Kneser-Ney smoothing training data: target side of bilingual data.", "labels": [], "entities": [{"text": "BOLT task alignment", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.4978986084461212}, {"text": "NIST06 test corpora", "start_pos": 98, "end_pos": 117, "type": "DATASET", "confidence": 0.9447336395581564}]}, {"text": "\u2022 BLEU () and TER () reported all scores calculated in lowercase way.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9991949200630188}, {"text": "TER", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.998170018196106}]}, {"text": "\u2022 Wapiti toolkit ( contains the data statistics used for translation model and LM.", "labels": [], "entities": [{"text": "translation model", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.9077647030353546}]}, {"text": "For the reordering model, we take two further filtering steps.", "labels": [], "entities": []}, {"text": "Firstly, we delete the sentence pairs if the source sentence length is one.", "labels": [], "entities": []}, {"text": "When the source sentence has only one word, the translation will be always monotonic and the reordering model does not need to learn this.", "labels": [], "entities": []}, {"text": "Secondly, we delete the sentence pairs if the source sentence contains more than three contiguous unaligned words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: training data statistics", "labels": [], "entities": []}]}