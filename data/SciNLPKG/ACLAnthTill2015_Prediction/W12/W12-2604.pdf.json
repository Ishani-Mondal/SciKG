{"title": [{"text": "Ecological Validity and the Evaluation of Speech Summarization Quality", "labels": [], "entities": [{"text": "Ecological Validity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8773028552532196}, {"text": "Evaluation of Speech Summarization", "start_pos": 28, "end_pos": 62, "type": "TASK", "confidence": 0.6218152493238449}]}], "abstractContent": [{"text": "There is little evidence of widespread adoption of speech summarization systems.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.661599650979042}]}, {"text": "This maybe due in part to the fact that the natural language heuristics used to generate summaries are often optimized with respect to a class of evaluation measures that, while computationally and experimentally inexpensive, rely on subjectively selected gold standards against which automatically generated summaries are scored.", "labels": [], "entities": []}, {"text": "This evaluation protocol does not take into account the usefulness of a summary in assisting the listener in achieving his or her goal.", "labels": [], "entities": []}, {"text": "In this paper we study how current measures and methods for evaluating summarization systems compare to human-centric evaluation criteria.", "labels": [], "entities": [{"text": "summarization", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.9749367237091064}]}, {"text": "For this, we have designed and conducted an ecologically valid evaluation that determines the value of a summary when embedded in a task, rather than how closely a summary resembles a gold standard.", "labels": [], "entities": []}, {"text": "The results of our evaluation demonstrate that in the domain of lecture summarization, the wellknown baseline of maximal marginal relevance) is statistically significantly worse than human-generated extractive summaries, and even worse than having no summary at all in a simple quiz-taking task.", "labels": [], "entities": [{"text": "lecture summarization", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.6138769537210464}]}, {"text": "Priming seems to have no statistically significant effect on the usefulness of the human summaries.", "labels": [], "entities": []}, {"text": "In addition, ROUGE scores and, in particular, the contextfree annotations that are often supplied to ROUGE as references, may not always be reliable as inexpensive proxies for ecologically valid evaluations.", "labels": [], "entities": []}, {"text": "In fact, under some conditions, relying exclusively on ROUGE may even lead to scoring human-generated summaries that are inconsistent in their usefulness relative to using no summaries very favourably.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.795639157295227}]}], "introductionContent": [], "datasetContent": [{"text": "As pointed out by, current speech summarizers have been optimized to perform an utterance selection task that may not necessarily reflect how a summarizer is able to capture the goal orientation or purpose of the speech data.", "labels": [], "entities": [{"text": "speech summarizers", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.6347024738788605}]}, {"text": "In our study, we follow methodologies established in the field of Human-Computer Interaction (HCI) for evaluating an algorithm or system -that is, determining the benefits a system brings to its users, namely usefulness, usability, or utility, in allowing a user to reach a specific goal.", "labels": [], "entities": [{"text": "Human-Computer Interaction (HCI)", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.6949724197387696}]}, {"text": "Increasingly, such user-centric evaluations are carried out within various natural language processing applications ().", "labels": [], "entities": []}, {"text": "The prevailing trend in HCI is for conducting extrinsic summary evaluations (, where the value of a summary is determined by how well the summary can be used to perform a specific task rather than comparing the content of a summary to an artificially created gold standard.", "labels": [], "entities": []}, {"text": "We have conducted an ecologically valid evaluation of speech summarization that has evaluated summaries under real-world conditions, in a task-based manner.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.622459813952446}]}, {"text": "The university lecture domain is an example of a domain where summaries are an especially suitable tool for navigation.", "labels": [], "entities": []}, {"text": "Simply performing a search will not result in the type of understanding required of students in their lectures.", "labels": [], "entities": []}, {"text": "Lectures have topics, and there is a clear communicative goal.", "labels": [], "entities": []}, {"text": "For these reasons, we have chosen this domain for our evaluation.", "labels": [], "entities": []}, {"text": "By using actual university lectures as well as university students representative of the users who would make use of a speech summarization system in this domain, all results obtained are ecologically valid.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 119, "end_pos": 139, "type": "TASK", "confidence": 0.7139196991920471}]}, {"text": "We conducted a within-subject experiment where participants were provided with first year sociology university lectures on a lecture browser system installed on a desktop computer.", "labels": [], "entities": []}, {"text": "For each lecture, the browser made accessible the audio, manual transcripts, and an optional summary.", "labels": [], "entities": []}, {"text": "Evaluation of a summary was based on how well the user of the summary was able to complete a quiz based on the content of the original lecture material.", "labels": [], "entities": []}, {"text": "It is important to note that not all extrinsic evaluation is ecologically valid.", "labels": [], "entities": []}, {"text": "To ensure ecological validity in this study, great care was taken to ensure that human subjects were placed under conditions that result in behavior that would be expected in actual real-world tasks.", "labels": [], "entities": []}, {"text": "Each quiz consisted of 12 questions, and were designed to be representative of what students were expected to learn in the class, incorporating factual questions only, to ensure that variation in participant intelligence had a minimal impact on results.", "labels": [], "entities": []}, {"text": "In addition, questions involved information that was distributed equally throughout the lecture, but at the same time not linearly in the transcript or audio slider, which would have allowed participants to predict where the next answer might be located.", "labels": [], "entities": []}, {"text": "Finally, questions were designed to avoid content that was thought to be common knowledge in order to minimize the chance of participants having previous knowledge of the answers.", "labels": [], "entities": []}, {"text": "All questions were short answer or fill-in-theblank.", "labels": [], "entities": []}, {"text": "Each quiz consisted of an equal number of four distinct types of questions, designed so that performing a simple search would not be effective, though no search functionality was provided.", "labels": [], "entities": []}, {"text": "Question types do not appear in any particular order on the quiz and were not grouped together.", "labels": [], "entities": []}, {"text": "Type 1: These questions can be answered simply by looking at the slides.", "labels": [], "entities": []}, {"text": "As such, these questions could be answered correctly with or without a summary as slides were available in all conditions.", "labels": [], "entities": []}, {"text": "Type 2: Slides provide an indication of where the content required to answer these questions are located.", "labels": [], "entities": []}, {"text": "Access to the corresponding utterances is still required to find the answer to the questions.", "labels": [], "entities": []}, {"text": "Type 3: Answers to these questions can only be found in the transcript and audio.", "labels": [], "entities": []}, {"text": "The slides provide no hint as to where the relevant content is located.", "labels": [], "entities": []}, {"text": "Type 4: These questions are more complicated and require a certain level of topic comprehension.", "labels": [], "entities": []}, {"text": "These questions often require connecting concepts from various portions of the lecture.", "labels": [], "entities": []}, {"text": "These questions are more difficult and were included to minimize the chance that participants would already know the answer to questions without watching the lecture.", "labels": [], "entities": []}, {"text": "A teaching assistant for the sociology class from which our lectures were obtained generated the quizzes used in the evaluation.", "labels": [], "entities": []}, {"text": "This teaching assistant had significant experience in the course, but was not involved in the design of this study and did not have any knowledge relating to our hypotheses or the topic of extractive summarization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 189, "end_pos": 213, "type": "TASK", "confidence": 0.7503075003623962}]}, {"text": "These quizzes provided an ecologically valid quantitative measure of whether a given summary was useful.", "labels": [], "entities": []}, {"text": "Having this evaluation metric in place, automated summaries were compared to manual summaries created by each participant in a previous session.", "labels": [], "entities": [{"text": "summaries", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.9386435747146606}]}, {"text": "More common than F-measure, ROUGE) is often used to evaluate summarization.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9405452013015747}, {"text": "ROUGE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9958593249320984}, {"text": "summarization", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.9793368577957153}]}, {"text": "Although claimed to have demonstrated that ROUGE correlates well with human summaries, both, and have cast doubt upon this.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9292792081832886}]}, {"text": "It is important to acknowledge, however, that ROUGE is actually a family of measures, distinguished not only by the manner in which overlap is measured (1-grams, longest common subsequences, etc.), but by the provenience of the summaries that are provided to it as references.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.6764382123947144}]}, {"text": "If these are not ecologically valid, there is no sense in holding ROUGE accountable for an erratic result.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.8137117624282837}]}, {"text": "To examine how ROUGE fairs under ecologically valid conditions, we calculated ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-SU4 on our data using the standard options outlined in previous DUC evaluations.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9735872745513916}, {"text": "ROUGE-2", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.894741415977478}, {"text": "ROUGE-L", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.882273256778717}]}, {"text": "ROUGE scores were calculated for each of the generic manual summary, primed manual summary, and automatic summary conditions.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.97536700963974}]}, {"text": "Each summary in a given condition was evaluated once against the generic manual summaries and once using the primed manual summaries.", "labels": [], "entities": []}, {"text": "Similar to, ROUGE evaluation was conducted using leaveone-out on the model summary type and averaging the results.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.7379016280174255}]}, {"text": "In addition to calculating ROUGE on the summaries from our ecologically valid evaluation, we also followed more conventional ROUGE evaluation and used the same context-free annotator summaries as were used in our F-measure calculations above.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9815370440483093}]}, {"text": "Using these context-free summaries, the original generic manual, primed manual, and automatic summaries were evaluated using ROUGE.", "labels": [], "entities": [{"text": "summaries", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.8664833307266235}, {"text": "ROUGE", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.9851294755935669}]}, {"text": "The result of these evaluations are presented in.", "labels": [], "entities": []}, {"text": "Looking at the ROUGE scores, we can see that when evaluated by each type of model summary, MMR performs worse than either generic or primed manual summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9888901710510254}, {"text": "MMR", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.8286436200141907}]}, {"text": "This is consistent with our quiz results, and perhaps shows that ROUGE maybe able to distinguish human summaries from MMR.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.986155092716217}, {"text": "summaries from MMR", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.5647499958674113}]}, {"text": "Looking at the generic-generic, primedprimed, and context-free-context-free scores, we can get a sense of how much agreement there was between summaries.", "labels": [], "entities": []}, {"text": "It is not surprising that context-free annotator summaries showed the least agreement, as these summaries were generated with no higher purpose in mind.", "labels": [], "entities": [{"text": "agreement", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9713225960731506}]}, {"text": "This suggests that using annotators to generate gold standards in such a manner is not ideal.", "labels": [], "entities": []}, {"text": "In addition, real world applications for summarization would conceivably rarely consist of a situation where a summary was created for no apparent reason.", "labels": [], "entities": [{"text": "summarization", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.9919955730438232}]}, {"text": "More interesting is the observation that, when measured by ROUGE, primed summaries have less in common with each other than generic summaries do.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9904086589813232}]}, {"text": "The difference, however, is less pronounced when measured by ROUGE than by F-measure.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9943171143531799}, {"text": "F-measure", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.972429096698761}]}, {"text": "This is likely due to the fact that ROUGE can account for semantically similar utterances.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.8686746954917908}]}], "tableCaptions": [{"text": " Table 3. Average ROUGE Scores", "labels": [], "entities": [{"text": "Average ROUGE Scores", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8780002991358439}]}]}