{"title": [{"text": "Methods Combination and ML-based Re-ranking of Multiple Hypothesis for Question-Answering Systems", "labels": [], "entities": [{"text": "ML-based Re-ranking", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7912529110908508}]}], "abstractContent": [{"text": "Question answering systems answer correctly to different questions because they are based on different strategies.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8619382083415985}]}, {"text": "In order to increase the number of questions which can be answered by a single process, we propose solutions to combine two question answering systems, QAVAL and RITEL.", "labels": [], "entities": [{"text": "question answering", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7354351580142975}, {"text": "QAVAL", "start_pos": 152, "end_pos": 157, "type": "METRIC", "confidence": 0.5706155300140381}, {"text": "RITEL", "start_pos": 162, "end_pos": 167, "type": "METRIC", "confidence": 0.9099676609039307}]}, {"text": "QAVAL proceeds by selecting short passages , annotates them by question terms, and then extracts from them answers which are ordered by a machine learning validation process.", "labels": [], "entities": []}, {"text": "RITEL develops a multi-level analysis of questions and documents.", "labels": [], "entities": [{"text": "RITEL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.665952205657959}]}, {"text": "Answers are extracted and ordered according to two strategies: by exploiting the redundancy of candidates and a Bayesian model.", "labels": [], "entities": []}, {"text": "In order to merge the system results, we developed different methods either by merging passages before answer ordering, or by merging end-results.", "labels": [], "entities": [{"text": "answer ordering", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.7512578368186951}]}, {"text": "The fusion of end-results is realized by voting, merging, and by a machine learning process on answer characteristics, which lead to an improvement of the best system results of 19 %.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question-answering systems aim at giving short and precise answers to natural language questions.", "labels": [], "entities": []}, {"text": "These systems are quite complex, and include many different components.", "labels": [], "entities": []}, {"text": "QuestionAnswering systems are generally organized within a pipeline which includes at a high level at least three components: questions processing, snippets selection and answers extraction.", "labels": [], "entities": [{"text": "questions processing", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.7333849370479584}, {"text": "snippets selection", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.6857445538043976}, {"text": "answers extraction", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.7496246099472046}]}, {"text": "But each module of these systems is quite different.", "labels": [], "entities": []}, {"text": "They are based on different knowledge sources and processing.", "labels": [], "entities": []}, {"text": "Even if the global performance of these systems are similar, they show great disparity when examining local results.", "labels": [], "entities": []}, {"text": "Moreover there is no question-answering system able to answer correctly to all possible questions.", "labels": [], "entities": []}, {"text": "Considering all QA evaluation campaigns in French like CLEF, EQUER or Quaero, or for other languages like TREC, no system obtained 100% correct answers at first rank.", "labels": [], "entities": []}, {"text": "A new direction of research was built upon these observations: how can we combine correct answers provided by different systems?", "labels": [], "entities": []}, {"text": "This work deals with this issue . In this paper we describe different experiments concerning the combination of QA systems.", "labels": [], "entities": []}, {"text": "We used two different available systems, QAVAL and RITEL, while RITEL includes two different answer extraction strategies.", "labels": [], "entities": [{"text": "QAVAL", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.5788290500640869}, {"text": "RITEL", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.926971971988678}, {"text": "answer extraction", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.8060220181941986}]}, {"text": "We propose to merge the results of these systems at different levels.", "labels": [], "entities": []}, {"text": "First, at an intermediary step (for example, between snippet selection and answer extraction).", "labels": [], "entities": [{"text": "snippet selection", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7503255605697632}, {"text": "answer extraction", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.8006924390792847}]}, {"text": "This approach allows to evaluate a fusion process based on the integration of different strategies.", "labels": [], "entities": []}, {"text": "Another way to proceed is to execute the fusion at the end of each system.", "labels": [], "entities": []}, {"text": "The aim is then to choose between all the candidate answers the best one for each question.", "labels": [], "entities": []}, {"text": "Such an approach has been successfully applied in the information retrieval field, with the definition of different functions for combining results of search engines.", "labels": [], "entities": [{"text": "information retrieval field", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.8188988367716471}]}, {"text": "However, in QA, the problem is different as answers to questions are not made of a list of answers, but are made of excerpts of texts, which maybe different in their writing, but which correspond to a unique and same answer.", "labels": [], "entities": []}, {"text": "Thus, we propose fusion methods that rely on the information generally computed by QA systems, such as score, rank, an-swer redundancy, etc.", "labels": [], "entities": [{"text": "score", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9843175411224365}]}, {"text": "We defined new voting and scoring functions, and a machine learning system to combine these features.", "labels": [], "entities": [{"text": "voting and scoring", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8462333480517069}]}, {"text": "Most of the strategies presented here allow a clear improvement (up to 19 %) on the first ranked correct answers.", "labels": [], "entities": []}, {"text": "In the following, related work is presented in the section 2.", "labels": [], "entities": []}, {"text": "We then describe the different systems used in this work (Section 3.1 and 3.2).", "labels": [], "entities": []}, {"text": "The proposed approach are presented (Section 4 and 5).", "labels": [], "entities": []}, {"text": "The methods and the different systems are then evaluated on the same corpus.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. This simple approach does not allow any", "labels": [], "entities": []}, {"text": " Table 4: Impact of the number of candidate answers", "labels": [], "entities": []}, {"text": " Table 5: Impact of the normalization", "labels": [], "entities": [{"text": "normalization", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9334924817085266}]}]}