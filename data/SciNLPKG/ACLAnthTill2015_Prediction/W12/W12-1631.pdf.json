{"title": [{"text": "Dialog System Using Real-Time Crowdsourcing and Twitter Large-Scale Corpus", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a dialog system that creates responses based on a large-scale dialog corpus retrieved from Twitter and real-time crowd-sourcing.", "labels": [], "entities": []}, {"text": "Instead of using complex dialog management, our system replies with the utterance from the database that is most similar to the user input.", "labels": [], "entities": []}, {"text": "We also propose a real-time crowdsourcing framework for handling the casein which there is no adequate response in the database.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is a lot of language data on the Internet.", "labels": [], "entities": []}, {"text": "Twitter offers many APIs to retrieve or search post status data, and this data is frequently used in research, such as in stock market prediction), the spread of information through social media (, and representations of textual content).", "labels": [], "entities": [{"text": "stock market prediction", "start_pos": 122, "end_pos": 145, "type": "TASK", "confidence": 0.6936822533607483}]}, {"text": "Several models for conversation using Twitter data () have been proposed because of the data's vast size and conversational nature.", "labels": [], "entities": []}, {"text": "Kelly previously showed that 37% of English tweets are Conversational, of which 69% are two-length (one status post and a reply).", "labels": [], "entities": []}, {"text": "In our analysis of over 2.5 million tweets, 37.5% of all Japanese tweets are Conversational, which matches Kelly's data.", "labels": [], "entities": [{"text": "Kelly's data", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.7952075004577637}]}, {"text": "However, less than 58.3% of these are twolength tweets.", "labels": [], "entities": []}, {"text": "Many chat bots are rule-based, which requires a lot of human effort to create or add new rules.", "labels": [], "entities": []}, {"text": "For example, A.L.I.C.E, which won the  Loebner Prize three times, creates responses based on a dialog strategy database written in a markup language named AIML.", "labels": [], "entities": []}, {"text": "Recently, some other chat bots based on a large-scale dialog corpus have been proposed . In this paper, we propose a novel dialog system (chat bot) that uses real-time crowdsourcing and Twitter large-scale corpus.", "labels": [], "entities": []}, {"text": "We evaluate response selection methods based on positive/negative example to judge if each feature could be exploited to judge similarity between uterrances.", "labels": [], "entities": []}], "datasetContent": [{"text": "We prepared 90 user input examples and extracted 20 utterance-pairs (utterance and responses in the database retrieved from Twitter) per user input, so that a total of 1,800 of triples (a user input and an utterance pair) were included in our sample.", "labels": [], "entities": []}, {"text": "Thirty subjects evaluated the naturalness and versatility of the responses.", "labels": [], "entities": []}, {"text": "Each subject evaluated 600 triples.", "labels": [], "entities": []}, {"text": "We note that subjects saw only the user input and response in an utterance pair (B in), and were not shown A in in the survey sheets.", "labels": [], "entities": [{"text": "A", "start_pos": 107, "end_pos": 108, "type": "METRIC", "confidence": 0.980976402759552}]}, {"text": "In this paper, versatility of a response corresponds to the number of utterances to which it was rated as sensible response (e.g., \"What do you mean?\" can serve as a response to almost any input, and is therefore highly versatile).", "labels": [], "entities": []}, {"text": "points out that chat bots have many tricks to fool people, and providing a versatile answer is one of them.", "labels": [], "entities": []}, {"text": "We believe our system can avoids versatile answers by using a large-scale database.", "labels": [], "entities": []}, {"text": "In the following, we describe how we evaluate each scoring function (which takes a triplet as an input and the score as the output) using positive/negative learning data.", "labels": [], "entities": []}, {"text": "We treat scoring functions as classifiers, that is, when the function receives a triplet as input, we assume that the function judges the triplet as positive data if the output score is above a certain threshold and negative data if it is below it.", "labels": [], "entities": []}, {"text": "Triplets collected in the survey were divided into positive and negative triplets.", "labels": [], "entities": []}, {"text": "We consider an utterance pair to be positive if it is judged as natural by more than 7 out of 10 subjects and versatile by less than 7 out of 10 subjects.", "labels": [], "entities": []}, {"text": "All else were considered negative triplets.", "labels": [], "entities": []}, {"text": "The ROC curve is used to illustrate the performance of the classifiers.", "labels": [], "entities": [{"text": "ROC curve", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9623048305511475}]}, {"text": "It is a two-dimensional graph in which true positive rate is plotted on the Y axis and false positive rate is plotted on the X axis.", "labels": [], "entities": [{"text": "false positive rate", "start_pos": 87, "end_pos": 106, "type": "METRIC", "confidence": 0.7083920439084371}]}, {"text": "Here, the true positive rate (r T P ) and false positive rate (r F P ) are given by The area under the curve (AUC), or the area under the ROC curve, was used to measure classifier performance.", "labels": [], "entities": [{"text": "true positive rate (r T P )", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.7641439735889435}, {"text": "false positive rate (r F P )", "start_pos": 42, "end_pos": 70, "type": "METRIC", "confidence": 0.8192496858537197}, {"text": "area under the curve (AUC)", "start_pos": 88, "end_pos": 114, "type": "METRIC", "confidence": 0.7577337409768786}]}, {"text": "A random classifier has an AUC of 0.5, and ideal classifier has an AUC of 1.0.", "labels": [], "entities": [{"text": "AUC", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9810164570808411}, {"text": "AUC", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9717462062835693}]}, {"text": "We applied a number of scoring functions to the triples, and then calculated the AUC for each function (classifier) for validation.", "labels": [], "entities": [{"text": "AUC", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9961228966712952}]}, {"text": "We chose scoring functions which \u2022 calculate similarity only with A in, or A and B in, \u2022 use term frequency (tf) when the document vector is calculated, or not, \u2022 use inverse document frequency (idf) when the document vector is calculated, or not, \u2022 eliminate Twitter-specific representations (see Section 2.1) or not, \u2022 normalize by character count or not, \u2022 filter POS or not.", "labels": [], "entities": [{"text": "term frequency (tf)", "start_pos": 93, "end_pos": 112, "type": "METRIC", "confidence": 0.9465984582901001}, {"text": "inverse document frequency (idf)", "start_pos": 167, "end_pos": 199, "type": "METRIC", "confidence": 0.7752225051323572}]}, {"text": "We compared a total of 64 (=2 6 ) scoring functions.", "labels": [], "entities": []}, {"text": "illustrates some or our results.", "labels": [], "entities": []}, {"text": "As it shows, when only Twitter-specific expressions are filtered, classifier performance is similar to a random classifier.", "labels": [], "entities": []}, {"text": "The addition of word count normalization and POS filter improved to classification performance.", "labels": [], "entities": [{"text": "word count normalization", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.6458121637503306}, {"text": "POS filter", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.975479781627655}]}, {"text": "This is because longer utterances normally include more specific information, so that the topic is more prone to be missed during the response selection process.", "labels": [], "entities": []}, {"text": "Adverbs (e.g. \"very\") or particles (corresponds preposition in English, e.g. \"as\") had little effect on the context of an utterance, so POS filtering acts as noise elimination.", "labels": [], "entities": [{"text": "POS filtering", "start_pos": 136, "end_pos": 149, "type": "TASK", "confidence": 0.8697181046009064}]}, {"text": "With respect to tf and idf, the effect of tf varied widely, and idf hindered classification performance.", "labels": [], "entities": [{"text": "classification", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.9565606117248535}]}, {"text": "We chose the scoring function with the best performance (see for details), of which the AUC is 0.803.", "labels": [], "entities": [{"text": "AUC", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9976078271865845}]}], "tableCaptions": []}