{"title": [{"text": "Syntax-aware Phrase-based Statistical Machine Translation: System Description", "labels": [], "entities": [{"text": "Phrase-based Statistical Machine Translation", "start_pos": 13, "end_pos": 57, "type": "TASK", "confidence": 0.6565648689866066}]}], "abstractContent": [{"text": "We present a variant of phrase-based SMT that uses source-side parsing and a constituent reordering model based on word alignments in the word-aligned training corpus to predict hierarchical block-wise reordering of the input.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.707156777381897}]}, {"text": "Multiple possible translation orders are represented compactly in a source order lattice.", "labels": [], "entities": []}, {"text": "This source order lattice is then annotated with phrase-level translations to form a lattice of tokens in the target language.", "labels": [], "entities": []}, {"text": "Various feature functions are combined in a log-linear fashion to evaluate paths through that lattice.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dealing with word order differences is one of the major challenges in automatic translation between human languages.", "labels": [], "entities": [{"text": "automatic translation between human languages", "start_pos": 70, "end_pos": 115, "type": "TASK", "confidence": 0.8099353075027466}]}, {"text": "With its moderate context sensitivity and reliance on n-gram language models, phrase-based statistical machine translation (PB-SMT)) is usually quite good at performing small word order changes -for instance, the inversion of adjective and noun in English-to-French translation and vice versa.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 78, "end_pos": 122, "type": "TASK", "confidence": 0.58946643024683}]}, {"text": "However, it regularly fails to execute word order changes overlong distances, as they are required, for example, to accommodate the substantial differences in the word order insubordinate clauses between German and English, or to cope with the phenomenon of the \"sentence bracket\" (Satzklammer) in German main clauses, in which the finite part of the verb complex and additional elements (separable prefixes, participles, infinitives, etc.) form a bracket that encloses most of the arguments and other adverbial constituents, as shown in.", "labels": [], "entities": []}, {"text": "In order to keep decoding complexity in check, phrase-based decoders such as the Moses system () routinely limit the maximum distance for word order changes to six or seven word positions, thus ruling out, a priori, word order changes necessary to achieve good and fluent translations.", "labels": [], "entities": []}, {"text": "As is generally acknowledged, word order differences are not entirely arbitrary.", "labels": [], "entities": []}, {"text": "By and large they follow syntactic structure.", "labels": [], "entities": []}, {"text": "An analysis of word-aligned French-English data by showed that word alignment links rarely cross syntactic boundaries.", "labels": [], "entities": []}, {"text": "Inversion Transaction Grammar (ITG), assumes that word order differences can be accounted for by hierarchical inversion of adjacent blocks of text.", "labels": [], "entities": [{"text": "Inversion Transaction Grammar (ITG)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8535647143920263}]}, {"text": "present a stochastic model for transforming English parse trees into Japanese word sequences within a source-channel framework for Japaneseto-English translation.", "labels": [], "entities": [{"text": "Japaneseto-English translation", "start_pos": 131, "end_pos": 161, "type": "TASK", "confidence": 0.6738978326320648}]}, {"text": "perform heuristic word re-ordering from German into English word order based on German parse trees with a particular focus on the aforementioned drastic word order differences between German and English clause structure.", "labels": [], "entities": [{"text": "heuristic word re-ordering from German into English word order", "start_pos": 8, "end_pos": 70, "type": "TASK", "confidence": 0.7473417619864146}]}, {"text": "Building on, several systems under active development (e.g.,) rely on synchronous context-free grammars to deal with word order differences.", "labels": [], "entities": []}, {"text": "In essence, these systems parse the input while synchronously building a parse tree in the translation target language, using probabilities of the source and target trees as well as correspondence probabilities to evaluate translation hypotheses.", "labels": [], "entities": []}, {"text": "\"Dieser 1 Vorschlag 2 wird 3 sicherlich 4 im 5 Ausschu\u00df 6 gr\u00fcndlich 7 diskutiert 8 werden 9 m\u00fcssen 10 .\" \"This 1 proposal 2 will 3 certainly 4 have 10 to 10 be 9 discussed 8 toroughly 7 in 5 the 5 commission 6 .\": The sentence bracket (Satzklammer) in German.", "labels": [], "entities": []}, {"text": "The system presented in this paper takes a slightly different route and is closer to the approach taken by: we parse only monolingually on the source side, re-order, and then translate.", "labels": [], "entities": []}, {"text": "However, unlike Collins et al. we do not use a series of rules to perform the transformations (nor do we re-order the training data on the source side), but try to learn reordering rules from the word-aligned corpus with the original word order on both sides.", "labels": [], "entities": []}, {"text": "Moreover, we do not commit to a single parse and a single re-ordering of the source at translation time but consider multiple parse alternatives to create a lattice of possible translation orders.", "labels": [], "entities": []}, {"text": "Each vertex in the lattice corresponds to a specific subset of source words translated up to that point.", "labels": [], "entities": []}, {"text": "Individual edges and sequences of edges in this lattice are annotated with word-and phrase 1 -level translations extracted from the word-aligned training corpus, in the same way as phrase tables for PB-SMT are constructed 2 . An optimal path through the lattice is determined by dynamic programming, considering a variety of feature functions combined in a log-linear fashion.", "labels": [], "entities": []}, {"text": "In the following, we first describe the individual processing steps in more detail and then try to shed some light on the system's performance in this year's shared task.", "labels": [], "entities": []}, {"text": "Due to space limitations, many details will have to be skipped.", "labels": [], "entities": []}], "datasetContent": [{"text": "Unfortunately, with a BLEU score of .121, (.150 after several bug fixes in the program code), our system performed extremely poorly in the shared task.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9803100526332855}]}, {"text": "We have since tried to track down the reasons for the poor performance, but have not been able to find a compelling explanation for it.", "labels": [], "entities": []}, {"text": "A partial explanation may lie in the fact that we used only the Europarl data for training.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.9950920939445496}]}, {"text": "3 However, our system also lags far behind a baseline Moses system trained on the same subset of data used for our system, which achieves a BLEU score of .184.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 140, "end_pos": 150, "type": "METRIC", "confidence": 0.9856454133987427}]}, {"text": "Since our feature functions are very similar to those used in MOSES, we suspect that better tuning of the feature weights might close the gap.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.6819901466369629}]}, {"text": "We are currently in the process of implementing and testing other parameter tuning methods (in addition to manual tuning and PRO), specifically lattice-based minimum error rate training and batch MIRA (Cherry and Foster, 2012).", "labels": [], "entities": [{"text": "MIRA", "start_pos": 196, "end_pos": 200, "type": "METRIC", "confidence": 0.711836040019989}]}], "tableCaptions": []}