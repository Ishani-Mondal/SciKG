{"title": [{"text": "Optimising Incremental Generation for Spoken Dialogue Systems: Reducing the Need for Fillers", "labels": [], "entities": [{"text": "Optimising Incremental Generation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7518249849478403}]}], "abstractContent": [{"text": "Recent studies have shown that incremental systems are perceived as more reactive, natural , and easier to use than non-incremental systems.", "labels": [], "entities": []}, {"text": "However, previous work on incre-mental NLG has not employed recent advances in statistical optimisation using machine learning.", "labels": [], "entities": []}, {"text": "This paper combines the two approaches, showing how the update, revoke and purge operations typically used in in-cremental approaches can be implemented as state transitions in a Markov Decision Process.", "labels": [], "entities": []}, {"text": "We design a model of incremental NLG that generates output based on micro-turn interpretations of the user's utterances and is able to optimise its decisions using statistical machine learning.", "labels": [], "entities": []}, {"text": "We present a proof-of-concept study in the domain of Information Presentation (IP), where a learning agent faces the trade-off of whether to present information as soon as it is available (for high reactiveness) or else to wait until input ASR hypotheses are more reliable.", "labels": [], "entities": [{"text": "Information Presentation (IP)", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.7836271524429321}]}, {"text": "Results show that the agent learns to avoid long waiting times, fillers and self-corrections, by reordering content based on its confidence.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditionally, the smallest unit of speech processing for interactive systems has been a full utterance with strict, rigid turn-taking.", "labels": [], "entities": []}, {"text": "Components of these interactive systems, including NLG systems, have so far treated the utterance as the smallest processing unit that triggers a module into action.", "labels": [], "entities": []}, {"text": "More recently, work on incremental systems has shown that processing smaller 'chunks' of user input can improve the user experience).", "labels": [], "entities": []}, {"text": "Incrementality in NLG systems enables the system designer to model several dialogue phenomena that play a vital role inhuman discourse) but have so far been absent from NLG systems.", "labels": [], "entities": []}, {"text": "These include more natural turn-taking through rapid system responses, grounding through the generation of backchannels and feedback, and barge-ins (from both user and system).", "labels": [], "entities": []}, {"text": "In addition, corrections and self-corrections through constant monitoring of user and system utterances play an important role, enabling the system to recover smoothly from a recognition error or a change in the user's preferences.", "labels": [], "entities": []}, {"text": "Some examples of the phenomena we are targeting are given in. present a model of incremental speech generation in which input processing and output planning are parallel processes and the system can self-monitor its own generation process.", "labels": [], "entities": [{"text": "speech generation", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7631560862064362}]}, {"text": "In an evaluation with human users they showed that their incremental system started to speak significantly faster than a non-incremental system (roughly 600 ms) and was perceived as significantly more polite and efficient.", "labels": [], "entities": []}, {"text": "Users also indicated that they knew better when to start speaking themselves.", "labels": [], "entities": []}, {"text": "Alternative approaches to incremental NLG include who present an early approach based on Tree-Adjoining Grammar, and who define an incremental generator based on Dynamic Syntax.", "labels": [], "entities": []}, {"text": "Both of these generators can monitor their own output and initiate corrections if necessary.", "labels": [], "entities": []}, {"text": "Over recent years, adaptive and data-driven ap-  proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (.", "labels": [], "entities": []}, {"text": "This work has established that NLG can fruitfully be treated as a data-driven statistical planning process, where the objective is to maximise expected utility of the generated utterances, by adapting them to the context and user.", "labels": [], "entities": []}, {"text": "Statistical approaches to sentence planning and surface realisation have also been explored.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.8589299321174622}, {"text": "surface realisation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.767188161611557}]}, {"text": "The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contexts and, trained on real data, can produce more natural and desirable variation in system utterances.", "labels": [], "entities": []}, {"text": "This paper describes an initial investigation into a novel NLG architecture that combines incremental processing with statistical optimisation.", "labels": [], "entities": []}, {"text": "In order to move away from conventional strict-turn taking, we have to be able to model the complex interactions observed in human-human conversation.", "labels": [], "entities": [{"text": "strict-turn taking", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.741786003112793}]}, {"text": "Doing this in a deterministic fashion through hand-written rules would be time consuming and potentially inaccurate, with no guarantee of optimality.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that it is possible to learn incremental generation behaviour in a reward-driven fashion.", "labels": [], "entities": []}], "datasetContent": [{"text": "After training, the RL agent has learnt the following incremental IP strategy.", "labels": [], "entities": [{"text": "RL agent", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.8284545540809631}]}, {"text": "It will present information slots as soon as they become available if they have a medium or high confidence score.", "labels": [], "entities": []}, {"text": "The agent will then order attributes so that those slots with the highest confidence scores are presented first and slots with lower confidence are presented later (by which time they may have achieved higher confidence).", "labels": [], "entities": []}, {"text": "If no information is known with medium or high con- fidence, the agent will hold the floor or wait.", "labels": [], "entities": []}, {"text": "In this way, it can prevent self-corrections and minimise waiting time-both of which yield negative rewards.", "labels": [], "entities": []}, {"text": "It can thus start speaking very early (avoiding long pauses or semantically empty utterances) and still has a low likelihood of having to self-correct.", "labels": [], "entities": []}, {"text": "For a comparison of the learnt policy with possible hand-crafted policies (because current incremental NLG systems are rule-based), we designed three baselines.", "labels": [], "entities": []}, {"text": "Baseline 1 always presents information as soon as it is available, i.e. never waits.", "labels": [], "entities": []}, {"text": "Baseline 2 always waits until all information is known with high confidence (i.e. until all confidence scores are 3).", "labels": [], "entities": []}, {"text": "Baseline 3 was chosen to be more ambitious.", "labels": [], "entities": [{"text": "Baseline 3", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8803218603134155}]}, {"text": "It always presents information as soon as possible, using a decreasing order of confidence to minimise self-corrections (i.e. very similar to the learnt policy).", "labels": [], "entities": []}, {"text": "It chooses randomly among slots with equal confidence.", "labels": [], "entities": []}, {"text": "All baseline policies have an optimised IP strategy (recommend/summary etc.) and differ only in their incremental processing strategies.", "labels": [], "entities": []}, {"text": "Baseline 1 is most similar to the current approach used in spoken dialogue systems, where the Dialogue Manager triggers an NLG component as soon as a task-relevant user utterance is processed.", "labels": [], "entities": []}, {"text": "Here we do not compare the different strategies for IP generally because this has been done by, even if not for incremental NLG.", "labels": [], "entities": []}, {"text": "shows the performance of all behaviours in terms of average rewards.", "labels": [], "entities": []}, {"text": "Baseline 1 obtains on average roughly 100 reward points less than the RL agent.", "labels": [], "entities": []}, {"text": "This corresponds to the (negative) reward of one self-correction (\u2212100).", "labels": [], "entities": []}, {"text": "Since information is always presented as soon as it is available, this baseline needs to produce on average one  self-correction per episode.", "labels": [], "entities": []}, {"text": "Baseline 2 needs to wait until all information is known with high confidence and obtains on average 125 to 130 rewards less than the RL agent.", "labels": [], "entities": []}, {"text": "This corresponds to approximately 11 time steps of waiting (for input to reach higher confidence) before presentation since 11 is (approximately) the square root of 130.", "labels": [], "entities": []}, {"text": "Baseline 3 is roughly a reward of \u221210 worse than the RL agent's be-haviour, which is due to a combination of more selfcorrections, even if they just occur occasionally, and a higher number of turn holding markers.", "labels": [], "entities": []}, {"text": "The latter is due to the baseline starting to present as soon as possible, so that whenever all confidence scores are too low to start presenting, a turn holding marker is generated.", "labels": [], "entities": []}, {"text": "The learning agent learns to outperform all baselines significantly, by presenting information slots in decreasing order of confidence, combined with waiting and holding the floor at appropriate moments.", "labels": [], "entities": []}, {"text": "Anticipating the rewards for waiting vs. holding the floor at particular moments is the main reason that the learnt policy outperforms Baseline 3.", "labels": [], "entities": []}, {"text": "Subtle moments of timing as in this case are difficult to hand-craft and more appropriately balanced using optimisation.", "labels": [], "entities": []}, {"text": "An absolute comparison of the last 1000 episodes of each behaviour shows that the improvement of the RL agent corresponds to 126.8% over Baseline 1, to 137.7% over Baseline 2 and to 16.76% over Baseline 3.", "labels": [], "entities": []}, {"text": "All differences are significant at p < 0.001 according to a paired t-test and have a high effect sizer > 0.9.", "labels": [], "entities": []}, {"text": "The high percentage improvement of the learnt policy over Baselines 1 and 2 is mainly due to the high numeric values chosen for the rewards as can be observed from their qualitative behaviour.", "labels": [], "entities": []}, {"text": "Thus, if the negative numeric values of, e.g., a self-correction were reduced, the percentage reward would reduce, but the policy would not change qualitatively.", "labels": [], "entities": [{"text": "percentage reward", "start_pos": 83, "end_pos": 100, "type": "METRIC", "confidence": 0.9140365719795227}]}, {"text": "shows some examples of the learnt policy including several incremental phenomena.", "labels": [], "entities": []}, {"text": "In contrast, shows examples generated with the baselines.", "labels": [], "entities": []}], "tableCaptions": []}