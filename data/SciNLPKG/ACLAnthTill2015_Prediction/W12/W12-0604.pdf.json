{"title": [{"text": "Topic Classification of Blog Posts Using Distant Supervision", "labels": [], "entities": [{"text": "Topic Classification of Blog Posts", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8604460954666138}]}], "abstractContent": [{"text": "Classifying blog posts by topics is useful for applications such as search and marketing.", "labels": [], "entities": [{"text": "Classifying blog posts by topics", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.877595329284668}]}, {"text": "However, topic classification is time consuming and error prone, especially in an open domain such as the blogosphere.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.8161709904670715}]}, {"text": "The state-of-the-art relies on supervised methods , requiring considerable training effort, that use the whole corpus vocabulary as features , demanding considerable memory to process.", "labels": [], "entities": []}, {"text": "We show an effective alternative whereby distant supervision is used to obtain training data: we use Wikipedia articles labelled with Freebase domains.", "labels": [], "entities": []}, {"text": "We address the memory requirements by using only named entities as features.", "labels": [], "entities": []}, {"text": "We test our classifier on a sample of blog posts, and report up to 0.69 accuracy for multi-class labelling and 0.9 for binary classification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9984809756278992}]}], "introductionContent": [{"text": "With the ever increasing popularity of blogging grows the need of finding ways for better organizing the blogosphere.", "labels": [], "entities": []}, {"text": "Besides identifying SPAM from legitimate blogs, one promising idea is to classify blog posts into topics such as travel, sports, religion, and soon, which could lead to better ways of exploring the blogosphere.", "labels": [], "entities": []}, {"text": "Besides navigation, blog classification can be useful as a data preprocessing step before other forms of analysis can be done: for example companies can view the perception and reception of products, movies, books and more based on opinions in blogs of different segments.", "labels": [], "entities": [{"text": "blog classification", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7462822794914246}]}, {"text": "We approach the problem by using machine learning.", "labels": [], "entities": []}, {"text": "In particular, in the development of a learning-based classifier, two crucial tasks are the choice of the features and the building of training data.", "labels": [], "entities": []}, {"text": "We adopt a novel approach when selecting features: we use an off-the-shelf Named Entity Recognition (NER) tool to identify entities in the text.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.7670336167017618}]}, {"text": "Our hypothesis is that one can detect the topic of a post by focusing on the entities discussed in the post.", "labels": [], "entities": []}, {"text": "Previous text classification tools use the entire vocabulary as potential features, which is a superset of our feature set.", "labels": [], "entities": [{"text": "text classification", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7105943262577057}]}, {"text": "Our results show that despite using a smaller feature set, our method can achieve very high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9975489974021912}]}, {"text": "Obtaining training data is a challenge for most learning tools, as it often involves manual inspection of hundreds or thousands of examples.", "labels": [], "entities": []}, {"text": "We address this by using distant supervision, where a separate dataset is used to obtain training data for the classifier.", "labels": [], "entities": []}, {"text": "The distant dataset used here is Freebase 1 , which is an open online database, along with related Wikipedia articles.", "labels": [], "entities": [{"text": "Freebase 1", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9380159676074982}]}, {"text": "The classes in our tests are domains in Freebase, which are defined by their curators.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.9739239811897278}]}, {"text": "For our evaluation, we use a large sample of blog posts from a public snapshot of the blogosphere, collected around 2008.", "labels": [], "entities": []}, {"text": "These posts are manually labeled by volunteers (undergraduate students in Computing Science), and used as the ground-truth test data.", "labels": [], "entities": []}, {"text": "Our results indicate that training a classifier relying on named entities using Freebase and Wikipedia, can achieve high accuracy levels on manually annotated data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9972280859947205}]}, {"text": "We also identify some potential problems related to selecting the categories to be used in the classification.", "labels": [], "entities": []}, {"text": "Overall, our results indicate that robust classifiers are possible using off-the-shelf tools and freely available 1 http://www.freebase.com/.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected the training data as follows.", "labels": [], "entities": []}, {"text": "First, we discarded generic Freebase domains such as Common and Metaweb System Types, which do not correspond to meaningful topics.", "labels": [], "entities": []}, {"text": "We also discarded other domains which were too narrow, comprising only a few objects.", "labels": [], "entities": []}, {"text": "We then concentrated on domains for which we could find many objects and for which we could perform a reasonable evaluation.", "labels": [], "entities": []}, {"text": "For the purposes of this paper, the 7 domains shown in were used as topics.", "labels": [], "entities": []}, {"text": "For each topic, we find all Freebase objects and their corresponding Wikipedia articles, and we collect the 2,000 longest articles (as those are most likely to contain the most named entities).", "labels": [], "entities": []}, {"text": "The exception was the celebrities topic, for which only 1,605 articles were used.", "labels": [], "entities": []}, {"text": "From these articles, we extract the named entities (i.e., the features), thus obtaining our training data.", "labels": [], "entities": []}, {"text": "In the end, we used 4,000 articles for each binary classification experiment and 13,605 for the multi-class one.", "labels": [], "entities": []}, {"text": "our evaluations, we relied on volunteers 2 who labeled hundreds of blogs, chosen among the most popular ones (this information is provided in the dataset), until we collected 50 blogs for each category.", "labels": [], "entities": []}, {"text": "For the binary classifications, we used 50 blogs as positive examples and 200 blogs randomly chosen from the other topics as negative examples.", "labels": [], "entities": []}, {"text": "For the multi-class experiment, we use the 350 blogs corresponding to the 7 categories.", "labels": [], "entities": []}, {"text": "Both the blogs and the Wikipedia articles were tagged using the Stanford Named Entity Recognizer (), which labels the entities according to these types: Time, Location, Organization, Person, Money, Percent, Date, and Miscellaneous.", "labels": [], "entities": [{"text": "Stanford Named Entity Recognizer", "start_pos": 64, "end_pos": 96, "type": "DATASET", "confidence": 0.8620736598968506}]}, {"text": "After several tests, we found that Location, Organization, Person and Miscellaneous were the most useful for topic classification, and we thus ignored the rest for the results presented here.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.8156300187110901}]}, {"text": "As mentioned above, we use only the named entities in both the training and test data, which, in our experiments, consisted of 14,995 unique entities.", "labels": [], "entities": []}, {"text": "We performed all our tests using the Weka suite (), and we tested the following classifiers.", "labels": [], "entities": [{"text": "Weka suite", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.9663316011428833}]}, {"text": "The first was the Naive Bayes (John and Langley, 1995) (NB for short), which has been successfully applied to text classification problems (.", "labels": [], "entities": [{"text": "Naive Bayes (John and Langley, 1995) (NB", "start_pos": 18, "end_pos": 58, "type": "DATASET", "confidence": 0.5854349082166498}, {"text": "text classification", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.8428698182106018}]}, {"text": "It assumes attribute independence, which makes learning simpler when the number of attributes is large.", "labels": [], "entities": []}, {"text": "A variation of the NB classifier, called Naive Bayes Multinomial (NBM), was also tested, as it was shown to perform better for text classification tasks in which the vocabulary is large (as in our case).", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 127, "end_pos": 152, "type": "TASK", "confidence": 0.8270487586657206}]}, {"text": "Finally, we also used the LibSVM classifier (Chang Undergraduate students in our lab.).", "labels": [], "entities": []}, {"text": "These classifiers were chosen specifically due to their success rate with text classification as well as with other applications of distant supervision.", "labels": [], "entities": [{"text": "text classification", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.8494151830673218}]}, {"text": "We now present our experimental results, starting with the multi-class task, in which the goal is to classify each post into one of 7 possible classes (as in).", "labels": [], "entities": []}, {"text": "Accuracy in the Multi-class Task We report accuracy numbers both for 10-fold cross validation (on the training data) as well as on the manually labelled blog posts (test data).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9811645746231079}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9984925985336304}]}, {"text": "The summary of results is given in.", "labels": [], "entities": []}, {"text": "Accuracy as high as 60% was obtained using the NBM classifier.", "labels": [], "entities": [{"text": "NBM classifier", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9372835159301758}]}, {"text": "The standard NB technique performed quite poorly in this case; as expected, NBM outperformed NB by a factor of almost two, using the count representation.", "labels": [], "entities": []}, {"text": "Overall, the count representation produced better results than in-out on the test data, while losing on the cross-validation tests.", "labels": [], "entities": []}, {"text": "Surprisingly, SVM performed very poorly in our tests.", "labels": [], "entities": [{"text": "SVM", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.6797178983688354}]}, {"text": "These results were not as high as expected, so 32 In-Out   we inspected why that was the case.", "labels": [], "entities": []}, {"text": "What we found was that the classifiers were strongly biased towards the travel topic: NB, for instance, classified 211/350=60% of the samples that way, instead of the expected 14% (50/350).", "labels": [], "entities": [{"text": "NB", "start_pos": 86, "end_pos": 88, "type": "DATASET", "confidence": 0.8540135622024536}]}, {"text": "In the case of SVM, this effect was more pronounced: 88% of the posts were classified as travel.", "labels": [], "entities": [{"text": "SVM", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.6583023071289062}]}, {"text": "shows the confusion matrix for the worst results in our tests (SVM with in-out feature representation), and fully illustrates the point.", "labels": [], "entities": []}, {"text": "We then repeated the tests after removing the travel topic, resulting in an increase inaccuracy of about 5%, as shown in.", "labels": [], "entities": [{"text": "inaccuracy", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9959676265716553}]}, {"text": "However, another inspection at the confusion matrices in this case revealed that the food & drink class received a disproportionate number of classifications.", "labels": [], "entities": []}, {"text": "The highest accuracy numbers we obtained for the multi-class setting were when we further removed the food & drink class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9992673993110657}]}, {"text": "Consistent with previous results, our highest accuracy was achieved with NBM using the count feature representation: 69%..", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9994939565658569}, {"text": "NBM", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.9053088426589966}]}, {"text": "gives the confusion matrix for this task, using NB.", "labels": [], "entities": []}, {"text": "We can see that the posts are much better distributed now than in the previous cases, approximating the ideal confusion matrix which would have only non-zero entries in the diagonal, signifying all instances were correctly classified.", "labels": [], "entities": []}, {"text": "Accuracy (or precision, as used in information retrieval) measures the fraction of correct answers among those provided by the classifier.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9917638301849365}, {"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.7837986350059509}]}, {"text": "A complementary performance metric is recall, which indicates the fraction of correctly classified instances out of the total instances of the class.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.999428927898407}]}, {"text": "shows the breakdown of precision and recall for each class using the NBM classifier, using the Count feature representation for the tests with all 7 classes (a), as well as after removing travel (b) and both travel and food&drink (c).", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9971924424171448}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9982953667640686}, {"text": "NBM classifier", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.8908376097679138}, {"text": "Count", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9931249022483826}]}, {"text": "As one can see, the overall accuracy by class does change (and improves) as we remove travel and then food&drink.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9964469075202942}]}, {"text": "However, the most significant change is for the class other.", "labels": [], "entities": []}, {"text": "On the other hand, both the accuracy and recall for celebrities, religion and sports remain virtually unchanged with the removal of these classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9996339082717896}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9995442032814026}]}, {"text": "One clear conclusion from our tests is the superiority of NBM using Count features for this task.", "labels": [], "entities": []}, {"text": "The margin of this superiority comes somewhat as a surprise in some cases, especially when one compares against SVM, but does not leave much room 33 for argument.", "labels": [], "entities": [{"text": "margin", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.989108681678772}, {"text": "SVM", "start_pos": 112, "end_pos": 115, "type": "DATASET", "confidence": 0.6768369078636169}]}, {"text": "As expected, some classes are much easier to handle than others.", "labels": [], "entities": []}, {"text": "Classes such as celebrities are expected to be hard as documents in this topic deal with everything about the celebrities, including their preferences in politics, sports, the food they like and the places they travel.", "labels": [], "entities": []}, {"text": "Looking at, one possible factor for the relatively lower performance for travel and food & drink could be that the training data in these categories have the lowest average word count and entity count (recall).", "labels": [], "entities": [{"text": "entity count", "start_pos": 188, "end_pos": 200, "type": "METRIC", "confidence": 0.8772359192371368}, {"text": "recall", "start_pos": 202, "end_pos": 208, "type": "METRIC", "confidence": 0.6913093328475952}]}, {"text": "Another category with relatively less counts is celebrities, which can also be explained by the lower document count (1,605 available articles relating to this topic in Freebase).", "labels": [], "entities": []}, {"text": "Another plausible explanation is that articles in some classes can often be classified in either topic.", "labels": [], "entities": []}, {"text": "Articles in the travel topic can include information about many things that can be done and seen around the world, such as the culinary traits of the places being discussed and the celebrities that visited them, or the religious figures that represent them.", "labels": [], "entities": []}, {"text": "Thus, one would expect some overlap among the named entities relating to these less well-defined classes.", "labels": [], "entities": []}, {"text": "These concepts tie easily into the various other topic categories we have considered and help to explain why misclassification was higher for these cases.", "labels": [], "entities": []}, {"text": "We also observed that with the NBM results, in all three variations of the multi-class experiments, there was a fairly consistent trade-off between recall and precision for the celebrities class.", "labels": [], "entities": [{"text": "NBM", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.8052725791931152}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9992026686668396}, {"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9984153509140015}]}, {"text": "The erroneous classification of posts into celebrities could be explained in a similar way to those in food&travel.", "labels": [], "entities": []}, {"text": "The fact that celebrities can exist in sports, politics, and religion means that many of the posts may fit into two or more classes and explains the errors.", "labels": [], "entities": []}, {"text": "The best way to explore this further would be to do multiple class labels per post rather than just choosing a single label.", "labels": [], "entities": []}, {"text": "One interesting point that supports is the following.", "labels": [], "entities": []}, {"text": "Recall that the need for the class other is mostly to test whether the classifier can handle \"noise\" (blogs which are too general to be classified).", "labels": [], "entities": []}, {"text": "With this in mind, the trend in (increasing classification performance as classes are removed) is encouraging, as it indicates that more focused classes (e.g., religion and sports) can actually be separated well by a classifier using distant supervision, even in the presence of less well-defined classes.", "labels": [], "entities": []}, {"text": "Indeed, taken to the extreme, this argument would suggest that the performance in the binary classification scenario for such classes would be the highest (which is indeed the case as we discuss next).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Topic categories chosen from Freebase do- mains", "labels": [], "entities": [{"text": "Freebase do- mains", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.9354119896888733}]}, {"text": " Table 2: Average word count and entity count per blog post and per Wikipedia article.", "labels": [], "entities": [{"text": "Average word count", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.8238363067309061}]}, {"text": " Table 3: Summary of Accuracy on Multi-Class Data", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9728448987007141}]}, {"text": " Table 4: Confusion Matrix of SVM on Test Set with  In-Out Rep.", "labels": [], "entities": []}, {"text": " Table 5: Summary of Accuracy on Multi-Class with- out Travel", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9951542615890503}]}, {"text": " Table 6: Confusion Matrix of NB on Test Set with In- Out Rep", "labels": [], "entities": []}, {"text": " Table 7: Summary of Accuracy on Multi-Class sans Travel, Food", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9766495227813721}]}, {"text": " Table 8: Accuracy of Binary Classification.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9305851459503174}, {"text": "Binary Classification", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.8504660129547119}]}]}