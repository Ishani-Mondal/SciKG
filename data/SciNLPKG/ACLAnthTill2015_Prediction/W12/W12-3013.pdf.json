{"title": [{"text": "Evaluating the Quality of a Knowledge Base Populated from Text", "labels": [], "entities": []}], "abstractContent": [{"text": "The steady progress of information extraction systems has been helped by sound methodolo-gies for evaluating their performance in controlled experiments.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.8186920881271362}]}, {"text": "Annual events like MUC, ACE and TAC have developed evaluation approaches enabling researchers to score and rank their systems relative to reference results.", "labels": [], "entities": [{"text": "MUC", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.8904041647911072}, {"text": "ACE", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.7426304221153259}, {"text": "TAC", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.5054371953010559}]}, {"text": "Yet these evaluations have only assessed component technologies needed by a knowledge base population system; none has required the construction of a knowledge base that is then evaluated directly.", "labels": [], "entities": []}, {"text": "We describe an approach to the direct evaluation of a knowledge base and an instantiation that will be used in a 2012 TAC Knowledge Base Population track.", "labels": [], "entities": [{"text": "TAC Knowledge Base Population track", "start_pos": 118, "end_pos": 153, "type": "DATASET", "confidence": 0.8485826253890991}]}], "introductionContent": [{"text": "Many activities might fall under the rubric of automatic knowledge base (KB) generation, including information extraction, entity linking, open information extraction and machine reading.", "labels": [], "entities": [{"text": "automatic knowledge base (KB) generation", "start_pos": 47, "end_pos": 87, "type": "TASK", "confidence": 0.6692413772855487}, {"text": "information extraction", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.8191783428192139}, {"text": "entity linking", "start_pos": 123, "end_pos": 137, "type": "TASK", "confidence": 0.7989630997180939}, {"text": "open information extraction", "start_pos": 139, "end_pos": 166, "type": "TASK", "confidence": 0.6184315085411072}, {"text": "machine reading", "start_pos": 171, "end_pos": 186, "type": "TASK", "confidence": 0.7954352796077728}]}, {"text": "The task is broad and challenging: process a large text corpus to extract a KB schema or ontology and populate it with entities, relations and facts.", "labels": [], "entities": []}, {"text": "The term knowledge base population (KBP) is often used for the narrower task in which we start with a predefined and fixed KB schema or ontology and focus on the problem of extracting information from a text corpus to populate the KB with entities, relations and facts using that ontology.", "labels": [], "entities": []}, {"text": "To evaluate progress on such systems, we must answer the question \"how do you know that the knowledge base you built is any good?\"", "labels": [], "entities": []}, {"text": "Before we can say whether an automatically created knowledge base is good, we must first say what a knowledge base is.", "labels": [], "entities": []}, {"text": "We define a knowledge base as a combination of four things: a database of facts; a descriptive schema for those facts; a collection of existing background knowledge; and inference capability.", "labels": [], "entities": []}, {"text": "We are concerned in this paper primarily with knowledge bases that use a known schema.", "labels": [], "entities": []}, {"text": "Some of the work in open information extraction addresses the question of how a knowledge schema could be derived from text.", "labels": [], "entities": [{"text": "open information extraction", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.6666365265846252}]}, {"text": "While this is important work, it nonetheless falls outside the scope of our current inquiry.", "labels": [], "entities": []}, {"text": "We seek to assess whether a KB populated according to a known schema accurately encodes the knowledge sources used to create it.", "labels": [], "entities": []}, {"text": "These underlying knowledge sources might be structured (e.g., a database), semi-structured (e.g., Wikipedia Infoboxes), or entirely unstructured (e.g., free text).", "labels": [], "entities": []}, {"text": "We also do not wish to directly evaluate the breadth or accuracy of the KB's background knowledge.", "labels": [], "entities": [{"text": "breadth", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9986031651496887}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9779266715049744}]}, {"text": "Our proposed approach can be used to evaluate the KB's inferencing ability; however, for the current study, we require that the KB materialize all of the relevant facts it can infer.", "labels": [], "entities": []}, {"text": "We also require that the KB justify, where appropriate, the sources (e.g., a document) from which each fact is derived.", "labels": [], "entities": []}, {"text": "Our evaluation approach is characterized by three design decisions.", "labels": [], "entities": []}, {"text": "First, we require that KBs be submitted in a simple abstract format that we use to create an equivalent KB in RDF.", "labels": [], "entities": []}, {"text": "This gives us a well defined and relatively simple KB that can be tested with mature software tools.", "labels": [], "entities": []}, {"text": "Second, instead of assessing the entire KB, the evaluation samples the KB through a set of queries on the RDF KB; each query result is then assessed for correctness.", "labels": [], "entities": [{"text": "RDF KB", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.9254453480243683}]}, {"text": "Third, we do not assume an initial set of KB entities with predefined identifiers.", "labels": [], "entities": []}, {"text": "We avoid the complexity of aligning entities in the KB and reference model by using the concept of a KB entry point specified by an entity mention in an input document.", "labels": [], "entities": []}, {"text": "In the next section we discuss the general problem of KB evaluation and present a concrete proposal for 68 evaluating a KB constructed from text, which will be implemented at the TAC 2012 evaluation.", "labels": [], "entities": [{"text": "KB evaluation", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.9161947965621948}, {"text": "TAC 2012 evaluation", "start_pos": 179, "end_pos": 198, "type": "DATASET", "confidence": 0.9051937063535055}]}, {"text": "introduced the problem of direct evaluation of an automatically populated knowledge base and identified six axes along which they might be evaluated: accuracy, usefulness, augmentation, explanation, adaptation and temporal qualification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9982726573944092}]}, {"text": "In this paper we begin by asking the most elementary of those questions: how accurate is a given static knowledge base?", "labels": [], "entities": []}, {"text": "Accuracy has two components, which correspond to the ideas of recall and precision in information retrieval.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9665150046348572}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9975007176399231}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9979459643363953}]}, {"text": "First, we would like to know whether all of the facts present in or implied by the underlying sources can be retrieved from the KB.", "labels": [], "entities": []}, {"text": "Second, are there facts that are not present in or implied by the underlying sources that can nonetheless be retrieved.", "labels": [], "entities": []}, {"text": "If all and only the implied facts can be retrieved, we can conclude that the knowledge base accurately reflects those sources.", "labels": [], "entities": []}, {"text": "The central tenet of our evaluation approach is that the KB should be judged based on its responses to direct queries about its content.", "labels": [], "entities": []}, {"text": "We call such queries evaluation queries.", "labels": [], "entities": []}], "datasetContent": [{"text": "In practice, it will not be possible to examine all of the possible facts that should be present in a knowledge base unless the underlying sources are extremely small.", "labels": [], "entities": []}, {"text": "Even for relatively small KBs, a complete comparison fora moderately expressive representation language like OWL DL is a complex task ().", "labels": [], "entities": []}, {"text": "We believe that an approach using sampling of the space of possible queries is therefore a pragmatic necessity.", "labels": [], "entities": []}, {"text": "A central problem in evaluating a KB is aligning the entities in the KB with known ground truth.", "labels": [], "entities": []}, {"text": "For example, if we had a reference ground truth KB, we could try to evaluate the created KB by aligning the nodes of the two KBs, then looking for structural differences.", "labels": [], "entities": []}, {"text": "Aligning entities is a complex task that, in the worst case, can have exponential complexity in the number of entities involved.", "labels": [], "entities": [{"text": "Aligning entities", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9193248748779297}]}, {"text": "Our approach to avoiding this problem is to use known entry points into the KB that are defined by a document and an entity mention string.", "labels": [], "entities": []}, {"text": "For example, an entry point could be defined as \"the entity that is associated with the mention Bart Simpson in document DO14.\"", "labels": [], "entities": [{"text": "mention Bart Simpson in document DO14", "start_pos": 88, "end_pos": 125, "type": "DATASET", "confidence": 0.7249251405398051}]}, {"text": "We require that a set of entry points is aligned with the KB by the KB constructor.", "labels": [], "entities": []}, {"text": "In practice this is easy if the KB is being constructed from the text that contains the entry point mentions.", "labels": [], "entities": []}, {"text": "Different classes of evaluation queries can assess different capabilities.", "labels": [], "entities": []}, {"text": "For example, asking whether two entry points refer to same KB node evaluates coreference resolution (or entity linking if one of the entry points is an existing KB node).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.9602755606174469}, {"text": "entity linking", "start_pos": 104, "end_pos": 118, "type": "TASK", "confidence": 0.7141814231872559}]}, {"text": "Asking facts about the KB node associated with a single entry point evaluates simple slot-filling.", "labels": [], "entities": []}, {"text": "More complicated queries that start with one or more entry points can be used to evaluate the overall result of the extraction process involving entity linking, fact extraction, appropriate priors and inference.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 145, "end_pos": 159, "type": "TASK", "confidence": 0.718962550163269}, {"text": "fact extraction", "start_pos": 161, "end_pos": 176, "type": "TASK", "confidence": 0.7592113018035889}]}, {"text": "Note that this approach to KB evaluation is agnostic toward inference.", "labels": [], "entities": [{"text": "KB evaluation", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9208859801292419}]}, {"text": "That is, the original KB system may perform sophisticated backward chaining inference or no inference at all; the evaluation mechanism works the same either way.", "labels": [], "entities": []}, {"text": "We have defined a simplified graph path notation for evaluation queries to make constructing them easier; this notation is then automatically compiled into corresponding SPARQL queries.", "labels": [], "entities": []}, {"text": "For example, one pattern starts with an entry point (a mention in a document) and continues with a sequence of properties.", "labels": [], "entities": []}, {"text": "The general form of such a path expression is M DP 1 ...P n where M is a mention string, Dis a document identifier, and each P i is a property from the target ontology.", "labels": [], "entities": []}, {"text": "All of the properties in the path except the final one must go from entities to entities.", "labels": [], "entities": []}, {"text": "The final one can have a range that is either an entity or a string.", "labels": [], "entities": []}, {"text": "For example, to generate a query for \"The ages of the siblings of the entity mentioned as 70 \"Bart Simpson\" in document D012\" we use the path expression \"Bart Simpson\" D012 sibling age.", "labels": [], "entities": [{"text": "Bart Simpson\" in document D012", "start_pos": 94, "end_pos": 124, "type": "DATASET", "confidence": 0.9082250992457072}, {"text": "Bart Simpson\" D012 sibling age", "start_pos": 154, "end_pos": 184, "type": "DATASET", "confidence": 0.8122380375862122}]}, {"text": "This lets an assessor verify that the correct entities are identified and that there is explicit support for the slot values.", "labels": [], "entities": []}], "tableCaptions": []}