{"title": [{"text": "Neural Probabilistic Language Model for System Combination", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper gives the system description of the neural probabilistic language modeling (NPLM) team of Dublin City University for our participation in the system combination task in the Second Workshop on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid MT (ML4HMT-12).", "labels": [], "entities": [{"text": "neural probabilistic language modeling (NPLM)", "start_pos": 47, "end_pos": 92, "type": "TASK", "confidence": 0.7852822882788522}, {"text": "Dublin City University", "start_pos": 101, "end_pos": 123, "type": "DATASET", "confidence": 0.9634193579355875}, {"text": "Division of Labour in Hybrid MT (ML4HMT-12)", "start_pos": 256, "end_pos": 299, "type": "TASK", "confidence": 0.6854220959875319}]}, {"text": "We used the information obtained by NPLM as meta information to the system combination module.", "labels": [], "entities": [{"text": "NPLM", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.852527916431427}]}, {"text": "For the Spanish-English data, our paraphrasing approach achieved 25.81 BLEU points, which lost 0.19 BLEU points absolute compared to the standard confusion network-based system combination.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9991521835327148}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9985424280166626}]}, {"text": "We note that our current usage of NPLM is very limited due to the difficulty in combining NPLM and system combination.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes anew extension to our system combination module developed in Dublin City University (.", "labels": [], "entities": [{"text": "Dublin City University", "start_pos": 82, "end_pos": 104, "type": "DATASET", "confidence": 0.9842473069826762}]}, {"text": "We have added a neural probabilistic language model (NPLM) () to our system combination module and tested it in the system combination task at the ML4HMT-2012 workshop.", "labels": [], "entities": []}, {"text": "A neural probabilistic language model (NPLM) () and the distributed representations ( provide an idea to achieve the better perplexity than ngram language model and their smoothed language models).", "labels": [], "entities": []}, {"text": "Recently, the latter one, i.e. smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM)) and Sequence Memoizer (SM) (, including an application to SMT.", "labels": [], "entities": [{"text": "Sequence Memoizer (SM)", "start_pos": 192, "end_pos": 214, "type": "TASK", "confidence": 0.667487770318985}, {"text": "SMT", "start_pos": 246, "end_pos": 249, "type": "TASK", "confidence": 0.9695556163787842}]}, {"text": "A NPLM considers the representation of data in order to make the probability distribution of word sequences more compact where we focus on the similar semantical and syntactical roles of words.", "labels": [], "entities": []}, {"text": "For example, when we have two sentences \"The cat is walking in the bedroom\" and \"A dog was running in a room\", these sentences can be more compactly stored than the n-gram language model if we focus on the similarity between (the, a), (bedroom, room), (is, was), and (running, walking).", "labels": [], "entities": []}, {"text": "Thus, a NPLM provides the semantical and syntactical roles of words as a language model.", "labels": [], "entities": []}, {"text": "A NPLM of implemented this using the multi-layer neural network and yielded 20% to 35% better perplexity than the language model with the modified Kneser-Ney methods).", "labels": [], "entities": []}, {"text": "There are several successful applications of NPLM).", "labels": [], "entities": []}, {"text": "First, one category of applications include POS tagging, NER tagging, and parsing).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.7644944190979004}, {"text": "NER tagging", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.8337545096874237}, {"text": "parsing", "start_pos": 74, "end_pos": 81, "type": "TASK", "confidence": 0.9699585437774658}]}, {"text": "This category uses the features provided by a NPLM in the limited window size.", "labels": [], "entities": []}, {"text": "1 It is often the case that there is no such long range effects that the decision cannot be made beyond the limited windows which requires to look carefully the elements in along distance.", "labels": [], "entities": []}, {"text": "Second, the other category of applications include Semantic Role Labeling (SRL) task.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL) task", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.8266449059758868}]}, {"text": "This category uses the features within a sentence.", "labels": [], "entities": []}, {"text": "A typical element is the predicate in a SRL task which requires the information which sometimes in along distance but within a sentence.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 40, "end_pos": 48, "type": "TASK", "confidence": 0.9298549294471741}]}, {"text": "Both of these approaches do not require to obtain the best tag sequence, but these tags are independent.", "labels": [], "entities": []}, {"text": "Third, the final category includes MERT process and possibly many others where most of them remain undeveloped.", "labels": [], "entities": [{"text": "MERT", "start_pos": 35, "end_pos": 39, "type": "TASK", "confidence": 0.5771059393882751}]}, {"text": "The objective of this learning in this category is not to search the best tag fora word but the best sequence fora sentence.", "labels": [], "entities": []}, {"text": "Hence, we need to apply the sequential learning approach.", "labels": [], "entities": []}, {"text": "Although most of the applications described in) are monolingual tasks, the application of this approach to a bilingual task introduces really astonishing aspects, which we can call \"creative words\", automatically into the traditional resource constrained SMT components.", "labels": [], "entities": [{"text": "SMT", "start_pos": 255, "end_pos": 258, "type": "TASK", "confidence": 0.9670876264572144}]}, {"text": "For example, the training corpus of word aligner is often strictly restricted to the given parallel corpus.", "labels": [], "entities": []}, {"text": "However, a NPLM allows this training with huge monolingual corpus.", "labels": [], "entities": []}, {"text": "Although most of this line has not been even tested mostly due to the problem of computational complexity of training NPLM, applied this to MERT process which reranks the n-best lists using NPLM.", "labels": [], "entities": []}, {"text": "This paper aims at different task, a task of system combination (.", "labels": [], "entities": []}, {"text": "This category of tasks employs the sequential method such as Maximum A Posteriori (MAP) inference (Viterbi decoding) on Conditional Random Fields (CRFs) / Markov Random Fields (MRFs).", "labels": [], "entities": [{"text": "Maximum A Posteriori (MAP)", "start_pos": 61, "end_pos": 87, "type": "METRIC", "confidence": 0.7905991325775782}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our algorithms.", "labels": [], "entities": []}, {"text": "In Section 3, our experimental results are presented.", "labels": [], "entities": []}, {"text": "We conclude in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "ML4HMT-2012 provides four translation outputs (s1 to s4) which are MT outputs by two RBMT systems, APERTIUM and LUCY, PB-SMT (MOSES) and HPB-SMT (MOSES), respectively.", "labels": [], "entities": [{"text": "ML4HMT-2012", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8959860801696777}, {"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9644311666488647}, {"text": "APERTIUM", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9861030578613281}, {"text": "LUCY", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.8853234648704529}]}, {"text": "The tuning data consists of 20,000 sentence pairs, while the test data consists of 3,003 sentence pairs.", "labels": [], "entities": []}, {"text": "Our experimental setting is as follows.", "labels": [], "entities": []}, {"text": "We use our system combination module (Du and Way, 2010a,b; Okita and van Genabith, 2012), which has its own language modeling tool, MERT process, and MBR decoding.", "labels": [], "entities": [{"text": "MERT", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.6779240369796753}]}, {"text": "We use the BLEU metric as loss function in MBR decoding.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.9752911627292633}]}, {"text": "We use TERP 4 as alignment metrics in monolingual word alignment.", "labels": [], "entities": [{"text": "monolingual word alignment", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6522284944852194}]}, {"text": "The second algorithm, NPLM dep, achieved slightly better results of 25.81 BLEU points, which lost 0.19 BLEU points absolute over the standard system combination.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9986475110054016}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9977589845657349}]}, {"text": "The four lines at the bottom marked with average s1 to s4 indicates the average performance of s1 in terms of precision, recall, and F-score (from the 2nd to 4th columns) when we make the backbone by choosing the maximum score in terms of the modified dependency score.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9995630383491516}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9989284873008728}, {"text": "F-score", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9992249011993408}]}, {"text": "For example, the first line of \"modDep precision\" shows when we chose a backbone by the maximum modified dependency score in terms of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9146780967712402}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9969283938407898}]}, {"text": "586 sentences were selected from s1, 710 sentences were from s2, and so forth.", "labels": [], "entities": []}, {"text": "The average BLEU score of these 586 sentences was 24.4.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9722507297992706}]}], "tableCaptions": [{"text": " Table 1: An example of modified dependency score for a set of translation outputs.", "labels": [], "entities": []}, {"text": " Table 3: This table shows single best performance, the performance of two algorithms in  this paper (NPLM plain and dep), MBR-decoding with BLEU loss function and TER loss  function, the performance of domain adaptation (Okita et al., 2012b) and quality estimation  (", "labels": [], "entities": [{"text": "MBR-decoding", "start_pos": 123, "end_pos": 135, "type": "METRIC", "confidence": 0.7265116572380066}, {"text": "BLEU loss function", "start_pos": 141, "end_pos": 159, "type": "METRIC", "confidence": 0.9760465025901794}, {"text": "TER loss  function", "start_pos": 164, "end_pos": 182, "type": "METRIC", "confidence": 0.9763909180959066}, {"text": "domain adaptation", "start_pos": 203, "end_pos": 220, "type": "TASK", "confidence": 0.7783212959766388}]}]}