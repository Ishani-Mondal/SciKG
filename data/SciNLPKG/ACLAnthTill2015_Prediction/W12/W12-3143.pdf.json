{"title": [{"text": "PROMT DeepHybrid system for WMT12 shared translation task", "labels": [], "entities": [{"text": "PROMT DeepHybrid", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.4874280095100403}, {"text": "WMT12 shared translation", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.7307146787643433}]}], "abstractContent": [{"text": "This paper describes the PROMT submission for the WMT12 shared translation task.", "labels": [], "entities": [{"text": "PROMT submission", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7362695634365082}, {"text": "WMT12 shared translation task", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.7953537404537201}]}, {"text": "We participated in two language pairs: English-French and English-Spanish.", "labels": [], "entities": []}, {"text": "The translations were made using the PROMT DeepHybrid engine, which is the first hybrid version of the PROMT system.", "labels": [], "entities": [{"text": "translations", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9711624383926392}, {"text": "PROMT DeepHybrid engine", "start_pos": 37, "end_pos": 60, "type": "DATASET", "confidence": 0.8127925594647726}]}, {"text": "We report on improvements over our baseline RBMT output both in terms of automatic evaluation metrics and linguistic analysis.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 44, "end_pos": 48, "type": "TASK", "confidence": 0.94544517993927}]}], "introductionContent": [{"text": "In this paper we present the PROMT DeepHybrid submission for WMT12 shared translation task for two language pairs: English-French and English-Spanish.", "labels": [], "entities": [{"text": "PROMT DeepHybrid submission", "start_pos": 29, "end_pos": 56, "type": "DATASET", "confidence": 0.6644485592842102}, {"text": "WMT12 shared translation task", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.7329045832157135}]}, {"text": "A common approach to create hybrid machine translation (MT) systems on the basis of rule-based machine translation (RBMT) systems is to build a statistical phrase-based post-editing (SPE) system using state-of-the-art SMT technologies (see).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.8796727657318115}, {"text": "rule-based machine translation (RBMT)", "start_pos": 84, "end_pos": 121, "type": "TASK", "confidence": 0.8044145305951437}, {"text": "SMT", "start_pos": 218, "end_pos": 221, "type": "TASK", "confidence": 0.9700528979301453}]}, {"text": "An SPE system views the output of the RBMT system as the source language, and reference human translations as the target language.", "labels": [], "entities": []}, {"text": "SPE systems are used to correct typical mistakes of the RBMT output and to adapt RBMT systems to specific domains.", "labels": [], "entities": []}, {"text": "() report on good results both in terms of automatic evaluation metrics and human evaluation for the SPE systems based on PORTAGE () and Moses ().", "labels": [], "entities": [{"text": "SPE", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.8126371502876282}, {"text": "PORTAGE", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.8021330237388611}]}, {"text": "However, an SMT model in fact makes translation output less predictable in comparison with RBMT output.", "labels": [], "entities": [{"text": "SMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9871009588241577}]}, {"text": "We propose a different approach to hybrid MT technology.", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9703844785690308}]}, {"text": "We developed and incorporated the SPE component into our translation system (the statistical post-editing data is controlled by the PROMT hybrid translation engine).", "labels": [], "entities": [{"text": "PROMT hybrid translation", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.59076060851415}]}, {"text": "Besides, we have an internal language model (LM) component that scores the generated translation candidates.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: in section 2 we provide the detailed description of our hybrid MT technology.", "labels": [], "entities": [{"text": "MT", "start_pos": 115, "end_pos": 117, "type": "TASK", "confidence": 0.9885289072990417}]}, {"text": "In section 3 we evaluate the performance of the technology on two language pairs: English-French and EnglishSpanish.", "labels": [], "entities": []}, {"text": "We gain improvements over the baseline RBMT system in terms of BLEU score on test sets.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.41602060198783875}, {"text": "BLEU score", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9795785844326019}]}, {"text": "We also introduce the results of linguistic evaluation performed by our experts.", "labels": [], "entities": []}, {"text": "Section 4 summarizes the key findings and outlines open issues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the total Europarliament (EP) and NewsCommentary (NC) corpora provided by the organizers for the English-Spanish submission.", "labels": [], "entities": [{"text": "Europarliament (EP)", "start_pos": 18, "end_pos": 37, "type": "DATASET", "confidence": 0.8998513370752335}, {"text": "NewsCommentary (NC) corpora", "start_pos": 42, "end_pos": 69, "type": "DATASET", "confidence": 0.7802287340164185}]}, {"text": "We translated both (EP and NC) corpora using the RBMT engine and then built a single phrase-table for both corpora.", "labels": [], "entities": [{"text": "RBMT engine", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.9375390708446503}]}, {"text": "Then we filtered the phrase-table according to the source phrase length and transla- tion probabilities as described in section 2.4.", "labels": [], "entities": []}, {"text": "Only 10% of the initial phrase-table were used as statistical post-editing data.", "labels": [], "entities": []}, {"text": "The target 5-gram language model was trained on all provided monolingual data except the LDC corpora.", "labels": [], "entities": []}, {"text": "We did not extract the dictionary for this language pair.", "labels": [], "entities": []}, {"text": "As for the English-French submission, we performed bilingual training data selection from EP and United Nations (UN) corpora.", "labels": [], "entities": []}, {"text": "We trained the source and target language models on English and French monolingual News corpora respectively.", "labels": [], "entities": []}, {"text": "These models were used to score each sentence pair of EP and UN corpora.", "labels": [], "entities": []}, {"text": "Then we selected sentence pairs from EP and UN corpora via the geometric mean of perplexities of the source and target sentences.", "labels": [], "entities": []}, {"text": "About 85% of EP (35M words of the source corpus) and 35% of UN (68M words of the source corpus) were selected.", "labels": [], "entities": [{"text": "UN", "start_pos": 60, "end_pos": 62, "type": "DATASET", "confidence": 0.5941228270530701}]}, {"text": "Then we translated the selected EP and UN subcorpora and the whole NC corpus with the RBMT engine.", "labels": [], "entities": [{"text": "EP and UN subcorpora", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.7559390962123871}, {"text": "NC corpus", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.9479208588600159}, {"text": "RBMT engine", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.9305353164672852}]}, {"text": "A single phrase-table was built for all three corpora.", "labels": [], "entities": []}, {"text": "The phrase-table was fitered with the same parameters as for the English-Spanish submission.", "labels": [], "entities": []}, {"text": "Approximately 8% of the initial phrase-table were used as statistical post-editing data.", "labels": [], "entities": []}, {"text": "The target 5-gram language model was trained on all provided monolingual data except the LDC corpora.", "labels": [], "entities": []}, {"text": "We also performed automated dictionary extraction for the English-French pair.", "labels": [], "entities": [{"text": "dictionary extraction", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.746412456035614}]}, {"text": "Examples of the extracted entries can be found in.", "labels": [], "entities": []}, {"text": "The details about the extracted dictionary can be found in.", "labels": [], "entities": []}, {"text": "We only extracted verbs, nouns and noun phrases for this shared task.", "labels": [], "entities": []}, {"text": "The translations for extracted verbs and nouns are automatically added into the existing PROMT dictionary entries using our multifunctional dictionary component.", "labels": [], "entities": [{"text": "PROMT dictionary entries", "start_pos": 89, "end_pos": 113, "type": "DATASET", "confidence": 0.8904034694035848}]}, {"text": "Thus we increase the number of lexical variants and generated translation candidates.", "labels": [], "entities": []}, {"text": "The extracted noun phrases are added to the PROMT dictionary as new entries.", "labels": [], "entities": [{"text": "PROMT dictionary", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.865420937538147}]}, {"text": "We only extract 'informative' entries, i.e. the noun phrases which are absent in the baseline PROMT dictionary or have an incorrect or infrequent translation.", "labels": [], "entities": [{"text": "PROMT dictionary", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.892705500125885}]}, {"text": "It should also be mentioned that the initial size of the noun phrases glossary was over 25K entries, but we decided to raise the source phrase frequency threshold a bit.", "labels": [], "entities": []}, {"text": "Our hypothesis was that non-frequent phrases from outof-domain corpora (EP and UN) would not fit for translation of news texts.", "labels": [], "entities": [{"text": "translation of news texts", "start_pos": 101, "end_pos": 126, "type": "TASK", "confidence": 0.8831725865602493}]}, {"text": "In this section we present the results of our experiments on newstest2012.", "labels": [], "entities": [{"text": "newstest2012", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.9696993231773376}]}, {"text": "BLEU scores for different system configurations are presented in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9803262948989868}]}, {"text": "The percentage of sentences changed by statistical post-editing compared to baseline RBMT output is presented in provide details of linguistic evaluation performed for the English-French submission..", "labels": [], "entities": []}, {"text": "Impact of statistical post-editing on newstest2012 (percentage of sentences changed by statistical post-editing)..", "labels": [], "entities": []}, {"text": "Statistics on improvements, degradations and equivalents for the DeepHybrid translation compared to baseline RBMT output (newstest2012).", "labels": [], "entities": [{"text": "DeepHybrid translation", "start_pos": 65, "end_pos": 87, "type": "DATASET", "confidence": 0.8090474307537079}, {"text": "newstest2012", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.9189196825027466}]}, {"text": "Our linguists compared 100 random RBMT and DeepHybrid (with extracted dictionary and statistical post-editing) translations for both language pairs in terms of improvements and degradations.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.6497579216957092}]}, {"text": "The results presented in show that the DeepHybrid engine outperforms the RBMT engine according to human evaluation.", "labels": [], "entities": []}, {"text": "Most of the degradations are minor grammatical issues (wrong number, disagreement etc.).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Number of entries in the extracted English- French dictionary.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9597628116607666}, {"text": "English- French dictionary", "start_pos": 45, "end_pos": 71, "type": "DATASET", "confidence": 0.6126591861248016}]}, {"text": " Table 3. Translation results in terms of BLEU score for  newstest2012.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7842293977737427}, {"text": "BLEU score", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9752040803432465}, {"text": "newstest2012", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9303995370864868}]}, {"text": " Table 4. Impact of statistical post-editing on  newstest2012 (percentage of sentences changed by sta- tistical post-editing).", "labels": [], "entities": []}]}