{"title": [{"text": "Parsing the Past -Identification of Verb Constructions in Historical Text", "labels": [], "entities": [{"text": "Parsing the Past -Identification of Verb Constructions", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.8156991302967072}]}], "abstractContent": [{"text": "Even though NLP tools are widely used for contemporary text today, there is alack of tools that can handle historical documents.", "labels": [], "entities": []}, {"text": "Such tools could greatly facilitate the work of researchers dealing with large volumes of historical texts.", "labels": [], "entities": []}, {"text": "In this paper we propose a method for extracting verbs and their complements from historical Swedish text, using NLP tools and dictionaries developed for contemporary Swedish and a set of nor-malisation rules that are applied before tagging and parsing the text.", "labels": [], "entities": [{"text": "extracting verbs and their complements from historical Swedish text", "start_pos": 38, "end_pos": 105, "type": "TASK", "confidence": 0.7998874849743314}, {"text": "tagging and parsing the text", "start_pos": 233, "end_pos": 261, "type": "TASK", "confidence": 0.7002384960651398}]}, {"text": "When evaluated on a sample of texts from the period 1550-1880, this method identifies verbs with an F-score of 77.2% and finds a partially or completely correct set of complements for 55.6% of the verbs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9991410970687866}]}, {"text": "Although these results are in general lower than for contemporary Swedish, they are strong enough to make the approach useful for information extraction in historical research.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.8706744611263275}]}, {"text": "Moreover, the exact match rate for complete verb constructions is in fact higher for historical texts than for contemporary texts (38.7% vs. 30.8%).", "labels": [], "entities": [{"text": "match rate", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9380062222480774}]}], "introductionContent": [{"text": "Today there is an abundance of NLP tools that can analyse contemporary language and extract information relevant to a particular user need, but there is areal lack of tools that can handle historical documents.", "labels": [], "entities": []}, {"text": "Historians and other researchers working with older texts are still mostly forced to manually search large amounts of text in order to find the passages of interest to their research.", "labels": [], "entities": []}, {"text": "Developing tools to facilitate this process is a great challenge, however, as historical texts vary greatly in both spelling and grammar between different authors, genres and time periods, and even within the same text, due to the lack of spelling conventions.", "labels": [], "entities": []}, {"text": "In addition to this, there is a shortage of annotated resources that can be used for the development and evaluation of new tools.", "labels": [], "entities": []}, {"text": "The work presented in this paper has been carried out in cooperation with historians, who are studying what men and women did fora living in the Early Modern Swedish society.", "labels": [], "entities": []}, {"text": "Currently, the historians are manually extracting segments describing work activities from a number of historical texts, and entering them into a database, the Gender and Work Database.", "labels": [], "entities": []}, {"text": "Their work so far has shown that the typical segment describing an activity of work is a verb construction, that is, a verb together with its complements.", "labels": [], "entities": []}, {"text": "(Examples of such segments can be found below in.)", "labels": [], "entities": []}, {"text": "It is very likely that the manual effort and the time needed by the historians to find these segments and enter them into the database could be substantially reduced if verbs and their complements were automatically extracted and presented to the historian.", "labels": [], "entities": []}, {"text": "This would give a general overview of the content of a text, and the task of the historian would be to select those segments that are actually describing work activities.", "labels": [], "entities": []}, {"text": "By linking extracted segments back to larger passages of text, historians would also be able to find additional segments that were missed by the first automatic extraction.", "labels": [], "entities": []}, {"text": "The core of such a system would be a component for identifying verb constructions in running text.", "labels": [], "entities": [{"text": "identifying verb constructions in running text", "start_pos": 51, "end_pos": 97, "type": "TASK", "confidence": 0.8445530633131663}]}, {"text": "We propose a method for automatically identifying verbs and their complements in various types of historical documents, produced in the Early Modern Swedish period (1550-1800).", "labels": [], "entities": [{"text": "automatically identifying verbs and their complements", "start_pos": 24, "end_pos": 77, "type": "TASK", "confidence": 0.6863223761320114}]}, {"text": "The method is based on using existing NLP tools for contemporary Swedish, in particular a part-ofspeech tagger and a syntactic parser, and automatically normalising the input text into a more modern orthography before running it through the tagger and parser.", "labels": [], "entities": []}, {"text": "In order to increase the precision of complement extraction, we use valency dictionaries to filter out unlikely complements in the output of the syntactic parser.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.998584508895874}, {"text": "complement extraction", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.8263325691223145}]}, {"text": "Using this method, we are able to identify verbs with an F-score of 77.2% and find a partially or completely correct set of complements for 55.6% of the verbs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9991087317466736}]}, {"text": "To our knowledge, extracting verb constructions from historical texts is a task that has not been directly addressed in previous research, which means that these results are also important insetting benchmarks for future research.", "labels": [], "entities": [{"text": "extracting verb constructions from historical texts", "start_pos": 18, "end_pos": 69, "type": "TASK", "confidence": 0.8975601295630137}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the method for identification of verbs and complements in more detail.", "labels": [], "entities": [{"text": "identification of verbs and complements", "start_pos": 35, "end_pos": 74, "type": "TASK", "confidence": 0.8791619181632996}]}, {"text": "Section 4 presents the data and evaluation metrics used in our experiments, and Section 5 discusses the results of the evaluation.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to get a more fine-grained picture of the system's performance, we want to evaluate three different aspects: 1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Identification of verbs by tagging. Raw = Un- normalised input text. Norm = Normalisation of input  prior to tagging. SUC = Subset of Stockholm-Ume\u00e5  corpus of contemporary Swedish texts, as described in  section 4.1.", "labels": [], "entities": [{"text": "Norm", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9882028698921204}, {"text": "Stockholm-Ume\u00e5  corpus of contemporary Swedish texts", "start_pos": 144, "end_pos": 196, "type": "DATASET", "confidence": 0.911178340514501}]}, {"text": " Table 3: Identification of complements by parsing.  Raw = Unnormalised input text. Norm = Normalisa- tion of input prior to tagging and parsing. +Valency =  Adding valency filtering to the setting in the preced- ing row. SUC = Subset of Stockholm-Ume\u00e5 corpus of  contemporary Swedish texts, as described in section  4.1.", "labels": [], "entities": [{"text": "Norm", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9896876215934753}, {"text": "Valency", "start_pos": 147, "end_pos": 154, "type": "METRIC", "confidence": 0.9592146277427673}, {"text": "Stockholm-Ume\u00e5 corpus of  contemporary Swedish texts", "start_pos": 238, "end_pos": 290, "type": "DATASET", "confidence": 0.9188456833362579}]}]}