{"title": [{"text": "Large-scale discriminative language model reranking for voice-search", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a distributed framework for large-scale discriminative language models that can be integrated within a large vocabulary continuous speech recognition (LVCSR) system using lattice rescoring.", "labels": [], "entities": [{"text": "large vocabulary continuous speech recognition (LVCSR)", "start_pos": 114, "end_pos": 168, "type": "TASK", "confidence": 0.8057654798030853}]}, {"text": "We intentionally use a weakened acoustic model in a base-line LVCSR system to generate candidate hypotheses for voice-search data; this allows us to utilize large amounts of unsupervised data to train our models.", "labels": [], "entities": []}, {"text": "We propose an efficient and scalable MapReduce framework that uses a perceptron-style distributed training strategy to handle these large amounts of data.", "labels": [], "entities": []}, {"text": "We report small but significant improvements in recognition accuracies on a standard voice-search data set using our discriminative reranking model.", "labels": [], "entities": []}, {"text": "We also provide an analysis of the various parameters of our models including model size, types of features, size of partitions in the MapReduce framework with the help of supporting experiments.", "labels": [], "entities": []}], "introductionContent": [{"text": "The language model is a critical component of an automatic speech recognition (ASR) system that assigns probabilities or scores to word sequences.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 49, "end_pos": 83, "type": "TASK", "confidence": 0.8221354881922404}]}, {"text": "It is typically derived from a large corpus of text via maximum likelihood estimation in conjunction with some smoothing constraints.", "labels": [], "entities": []}, {"text": "N-gram models have become the most dominant form of LMs inmost ASR systems.", "labels": [], "entities": [{"text": "ASR", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9587147235870361}]}, {"text": "Although these models are robust, scalable and easy to build, we illustrate a limitation with the following example from voice-search.", "labels": [], "entities": []}, {"text": "We expect a low probability for an ungrammatical or implausible word sequence.", "labels": [], "entities": []}, {"text": "However, fora trigram like \"a navigate to\", a backoff trigram LM gives a fairly large LM log probability of -0.266 because both \"a\" and \"navigate to\" are popular words in voice-search!", "labels": [], "entities": []}, {"text": "Discriminative language models (DLMs) attempt to directly optimize error rate by rewarding features that appear in low error hypotheses and penalizing features in misrecognized hypotheses.", "labels": [], "entities": []}, {"text": "The trigram \"a navigate to\" receives a fairly large negative weight of -6.5 thus decreasing its chances of appearing as an ASR output.", "labels": [], "entities": [{"text": "ASR", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.8745478391647339}]}, {"text": "There have been numerous approaches towards estimating DLMs for large vocabulary continuous speech recognition (LVCSR);).", "labels": [], "entities": [{"text": "large vocabulary continuous speech recognition", "start_pos": 64, "end_pos": 110, "type": "TASK", "confidence": 0.6375646114349365}]}, {"text": "There are two central issues that we discuss regarding DLMs.", "labels": [], "entities": [{"text": "DLMs", "start_pos": 55, "end_pos": 59, "type": "TASK", "confidence": 0.9465314745903015}]}, {"text": "Firstly, DLM training requires large amounts of parallel data (in the form of correct transcripts and candidate hypotheses output by an ASR system) to be able to effectively compete with ngram LMs trained on large amounts of text.", "labels": [], "entities": []}, {"text": "This data could be simulated using voice-search logs that are confidence-filtered from a baseline ASR system to obtain reference transcripts.", "labels": [], "entities": []}, {"text": "However, this data is perfectly discriminated by first pass features and leaves little room for learning.", "labels": [], "entities": []}, {"text": "We propose a novel training strategy of using lattices generated with a weaker acoustic model (henceforth referred to as weakAM) than the one used to generate reference transcripts for the unsupervised parallel data (referred to as the strongAM).", "labels": [], "entities": []}, {"text": "This provides us with enough errors to derive large numbers of potentially useful word features; this is akin to using a weak LM in discriminative acoustic modeling to give more room for diversity in the word lattices resulting in better generalization).", "labels": [], "entities": []}, {"text": "We conduct experiments to verify whether these weakAMtrained models will provide performance gains on rescoring lattices from a standard test set generated using strongAM (discussed in Section 3.3).", "labels": [], "entities": []}, {"text": "The second issue is that discriminative estimation of LMs is computationally more intensive than regular N-gram LM estimation.", "labels": [], "entities": []}, {"text": "The advent of distributed learning algorithms ( and supporting parallel computing infrastructure like MapReduce () has made it increasingly feasible to use large amounts of parallel data to train DLMs.", "labels": [], "entities": []}, {"text": "We implement a distributed training strategy for the perceptron algorithm (introduced by using the MapReduce framework.", "labels": [], "entities": []}, {"text": "Our design choices for the MapReduce implementation are specified in Section 2.2 along with its modular nature thus enabling us to experiment with different variants of the distributed structured perceptron algorithm.", "labels": [], "entities": []}, {"text": "Some of the descriptions in this paper have been adapted from previous work ().", "labels": [], "entities": []}], "datasetContent": [{"text": "Our DLMs are evaluated in two ways: 1) we extract a development set (weakAM-dev) and a test set (weakAM-test) from the speech data that is redecoded with a weakAM to evaluate our learning setup, and 2) we use a standard voice-search test set (v-search-test)) to evaluate actual ASR performance on voice-search.", "labels": [], "entities": [{"text": "ASR", "start_pos": 278, "end_pos": 281, "type": "TASK", "confidence": 0.9812776446342468}]}, {"text": "More details regarding our experimental setup along with a discussion of our experiments and results are described in the rest of the section.", "labels": [], "entities": []}, {"text": "We generate training lattices using speech data that is re-decoded with a weakAM acoustic model and a baseline language model.", "labels": [], "entities": []}, {"text": "We use maximum likelihood trained single mixture Gaussians for our weakAM.", "labels": [], "entities": []}, {"text": "And, we use a sufficiently small baseline LM (\u223c21 million n-grams) to allow for subreal time lattice generation on the training data with a small memory footprint, without compromising on its strength.", "labels": [], "entities": []}, {"text": "demonstrate that it takes much larger LMs to get a significant relative gain in WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.8603125810623169}]}, {"text": "Our largest models are trained on 87,000 hours of speech, or \u223c350 million words (weakAM-train) obtained by filtering voice-search logs at 0.8 confidence, and re-decoding the speech data with a weakAM to generate N-best lists.", "labels": [], "entities": []}, {"text": "We set aside apart of this weakAM-train data to create weakAM-dev and weakAM-test: these data sets consist of 328,460/316,992 utterances, or 1,182,756/1,129,065 words, respectively.", "labels": [], "entities": []}, {"text": "We use a manually-transcribed, standard voice-search test set (v-search-test)) consisting of 27,273 utterances, or 87,360 words to evaluate actual ASR performance using our weakAM-trained models.", "labels": [], "entities": [{"text": "ASR", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.9893488883972168}]}, {"text": "All voice-search data used in the experiments is anonymized.", "labels": [], "entities": []}, {"text": "shows oracle error rates, both at the sentence and word level, using N-best lists of utterances in weakAM-dev and v-search-test.", "labels": [], "entities": []}, {"text": "These error rates are obtained by choosing the best of the top N hypotheses that is either an exact match (for sentence error rate) or closest in edit distance (for word error rate) to the correct transcript.", "labels": [], "entities": []}, {"text": "The N-best lists for weakAM-dev are generated using a weak AM and N-best lists for v-search-test are generated using the baseline (strong) AM.", "labels": [], "entities": []}, {"text": "shows these error rates plotted against a varying threshold N for the N-best lists.", "labels": [], "entities": []}, {"text": "Note there are sufficient word errors in the weakAM data to train DLMs; also, we observe that the plot flattens out after N=100, thus informing us that N=100 is a reasonable threshold to use when training our DLMs.", "labels": [], "entities": [{"text": "weakAM data", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.903301864862442}]}, {"text": "Experiments in Section 3.2 involve evaluating our learning setup using weakAM-dev/test.", "labels": [], "entities": [{"text": "weakAM-dev/test", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.8256923158963522}]}, {"text": "We then investigate whether improvements on weakAMdev/test translate to v-search-test where N-best are generated using the strongAM, and scored against manual transcripts using fully fledged text normalization instead of the string edit distance used in training the DLM.", "labels": [], "entities": []}, {"text": "More details about the implications of this text normalization on WER can be found in Section 3.3.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7451773285865784}, {"text": "WER", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.5550622344017029}]}], "tableCaptions": [{"text": " Table 1: WERs on weakAM-dev using the baseline 1-best  system, ML-3gram and DLM-1/2/3gram.", "labels": [], "entities": [{"text": "WERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9909953474998474}, {"text": "weakAM-dev", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.9081424474716187}]}, {"text": " Table 2: WERs on weakAM-dev using DLM-3gram,  DLM-4gram and DLM-5gram of six training epochs.", "labels": [], "entities": [{"text": "WERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9926562309265137}, {"text": "weakAM-dev", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.8824224472045898}]}, {"text": " Table 3: WERs on weakAM-test using DLMs of varying  sizes.", "labels": [], "entities": [{"text": "WERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9897193908691406}, {"text": "weakAM-test", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.8677425980567932}]}, {"text": " Table 4: WERs on weakAM-test and v-search-test.", "labels": [], "entities": [{"text": "WERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9975411891937256}, {"text": "weakAM-test", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.8094429969787598}]}]}