{"title": [{"text": "\"Hidden semantics\": what can we learn from the names in an ontology? *", "labels": [], "entities": []}], "abstractContent": [{"text": "Despite their flat, semantics-free structure, on-tology identifiers are often given names or labels corresponding to natural language words or phrases which are very dense with information as to their intended referents.", "labels": [], "entities": []}, {"text": "We argue that by taking advantage of this information density, NLG systems applied to ontologies can guide the choice and construction of sentences to express useful ontological information , solely through the verbalisations of iden-tifier names, and that by doing so, they can replace the extremely fussy and repetitive texts produced by ontology verbalisers with shorter and simpler texts which are clearer and easier for human readers to understand.", "labels": [], "entities": []}, {"text": "We specify which axioms in an ontology are \"defin-ing axioms\" for linguistically-complex identi-fiers and analyse a large corpus of OWL on-tologies to identify common patterns among all defining axioms.", "labels": [], "entities": []}, {"text": "By generating texts from ontologies, and selectively including or omitting these defining axioms, we show by surveys that human readers are typically capable of inferring information implicitly encoded in identifier phrases, and that texts which do not make such \"obvious\" information explicit are preferred by readers and yet communicate the same information as the longer texts in which such information is spelled out explicitly.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been increasing interest in recent years in the generation of natural language texts from, or us- * Many thanks to Richard Power and Sandra Williams for their help and comments.", "labels": [], "entities": [{"text": "Richard Power and Sandra Williams", "start_pos": 125, "end_pos": 158, "type": "DATASET", "confidence": 0.6895825266838074}]}, {"text": "This work was supported by Engineering and Physical Sciences Research Council Grant Ref.", "labels": [], "entities": [{"text": "Physical Sciences Research Council Grant Ref.", "start_pos": 43, "end_pos": 88, "type": "DATASET", "confidence": 0.69431300674166}]}, {"text": "ing, ontologies, for example).", "labels": [], "entities": []}, {"text": "Such \"verbalisations\" -translations of the logic of, for example, OWL (W3C Consortium, 2012), into humanreadable natural language -can be useful fora variety of purposes, such as communicating the results of ontology inference, generating custom texts to suit a particular application domain or assisting nonontology-experts in authoring, reviewing and validating ontologies.", "labels": [], "entities": [{"text": "OWL (W3C Consortium, 2012)", "start_pos": 66, "end_pos": 92, "type": "DATASET", "confidence": 0.9160896624837603}]}, {"text": "This paper takes as its starting point an observation about ontology structure and use.", "labels": [], "entities": []}, {"text": "The purpose of an ontology (specifically, the so-called \"T-box\" 1 ) is to define the terms of a particular domain in order to allow automated inference of the semantics of that domain.", "labels": [], "entities": []}, {"text": "Given that machines are essentially tabulae rasae with regard to nearly any kind of world knowledge, it is therefore necessary to spell out the meanings of most terms in what (to a human) would be excruciating detail.", "labels": [], "entities": []}, {"text": "In most, if not all, ontology languages, and certainly in OWL, identifiers -the \"names\" for individual entities, classes and relations 2 -are atomic units.", "labels": [], "entities": []}, {"text": "That is to say, every identifier is treated by the machine as simply a flat string, with no internal structure or semantics.", "labels": [], "entities": []}, {"text": "The corresponding natural language constructions -noun and verb phrasesby contrast have a very rich internal structure which can communicate very subtle semantic distinctions.", "labels": [], "entities": []}, {"text": "Best practice for human ontology developers recommends that for every entity in an ontology, either its identifier should be a meaningful simple or complex term, or it should have a (localised) label which is a meaningful simple or complex natural language 1 \"Terminology box\" 2 \"Property\" is the OWL terminology fora relation between two entities term.", "labels": [], "entities": []}, {"text": "For example, in the domain of education, a class intended to represent the real-world class of junior schools ought to have (in English) an identifier such as junior school or a label such as \"junior school\".", "labels": [], "entities": []}, {"text": "Ontology developers who follow this best practice (and, according to, the vast majority do) produce ontologies in which the entities are easily recognisable and understood by human readers who can parse these identifiers, to infer, for example, that \"junior school\" is a subclass of the class \"school\".", "labels": [], "entities": []}, {"text": "As it stands, however, a machine will not make this inference.", "labels": [], "entities": []}, {"text": "In order for the machine to comprehend the semantics of this example, there must additionally bean axiom equivalent to \"a junior school is a school\".", "labels": [], "entities": []}, {"text": "The motivation for this work is the desire to identify which kinds of identifier or label are \"obvious\" in this way.", "labels": [], "entities": []}, {"text": "That is to say, if we treat an OWL identifier as if it were in fact a multi-word natural language expression, can we infer at least some of its semantics from its properties as a noun phrase, for example?", "labels": [], "entities": []}, {"text": "This paper addresses the first of these two purposes.", "labels": [], "entities": []}, {"text": "Note that the aim of this work is not particular to consider how best to realise entity names in a verbalisation, but rather, how to use the names of entities to guide the choice and construction of sentences.", "labels": [], "entities": []}, {"text": "This work was undertaken in the context of the SWAT (Semantic Web Authoring Tool) project, which is investigating the application of NLG/NLP to ontology authoring and editing ,,,,,).", "labels": [], "entities": [{"text": "SWAT (Semantic Web Authoring Tool)", "start_pos": 47, "end_pos": 81, "type": "TASK", "confidence": 0.4885827090059008}, {"text": "ontology authoring and editing", "start_pos": 144, "end_pos": 174, "type": "TASK", "confidence": 0.7630170285701752}]}], "datasetContent": [{"text": "The first survey attracted 30 respondents, the second 29.", "labels": [], "entities": []}, {"text": "The data collected from the first survey are summarised in, where S is \"sentence predicted to be obvious by a defining axiom pattern\" and J is \"sentence judged inferrable from the given identifier\".", "labels": [], "entities": []}, {"text": "Applying a 2 \u00d7 2 \u03c7 2 contingency test results in \u03c7 2 = 342.917, df = 1 and P < 0.0001, indicating an extremely significant association between the predicted obviousness of a sentence and respondent judgement of that sentence as following from the given identifier.", "labels": [], "entities": []}, {"text": "It is interesting, however, to note the top row of: for sentences which are predicted to hold, human judges are ambivalent as to whether to judge it as definitely true or not.", "labels": [], "entities": []}, {"text": "One interpretation of this result is that, while it is very clear that non-defining axioms cannot be inferred from identifier phrases, people are hesitant to commit to asserting these axioms in an unfamiliar domain, perhaps for fear of an unknown exception to the general rule.", "labels": [], "entities": []}, {"text": "For example, while \"a Qualifier Noun is a Noun\" is usually a good rule of thumb, \"a clothes-horse is a horse\" is a clear counterexample.", "labels": [], "entities": []}, {"text": "So perhaps the better interpretation of these results would be to say that, presented for example with a phrase of the form \"Qualifier Noun\", a reader would not be surprised if it turned out that the entity referred to is also a \"Noun\".", "labels": [], "entities": []}, {"text": "Either way, these statistics suggest that it could well be safe, when generating texts, to omit defining axioms and allow readers' default assumptions to apply.", "labels": [], "entities": []}, {"text": "A simple improvement suggests itself.", "labels": [], "entities": []}, {"text": "In the situation where a particular defining axiom pattern would be predicted, but its negation is in fact present, the said negation is automatically highly-salient.", "labels": [], "entities": []}, {"text": "It is always likely to be worthwhile verbalising \"a clothes-horse is not a horse.\"", "labels": [], "entities": []}, {"text": "It is also interesting to separate out the results of this survey by type of axiom.", "labels": [], "entities": []}, {"text": "There were three general families of defining axiom type tested -SubClassOf (\"A junior school is a school\"), InverseObjectProperties (\"Bill is father of John if and only if John has father Bill\") and ObjectPropertyRange (\"If something has as segment X, then X is a segment\").", "labels": [], "entities": []}, {"text": "shows the results broken down by these categories, where \"SC\" is SubClassOf, \"IOP\" is InverseObjectProperties\" and \"OPR\" is ObjectPropertyRange.", "labels": [], "entities": []}, {"text": "A 3 \u00d7 2 \u03c7 2 test results in \u03c7 2 = 13.54, df = 2 and P = 0.001148, indicating that the judgement of a sentence as obvious or not varies to a significant degree with the type of sentence it is.", "labels": [], "entities": [{"text": "P", "start_pos": 52, "end_pos": 53, "type": "METRIC", "confidence": 0.9547103047370911}]}, {"text": "This is perhaps to be expected, given that not all axiomtypes can be verbalised by sentences of similar linguistic complexities.", "labels": [], "entities": []}, {"text": "In particular, it is very difficult to see how to verbalise ObjectPropertyRange sentences without appealing to the use of variables such as X and Y, which tend to lead to rather clunky sentences.", "labels": [], "entities": []}, {"text": "Sentences corresponding to SubClassOf axioms are most likely to be judged as obvious.", "labels": [], "entities": []}, {"text": "Further work is necessary to determine the reasons for these differences empirically.", "labels": [], "entities": []}, {"text": "summarises the results of the \"same information\" and \"preference\" questions from the paragraph-comparison survey, aggregated across questions.", "labels": [], "entities": []}, {"text": "Comparing each of these to a random distribution of Yes/No answers gives, in turn, \u03c7 2 = 15.198, df = 1 and P < 0.0001 (same information) and \u03c7 2 = 8.498, df = 1 and P = 0.0036 (preference), indicating an extremely significant likelihood of judging two paragraphs containing the same information as in fact doing so, and a significant likelihood of preferring the more concise of such paragraphs.", "labels": [], "entities": []}, {"text": "More interesting are the results shown in.", "labels": [], "entities": []}, {"text": "Here, taken across all paragraph-pairs, E denotes that the information expressed by a sentence in one paragraph is explicitly expressed in the other paragraph, and J denotes the judgement as to whether each sentence was judged to express information not also expressed in the other paragraph.", "labels": [], "entities": []}, {"text": "These distributions of observations need to be compared, for explicit and implicit in turn, to the expected distributions of judgements as to whether the information is missing or not.", "labels": [], "entities": []}, {"text": "For explicit information, the expected distribution is zero judgements of \"missing\" -where sentences were explicit in both paragraphs, they were in fact identical in both paragraphs and so should never have been judged missing -and 696 judgements of \"not missing\".", "labels": [], "entities": []}, {"text": "It scarcely needs a statistical test to show that the actual observations of 3, and 693, respectively, do not differ significantly from these expectations.", "labels": [], "entities": []}, {"text": "Nonetheless, Fisher's exact test (since one of the expected values is 0, ruling out \u03c7 2 ) gives P=0.2495.", "labels": [], "entities": [{"text": "exact", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9725816249847412}, {"text": "P", "start_pos": 96, "end_pos": 97, "type": "METRIC", "confidence": 0.9870125651359558}]}, {"text": "For implicit information, the null hypothesis is that implicit information is indistinguishable from absent information, and so the expected distribution is 290 judgements of \"missing\" and zero judgements of \"not missing\", compared to observations of 33, and 257, respectively.", "labels": [], "entities": []}, {"text": "Applying Fisher's exact test gives P less than 0.0001, indicating an extremely significant difference.", "labels": [], "entities": [{"text": "P", "start_pos": 35, "end_pos": 36, "type": "METRIC", "confidence": 0.9953342080116272}]}, {"text": "In other words, implicit information is readily distinguishable from absent information, as predicted.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Breakdown of identifier inference results by ax- iom type.", "labels": [], "entities": [{"text": "Breakdown of identifier inference", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.7812163680791855}]}, {"text": " Table 4: Results of paragraph comparison survey (I)", "labels": [], "entities": [{"text": "paragraph comparison", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.8893470764160156}]}]}