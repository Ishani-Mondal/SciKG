{"title": [{"text": "Fast Unsupervised Dependency Parsing with Arc-Standard Transitions", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.6880965828895569}]}], "abstractContent": [{"text": "Unsupervised dependency parsing is one of the most challenging tasks in natural languages processing.", "labels": [], "entities": [{"text": "Unsupervised dependency parsing", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6723695397377014}, {"text": "natural languages processing", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.637175718943278}]}, {"text": "The task involves finding the best possible dependency trees from raw sentences without getting any aid from annotated data.", "labels": [], "entities": []}, {"text": "In this paper, we illustrate that by applying a supervised incre-mental parsing model to unsupervised parsing ; parsing with a linear time complexity will be faster than the other methods.", "labels": [], "entities": []}, {"text": "With only 15 training iterations with linear time complexity, we gain results comparable to those of other state of the art methods.", "labels": [], "entities": []}, {"text": "By employing two simple universal linguistic rules inspired from the classical dependency grammar, we improve the results in some languages and get the state of the art results.", "labels": [], "entities": []}, {"text": "We also test our model on apart of the ongoing Persian dependency treebank.", "labels": [], "entities": [{"text": "Persian dependency treebank", "start_pos": 47, "end_pos": 74, "type": "DATASET", "confidence": 0.8941337863604227}]}, {"text": "This work is the first work done on the Per-sian language.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised learning of grammars has achieved considerable focus in recent years.", "labels": [], "entities": []}, {"text": "The lack of sufficient manually tagged linguistic data and the considerable successes of unsupervised approaches on some languages have motivated researchers to test different models of unsupervised learning on different linguistic representations.", "labels": [], "entities": []}, {"text": "Since the introduction of the dependency model with valence (DMV) proposed by, dependency grammar induction has received great attention by researchers.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.8354532321294149}]}, {"text": "DMV was the first model to outperform the right attachment accuracy in English.", "labels": [], "entities": [{"text": "DMV", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9528145790100098}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.8874881267547607}]}, {"text": "Since this achievement, the model has been used by many researchers (e.g.;);); and ().", "labels": [], "entities": []}, {"text": "The main task of unsupervised dependency parsing is to obtain the most likely dependency tree of a sentence without using any annotated training data.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.6854865550994873}]}, {"text": "In dependency trees, each word has only one head and the head of the sentence is a dependent of an artificial root word.", "labels": [], "entities": []}, {"text": "Problems such as data sparsity and a large search space that increases the ambiguity have made the task difficult.", "labels": [], "entities": []}, {"text": "Even deciding the direction of the link between two words in a dependency relation has made the task more difficult than finding phrase structures themselves (.", "labels": [], "entities": []}, {"text": "In this paper, we propose a model based on Arc-Standard Transition System of, which is known as an incremental greedy projective parsing model that parses sentences in linear time.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, the only incremental unsupervised dependency parsing is the model of Daum\u00e9 III (2009) with Shift-Reduce parsing model.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7196803390979767}, {"text": "Daum\u00e9 III (2009)", "start_pos": 99, "end_pos": 115, "type": "DATASET", "confidence": 0.9208731174468994}, {"text": "Shift-Reduce parsing", "start_pos": 121, "end_pos": 141, "type": "TASK", "confidence": 0.8211641311645508}]}, {"text": "Our model is not lexicalized, has a simple feature space and converges in 15 iterations with a linear (O(n)) parsing and training time, while other methods based on DMV in the best casework in O(n 3 ) time complexity with O(n 3 ) memory use for sentences with of length n.", "labels": [], "entities": []}, {"text": "We believe that the output of this model can also improve DMV.", "labels": [], "entities": [{"text": "DMV", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.6019663214683533}]}, {"text": "In addition, we use punctuation clues (), tying feature similarity in the transition system configuration, and \"baby steps\" notion () to improve the model accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9926344752311707}]}, {"text": "We test our model on 9 CoNLL 2006 and 2007 shared task data sets and WSJ part of Penn treebank and show that in some languages our model is better than the recent models.", "labels": [], "entities": [{"text": "9 CoNLL 2006 and 2007 shared task data sets", "start_pos": 21, "end_pos": 64, "type": "DATASET", "confidence": 0.8808400862746768}, {"text": "WSJ part of Penn treebank", "start_pos": 69, "end_pos": 94, "type": "DATASET", "confidence": 0.860429847240448}]}, {"text": "We also test our model on apart of an ongoing first Persian dependency corpus).", "labels": [], "entities": [{"text": "Persian dependency corpus", "start_pos": 52, "end_pos": 77, "type": "DATASET", "confidence": 0.6683479845523834}]}, {"text": "Our study maybe the first work to test dependency parsing on the Persian language.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8613080382347107}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, related work on unsupervised dependency parsing is reviewed.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7617274522781372}]}, {"text": "In Section 3, we describe our dependency parsing model.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8141809105873108}]}, {"text": "In Section 4 and Section 5, after the reporting experimental results on several languages, the conclusion is made.", "labels": [], "entities": []}], "datasetContent": [{"text": "Although training is done on sentences of length less than 16, the test was done on all sentences in the test data without dropping any sentences form the test data.", "labels": [], "entities": []}, {"text": "Results are shown in on 9 languages.", "labels": [], "entities": []}, {"text": "In, \"h1\" and \"h2\" refer to the two linguistic heuristics that are used in this paper.", "labels": [], "entities": []}, {"text": "We also compare our work with and Mare\u010dek and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y: Results tested on CoNLL data sets and the Persian data set.", "labels": [], "entities": [{"text": "CoNLL data sets", "start_pos": 107, "end_pos": 122, "type": "DATASET", "confidence": 0.9805754025777181}, {"text": "Persian data set", "start_pos": 131, "end_pos": 147, "type": "DATASET", "confidence": 0.9764783183733622}]}, {"text": "\"Rand\", \"LA\" and \"RA\" stand for random, left-attach and right-attach, respectively; \"punc\" refers to punctuation clues and fs refers to feature similarity cue; \"all\" refers to using both heuristics h1 and h2; and \"simp.\" refers to the simple model. in.", "labels": [], "entities": [{"text": "RA", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.7656069993972778}]}, {"text": "As shown in, our model outperforms the accuracy in 7 out of 9 languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9995607733726501}]}, {"text": "We also test our data on Penn Treebank but we do not gain better results than state of the art methods.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.993658721446991}]}, {"text": "We use the same train and test set as in and, other results are reported, we only limit our report to some of the results on WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.9809123873710632}]}, {"text": "In the Table, \"h1\" shows heuristic 1 and \"fs\" shows the use of feature similarity.", "labels": [], "entities": []}, {"text": "Stochastic EM(1-5) is one test that have done only by applying baby stepson sentences with the length 1 to 5 without using unsupervised search-based model.", "labels": [], "entities": [{"text": "Stochastic EM", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6620237082242966}]}, {"text": "A1 refers to a change in the model in which smoothing variable in steps 1 to 5 is multiplied by 10..", "labels": [], "entities": [{"text": "A1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9784771800041199}]}, {"text": "We convert the Penn treebank data via automatic \"head-percolation\" rules).", "labels": [], "entities": [{"text": "Penn treebank data", "start_pos": 15, "end_pos": 33, "type": "DATASET", "confidence": 0.9917218287785848}]}, {"text": "We have also tested our model via simple stochastic EM (without using unsupervised structure prediction) and show that the main problem with this method in English is its fast divergence when jumping from sentence length 5 to 6.", "labels": [], "entities": []}, {"text": "In the model settings tested for English, the model with heuristic 1 with the feature similarity is the best setting that we find.", "labels": [], "entities": []}, {"text": "By testing with a smoothing variable ten times bigger (\"Spi5\") and (\"Spi6\") in and Mare\u010dek and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y (2011) with Noun-Root constraint (\"MZ-NR\") and no constraint (\"MZ\").", "labels": [], "entities": []}, {"text": "The comparison results are from in Mare\u010dek and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y (2011).", "labels": [], "entities": [{"text": "Mare\u010dek and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y (2011)", "start_pos": 35, "end_pos": 95, "type": "DATASET", "confidence": 0.919464361667633}]}, {"text": "\"Our Best\" refers to the bold scores in in the first 5 steps, we have seen that the results change significantly.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "One main problem of the converted dependencies in English is their conversion errors like multi-root trees.", "labels": [], "entities": []}, {"text": "There are many trees in the corpus that have wrong multi-root dependencies.", "labels": [], "entities": []}, {"text": "Such problems lead us to believe that we should not rely too much on the results on WSJ part of the Penn treebank.", "labels": [], "entities": [{"text": "WSJ part of the Penn treebank", "start_pos": 84, "end_pos": 113, "type": "DATASET", "confidence": 0.8680795828501383}]}], "tableCaptions": [{"text": " Table 1: Results tested on CoNLL data sets and the Persian data set. \"Rand\", \"LA\" and \"RA\" stand for random,  left-attach and right-attach, respectively; \"punc\" refers to punctuation clues and fs refers to feature similarity cue;  \"all\" refers to using both heuristics h1 and h2; and \"simp.\" refers to the simple model.", "labels": [], "entities": [{"text": "CoNLL data sets", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9748305082321167}, {"text": "Persian data set", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.9419553081194559}, {"text": "Rand\", \"LA\" and \"RA\"", "start_pos": 71, "end_pos": 91, "type": "METRIC", "confidence": 0.7813807547092437}]}, {"text": " Table 3: Results of our model on WSJ, compared  to its counterpart Daum\u00e9 III (2009) and other DMV- based models. Since in", "labels": [], "entities": [{"text": "WSJ", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.8869161605834961}, {"text": "Daum\u00e9 III (2009)", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.9541767597198486}]}]}