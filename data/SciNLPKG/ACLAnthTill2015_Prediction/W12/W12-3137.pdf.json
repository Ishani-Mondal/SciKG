{"title": [{"text": "The RWTH Aachen Machine Translation System for WMT 2012", "labels": [], "entities": [{"text": "RWTH Aachen Machine Translation", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.808086708188057}, {"text": "WMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8689528107643127}]}], "abstractContent": [{"text": "This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the translation task of the NAACL 2012 Seventh Workshop on Statistical Machine Translation (WMT 2012).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.8081884930531184}, {"text": "translation task of the NAACL 2012 Seventh Workshop on Statistical Machine Translation (WMT 2012)", "start_pos": 115, "end_pos": 212, "type": "TASK", "confidence": 0.8249097689986229}]}, {"text": "We participated in the evaluation campaign for the French-English and German-English language pairs in both translation directions.", "labels": [], "entities": []}, {"text": "Both hierarchical and phrase-based SMT systems are applied.", "labels": [], "entities": [{"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.8704503774642944}]}, {"text": "A number of different techniques are evaluated, including an insertion model, different lexical smoothing methods, a discriminative reordering extension for the hierarchical system, reverse translation, and system combination.", "labels": [], "entities": [{"text": "reverse translation", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.7374445199966431}]}, {"text": "By application of these methods we achieve considerable improvements over the respective baseline systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "For the WMT 2012 shared translation task 1 RWTH utilized state-of-the-art phrase-based and hierarchical translation systems as well as an in-house system combination framework.", "labels": [], "entities": [{"text": "WMT 2012 shared translation task", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.7430468559265136}]}, {"text": "We give a survey of these systems and the basic methods they implement in Section 2.", "labels": [], "entities": []}, {"text": "For both the French-English (Section 3) and the German-English (Section 4) language pair, we investigate several different advanced techniques.", "labels": [], "entities": []}, {"text": "We concentrate on specific research directions for each of the translation tasks and present the respective techniques along with the empirical results they yield: For the French\u2192English task (Section 3.1), we apply a standard phrase-based system.", "labels": [], "entities": []}, {"text": "For the English\u2192French task (Section 3.2), we augment a hierarchical phrase-based setup with a number of enhancements like an insertion model, different lexical smoothing methods, and a discriminative reordering extension.", "labels": [], "entities": []}, {"text": "For the German\u2192English (Section 4.3) and English\u2192German (Section 4.4) tasks, we utilize morpho-syntactic analysis to preprocess the data (Section 4.1) and employ system combination to produce a consensus hypothesis from normal and reverse translations (Section 4.2) of phrase-based and hierarchical phrase-based setups.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the French\u2192English task, the phrase-based SMT system (PBT) is setup using the standard models listed in Section 2.1.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.7939034104347229}]}, {"text": "We vary the training data we use to train the system and compare the results.: Results for the English\u2192French task (truecase).", "labels": [], "entities": []}, {"text": "newstest2009 is used as development set.", "labels": [], "entities": [{"text": "newstest2009", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9145144820213318}]}, {"text": "BLEU and TER are given in percentage.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9928451180458069}, {"text": "TER", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9991687536239624}]}, {"text": "It should be noted that these setups do not use any English LDC Gigaword data for LM training at all.", "labels": [], "entities": [{"text": "LM training", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.8938604593276978}]}, {"text": "Our baseline system uses the Europarl and News Commentary data for training LM and phrase table.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9913689494132996}, {"text": "News Commentary data", "start_pos": 42, "end_pos": 62, "type": "DATASET", "confidence": 0.8386192123095194}]}, {"text": "Corpus statistics are shown in the \"EP+NC\" section of.", "labels": [], "entities": [{"text": "EP+NC\"", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.886077344417572}]}, {"text": "This results in a performance of 24.7 points BLEU on newstest2011.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9989344477653503}, {"text": "newstest2011", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.9651517868041992}]}, {"text": "Then we add the 10 9 as well as UN data and more monolingual English data from the News Crawl corpus to the data used for training the language model.", "labels": [], "entities": [{"text": "UN data", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.840240091085434}, {"text": "News Crawl corpus", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.9524988134702047}]}, {"text": "This system obtains a score of 27.7 points BLEU on newstest2011.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9990543723106384}, {"text": "newstest2011", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9668608903884888}]}, {"text": "Our final system uses Europarl, News Commentary, 10 9 and UN data and News Crawl monolingual data for LM training and the Europarl, News Commentary and 10 9 data for phrase table training.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.9596160054206848}, {"text": "UN data", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.8719713687896729}, {"text": "News Crawl monolingual data", "start_pos": 70, "end_pos": 97, "type": "DATASET", "confidence": 0.8160383105278015}, {"text": "Europarl", "start_pos": 122, "end_pos": 130, "type": "DATASET", "confidence": 0.9617272019386292}, {"text": "phrase table training", "start_pos": 166, "end_pos": 187, "type": "TASK", "confidence": 0.8285136222839355}]}, {"text": "Using these data sets the system reaches 29.1 points BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9991046786308289}]}, {"text": "The experimental results are summarized in Table 2.", "labels": [], "entities": []}, {"text": "For the English\u2192French task, the baseline system is a hierarchical phrase-based setup including the standard models as listed in Section 2.2, apart from the binary count features.", "labels": [], "entities": []}, {"text": "We limit the recursion depth for hierarchical rules with a shallow-1 grammar (de.", "labels": [], "entities": []}, {"text": "Ina shallow-1 grammar, the generic non-terminal X of the standard hierarchical approach is replaced by two distinct non-terminals XH and XP . By changing the left-hand sides of the rules, lexical phrases are allowed to be derived from XP only, hierarchical phrases from XH only.", "labels": [], "entities": []}, {"text": "On all right-hand sides of hierarchical rules, the X is replaced by XP . Gaps within hierarchical phrases can thus solely be filled with purely lexicalized phrases, but not a second time with hierarchical phrases.", "labels": [], "entities": []}, {"text": "The initial rule is substituted with and the glue rule is substituted with The main benefit of a restriction of the recursion depth is again in decoding efficiency, thus allowing us to setup systems more rapidly and to explore more model combinations and more system configurations.", "labels": [], "entities": []}, {"text": "The experimental results for English\u2192French are given in.", "labels": [], "entities": []}, {"text": "Starting from the shallow hierarchical baseline setup on Europarl and News Commentary parallel data only (but Europarl, News Commentary, 10 9 , UN, and News Crawl data for LM training), we are able to improve translation quality considerably by first adopting more parallel (10 9 and UN) and monolingual (French LDC Gigaword v2) training resources and then employing several different models that are not included in the baseline already.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9639430046081543}, {"text": "News Commentary parallel data", "start_pos": 70, "end_pos": 99, "type": "DATASET", "confidence": 0.8296990990638733}, {"text": "Europarl", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.9691160917282104}, {"text": "News Crawl data", "start_pos": 152, "end_pos": 167, "type": "DATASET", "confidence": 0.9034477670987447}, {"text": "translation", "start_pos": 209, "end_pos": 220, "type": "TASK", "confidence": 0.9618350863456726}]}, {"text": "We proceed with individual descriptions of the methods we use and report their respective effect in BLEU on the test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9989699125289917}]}, {"text": "10 9 and UN (up to +2.5 points BLEU) While the amount of provided parallel data from Europarl and News Commentary sources is rather limited (around 2M sentence pairs in total), the UN and the 10 9 corpus each provide a substantial collection of further training material.", "labels": [], "entities": [{"text": "UN", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.8702446222305298}, {"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9990662932395935}, {"text": "Europarl and News Commentary sources", "start_pos": 85, "end_pos": 121, "type": "DATASET", "confidence": 0.7874551773071289}]}, {"text": "By appending both corpora, we end up at roughly 35M parallel sentences (cf.).", "labels": [], "entities": []}, {"text": "We utilize this full amount of data in our system, but extract a phrase table with only lexical (i.e. nonhierarchical) phrases from the full parallel data.", "labels": [], "entities": []}, {"text": "We add it as a second phrase table to the baseline system, with a binary feature that enables the system to reward or penalize the application of phrases from this table.", "labels": [], "entities": []}, {"text": "LDC Gigaword v2 (up to +0.5 points BLEU) The LDC French Gigaword Second Edition (LDC2009T28) provides some more monolingual French resources.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9989691972732544}, {"text": "LDC French Gigaword Second Edition (LDC2009T28)", "start_pos": 45, "end_pos": 92, "type": "DATASET", "confidence": 0.7152615785598755}]}, {"text": "We include a total of 28.2M sentences from both the AFP and APW collections in our LM training data.", "labels": [], "entities": [{"text": "AFP and APW collections", "start_pos": 52, "end_pos": 75, "type": "DATASET", "confidence": 0.7656713202595711}]}, {"text": "insertion model (up to +0.4 points BLEU) We add an insertion model to the log-linear model combination.", "labels": [], "entities": [{"text": "insertion", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9582149982452393}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9990476965904236}]}, {"text": "This model is designed as a means to avoid the omission of content words in the hypotheses.", "labels": [], "entities": []}, {"text": "It is implemented as a phrase-level feature function which counts the number of inserted words.", "labels": [], "entities": []}, {"text": "We apply the model in source-totarget and target-to-source direction.", "labels": [], "entities": []}, {"text": "A targetside word is considered inserted based on lexical probabilities with the words on the foreign language side of the phrase, and vice versa fora source-side word.", "labels": [], "entities": []}, {"text": "As thresholds, we compute individual arithmetic averages for each word from the vocabulary . noisy-or lexical scores (up to +0.4 points BLEU) In our baseline system, the t Norm (\u00b7) lexical scoring variant as described in) is employed with a relative frequency (RF) lexicon model for phrase table smoothing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.998785674571991}, {"text": "phrase table smoothing", "start_pos": 283, "end_pos": 305, "type": "TASK", "confidence": 0.7527612249056498}]}, {"text": "The single-word based translation probabilities of the RF lexicon model are extracted from wordaligned parallel training data, in the fashion of (.", "labels": [], "entities": []}, {"text": "We exchange the baseline lexical scoring with a noisy-or () lexical scoring variant t NoisyOr (\u00b7).", "labels": [], "entities": []}, {"text": "DWL (up to +0.3 points BLEU) We augment our system with phrase-level lexical scores from discriminative word lexicon (DWL) models () in both source-to-target and target-to-source direction.", "labels": [], "entities": [{"text": "DWL", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7997534871101379}, {"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9992280006408691}]}, {"text": "The DWLs are trained on News Commentary data only.", "labels": [], "entities": [{"text": "DWLs", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8648825287818909}, {"text": "News Commentary data", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.8783243497212728}]}, {"text": "IBM-1 (up to +0.1 points BLEU) On News Commentary and Europarl data, we train IBM model-1 () lexicons in both translation directions and also use them to compute phrase-level scores. discrim.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9987978935241699}, {"text": "Europarl data", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.9331285059452057}]}, {"text": "RO (up to +0.4 points BLEU) The modification of the grammar to a shallow-1 version restricts the search space of the decoder and is convenient to prevent overgeneration.", "labels": [], "entities": [{"text": "RO", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.985944390296936}, {"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9990530610084534}]}, {"text": "In order not to be too restrictive, we reintroduce more flexibility into the search process by extending the grammar with specific reordering rules The upper rule in Equation is a swap rule that allows adjacent lexical phrases to be transposed, the lower rule is added for symmetry reasons, in particular because sequences assembled with these rules are allowed to fill gaps within hierarchical phrases.", "labels": [], "entities": []}, {"text": "Note that we apply a length constraint of 10 to the number of terminals spanned by an XP . We introduce two binary indicator features, one for each of the two rules in Equation (3).", "labels": [], "entities": []}, {"text": "In addition to adding: Corpus statistics of the preprocessed GermanEnglish parallel training data (Europarl and News Commentary).", "labels": [], "entities": [{"text": "GermanEnglish parallel training data", "start_pos": 61, "end_pos": 97, "type": "DATASET", "confidence": 0.8936519473791122}, {"text": "Europarl", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.9043682813644409}]}, {"text": "In the data, numerical quantities have been replaced by a single category symbol.", "labels": [], "entities": []}, {"text": "these rules, a discriminatively trained lexicalized reordering model is applied ).", "labels": [], "entities": []}, {"text": "Our results for the German\u2192English task are shown in.", "labels": [], "entities": []}, {"text": "For this task, we apply the idea of reverse translation for both the phrase-based and the hierarchical approach.", "labels": [], "entities": [{"text": "reverse translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7098610252141953}]}, {"text": "It seems that the reversed systems perform slightly worse.", "labels": [], "entities": []}, {"text": "However, when we employ system combination using both reverse translation setups (PBT reverse and HPBT reverse) and both baseline setups (PBT baseline and HPBT baseline), the translation quality is improved by up to 0.4 points in BLEU and 1.0 points TER compared to the best single system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 230, "end_pos": 234, "type": "METRIC", "confidence": 0.9986448884010315}, {"text": "TER", "start_pos": 250, "end_pos": 253, "type": "METRIC", "confidence": 0.9966613054275513}]}, {"text": "The addition of LDC Gigaword corpora (+GW) to the language model training data of the baseline setups shows improvements in both BLEU and TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9982232451438904}, {"text": "TER", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.9661530256271362}]}, {"text": "Furthermore, with the system combination including these setups, we are able to report an improvement of up to 0.7 points BLEU and 1.0 points TER over the best single setup.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9995338916778564}, {"text": "TER", "start_pos": 142, "end_pos": 145, "type": "METRIC", "confidence": 0.9989319443702698}]}, {"text": "Compared to the system combination based on systems which are not using the LDC Gigaword corpora, we gain 0.3 points in BLEU and 0.4 points in TER.", "labels": [], "entities": [{"text": "LDC Gigaword corpora", "start_pos": 76, "end_pos": 96, "type": "DATASET", "confidence": 0.844612181186676}, {"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9975806474685669}, {"text": "TER", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.9897206425666809}]}, {"text": "Our results for the English\u2192German task are shown in.", "labels": [], "entities": []}, {"text": "For this task, we first compare systems using one, two or three language models of different parts of the data.", "labels": [], "entities": []}, {"text": "The language model for systems with only one language model is created with all monolingual and parallel data.", "labels": [], "entities": []}, {"text": "A language model with all monolingual data and a language model with all parallel data is created for the systems with two language models.", "labels": [], "entities": []}, {"text": "For the systems with three language models, we also split the parallel data in two parts consisting of either only Europarl data or only News Commentary data.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 115, "end_pos": 128, "type": "DATASET", "confidence": 0.9787818789482117}, {"text": "News Commentary data", "start_pos": 137, "end_pos": 157, "type": "DATASET", "confidence": 0.8880364497502645}]}, {"text": "For PBT the system with two language models performs best for all test sets.", "labels": [], "entities": [{"text": "PBT", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.5967468619346619}]}, {"text": "Further, we apply the idea of reverse translation for both the phrase-based and the hierarchical approach.", "labels": [], "entities": [{"text": "reverse translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.764573484659195}]}, {"text": "The PBT reverse 2 LM systems perform slightly worse compared to PBT baseline 2 LM.", "labels": [], "entities": [{"text": "PBT reverse 2 LM", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8352439254522324}]}, {"text": "The HPBT reverse 2 LM performs better compared to HPBT baseline 2 LM.", "labels": [], "entities": [{"text": "HPBT reverse 2 LM", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.6790581196546555}, {"text": "HPBT baseline 2 LM", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.9281273484230042}]}, {"text": "When we employ system combination using both reverse translation setups (PBT reverse 2 LM and HPBT reverse 2 LM) and both baseline setups (PBT baseline 2 LM and HPBT baseline 2 LM), the translation quality is improved by up to 0.2 points in BLEU and 2.1 points in TER compared to the best single system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 241, "end_pos": 245, "type": "METRIC", "confidence": 0.9978075623512268}, {"text": "TER", "start_pos": 264, "end_pos": 267, "type": "METRIC", "confidence": 0.9882217049598694}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics of the preprocessed French- English parallel training data. EP denotes Europarl, NC  denotes News Commentary. In the data, numerical quan- tities have been replaced by a single category symbol.", "labels": [], "entities": [{"text": "French- English parallel training data", "start_pos": 48, "end_pos": 86, "type": "DATASET", "confidence": 0.7784831821918488}, {"text": "Europarl", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.9556236863136292}]}, {"text": " Table 2: Results for the French\u2192English task (truecase). newstest2009 is used as development set. BLEU and TER  are given in percentage.", "labels": [], "entities": [{"text": "newstest2009", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9301531910896301}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9990102052688599}, {"text": "TER", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9940890073776245}]}, {"text": " Table 3: Results for the English\u2192French task (truecase). newstest2009 is used as development set. BLEU and TER  are given in percentage.", "labels": [], "entities": [{"text": "newstest2009", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9274008870124817}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9989748001098633}, {"text": "TER", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9938672184944153}]}, {"text": " Table 1. This results in a performance of 24.7  points BLEU on newstest2011. Then we add the 10 9  as well as UN data and more monolingual English  data from the News Crawl corpus to the data used  for training the language model. This system ob- tains a score of 27.7 points BLEU on newstest2011.  Our final system uses Europarl, News Commentary,  10 9 and UN data and News Crawl monolingual data  for LM training and the Europarl, News Commen- tary and 10 9 data", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.99907386302948}, {"text": "News Crawl corpus", "start_pos": 163, "end_pos": 180, "type": "DATASET", "confidence": 0.9117593963940939}, {"text": "BLEU", "start_pos": 277, "end_pos": 281, "type": "METRIC", "confidence": 0.9988853335380554}, {"text": "Europarl", "start_pos": 322, "end_pos": 330, "type": "DATASET", "confidence": 0.9508321285247803}, {"text": "UN data", "start_pos": 359, "end_pos": 366, "type": "DATASET", "confidence": 0.7776278257369995}, {"text": "News Crawl monolingual data", "start_pos": 371, "end_pos": 398, "type": "DATASET", "confidence": 0.8406802862882614}, {"text": "Europarl", "start_pos": 424, "end_pos": 432, "type": "DATASET", "confidence": 0.9816715121269226}]}, {"text": " Table 4. The lan- guage models are 4-grams trained on the respective  target side of the bilingual data as well as on the pro- vided News Crawl corpus. For the English language  model the 10 9 French-English, UN and LDC Giga- word Fourth Edition corpora are used additionally.  For the 10 9 French-English, UN and LDC Gigaword  corpora we apply the data selection technique de- scribed in", "labels": [], "entities": [{"text": "News Crawl corpus", "start_pos": 134, "end_pos": 151, "type": "DATASET", "confidence": 0.9459747672080994}]}]}