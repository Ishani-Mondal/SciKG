{"title": [{"text": "A Reranking Model for Discourse Segmentation using Subtree Features", "labels": [], "entities": [{"text": "Discourse Segmentation", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.752051591873169}]}], "abstractContent": [{"text": "This paper presents a discriminative reranking model for the discourse segmentation task, the first step in a discourse parsing system.", "labels": [], "entities": [{"text": "discourse segmentation task", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.7831969857215881}, {"text": "discourse parsing", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.6901556551456451}]}, {"text": "Our model exploits subtree features to rerank N-best outputs of abase segmenter, which uses syntactic and lexical features in a CRF framework.", "labels": [], "entities": []}, {"text": "Experimental results on the RST Discourse Treebank corpus show that our model outperforms existing discourse segmenters in both settings that use gold standard Penn Tree-bank parse trees and Stanford parse trees.", "labels": [], "entities": [{"text": "RST Discourse Treebank corpus", "start_pos": 28, "end_pos": 57, "type": "DATASET", "confidence": 0.7510566711425781}, {"text": "Penn Tree-bank parse trees", "start_pos": 160, "end_pos": 186, "type": "DATASET", "confidence": 0.9638957381248474}]}], "introductionContent": [{"text": "Discourse structure has been shown to have an important role in many natural language applications, such as text summarization, information presentation (), question answering (, and dialogue generation.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7491656541824341}, {"text": "information presentation", "start_pos": 128, "end_pos": 152, "type": "TASK", "confidence": 0.77266526222229}, {"text": "question answering", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.9028190076351166}, {"text": "dialogue generation", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.7810667753219604}]}, {"text": "To produce such kinds of discourse structure, several attempts have been made to build discourse parsers in the framework of Rhetorical Structure Theory (RST) (, one of the most widely used theories of text structure.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 125, "end_pos": 158, "type": "TASK", "confidence": 0.7897818187872568}]}, {"text": "In the RST framework, a text is first divided into several elementary discourse units (EDUs).", "labels": [], "entities": [{"text": "RST", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9788819551467896}]}, {"text": "Each EDU maybe a simple sentence or a clause in a complex sentence.", "labels": [], "entities": []}, {"text": "Consecutive EDUs are then put in relation with each other to build a discourse tree.", "labels": [], "entities": []}, {"text": "shows an example of a discourse tree with three EDUs.", "labels": [], "entities": []}, {"text": "The goal of the discourse segmentation task is to divide the input text into such EDUs.).", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7325889617204666}]}, {"text": "The quality of the discourse segmenter contributes a significant part to the overall accuracy of every discourse parsing system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9990854263305664}, {"text": "discourse parsing", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.7236769497394562}]}, {"text": "If a text is wrongly segmented, no discourse parsing algorithm can build a correct discourse tree.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7357922494411469}]}, {"text": "Existing discourse segmenters usually exploit lexical and syntactic features to label each word in a sentence with one of two labels, boundary or noboundary.", "labels": [], "entities": []}, {"text": "The limitation of this approach is that it only focuses on the boundaries of EDUs.", "labels": [], "entities": []}, {"text": "It cannot capture features that describe whole EDUs.", "labels": [], "entities": []}, {"text": "Recently, discriminative reranking has been used successfully in some NLP tasks such as POS tagging, chunking, and statistical parsing).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.8381866216659546}, {"text": "statistical parsing", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.8042762279510498}]}, {"text": "The advantage of the reranking method is that it can exploit the output of abase model to learn.", "labels": [], "entities": []}, {"text": "Based on that output, we can extract longdistance non-local features to rerank.", "labels": [], "entities": []}, {"text": "In this paper, we present a reranking model for the discourse segmentation task.", "labels": [], "entities": [{"text": "discourse segmentation task", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.8110221028327942}]}, {"text": "We show how to use subtree features, features extracted from whole EDUs, to rerank outputs of abase model.", "labels": [], "entities": []}, {"text": "Experimental results on RST Discourse Treebank (RST-DT)) show that our model out-performs existing systems.", "labels": [], "entities": [{"text": "RST Discourse Treebank (RST-DT))", "start_pos": 24, "end_pos": 56, "type": "DATASET", "confidence": 0.809935082991918}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 summarizes related work.", "labels": [], "entities": []}, {"text": "Section 3 presents our method.", "labels": [], "entities": []}, {"text": "Experimental results on RST-DT are described in Section 4.", "labels": [], "entities": [{"text": "RST-DT", "start_pos": 24, "end_pos": 30, "type": "TASK", "confidence": 0.8929004073143005}]}, {"text": "Finally, Section 5 gives conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our model on the RST Discourse Treebank corpus.", "labels": [], "entities": [{"text": "RST Discourse Treebank corpus", "start_pos": 27, "end_pos": 56, "type": "DATASET", "confidence": 0.9397185444831848}]}, {"text": "This corpus consists of 385 articles from the Penn Treebank, which are divided into a Training set and a Test set.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.9770961999893188}]}, {"text": "The Training set consists of 347 articles (6132 sentences), and the Test set consists of 38 articles (991 sentences).", "labels": [], "entities": []}, {"text": "There are two evaluation methods that have been used in previous work.", "labels": [], "entities": []}, {"text": "The first method measures only beginning labels (B labels)).", "labels": [], "entities": []}, {"text": "The second method) measures both beginning and continuation labels (B and C labels) . This method first calculates scores on B labels and scores on C labels, and then produces the average of them.", "labels": [], "entities": []}, {"text": "Due to the number of C labels being much higher than the number of B labels, the second evaluation method yields much higher results.", "labels": [], "entities": []}, {"text": "In Hernault et al., the authors compare their systems with previous work despite using different evaluation methods.", "labels": [], "entities": []}, {"text": "Such comparisons are not valid.", "labels": [], "entities": []}, {"text": "In our work, we measure the performance of the proposed model using both methods.", "labels": [], "entities": []}, {"text": "We learned the base model on the Training set and tested on the Test set to get N-best outputs to rerank.", "labels": [], "entities": [{"text": "Training set", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.9316543638706207}, {"text": "Test set", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9287063777446747}]}, {"text": "To learn parameters of the reranking model, we conducted 5-fold cross-validation tests on the Training set.", "labels": [], "entities": [{"text": "Training set", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.8849022686481476}]}, {"text": "In all experiments, we set N to 20.", "labels": [], "entities": []}, {"text": "To choose the number of iterations, we used a development set, which is about 20 percent of the Training set.", "labels": [], "entities": []}, {"text": "shows experimental results when evaluating only beginning (B) labels, in which SPADE is the work of, NNDS is a segmenter that uses neural networks, and CRFSeg is a CRF-based segmenter (.", "labels": [], "entities": []}, {"text": "When using gold parse trees, our base model got 92.5% in the F 1 score, which improves 1.3% compared to the stateof-the-art segmenter (CRFSeg).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9890099366505941}]}, {"text": "When using Stanford parse trees (), our base model improved 1.7% compared to CRFSeg.", "labels": [], "entities": [{"text": "CRFSeg", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.9065184593200684}]}, {"text": "It demonstrates the effectiveness of our feature ex- Neither evaluation method counts sentence boundaries.", "labels": [], "entities": []}, {"text": "traction method in the base model.", "labels": [], "entities": []}, {"text": "As expected, our reranking model got higher results compared to the base model in both settings.", "labels": [], "entities": []}, {"text": "The reranking model got 93.7% and 91.0% in two settings, which improves 2.5% and 2.0% compared to CRFSeg.", "labels": [], "entities": [{"text": "CRFSeg", "start_pos": 98, "end_pos": 104, "type": "DATASET", "confidence": 0.9157940745353699}]}, {"text": "Also note that, when using Stanford parse trees, our reranking model got competitive results with CRFSeg when using gold parse trees (91.0% compared to 91.2%).", "labels": [], "entities": [{"text": "CRFSeg", "start_pos": 98, "end_pos": 104, "type": "DATASET", "confidence": 0.83370441198349}]}, {"text": "shows experimental results when evaluating on both beginning and continuation labels.", "labels": [], "entities": []}, {"text": "Our models also outperformed CRFSeg in both settings, using gold parse trees and using Stanford parse trees (96.6% compared to 95.3% in the first setting, and 95.1% compared to 94.1% in the second setting).", "labels": [], "entities": []}, {"text": "Both evaluation methods have a weak point in that they do not measure the ability to find EDUs exactly.", "labels": [], "entities": []}, {"text": "We suggest that the discourse segmentation task should be measured on EDUs rather than boundaries of EDUs.", "labels": [], "entities": [{"text": "discourse segmentation task", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.7409170667330424}]}, {"text": "Under this evaluation scheme, our model achieved 90.0% and 86.2% when using gold parse trees and Stanford parse trees, respectively.", "labels": [], "entities": []}, {"text": "We do not compare our segmenter to systems described in Thanh et al.", "labels": [], "entities": []}, {"text": "An important question is which subtree features were useful for the reranking model.", "labels": [], "entities": []}, {"text": "This question can be answered by looking at the weights of subtree features (the parameter vector learned by the average perceptron algorithm).", "labels": [], "entities": []}, {"text": "shows 30 subtree features with the highest weights in absolute value.", "labels": [], "entities": []}, {"text": "These features are thus useful for reranking candidates in the reranking model.", "labels": [], "entities": []}, {"text": "We can see that most subtree features at the top are splitting trees, so splitting trees have a more important role than bound trees in our model.", "labels": [], "entities": []}, {"text": "Among three types of subtrees (left tree, right tree, and full tree), full tree is the most important type.", "labels": [], "entities": []}, {"text": "It is understandable because subtrees in this type convey much information; and therefore describe splitting trees and bound trees more precise than subtrees in other types.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance when evaluating on B labels", "labels": [], "entities": [{"text": "Performance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9630304574966431}]}, {"text": " Table 2: Performance when evaluating on B and C labels", "labels": [], "entities": [{"text": "Performance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9599884152412415}]}, {"text": " Table 4: Top error words", "labels": [], "entities": []}, {"text": " Table 3: Top 30 subtree features with the highest weights", "labels": [], "entities": []}, {"text": " Table 5: Most frequent words that appear before error  words", "labels": [], "entities": []}]}