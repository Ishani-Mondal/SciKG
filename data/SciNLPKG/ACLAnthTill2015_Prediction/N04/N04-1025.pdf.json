{"title": [{"text": "A Language Modeling Approach to Predicting Reading Difficulty", "labels": [], "entities": [{"text": "Language Modeling Approach", "start_pos": 2, "end_pos": 28, "type": "TASK", "confidence": 0.7305524746576945}, {"text": "Predicting Reading Difficulty", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.9056036869684855}]}], "abstractContent": [{"text": "We demonstrate anew research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.", "labels": [], "entities": []}, {"text": "We derive a measure based on an extension of multinomial na\u00efve Bayes classification that combines multiple language models to estimate the most likely grade level fora given passage.", "labels": [], "entities": []}, {"text": "The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data.", "labels": [], "entities": []}, {"text": "We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures.", "labels": [], "entities": []}, {"text": "We show that with minimal changes, the classifier maybe retrained for use with French Web documents.", "labels": [], "entities": []}, {"text": "For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets.", "labels": [], "entities": [{"text": "labeled grade level", "start_pos": 89, "end_pos": 108, "type": "METRIC", "confidence": 0.8445258935292562}]}, {"text": "Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9975501894950867}]}], "introductionContent": [{"text": "In the course of constructing a search engine for students, we wanted a method for retrieving Web pages that were not only relevant to a student's query, but also well-matched to their reading ability.", "labels": [], "entities": []}, {"text": "Widely-used traditional readability formulas such as Flesch-Kincaid usually perform poorly in this scenario.", "labels": [], "entities": []}, {"text": "Such formulas make certain assumptions about the text: for example, that the sample has at least 100 words and uses welldefined sentences.", "labels": [], "entities": []}, {"text": "Neither of these assumptions need be true for Web pages or other non-traditional documents.", "labels": [], "entities": []}, {"text": "We seek a more robust technique for predicting reading difficulty that works well on a wide variety of document types.", "labels": [], "entities": [{"text": "predicting reading difficulty", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.7824050386746725}]}, {"text": "To do this, we turn to simple techniques from statistical language modeling.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.7974626024564108}]}, {"text": "Advances in this field in the past 20 years, along with greater access to training data, make the application of such techniques to readability quite timely.", "labels": [], "entities": []}, {"text": "While traditional formulas are based on linear regression with two or three variables, statistical language models can capture more detailed patterns of individual word usage.", "labels": [], "entities": []}, {"text": "As we show in our evaluation, this generally results in better accuracy for Web documents and very short passages (less than 10 words).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9990063309669495}]}, {"text": "Another benefit of a language modeling approach is that we obtain a probability distribution across all grade models, not just a single grade prediction.", "labels": [], "entities": []}, {"text": "Statistical models of text rely on training data, so in Section 2 we describe our Web training corpus and note some trends that are evident in word usage.", "labels": [], "entities": []}, {"text": "Section 3 summarizes related work on readability, focusing on existing vocabulary-based measures that can bethought of as simplified language model techniques.", "labels": [], "entities": []}, {"text": "Section 4 defines the modified multinomial na\u00efve Bayes model.", "labels": [], "entities": []}, {"text": "Section 5 describes our smoothing and feature selection techniques.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7395631074905396}]}, {"text": "Section 6 evaluates our model's generalization performance, accuracy on short passages, and sensitivity to the amount of training data.", "labels": [], "entities": [{"text": "generalization", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.9503200650215149}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9994327425956726}]}, {"text": "Sections 7 and 8 discuss the evaluation results and give our observations and conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "State-of-the-art performance for this classification task is hard to estimate.", "labels": [], "entities": []}, {"text": "The results from the most closely related previous work) are not directly comparable to ours; among other factors, their task used a dataset trained on science curriculum descriptions, not text written at different levels of difficulty.", "labels": [], "entities": []}, {"text": "There also appear to be few reliable studies of human-human interlabeler agreement.", "labels": [], "entities": []}, {"text": "Avery limited study by gave a mean interlabeler standard deviation of 1.67 grade levels, but this study was limited to just 3 samples across 10 judges.", "labels": [], "entities": [{"text": "interlabeler standard deviation", "start_pos": 35, "end_pos": 66, "type": "METRIC", "confidence": 0.9036261836687723}]}, {"text": "Nevertheless, we believe that an objective element to readability assessment exists, and we state our main results in terms of correlation with difficulty level, so that at least abroad comparison with existing measures is possible.", "labels": [], "entities": [{"text": "readability assessment", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.8423512578010559}]}, {"text": "Our evaluation looked at four aspects of the model.", "labels": [], "entities": []}, {"text": "First, we measured how well the model trained on our Web corpus generalized to other, previously unseen, test data.", "labels": [], "entities": []}, {"text": "Second, we looked at the effect of passage length on accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9963023662567139}]}, {"text": "Third, we estimated the effect of additional training data on the accuracy of the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9994379878044128}]}, {"text": "Finally, we looked at how well the model could be extended to a language other than English -in this study, we give results for French.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Correlations between predictors and grade level, for the English collections used in our study.  All predictors were trained on the Web corpus, with the Web tests using 10-fold cross-validation.", "labels": [], "entities": []}]}