{"title": [], "abstractContent": [{"text": "In this paper we address the question of assigning semantic roles to sentences in Chinese.", "labels": [], "entities": [{"text": "assigning semantic roles to sentences in Chinese", "start_pos": 41, "end_pos": 89, "type": "TASK", "confidence": 0.8556776217051915}]}, {"text": "We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.7253519296646118}]}, {"text": "In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our head-rules in the appendix.", "labels": [], "entities": [{"text": "Collins", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9539629220962524}]}, {"text": "Finally, we compare English and Chinese semantic-parsing performance.", "labels": [], "entities": []}, {"text": "While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese.", "labels": [], "entities": [{"text": "argument labeling", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.6970140784978867}]}, {"text": "We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese.", "labels": [], "entities": []}], "introductionContent": [{"text": "Thematic roles (AGENT, THEME, LOCATION, etc) provide a natural level of shallow semantic representation fora sentence.", "labels": [], "entities": [{"text": "AGENT", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.9781766533851624}, {"text": "THEME", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9487721920013428}, {"text": "LOCATION", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9732970595359802}]}, {"text": "A number of algorithms have been proposed for automatically assigning such shallow semantic structure to English sentences.", "labels": [], "entities": [{"text": "automatically assigning such shallow semantic structure to English sentences", "start_pos": 46, "end_pos": 122, "type": "TASK", "confidence": 0.6746711532274882}]}, {"text": "But little is understood about how these algorithms may perform in other languages, and in general the role of language-specific idiosyncracies in the extraction of semantic content and how to train these algorithms when large hand-labeled training sets are not available.", "labels": [], "entities": []}, {"text": "In this paper we address the question of assigning semantic roles to sentences in Chinese.", "labels": [], "entities": [{"text": "assigning semantic roles to sentences in Chinese", "start_pos": 41, "end_pos": 89, "type": "TASK", "confidence": 0.8556776217051915}]}, {"text": "Our work is based on the SVM-based algorithm proposed for English by.", "labels": [], "entities": []}, {"text": "We first describe our creation of a small 1100-sentence Chinese corpus labeled according to principles from the English and (in-progress) Chinese PropBanks.", "labels": [], "entities": []}, {"text": "We then introduce the features used by our SVM classifier, and show their performance on semantic parsing for both seen and unseen verbs, given hand-corrected (Chinese TreeBank) syntactic parses.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.7510855793952942}, {"text": "Chinese TreeBank) syntactic parses", "start_pos": 160, "end_pos": 194, "type": "DATASET", "confidence": 0.8385143876075745}]}, {"text": "We then describe our port of the Collins (1999) parser to Chinese.", "labels": [], "entities": [{"text": "Collins (1999) parser", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.9626775026321411}]}, {"text": "Finally, we apply our SVM semantic parser to a matching English corpus, and discuss the differences between English and Chinese that lead to significantly better performance on Chinese.", "labels": [], "entities": [{"text": "SVM semantic parser", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7003299196561178}]}], "datasetContent": [{"text": "We now test the performance of our classifier, trained on the 1025-sentence training set and tested on the 113-sentence test set introduced in Section 2.2.", "labels": [], "entities": []}, {"text": "Recall that in this 'stratified' test set, each verb has been seen in the training data.", "labels": [], "entities": []}, {"text": "The last row in shows the current best performance of our system on this test set.", "labels": [], "entities": []}, {"text": "The preceding rows show various subsets of the feature set, beginning with the path feature.", "labels": [], "entities": []}, {"text": "As shows, the most important feature is path, followed by target verb and headword.", "labels": [], "entities": []}, {"text": "In general, the lexicalized features are more important than the other features.", "labels": [], "entities": []}, {"text": "The combined feature set outperforms any other feature sets with less features and it has an Fscore of 83.1.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9991061091423035}]}, {"text": "The performance is better for the arguments (i.e., only ARG0-2), 86.7 for arg0 and 89.4 for arg1.", "labels": [], "entities": [{"text": "ARG0-2", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.7764541506767273}]}, {"text": "To test the performance of the semantic parser on unseen verbs, we used cross-validation, selecting one verb as test and the other 9 as training, and iterating with each verb as test.", "labels": [], "entities": []}, {"text": "All the results are given in.", "labels": [], "entities": []}, {"text": "The results for some verbs are almost equal to the performance on seen verbs.", "labels": [], "entities": []}, {"text": "For example for \"\u00b7 \u00a2 \u00b1 \u00ed \" and \"\u00cd \u00a8 \u00b9 \u00fd \", the F-scores are over 80.", "labels": [], "entities": [{"text": "\u00cd \u00a8 \u00b9 \u00fd", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.917827695608139}, {"text": "F-scores", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9991757273674011}]}, {"text": "However, for some verbs, the results are much worse.", "labels": [], "entities": []}, {"text": "The worst case is the verb \"\u00b3 \u00f6 \u00cf \u00d6 \", which has an F-score of 11.", "labels": [], "entities": [{"text": "\u00d6", "start_pos": 34, "end_pos": 35, "type": "METRIC", "confidence": 0.9593716859817505}, {"text": "F-score", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9995457530021667}]}, {"text": "This is due to the special syntactic characteristics of this verb.", "labels": [], "entities": []}, {"text": "This verb can only have one argument and this argument most often follows the verb, in object position.", "labels": [], "entities": []}, {"text": "In the surface structure, there is often an NP before the verb working as its subject, but semantically this subject cannot be analyzed as arg0.", "labels": [], "entities": []}, {"text": "/depression\" in (2), are analyzed as arg0.", "labels": [], "entities": [{"text": "arg0", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9934792518615723}]}, {"text": "But the parser classified the subjects as arg0 and the objects as arg1.", "labels": [], "entities": [{"text": "arg0", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9555948376655579}]}, {"text": "These are correct for most common verbs but wrong for this particular verb.", "labels": [], "entities": []}, {"text": "It is difficult to know how common this problem would be in a larger, test set.", "labels": [], "entities": []}, {"text": "The fact that we considered diversity of syntactic behavior when selecting verbs certainly helps make this test set reflect the difficult cases.", "labels": [], "entities": []}, {"text": "If most verbs prove not to be as idiosyncratic as \"\u00b3 \u00f6 \u00cf \u00d6 /emerge\", the real performance of the parser on unseen verbs maybe better than the average given here. that he rarely gave his blessing to the claptrap that passes for consensus in various international institutions.", "labels": [], "entities": []}, {"text": "In (a), arg2 represents the goal of \"give\", in (b), it represents the amount of increase, and in (c) it represents yet another role.", "labels": [], "entities": [{"text": "arg2", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9840410947799683}]}, {"text": "These complete different semantic relations are given the same semantic label.", "labels": [], "entities": []}, {"text": "For unseen verbs, this makes it difficult for the semantic parser to know what would count as an arg2.", "labels": [], "entities": []}, {"text": "As in our Chinese experiments, we used our SVMbased classifier, using N one-versus-all classifiers.", "labels": [], "entities": []}, {"text": "shows the performance on our English test set (with Chinese for comparison), beginning with the path feature, and incrementally adding features until in the last row we combine all 8 features together. that using similar verbs, the same amount of data, the same classifier, the same number of roles, and the same features, the results from English are much worse than those for Chinese.", "labels": [], "entities": [{"text": "English test set", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.738338569800059}]}, {"text": "While some part of the difference is probably due to idiosyncracies of particular sentences in the English and Chinese data, other aspects of the difference might be accounted for systematically, as we discuss in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2 List of verbs for experiments  Verb  # of  senses  Arg  number  Freq", "labels": [], "entities": [{"text": "Verb", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9328451156616211}, {"text": "Arg  number  Freq", "start_pos": 60, "end_pos": 77, "type": "METRIC", "confidence": 0.8272417585055033}]}, {"text": " Table 3 The positional distribution of roles", "labels": [], "entities": []}, {"text": " Table 4 Top 20 head words for roles  Word  Freq Word  Freq  \u00d4 \u00da /in  214  \u00d6 \u00d0 \u00b9 \u00fa  /China  25  \u00bb  \u00e1  \u00d2  \u00e9  /meeting  43   \u00b6 \u00d4 /for  23  \u00bd  \u00f1  \u00cc  \u00ec  /today  41  \u00c9 \u00f9 \u00c3 \u00f7  /statement  19  \u00d3 \u00da /at  40  \u00bd \u00b2 \u00bb \u00b0  /speech  18  \u00d2 \u00d1 /already  38  \u00bd \u00d7  \u00b6 \u00ce  /stage  17  AE  \u00f3  \u00d2  \u00b5  /enterprise 35  \u00d5 \u00fe \u00b8 \u00ae  /government 16  \u00b9  \u00ab  \u00cb  \u00be  /company 32  \u00c4 \u00bf \u00c7 \u00b0  /present  16  \u00b1 \u00c8 /than  31  \u00d2 \u00f8 \u00d0 \u00d0  /bank  15  \u00bd \u00ab /will  30  \u00c8 \u00d5 \u00c7 \u00b0  /recently  14  \u00d2  \u00c7  \u00ca  \u00bd  /ceremony 28  \u00bb \u00f9 \u00b5 \u00d8  /base  14", "labels": [], "entities": [{"text": "AE", "start_pos": 261, "end_pos": 263, "type": "METRIC", "confidence": 0.8882890939712524}]}, {"text": " Table 5 Semantic parsing results on seen verbs  feature set  P  R  F  (%)  (%)  (%)  path  71.8  59.4  65.0  path + pt  72.9  62.9  67.5  path + position  72.5  60.8  66.2  path + head POS  77.6  63.3  69.7  path + sub-cat  80.8  63.6  71.2  path + head word  85.0  66.0  74.3  path + target verb  85.8  68.4  76.1  path + pt + gov + position  + subcat + target  + head word  + head POS  91.7  76.0  83.1", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.817774772644043}]}, {"text": " Table 6 Experimental Results for Unseen Verbs  target  P(%)  R(%)  F(%)  \u00b7  \u00a2  \u00b1  \u00ed  /publish  90.7  72.9  80.8  \u00d4  \u00f6  \u00bc  \u00d3  /increase  49.6  34.3  40.5  \u00be  \u00d9  \u00d0  \u00d0  /take place  90.1  63.3  74.4  \u00bd  \u00a8  \u00b3  \u00c9  /build into  65.2  55.5  60.0  \u00b8  \u00f8  \u00d3  \u00e8  /give  65.7  37.9  48.1  \u00cd  \u00a8  \u00b9  \u00fd  /pass  85.9  77.0  81.2  \u00b3  \u00f6  \u00cf  \u00d6  /emerge  12.6  10.2  11.3  \u00bd  \u00f8  \u00c8  \u00eb  /enter  81.9  58.8  68.4  \u00b3  \u00c9  \u00c1  \u00a2  /set up  79.0  61.1  68.9  \u00cf  \u00a3  \u00cd  \u00fb  /hope  77.7  35.9  49.1  Average  69.8  50.7  58.3  Another important difficulty in processing unseen  verbs is the fact that roles in PropBank are defined in a  verb-dependent way. This may be easiest to see with an  English example. The roles arg2, arg3, arg4 have  different meaning for different verbs; underlined in the  following are some examples of arg2:  (a) The state gave CenTrust 30 days to sell the Rubens.  (b) Revenue increased 11 to 2.73 billion from 2.46  billion.  (c) One of Ronald Reagan 's attributes as President was", "labels": [], "entities": []}, {"text": " Table 7 Results for syntactic parsing, trained on  CTB Release 2, tested on test set in semantic parsing  LP(%)  LR(%)  F1(%)  overall  81.6  82.1  81.0  len<=40  86.1  85.5  86.7", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8326762020587921}, {"text": "CTB Release 2", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9706905484199524}, {"text": "semantic parsing  LP(%)  LR(%)  F1(%)  overall  81.6  82.1  81.0  len", "start_pos": 89, "end_pos": 158, "type": "METRIC", "confidence": 0.7587896012342893}]}, {"text": " Table 8  Comparison with other parsers: TEST2  \u2264 40 words  LP(%) LR(%) F1(%)  Bikel & Chiang 2000  77.2  76.2  76.7  Chiang & Bikel 2002  81.1  78.8  79.9  Levy & Manning 2003  78.4  79.2  78.8  Collins parser  86.4  85.5  85.9", "labels": [], "entities": [{"text": "TEST2", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9781307578086853}, {"text": "F1", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.7990433573722839}, {"text": "Chiang 2000  77.2  76.2  76.7  Chiang & Bikel 2002  81.1  78.8  79.9  Levy & Manning 2003  78.4  79.2  78.8  Collins parser  86.4  85.5  85.9", "start_pos": 87, "end_pos": 228, "type": "DATASET", "confidence": 0.7849302552640438}]}, {"text": " Table 9 Result for semantic parsing using automatic  syntactic parses  P(%)  R(%)  F(%)  110 sentences  86.0  70.8  77.6  113 sentences  86.0  69.2  76.7", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.761967122554779}]}, {"text": " Table 11  Experimental results of English  Chinese  English  feature set  R/F/P  P/R/F  path  71.8/59.4/65.0  78.2/48.3/59.7  path + pt  72.9/62.9/67.5  77.4/51.2/61.6  path + position 72.5/60.8/66.2  75.7/50.9/60.8  path + hd POS 77.6/63.3/69.7  79.1/49.7/61.0  path + sub-cat  80.8/63.6/71.2  79.9/45.3/57.8  path + hd word 85.0/66.0/74.3  84.0/47.7/60.8  path + target  85.8/68.4/76.1  85.7/49.1/62.5  COMBINED  91.7/76.0/83.1  84.1/62.2/71.5", "labels": [], "entities": []}, {"text": " Table 13. Improvement in English semantic parsing  with the addition of the voice feature  -voice  +voice  P  R F  P  R  F  arg0  88.9 75.3 81.5  94.4 80 86.6  arg1  86.5 82.8 84.6  88.5 86.2 87.", "labels": [], "entities": [{"text": "English semantic parsing", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.6451129416624705}]}]}