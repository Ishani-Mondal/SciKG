{"title": [{"text": "Speaker Recognition with Mixtures of Gaussians with Sparse Regression Matrices", "labels": [], "entities": [{"text": "Speaker Recognition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7827268540859222}]}], "abstractContent": [{"text": "When estimating a mixture of Gaussians there are usually two choices for the covariance type of each Gaussian component.", "labels": [], "entities": []}, {"text": "Either diagonal or full covariance.", "labels": [], "entities": []}, {"text": "Imposing a structure though maybe restrictive and lead to degraded performance and/or increased computations.", "labels": [], "entities": []}, {"text": "In this work, several criteria to estimate the structure of regression matrices of a mixture of Gaussians are introduced and evaluated.", "labels": [], "entities": []}, {"text": "Most of the criteria attempt to estimate a discriminative structure, which is suited for classification tasks.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.9137296080589294}]}, {"text": "Results are reported on the 1996 NIST speaker recognition task and performance is compared with structural EM, a well-known, non-discriminative, structure-finding algorithm.", "labels": [], "entities": [{"text": "NIST speaker recognition task", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.7326888516545296}]}], "introductionContent": [{"text": "Most state-of-the-art systems in speech and speaker recognition use mixtures of Gaussians when fitting a probability distribution to data.", "labels": [], "entities": [{"text": "speaker recognition", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7508493065834045}]}, {"text": "Reasons for this choice are the easily implementable estimation formulas and the modeling power of mixtures of Gaussians.", "labels": [], "entities": []}, {"text": "For example, a mixture of diagonal Gaussians can still model dependencies on the global level.", "labels": [], "entities": []}, {"text": "An established practice when applying mixtures of Gaussians is to use either full or diagonal covariances.", "labels": [], "entities": []}, {"text": "However, imposing a structure may not be optimum and a more general methodology should allow for joint estimation of both the structure and parameter values.", "labels": [], "entities": []}, {"text": "The first question we have to answer is what type of structure we want to estimate.", "labels": [], "entities": []}, {"text": "For mixtures of Gaussians there are three choices.", "labels": [], "entities": []}, {"text": "Covariances, inverse covariances or regression matrices.", "labels": [], "entities": []}, {"text": "For all cases, we can see as selecting a structure by introducing zeros in the respective matrix.", "labels": [], "entities": []}, {"text": "The three structures are distinctively different and zeros in one matrix do not, in general, map to zeros in another matrix.", "labels": [], "entities": []}, {"text": "For example, we can have sparse covariance but full inverse covariance or sparse inverse covariance and full regression matrix.", "labels": [], "entities": []}, {"text": "There are no clear theoretical reasons why one choice of structure is more suitable than others.", "labels": [], "entities": []}, {"text": "However, introducing zeros in the inverse covariance can be seen as deleting arcs in an Undirected Graphical Model (UGM) where each node represents each dimension of a single Gaussian).", "labels": [], "entities": []}, {"text": "Similarly, introducing zeros in the regression matrix can be seen as deleting arcs in a Directed Graphical Model (DGM).", "labels": [], "entities": []}, {"text": "There is a rich body of work on structure learning for UGM and DGM and therefore the view of a mixture of Gaussians as a mixture of DGM or UGM maybe advantageous.", "labels": [], "entities": []}, {"text": "Under the DGM framework, the problem of Gaussian parameter estimation can be cast as a problem of estimating linear regression coefficients.", "labels": [], "entities": [{"text": "Gaussian parameter estimation", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.5789724191029867}]}, {"text": "Since the specific problem of selecting features for linear regression has been encountered in different fields in the past, we adopt the view of a mixture of Gaussians as a mixture of DGM.", "labels": [], "entities": []}, {"text": "In), the problem of introducing zeros in regression matrices of a mixture of Gaussians was presented.", "labels": [], "entities": []}, {"text": "The approach taken was to set to zero the pairs with the lowest mutual information, i.e. b m i,j = 0 \u21d0\u21d2 I(X i , X j ) \u2248 0, where m is the Gaussian index and b i,j is the (i, j) element of regression matrix B.", "labels": [], "entities": []}, {"text": "The approach was tested for the task of speech recognition in a limited vocabulary corpus and was shown to offer the same performance with the mixture of full-covariance Gaussians with 30% less parameters.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7995844781398773}]}, {"text": "One issue with the work in) is that the structureestimation criterion that was used was not discriminative.", "labels": [], "entities": []}, {"text": "For classification tasks, like speaker or speech recognition, discriminative parameter estimation approaches achieve better performance than generative ones, but are in general hard to estimate especially fora high number of classes.", "labels": [], "entities": [{"text": "speaker or speech recognition", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.574431985616684}]}, {"text": "In this work, a number of discriminative structure-estimation criteria tailored for the task of speaker recognition are introduced.", "labels": [], "entities": [{"text": "speaker recognition", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.8315151631832123}]}, {"text": "We avoid the complexities of discriminative parameter estimation by estimating a discriminative structure and then applying generative parameter estimation techniques.", "labels": [], "entities": [{"text": "generative parameter estimation", "start_pos": 124, "end_pos": 155, "type": "TASK", "confidence": 0.8356554905573527}]}, {"text": "Thus, overall the models attempt to model the discriminability between classes without the numerical and implementation diffi- This paper is structured as follows.", "labels": [], "entities": []}, {"text": "In section 2, the view of a Gaussian as a directed graphical model is presented.", "labels": [], "entities": []}, {"text": "In section 3, discriminative and generative structure-estimation criteria for the task of speaker recognition are detailed, along with a description of the structural EM algorithm.", "labels": [], "entities": [{"text": "speaker recognition", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8309946656227112}]}, {"text": "In section 4, the application task is described and the experiments are presented.", "labels": [], "entities": []}, {"text": "Finally, in section 5, a summary and possible connections of this work with the speaker adaptation problem are discussed.", "labels": [], "entities": [{"text": "speaker adaptation", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.902246505022049}]}], "datasetContent": [{"text": "We evaluated our approach in the male subset of the 1996 NIST speaker recognition task).", "labels": [], "entities": [{"text": "NIST speaker recognition task", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.7252468541264534}]}, {"text": "The problem can be described as following.", "labels": [], "entities": []}, {"text": "Given 21 target speakers, perform 21 binary classifications (one for each target speaker) for each one of the test sentences.", "labels": [], "entities": []}, {"text": "Each one of the binary classifications is a YES if the sentence belongs to the target speaker and NO otherwise.", "labels": [], "entities": [{"text": "YES", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9907783269882202}]}, {"text": "Under this setting, one sentence maybe decided to have been generated by more than one speaker, in which case there will beat least one false alarm.", "labels": [], "entities": []}, {"text": "Also, some of the test sentences were spoken by non-target speakers (impostors) in which case the correct answer would be 21 NO.", "labels": [], "entities": [{"text": "NO", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.9828926920890808}]}, {"text": "All speakers are male and the data are from the Switchboard database ().", "labels": [], "entities": [{"text": "Switchboard database", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.9213835597038269}]}, {"text": "There are approximately 2 minutes of training data for each target speaker.", "labels": [], "entities": []}, {"text": "All the training data fora speaker come from the same session and the testing data come from different sessions, but from the same handset type and phone number (matched conditions).", "labels": [], "entities": []}, {"text": "The algorithms were evaluated on sentence sizes of three and thirty seconds.", "labels": [], "entities": []}, {"text": "The features are 20-dimensional MFCC vectors, cepstrum mean normalized and with all silences and pauses removed.", "labels": [], "entities": []}, {"text": "In the test data there are impostors who don't appear in the training data and maybe of different gender than the target speakers.", "labels": [], "entities": []}, {"text": "A mixture of Gaussians is trained on each one of the target speakers.", "labels": [], "entities": []}, {"text": "For impostor modeling, a separate model is estimated for each gender.", "labels": [], "entities": [{"text": "impostor modeling", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.9255029261112213}]}, {"text": "There are 43 impostors for each gender, each impostor with 2 minutes of speech.", "labels": [], "entities": []}, {"text": "Same-gender speakers are pooled together and a mixture of 100 diagonal Gaussians is estimated on each pool.", "labels": [], "entities": []}, {"text": "Impostor models remained fixed for all the experiments reported in this work.", "labels": [], "entities": [{"text": "Impostor", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9764397740364075}]}, {"text": "During testing and because some of the impostors are of different gender than the target speakers, each test sentence is evaluated against both impostor models and the one with the highest log-likelihood is chosen.", "labels": [], "entities": []}, {"text": "For each test sentence the log-likelihood of each target speaker's model is subtracted from the loglikelihood of the best impostor model.", "labels": [], "entities": []}, {"text": "A decision for YES is made if the difference of the log-likelihoods is above a threshold.", "labels": [], "entities": [{"text": "YES", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.7628750205039978}]}, {"text": "Although in real operation of the system the thresholds are parameters that need to be estimated from the training data, in this evaluation the thresholds are optimized for the current test set.", "labels": [], "entities": []}, {"text": "Therefore the results reported should be viewed as a best case scenario, but are nevertheless useful for comparing different approaches.", "labels": [], "entities": []}, {"text": "The metric used in all experiments was Equal Error Rate (EER).", "labels": [], "entities": [{"text": "Equal Error Rate (EER)", "start_pos": 39, "end_pos": 61, "type": "METRIC", "confidence": 0.974565734465917}]}, {"text": "EER is defined as the point where the probability of false alarms is equal to the probability of missed detections.", "labels": [], "entities": [{"text": "EER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9569815993309021}]}, {"text": "Standard NIST software tools were used for the evalution of the algorithms ().", "labels": [], "entities": []}, {"text": "It should be noted that the number of components per Gaussian is kept the same for all speakers.", "labels": [], "entities": []}, {"text": "A scheme that allowed for different number of Gaussians per speaker did not show any gains.", "labels": [], "entities": [{"text": "Gaussians", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9483546018600464}]}, {"text": "Also, the number of components is optimized on the test set which will not be the casein the real operation of the system.", "labels": [], "entities": []}, {"text": "However, since there are only a few discrete values for the number of components and EER was not particularly sensitive to that parameter, we do not view this as a major problem.", "labels": [], "entities": [{"text": "EER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9844345450401306}]}, {"text": "shows the EER obtained for different baseline systems.", "labels": [], "entities": [{"text": "EER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9968769550323486}]}, {"text": "Each cell contains two EER numbers, the left is for 30-second test utterances and the right for 3-second.", "labels": [], "entities": [{"text": "EER", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9803589582443237}]}, {"text": "For the Diagonal case 35 components were used, while for the full case 12 components were used.", "labels": [], "entities": []}, {"text": "The Random case corresponds to randomly zeroing 10% of the regression coefficients of a mixture of 16 components.", "labels": [], "entities": []}, {"text": "This particular combination of number of parameters pruned and number of components was shown to provide the best results fora subset of the test set.", "labels": [], "entities": []}, {"text": "All structure-finding experiments are with the same number of components and percent of regression coefficients pruned.", "labels": [], "entities": []}, {"text": "shows the EER obtained for different baseline Algorithm: Finding both structure and parameter values using structural EM Start with the full model fora given number of Gaussians while (number of pruned regression coefficients < T ) E \u2212 step: Collect sufficient statistics forgiven structure, i.e, \u03b3 m (n) = p(z n = m|x n , M old ) StructureSearch: Remove one arc from a Gaussian at a time, i.e. set b m i,j = 0.", "labels": [], "entities": [{"text": "EER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9710915088653564}]}, {"text": "The score associated with zeroing a single regression coefficient is.", "labels": [], "entities": []}, {"text": "Order coefficients in ascending order of score.", "labels": [], "entities": []}, {"text": "P is the set of the first K coefficients.", "labels": [], "entities": []}, {"text": "Set the new structure M new as M new = M old \\{P }.", "labels": [], "entities": []}, {"text": "M \u2212 step: Calculate the new parameters given M new . This step can be followed by a number of EM iterations to obtain better parameter values.", "labels": [], "entities": []}, {"text": "end   From we can see improved results from the full-covariance case but results are not better than the diagonal-covariance case.", "labels": [], "entities": []}, {"text": "All criteria appear to perform similarly.", "labels": [], "entities": []}, {"text": "Table 4 also shows that zeroing the regression coefficients with the maximum of each criterion function does not lead to systems with much different performance.", "labels": [], "entities": []}, {"text": "Also from we can see that randomly zeroing regression coefficients performs approximately the same as taking the minimum or maximum.", "labels": [], "entities": []}, {"text": "These numbers, seem to suggest that the structure of a mixture of Gaussians is not a critical issue for speaker recognition, at least with the current structure-estimation criteria used.", "labels": [], "entities": [{"text": "speaker recognition", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.8685061931610107}]}], "tableCaptions": [{"text": " Table 2: The Structural EM algorithm for a mixture of Gaussians", "labels": [], "entities": [{"text": "Structural EM", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.8575882911682129}]}, {"text": " Table 3: Baseline EER, left number is for 30-second test  utterances and right number for 3-second", "labels": [], "entities": [{"text": "EER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7867962718009949}]}, {"text": " Table 4: EER for different sparse structures, left number  is for 30 second test utterances and right number for 3- second.", "labels": [], "entities": [{"text": "EER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9982194304466248}]}]}