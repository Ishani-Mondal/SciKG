{"title": [{"text": "Comparison of Two Interactive Search Refinement Techniques", "labels": [], "entities": [{"text": "Interactive Search Refinement", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.6726177136103312}]}], "abstractContent": [{"text": "The paper presents two approaches to interactively refining user search formulations and their evaluation in the new High Accuracy Retrieval from Documents (HARD) track of TREC-12.", "labels": [], "entities": [{"text": "High Accuracy Retrieval from Documents (HARD) track of TREC-12", "start_pos": 117, "end_pos": 179, "type": "DATASET", "confidence": 0.5820436071265828}]}, {"text": "One method consists of asking the user to select a number of sentences that may represent relevant documents, and then using the documents, whose sentences were selected for query expansion.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 174, "end_pos": 189, "type": "TASK", "confidence": 0.7362377941608429}]}, {"text": "The second method consists of showing to the user a list of noun phrases, extracted from the initial document set, and then expanding the query with the terms from the phrases selected by the user.", "labels": [], "entities": []}], "introductionContent": [{"text": "Query expansion following relevance feedback is a well established technique in information retrieval, which aims at improving user search performance.", "labels": [], "entities": [{"text": "Query expansion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8559289276599884}, {"text": "information retrieval", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.7930643558502197}]}, {"text": "It combines user and system effort towards selecting and adding extra terms to the original query.", "labels": [], "entities": []}, {"text": "The traditional model of query expansion following relevance feedback is as follows: the user reads a representation of a retrieved document, typically its full-text or abstract, and provides the system with a binary relevance judgement.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7781660556793213}]}, {"text": "After that the system extracts query expansion terms from the document, which are then added to the query either manually by the searcher -interactive query expansion, or automatically -automatic query expansion.", "labels": [], "entities": []}, {"text": "Intuitively interactive query expansion should produce better results than automatic, however this is not consistently so (.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7033738493919373}]}, {"text": "In this paper we present two new approaches to automatic and interactive query expansion, which we developed and tested within the framework of the High Accuracy Retrieval from Documents (HARD) track of TREC (Text Retrieval Conference).", "labels": [], "entities": [{"text": "query expansion", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.7280331254005432}, {"text": "High Accuracy Retrieval from Documents (HARD) track of TREC (Text Retrieval Conference)", "start_pos": 148, "end_pos": 235, "type": "TASK", "confidence": 0.6751143485307693}]}], "datasetContent": [{"text": "Every run submitted to the HARD track was evaluated in three different ways.", "labels": [], "entities": [{"text": "HARD track", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.8503967225551605}]}, {"text": "The first two evaluations are done at the document level only, whereas the last one takes into account the granularity metadata.", "labels": [], "entities": []}, {"text": "1. SOFT-DOC -document-level evaluation, where only the traditional TREC topic formulations (title, description, narrative) are used as relevance criteria.", "labels": [], "entities": []}, {"text": "2. HARD-DOC -the same as the above, plus 'purpose', 'genre' and 'familiarity' metadata are used as additional relevance criteria.", "labels": [], "entities": [{"text": "HARD-DOC", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9730336666107178}]}, {"text": "3. HARD-PSG -passage-level evaluation, which in addition to all criteria in HARD-DOC also requires that retrieved items satisfy the granularity metadata).", "labels": [], "entities": []}, {"text": "Document-level evaluation was done by the traditional IR metrics of mean average precision and precision at various document cutoff points.", "labels": [], "entities": [{"text": "Document-level evaluation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.80633145570755}, {"text": "IR", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9248321652412415}, {"text": "mean average precision", "start_pos": 68, "end_pos": 90, "type": "METRIC", "confidence": 0.8159145911534628}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9976714253425598}]}, {"text": "In this paper we focus on document-level evaluation.", "labels": [], "entities": []}, {"text": "Passagelevel evaluation is discussed elsewhere ().", "labels": [], "entities": [{"text": "Passagelevel evaluation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6825661212205887}]}, {"text": "For all of our runs we used Okapi BSS (Basic Search System).", "labels": [], "entities": [{"text": "Okapi BSS", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.795979917049408}]}, {"text": "For the baseline run we used keywords from the title field only, as these proved to be most effective in our preliminary experiments described in section 2.2.", "labels": [], "entities": []}, {"text": "Topic titles were parsed in Okapi, weighted and searched using BM25 function against the HARD track corpus.", "labels": [], "entities": [{"text": "HARD track corpus", "start_pos": 89, "end_pos": 106, "type": "DATASET", "confidence": 0.89544677734375}]}, {"text": "Document-level results of the three submitted runs are given in table 1.", "labels": [], "entities": []}, {"text": "UWAThard1 is the baseline run using original query terms from the topic titles.", "labels": [], "entities": [{"text": "UWAThard1", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9557844996452332}]}, {"text": "UWAThard2 is a final run using query expansion method 1, outlined earlier, plus the granularity and known relevant documents metadata.", "labels": [], "entities": [{"text": "UWAThard2", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9543015956878662}]}, {"text": "UWAThard3 is a final run using query expansion method 2 plus the granularity metadata.", "labels": [], "entities": [{"text": "UWAThard3", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9503763318061829}]}, {"text": "The fact that the query expansion method 1 (UWAThard2) produced no improvement over the baseline (UWAThard1) was a surprise, and did not correspond to our training runs with the Financial Times and Los Angeles Times collections, which showed 21% improvement over the original title-only query run.", "labels": [], "entities": [{"text": "Financial Times and Los Angeles Times collections", "start_pos": 178, "end_pos": 227, "type": "DATASET", "confidence": 0.848798223904201}]}, {"text": "We evaluated the user selection of the sentence using average precision, calculated as the number of relevant sentences selected by the user out of the total number of sentences selected, and average recall -the number of relevant sentences selected by the user out of the total number of relevant sentences shown in the clarification form.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9486480355262756}, {"text": "recall", "start_pos": 200, "end_pos": 206, "type": "METRIC", "confidence": 0.9902428984642029}]}, {"text": "Average precision of TREC sentence selections made by TREC annotators is 0.73, recall -0.69, what is slightly better than our selections during training runs (precision: 0.70, recall: 0.64).", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9992497563362122}, {"text": "recall -0.69", "start_pos": 79, "end_pos": 91, "type": "METRIC", "confidence": 0.9822279612223307}, {"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9987518787384033}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9989650249481201}]}, {"text": "On average 7.14 relevant sentences were included in the forms.", "labels": [], "entities": []}, {"text": "The annotators on average selected 4.9 relevant and 1.8 non-relevant sentences.", "labels": [], "entities": []}, {"text": "shows the number of relevant/non-relevant selected sentences by topic.", "labels": [], "entities": []}, {"text": "It is not clear why query expansion method 1 performed worse in the official UWAThard2 run compared to the training run, given very similar numbers of relevant sentences selected.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.8569925427436829}, {"text": "UWAThard2 run", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.8818365931510925}]}, {"text": "Corpus differences could be one reason for that -HARD corpus contains a large proportion of governmental documents, and we have only evaluated our algorithm on newswire corpora.", "labels": [], "entities": [{"text": "HARD corpus", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.8426259160041809}]}, {"text": "More experiments need to be done to determine the effect of the governmental documents on our query expansion algorithm.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.7548030018806458}]}], "tableCaptions": [{"text": " Table 1. Document-level evaluation results (* runs submitted to TREC)", "labels": [], "entities": [{"text": "TREC", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.6762404441833496}]}]}