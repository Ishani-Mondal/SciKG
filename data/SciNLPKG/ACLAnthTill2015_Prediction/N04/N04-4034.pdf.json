{"title": [], "abstractContent": [{"text": "In conventional language modeling, the words from only one speaker at a time are represented , even for conversational tasks such as meetings and telephone calls.", "labels": [], "entities": []}, {"text": "Ina conversational or meeting setting, however, speakers can have significant influence on each other.", "labels": [], "entities": []}, {"text": "To recover such un-modeled inter-speaker information , we introduce an approach for conversational language modeling that considers words from other speakers when predicting words from the current one.", "labels": [], "entities": [{"text": "conversational language modeling", "start_pos": 84, "end_pos": 116, "type": "TASK", "confidence": 0.6576737662156423}]}, {"text": "By augmenting a normal trigram context, our new multi-speaker language model (MSLM) improves on both Switchboard and ICSI Meeting Recorder corpora.", "labels": [], "entities": [{"text": "ICSI Meeting Recorder corpora", "start_pos": 117, "end_pos": 146, "type": "DATASET", "confidence": 0.887220486998558}]}, {"text": "Using an MSLM and a conditional mutual information based word clustering algorithm , we achieve a 8.9% perplexity reduction on Switchboard and a 12.2% reduction on the ICSI Meeting Recorder data.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.6818503141403198}, {"text": "Switchboard", "start_pos": 127, "end_pos": 138, "type": "DATASET", "confidence": 0.840883195400238}, {"text": "ICSI Meeting Recorder data", "start_pos": 168, "end_pos": 194, "type": "DATASET", "confidence": 0.9453729689121246}]}], "introductionContent": [{"text": "Statistical language models (LMs) are used in many applications such as speech recognition, handwriting recognition, spelling correction, machine translation, and information retrieval.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7803515493869781}, {"text": "handwriting recognition", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.9003720581531525}, {"text": "spelling correction", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.9432643353939056}, {"text": "machine translation", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.8160455226898193}, {"text": "information retrieval", "start_pos": 163, "end_pos": 184, "type": "TASK", "confidence": 0.8401457965373993}]}, {"text": "The goal is to produce a probability model over a word sequence P (W ) = P (w 1 , \u00b7 \u00b7 \u00b7 , w T ).", "labels": [], "entities": []}, {"text": "Conventional language models are often based on a factorized form P (W ) \u2248 \ud97b\udf59 t P (w t |\u03a6(h t )), where ht is the history for wt and \u03a6 is a history mapping.", "labels": [], "entities": []}, {"text": "The case of n-gram language modeling, where \u03a6(h t ) = w t\u2212n+1 \u00b7 \u00b7 \u00b7 w t\u22121 , is widely used.", "labels": [], "entities": [{"text": "n-gram language modeling", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.66429469982783}]}, {"text": "Typically, n = 3, which yields a trigram model.", "labels": [], "entities": []}, {"text": "A refinement of this model is the class-based n-gram where the words are partitioned into equivalence classes.", "labels": [], "entities": []}, {"text": "* This work was funded by NSF under grant IIS-0121396.", "labels": [], "entities": [{"text": "IIS-0121396", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.6673681735992432}]}, {"text": "In general, smoothing techniques are applied to lessen the curse of dimensionality.", "labels": [], "entities": []}, {"text": "Among all methods, modified Kneser-Ney smoothing is widely used because of its good performance.", "labels": [], "entities": [{"text": "Kneser-Ney smoothing", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.5746508538722992}]}, {"text": "Modeling conversational language is a particularly difficult task.", "labels": [], "entities": [{"text": "Modeling conversational language", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9144022663434347}]}, {"text": "Even though conventional techniques work well on read or prepared speech, situations such as telephone conversations or multi-person meetings pose great research challenges due to disfluencies, and odd syntactic/discourse patterns.", "labels": [], "entities": []}, {"text": "Other difficulties include false starts, interruptions, and poor or unrepresented grammar.", "labels": [], "entities": []}, {"text": "Most state-of-the-art language models consider word streams individually and treat different phrases independently.", "labels": [], "entities": []}, {"text": "In this work, we introduce multi-speaker language modeling (MSLM), which models the effects on a speaker of words spoken by other speakers participating in the same conversation or meeting.", "labels": [], "entities": [{"text": "multi-speaker language modeling (MSLM)", "start_pos": 27, "end_pos": 65, "type": "TASK", "confidence": 0.7560349404811859}]}, {"text": "Our new model achieves initial perplexity reductions of 6.2% on Switchboard-I, 5.8% on Switchboard.3% on ICSI Meeting data.", "labels": [], "entities": [{"text": "Switchboard-I", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.8620525002479553}, {"text": "ICSI Meeting data", "start_pos": 105, "end_pos": 122, "type": "DATASET", "confidence": 0.9521324237187704}]}, {"text": "In addition, we developed a word clustering procedure (based on a standard approach () that optimizes conditional word clusters.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7770466506481171}]}, {"text": "Our class-based MSLMs using our new algorithm yield improvements of 7.1% on Switchboard-I, 8.9% on Switchboard Eval-2003, and 12.2% on meetings.", "labels": [], "entities": []}, {"text": "A brief outline follows: Section 2 introduces multispeaker language modeling.", "labels": [], "entities": [{"text": "multispeaker language modeling", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.7675952116648356}]}, {"text": "Section 3 provides initial evaluations on Switchboard and the ICSI Meeting data.", "labels": [], "entities": [{"text": "Switchboard", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.7601572871208191}, {"text": "ICSI Meeting data", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.9658363461494446}]}, {"text": "Section 4 presents evaluations using our class-based multi-speaker language models, and Section 5 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate MSLMs on three corpora: Switchboard-I, Switchboard Eval-2003, and ICSI Meeting data.", "labels": [], "entities": [{"text": "ICSI Meeting data", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.901742696762085}]}, {"text": "In Switchboard-I, 6.83% of the words are overlapped in time, where we define w 1 and w 2 as being overlapped if s(w 1 ) \u2264 s(w 2 ) < e(w 1 ) or s(w 2 ) \u2264 s(w 1 ) < e(w 2 ), where s(\u00b7) and e(\u00b7) are the starting and ending time of a word.", "labels": [], "entities": []}, {"text": "The ICSI Meeting Recorder corpus (Janin et al., 2003) consists of a number of meeting conversations with three or more participants.", "labels": [], "entities": [{"text": "ICSI Meeting Recorder corpus (Janin et al., 2003)", "start_pos": 4, "end_pos": 53, "type": "DATASET", "confidence": 0.9043826298280195}]}, {"text": "The data we employed has 32 conversations, 35,000 sentences and 307,000 total words, where 8.5% of the words were overlapped.", "labels": [], "entities": []}, {"text": "As mentioned previously, we collapse the words from all other speakers into one stream A as a conditioning set for W . The data consists of all speakers taking their turn being W . To be used in an FLM, the words in each stream need to be aligned at discrete time points.", "labels": [], "entities": [{"text": "FLM", "start_pos": 198, "end_pos": 201, "type": "TASK", "confidence": 0.9627563953399658}]}, {"text": "Clearly, at should not come from wt 's future.", "labels": [], "entities": []}, {"text": "Therefore, for each wt , we use the closest previous A word in the past fora t such that s(w t\u22121 ) \u2264 s(a t ) < s(w t ).", "labels": [], "entities": []}, {"text": "Therefore, each at is used only once and no constraints are placed on at 's end time.", "labels": [], "entities": []}, {"text": "This is reasonable since one can often predict a speaker's word after it starts but before it completes.", "labels": [], "entities": []}, {"text": "We score using the model P (w t |w t\u22121 , w t\u22122 , at ).", "labels": [], "entities": []}, {"text": "1 Different back-off strategies, including different back-off paths as well as combination methods, were tried and here we present the best results.", "labels": [], "entities": []}, {"text": "The backoff order (for Switchboard-I and Meeting) first dropped at , then w t\u22122 , w t\u22121 , ending with the uniform distribution.", "labels": [], "entities": []}, {"text": "For Switchboard eval-2003, we used a generalized parallel backoff mechanism.", "labels": [], "entities": [{"text": "Switchboard eval-2003", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.6911948323249817}]}, {"text": "In all cases, modified Kneser-Ney smoothing) was used at all back-off points.", "labels": [], "entities": []}, {"text": "Results on Switchboard-I and the meeting data employed 5-fold cross-validation.", "labels": [], "entities": [{"text": "Switchboard-I", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.8262249827384949}]}, {"text": "Training data for Switchboard eval-2003 consisted of all of Switchboard-I. In Switchboard eval-2003, hand-transcribed time marks are unavailable, so A was available only at the beginning of utterances of W . 2 Results (mean perplexities and standard deviations) are listed in (Switchboard-I and meeting) and the |V | column in.", "labels": [], "entities": [{"text": "A", "start_pos": 149, "end_pos": 150, "type": "METRIC", "confidence": 0.9558660984039307}]}, {"text": "In, the first column shows data set names.", "labels": [], "entities": []}, {"text": "The second and third columns show our best baseline trigram and four-gram perplexities, both of which used interpolation and modified Kneser-Ney at every back-off point.", "labels": [], "entities": []}, {"text": "The trigram outperforms the four-gram.", "labels": [], "entities": []}, {"text": "The fourth column shows the perplexity results with MSLMs and the last column shows the MSLM's relative perplexity reduction over the (better) trigram baseline.", "labels": [], "entities": []}, {"text": "This positive reduction indicates that for both data sets, the utilization of additional information from other speakers can better predict the words of the current speaker.", "labels": [], "entities": []}, {"text": "The improvement is larger in the highly conversational meeting setting since additional speakers, and thus more interruptions, occur.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexities from MSLM on Switchboard-I  (swbd-I) and ICSI Meeting data (mr)", "labels": [], "entities": [{"text": "ICSI Meeting data", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.8476298054059347}]}, {"text": " Table 2: Three types of class-based MSLMs on  Switchboard-I (swbd) and ICSI Meeting (mr) corpora", "labels": [], "entities": [{"text": "ICSI Meeting (mr) corpora", "start_pos": 72, "end_pos": 97, "type": "DATASET", "confidence": 0.7296573966741562}]}, {"text": " Table 3: Class-based MSLM on Switchboard Eval-2003", "labels": [], "entities": []}]}