{"title": [{"text": "Feature-based Pronunciation Modeling for Speech Recognition", "labels": [], "entities": [{"text": "Speech Recognition", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7773563861846924}]}], "abstractContent": [{"text": "We present an approach to pronunciation mod-eling in which the evolution of multiple linguistic feature streams is explicitly represented.", "labels": [], "entities": [{"text": "pronunciation mod-eling", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7366130948066711}]}, {"text": "This differs from phone-based models in that pronunciation variation is viewed as the result of feature asynchrony and changes in feature values, rather than phone substitutions, insertions , and deletions.", "labels": [], "entities": []}, {"text": "We have implemented a flexible feature-based pronunciation model using dynamic Bayesian networks.", "labels": [], "entities": []}, {"text": "In this paper, we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 126, "end_pos": 144, "type": "DATASET", "confidence": 0.9523854553699493}]}, {"text": "The experimental results, as well as the model's qualitative behavior, suggest that this is a promising way of accounting for the types of pronunciation variation often seen in spontaneous speech.", "labels": [], "entities": []}], "introductionContent": [{"text": "Pronunciation variation in spontaneous speech has been cited as a serious obstacle for automatic speech recognition.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.6447301109631857}]}, {"text": "Typical pronunciation models approach this problem by augmenting a phonemic dictionary with additional pronunciations, often resulting from the application of phone substitution, insertion, and deletion rules.", "labels": [], "entities": [{"text": "phone substitution, insertion", "start_pos": 159, "end_pos": 188, "type": "TASK", "confidence": 0.7116705477237701}]}, {"text": "By carefully constructing a rule set (), or by deriving rules or variants from data (), many phenomena can be accounted for.", "labels": [], "entities": []}, {"text": "However, the recognition improvement over a phonemic dictionary is typically modest, and some types of variation remain awkward to represent.", "labels": [], "entities": []}, {"text": "These observations have motivated approaches to speech recognition based on multiple streams of linguistic features rather than a single stream of phones (e.g.,;;).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7997127175331116}]}, {"text": "Most of this work, however, has focused on acoustic modeling, i.e. the mapping between the features and acoustic observations.", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.8117251098155975}]}, {"text": "The pronunciation model is typically still phone-based, limiting the feature values to the target configurations of phones and forcing them to behave as asynchronous \"bundle\".", "labels": [], "entities": []}, {"text": "Some approaches have begun to relax these constraints.", "labels": [], "entities": []}, {"text": "For example, and model asynchronous feature trajectories using hidden Markov models (HMMs), with each state corresponding to a vector of feature values.", "labels": [], "entities": []}, {"text": "This approach is powerful, but it cannot represent independencies between features., in contrast, models the feature streams as independent, except fora requirement that they synchronize at syllable boundaries.", "labels": [], "entities": []}, {"text": "As pointed out by, such independence assumptions may allow for too much variability.", "labels": [], "entities": []}, {"text": "In this paper, we propose a more general featurebased pronunciation model implemented using dynamic Bayesian networks, which allow us to take advantage of inter-feature independencies while avoiding overly strong independence assumptions.", "labels": [], "entities": []}, {"text": "In the following sections, we describe the model and present proof-of-concept experiments using phonetic transcriptions of utterances from the Switchboard conversational speech corpus ().", "labels": [], "entities": [{"text": "Switchboard conversational speech corpus", "start_pos": 143, "end_pos": 183, "type": "DATASET", "confidence": 0.6870120465755463}]}], "datasetContent": [{"text": "We have performed a pilot experiment using the following feature set, based on the vocal tract variables of articulatory phonology: degree of lip opening; tongue tip location and opening degree; tongue body location and opening degree; velum state; and glottal (voicing) state.", "labels": [], "entities": []}, {"text": "We imposed the following synchrony constraints: (1) All four tongue features are completely synchronized; (2) the lips can desynchronize from the tongue by up to one index; and (3) the glottis and velum are synchronized, and their index must be within 2 of the mean index of the tongue and lips.", "labels": [], "entities": []}, {"text": "We used the Graphical Models Toolkit () to implement the model.", "labels": [], "entities": []}, {"text": "The distributions p(S j t |U j t ) were constructed by hand based on linguistic considerations, e.g. that features tend to go from more \"constricted\" values to less constricted ones, but not vice versa.", "labels": [], "entities": []}, {"text": "p(U j t |lexEntry t , ind j t ) was derived from manually-constructed phoneme-to-featureprobability mappings.", "labels": [], "entities": []}, {"text": "For these experiments, no parameter learning has been done.", "labels": [], "entities": []}, {"text": "The task was to recognize an isolated word, given a set of observed surface feature sequences S j t . To create the observations, we used the detailed phonetic transcriptions created at ICSI for the Switchboard corpus (.", "labels": [], "entities": [{"text": "ICSI", "start_pos": 186, "end_pos": 190, "type": "DATASET", "confidence": 0.9514750838279724}, {"text": "Switchboard corpus", "start_pos": 199, "end_pos": 217, "type": "DATASET", "confidence": 0.9367212951183319}]}, {"text": "For each word, we converted its transcription to a sequence of feature vectors, one vector per 10 ms frame.", "labels": [], "entities": []}, {"text": "For this purpose, we divided diphthongs and stops into pairs of feature configurations.", "labels": [], "entities": []}, {"text": "Given the input feature sequences, we computed a Viterbi score for each lexical entry in a 3000+-word (5500+-lexEntry) vocabulary, by \"observing\" the lexEntry variable and finding the most likely settings of all remaining variables.", "labels": [], "entities": []}, {"text": "The most likely variable settings can bethought of as a multistream alignment between the surface and underlying feature streams.", "labels": [], "entities": []}, {"text": "Finally, we output the word corresponding to the highest-scoring lexical entry.", "labels": [], "entities": []}, {"text": "We performed this procedure on a development set of 165 word transcriptions, which was used to tune settings such as synchronization constraints, and a test set of 236 transcriptions 2 . We compared the performance of several models, measured in terms of word error rate (WER) and failure rate (FR), the percentage of inputs that had no Viterbi alignment with the correct word.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 255, "end_pos": 276, "type": "METRIC", "confidence": 0.902082751194636}, {"text": "failure rate (FR)", "start_pos": 281, "end_pos": 298, "type": "METRIC", "confidence": 0.9413288474082947}]}, {"text": "To get a sense of the effect of feature asynchrony, we compared our asynchronous model with aversion in which all features are forced to be synchronized, so that only feature substitution is allowed.", "labels": [], "entities": []}, {"text": "This uses the same DBN, but with degenerate distributions for the synchronization variables.", "labels": [], "entities": []}, {"text": "Also, since the S j values are derived from phonetic transcriptions, and are therefore constant over several frames at a time, we also built a variant of the DBN in which S j is allowed to change value with non-zero probability only when ind j changes (by adding parents ind j t , ind j t\u22121 , S j t\u22121 to S j t ); we refer to this DBN as \"segment-based\", and to the original as \"frame-based\".", "labels": [], "entities": []}, {"text": "We compared four variants, differing along the \"synchronous vs. asynchronous\" and \"frame-based vs. segment-based\" dimensions.", "labels": [], "entities": []}, {"text": "The variant which is both synchronous and segment-based is similar to a phone-based pronunciation model with only context-independent phone substitutions.: Results of Switchboard ranking experiment.", "labels": [], "entities": []}, {"text": "shows the performance of these four models, as well as of two \"baseline\" models: one allowing only the baseform pronunciations (on average 1.7 per word), and another including all pronunciations produced by an extensive set of context-dependent phonological rules (about 4 per word), with no feature substitutions or asynchrony in either case.", "labels": [], "entities": []}, {"text": "The phonological rules are the \"full rule set\" described in.", "labels": [], "entities": []}, {"text": "We note that they were not designed with Switchboard in mind.", "labels": [], "entities": []}, {"text": "The models that allow asynchrony outperform the ones that do not, in terms of both WER and FR.", "labels": [], "entities": [{"text": "WER", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9777175188064575}, {"text": "FR", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9655391573905945}]}, {"text": "Looking more closely at the performance on the development set, the inputs on which the synchronous models failed but the asynchronous models succeeded were in fact the kinds of pronunciations that we expect to arise from feature asynchrony, including: nasals replaced by nasalization on a preceding vowel; a /t r/ sequence realized as /ch/; and everybody \u2192 [eh r uw ay].", "labels": [], "entities": []}, {"text": "The relative merits of the frame-based and segment-based models is less clear, as they have opposite relative performance on the development and test sets.", "labels": [], "entities": []}, {"text": "For 27 (16.4%) development utterances, none of the models was able to find an alignment with the correct word.", "labels": [], "entities": []}, {"text": "Most of these were due to apparent gesture deletions and context-dependent feature changes, which are not yet included in the model.", "labels": [], "entities": []}, {"text": "shows apart of the Viterbi alignment of everybody with [eh r uw ay], produced by the segmentbased, asynchronous model.", "labels": [], "entities": []}, {"text": "Using this model, everybody was the top-ranked word.", "labels": [], "entities": []}, {"text": "As expected, the asynchrony is manifested in the [uw] region, and the lips do not close but reach only a narrow (glide-like) configuration.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Part of a target pronunciation for everybody.  In this feature set, LIP-OPEN is the lip opening degree;  TT-LOC is the location along the palate to which the  tongue tip is closest (alv. = alveolar; ret. = retroflex).", "labels": [], "entities": [{"text": "LIP-OPEN", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.991124153137207}, {"text": "lip opening degree", "start_pos": 94, "end_pos": 112, "type": "METRIC", "confidence": 0.6364197432994843}, {"text": "TT-LOC", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9767352938652039}]}, {"text": " Table 2: Results of Switchboard ranking experiment.", "labels": [], "entities": [{"text": "Switchboard ranking", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8884264230728149}]}]}