{"title": [], "abstractContent": [{"text": "Information extraction techniques automatically create structured databases from un-structured data sources, such as the Web or newswire documents.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8040044605731964}]}, {"text": "Despite the successes of these systems, accuracy will always be imperfect.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9990718364715576}]}, {"text": "For many reasons, it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field.", "labels": [], "entities": []}, {"text": "The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8074372112751007}, {"text": "information extraction", "start_pos": 155, "end_pos": 177, "type": "TASK", "confidence": 0.7743128538131714}]}, {"text": "We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records, obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records.", "labels": [], "entities": [{"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9978006482124329}]}], "introductionContent": [{"text": "Information extraction usually consists of tagging a sequence of words (e.g. a Web document) with semantic labels (e.g. PERSONNAME, PHONENUMBER) and depositing these extracted fields into a database.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8621966242790222}, {"text": "tagging a sequence of words (e.g. a Web document) with semantic labels", "start_pos": 43, "end_pos": 113, "type": "TASK", "confidence": 0.6356442634548459}]}, {"text": "Because automated information extraction will never be perfectly accurate, it is helpful to have an effective measure of the confidence that the proposed database entries are correct.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7110321074724197}]}, {"text": "There are at least three important applications of accurate confidence estimation.", "labels": [], "entities": [{"text": "accurate confidence estimation", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.5612276991208395}]}, {"text": "First, accuracy-coverage trade-offs area common way to improve data integrity in databases.", "labels": [], "entities": [{"text": "accuracy-coverage", "start_pos": 7, "end_pos": 24, "type": "METRIC", "confidence": 0.98049396276474}]}, {"text": "Efficiently making these trade-offs requires an accurate prediction of correctness.", "labels": [], "entities": []}, {"text": "Second, confidence estimates are essential for interactive information extraction, in which users may correct incorrectly extracted fields.", "labels": [], "entities": [{"text": "interactive information extraction", "start_pos": 47, "end_pos": 81, "type": "TASK", "confidence": 0.6081731120745341}]}, {"text": "These corrections are then automatically propagated in order to correct other mistakes in the same record.", "labels": [], "entities": []}, {"text": "Directing the user to the least confident field allows the system to improve its performance with a minimal amount of user effort.", "labels": [], "entities": []}, {"text": "show that using accurate confidence estimation reduces error rates by 46%.", "labels": [], "entities": [{"text": "error rates", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9856831729412079}]}, {"text": "Third, confidence estimates can improve performance of data mining algorithms that depend upon databases created by information extraction systems.", "labels": [], "entities": [{"text": "data mining", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7678749263286591}]}, {"text": "Confidence estimates provide data mining applications with a richer set of \"bottom-up\" hypotheses, resulting in more accurate inferences.", "labels": [], "entities": [{"text": "data mining", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.7391939461231232}]}, {"text": "An example of this occurs in the task of citation co-reference resolution.", "labels": [], "entities": [{"text": "citation co-reference resolution", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.8340189456939697}]}, {"text": "An information extraction system labels each field of a paper citation (e.g. AUTHOR, TITLE), and then co-reference resolution merges disparate references to the same paper.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.7031782120466232}, {"text": "AUTHOR", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.7201676368713379}, {"text": "TITLE", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.5535225868225098}]}, {"text": "Attaching a confidence value to each field allows the system to examine alternate labelings for less confident fields to improve performance.", "labels": [], "entities": []}, {"text": "Sound probabilistic extraction models are most conducive to accurate confidence estimation because of their intelligent handling of uncertainty information.", "labels": [], "entities": [{"text": "Sound probabilistic extraction", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7186723550160726}]}, {"text": "In this work we use conditional random fields (), a type of undirected graphical model, to automatically label fields of contact records.", "labels": [], "entities": []}, {"text": "Here, a record is an entire block of a person's contact information, and afield is one element of that record (e.g. COMPANYNAME).", "labels": [], "entities": [{"text": "COMPANYNAME", "start_pos": 116, "end_pos": 127, "type": "METRIC", "confidence": 0.7411364912986755}]}, {"text": "We implement several techniques to estimate both field confidence and record confidence, obtaining an average precision of 98% for fields and 87% for records.", "labels": [], "entities": [{"text": "field confidence", "start_pos": 49, "end_pos": 65, "type": "METRIC", "confidence": 0.7760273218154907}, {"text": "record confidence", "start_pos": 70, "end_pos": 87, "type": "METRIC", "confidence": 0.756165087223053}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9954635500907898}]}], "datasetContent": [{"text": "2187 contact records (27,560 words) were collected from Web pages and email and 25 classes of data fields were hand-labeled.", "labels": [], "entities": []}, {"text": "The features for the CRF consist of the token text, capitalization features, 24 regular expressions over the token text (e.g. CONTAINSHYPHEN), and offsets of these features within a window of size 5.", "labels": [], "entities": [{"text": "CONTAINSHYPHEN", "start_pos": 126, "end_pos": 140, "type": "METRIC", "confidence": 0.9132272601127625}]}, {"text": "We also use 19 lexicons, including \"US Last Names,\" \"US First Names,\" and \"State Names.\"", "labels": [], "entities": []}, {"text": "Feature induction is not used in these experiments.", "labels": [], "entities": [{"text": "Feature induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.736034095287323}]}, {"text": "The CRF is trained on 60% of the data, and the remaining 40% is split evenly into development and testing sets.", "labels": [], "entities": []}, {"text": "The development set is used to train the maximum entropy classifiers, and the testing set is used to measure the accuracy of the confidence estimates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9992252588272095}]}, {"text": "The CRF achieves an overall token accuracy of 87.32 on the testing data, with a field-level performance of F1 = 84.11, precision = 85.43, and recall = 82.83.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9776483178138733}, {"text": "F1", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9997872710227966}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9996955394744873}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.999700665473938}]}, {"text": "To evaluate confidence estimation, we use three methods.", "labels": [], "entities": [{"text": "confidence estimation", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7791531383991241}]}, {"text": "The first is Pearson's r, a correlation coefficient ranging from -1 to 1 that measures the correlation between a confidence score and whether or not the field (or record) is correctly labeled.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9264898896217346}]}, {"text": "The second is average precision, used in the Information Retrieval community   to evaluate ranked lists.", "labels": [], "entities": [{"text": "average", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9673628807067871}, {"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.7061349749565125}, {"text": "Information Retrieval community", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.8130306204160055}]}, {"text": "It calculates the precision at each point in the ranked list where a relevant document is found and then averages these values.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.999525785446167}]}, {"text": "Instead of ranking documents by their relevance score, here we rank fields (and records) by their confidence score, where a correctly labeled field is analogous to a relevant document.", "labels": [], "entities": []}, {"text": "WORSTCASE is the average precision obtained by ranking all incorrect instances above all correct instances.", "labels": [], "entities": [{"text": "WORSTCASE", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9599062204360962}, {"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9926490187644958}]}, {"text": "show that CFB and MAXENT are statistically similar, and that both outperform competing methods.", "labels": [], "entities": [{"text": "CFB", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.8684915900230408}, {"text": "MAXENT", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.6205674409866333}]}, {"text": "Note that WORSTCASE achieves a high average precision simply because so many fields are correctly labeled.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9885721206665039}]}, {"text": "In all experiments, RANDOM assigns confidence values chosen uniformly at random between 0 and 1.", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.7192368507385254}]}, {"text": "The third measure is an accuracy-coverage graph.", "labels": [], "entities": [{"text": "accuracy-coverage", "start_pos": 24, "end_pos": 41, "type": "METRIC", "confidence": 0.9972389936447144}]}, {"text": "Better confidence estimates push the curve to the upper-right.", "labels": [], "entities": []}, {"text": "shows that CFB and MAXENT dramatically outperform GAMMA.", "labels": [], "entities": [{"text": "CFB", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.8563312292098999}, {"text": "MAXENT", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.7211641073226929}, {"text": "GAMMA", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.6053605079650879}]}, {"text": "Although omitted for space, similar results are also achieved on a noun-phrase chunking task (CFB r = .516, GAMMA r = .432) and a named-entity extraction task (CFB r = .508, GAMMA r = .480).", "labels": [], "entities": [{"text": "noun-phrase chunking task", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.7614730099836985}, {"text": "named-entity extraction task", "start_pos": 130, "end_pos": 158, "type": "TASK", "confidence": 0.8002112507820129}]}], "tableCaptions": [{"text": " Table 1: Evaluation of confidence estimates for field confi-", "labels": [], "entities": [{"text": "field confi-", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.6067782243092855}]}, {"text": " Table 2: Evaluation of confidence estimates for record confi-", "labels": [], "entities": []}]}