{"title": [{"text": "A Unigram Orientation Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Unigram Orientation", "start_pos": 2, "end_pos": 21, "type": "TASK", "confidence": 0.7973233163356781}, {"text": "Statistical Machine Translation", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.8575782577196757}]}], "abstractContent": [{"text": "In this paper, we present a unigram segmen-tation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.7209068338076273}]}, {"text": "The segmentation model uses a novel orientation component to handle swapping of neighbor blocks.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9749644994735718}]}, {"text": "During training, we collect block un-igram counts with orientation: we count how often a block occurs to the left or to the right of some predecessor block.", "labels": [], "entities": []}, {"text": "The orientation model is shown to improve translation performance over two models: 1) no block reordering is used, and 2) the block swapping is controlled only by a language model.", "labels": [], "entities": [{"text": "translation", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9662745594978333}, {"text": "block swapping", "start_pos": 126, "end_pos": 140, "type": "TASK", "confidence": 0.735641211271286}]}, {"text": "We show experimental results on a standard Arabic-English translation task.", "labels": [], "entities": [{"text": "Arabic-English translation task", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.7340705990791321}]}], "introductionContent": [{"text": "In recent years, phrase-based systems for statistical machine translation) have delivered state-of-the-art performance on standard translation tasks.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6580031315485636}]}, {"text": "In this paper, we present a phrase-based unigram system similar to the one in (, which is extended by an unigram orientation model.", "labels": [], "entities": []}, {"text": "The units of translation are blocks, pairs of phrases without internal structure.", "labels": [], "entities": []}, {"text": "shows an example block translation using five Arabic-English blocks . The unigram orientation model is trained from word-aligned training data.", "labels": [], "entities": [{"text": "block translation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7218481004238129}]}, {"text": "During decoding, we view translation as a block segmentation process, where the input sentence is segmented from left to right and the target sentence is generated from bottom to top, one block at a time.", "labels": [], "entities": []}, {"text": "A monotone block sequence is generated except for the possibility to swap a pair of neighbor blocks.", "labels": [], "entities": []}, {"text": "The novel orientation model is used to assist the block swapping: as shown in section 3, block swapping where only a trigram language model is used to compute probabilities between neighbor blocks fails to improve translation performance.) present re-ordering models that make use of a straight/inverted orientation model that is related to our work.", "labels": [], "entities": [{"text": "block swapping", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.722284346818924}, {"text": "block swapping", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.7554971873760223}]}, {"text": "Here, we investigate in detail the effect of restricting the word re-ordering to neighbor block swapping only.", "labels": [], "entities": [{"text": "neighbor block swapping", "start_pos": 81, "end_pos": 104, "type": "TASK", "confidence": 0.7266170581181844}]}, {"text": "In this paper, we assume a block generation process that generates block sequences from bottom to top, one block at a time.", "labels": [], "entities": [{"text": "block generation", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7486324906349182}]}, {"text": "The score of a successor block depends on its predecessor block \u00a9 and on its orientation relative to the block\u00a8\u00a9 block\u00a8block\u00a8\u00a9 . In  ) if it has a left adjacent predecessor.", "labels": [], "entities": []}, {"text": "Accordingly, a block has left orientation ( ) if it has aright adjacent predecessor.", "labels": [], "entities": []}, {"text": "If a block has neither a left or right adjacent predecessor, its orientation is neutral ( ).", "labels": [], "entities": []}, {"text": "The neutral orientation is not modeled explicitly in this paper, rather it is handled as a default case as explained below.", "labels": [], "entities": []}, {"text": "In, the orientation sequence is , since the block translations are mostly monotone.", "labels": [], "entities": []}, {"text": "We try to find a block sequence with orientation The following three types of parameters are used to model the block bigram score . These counts are defined via an enumeration process and are used to define the orientation model is computed as the probability of the first target word in the target clump of given the final two words of the target clump of # \" \u00a1 . The three models are combined in a log-linear way, as shown in the following section.", "labels": [], "entities": []}], "datasetContent": [{"text": "The translation system is tested on an Arabic-to-English translation task. is the baseline block unigram model without re-ordering.", "labels": [], "entities": [{"text": "Arabic-to-English translation task.", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.7775254845619202}]}, {"text": "Here, monotone block alignments are generated: the blocks have only left predecessors (no blocks are swapped).", "labels": [], "entities": []}, {"text": "This is the model presented in (.", "labels": [], "entities": []}, {"text": "model, but additionally uses the orientation component described in Section 2: the block swapping is controlled", "labels": [], "entities": [{"text": "block swapping", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.7328476309776306}]}], "tableCaptions": [{"text": " Table 1: Effect of the orientation model on Arabic- English test data: LDC devtest set and DARPA MT 03  blind test set.", "labels": [], "entities": [{"text": "Arabic- English test data", "start_pos": 45, "end_pos": 70, "type": "DATASET", "confidence": 0.660196590423584}, {"text": "LDC devtest set", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.5860327879587809}, {"text": "DARPA MT 03  blind test set", "start_pos": 92, "end_pos": 119, "type": "DATASET", "confidence": 0.8469161490599314}]}, {"text": " Table 2: Arabic-English example blocks from the de- vtest set: the Arabic phrases are romanized. The example  blocks were swapped in the development test set transla- tions. The counts are obtained from the parallel training  data.", "labels": [], "entities": []}]}