{"title": [{"text": "A Preliminary Look into the Use of Named Entity Information for Bioscience Text Tokenization", "labels": [], "entities": []}], "abstractContent": [{"text": "Tokenization in the bioscience domain is often difficult.", "labels": [], "entities": [{"text": "Tokenization", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9790528416633606}]}, {"text": "New terms, technical terminology, and nonstandard orthography, all common in bioscience text, contribute to this difficulty.", "labels": [], "entities": []}, {"text": "This paper will introduce the tasks of tokenization, normalization before introducing BAccHANT, a system built for bioscience text normalization.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9661210775375366}, {"text": "BAccHANT", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9609638452529907}, {"text": "bioscience text normalization", "start_pos": 115, "end_pos": 144, "type": "TASK", "confidence": 0.643418033917745}]}, {"text": "Casting tokenization / normalization as a problem of punctuation classification motivates using machine learning methods in the implementation of this system.", "labels": [], "entities": [{"text": "Casting tokenization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8409915268421173}, {"text": "punctuation classification", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.7266300916671753}]}, {"text": "The evaluation of BAccHANT's performance included error analysis of the system's performance inside and outside of named entities (NEs) from the GENIA corpus, which led to the creation of a normalization system trained solely on data from inside NEs, BAccHANT-N.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 145, "end_pos": 157, "type": "DATASET", "confidence": 0.9729907512664795}]}, {"text": "Evaluation of this new system indicated that normalization systems trained on data inside NEs perform better than systems trained both inside and outside NEs, motivating a merging of tokenization and named entity tagging processes as opposed to the standard pipelining approach.", "labels": [], "entities": [{"text": "tokenization and named entity tagging", "start_pos": 183, "end_pos": 220, "type": "TASK", "confidence": 0.7148240804672241}]}], "introductionContent": [{"text": "For the purposes of this paper, a token can be defined as the smallest discrete unit of meaning in a document relevant to the task at hand, the smallest entity of information that cannot be further reduced inform and still carry that information.", "labels": [], "entities": []}, {"text": "This definition of a token is dependent on both the type of information we wish to extract from a document, and the nature of the document itself; that is, tokenization is task-specific.", "labels": [], "entities": []}, {"text": "For example, tokenizing technical reports in order to search them by keyword may require a conservative tokenization scheme; e.g. a document containing the term \"2.0-gigahertz processor\" would not want to tokenize \"2.0\" away from \"gigahertz\" for fear that the document would be missed if the user searched for that exact phrase.", "labels": [], "entities": [{"text": "tokenizing technical reports", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.8855202595392863}]}, {"text": "However, if the same set of documents was being tokenized to build a database of processor speeds, \"2.0\" would need to be tokenized away from \"gigahertz\" in order to store its speed Tokenization is often straightforward; discovering the words in the sentence, \"I saw a cat.\" is not difficult, as the tokens are bounded by punctuation, including space characters.", "labels": [], "entities": [{"text": "Tokenization", "start_pos": 182, "end_pos": 194, "type": "TASK", "confidence": 0.9897111058235168}]}, {"text": "However, discovering the words in the sentence, \"I studied E. coli in a 2.5% solution.\" presents some problems.", "labels": [], "entities": []}, {"text": "The period following \"E.\" is being used to indicate an acronym instead of a sentence boundary, and must be recognized as such.", "labels": [], "entities": []}, {"text": "Even if we were not to concern ourselves with sentence boundaries, deciding that any period simply ends a token, the sentence would again present a problem since we would not want to tokenize the 2 from the 5 in \"2.5\".", "labels": [], "entities": []}, {"text": "The difficulty in tokenization stems from ambiguous punctuation.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.9701820015907288}]}, {"text": "In order to tokenize, one must be able to tell with certainty when apiece of punctuation ends a token.", "labels": [], "entities": [{"text": "tokenize", "start_pos": 12, "end_pos": 20, "type": "TASK", "confidence": 0.9763726592063904}, {"text": "certainty", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.918956995010376}]}, {"text": "The bioscience domain presents additional difficulties to tokenizing.", "labels": [], "entities": [{"text": "tokenizing", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.9695176482200623}]}, {"text": "Bioscience literature contains technical terminology and includes ambiguous punctuation, similar to the E. coli sentence above.", "labels": [], "entities": [{"text": "Bioscience literature", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.7557194828987122}]}, {"text": "The domain is dynamic, with thousands of researchers adding to the literature (the MEDLINE database adds approximately 400,000 new entries consisting of journal articles and abstracts per year).", "labels": [], "entities": [{"text": "MEDLINE database", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.9310320317745209}]}, {"text": "Bioscience literature contains heterogeneous orthographics; for example, the literature contains the terms \"NF-kappaB\", \"NF-kappa B\", and \"NF-kappa-B,\" and while each refers to the same protein, tokenizers using spaces and dashes as breaking criteria will return a different tokenization of each term though one standard tokenization would be preferable.", "labels": [], "entities": []}, {"text": "The problem of nonstandard orthography is of particular importance for document retrieval.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7245602905750275}]}, {"text": "Consider the NF-kappaB example above; if a document repository contains documents with different orthographic versions of NF-kappaB, a researcher searching for NF-kappaB would have to search for all possible orthographic variations and would miss documents containing unanticipated orthography.", "labels": [], "entities": []}, {"text": "Normalization attempts to solve this problem by removing orthographic variation from tokens, bringing them to one normalized form.", "labels": [], "entities": [{"text": "Normalization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9637983441352844}]}, {"text": "For example, if all three versions of NF-kappaB had all spaces and dashes removed, all three would look like \"NFkappaB,\" and a document retrieval system would find all instances in a search.", "labels": [], "entities": [{"text": "NFkappaB", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.9269921779632568}]}, {"text": "A related strategy, query expansion, attempts to solve the same problem by accounting for as many orthographic variants of the user query as possible, and searching for all of them.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.8416136801242828}]}, {"text": "Normalization acts as a special case of tokenization by deciding which instances of punctuation break a token, and removing all punctuation that does not break the token in order to bring it to a normalized form.", "labels": [], "entities": []}, {"text": "The remainder of this paper will consider work relevant to tokenization and normalization for the bioscience domain.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.982461154460907}, {"text": "normalization", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.9509897232055664}]}, {"text": "Casting tokenization, and by extension normalization, as a classification problem motivates the creation of BAccHANT, a machine learning system designed to normalize bioscience text.", "labels": [], "entities": [{"text": "Casting tokenization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8827050030231476}, {"text": "BAccHANT", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.8349221348762512}]}, {"text": "Evaluation of this system includes an evaluation of the system's performance inside and outside of named entities, and results from this evaluation motivate the creation of anew system, BAccHANT-N, trained solely on date from inside NEs.", "labels": [], "entities": [{"text": "BAccHANT-N", "start_pos": 186, "end_pos": 196, "type": "METRIC", "confidence": 0.9621887803077698}]}, {"text": "The improvement in performance of BAccHANT-N over BAccHANT when normalizing inside NE text indicates that named entity information is useful for bioscience text tokenization tasks, motivating future work in systems that perform tokenization and NE tagging concurrently.", "labels": [], "entities": [{"text": "BAccHANT-N", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.8900479078292847}, {"text": "bioscience text tokenization", "start_pos": 145, "end_pos": 173, "type": "TASK", "confidence": 0.530943900346756}, {"text": "NE tagging", "start_pos": 245, "end_pos": 255, "type": "TASK", "confidence": 0.8056742250919342}]}], "datasetContent": [{"text": "The baseline used for evaluation was to simply break on every instance of punctuation; that is, assume no punctuation needs to be removed.", "labels": [], "entities": []}, {"text": "This achieves an accuracy of 92.73%, where accuracy is the percentage of correctly classified punctuation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9996066689491272}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9995429515838623}]}, {"text": "This baseline was chosen for its high accuracy; however, as it is a simple majority class baseline which always predicts 'break', giving it a precision score of 1, a recall score of 0, and an f-measure of 0 for the 'remove' class.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9992850422859192}, {"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9992976188659668}, {"text": "recall score", "start_pos": 166, "end_pos": 178, "type": "METRIC", "confidence": 0.9841873645782471}]}, {"text": "BAccHANT was trained and tested using 10-fold cross-validation.", "labels": [], "entities": [{"text": "BAccHANT", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.6885758638381958}]}, {"text": "It achieved an accuracy of 96.60%, which was a statistically significant improvement over the baseline (all significance testing was done using a two-tailed t-test with a p-value of 0.05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996685981750488}]}, {"text": "More detailed results follow.: Precision, recall, and f-measure", "labels": [], "entities": [{"text": "Precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9963397979736328}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.998185932636261}]}], "tableCaptions": [{"text": " Table 1: The features and their possible values.", "labels": [], "entities": []}, {"text": " Table 2: Punctuation distribution of the MEDLINE  train/test set", "labels": [], "entities": [{"text": "MEDLINE  train/test set", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.8497496247291565}]}, {"text": " Table 3: Precision, recall, and f-measure", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9940477609634399}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9988077878952026}]}, {"text": " Table 4: Punctuation distribution in the GENIA  corpus test set", "labels": [], "entities": [{"text": "GENIA  corpus test set", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.9736977517604828}]}, {"text": " Table 5: Accuracy, precision, recall, and F-measure  for BAccHANT tested on all text vs. inside NEs.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993849992752075}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9995924830436707}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9996459484100342}, {"text": "F-measure", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9997808337211609}, {"text": "BAccHANT", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9514656066894531}]}, {"text": " Table 6: Precision, recall, and F-measure for  BAccHANT-N tested on named entity data.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9994895458221436}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9993921518325806}, {"text": "F-measure", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9995244741439819}, {"text": "BAccHANT-N", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9204874038696289}]}, {"text": " Table 7: Results summary across all systems.  Precision, recall, and f-measure are given for the  'remove' class.", "labels": [], "entities": [{"text": "Precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9993690848350525}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9969235062599182}, {"text": "f-measure", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9759513139724731}]}]}