{"title": [{"text": "Predicting Emotion in Spoken Dialogue from Multiple Knowledge Sources", "labels": [], "entities": [{"text": "Predicting Emotion", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9005031287670135}]}], "abstractContent": [{"text": "We examine the utility of multiple types of turn-level and contextual linguistic features for automatically predicting student emotions in human-human spoken tutoring dialogues.", "labels": [], "entities": [{"text": "predicting student emotions in human-human spoken tutoring dialogues", "start_pos": 108, "end_pos": 176, "type": "TASK", "confidence": 0.7449439764022827}]}, {"text": "We first annotate student turns in our corpus for negative, neutral and positive emotions.", "labels": [], "entities": []}, {"text": "We then automatically extract features representing acoustic-prosodic and other linguistic information from the speech signal and associated transcriptions.", "labels": [], "entities": []}, {"text": "We compare the results of machine learning experiments using different feature sets to predict the annotated emotions.", "labels": [], "entities": []}, {"text": "Our best performing feature set contains both acoustic-prosodic and other types of linguistic features, extracted from both the current turn and a context of previous student turns, and yields a prediction accuracy of 84.75%, which is a 44% relative improvement in error reduction over a baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 206, "end_pos": 214, "type": "METRIC", "confidence": 0.7506428360939026}, {"text": "error reduction", "start_pos": 265, "end_pos": 280, "type": "METRIC", "confidence": 0.9633040130138397}]}, {"text": "Our results suggest that the intelligent tutoring spoken dialogue system we are developing can be enhanced to automatically predict and adapt to student emotions.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper investigates the automatic classification of student emotional states using acoustic-prosodic, nonacoustic-prosodic, and contextual information, in a corpus of human-human spoken tutoring dialogues.", "labels": [], "entities": [{"text": "automatic classification of student emotional states", "start_pos": 28, "end_pos": 80, "type": "TASK", "confidence": 0.761315921942393}]}, {"text": "Motivation for this work comes from the discrepancy between the performance of human tutors and current machine tutors.", "labels": [], "entities": []}, {"text": "In recent years, the development of computational tutorial dialogue systems has become more prevalent, as one method of attempting to close the performance gap between human and computer tutors.", "labels": [], "entities": []}, {"text": "It has been hypothesized that the success of such computer dialogue tutors could be further increased by modeling and adapting to student emotion; for example) have shown that adding human-provided emotional scaffolding to an automated reading tutor increases student persistence.", "labels": [], "entities": []}, {"text": "We are building an intelligent tutoring spoken dialogue system with the goal of using spoken and natural language processing capabilities to automatically predict and adapt to student emotions.", "labels": [], "entities": []}, {"text": "Here we present results of an empirical study demonstrating the feasibility of modeling student emotion in a corresponding corpus of human-human spoken tutoring dialogues.", "labels": [], "entities": []}, {"text": "Research in emotional speech has already shown that acoustic and prosodic features can be extracted from the speech signal and used to develop predictive models of emotion.", "labels": [], "entities": []}, {"text": "Much of this research has used databases of speech read by actors or native speakers as training data (often with semantically neutral content).", "labels": [], "entities": []}, {"text": "However, such prototypical emotional speech does not necessarily reflect natural speech (, such as found in tutoring dialogues.", "labels": [], "entities": []}, {"text": "When actors are asked to read the same sentence with different emotions, they are restricted to conveying emotion using only acoustic and prosodic features.", "labels": [], "entities": []}, {"text": "In natural interactions, however, speakers can convey emotions using other types of features, and can also combine acoustic-prosodic and other feature types.", "labels": [], "entities": []}, {"text": "As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (;;), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time.", "labels": [], "entities": []}, {"text": "With noisier data and fewer features, it is not surprising that acoustic-prosodic features alone have been found to be of less predictive utility in these studies, leading spoken dialogue researchers to supplement such features with features based on other sources of information (e.g., lexical, syntactic, discourse).", "labels": [], "entities": []}, {"text": "Our methodology builds on and generalizes the results of this prior work in spoken dialogue emotion prediction, by introducing new linguistic and contextual features, and exploring emotion prediction in the domain of naturally occurring tutoring dialogues.", "labels": [], "entities": [{"text": "spoken dialogue emotion prediction", "start_pos": 76, "end_pos": 110, "type": "TASK", "confidence": 0.6827990338206291}, {"text": "emotion prediction", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.7542123794555664}]}, {"text": "We first annotate student turns in our human-human tutoring corpus for emotion.", "labels": [], "entities": []}, {"text": "We then automatically extract acousticprosodic and other types of linguistic features from the student utterances in our corpus, and from their local and global dialogue contexts.", "labels": [], "entities": []}, {"text": "We perform a variety of machine learning experiments using different feature combinations to predict our emotion categorizations.", "labels": [], "entities": []}, {"text": "Our experiments show that 1) by using either acoustic-prosodic or other types of features alone, prediction accuracy is significantly improved compared to a baseline classifier for emotion prediction, 2) the addition of features identifying specific subjects and tutoring sessions only sometimes improves performance, and 3) prediction accuracy can typically be improved by combining features across multiple knowledge sources, and/or by adding contextual information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9218431115150452}, {"text": "emotion prediction", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.7362658977508545}, {"text": "accuracy", "start_pos": 336, "end_pos": 344, "type": "METRIC", "confidence": 0.7343332171440125}]}, {"text": "Our best learned model achieves a prediction accuracy of 84.75%, which is a relative improvement of 44% over the baseline error.", "labels": [], "entities": [{"text": "prediction", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.6352077126502991}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.8190277218818665}]}, {"text": "Our results provide an empirical basis for enhancing the corresponding spoken dialogue tutoring system we are developing to automatically predict and ultimately to adapt to a student model that includes emotional states.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: %Correct on Speech vs. Text (cross-val.)", "labels": [], "entities": [{"text": "Correct", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9787799119949341}]}, {"text": " Table 2: %Correct on Speech+Text (cross-val.)", "labels": [], "entities": [{"text": "Correct", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9687401652336121}]}, {"text": " Table 3: Other Metrics on \"alltext+speech+ident\" (LOO)", "labels": [], "entities": []}, {"text": " Table 4: %Correct, Speech vs. Text, +context (cross-val.)", "labels": [], "entities": []}, {"text": " Table 5: %Correct on Text+Speech+Context (cross-val.)", "labels": [], "entities": []}, {"text": " Table 6: Feature Usage for \"alltext+speech+glob-ident\"", "labels": [], "entities": []}]}