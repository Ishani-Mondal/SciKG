{"title": [{"text": "Language model adaptation with MAP estimation and the perceptron algorithm", "labels": [], "entities": [{"text": "Language model adaptation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5915554662545522}, {"text": "MAP estimation", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7459710836410522}]}], "abstractContent": [{"text": "In this paper, we contrast two language model adaptation approaches: MAP estimation and the perceptron algorithm.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.864643007516861}]}, {"text": "Used in isolation, we show that MAP estimation outperforms the latter approach, for reasons which argue for combining the two approaches.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.9476762413978577}]}, {"text": "When combined, the resulting system provides a 0.7 percent absolute reduction in word error rate over MAP estimation alone.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 81, "end_pos": 96, "type": "METRIC", "confidence": 0.7545157074928284}, {"text": "MAP estimation", "start_pos": 102, "end_pos": 116, "type": "TASK", "confidence": 0.864039808511734}]}, {"text": "In addition, we demonstrate that, in a multi-pass recognition scenario, it is better to use the perceptron algorithm on early password lattices, since the improved error rate improves acoustic model adaptation.", "labels": [], "entities": [{"text": "multi-pass recognition", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.7162387669086456}]}], "introductionContent": [{"text": "Most common approaches to language model adaptation, such as count merging and model interpolation, are special cases of maximum a posteriori (MAP) estimation (.", "labels": [], "entities": [{"text": "language model adaptation", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.6431931455930074}, {"text": "count merging", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7554895281791687}]}, {"text": "In essence, these approaches involve beginning from a smoothed language model trained on out-of-domain observations, and adjusting the model parameters based on in-domain observations.", "labels": [], "entities": []}, {"text": "The approach ensures convergence, in the limit, to the maximum likelihood model of the in-domain observations.", "labels": [], "entities": []}, {"text": "The more in-domain observations, the less the out-of-domain model is relied upon.", "labels": [], "entities": []}, {"text": "In this approach, the main idea is to change the out-of-domain model parameters to match the in-domain distribution.", "labels": [], "entities": []}, {"text": "Another approach to language model adaptation would be to change model parameters to correct the errors made by the out-of-domain model on the in-domain data through discriminative training.", "labels": [], "entities": [{"text": "language model adaptation", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.69509224096934}]}, {"text": "In such an approach, the baseline recognizer would be used to recognize indomain utterances, and the parameters of the model adjusted to minimize recognition errors.", "labels": [], "entities": []}, {"text": "Discriminative training has been used for language modeling, using various estimation techniques), but language model adaptation to novel domains is a particularly attractive scenario for discriminative training, for reasons we discuss next.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8122124075889587}, {"text": "language model adaptation", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.627426415681839}]}, {"text": "A key requirement for discriminative modeling approaches is training data produced under conditions that are close to testing conditions.", "labels": [], "entities": []}, {"text": "For example, showed that excluding an utterance from the language model training corpus of the baseline model used to recognize that utterance is essential to getting word error rate (WER) improvements with the perceptron algorithm in the Switchboard domain.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 167, "end_pos": 188, "type": "METRIC", "confidence": 0.8484826385974884}]}, {"text": "In that paper, 28 different language models were built, each omitting one of 28 sections, for use in generating word lattices for the omitted section.", "labels": [], "entities": []}, {"text": "Without removing the section, no benefit was had from models built with the perceptron algorithm; with removal, the approach yielded a solid improvement.", "labels": [], "entities": []}, {"text": "More time consuming is controlling acoustic model training.", "labels": [], "entities": [{"text": "acoustic model training", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7671538591384888}]}, {"text": "For a task such as Switchboard, on which the above citation was evaluated, acoustic model estimation is expensive.", "labels": [], "entities": [{"text": "Switchboard", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.7146695256233215}, {"text": "acoustic model estimation", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.6427256365617117}]}, {"text": "Hence building multiple models, omitting various subsections is a substantial undertaking, especially when discriminative estimation techniques are used.", "labels": [], "entities": []}, {"text": "Language model adaptation to anew domain, however, can dramatically simplify the issue of controlling the baseline model for producing discriminative training data, since the in-domain training data is not used for building the baseline models.", "labels": [], "entities": [{"text": "Language model adaptation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5805524786313375}]}, {"text": "The purpose of this paper is to compare a particular discriminative approach, the perceptron algorithm, which has been successfully applied in the Switchboard domain, with MAP estimation, for adapting a language model to a novel domain.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 172, "end_pos": 186, "type": "TASK", "confidence": 0.6996261775493622}]}, {"text": "In addition, since the MAP and perceptron approaches optimize different objectives, we investigate the benefit from combination of these approaches within a multi-pass recognition system.", "labels": [], "entities": []}, {"text": "The task that we focus upon, adaptation of a general voicemail recognition language model to a customer service domain, has been shown to benefit greatly from MAP estimation (.", "labels": [], "entities": [{"text": "voicemail recognition language", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.7653148372968038}, {"text": "MAP estimation", "start_pos": 159, "end_pos": 173, "type": "TASK", "confidence": 0.5426125973463058}]}, {"text": "It is an attractive test for studying language model adaptation, since the out-of-domain acoustic model is matched to the new domain, and the domain shift does not raise the OOV rate significantly.", "labels": [], "entities": [{"text": "language model adaptation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6798193355401357}, {"text": "OOV rate", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9883649349212646}]}, {"text": "Using 17 hours of in-domain observations, versus 100 hours of out-of-domain utterances, () reported a reduction in WER from 28.0% using the baseline system to 20.3% with the best performing MAP adapted model.", "labels": [], "entities": [{"text": "WER", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9952670335769653}]}, {"text": "In this paper, our best scenario, which uses MAP adaptation and the perceptron algorithm in combination, achieves an additional 0.7% reduction, to 19.6% WER.", "labels": [], "entities": [{"text": "MAP adaptation", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.7877095639705658}, {"text": "WER", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.9972224235534668}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In the next section, we provide a brief background for both MAP estimation and the perceptron algorithm.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.8977261483669281}]}, {"text": "This is followed by an experimental results section, in which we present the performance of each approach in isolation, as well as several ways of combining them.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the language model adaptation algorithms by measuring the transcription accuracy of an adapted voicemail transcription system on voicemail messages received at a customer care line of a telecommunications network center.", "labels": [], "entities": [{"text": "language model adaptation", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.6813308397928873}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.8332803845405579}]}, {"text": "The initial voicemail system, named Scanmail, was trained on general voicemail messages collected from the mailboxes of people at our research site in Florham Park, NJ.", "labels": [], "entities": []}, {"text": "The target domain is also composed of voicemail messages, but fora mailbox that receives messages from customer care agents regarding network outages.", "labels": [], "entities": []}, {"text": "In contrast to the general voicemail messages from the training corpus of the Scanmail system, the messages from the target domain, named SS-NIFR, will be focused solely on network related problems.", "labels": [], "entities": []}, {"text": "It contains frequent mention of various network related acronyms and trouble ticket numbers, rarely (if at all) found in the training corpus of the Scanmail system.", "labels": [], "entities": []}, {"text": "To evaluate the transcription accuracy, we used a multipass speech recognition system that employs various unsupervised speaker and channel normalization techniques.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9694355130195618}, {"text": "multipass speech recognition", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.605695883433024}]}, {"text": "An initial search pass produces word-lattice output that is used as the grammar in subsequent search passes.", "labels": [], "entities": []}, {"text": "The system is almost identical to the one described in detail in).", "labels": [], "entities": []}, {"text": "The main differences in terms of the acoustic model of the system are the use of linear discriminant analysis features; use of a 100 hour training set as opposed to a 60 hour training set; and the modeling of the speaker gender which in this system is identical to that described in.", "labels": [], "entities": []}, {"text": "Note that the acoustic model is appropriate for either domain as the messages are collected on a voicemail system of the same type.", "labels": [], "entities": []}, {"text": "This parallels the experiments in (), where the focus was on AM adaptation in the case where the LM was deemed appropriate for either domain.", "labels": [], "entities": [{"text": "AM adaptation", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.9862185716629028}]}, {"text": "The language model of the Scanmail system is a Katz backoff trigram, trained on hand-transcribed messages of approximately 100 hours of voicemail (1 million words).", "labels": [], "entities": []}, {"text": "The model contains 13460 unigram, 175777 bigram, and 495629 trigram probabilities.", "labels": [], "entities": []}, {"text": "The lexicon of the Scanmail system contains 13460 words and was compiled from all the unique words found in the 100 hours of transcripts of the Scanmail training set.", "labels": [], "entities": [{"text": "Scanmail training set", "start_pos": 144, "end_pos": 165, "type": "DATASET", "confidence": 0.7710891366004944}]}, {"text": "For every experiment, we report the accuracy of the one-best transcripts obtained at 2 stages of the recognition process: after the first pass lattice construction (FP), and after vocal tract length normalization and gender modeling (VTLN), Constrained Model-space Adaptation (CMA), and Maximum Likelihood Linear regression adaptation (MLLR).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.998833954334259}, {"text": "first pass lattice construction (FP)", "start_pos": 132, "end_pos": 168, "type": "METRIC", "confidence": 0.6010180285998753}, {"text": "vocal tract length normalization", "start_pos": 180, "end_pos": 212, "type": "TASK", "confidence": 0.591152548789978}, {"text": "Constrained Model-space Adaptation (CMA)", "start_pos": 241, "end_pos": 281, "type": "TASK", "confidence": 0.7684347728888193}, {"text": "Maximum Likelihood Linear regression adaptation (MLLR)", "start_pos": 287, "end_pos": 341, "type": "METRIC", "confidence": 0.724612656980753}]}, {"text": "Results after FP will be denoted FP; results after VTLN, CMA and MLLR will be denoted MP.", "labels": [], "entities": [{"text": "FP", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.7326775193214417}, {"text": "FP", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9623227715492249}, {"text": "VTLN", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.9388208985328674}]}, {"text": "For the SSNIFR domain we have available a 1 hour manually transcribed test set (10819 words) and approximately 17 hours of manually transcribed adaptation data (163343 words).", "labels": [], "entities": [{"text": "SSNIFR domain", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.8067558705806732}]}, {"text": "In all experiments, the vocabulary of the system is left unchanged.", "labels": [], "entities": []}, {"text": "Generally, fora domain shift this can raise the error rate significantly due to an increase in the OOV rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9806555211544037}, {"text": "OOV rate", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9823063015937805}]}, {"text": "However, this increase in error rate is limited in these experiments, because the majority of the new domain-dependent vocabulary are acronyms: Recognition on the 1 hour SSNIFR test set using systems obtained by supervised LM adaptation on the 17 hour adaptation set using the two methods, versus the baseline out-of-domain system.", "labels": [], "entities": [{"text": "error rate", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9559547603130341}, {"text": "SSNIFR test set", "start_pos": 170, "end_pos": 185, "type": "DATASET", "confidence": 0.8218318621317545}]}, {"text": "which are covered by the Scanmail vocabulary through individual letters.", "labels": [], "entities": [{"text": "Scanmail vocabulary", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.7244847565889359}]}, {"text": "The OOV rate of the SSNIFR test set, using the Scanmail vocabulary is 2%.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9847593009471893}, {"text": "SSNIFR test set", "start_pos": 20, "end_pos": 35, "type": "DATASET", "confidence": 0.8134296238422394}, {"text": "Scanmail vocabulary", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.772778183221817}]}, {"text": "Following (, \u03c4 h in Eq.", "labels": [], "entities": []}, {"text": "2 is set to 0.2 for all reported MAP estimation trials.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.9176875352859497}]}, {"text": "Following (), \u03bb in Eq.", "labels": [], "entities": []}, {"text": "4 is also (coincidentally) set to 0.2 for all reported perceptron trials.", "labels": [], "entities": []}, {"text": "For the perceptron algorithm, approximately 10 percent of the training data is reserved as a held-out set, for deciding when to stop the algorithm.", "labels": [], "entities": []}, {"text": "shows the results using MAP estimation and the perceptron algorithm independently.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.6327514797449112}]}, {"text": "For the perceptron algorithm, the baseline Scanmail system was used to produce the word lattices used in estimating the feature weights.", "labels": [], "entities": []}, {"text": "There are two ways to do this.", "labels": [], "entities": []}, {"text": "One is to use the lattices produced after FP; the other is to use the lattices produced after MP.", "labels": [], "entities": [{"text": "FP", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.5250633358955383}]}, {"text": "These results show two things.", "labels": [], "entities": []}, {"text": "First, MAP estimation on its own is clearly better than the perceptron algorithm on its own.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.9245132207870483}]}, {"text": "Since the MAP model is used in the initial search pass that produces the lattices, it can consider all possible hypotheses.", "labels": [], "entities": []}, {"text": "In contrast, the perceptron algorithm is limited to the hypotheses available in the lattice produced with the unadapted model.", "labels": [], "entities": []}, {"text": "Second, training the perceptron model on FP lattices and applying that perceptron at each decoding step outperformed training on MP lattices and only applying the perceptron on that decoding step.", "labels": [], "entities": []}, {"text": "This demonstrates the benefit of better transcripts for the unsupervised adaptation steps.", "labels": [], "entities": []}, {"text": "The benefit of MAP adaptation that leads to its superior performance in suggests a hybrid approach, that uses MAP estimation to ensure that good hypotheses are present in the lattices, and the perceptron algorithm to further reduce the WER.", "labels": [], "entities": [{"text": "MAP adaptation", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.943425714969635}, {"text": "WER", "start_pos": 236, "end_pos": 239, "type": "METRIC", "confidence": 0.9874572157859802}]}, {"text": "Within the multi-pass recognition approach, several scenarios could be considered to implement this combination.", "labels": [], "entities": [{"text": "multi-pass recognition", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.8016779124736786}]}, {"text": "For each scenario, we split the 17 hour adaptation set into four roughly equi-sized sets.", "labels": [], "entities": []}, {"text": "Ina first scenario, we produced a MAP estimated model on the first 4.25 hour subset, and produced word lattices on the other three subsets, for use with the perceptron algorithm.: Recognition on the 1 hour SSNIFR test set using systems obtained by supervised LM adaptation on the 17 hour adaptation set using the first method of combination of the two methods, versus the baseline out-of-domain system.", "labels": [], "entities": [{"text": "SSNIFR test set", "start_pos": 206, "end_pos": 221, "type": "DATASET", "confidence": 0.8121573726336161}]}, {"text": "the results for this training scenario.", "labels": [], "entities": []}, {"text": "A second scenario involves making use of all of the adaptation data for both MAP estimation and the perceptron algorithm.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.8375953435897827}]}, {"text": "As a result, it requires a more complicated control of the baseline models used for producing the word lattices for perceptron training.", "labels": [], "entities": []}, {"text": "For each of the four sub-sections of the adaptation data, we produced a baseline MAP estimated model using the other three subsections.", "labels": [], "entities": []}, {"text": "Using these models, we produced training lattices for the perceptron algorithm for the entire adaptation data set.", "labels": [], "entities": []}, {"text": "At test time, we used the MAP estimated model trained on the entire adaptation set, as well as the perceptron model trained on the entire set.", "labels": [], "entities": [{"text": "MAP", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.8713284134864807}]}, {"text": "The results for this training scenario are shown in table 3.", "labels": [], "entities": []}, {"text": "Both of these hybrid training scenarios demonstrate a small improvement by using the perceptron algorithm on FP lattices rather than MP lattices.", "labels": [], "entities": []}, {"text": "Closely matching the testing condition for perceptron training is important: applying a perceptron trained on MP lattices to FP lattices hurts performance.", "labels": [], "entities": []}, {"text": "Iterative training did not produce further improvements: training a perceptron on MP lattices produced by using both MAP estimation and a perceptron trained on FP lattices, achieved no improvement over the 19.6 percent WER shown above.", "labels": [], "entities": [{"text": "WER", "start_pos": 219, "end_pos": 222, "type": "METRIC", "confidence": 0.9955567717552185}]}], "tableCaptions": [{"text": " Table 1: Recognition on the 1 hour SSNIFR test set us- ing systems obtained by supervised LM adaptation on the  17 hour adaptation set using the two methods, versus the  baseline out-of-domain system.", "labels": [], "entities": [{"text": "Recognition", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9539569616317749}, {"text": "SSNIFR test set", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.8267518480618795}, {"text": "LM adaptation", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.8258579671382904}]}, {"text": " Table 2: Recognition on the 1 hour SSNIFR test set using  systems obtained by supervised LM adaptation on the 17  hour adaptation set using the first method of combination  of the two methods, versus the baseline out-of-domain  system.", "labels": [], "entities": [{"text": "SSNIFR test set", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.8326133489608765}]}, {"text": " Table 3: Recognition on the 1 hour SSNIFR test set us- ing systems obtained by supervised LM adaptation on the  17 hour adaptation set using the second method of com- bination of the two methods, versus the baseline out-of- domain system.", "labels": [], "entities": [{"text": "Recognition", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9229437708854675}, {"text": "SSNIFR test set", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.8184919655323029}]}]}