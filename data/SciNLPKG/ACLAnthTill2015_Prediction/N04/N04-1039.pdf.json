{"title": [{"text": "Exponential Priors for Maximum Entropy Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Maximum entropy models area common mod-eling technique, but prone to overfitting.", "labels": [], "entities": []}, {"text": "We show that using an exponential distribution as a prior leads to bounded absolute discounting by a constant.", "labels": [], "entities": []}, {"text": "We show that this prior is better motivated by the data than previous techniques such as a Gaussian prior, and often produces lower error rates.", "labels": [], "entities": [{"text": "error rates", "start_pos": 132, "end_pos": 143, "type": "METRIC", "confidence": 0.9585447907447815}]}, {"text": "Exponential priors also lead to a simpler learning algorithm and to easier to understand behavior.", "labels": [], "entities": []}, {"text": "Furthermore, exponential priors help explain the success of some previous smoothing techniques, and suggest simple variations that work better.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conditional Maximum Entropy (maxent) models have been widely used fora variety of tasks, including language modeling, part-of-speech tagging, prepositional phrase attachment, and parsing), word selection for machine translation, and finding sentence boundaries.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7600933909416199}, {"text": "part-of-speech tagging", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.7024114727973938}, {"text": "prepositional phrase attachment", "start_pos": 142, "end_pos": 173, "type": "TASK", "confidence": 0.6200239757696787}, {"text": "word selection", "start_pos": 189, "end_pos": 203, "type": "TASK", "confidence": 0.7714925408363342}, {"text": "machine translation", "start_pos": 208, "end_pos": 227, "type": "TASK", "confidence": 0.7534301280975342}]}, {"text": "They are also sometimes called logistic regression models, maximum likelihood exponential models, log-linear models, and are even equivalent to a form of perceptrons/single layer neural networks.", "labels": [], "entities": []}, {"text": "In particular, perceptrons that use the standard sigmoid function, and optimize for log-loss are equivalent to maxent.", "labels": [], "entities": []}, {"text": "Multi-layer neural networks that optimize logloss are closely related, and much of the discussion will apply to them implicitly.", "labels": [], "entities": []}, {"text": "Conditional maxent models have traditionally either been unregularized or regularized by using a Gaussian prior on the parameters.", "labels": [], "entities": []}, {"text": "We will show that by using an exponential distribution as the prior, several advantages can be gained.", "labels": [], "entities": []}, {"text": "We will show that in at least one case, an exponential prior experimentally better matches the actual distribution of the parameters.", "labels": [], "entities": []}, {"text": "We will also show that it can lead to improved accuracy, and a simpler learning algorithm.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9994981288909912}]}, {"text": "In addition, the exponential prior inspires an improved version of Good Turing discounting with lower perplexity.", "labels": [], "entities": [{"text": "Good Turing discounting", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.5298507809638977}]}, {"text": "Conditional maxent models are of the form where x is an input vector, y is an output, the f i are the socalled indicator functions or feature values that are true if a particular property of x, y is true, F is the number of such features, \u039b represents the parameter set \u03bb 1 ...\u03bb n , and \u03bb i is a weight for the indicator f i . For instance, if trying to do word sense disambiguation for the word \"bank\", x would be the context around an occurrence of the word; y would be a particular sense, e.g. financial or river; f i (x, y) could be 1 if the context includes the word \"money\" and y is the financial sense; and \u03bb i would be a large positive number.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 357, "end_pos": 382, "type": "TASK", "confidence": 0.6260084311167399}]}, {"text": "Maxent models have several valuable properties (Della give a good overview.)", "labels": [], "entities": []}, {"text": "The most important is constraint satisfaction.", "labels": [], "entities": [{"text": "constraint satisfaction", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.6743292510509491}]}, {"text": "For a given f i , we can count how many times f i was observed in the training data with value y, observed[i] = \ud97b\udf59 j f i (x j , y j ).", "labels": [], "entities": []}, {"text": "For a model P \u039b with parameters \u039b, we can see how many times the model predicts that f i would be expected to occur: for all i and y.", "labels": [], "entities": []}, {"text": "These equalities are called constraints.", "labels": [], "entities": []}, {"text": "The next important property is that the likelihood of the training data is maximized (thus, the name maximum likelihood exponential model.)", "labels": [], "entities": []}, {"text": "Third, the model is as similar as possible to the uniform distribution (minimizes the Kullback-Leibler divergence), given the constraints, which is why these models are called maximum entropy models.", "labels": [], "entities": []}, {"text": "This last property -similarity to the uniform distribution -is a form of regularization.", "labels": [], "entities": []}, {"text": "However, it turns out to bean extremely weak one -it is not uncommon for models, especially those that use all or most possible features, to assign near-zero probabilities (or, if \u03bbs maybe infinite, even actual zero probabilities), and to exhibit other symptoms of severe overfitting.", "labels": [], "entities": []}, {"text": "There have been a number of approaches to this problem, which we will discuss in more detail in Section 3.", "labels": [], "entities": []}, {"text": "The most relevant approach, however, is the work of, who implemented a Gaussian prior for maxent models.", "labels": [], "entities": []}, {"text": "They compared this technique to most of the previously implemented techniques, on a language modeling task, and concluded that it was consistently the best.", "labels": [], "entities": [{"text": "language modeling task", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.7709101935227712}]}, {"text": "We thus use it as a baseline for our comparisons, and similar considerations motivate our own technique, an exponential prior.", "labels": [], "entities": []}, {"text": "Chen et al. place a Gaussian prior with 0 mean and \u03c3 2 i variance on the model parameters (the \u03bb i s), and then find a model that maximizes the posterior probability of the data and the model.", "labels": [], "entities": []}, {"text": "In particular, maxent models without priors use the parameters \u039b that maximize arg max where x j , y j are training data instances.", "labels": [], "entities": []}, {"text": "With a Gaussian prior we find arg max That is, instead of a model that matches the observed count, we get a model matching the observed count minus \u03bbi \u03c3 2 i : in language modeling terms, this is \"discounting.\"", "labels": [], "entities": []}, {"text": "We do not believe that all models are generated by the same process, and therefore we do not believe that a single prior will work best for all problem types.", "labels": [], "entities": []}, {"text": "In particular, as we will describe in our experimental results section, when looking atone particular set of parameters, we noticed that it was not at all Gaussian, but much more similar to a 0 mean Laplacian, , which is non-zero only for non-negative \u03bb i . In some cases, learned parameter distributions will not match the prior distribution, but in some cases they will, so it seemed worth exploring one of these alternate forms.", "labels": [], "entities": []}, {"text": "Later, when we try to optimize our models, the parameter seach will turnout to be much simpler with an exponential prior, so we focus on that distribution.", "labels": [], "entities": []}, {"text": "With an exponential prior, we maximize arg max As we will describe, it is significantly simpler to perform this maximization than the Gaussian maximization.", "labels": [], "entities": []}, {"text": "Furthermore, as we will describe, models satisfying Equation 2 will have the property that, for each \u03bb i , either a) In other words, we essentially just discount the observed counts by the constant \u03b1 i (which is the reciprocal of the standard deviation), subject to the constraint that \u03bb i is non-negative.", "labels": [], "entities": []}, {"text": "This is much simpler and more intuitive than the constraints with the Gaussian prior (Equation 1), since those constraints change as the values of \u03bb i change.", "labels": [], "entities": []}, {"text": "Furthermore, as we will describe in Section 3, discounting by a constant is a common technique for language model smoothing), but one that has not previously been well justified; the exponential prior gives some Bayesian justification.", "labels": [], "entities": [{"text": "language model smoothing", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.6750047206878662}]}, {"text": "In Section 5 we will show that on two very different tasks -grammar checking and a collaborative filtering task -the exponential prior yields lower error rates than the Gaussian.", "labels": [], "entities": [{"text": "grammar checking", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.8797273337841034}, {"text": "error rates", "start_pos": 148, "end_pos": 159, "type": "METRIC", "confidence": 0.9446716606616974}]}], "datasetContent": [{"text": "In this section, we detail our experimental results, showing that exponential priors outperform Gaussian priors on two different data sets, and inspire improvements fora third.", "labels": [], "entities": []}, {"text": "For all experiments, except the language model experiments, we used a single variance for both the Gaussian and the exponential prior, rather than one per parameter, with the variance optimized on held out data.", "labels": [], "entities": []}, {"text": "For the language modeling experiments, we used three variances, one each for the unigram, bigram, and trigram models, again optimized on held out data.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7012457698583603}]}, {"text": "We were inspired to use an exponential prior by an actual examination of a data set.", "labels": [], "entities": []}, {"text": "In particular, we used the grammar-checking data of.", "labels": [], "entities": []}, {"text": "We chose this set because there are commonly used versions both with small amounts of data (which is when we expect the prior to matter) and with large amounts of data (which is required to easily see what the distribution over \"correct\" parameter values is.)", "labels": [], "entities": []}, {"text": "For one experiment, we trained a model using a Gaussian prior, using a large amount of data.", "labels": [], "entities": []}, {"text": "We then found those parameters (\u03bb's) that had at least 35 training instances -enough to typically overcome the prior and train the parameter reliably.", "labels": [], "entities": []}, {"text": "We then graphed the distribution of these parameters.", "labels": [], "entities": []}, {"text": "While it is common to look at the distribution of data, the NLP and machine learning communities rarely examine distributions of model parameters, and yet this seems like a good way to get inspiration for priors to try, using those parameters with enough data to help guess the priors for those with less, or at least to determine the correct form for the prior, if not the exact values.", "labels": [], "entities": []}, {"text": "The results are shown in, which is a histogram of \u03bb's with a given value.", "labels": [], "entities": []}, {"text": "If the distribution were Gaussian, we would expect this to look like an upside-down parabola.", "labels": [], "entities": []}, {"text": "If the distribution were Laplacian, we would expect it to appear as a triangle (the bottom formed from the X-axis.)", "labels": [], "entities": []}, {"text": "Indeed, it does appear to be roughly triangular, and to the extent that it diverges from this shape, it is convex, while a Gaussian would be concave.", "labels": [], "entities": []}, {"text": "We don't believe that the exponential prior is right for every problem -our argument here is that based on both better accuracy (our next experiment) and a better fit to at least some of the parameters, that the exponential prior is better for some models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9991554021835327}]}, {"text": "We then tried actually using exponential priors with this application, and were able to demonstrate improve- We used a small data set, 100,000 sentences of training data and ten different confusable word pairs.", "labels": [], "entities": []}, {"text": "(Most training sentences did not contain examples of the confusable word pairs of interest, so the actual number of training examples for each word-pair was less than 100,000).", "labels": [], "entities": []}, {"text": "We tried different priors for the Gaussian and exponential prior, and found the best single prior on average across all ten pairs.", "labels": [], "entities": []}, {"text": "With this best setting, we achieved a 14.51% geometric average error rate with the exponential prior, and 15.45% with the Gaussian.", "labels": [], "entities": [{"text": "geometric average error rate", "start_pos": 45, "end_pos": 73, "type": "METRIC", "confidence": 0.7966075092554092}]}, {"text": "To avoid any form of cheating, we then tried 10 different word pairs (the same as those used by) with this best parameter setting.", "labels": [], "entities": [{"text": "cheating", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9220185875892639}]}, {"text": "The results were 18.07% and 19.47% for the exponential and Gaussian priors respectively.", "labels": [], "entities": []}, {"text": "(The overall higher rate is due to the test set words being slightly more difficult.)", "labels": [], "entities": [{"text": "rate", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9692115187644958}]}, {"text": "We also tried experiments with 1 million and 10 million words, but there were not consistent differences; improved smoothing mostly matters with small amounts of training data.", "labels": [], "entities": [{"text": "smoothing", "start_pos": 115, "end_pos": 124, "type": "TASK", "confidence": 0.964038074016571}]}, {"text": "We also tried experiments with a collaborative-filtering style task, television show recommendation, based on Nielsen data.", "labels": [], "entities": [{"text": "Nielsen data", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.8751245141029358}]}, {"text": "The dataset used, and the definition of a collaborative filtering (CF) score is the same as was used by, although our random train/test split is not the same, so the results are not strictly comparable.", "labels": [], "entities": []}, {"text": "We first ran experiments with different priors on a heldout section of the training data, and then using the single best value for the prior (the same one across all features), we ran on the test data.", "labels": [], "entities": []}, {"text": "With a Gaussian prior, the CF score was 42.11, while with an exponential prior, it was 45.86, a large improvement.", "labels": [], "entities": [{"text": "CF score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9912315905094147}]}, {"text": "Finally, we ran experiments with language modeling, with mixed success.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8218764364719391}]}, {"text": "We used 1,000,000 words of training data (a small model, but one where smoothing matters) from the WSJ corpus and a trigram model with a cluster-based speedup.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 99, "end_pos": 109, "type": "DATASET", "confidence": 0.9789213538169861}]}, {"text": "We evaluated on test data using the standard language modeling measure, perplexity, where lower scores are better.", "labels": [], "entities": []}, {"text": "We tried five experiments: using Katz smoothing (a widely used version of Good-Turing smoothing) (perplexity 238.0); using Good Turing discounting to smooth maxent (perplexity 224.8); using our variation on Good-Turing, inspired by exponential priors, where \u03bb's are bounded at 0 (perplexity 204.5); using an exponential prior; using a Gaussian prior (perplexity 183.7); and using interpolated modified Kneser-Ney smoothing (perplexity 180.2).", "labels": [], "entities": []}, {"text": "On the one hand, an exponential prior is worse than a Gaussian prior in this case, and modified interpolated Kneser-Ney smoothing is still the best known smoothing technique, within noise of a Gaussian prior.", "labels": [], "entities": []}, {"text": "On the other hand, searching for parameters is extremely time consuming, and GoodTuring is one of the few parameter-free smoothing methods.", "labels": [], "entities": [{"text": "GoodTuring", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.8964830636978149}]}, {"text": "Of the three Good-Turing smoothing methods, the one inspired by exponentials priors was the best.", "labels": [], "entities": []}, {"text": "Note that perplexity is 2 entropy and in general, we have found that exponential priors work slightly worse on entropy measures than the Gaussian prior, even when they are better on accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9973003268241882}]}, {"text": "This maybe due to the fact that an exponential prior \"throws away\" some information, whenever the \u03bb would be negative.", "labels": [], "entities": []}, {"text": "(In a pilot experiment with a variation that does not throwaway information, the entropies are closer to the Gaussian.)", "labels": [], "entities": []}], "tableCaptions": []}