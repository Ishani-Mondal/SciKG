{"title": [{"text": "Answering Definition Questions Using Multiple Knowledge Sources", "labels": [], "entities": [{"text": "Answering Definition Questions", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8943553566932678}]}], "abstractContent": [{"text": "Definition questions represent a largely unex-plored area of question answering-they are different from factoid questions in that the goal is to return as many relevant \"nuggets\" of information about a concept as possible.", "labels": [], "entities": [{"text": "question answering-they", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.7637121975421906}]}, {"text": "We describe a multi-strategy approach to answering such questions using a database constructed offline with surface patterns, a Web-based dictionary, and an off-the-shelf document retriever.", "labels": [], "entities": []}, {"text": "Results are presented from component-level evaluation and from an end-to-end evaluation of our implemented system at the TREC 2003 Question Answering Track.", "labels": [], "entities": [{"text": "TREC 2003 Question Answering Track", "start_pos": 121, "end_pos": 155, "type": "TASK", "confidence": 0.7787120580673218}]}], "introductionContent": [{"text": "To date, research in question answering has concentrated on factoid questions such as \"Who was Abraham Lincoln married to?\"", "labels": [], "entities": [{"text": "question answering", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8574738502502441}, {"text": "\"Who was Abraham Lincoln married to?\"", "start_pos": 86, "end_pos": 123, "type": "TASK", "confidence": 0.7388773593637679}]}, {"text": "The standard strategy for answering these questions using a textual corpus involves a combination of information retrieval and named-entity extraction technology; see) for an overview.", "labels": [], "entities": [{"text": "named-entity extraction", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.7071731686592102}]}, {"text": "Factoid questions, however, represent but one facet of question answering, whose broader goal is to provide humans with intuitive information access using natural language.", "labels": [], "entities": [{"text": "question answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7777936160564423}]}, {"text": "In contrast to factoid questions, the objective for \"definition\" questions is to produce as many useful \"nuggets\" of information as possible.", "labels": [], "entities": []}, {"text": "For example, the answer to \"Who is Aaron Copland?\" might include the following: American composer wrote ballets and symphonies born in Brooklyn, New York, in 1900 son of a Jewish immigrant American communist civil rights advocate Until recently, definition questions remained a largely unexplored area of question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 305, "end_pos": 323, "type": "TASK", "confidence": 0.763864666223526}]}, {"text": "Standard factoid question answering technology, designed to extract single answers, cannot be directly applied to this task.", "labels": [], "entities": [{"text": "factoid question answering", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.6743989884853363}]}, {"text": "The solution to this interesting research challenge will draw from related fields such as information extraction, multidocument summarization, and answer fusion.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.8640601933002472}, {"text": "multidocument summarization", "start_pos": 114, "end_pos": 141, "type": "TASK", "confidence": 0.7596272230148315}, {"text": "answer fusion", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.9341623783111572}]}, {"text": "In this paper, we present an approach to answering definition questions that combines knowledge from three sources.", "labels": [], "entities": [{"text": "answering definition questions", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.9259976347287496}]}, {"text": "We present results from our own component analysis and the TREC 2003 Question Answering Track.", "labels": [], "entities": [{"text": "TREC 2003 Question Answering Track", "start_pos": 59, "end_pos": 93, "type": "DATASET", "confidence": 0.7983096361160278}]}], "datasetContent": [{"text": "In this section we present two separate evaluations of our system.", "labels": [], "entities": []}, {"text": "The first is a component analysis of our database and dictionary techniques, and the second involves our participation in the TREC 2003 Question Answering Track.", "labels": [], "entities": [{"text": "TREC 2003 Question Answering Track", "start_pos": 126, "end_pos": 160, "type": "TASK", "confidence": 0.7912662148475647}]}, {"text": "We evaluated the performance of each individual surface pattern and the dictionary lookup technique on 160 definition questions selected from the TREC-9 and TREC-10 QA Track testsets.", "labels": [], "entities": [{"text": "TREC-9", "start_pos": 146, "end_pos": 152, "type": "DATASET", "confidence": 0.8935106992721558}, {"text": "TREC-10 QA Track testsets", "start_pos": 157, "end_pos": 182, "type": "DATASET", "confidence": 0.8524999469518661}]}, {"text": "Since we primarily generated our patterns by directly analyzing the corpus, these questions can be considered a blind testset.", "labels": [], "entities": []}, {"text": "The performance of our surface patterns and our dictionary lookup technique is shown in.", "labels": [], "entities": []}, {"text": "Overall, database lookup retrieved approximately eight nuggets per question at an accuracy nearing 40%; dictionary lookup retrieved about 1.5 nuggets per question at an accuracy of 45%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9984579086303711}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.998765230178833}]}, {"text": "Obviously, recall of our techniques is extremely hard to measure directly; instead, we use the prevalence of each pattern as a poor substitute.", "labels": [], "entities": [{"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9989166259765625}]}, {"text": "As shown in, some patterns occur frequently (e.g., e1 is and e1 appo), but others are relatively rare, such as the relative clause pattern, which yielded only six nuggets for the entire testset.", "labels": [], "entities": []}, {"text": "These results represent a baseline for the performance of each technique.", "labels": [], "entities": []}, {"text": "Our focus was not on perfecting each individual pattern and the dictionary matching algorithm, but on building a complete working system.", "labels": [], "entities": [{"text": "dictionary matching", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.6641482710838318}]}, {"text": "We will discuss future improvements and refinements in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.8548609018325806}]}, {"text": "This section takes a closer look at the setup of the definition question evaluation at TREC 2003.", "labels": [], "entities": [{"text": "TREC 2003", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.8003510534763336}]}, {"text": "In particular, we examine three issues: the scoring metric, error inherent in the evaluation process, and variations in judgments.", "labels": [], "entities": [{"text": "error", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.956900417804718}]}, {"text": "In the TREC 2003 evaluation, we submitted three identical runs, but nevertheless received different scores for each of the runs.", "labels": [], "entities": [{"text": "TREC 2003 evaluation", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.8458471496899923}]}, {"text": "This situation can be viewed as a probe into the error margin of the evaluation-assessors are human and naturally make mistakes, and to ensure the quality of the evaluation we need to quantify this variation.", "labels": [], "entities": [{"text": "error margin", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.9011605083942413}]}, {"text": "revealed that scores for pairs of identical runs differed by as much as 0.043 in F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9945425987243652}]}, {"text": "For the three identical runs we submitted, there was one nugget missed in our first run that was found in the other two runs, ten nuggets from six questions missed for our second run that were found in the other runs, and ten nuggets from five questions missed in our third run.", "labels": [], "entities": []}, {"text": "There were also nine nuggets from seven questions that were missed for all three runs, even though they were clearly present in our answers.", "labels": [], "entities": []}, {"text": "Together over our three runs, there were 48 nuggets from 13 questions that were clearly present in our responses but were not consistently recognized by the assessors.", "labels": [], "entities": []}, {"text": "The question affected most by these discrepancies was \"Who is Alger Hiss?\", for which we received an F-measure of 0.671 in our first run, while for the second and third runs we received a score of zero.", "labels": [], "entities": [{"text": "Who is Alger Hiss?\"", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.5771952450275422}, {"text": "F-measure", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.999067485332489}]}, {"text": "If the 48 missed nuggets had been recognized by the assessors, our F-measure would be 0.327, 0.045 higher than the score we actually received for runs band c.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.998981773853302}]}, {"text": "This single-point investigation is not meant to contest the relative rankings of submitted runs, but simply to demonstrate the magnitude of the human error currently present in the evaluation of definition questions (presumably, all groups suffered equally from these variations).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Performance of each surface pattern and the dic- tionary lookup technique for all 160 test questions.", "labels": [], "entities": []}, {"text": " Table 4: Official TREC 2003 results.", "labels": [], "entities": [{"text": "Official TREC 2003 results", "start_pos": 10, "end_pos": 36, "type": "DATASET", "confidence": 0.8082875460386276}]}]}