{"title": [{"text": "A Lightweight Semantic Chunking Model Based On Tagging", "labels": [], "entities": [{"text": "Tagging", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.8961429595947266}]}], "abstractContent": [{"text": "In this paper, a framework for the development of a fast, accurate, and highly portable semantic chunker is introduced.", "labels": [], "entities": []}, {"text": "The framework is based on a non-overlapping, shallow tree-structured language.", "labels": [], "entities": []}, {"text": "The derivation of the tree is considered as a sequence of tagging actions in a predefined linguistic context, and a novel semantic chunker is accordingly developed.", "labels": [], "entities": []}, {"text": "It groups the phrase chunks into the arguments of a given predicate in a bottom-up fashion.", "labels": [], "entities": []}, {"text": "This is quite different from current approaches to semantic parsing or chunking that depend on full statistical syntactic parsers that require tree bank style annotation.", "labels": [], "entities": [{"text": "semantic parsing or chunking", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.6443679332733154}]}, {"text": "We compare it with a recently proposed word-byword semantic chunker and present results that show that the phrase-by-phrase approach performs better than its word-byword counterpart .", "labels": [], "entities": [{"text": "word-byword semantic chunker", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.609469085931778}]}], "introductionContent": [{"text": "Semantic representation, and, obviously, its extraction from an input text, are very important for several natural language processing tasks; namely, information extraction, question answering, summarization, machine translation and dialog management.", "labels": [], "entities": [{"text": "Semantic representation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.813106119632721}, {"text": "information extraction", "start_pos": 150, "end_pos": 172, "type": "TASK", "confidence": 0.851621001958847}, {"text": "question answering", "start_pos": 174, "end_pos": 192, "type": "TASK", "confidence": 0.8706105053424835}, {"text": "summarization", "start_pos": 194, "end_pos": 207, "type": "TASK", "confidence": 0.9854029417037964}, {"text": "machine translation", "start_pos": 209, "end_pos": 228, "type": "TASK", "confidence": 0.8102797269821167}, {"text": "dialog management", "start_pos": 233, "end_pos": 250, "type": "TASK", "confidence": 0.8279087841510773}]}, {"text": "For example, in question answering systems, semantic representations can be used to understand the user's question, expand the query, find relevant documents and present a summary of multiple documents as the answer.", "labels": [], "entities": [{"text": "question answering", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8045780956745148}]}, {"text": "Semantic representations are often defined as a collection of frames with a number of slots for each frame to represent the task structure and domain objects.", "labels": [], "entities": []}, {"text": "This frame-based semantic representation has been successfully used in many limited-domain tasks.", "labels": [], "entities": []}, {"text": "For This work is supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grant fully used in many limited-domain tasks.", "labels": [], "entities": [{"text": "ARDA AQUAINT", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.6518782675266266}, {"text": "OCG4423B", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.6727772951126099}, {"text": "NSF", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.9176214337348938}]}, {"text": "For example, in a spoken dialog system designed for travel planning one might have an Air frame with slots Origin, Destination, Depart_date, Airline etc.", "labels": [], "entities": [{"text": "travel planning", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.6938508152961731}, {"text": "Origin", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9279769062995911}]}, {"text": "The drawback of this domain specific representation is the high cost to achieve adequate coverage in anew domain.", "labels": [], "entities": []}, {"text": "A new set of frames and slots are needed when the task is extended or changed.", "labels": [], "entities": []}, {"text": "Authoring the patterns that instantiate those frames is time consuming and expensive.", "labels": [], "entities": []}, {"text": "Domain independent semantic representations can overcome the poor portability of domain specific representations.", "labels": [], "entities": []}, {"text": "A natural candidate for this representation is the predicate-argument structure of a sentence that exists inmost languages.", "labels": [], "entities": []}, {"text": "In this structure, a word is specified as a predicate and a number of word groups are considered as arguments accompanying the predicate.", "labels": [], "entities": []}, {"text": "Those arguments are assigned different semantic categories depending on the roles that they play with respect to the predicate.", "labels": [], "entities": []}, {"text": "Researchers have used several different sets of argument labels.", "labels": [], "entities": []}, {"text": "One possibility are the nonmnemonic labels used in the PropBank corpus): ARG0, ARG1, \u2026, ARGM-LOC, etc.", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.9720369875431061}, {"text": "ARG0", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.5258477330207825}, {"text": "\u2026", "start_pos": 85, "end_pos": 86, "type": "METRIC", "confidence": 0.9827945828437805}]}, {"text": "An alternative set are thematic roles similar to those proposed in (): AGENT, ACTOR, BENEFICIARY, CAUSE, etc.", "labels": [], "entities": [{"text": "AGENT", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9575327038764954}, {"text": "ACTOR", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9831244945526123}, {"text": "BENEFICIARY", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.9964700937271118}, {"text": "CAUSE", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.7553385496139526}]}, {"text": "Shallow semantic parsing with the goal of creating a domain independent meaning representation based on predicate/argument structure was first explored in detail by).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.7607438564300537}]}, {"text": "Since then several variants of the basic approach have been introduced using different features and different classifiers based on various machine-learning methods (;..", "labels": [], "entities": []}, {"text": "Large semantically annotated databases, like FrameNet) and PropBank () have been used to train and test the classifiers.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9019573926925659}, {"text": "PropBank", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.914586067199707}]}, {"text": "Most of these approaches can be divided into two broad classes: Constituent-byConstituent (C-by-C) or Word-by-Word (W-by-W) classifiers.", "labels": [], "entities": []}, {"text": "In C-by-C classification, the syntactic tree.", "labels": [], "entities": [{"text": "C-by-C classification", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.6773243993520737}]}, {"text": "Proposed non-overlapping, shallow lexicalized syntactic/semantic tree structure representation of a sentence is linearized into a sequence of its syntactic constituents (non-terminals).", "labels": [], "entities": []}, {"text": "Then each constituent is classified into one of several arguments or semantic roles using a number of features derived from its respective context.", "labels": [], "entities": []}, {"text": "In the W-by-W method ( ) the problem is formulated as a chunking task and the features are derived for each word (assuming part of speech tags and syntactic phrase chunks are available), and the word is classified into one of the semantic labels using an IOB2 representation.", "labels": [], "entities": []}, {"text": "Among those methods, only the W-by-W method considered semantic classification with features created in a bottom-up manner.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7752377986907959}]}, {"text": "The motivations for bottom-up analysis are \u2022 Full syntactic parsing is computationally expensive \u2022 Taggers and chunkers are fast \u2022 Not all languages have full syntactic parsers \u2022 The annotation effort required fora full syntactic parser is larger than that required for taggers and chunkers.", "labels": [], "entities": []}, {"text": "In this paper, we propose a non-overlapping shallow tree structure, at lexical, syntactic and semantic levels to represent the language.", "labels": [], "entities": []}, {"text": "The goal is to improve the portability of semantic processing to other applications, domains and languages.", "labels": [], "entities": [{"text": "portability of semantic processing", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.6733787804841995}]}, {"text": "The new structure is complex enough to capture crucial (non-exclusive) semantic knowledge for intended applications and simple enough to allow flat, easier and fast annotation.", "labels": [], "entities": []}, {"text": "The human effort required for flat labeling is significantly less than that required for creating tree bank style labels.", "labels": [], "entities": [{"text": "flat labeling", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.6287777274847031}]}, {"text": "We present a particular derivation of the structure yielding a lightweight machine learned semantic chunker.", "labels": [], "entities": []}], "datasetContent": [{"text": "All experiments were carried out using sections 15-18 of the PropBank data holding out Section-00 and Section-23 for development and test, respectively.", "labels": [], "entities": [{"text": "PropBank data", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9781987071037292}]}, {"text": "We used chunklink 2 to flatten syntactic trees.", "labels": [], "entities": []}, {"text": "Then using the predicate argument annotation we obtained anew corpus of the tree structure introduced in Section 2.", "labels": [], "entities": []}, {"text": "All SVM classifiers, for POS tagging, syntactic phrase chunking and semantic argument labeling, were realized using the TinySVM 3 with the polynomial kernel of degree 2 and the general purpose SVM based chunker YamCha . The results were evaluated using precision and recall numbers along with the F metric.", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8550541400909424}, {"text": "POS tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.8243739306926727}, {"text": "syntactic phrase chunking", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6445593039194742}, {"text": "semantic argument labeling", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.6354995171229044}, {"text": "precision", "start_pos": 253, "end_pos": 262, "type": "METRIC", "confidence": 0.9987084865570068}, {"text": "recall", "start_pos": 267, "end_pos": 273, "type": "METRIC", "confidence": 0.9923175573348999}, {"text": "F metric", "start_pos": 297, "end_pos": 305, "type": "METRIC", "confidence": 0.9733249545097351}]}, {"text": "compares W-by-W and P-by-P approaches.", "labels": [], "entities": []}, {"text": "The base features described in Section 4 along with two additional predicate specific features were used; the lemma of the predicate and a binary feature that indicates the word is before or after the predicate.", "labels": [], "entities": []}], "tableCaptions": []}