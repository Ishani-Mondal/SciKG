{"title": [{"text": "Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.9694133102893829}]}], "abstractContent": [{"text": "We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7376899421215057}, {"text": "BABAR", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.977554440498352}]}, {"text": "BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsu-pervised learning.", "labels": [], "entities": [{"text": "BABAR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5794570446014404}]}, {"text": "These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.", "labels": [], "entities": []}, {"text": "BABAR applies a Dempster-Shafer probabilis-tic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.", "labels": [], "entities": [{"text": "BABAR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6670278906822205}]}, {"text": "Experiments in two domains showed that the contextual role knowledge improved corefer-ence performance, especially on pronouns.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (), syntactic algorithms (e.g.,), and supervised machine learning systems).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.9734304547309875}]}, {"text": "Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.97137850522995}]}, {"text": "The focus of our work is on the use of contextual role knowledge for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.979042112827301}]}, {"text": "A contextual role represents the role that a noun phrase plays in an event or relationship.", "labels": [], "entities": []}, {"text": "Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.", "labels": [], "entities": []}, {"text": "Consider the following sentences: (a) Jose Maria Martinez, Roberto Lisandy, and Dino Rossy, who were staying at a Tecun Uman hotel, were kidnapped by armed men who took them to an unknown place.", "labels": [], "entities": []}, {"text": "(b) After they were released...", "labels": [], "entities": []}, {"text": "(c) After they blindfolded the men...", "labels": [], "entities": []}, {"text": "In (b) \"they\" refers to the kidnapping victims, but in (c) \"they\" refers to the armed men.", "labels": [], "entities": []}, {"text": "The role that each noun phrase plays in the kidnapping event is key to distinguishing these cases.", "labels": [], "entities": []}, {"text": "The correct resolution in sentence (b) comes from knowledge that people who are kidnapped are often subsequently released.", "labels": [], "entities": []}, {"text": "The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims.", "labels": [], "entities": []}, {"text": "We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.8459849059581757}, {"text": "BABAR", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.8419359922409058}]}, {"text": "BABAR employs information extraction techniques to represent and learn role relationships.", "labels": [], "entities": [{"text": "BABAR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6947411298751831}, {"text": "information extraction", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.7429268658161163}]}, {"text": "Each pattern represents the role that a noun phrase plays in the surrounding context.", "labels": [], "entities": []}, {"text": "BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data.", "labels": [], "entities": [{"text": "BABAR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6302210688591003}]}, {"text": "Training examples are generated automatically by identifying noun phrases that can be easily resolved with their antecedents using lexical and syntactic heuristics.", "labels": [], "entities": []}, {"text": "BABAR then computes statistics over the training examples measuring the frequency with which extraction patterns and noun phrases co-occur in coreference resolutions.", "labels": [], "entities": [{"text": "BABAR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5313798785209656}]}, {"text": "In this paper, Section 2 begins by explaining how contextual role knowledge is represented and learned.", "labels": [], "entities": []}, {"text": "Section 3 describes the complete coreference resolution model, which uses the contextual role knowledge as well as more traditional coreference features.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.9468573927879333}]}, {"text": "Our coreference resolver also incorporates an existential noun phrase recognizer and a Dempster-Shafer probabilistic model to make resolution decisions.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8779839277267456}, {"text": "existential noun phrase recognizer", "start_pos": 46, "end_pos": 80, "type": "TASK", "confidence": 0.6863928139209747}]}, {"text": "Section 4 presents experimental results on two corpora: the MUC-4 terrorism corpus, and Reuters texts about natural disasters.", "labels": [], "entities": [{"text": "MUC-4 terrorism corpus", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.9542828996976217}, {"text": "Reuters texts", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.9085691571235657}]}, {"text": "Our results show that BABAR achieves good performance in both domains, and that the contextual role knowledge improves performance, especially on pronouns.", "labels": [], "entities": [{"text": "BABAR", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.8779324889183044}]}, {"text": "Finally, Section 5 explains how BABAR relates to previous work, and Section 6 summarizes our conclusions.", "labels": [], "entities": [{"text": "BABAR", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.7301461100578308}]}], "datasetContent": [{"text": "We adopted the MUC-6 guidelines for evaluating coreference relationships based on transitivity in anaphoric chains.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.8179911971092224}]}, {"text": "For example, if {N P 1 , NP 2 , NP 3 } are all coreferent, then each NP must be linked to one of the other two NPs.", "labels": [], "entities": []}, {"text": "First, we evaluated BABAR using only the seven general knowledge sources.", "labels": [], "entities": [{"text": "BABAR", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.856506884098053}]}, {"text": "We measured recall (Rec), precision (Pr), and the F-measure (F) with recall and precision equally weighted.", "labels": [], "entities": [{"text": "recall (Rec)", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.9589931815862656}, {"text": "precision (Pr)", "start_pos": 26, "end_pos": 40, "type": "METRIC", "confidence": 0.9670723974704742}, {"text": "F-measure (F)", "start_pos": 50, "end_pos": 63, "type": "METRIC", "confidence": 0.9570068717002869}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9979779124259949}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9377726316452026}]}, {"text": "BABAR achieved recall in the 42-50% range for both domains, with 76% precision overall for terrorism and 87% precision for natural disasters.", "labels": [], "entities": [{"text": "BABAR", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5709993243217468}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9994833469390869}, {"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.999555766582489}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9991057515144348}]}, {"text": "We suspect that the higher precision in the disasters domain maybe due to its substantially larger training corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9993823766708374}]}, {"text": "shows BABAR's performance when the four contextual role knowledge sources are added.", "labels": [], "entities": [{"text": "BABAR", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.9608877897262573}]}, {"text": "The Fmeasure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision.", "labels": [], "entities": [{"text": "Fmeasure score", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.985266387462616}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9996337890625}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.998522937297821}]}, {"text": "The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters.", "labels": [], "entities": [{"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9979257583618164}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9984986782073975}, {"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9992621541023254}, {"text": "precision", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.998872697353363}]}, {"text": "The difference in performance between pronouns and definite noun phrases surprised us.", "labels": [], "entities": []}, {"text": "Analysis of the data revealed that the contextual role knowledge is especially helpful for resolving pronouns because, in general, they are semantically weaker than definite NPs.", "labels": [], "entities": []}, {"text": "Since pronouns carry little semantics of their own, resolving them depends almost entirely on context.", "labels": [], "entities": []}, {"text": "In contrast, even though context can be helpful for resolving definite NPs, context can be trumped by the semantics of the nouns themselves.", "labels": [], "entities": []}, {"text": "For example, even if the contexts surrounding an anaphor and candidate match exactly, they are not coreferent if they have substantially different meanings We would be happy to make our manually annotated test data available to others who also want to evaluate their coreference resolver on the MUC-4 or Reuters collections.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 267, "end_pos": 287, "type": "TASK", "confidence": 0.8428726196289062}, {"text": "MUC-4 or Reuters collections", "start_pos": 295, "end_pos": 323, "type": "DATASET", "confidence": 0.7856597751379013}]}], "tableCaptions": [{"text": " Table 2: General Knowledge Sources", "labels": [], "entities": []}, {"text": " Table 3: General + Contextual Role Knowledge Sources", "labels": [], "entities": []}, {"text": " Table 4: Individual Performance of KSs for Terrorism", "labels": [], "entities": []}, {"text": " Table 5: Individual Performance of KSs for Disasters", "labels": [], "entities": [{"text": "Disasters", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.39448824524879456}]}]}