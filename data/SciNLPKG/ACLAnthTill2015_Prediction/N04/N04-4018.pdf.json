{"title": [{"text": "Improving Automatic Sentence Boundary Detection with Confusion Networks A. Stolcke\u00a3", "labels": [], "entities": [{"text": "Improving Automatic Sentence Boundary Detection", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8560281872749329}]}], "abstractContent": [{"text": "We extend existing methods for automatic sentence boundary detection by leveraging multiple recognizer hypotheses in order to provide robustness to speech recognition errors.", "labels": [], "entities": [{"text": "automatic sentence boundary detection", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.6536704152822495}, {"text": "speech recognition errors", "start_pos": 148, "end_pos": 173, "type": "TASK", "confidence": 0.77586563428243}]}, {"text": "For each hypothesized word sequence, an HMM is used to estimate the posterior probability of a sentence boundary at each word boundary.", "labels": [], "entities": []}, {"text": "The hypotheses are combined using confusion networks to determine the overall most likely events.", "labels": [], "entities": []}, {"text": "Experiments show improved detection of sentences for conversational telephone speech, though results are mixed for broadcast news.", "labels": [], "entities": []}], "introductionContent": [{"text": "The output of most current automatic speech recognition systems is an unstructured sequence of words.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7233509719371796}]}, {"text": "Additional information such as sentence boundaries and speaker labels are useful to improve readability and can provide structure relevant to subsequent language processing, including parsing, topic segmentation and summarization.", "labels": [], "entities": [{"text": "parsing", "start_pos": 184, "end_pos": 191, "type": "TASK", "confidence": 0.9701569080352783}, {"text": "topic segmentation", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.6927073150873184}, {"text": "summarization", "start_pos": 216, "end_pos": 229, "type": "TASK", "confidence": 0.9784470200538635}]}, {"text": "In this study, we focus on identifying sentence boundaries using word-based and prosodic cues, and in particular we develop a method that leverages additional information available from multiple recognizer hypotheses.", "labels": [], "entities": []}, {"text": "Multiple hypotheses are helpful because the single best recognizer output still has many errors even for stateof-the-art systems.", "labels": [], "entities": []}, {"text": "For conversational telephone speech (CTS) word error rates can be from 20-30%, and for broadcast news (BN) word error rates are 10-15%.", "labels": [], "entities": [{"text": "conversational telephone speech (CTS) word error rates", "start_pos": 4, "end_pos": 58, "type": "TASK", "confidence": 0.7099273337258233}]}, {"text": "These errors limit the effectiveness of sentence boundary prediction, because they introduce incorrect words to the word stream.", "labels": [], "entities": [{"text": "sentence boundary prediction", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.7547507882118225}]}, {"text": "Sentence boundary detection error rates on a baseline system increased by 50% relative for CTS when moving from the reference to the automatic speech condition, while for BN error rates increased by about 20% relative ( . Including additional recognizer hypotheses allows for alternative word choices to inform sentence boundary prediction.", "labels": [], "entities": [{"text": "Sentence boundary detection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7791245182355245}, {"text": "sentence boundary prediction", "start_pos": 311, "end_pos": 339, "type": "TASK", "confidence": 0.7313321828842163}]}, {"text": "To integrate the information from different alternatives, we first predict sentence boundaries in each hypothesized word sequence, using an HMM structure that integrates prosodic features in a decision tree with hidden event language modeling.", "labels": [], "entities": []}, {"text": "To facilitate merging predictions from multiple hypotheses, we represent each hypothesis as a confusion network, with confidences for sentence predictions from a baseline system.", "labels": [], "entities": []}, {"text": "The final prediction is based on a combination of predictions from individual hypotheses, each weighted by the recognizer posterior for that hypothesis.", "labels": [], "entities": []}, {"text": "Our methods build on related work in sentence boundary detection and confusion networks, as described in Section 2, and a baseline system and task domain reviewed in Section 3.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6959692239761353}]}, {"text": "Our approach integrates prediction on multiple recognizer hypotheses using confusion networks, as outlined in Section 4.", "labels": [], "entities": []}, {"text": "Experimental results are detailed in Section 5, and the main conclusions of this work are summarized in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Errors are measured by a slot error rate similar to the WER metric utilized by the speech recognition community, i.e. dividing the total number of inserted and deleted SUs by the total number of reference SUs.", "labels": [], "entities": [{"text": "slot error rate", "start_pos": 25, "end_pos": 40, "type": "METRIC", "confidence": 0.9473545551300049}, {"text": "WER", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.8721683025360107}, {"text": "speech recognition community", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.8190364042917887}]}, {"text": "(There are no substitution errors because there is only one sentence class.)", "labels": [], "entities": []}, {"text": "When recognition output is used, the words will generally not align perfectly with the reference transcription and hence the SU boundary predictions will require some alignment procedure to match to the reference location.", "labels": [], "entities": []}, {"text": "Here, the alignment is based on the minimum word error alignment of the reference and hypothesized word strings, and the minimum SU error alignment if the WER is equal for multiple alignments.", "labels": [], "entities": [{"text": "word error alignment", "start_pos": 44, "end_pos": 64, "type": "METRIC", "confidence": 0.7977940042813619}, {"text": "SU error alignment", "start_pos": 129, "end_pos": 147, "type": "METRIC", "confidence": 0.9642874201138815}, {"text": "WER", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.9732730984687805}]}, {"text": "We report numbers computed with the su-eval scoring tool from NIST.", "labels": [], "entities": [{"text": "NIST", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.9742721319198608}]}, {"text": "SU error rates for the reference words condition of our baseline system are 49.04% for BN, and 30.13% for CTS, as reported at the NIST RT03F evaluation ( . Results for the automatic speech recognition condition are described in Section 5.", "labels": [], "entities": [{"text": "SU error", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8386902809143066}, {"text": "CTS", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.6194303631782532}, {"text": "NIST RT03F evaluation", "start_pos": 130, "end_pos": 151, "type": "DATASET", "confidence": 0.9387195110321045}, {"text": "speech recognition", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.6730376929044724}]}, {"text": "Multiple hypotheses provide a reduction of error for both test sets of CTS (significant at p .02 using the: Word and SU error rates for single best vs. confusion nets.", "labels": [], "entities": [{"text": "error", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9744359254837036}]}, {"text": "maybe due to the fact that the 1-best parameters were tuned on different news shows than were represented in the evaluation data.", "labels": [], "entities": []}, {"text": "We expected a greater gain from the use of confusion networks in CTS than BN, given the previously shown impact of WER on 1-best SU detection.", "labels": [], "entities": [{"text": "BN", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.4924464225769043}, {"text": "WER", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9687774181365967}, {"text": "SU detection", "start_pos": 129, "end_pos": 141, "type": "TASK", "confidence": 0.9471060335636139}]}, {"text": "Additionally, incorporating a larger number of N-best hypotheses has improved results in all experiments so far, so we would expect this trend to continue for additional increases, but time constraints limited our ability to run these larger experiments.", "labels": [], "entities": []}, {"text": "One possible explanation for the relatively small performance gains is that we constrained the confusion network topology so that there was no change in the word recognition results.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 157, "end_pos": 173, "type": "TASK", "confidence": 0.8233571350574493}]}, {"text": "We imposed this constraint in our initial investigations to allow us to compare performance using the same words.", "labels": [], "entities": []}, {"text": "It it possible that better performance could be obtained by using confusion network topologies that link words and metadata.", "labels": [], "entities": []}, {"text": "A more specific breakout of error improvement for the CTS development set is given in, showing that both recall and precision benefit from using the N-best framework.", "labels": [], "entities": [{"text": "CTS development set", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.7172388434410095}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.999157190322876}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9989414811134338}]}, {"text": "Including multiple hypotheses reduces the number of SU deletions (improves recall), but the primary gain is in reducing insertion errors (higher precision).", "labels": [], "entities": [{"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9995644688606262}, {"text": "insertion errors", "start_pos": 120, "end_pos": 136, "type": "METRIC", "confidence": 0.9706848859786987}, {"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9985451698303223}]}, {"text": "The same effect holds for the CTS evaluation set.", "labels": [], "entities": [{"text": "CTS evaluation set", "start_pos": 30, "end_pos": 48, "type": "DATASET", "confidence": 0.7206289370854696}]}], "tableCaptions": [{"text": " Table 1: Word and SU error rates for single best vs. con- fusion nets.", "labels": [], "entities": []}, {"text": " Table 2: Errors for CTS development set", "labels": [], "entities": [{"text": "CTS development", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.9115284383296967}]}]}