{"title": [{"text": "Identifying Chemical Names in Biomedical Text: An Investigation of the Substring Co-occurrence Based Approaches", "labels": [], "entities": [{"text": "Identifying Chemical Names in Biomedical Text", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8774720927079519}]}], "abstractContent": [{"text": "We investigate various strategies for finding chemicals in biomedical text using substring co-occurrence information.", "labels": [], "entities": []}, {"text": "The goal i s to build a system from readily available data with minimal human involvement.", "labels": [], "entities": []}, {"text": "Our models are trained from a dictionary of chemical names and general biomedical text.", "labels": [], "entities": []}, {"text": "We investigated several strategies including Na\u00efve Bayes classifiers and several types of N-gram models.", "labels": [], "entities": []}, {"text": "We introduced anew way of interpolating N-grams that does not require tuning any parameters.", "labels": [], "entities": []}, {"text": "We also found the task to be similar to Language Identification.", "labels": [], "entities": [{"text": "Language Identification", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.748806893825531}]}], "introductionContent": [{"text": "Chemical names recognition is one of the first tasks needed for building an information extraction system in the biomedical domain.", "labels": [], "entities": [{"text": "Chemical names recognition", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7519243160883585}, {"text": "information extraction", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.7227164804935455}]}, {"text": "Chemicals, especially organic chemicals, are one of the main agents in many processes and relationships such a system would need to find.", "labels": [], "entities": []}, {"text": "In this work, we investigate a number of approaches to the problem of chemical names identification.", "labels": [], "entities": [{"text": "chemical names identification", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.6790476044019064}]}, {"text": "We focus on approaches that use string internal information for classification, those based on the character cooccurrence statistics within the strings that we would like to classify.", "labels": [], "entities": []}, {"text": "We would also like not to spend much time and effort to do manual annotation, and hence use readily publicly available data for training all the models.", "labels": [], "entities": []}, {"text": "Because of that, we would be satisfied with only moderate results.", "labels": [], "entities": []}, {"text": "In the course of this investigation, we have found that N-gram methods work best given these restrictions on the models.", "labels": [], "entities": []}, {"text": "Work has been done on a related task of named entity recognition (, and others).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6988358894983927}]}, {"text": "The aim of the named entity task is usually set to find names of people, organizations, and some other similar entities in text.", "labels": [], "entities": []}, {"text": "Adding features based on the internal substring patterns has been found useful by.", "labels": [], "entities": []}, {"text": "For finding chemicals, internal substring patterns are even more important source of information.", "labels": [], "entities": []}, {"text": "Many substrings of chemical names are very characteristic.", "labels": [], "entities": []}, {"text": "For example, seeing \"methyl\" as a substring of a word is a strong indicator of a chemical name.", "labels": [], "entities": []}, {"text": "The systematic chemical names are constructed from substrings like that, but even the generic names follow certain conventions, and have many characteristic substrings.", "labels": [], "entities": []}, {"text": "In this work, character co-occurrence patterns are extracted from available lists of chemicals that have been compiled for other purposes.", "labels": [], "entities": []}, {"text": "We built models based on the difference between strings occurring in chemical names and strings that occur in other words.", "labels": [], "entities": []}, {"text": "The use of only string internal information prevents us from disambiguating different word senses, but we accept this source of errors as a minor one.", "labels": [], "entities": []}, {"text": "Classification based solely on string internal information makes the chemical names recognition task similar to language identification.", "labels": [], "entities": [{"text": "chemical names recognition", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.691091408332189}, {"text": "language identification", "start_pos": 112, "end_pos": 135, "type": "TASK", "confidence": 0.7419082224369049}]}, {"text": "In the language identification task, these patterns are used to detect strings from a different language embedded into text.", "labels": [], "entities": [{"text": "language identification task", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.8122354745864868}]}, {"text": "Because chemicals are so different, we can view them as a different language, and borrow some of the Language Identification techniques.", "labels": [], "entities": [{"text": "Language Identification", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.6697770059108734}]}, {"text": "Danning, 1994 was able to achieve good results using character N -gram models on language identification even on short strings (20 symbols long).", "labels": [], "entities": [{"text": "language identification", "start_pos": 81, "end_pos": 104, "type": "TASK", "confidence": 0.7548681497573853}]}, {"text": "This suggests that his approach might be successful in chemical names identification setting.", "labels": [], "entities": [{"text": "chemical names identification setting", "start_pos": 55, "end_pos": 92, "type": "TASK", "confidence": 0.7985993251204491}]}, {"text": "N-gram based methods were previously used for chemicals recognition.", "labels": [], "entities": [{"text": "chemicals recognition", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.8187269270420074}]}, {"text": "used all substrings of a fixed length N, but they combined the training counts in a Bayesian framework, ignoring nonindependence of overlapping substring.", "labels": [], "entities": []}, {"text": "They claimed good performance for their data, but this approach showed significantly lower performance than alternatives on our data.", "labels": [], "entities": []}, {"text": "See the results section for more details.", "labels": [], "entities": []}, {"text": "The difference is that their data is carefully constructed to contain only chemicals and chemicals of all types in the test data, i.e. their training and testing data is in a very close correspondence.", "labels": [], "entities": []}, {"text": "We on the other hand tried to use readily available chemical lists without putting much manual labor into their construction.", "labels": [], "entities": []}, {"text": "Most of our training data comes from a single source -National Cancer Institute website -and hence represents only a very specific domain of chemicals, while testing data is coming from a random sample from MEDLINE.", "labels": [], "entities": [{"text": "National Cancer Institute website", "start_pos": 54, "end_pos": 87, "type": "DATASET", "confidence": 0.9827061593532562}, {"text": "MEDLINE", "start_pos": 207, "end_pos": 214, "type": "DATASET", "confidence": 0.9534808397293091}]}, {"text": "In addition, these lists were designed for use by human, and hence contain many comments and descriptions that are not easily separable for the chemical names themselves.", "labels": [], "entities": []}, {"text": "Several attempts on cleaning these out have been made.", "labels": [], "entities": []}, {"text": "Most aggressive attempts deleted about half the text from the list.", "labels": [], "entities": []}, {"text": "While deleting many useful names, this improved the results significantly.", "labels": [], "entities": []}, {"text": "While we found that N-grams worked best amoung the approaches we have tried, other approaches are also possible.", "labels": [], "entities": []}, {"text": "We did not explore the possibility of using substring as features to a generic classification algorithm, such as, for example, support vector machines).", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed cross validation experiments on 15 handannotated MEDLINE abstracts described in section \"Available Data\".", "labels": [], "entities": [{"text": "MEDLINE abstracts", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.6926813423633575}]}, {"text": "Experiments were done by holding out each abstract, tuning model parameters on 14 remaining abstracts, and testing on the held out one.", "labels": [], "entities": []}, {"text": "Fifteen such experiments were performed.", "labels": [], "entities": []}, {"text": "The results of these experiments were combined by taking weighed geometric mean of precision results at each recall level.", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.8072551488876343}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9879590272903442}]}, {"text": "The results were weighted according to the number of positive examples in each file to ensure equal contribution from each example.", "labels": [], "entities": []}, {"text": "shows the resulting precision/recall curves.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9993501305580139}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9353581666946411}]}, {"text": "As we can see, the N-gram approaches perform better than the other ones.", "labels": [], "entities": []}, {"text": "The interpolated model with quadratic coefficients needs a lot of development data, so it does not produce good results in our case.", "labels": [], "entities": []}, {"text": "Simple Laplacian smoothing needs less development data and produces much better results.", "labels": [], "entities": []}, {"text": "The model with confidence based coefficients works best.", "labels": [], "entities": []}, {"text": "The graph also shows the model introduced by.", "labels": [], "entities": []}, {"text": "It does not perform nearly as well on our data, even though it produces very good results on clean data they have used.", "labels": [], "entities": []}, {"text": "This (as well as some experiments we performed that have not been included into this work) suggests that quality of the training data has very strong effect on the model results.", "labels": [], "entities": []}], "tableCaptions": []}