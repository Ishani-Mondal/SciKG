{"title": [{"text": "What's in a translation rule?", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora.", "labels": [], "entities": []}, {"text": "We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ina very interesting study of syntax in statistical machine translation, looks at how well proposed translation models fit actual translation data.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.6306439836819967}]}, {"text": "One such model embodies a restricted, linguistically-motivated notion of word re-ordering.", "labels": [], "entities": []}, {"text": "Given an English parse tree, children at any node maybe reordered prior to translation.", "labels": [], "entities": []}, {"text": "Previous to, it had been observed that this model would prohibit certain re-orderings in certain language pairs (such as subject-VP(verb-object) into verb-subject-object), but Fox carried out the first careful empirical study, showing that many other common translation patterns fall outside the scope of the child-reordering model.", "labels": [], "entities": []}, {"text": "This is true even for languages as similar as English and French.", "labels": [], "entities": []}, {"text": "For example, English adverbs tend to move outside the local parent/children in environment.", "labels": [], "entities": []}, {"text": "The English word \"not\" translates to the discontiguous pair \"ne ... pas.\"", "labels": [], "entities": []}, {"text": "English parsing errors also cause trouble, as a normally well-behaved re-ordering environment can be disrupted by wrong phrase attachment.", "labels": [], "entities": [{"text": "English parsing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5414310246706009}]}, {"text": "For other language pairs, the divergence is expected to be greater.", "labels": [], "entities": []}, {"text": "In the face of these problems, we may choose among several alternatives.", "labels": [], "entities": []}, {"text": "The first is to abandon syntax in statistical machine translation, on the grounds that syntactic models area poor fit for the data.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.7097988426685333}]}, {"text": "On this view, adding syntax yields no improvement over robust phrasesubstitution models, and the only question is how much does syntax hurt performance.", "labels": [], "entities": []}, {"text": "Along this line, () present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance -the ability to translate nonconstituent phrases (such as \"there are\", \"note that\", and \"according to\") turns out to be critical and pervasive.", "labels": [], "entities": []}, {"text": "Another direction is to abandon conventional English syntax and move to more robust grammars that adapt to the parallel training corpus.", "labels": [], "entities": []}, {"text": "One approach here is that of, in which word-movement is modeled by rotations at unlabeled, binary-branching nodes.", "labels": [], "entities": []}, {"text": "At each sentence pair, the parse adapts to explain the translation pattern.", "labels": [], "entities": []}, {"text": "If the same unambiguous English sentence were to appear twice in the corpus, with different Chinese translations, then it could have different learned parses.", "labels": [], "entities": []}, {"text": "A third direction is to maintain English syntax and investigate alternate transformation models.", "labels": [], "entities": []}, {"text": "After all, many conventional translation systems are indeed based on syntactic transformations far more expressive than what has been proposed in syntax-based statistical MT.", "labels": [], "entities": []}, {"text": "We take this approach in our paper.", "labels": [], "entities": []}, {"text": "Of course, the broad statistical MT program is aimed at a wider goal than the conventional rule-based program -it seeks to understand and explain human translation data, and automatically learn from it.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.7772735953330994}]}, {"text": "For this reason, we think it is important to learn from the model/data explainability studies of and to extend her results.", "labels": [], "entities": []}, {"text": "In addition to being motivated by rule-based systems, we also see advantages to English syntax within the statistical framework, such as marrying syntax-based translation models with syntaxbased language models () and other potential benefits described by.", "labels": [], "entities": []}, {"text": "Our basic idea is to create transformation rules that condition on larger fragments of tree structure.", "labels": [], "entities": []}, {"text": "It is certainly possible to build such rules by hand, and we have done this to formally explain a number of humantranslation examples.", "labels": [], "entities": []}, {"text": "But our main interest is in collecting a large set of such rules automatically through corpus analysis.", "labels": [], "entities": []}, {"text": "The search for these rules is driven exactly by the problems raised by -cases of crossing and divergence motivate the algorithms to come up with better explanations of the data and better rules.", "labels": [], "entities": []}, {"text": "Section 2 of this paper describes algorithms for the acquisition of complex rules fora transformation model.", "labels": [], "entities": []}, {"text": "Section 3 gives empirical results on the explanatory power of the acquired rules versus previous models.", "labels": [], "entities": []}, {"text": "Section 4 presents examples of learned rules and shows the various types of transformations (lexical and nonlexical, contiguous and noncontiguous, simple and complex) that the algorithms are forced (by the data) to invent.", "labels": [], "entities": []}, {"text": "Due to space constraints, all proofs are omitted.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Running time in seconds of the two algorithms  on 1000 sentences. k represent the maximum size of rules  to extract.", "labels": [], "entities": []}]}