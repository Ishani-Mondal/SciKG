{"title": [{"text": "Shallow Semantic Parsing using Support Vector Machines *", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.7066572904586792}]}], "abstractContent": [{"text": "In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al.", "labels": [], "entities": [{"text": "shallow semantic parsing", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.6047529876232147}]}, {"text": "(2003) and others.", "labels": [], "entities": []}, {"text": "Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers.", "labels": [], "entities": []}, {"text": "We show performance improvements through a number of new features and measure their ability to generalize to anew test set drawn from the AQUAINT corpus.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 138, "end_pos": 152, "type": "DATASET", "confidence": 0.9508270025253296}]}], "introductionContent": [{"text": "Automatic, accurate and wide-coverage techniques that can annotate naturally occurring text with semantic argument structure can play a key role in NLP applications such as Information Extraction, Question Answering and Summarization.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.8176517486572266}, {"text": "Question Answering", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.8677662014961243}, {"text": "Summarization", "start_pos": 220, "end_pos": 233, "type": "TASK", "confidence": 0.9398897290229797}]}, {"text": "Shallow semantic parsing -the process of assigning a simple WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW, etc.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.6941830664873123}, {"text": "WHOM", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.44916465878486633}]}, {"text": "structure to sentences in text, is the process of producing such a markup.", "labels": [], "entities": []}, {"text": "When presented with a sentence, a parser should, for each predicate in the sentence, identify and label the predicate's semantic arguments.", "labels": [], "entities": []}, {"text": "This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning specific labels to them.", "labels": [], "entities": []}, {"text": "In recent work, a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it;;;;;;; ;; ).", "labels": [], "entities": []}, {"text": "In this * This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grant IS-9978025 paper, we report on a series of experiments exploring this approach.", "labels": [], "entities": [{"text": "ARDA AQUAINT", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.6220107078552246}]}, {"text": "For the initial experiments, we adopted the approach described by (G&J) and evaluated a series of modifications to improve its performance.", "labels": [], "entities": []}, {"text": "In the experiments reported here, we first replaced their statistical classification algorithm with one that uses Support Vector Machines and then added to the existing feature set.", "labels": [], "entities": [{"text": "statistical classification", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.7211331427097321}]}, {"text": "We evaluate results using both handcorrected TreeBank syntactic parses, and actual parses from the Charniak parser.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Baseline performance on all three tasks using  hand-corrected parses.", "labels": [], "entities": []}, {"text": " Table 2: Improvements on the task of argument identi- fication and classification after disallowing overlapping  constituents.", "labels": [], "entities": []}, {"text": " Table 4: Effect of each feature on the argument identifi- cation and classification tasks when added to the baseline  system.", "labels": [], "entities": []}, {"text": " Table 6: Best system performance on all tasks using  hand-corrected parses.", "labels": [], "entities": []}, {"text": " Table 7: Performance degradation when using automatic  parses instead of hand-corrected ones.", "labels": [], "entities": []}, {"text": " Table 8: Best system performance on all tasks using  hand-corrected parses using the latest PropBank data.", "labels": [], "entities": [{"text": "PropBank data", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.97060826420784}]}, {"text": " Table 9: Performance of various feature combinations on  the task of argument classification.", "labels": [], "entities": [{"text": "argument classification", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.7172686457633972}]}, {"text": " Table 10: Performance of various feature combinations  on the task of argument identification", "labels": [], "entities": [{"text": "argument identification", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.7530977129936218}]}, {"text": " Table 14: Identification and classification", "labels": [], "entities": [{"text": "classification", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7042649984359741}]}, {"text": " Table 15: Performance on the AQUAINT test set.", "labels": [], "entities": [{"text": "AQUAINT test set", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.8989448149998983}]}, {"text": " Table 16: Feature Coverage on PropBank test set using  parser trained on PropBank training set.", "labels": [], "entities": [{"text": "PropBank test set", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9754597942034403}, {"text": "PropBank training set", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.9800684054692587}]}, {"text": " Table 17: Coverage of features on AQUAINT test set us- ing parser trained on PropBank training set.", "labels": [], "entities": [{"text": "AQUAINT test set", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.9435641368230184}, {"text": "PropBank training set", "start_pos": 78, "end_pos": 99, "type": "DATASET", "confidence": 0.9696628848711649}]}]}