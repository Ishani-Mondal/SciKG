{"title": [{"text": "Enhancing Linguistically Oriented Automatic Keyword Extraction", "labels": [], "entities": [{"text": "Enhancing Linguistically Oriented Automatic Keyword Extraction", "start_pos": 0, "end_pos": 62, "type": "TASK", "confidence": 0.8499511380990347}]}], "abstractContent": [{"text": "This paper presents experiments on how the performance of automatic keyword extraction can be improved, as measured by keywords previously assigned by professional indexers.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7257857769727707}]}, {"text": "The keyword extraction algorithm consists of three prediction models that are combined to decide what words or sequences of words in the documents are suitable as keywords.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8874428272247314}]}, {"text": "The models, in turn, are built using different definitions of what constitutes a term in a written document.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic keyword indexing is the task of finding a small set of terms that describes the content of a specific document.", "labels": [], "entities": [{"text": "Automatic keyword indexing", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5897725721200308}]}, {"text": "If the keywords are chosen from the document at hand, it is referred to as keyword extraction, and this is the approach taken for the work presented in this paper.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8001216053962708}]}, {"text": "Once a document has a set of keywords, they can be useful for several tasks.", "labels": [], "entities": []}, {"text": "For example, they can be the entrance to a document collection, similar to a back-ofthe-book index; they can be used to refine a query to a search engine; or they may serve as a dense summary fora specific document.", "labels": [], "entities": []}, {"text": "In the presented research, the decision of what words or sequences of words in the documents that are suitable as keywords are made by prediction models trained on documents with manually assigned keywords.", "labels": [], "entities": []}, {"text": "This paper presents a number of modifications to an existing keyword extraction algorithm, as well as results of the empirical verifications.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.8042306005954742}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. As can be seen in this table, the recall increases  while the precision decreases. However, the high increase  in recall leads to an increase in the F-measure from 33.0  to 36.8.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9997941851615906}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9996955394744873}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9996297359466553}, {"text": "F-measure", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9978208541870117}]}, {"text": " Table 1: Extracting NP-chunks with and without the ini- tial determiners a, an, and the.", "labels": [], "entities": [{"text": "Extracting NP-chunks", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7830138206481934}]}, {"text": " Table 2: Calculating the collection frequency from the  abstracts, and from a general corpus (Gen. Corp.).", "labels": [], "entities": [{"text": "Gen. Corp.)", "start_pos": 95, "end_pos": 106, "type": "DATASET", "confidence": 0.9410609006881714}]}, {"text": " Table 3. As can be seen  in this table, the recall decreases, while the precision and  the F-measure increase.", "labels": [], "entities": [{"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9997982382774353}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9997745156288147}, {"text": "F-measure", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9986246824264526}]}, {"text": " Table 3: Combining the classifiers with the best individ- ual weight and with the best combination, respectively.", "labels": [], "entities": []}, {"text": " Table 4: Using classification and regression. 'Reg. ind.  thesh.' refers to a run where the regression value from  each model contributes only if it is over a certain thresh- old.", "labels": [], "entities": []}, {"text": " Table 5: Assigning all terms over the threshold (All),  and limiting the number of terms assigned per document  (Max. 12, and 3-12 respectively).", "labels": [], "entities": []}]}