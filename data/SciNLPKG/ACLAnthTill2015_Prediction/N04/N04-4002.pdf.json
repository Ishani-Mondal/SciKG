{"title": [{"text": "MMR-based feature selection for text categorization", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9097448388735453}, {"text": "text categorization", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7226133942604065}]}], "abstractContent": [{"text": "We introduce anew method of feature selection for text categorization.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7672364115715027}]}, {"text": "Our MMR-based feature selection method strives to reduce redundancy between features while maintaining information gain in selecting appropriate features for text categorization.", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.8265817165374756}]}, {"text": "Empirical results show that MMR-based feature selection is more effective than Koller & Sahami's method, which is one of greedy feature selection methods, and conventional information gain which is commonly used in feature selection for text categorization.", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.9074362516403198}, {"text": "feature selection", "start_pos": 215, "end_pos": 232, "type": "TASK", "confidence": 0.7047364860773087}, {"text": "text categorization", "start_pos": 237, "end_pos": 256, "type": "TASK", "confidence": 0.7617532014846802}]}, {"text": "Moreover, MMR-based feature selection sometimes produces some improvements of conventional machine learning algorithms over SVM which is known to give the best classification accuracy.", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.9167136351267496}, {"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9061523079872131}]}], "introductionContent": [{"text": "Text categorization is the problem of automatically assigning predefined categories to free text documents.", "labels": [], "entities": [{"text": "Text categorization", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7318470627069473}]}, {"text": "A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years.", "labels": [], "entities": [{"text": "statistical classification", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.7863666117191315}, {"text": "text categorization", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.8251023292541504}]}, {"text": "A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.747814416885376}]}, {"text": "The native feature space consists of the unique terms that occur in documents, which can be tensor hundreds of thousands of terms for even a moderatesized text collection.", "labels": [], "entities": []}, {"text": "This is prohibitively high for many machine learning algorithms.", "labels": [], "entities": []}, {"text": "If we reduce the set of features considered by the algorithm, we can serve two purposes.", "labels": [], "entities": []}, {"text": "We can considerably decrease the running time of the learning algorithm, and we can increase the accuracy of the resulting model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9995903372764587}]}, {"text": "In this line, a number of researches have recently addressed the issue of feature subset selection[4].", "labels": [], "entities": [{"text": "feature subset selection", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.6346283058325449}]}, {"text": "Yang and Pederson found information gain (IG) and chi-square test (CHI) most effective in aggressive term removal without losing categorization accuracy in their experiments.", "labels": [], "entities": [{"text": "information gain (IG)", "start_pos": 24, "end_pos": 45, "type": "METRIC", "confidence": 0.8473387360572815}, {"text": "chi-square test (CHI)", "start_pos": 50, "end_pos": 71, "type": "METRIC", "confidence": 0.8216019570827484}, {"text": "aggressive term removal", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.6412487129370371}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9261446595191956}]}, {"text": "Another major characteristic of text categorization problems is the high level of feature redundancy.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7829981446266174}]}, {"text": "While there are generally many different features relevant to classification task, often several such cues occur in one document.", "labels": [], "entities": [{"text": "classification task", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.915094405412674}]}, {"text": "These cues are partly redundant.", "labels": [], "entities": []}, {"text": "Na\u00efve Bayes, which is a popular learning algorithm, is commonly justified using assumptions of conditional independence or linked dependence.", "labels": [], "entities": []}, {"text": "However, theses assumptions are generally accepted to be false for text.", "labels": [], "entities": []}, {"text": "To remove these violations, more complex dependence models have been developed.", "labels": [], "entities": []}, {"text": "Most previous works of feature selection emphasized only the reduction of high dimensionality of the feature space[4].", "labels": [], "entities": [{"text": "feature selection", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.882815808057785}]}, {"text": "The most popular feature selection method is IG.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.6649182587862015}]}, {"text": "IG works well with texts and has often been used.", "labels": [], "entities": []}, {"text": "IG looks at each feature in isolation and measures how important it is for the prediction of the correct class label.", "labels": [], "entities": []}, {"text": "In cases where all features are not redundant with each other, IG is very appropriate.", "labels": [], "entities": [{"text": "IG", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.6878917813301086}]}, {"text": "But in cases where many features are highly redundant with each other, we must utilize other means, for example, more complex dependence models.", "labels": [], "entities": []}, {"text": "In this paper, for the high dimensionality of the feature space and the high level of feature redundancy, we propose anew feature selection method which selects each feature according to a combined criterion of information gain and novelty of information.", "labels": [], "entities": []}, {"text": "The latter measures the degree of dissimilarity between the feature being considered and previously selected features.", "labels": [], "entities": []}, {"text": "Maximal Marginal Relevance (MMR) provides precisely such functionality.", "labels": [], "entities": [{"text": "Maximal Marginal Relevance (MMR)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6976622541745504}]}, {"text": "So we propose MMRbased feature selection method which strives to reduce redundancy between features while maintaining information gain in selecting appropriate features for text categorization.", "labels": [], "entities": [{"text": "MMRbased feature selection", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.6998803019523621}]}, {"text": "In machine learning field, some greedy methods that add or subtract a single feature at a time have been developed for feature selection[14].", "labels": [], "entities": [{"text": "feature selection", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.6871970444917679}]}, {"text": "S. Della Pietra et al. proposed a method for incrementally constructing random field.", "labels": [], "entities": []}, {"text": "Their method builds increasingly complex fields to approximate the empirical distribution of a set of training examples by allowing features.", "labels": [], "entities": []}, {"text": "Features are incrementally added to the field using a topdown greedy algorithm, with the intent of capturing the salient properties of the empirical sample while allowing generalization to new configurations.", "labels": [], "entities": []}, {"text": "However the method is not simple, and this is problematic both computationally and statistically in large-scale problems.", "labels": [], "entities": []}, {"text": "Koller and Sahami proposed another greedy feature selection method which provides a mechanism for eliminating features whose predictive information with respect to the class is subsumed by the other features.", "labels": [], "entities": []}, {"text": "This method is also based on the Kullback-Leibler divergence to minimize the amount of predictive information lost during feature elimination.", "labels": [], "entities": [{"text": "feature elimination", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7422308623790741}]}, {"text": "In order to compare the performances of our method and greedy feature selection methods, we implemented Koller and Sahami's method, and empirically tested it in section 4.", "labels": [], "entities": []}, {"text": "We also compared the performance of conventional machine learning algorithms using our feature selection method with Support Vector Machine (SVM) using all features in section 4.", "labels": [], "entities": []}, {"text": "Previous works show that SVM consistently achieves good performance on text categorization tasks, outperforming existing methods substantially and significantly.", "labels": [], "entities": [{"text": "SVM", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9410895705223083}, {"text": "text categorization tasks", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.8104024132092794}]}, {"text": "With its ability to generalize well in high dimensional feature spaces and high level of feature redundancy, SVM is known that it does not need any feature selection.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe the Maximal Marginal Relevance, and in section 3, we describe the MMR-based feature selection.", "labels": [], "entities": [{"text": "Maximal Marginal Relevance", "start_pos": 30, "end_pos": 56, "type": "METRIC", "confidence": 0.664298951625824}, {"text": "MMR-based feature selection", "start_pos": 92, "end_pos": 119, "type": "TASK", "confidence": 0.9076830943425497}]}, {"text": "Section 4 presents the in-depth experiments and the results.", "labels": [], "entities": []}, {"text": "Section 5 concludes the research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to compare the performance of MMR-based feature selection method with conventional IG and greedy feature selection methods method, labeled 'Greedy'), we evaluated the three feature selection methods with four different learning algorithms: naive Bayes, TFIDF/Rocchio, Probabilistic Indexing (PrTFIDF) and Maximum Entropy using Rainbow.", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.823189357916514}]}, {"text": "We also compared the performance of conventional machine learning algorithms using our feature selection method and SVM using all features.", "labels": [], "entities": []}, {"text": "MMR-based feature selection and greedy feature selection method requires quadratic time with respect to the number of features.", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8001562158266703}]}, {"text": "To reduce this complexity, for each data set, we first selected 1000 features using IG, and then we applied MMR-based feature selection and greedy feature selection method to the selected 1000 features.", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.7028155128161112}]}, {"text": "For all datasets, we did not remove stopwords.", "labels": [], "entities": []}, {"text": "The results reported on all dataset are averaged over 10 times of different test/training splits.", "labels": [], "entities": []}, {"text": "A random subset of 20% of the data considered in an experiment was used for testing (i.e. we used Rainbow's '--test-set=0.2' and '--test=10' options), because Rainbow does not support 10-fold cross validation.", "labels": [], "entities": []}, {"text": "MMR-based feature selection method needs to tune for \u03bb . It appears that a tuning method based on held-out data is needed here.", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7016011079152426}]}, {"text": "We tested our method using 11 \u03bb values (i.e. 0, 0.1, 0.2, \u2026, 1) and selected the best \u03bb value.", "labels": [], "entities": []}, {"text": "Figure 1 displays the performance curves for four different machine learning algorithms on the subset of Reuters after term selection using MMR-based feature selection (number of features is 25).", "labels": [], "entities": [{"text": "Reuters", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.607961893081665}, {"text": "MMR-based feature selection", "start_pos": 140, "end_pos": 167, "type": "TASK", "confidence": 0.5413464506467184}]}, {"text": "When the parameter \u03bb =0.5, most machine learning algorithms have best performance and significant improvements compared to conventional information gain (i.e. \u03bb =1) and SVM using all features..", "labels": [], "entities": []}, {"text": "shows the performance of four machine learning algorithms on WebKB using three feature selection methods and all features (41763 terms).", "labels": [], "entities": [{"text": "WebKB", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.9666163325309753}]}, {"text": "In this data set, again MMR-based feature selection has best performance and significant improvements compared to greedy method and IG.", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.9073698918024699}]}, {"text": "Using MMR-based feature selection, for example, the vocabulary is reduced from 41763 terms to 200 (a 99.5% reduction), and the accuracy is improved from 85.26% to 90.49% in Na\u00efve Bayes.", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.7064006725947062}, {"text": "vocabulary", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9230634570121765}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9995309114456177}]}, {"text": "Using greedy method and IG, however, the accuracy is improved from 85.26% to about 87% in Na\u00efve Bayes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9997085928916931}]}, {"text": "PrTFIDF is most sensitive to feature selection method.", "labels": [], "entities": []}, {"text": "Using MMR-based feature selection the best accuracy is 82.47%.", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.8216348886489868}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9996681213378906}]}, {"text": "Using greedy method and IG, however, the best accuracy is only 72~74%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9994317889213562}]}, {"text": "In this dataset, however, MMR-based feature selection does not produce improvements of conventional machine learning algorithms over SVM.", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.9075448513031006}]}, {"text": "The observation in Reuters and WebKB are highly consistent.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9542743563652039}, {"text": "WebKB", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.9573468565940857}]}, {"text": "MMR-based feature selection is consistently more effective than greedy method and IG on two data sets, and sometimes produces improvements even over the best SVM.", "labels": [], "entities": [{"text": "MMR-based feature selection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8619876503944397}]}], "tableCaptions": []}