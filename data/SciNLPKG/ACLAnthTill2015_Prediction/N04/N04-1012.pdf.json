{"title": [{"text": "Ensemble-based Active Learning for Parse Selection", "labels": [], "entities": [{"text": "Parse Selection", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.860962450504303}]}], "abstractContent": [{"text": "Supervised estimation methods are widely seen as being superior to semi and fully unsuper-vised methods.", "labels": [], "entities": []}, {"text": "However, supervised methods crucially rely upon training sets that need to be manually annotated.", "labels": [], "entities": []}, {"text": "This can be very expensive , especially when skilled annotators are required.", "labels": [], "entities": []}, {"text": "Active learning (AL) promises to help reduce this annotation cost.", "labels": [], "entities": []}, {"text": "Within the complex domain of HPSG parse selection, we show that ideas from ensemble learning can help further reduce the cost of annotation.", "labels": [], "entities": [{"text": "HPSG parse selection", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.7851631045341492}]}, {"text": "Our main results show that at times, an ensemble model trained with randomly sampled examples can outperform a single model trained using AL.", "labels": [], "entities": []}, {"text": "However, converting the single-model AL method into an ensemble-based AL method shows that even this much stronger baseline model can be improved upon.", "labels": [], "entities": []}, {"text": "Our best results show a \u00a2 \u00a4 \u00a3 \u00a6 \u00a5 reduction in annotation cost compared with single-model random sampling.", "labels": [], "entities": []}], "introductionContent": [{"text": "Active learning (AL) methods, such as uncertainty sampling ( or query by committee (, can dramatically reduce the cost of creating an annotated dataset.", "labels": [], "entities": [{"text": "Active learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6775300025939941}]}, {"text": "In particular, they enable rapid creation of labeled datasets which can then be used for trainable speech and language technologies.", "labels": [], "entities": []}, {"text": "Progress in AL will therefore translate into even greater savings in annotation costs and hence faster creation of speech and language systems.", "labels": [], "entities": [{"text": "AL", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9111248254776001}]}, {"text": "In this paper, we: \u00a7 Present a novel way of improving uncertainty sampling by generalizing it from using a single model to using an ensemble model.", "labels": [], "entities": [{"text": "uncertainty sampling", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7178235203027725}]}, {"text": "This generalization easily outperforms single-model uncertainty sampling.", "labels": [], "entities": []}, {"text": "\u00a7 Introduce anew, extremely simple AL method (called lowest best probability selection) which is competitive with uncertainty sampling and can also be improved using ensemble techniques.", "labels": [], "entities": []}, {"text": "\u00a7 Show that an ensemble of models trained using randomly sampled examples can outperform a single model trained using (single model) AL methods.", "labels": [], "entities": []}, {"text": "\u00a7 Demonstrate further reductions in annotation cost when we train the ensemble parse selection model using examples selected by an ensemble-based active learner.", "labels": [], "entities": [{"text": "ensemble parse selection", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.7124276459217072}]}, {"text": "This result shows that ensemble learning can improve both the underlying model and also the way we select examples for it.", "labels": [], "entities": []}, {"text": "Our domain is parse selection for Head-Driven Phrase Structure Grammar (HPSG).", "labels": [], "entities": [{"text": "parse selection", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.8257251977920532}, {"text": "Head-Driven Phrase Structure Grammar (HPSG)", "start_pos": 34, "end_pos": 77, "type": "TASK", "confidence": 0.7256926468440464}]}, {"text": "Although annotated corpora exist for HPSG, such corpora do not exist in significant volumes and are limited to a few small domains ().", "labels": [], "entities": [{"text": "HPSG", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.8230948448181152}]}, {"text": "Even if it were possible to bootstrap from the Penn Treebank, it is still unlikely that there would be sufficient quantities of high quality material necessary to improve parse selection for detailed linguistic formalisms such as HPSG.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9966623187065125}, {"text": "parse selection", "start_pos": 171, "end_pos": 186, "type": "TASK", "confidence": 0.9141779243946075}, {"text": "HPSG", "start_pos": 230, "end_pos": 234, "type": "DATASET", "confidence": 0.8827170729637146}]}, {"text": "There is thus a pressing need to efficiently create significant volumes of annotated material.", "labels": [], "entities": []}, {"text": "AL applied to parse selection is much more challenging than applying it to simpler tasks such as text classification or part-of-speech tagging.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.9838503301143646}, {"text": "text classification", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.8183379173278809}, {"text": "part-of-speech tagging", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.6861214339733124}]}, {"text": "Our labels are complex objects rather than discrete values drawn from a small, fixed set.", "labels": [], "entities": []}, {"text": "Furthermore, the fact that sentences are of variable length and have variable numbers of parses potentially adds to the complexity of the task.", "labels": [], "entities": []}, {"text": "Our results specific to parse selection show that: ric based upon efficiently selecting the correct parse from a set of possible parses, we are able to show that some AL methods are more effective than others, even though they perform similarly when making the unit cost per example assumption.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.964209645986557}]}, {"text": "\u00a7 Ad-hoc selection methods based upon superficial characteristics of the data, such as sentence length or ambiguity rate, are typically worse than random sampling.", "labels": [], "entities": [{"text": "Ad-hoc selection", "start_pos": 2, "end_pos": 18, "type": "TASK", "confidence": 0.752965897321701}]}, {"text": "This motivates using AL methods.", "labels": [], "entities": []}, {"text": "\u00a7 Labeling sentences in the order they appear in the corpus -as is typically done in annotation -performs much worse than using random selection.", "labels": [], "entities": [{"text": "Labeling sentences", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.89271080493927}]}, {"text": "Throughout this paper, we shall treat the terms sentences and examples as interchangeable; we shall also consider parses and labels as equivalent.", "labels": [], "entities": []}, {"text": "Also, we shall use the term method whenever we are talking about AL, and model whenever we are talking about parse selection.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.8721439242362976}]}], "datasetContent": [{"text": "To test the effectiveness of the various AL strategies discussed in the previous section, we perform simulation studies of annotating version 3 of Redwoods.", "labels": [], "entities": [{"text": "Redwoods", "start_pos": 147, "end_pos": 155, "type": "DATASET", "confidence": 0.719559907913208}]}, {"text": "For all experiments, we used a tenfold cross-validation strategy by randomly selecting sentence subset according to the method until the annotated training material made available to the learners contains at least 6 examples and \u00a2 discriminants.", "labels": [], "entities": []}, {"text": "We select examples for manual annotation at each round, and exclude all examples that have more than 500 parses.", "labels": [], "entities": []}, {"text": "Other parameter settings did not produce substantially different results to those reported here.", "labels": [], "entities": []}, {"text": "AL results are usually presented in terms of the amount of labeling necessary to achieve given performance levels.", "labels": [], "entities": []}, {"text": "We say that one method is better than another method if, fora given performance level, less annotation is required.", "labels": [], "entities": []}, {"text": "The performance metric used here is parse selection accuracy as described in section 2.4.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.9259182512760162}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.8957647085189819}]}], "tableCaptions": [{"text": " Table 2: Discriminant costs required for selection methods to reach 70%, 75%, and 77% accuracy. The reduction  columns give the percentage reduction in cost compared to LL-CONFIG and LL-PROD using random sampling.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.99893718957901}]}]}