{"title": [{"text": "Detecting Structural Metadata with Decision Trees and Transformation-Based Learning", "labels": [], "entities": [{"text": "Detecting Structural Metadata", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8795368671417236}]}], "abstractContent": [{"text": "The regular occurrence of disfluencies is a distinguishing characteristic of spontaneous speech.", "labels": [], "entities": []}, {"text": "Detecting and removing such disflu-encies can substantially improve the usefulness of spontaneous speech transcripts.", "labels": [], "entities": []}, {"text": "This paper presents a system that detects various types of disfluencies and other structural information with cues obtained from lexical and prosodic information sources.", "labels": [], "entities": []}, {"text": "Specifically, combinations of decision trees and language models are used to predict sentence ends and interruption points and, given these events, transformation-based learning is used to detect edit disfluen-cies and conversational fillers.", "labels": [], "entities": []}, {"text": "Results are reported on human and automatic transcripts of conversational telephone speech.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic speech-to-text (STT) transcripts of spontaneous speech are often difficult to comprehend even without the challenges arising from word recognition errors introduced by imperfect STT systems (.", "labels": [], "entities": [{"text": "Automatic speech-to-text (STT) transcripts of spontaneous speech", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.7029351658291287}]}, {"text": "Such transcripts lack punctuation that indicates clausal or sentential boundaries, and they contain a number of disfluencies that would not normally occur in written language.", "labels": [], "entities": []}, {"text": "Repeated words, hesitations such as \"um\" and \"uh\", and corrections to a sentence in mid-stream area normal part of conversational speech.", "labels": [], "entities": []}, {"text": "These disfluencies are handled easily by human listeners, but their existence makes transcripts of spontaneous speech ill-suited for most natural language processing (NLP) systems developed for text, such as parsers or information extraction systems.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 219, "end_pos": 241, "type": "TASK", "confidence": 0.6748719364404678}]}, {"text": "Similarly, the lack of meaningful segmentation in automatically generated speech transcripts makes them problematic to use in NLP systems, most of which are designed to work at the sentence level.", "labels": [], "entities": []}, {"text": "Detecting and removing disfluencies and locating sentential unit boundaries in spontaneous speech transcripts can improve their readability and make them more suitable for NLP.", "labels": [], "entities": [{"text": "NLP", "start_pos": 172, "end_pos": 175, "type": "TASK", "confidence": 0.9167384505271912}]}, {"text": "Automatically annotating discourse markers and other conversational fillers is also likely to be useful, since proper handling is needed to follow the flow of conversation.", "labels": [], "entities": []}, {"text": "Hence, the overall goal of our work is to detect such structural information in conversational speech using features generated by currently available speech processing systems and statistical machine learning tools.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the types of metadata that this work addresses, followed by a discussion of related prior work in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 describes the system architecture and details the algorithms and features used by our system.", "labels": [], "entities": []}, {"text": "Section 5 discusses the experimental paradigm and results.", "labels": [], "entities": []}, {"text": "Finally we provide a summary and directions for future work in Section 6.: Filled pauses and discourse markers to be detected by our system.", "labels": [], "entities": []}], "datasetContent": [{"text": "For training our system and its components, we used two different subsets of Switchboard, a corpus of conversational telephone speech (CTS) (.", "labels": [], "entities": []}, {"text": "One of the data sets included 417 conversations (LDC1.3) that were hand-annotated by the Linguistic Data Consortium for disfluencies and SUs according to the V5 guidelines detailed in.", "labels": [], "entities": [{"text": "V5", "start_pos": 158, "end_pos": 160, "type": "DATASET", "confidence": 0.9122052788734436}]}, {"text": "Another set of 1086 conversations from the Switchboard corpus was annotated according to and is available as part of the Treebank3 corpus (TB3).", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.911195307970047}, {"text": "Treebank3 corpus (TB3)", "start_pos": 121, "end_pos": 143, "type": "DATASET", "confidence": 0.9598849415779114}]}, {"text": "We used aversion of this set that contained annotations machine-mapped to approximate the V5 annotation specification.", "labels": [], "entities": []}, {"text": "For development and testing of our system, we used hand transcripts and STT system output for 72 conversations from Switchboard and the Fisher corpus, a recent CTS data collection.", "labels": [], "entities": [{"text": "Switchboard and the Fisher corpus", "start_pos": 116, "end_pos": 149, "type": "DATASET", "confidence": 0.6654438674449921}, {"text": "CTS data collection", "start_pos": 160, "end_pos": 179, "type": "DATASET", "confidence": 0.8322970668474833}]}, {"text": "Half of these conversations were held out and used as development data (dev set), and the other 36 conversations were used as test data (eval set).", "labels": [], "entities": []}, {"text": "The STT output, used only in testing, was from a state-ofthe-art large vocabulary conversational speech recognizer developed by BBN.", "labels": [], "entities": [{"text": "STT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7681133151054382}, {"text": "large vocabulary conversational speech recognizer", "start_pos": 65, "end_pos": 114, "type": "TASK", "confidence": 0.6676477789878845}, {"text": "BBN", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.9849134087562561}]}, {"text": "The word error rates for the STT output were 27% on the dev set and 25% on the eval set.", "labels": [], "entities": []}, {"text": "To assess the performance of our overall system, disfluencies and boundary events were predicted and then evaluated by the scoring tools developed for the NIST Rich Transcript evaluation task.", "labels": [], "entities": [{"text": "NIST Rich Transcript evaluation task", "start_pos": 155, "end_pos": 191, "type": "DATASET", "confidence": 0.8864455819129944}]}], "tableCaptions": [{"text": " Table 4: Impact of different features on boundary event prediction using the joint tree model on reference transcripts.", "labels": [], "entities": [{"text": "boundary event prediction", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.7316963275273641}]}, {"text": " Table 5: Detection of boundary events and disfluencies  on STT output as scored by rt-eval.", "labels": [], "entities": [{"text": "Detection", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.7533895969390869}, {"text": "STT", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.7301288843154907}]}, {"text": " Table 6: Percentage of missed IPs on the dev set.", "labels": [], "entities": [{"text": "Percentage of missed IPs", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.8730737417936325}]}]}