{"title": [], "abstractContent": [{"text": "Summarizing threads of email is different from summarizing other types of written communication as it has an inherent dialog structure.", "labels": [], "entities": []}, {"text": "We present initial research which shows that sentence extraction techniques can work for email threads as well, but profit from email-specific features.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8166812658309937}]}, {"text": "In addition, the presentation of the summary should take into account the dialogic structure of email communication.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we discuss work on summarizing email threads, i.e., coherent exchanges of email messages among several participants.", "labels": [], "entities": [{"text": "summarizing email threads", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.8606993556022644}]}, {"text": "Email is a written medium of asynchronous multi-party communication.", "labels": [], "entities": []}, {"text": "This means that, unlike for example news stories but as in face-to-face spoken dialog, the email thread as a whole is a collaborative effort with interaction among the discourse participants.", "labels": [], "entities": []}, {"text": "However, unlike spoken dialog, the discourse participants are not physically co-present, so that the written word is the only channel of communication.", "labels": [], "entities": []}, {"text": "Furthermore, replies do not happen immediately, so that responses need to take special precautions to identify relevant elements of the discourse context (for example, by citing previous messages).", "labels": [], "entities": []}, {"text": "Thus, email is a distinct linguistic genre that poses its own challenges to summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.9873627424240112}]}, {"text": "In the approach we propose in this paper, we follow the paradigm used for other genres of summarization, namely sentence extraction: important sentences are extracted from the thread and are composed into a summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.9745235443115234}, {"text": "sentence extraction", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7445451319217682}]}, {"text": "Given the special characteristics of email, we predict that certain email-specific features can help in identifying relevant sentences for extraction.", "labels": [], "entities": []}, {"text": "In addition, in presenting the extracted summary, special \"wrappers\" ensure that the reader can reconstruct the interactional aspect of the thread, which we assume is crucial for understanding the summary.", "labels": [], "entities": []}, {"text": "We acknowledge that other techniques should also be explored for email summarization, but leave that to separate work.", "labels": [], "entities": [{"text": "email summarization", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7125081717967987}]}, {"text": "describe work on summarizing individual email messages using machine learning approaches to learn rules for salient noun phrase extraction.", "labels": [], "entities": [{"text": "summarizing individual email messages", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.903153657913208}, {"text": "noun phrase extraction", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.6231736143430074}]}, {"text": "In contrast, our work aims at summarizing whole threads and at capturing the interactive nature of email.", "labels": [], "entities": [{"text": "summarizing whole threads", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.8996259570121765}]}, {"text": "present work on generating extractive summaries of threads in archived discussions.", "labels": [], "entities": []}, {"text": "A sentence from the root message and from each response to the root extracted using ad-hoc algorithms crafted by hand.", "labels": [], "entities": []}, {"text": "This approach works best when the subject of the root email best describes the \"issue\" of the thread, and when the root email does not discuss more than one issue.", "labels": [], "entities": []}, {"text": "In our work, we do not make any assumptions about the nature of the email, and learn sentence extraction strategies using machine learning.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7165401577949524}]}, {"text": "also address the problem of summarizing archived discussion lists.", "labels": [], "entities": [{"text": "summarizing archived discussion lists", "start_pos": 28, "end_pos": 65, "type": "TASK", "confidence": 0.9169229418039322}]}, {"text": "They cluster messages into topic groups, and then extract summaries for each cluster.", "labels": [], "entities": []}, {"text": "The summary of a cluster is extracted using a scoring metric based on sentence position, lexical similarity of a sentence to cluster centroid, and a feature based on quotation, among others.", "labels": [], "entities": []}, {"text": "While the approach is quite different from ours (due to the underlying clustering algorithm and the absence of machine learning to select features), the use of email-specific features, in particular the feature related to quoted material, is similar.", "labels": [], "entities": []}, {"text": "present work on email summarization by exploiting the thread structure of email conversation and common features such as named entities and dates.", "labels": [], "entities": [{"text": "email summarization", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.6793303489685059}]}, {"text": "They summarize the message only, though the content of the message to be summarized is \"expanded\" using the content from its ancestor messages.", "labels": [], "entities": []}, {"text": "The expanded message is passed to a document summarizer which is used as a black box to generate summaries.", "labels": [], "entities": []}, {"text": "Our work, in contrast, aims at summarizing the whole thread, and we are precisely interested in changing the summarization algorithm itself, not in using a black box summarizer.", "labels": [], "entities": [{"text": "summarizing", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9829272627830505}, {"text": "summarization", "start_pos": 109, "end_pos": 122, "type": "TASK", "confidence": 0.9613585472106934}]}], "datasetContent": [{"text": "This section describes experiments using the machine learning program to automatically induce sentence classifiers, using the features described in Section 4.", "labels": [], "entities": []}, {"text": "Like many learning programs, Ripper takes as input the classes to be learned, a set of feature names and possible values, and training data specifying the class and feature values for each training example.", "labels": [], "entities": []}, {"text": "In our case, the training examples are the sentences from the threads as described in Section 3.", "labels": [], "entities": []}, {"text": "Ripper outputs a classification model for predicting the class (i.e., whether a sentence should be in a summary or not) of future examples; the model is expressed as an ordered set of if-then rules.", "labels": [], "entities": []}, {"text": "We obtained the results presented here using fivefold cross-validation.", "labels": [], "entities": []}, {"text": "In this paper, we only evaluate the results of the machine learning step; we acknowledge the need for an evaluation of the resulting summaries using: Results for combining two annotators (last two columns) using full feature set word/string based similarity metric and/or human judgments and leave that to future publications.", "labels": [], "entities": []}, {"text": "We show results for the two annotators and different feature sets in.", "labels": [], "entities": []}, {"text": "First consider the results for annotator DB.", "labels": [], "entities": [{"text": "annotator DB", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.3391217142343521}]}, {"text": "Recall that basic includes only standard features that can be used for all text genres, and considers the thread a single text.", "labels": [], "entities": []}, {"text": "basic+ takes the breakdown of the thread into messages into account.", "labels": [], "entities": []}, {"text": "full also uses features that are specific to email threads.", "labels": [], "entities": []}, {"text": "We can see that by using more features than the baseline set basic, performance improves.", "labels": [], "entities": []}, {"text": "Specifically, using email-specific features improves the performance over the basic baseline, as we expected.", "labels": [], "entities": []}, {"text": "We also give a second baseline, ctroid, which we determined by choosing the top 20% of sentences most similar to the thread centroid.", "labels": [], "entities": []}, {"text": "All results using Ripper improve on this baseline.", "labels": [], "entities": []}, {"text": "If we perform exactly the same experiments on the summaries written by annotator GR, we obtain the results shown in the bottom half of.", "labels": [], "entities": []}, {"text": "The results are much worse, and the centroid-based baseline outperforms all but the full feature set.", "labels": [], "entities": []}, {"text": "We leave to further research an explanation of why this maybe the case; we speculate that GR,as an annotator, is less consistent in her choice of material than is DB when forming a summary.", "labels": [], "entities": [{"text": "DB", "start_pos": 163, "end_pos": 165, "type": "METRIC", "confidence": 0.8775349855422974}]}, {"text": "Thus, the machine learner has less regularity to learn from.", "labels": [], "entities": []}, {"text": "However, we take this difference as evidence for the claim that one should not expect great regularity in human-written summaries.", "labels": [], "entities": []}, {"text": "Finally, we investigated what happens when we combine the data from both sources, DB and GR.", "labels": [], "entities": [{"text": "DB", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.829725444316864}]}, {"text": "Using SimFinder, we obtained two scores for each sentence, one that shows the similarity to the most similar sentence in DB's summary, and one that shows the similarity to the most similar sentence in GR's summary.", "labels": [], "entities": []}, {"text": "We can combine these two scores and then use the combined score in the same way that we used the score from a single annotator.", "labels": [], "entities": []}, {"text": "We explore two ways of combining the scores: the average, and the maximum.", "labels": [], "entities": []}, {"text": "Both ways of combining the scores result in worse scores than either annotator on his or her own; the average is worse than the maximum (see).", "labels": [], "entities": []}, {"text": "We interpret these results again as meaning that there is little convergence in the human-written summaries, and it maybe advantageous to learn from one particular annotator.", "labels": [], "entities": []}, {"text": "(Of course, another option might be to develop and enforce very precise guidelines for the an-  Regarding \"acm home/bjarney\", on Apr 9, 2001, Muriel Danslop wrote: Two things: Can someone be responsible for the press releases for Stroustrup?", "labels": [], "entities": []}, {"text": "Responding to this on Apr 10, 2001, Theresa Feng wrote: I think Phil, who is probably a better writer than most of us, is writing up something for dang and Dave to send out to various ACM chapters.", "labels": [], "entities": []}, {"text": "Phil, we can just use that as our \"press release\", right?", "labels": [], "entities": []}, {"text": "In another subthread, on Apr 12, 2001, Kevin Danquoit wrote: Are you sending out upcoming events for this week?: Sample summary obtained with the rule set in notators as to the contents of the summaries.)", "labels": [], "entities": []}, {"text": "A sample rule set obtained from DB data is shown in.", "labels": [], "entities": [{"text": "DB data", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.8263250291347504}]}, {"text": "Some rules are intuitively appealing: for example, rule 1 states that questions at the beginning of a thread that are similar to entire thread should be retained, and rule 2 states that sentence which are very similar to the thread and which have a high number of recipients should be retained.", "labels": [], "entities": []}, {"text": "However, some rules show signs of overfitting, for example rule 1 limits the average TF-IDF values to a rather narrow band.", "labels": [], "entities": []}, {"text": "Hopefully, more data will alleviate the overfitting problem.", "labels": [], "entities": []}, {"text": "(The data collection continues.)", "labels": [], "entities": []}], "tableCaptions": []}