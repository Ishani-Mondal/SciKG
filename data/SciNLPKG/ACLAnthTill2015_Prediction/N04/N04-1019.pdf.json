{"title": [{"text": "Evaluating Content Selection in Summarization: The Pyramid Method", "labels": [], "entities": [{"text": "Evaluating Content Selection in Summarization", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7205899238586426}]}], "abstractContent": [{"text": "We present an empirically grounded method for evaluating content selection in summariza-tion.", "labels": [], "entities": [{"text": "content selection", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7263116538524628}]}, {"text": "It incorporates the idea that no single best model summary fora collection of documents exists.", "labels": [], "entities": []}, {"text": "Our method quantifies the relative importance of facts to be conveyed.", "labels": [], "entities": []}, {"text": "We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.", "labels": [], "entities": [{"text": "Document Understanding Conference", "start_pos": 159, "end_pos": 192, "type": "TASK", "confidence": 0.793453315893809}]}], "introductionContent": [{"text": "Evaluating content selection in summarization has proven to be a difficult problem.", "labels": [], "entities": [{"text": "summarization", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9721438884735107}]}, {"text": "Our approach acknowledges the fact that no single best model summary exists, and takes this as a foundation rather than an obstacle.", "labels": [], "entities": []}, {"text": "In machine translation, the rankings from the automatic BLEU method () have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7272708714008331}, {"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9744544625282288}, {"text": "summarization", "start_pos": 192, "end_pos": 205, "type": "TASK", "confidence": 0.9894238114356995}]}, {"text": "To show that an automatic method is a reasonable approximation of human judgments, one needs to demonstrate that these can be reliably elicited.", "labels": [], "entities": []}, {"text": "However, in contrast to translation, where the evaluation criterion can be defined fairly precisely it is difficult to elicit stable human judgments for summarization) (.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9737478494644165}, {"text": "summarization", "start_pos": 153, "end_pos": 166, "type": "TASK", "confidence": 0.981134295463562}]}, {"text": "Our approach tailors the evaluation to observed distributions of content over a pool of human summaries, rather than to human judgments of summaries.", "labels": [], "entities": []}, {"text": "Our method involves semantic matching of content units to which differential weights are assigned based on their frequency in a corpus of summaries.", "labels": [], "entities": []}, {"text": "This can lead to more stable, more informative scores, and hence to a meaningful content evaluation.", "labels": [], "entities": []}, {"text": "We create a weighted inventory of Summary Content Units-a pyramid-that is reliable, predictive and diagnostic, and which constitutes a resource for investigating alternate realizations of the same meaning.", "labels": [], "entities": []}, {"text": "No other evaluation method predicts sets of equally informative summaries, identifies semantic differences between more and less highly ranked summaries, or constitutes a tool that can be applied directly to further analysis of content selection.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the DUC method.", "labels": [], "entities": []}, {"text": "In Section 3 we present an overview of our method, contrast our scores with other methods, and describe the distribution of scores as pyramids grow in size.", "labels": [], "entities": []}, {"text": "We compare our approach with previous work in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we present our conclusions and point to our next step, the feasibility of automating our method.", "labels": [], "entities": []}, {"text": "A more detailed account of the work described here, but not including the study of distributional properties of pyramid scores, can be found in ().", "labels": [], "entities": []}], "datasetContent": [{"text": "The procedure used for evaluating summaries in DUC is the following: 1.", "labels": [], "entities": [{"text": "summaries in DUC", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.6829685966173807}]}, {"text": "A human subject reads the entire input set and creates a 100 word summary for it, called a model.", "labels": [], "entities": []}, {"text": "2. The model summary is split into content units, roughly equal to clauses or elementary discourse units (EDUs).", "labels": [], "entities": []}, {"text": "This step is performed automatically using a tool for EDU annotation developed at ISI.", "labels": [], "entities": [{"text": "ISI", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.8392516374588013}]}, {"text": "The summary to be evaluated (a peer) is automatically split into sentences.", "labels": [], "entities": []}, {"text": "(Thus the content units are of different granularity-EDUs for the model, and sentences for the peer).", "labels": [], "entities": []}, {"text": "4. Then a human judge evaluates the peer against the model using the following instructions: For each model content unit: (a) Find all peer units that express at least some facts from the model unit and mark them.", "labels": [], "entities": []}, {"text": "(b) After all such peer units are marked, think about the whole set of marked peer units and answer the question: (c) \"The marked peer units, taken together, express about k% of the meaning expressed by the current model unit\", where k can be equal to 0, 20, 40, 60, 80 and 100.", "labels": [], "entities": []}, {"text": "The final score is based on the content unit coverage.", "labels": [], "entities": []}, {"text": "In the official DUC results tables, the score for the entire summary is the average of the scores of all the content model units, thus a number between 0 and 1.", "labels": [], "entities": [{"text": "DUC results tables", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.9556014140446981}]}, {"text": "Some participants use slightly modified versions of the coverage metric, where the proportion of marked peer units to the number of model units is factored in.", "labels": [], "entities": []}, {"text": "The selection of units with the same content is facilitated by the use of the Summary Evaluation Environment (SEE) 2 developed at ISI, which displays the model and peer summary side by side and allows the user to make selections by using a mouse.", "labels": [], "entities": []}, {"text": "There are numerous problems with the DUC human evaluation method.", "labels": [], "entities": [{"text": "DUC human evaluation", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.6950761874516805}]}, {"text": "The use of a single model summary is one of the surprises -all research in summarization evaluation has indicated that no single good model exists.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.9440518915653229}]}, {"text": "Also, since not much agreement is expected between two summaries, many model units will have no counterpart in the peer and thus the expected scores will necessarily be rather low.", "labels": [], "entities": []}, {"text": "Additionally, the task of determining the percentage overlap between two text units turns out to be difficult to annotate reliably -() report that humans agreed with their own prior judgment in only 82% of the cases.", "labels": [], "entities": []}, {"text": "These methodological anomalies lead to unreliable scores.", "labels": [], "entities": []}, {"text": "Human-written summaries can score as low as 0.1 while machine summaries can score as high as 0.5.", "labels": [], "entities": []}, {"text": "For each of the 30 test sets, three of the four humanwritten summaries and the machine summaries were scored against the fourth human model summary: each human was scored on ten summaries.", "labels": [], "entities": []}, {"text": "shows a scatterplot of human scores for all 30 sets, and illustrates an apparently random relation of summarizers to each other, and to document sets.", "labels": [], "entities": []}, {"text": "This suggests that the DUC scores cannot be used to distinguish a good human summarizer from a bad one.", "labels": [], "entities": [{"text": "DUC", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.6927558779716492}]}, {"text": "In addition, the DUC method is not powerful enough to distinguish between systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pyramid scores across annotations.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of DUC and Pyramid scores; capital  letters represent distinct human summarizers.", "labels": [], "entities": [{"text": "DUC", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.5310853123664856}, {"text": "Pyramid scores", "start_pos": 32, "end_pos": 46, "type": "METRIC", "confidence": 0.723880261182785}]}, {"text": " Table 3: Probabilities of errors E1, E2, E3 and total prob- ability of error", "labels": [], "entities": [{"text": "Probabilities of errors E1", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.8330774903297424}, {"text": "total prob- ability of error", "start_pos": 49, "end_pos": 77, "type": "METRIC", "confidence": 0.8532626330852509}]}, {"text": " Table 4. Again we can see that in order to have a ranking  of the summaries that is reasonably close to the rankings  produces by a pyramid of order n = 9, 4 or more sum- maries should be used.", "labels": [], "entities": []}, {"text": " Table 4: Spearman correlation coefficient average for  pyramids of order n \u2264 5", "labels": [], "entities": [{"text": "Spearman correlation coefficient average", "start_pos": 10, "end_pos": 50, "type": "METRIC", "confidence": 0.8339297920465469}]}, {"text": " Table 5: Spearman correlation coefficient average for un- igram overlap score assignment", "labels": [], "entities": [{"text": "Spearman correlation coefficient average", "start_pos": 10, "end_pos": 50, "type": "METRIC", "confidence": 0.7821782231330872}]}]}