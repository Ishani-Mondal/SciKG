{"title": [{"text": "Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization", "labels": [], "entities": [{"text": "Catching the Drift", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7790059049924215}, {"text": "Generation and Summarization", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.8006044824918112}]}], "abstractContent": [{"text": "We consider the problem of modeling the content structure of texts within a specific domain , in terms of the topics the texts address and the order in which these topics appear.", "labels": [], "entities": []}, {"text": "We first present an effective knowledge-lean method for learning content models from un-annotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.", "labels": [], "entities": []}, {"text": "We then apply our method to two complementary tasks: information ordering and ex-tractive summarization.", "labels": [], "entities": [{"text": "information ordering", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.8850879073143005}]}, {"text": "Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "The development and application of computational models of text structure is a central concern in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.6486101349194845}]}, {"text": "Document-level analysis of text structure is an important instance of such work.", "labels": [], "entities": [{"text": "Document-level analysis of text structure", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8824692249298096}]}, {"text": "Previous research has sought to characterize texts in terms of domain-independent rhetorical elements, such as schema items or rhetorical relations (.", "labels": [], "entities": []}, {"text": "The focus of our work, however, is on an equally fundamental but domaindependent dimension of the structure of text: content.", "labels": [], "entities": []}, {"text": "Our use of the term \"content\" corresponds roughly to the notions of topic and topic change.", "labels": [], "entities": []}, {"text": "We desire models that can specify, for example, that articles about earthquakes typically contain information about quake strength, location, and casualties, and that descriptions of casualties usually precede those of rescue efforts.", "labels": [], "entities": []}, {"text": "But rather than manually determine the topics fora given domain, we take a distributional view, learning them directly from un-annotated texts via analysis of word distribution patterns.", "labels": [], "entities": []}, {"text": "This idea dates back at least to, who claimed that \"various types of recurrence patterns seem to characterize various types of discourse\".", "labels": [], "entities": []}, {"text": "Advantages of a distributional perspective include both drastic reduction inhuman effort and recognition of \"topics\" that might not occur to a human expert and yet, when explicitly modeled, aid in applications.", "labels": [], "entities": []}, {"text": "Of course, the success of the distributional approach depends on the existence of recurrent patterns.", "labels": [], "entities": []}, {"text": "In arbitrary document collections, such patterns might be too variable to be easily detected by statistical means.", "labels": [], "entities": []}, {"text": "However, research has shown that texts from the same domain tend to exhibit high similarity).", "labels": [], "entities": [{"text": "similarity", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9575456976890564}]}, {"text": "Cognitive psychologists have long posited that this similarity is not accidental, arguing that formulaic text structure facilitates readers' comprehension and recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9149207472801208}]}, {"text": "In this paper, we investigate the utility of domainspecific content models for representing topics and topic shifts.", "labels": [], "entities": []}, {"text": "Content models are Hidden Markov Models (HMMs) wherein states correspond to types of information characteristic to the domain of interest (e.g., earthquake magnitude or previous earthquake occurrences), and state transitions capture possible information-presentation orderings within that domain.", "labels": [], "entities": []}, {"text": "We first describe an efficient, knowledge-lean method for learning both a set of topics and the relations between topics directly from un-annotated documents.", "labels": [], "entities": []}, {"text": "Our technique incorporates a novel adaptation of the standard HMM induction algorithm that is tailored to the task of modeling content.", "labels": [], "entities": [{"text": "HMM induction", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.8462934792041779}]}, {"text": "Then, we apply techniques based on content models to two complex text-processing tasks.", "labels": [], "entities": []}, {"text": "First, we consider information ordering, that is, choosing a sequence in which to present a pre-selected set of items; this is an essential step in concept-to-text generation, multi-document summarization, and other text-synthesis problems.", "labels": [], "entities": [{"text": "information ordering", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.7442471086978912}, {"text": "concept-to-text generation", "start_pos": 148, "end_pos": 174, "type": "TASK", "confidence": 0.763259619474411}, {"text": "multi-document summarization", "start_pos": 176, "end_pos": 204, "type": "TASK", "confidence": 0.6920339465141296}]}, {"text": "In our experiments, content models outperform state-of-the-art ordering method by a wide margin -for one domain and performance metric, the gap was 78 percentage points.", "labels": [], "entities": []}, {"text": "Second, we consider extractive summa-rization: the compression of a document by choosing a subsequence of its sentences.", "labels": [], "entities": []}, {"text": "For this task, we develop anew content-model-based learning algorithm for sentence selection.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7967207729816437}]}, {"text": "The resulting summaries yield 88% match with human-written output, which compares favorably to the 69% achieved by the standard \"leading sentences\" baseline.", "labels": [], "entities": []}, {"text": "The success of content models in these two complementary tasks demonstrates their flexibility and effectiveness, and indicates that they are sufficiently expressive to represent important text properties.", "labels": [], "entities": []}, {"text": "These observations, taken together with the fact that content models are conceptually intuitive and efficiently learnable from raw document collections, suggest that the formalism can prove useful in an even broader range of applications than we have considered here; exploring the options is an appealing line of future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply the techniques just described to two tasks that stand to benefit from models of content and changes in topic: information ordering for text generation and information selection for single-document summarization.", "labels": [], "entities": [{"text": "information ordering", "start_pos": 119, "end_pos": 139, "type": "TASK", "confidence": 0.7045616954565048}, {"text": "text generation", "start_pos": 144, "end_pos": 159, "type": "TASK", "confidence": 0.7606265246868134}, {"text": "information selection", "start_pos": 164, "end_pos": 185, "type": "TASK", "confidence": 0.7400059103965759}, {"text": "single-document summarization", "start_pos": 190, "end_pos": 219, "type": "TASK", "confidence": 0.5125123709440231}]}, {"text": "These are two complementary tasks that rely on disjoint model functionalities: the ability to order a set of pre-selected information-bearing items, and the ability to do the selection itself, extracting from an ordered sequence of information-bearing items a representative subsequence.", "labels": [], "entities": []}, {"text": "The evaluation of our summarization algorithm was driven by two questions: (1) Are the summaries produced of acceptable quality, in terms of selected content? and (2) Does the content-model representation provide additional advantages over more locally-focused methods?", "labels": [], "entities": [{"text": "summarization", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9697890281677246}]}, {"text": "To address the first question, we compare summaries created by our system against the \"lead\" baseline, which extracts the first sentences of the original text -despite its simplicity, the results from the annual Document Understanding Conference (DUC) evaluation suggest that most single-document summarization systems cannot beat this baseline.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)", "start_pos": 212, "end_pos": 251, "type": "TASK", "confidence": 0.6598593046267828}]}, {"text": "To address question (2), we consider a summarization system that learns extraction rules directly from a parallel corpus of full texts and their summaries ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.986365795135498}]}, {"text": "In this system, summarization is framed as a sentence-level binary classification problem: each sentence is labeled by the publiclyavailable BoosTexter system) as being either \"in\" or \"out\" of the summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9864067435264587}, {"text": "sentence-level binary classification", "start_pos": 45, "end_pos": 81, "type": "TASK", "confidence": 0.6667738159497579}]}, {"text": "The features considered for each sentence are its unigrams and its location within the text, namely beginning third, middle third and end third.", "labels": [], "entities": []}, {"text": "10 Hence, relationships between sentences are not explicitly modeled, making this system a good basis for comparison.", "labels": [], "entities": []}, {"text": "We evaluated our summarization system on the Earthquakes domain, since for some of the texts in this domain there is a condensed version written by AP journalists.", "labels": [], "entities": [{"text": "Earthquakes domain", "start_pos": 45, "end_pos": 63, "type": "DATASET", "confidence": 0.9202113151550293}, {"text": "AP journalists", "start_pos": 148, "end_pos": 162, "type": "DATASET", "confidence": 0.9280973672866821}]}, {"text": "These summaries are mostly extractive ; consequently, they can be easily aligned with sentences in the original articles.", "labels": [], "entities": []}, {"text": "From sixty document-summary pairs, half were randomly selected to be used for training and the other half for testing.", "labels": [], "entities": []}, {"text": "(While thirty documents may not seem like a large number, it is comparable to the size of the training corpora used in the competitive summarizationsystem evaluations mentioned above.)", "labels": [], "entities": [{"text": "summarizationsystem evaluations", "start_pos": 135, "end_pos": 166, "type": "TASK", "confidence": 0.8536858558654785}]}, {"text": "The average number of sentences in the full texts and summaries was 15 and 6, respectively, fora total of 450 sentences in each of the test and (full documents of the) training sets.", "labels": [], "entities": []}, {"text": "At runtime, we provided the systems with a full document and the desired output length, namely, the length in sentences of the corresponding shortened version.", "labels": [], "entities": []}, {"text": "The resulting summaries were judged as a whole by the fraction of their component sentences that appeared in the human-written summary of the input text.", "labels": [], "entities": []}, {"text": "The results in confirm our hypothesis about the benefits of content models for text summarizationour model outperforms both the sentence-level, locallyfocused classifier and the \"lead\" baseline.", "labels": [], "entities": [{"text": "text summarizationour", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.7190817594528198}]}, {"text": "Furthermore, as the learning curves shown in indicate, our method achieves good performance on a small subset of parallel training data: in fact, the accuracy of our method on one third of the training data is higher than that of the System Extraction accuracy Content-based 88% Sentence classifier 76% (words + location) Leading sentences 69%  sentence-level classifier on the full training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9989651441574097}]}, {"text": "Clearly, this performance gain demonstrates the effectiveness of content models for the summarization task.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.9303934574127197}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics. Length is in sentences. Vo- cabulary size and type/token ratio are computed after re- placement of proper names, numbers and dates.", "labels": [], "entities": [{"text": "Length", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9804737567901611}, {"text": "Vo- cabulary size", "start_pos": 53, "end_pos": 70, "type": "METRIC", "confidence": 0.6676716357469559}]}, {"text": " Table 2: Ordering results (averages over the test cases).", "labels": [], "entities": []}, {"text": " Table 3: Percentage of cases for which the content model  assigned to the OSO a rank within a given range.", "labels": [], "entities": []}]}