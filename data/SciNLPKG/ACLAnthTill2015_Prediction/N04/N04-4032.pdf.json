{"title": [{"text": "Parsing Conversational Speech Using Enhanced Segmentation", "labels": [], "entities": [{"text": "Parsing Conversational Speech", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8256871501604716}]}], "abstractContent": [{"text": "The lack of sentence boundaries and presence of dis-fluencies pose difficulties for parsing conversational speech.", "labels": [], "entities": [{"text": "parsing conversational speech", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.8748442331949869}]}, {"text": "This work investigates the effects of automatically detecting these phenomena on a proba-bilistic parser's performance.", "labels": [], "entities": []}, {"text": "We demonstrate that a state-of-the-art segmenter, relative to a pause-based segmenter, gives more than 45% of the possible error reduction in parser performance, and that presentation of interruption points to the parser improves performance over using sentence boundaries alone.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parsing speech can be useful fora number of tasks, including information extraction and question answering from audio transcripts.", "labels": [], "entities": [{"text": "Parsing speech", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8741348087787628}, {"text": "information extraction", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.7896675765514374}, {"text": "question answering from audio transcripts", "start_pos": 88, "end_pos": 129, "type": "TASK", "confidence": 0.8811967015266419}]}, {"text": "However, parsing conversational speech presents a different set of challenges than parsing text: sentence boundaries are not well-defined, punctuation is absent, and disfluencies (edits and restarts) impact the structure of language.", "labels": [], "entities": [{"text": "parsing conversational speech", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.8824764688809713}]}, {"text": "Several efforts have looked at detecting sentence boundaries in speech, e.g. ().", "labels": [], "entities": []}, {"text": "Metadata extraction efforts, like (, extend this task to include identifying self-interruption points (IPs) that indicate a disfluency or restart.", "labels": [], "entities": [{"text": "Metadata extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7649038732051849}]}, {"text": "This paper explores the usefulness of identifying boundaries of sentence-like units (referred to as SUs) and IPs in parsing conversational speech.", "labels": [], "entities": [{"text": "parsing conversational speech", "start_pos": 116, "end_pos": 145, "type": "TASK", "confidence": 0.8642621239026388}]}, {"text": "Early work in parsing conversational speech was rulebased and limited in domain (.", "labels": [], "entities": [{"text": "parsing conversational speech", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.9112764000892639}]}, {"text": "Results from another rule-based system (Core and) suggests that standard parsers can be used to identify speech repairs in conversational speech.", "labels": [], "entities": [{"text": "identify speech repairs in conversational speech", "start_pos": 96, "end_pos": 144, "type": "TASK", "confidence": 0.692645713686943}]}, {"text": "Work in statistically parsing conversational speech) has examined the performance of a parser that removes edit regions in an earlier step.", "labels": [], "entities": [{"text": "statistically parsing conversational speech", "start_pos": 8, "end_pos": 51, "type": "TASK", "confidence": 0.6963116973638535}]}, {"text": "In contrast, we train a parser on the complete (human-specified) segmentation, with edit-regions included.", "labels": [], "entities": []}, {"text": "We choose to work with all of the words within edit regions anticipating that making the parallel syntactic structures of the edit region available to the parser can improve its performance in identifying that structure.", "labels": [], "entities": []}, {"text": "Our work makes use of the Structured Language Model (SLM) as a parser and an existing SU-IP detection algorithm, described next.", "labels": [], "entities": [{"text": "SU-IP detection", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.8350370228290558}]}], "datasetContent": [{"text": "In all experiments, the SLM parser was trained on the baseline truth Switchboard corpus described above, with hand-annotated SUs and optionally IPs.", "labels": [], "entities": [{"text": "SLM parser", "start_pos": 24, "end_pos": 34, "type": "TASK", "confidence": 0.9078755974769592}]}, {"text": "For testing, the system was presented with conversation sides segmented according to the various SU-predictions, and evaluated on its performance in predicting the true syntactic structure.", "labels": [], "entities": []}, {"text": "We seek to explore how much impact current metadata detection algorithms have over the na\u00a8\u0131vena\u00a8\u0131ve pause-based segmentation.", "labels": [], "entities": [{"text": "metadata detection", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7345874607563019}]}, {"text": "To this end, we test along two experimental dimensions: SU segmentation and IP detection.", "labels": [], "entities": [{"text": "SU segmentation", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.9507989883422852}, {"text": "IP detection", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.8169167637825012}]}, {"text": "Some type of segmentation is critical to most parsers.", "labels": [], "entities": []}, {"text": "In the SU dimension, we tested three conditions.", "labels": [], "entities": []}, {"text": "Across these conditions, the parser training was held constant, but the test segmentation varied across three cases: (i) oracle, hand-labeled SU segmentation; (ii) automatic, SU segmentation from the automatic detection system using both prosody and lexical cues (); and (iii) na\u00a8\u0131vena\u00a8\u0131ve, SU segmentation from a decision tree predictor using only pause duration cues.", "labels": [], "entities": [{"text": "SU segmentation", "start_pos": 291, "end_pos": 306, "type": "TASK", "confidence": 0.7596688270568848}]}, {"text": "The SUs are included as words, similar to sentence boundaries in prior SLM work.", "labels": [], "entities": []}, {"text": "By varying the SU segmentation of the test data for our system, we gain insight into how the performance of SU detection changes the overall accuracy of the parser.", "labels": [], "entities": [{"text": "SU detection", "start_pos": 108, "end_pos": 120, "type": "TASK", "confidence": 0.9641914963722229}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.998600423336029}]}, {"text": "We expect interruption points to be useful to parsing, since edit points often indicate a restart point, and the preceding syntactic phrase should attach to the tree differently.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9762603640556335}]}, {"text": "In the IP dimension, we examined two conditions (present and absent).", "labels": [], "entities": []}, {"text": "For each condition, we retrained the parser including hand-labeled IPs, since the vocabulary of available \"words\" is different when the IP is included as an input token.", "labels": [], "entities": []}, {"text": "The two IP conditions are: (a) No IP, training the parser on syntax that did not include IPs as words, and testing on segmented input that also did not include IP tokens; and (b) IP, training and testing on input that includes IPs as words.", "labels": [], "entities": []}, {"text": "The incorporation of IPs as words may not be ideal, since it reduces the number of true words available to an N-gram model at a given point, but it has the advantages of simplicity and consistency with SU treatment.", "labels": [], "entities": []}, {"text": "Because the na\u00a8\u0131vena\u00a8\u0131ve system does not predict IPs, we only have experiments for 5 of the 6 possible combinations.", "labels": [], "entities": []}, {"text": "We evaluated parser performance by using bracket precision and recall scores, as well as bracket-crossing, using the parseval metric.", "labels": [], "entities": [{"text": "bracket precision", "start_pos": 41, "end_pos": 58, "type": "METRIC", "confidence": 0.6204501092433929}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9841808080673218}]}, {"text": "This bracket-counting metric for parsers, requires that the input words (and, by implication, sentences) beheld constant across test conditions.", "labels": [], "entities": []}, {"text": "Since our experiments deliberately vary the segmentation, we needed to evaluate each conversation side as a single \"sentence\" in order to obtain meaningful results across different segmentations.", "labels": [], "entities": []}, {"text": "We construct this top-level sentence by attaching the parser's proposed constituents for each SU to anew top-level constituent (labeled TIPTOP).", "labels": [], "entities": [{"text": "TIPTOP", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.869691789150238}]}, {"text": "Thus, we can compare two different segmentations of the same data, because it ensures that the segmentations will agree at least at the beginning and end of the conversation.", "labels": [], "entities": []}, {"text": "Segmentation errors will of course cause some mismatches, but that possibility is what we are investigating.", "labels": [], "entities": []}, {"text": "For evaluation, we ignore the TIPTOP bracket (which always contains the entire conversation side), so this technique does not interfere with accurate bracket counting, but allows segmentation errors to be evaluated at the level of bracket-counting.", "labels": [], "entities": [{"text": "TIPTOP", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9625563025474548}, {"text": "bracket counting", "start_pos": 150, "end_pos": 166, "type": "TASK", "confidence": 0.6787737458944321}]}, {"text": "The SLM parser uses binary trees, but the syntactic structures we are given as truth often branch in N-ary ways, where \u00a1 . The parse trees used for training the SLM use bar-level nodes to transform N-ary trees into binary ones; the reverse mapping of SLM-produced binary trees back to N-ary trees is done by simply removing the bar-level constituents.", "labels": [], "entities": []}, {"text": "Finally, to compare the IP-present conditions with the non-IP conditions, we ignore IP tokens when counting brackets.", "labels": [], "entities": []}, {"text": "The number of bracket-crossings per \"sentence\" is quite high, due to evaluating all text from a given conversation side into one \"TIPTOP sentence\".", "labels": [], "entities": []}, {"text": "Precision and recall are regarding the bracketing of all the tokens under consideration (i.e., not including bar-level brackets, and not including IP token labeling).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9619150161743164}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9994513392448425}]}, {"text": "All differences are highly significant (", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Bracket crossing, precision and recall results.", "labels": [], "entities": [{"text": "Bracket crossing", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8242976069450378}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9996505975723267}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9990170001983643}]}]}