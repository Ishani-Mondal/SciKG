{"title": [{"text": "Using N-best Lists for Named Entity Recognition from Chinese Speech", "labels": [], "entities": [{"text": "Named Entity Recognition from Chinese", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.7301702499389648}]}], "abstractContent": [{"text": "We present the first known result for named entity recognition (NER) in realistic large-vocabulary spoken Chinese.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.7873533169428507}]}, {"text": "We establish this result by applying a maximum entropy model, currently the single best known approach for textual Chinese NER, to the recognition output of the BBN LVCSR system on Chinese Broadcast News utterances.", "labels": [], "entities": [{"text": "BBN LVCSR system on Chinese Broadcast News utterances", "start_pos": 161, "end_pos": 214, "type": "DATASET", "confidence": 0.8672772720456123}]}, {"text": "Our results support the claim that transferring NER approaches from text to spoken language is a significantly more difficult task for Chinese than for English.", "labels": [], "entities": []}, {"text": "We propose re-segmenting the ASR hypotheses as well as applying post-classification to improve the performance.", "labels": [], "entities": [{"text": "ASR", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9273762702941895}]}, {"text": "Finally, we introduce a method of using n-best hypotheses that yields a small but nevertheless useful improvement NER accuracy.", "labels": [], "entities": [{"text": "NER", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.8729432821273804}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9300203919410706}]}, {"text": "We use acoustic, phonetic, language model, NER and other scores as confidence measure.", "labels": [], "entities": []}, {"text": "Experimental results show an average of 6.7% relative improvement in precision and 1.7% relative improvement in F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.999733030796051}, {"text": "F-measure", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9929266571998596}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) is the first step for many tasks in the fields of natural language processing and information retrieval.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7553923179705938}, {"text": "natural language processing", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.653785228729248}, {"text": "information retrieval", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7739239931106567}]}, {"text": "It is a designated task in a number of conferences, including the Message Understanding Conference (MUC), the Information Retrieval and Extraction Conference (IREX), the Conferences on Natural Language Learning (CoNLL) and the recent Automatic Content Extraction Conference (ACE).", "labels": [], "entities": [{"text": "Message Understanding Conference (MUC)", "start_pos": 66, "end_pos": 104, "type": "TASK", "confidence": 0.830242653687795}, {"text": "Information Retrieval and Extraction Conference (IREX)", "start_pos": 110, "end_pos": 164, "type": "TASK", "confidence": 0.8368596658110619}, {"text": "Automatic Content Extraction Conference (ACE)", "start_pos": 234, "end_pos": 279, "type": "TASK", "confidence": 0.7940033418791634}]}, {"text": "There has been a considerable amount of work on English NER yielding good performance).", "labels": [], "entities": [{"text": "English NER", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.6397570967674255}]}, {"text": "However, Chinese NER is more difficult, especially on speech output, due to two reasons.", "labels": [], "entities": [{"text": "Chinese NER", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.4856985807418823}]}, {"text": "First, Chinese has a large number of homonyms and the vocabulary used in Chinese person names is an open set so more characters/words are unseen in the training data.", "labels": [], "entities": []}, {"text": "Second, there is no standard definition of Chinese words.", "labels": [], "entities": []}, {"text": "Word segmentation errors made by recognizers may lead to NER errors.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6554766893386841}, {"text": "NER", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9478462338447571}]}, {"text": "Previous work on Chinese textual NER includes and but there has been no published work on NER in spoken Chinese.", "labels": [], "entities": [{"text": "Chinese textual NER", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.5223497549692789}, {"text": "NER in spoken Chinese", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.828307494521141}]}, {"text": "Named Entity Recognition for speech is more difficult than for text, since the most reliable features for textual NER (punctuation, capitalization, and syntactic patterns) are often not available in speech output.", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7316432595252991}]}, {"text": "NER on automatically recognized broadcast news was first conducted by MITRE in 1997, and was subsequently added to Hub-4 evaluation as a task.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9071007370948792}]}, {"text": "used error modeling, and proposed discriminative training to handle NER errors; both used a hidden Markov model (HMM).", "labels": [], "entities": [{"text": "NER errors", "start_pos": 68, "end_pos": 78, "type": "TASK", "confidence": 0.8574661612510681}]}, {"text": "also reported results in English speech NER using an HMM model.", "labels": [], "entities": [{"text": "English speech NER", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.5152033468087515}]}, {"text": "Ina NIST 1999 evaluation, it was found that NER errors on speech arise from a combination of ASR errors and errors of the underlying NER system.", "labels": [], "entities": [{"text": "NIST 1999 evaluation", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9192630251248678}]}, {"text": "In this work, we investigate whether the NIST finding holds for Chinese speech NER as well.", "labels": [], "entities": [{"text": "NIST finding", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.8237569034099579}, {"text": "Chinese speech NER", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.4498065213362376}]}, {"text": "We present the first known result for recognizing named entities in realistic large-vocabulary spoken Chinese.", "labels": [], "entities": []}, {"text": "We propose to use the best-known model for Chinese textual NERa maximum entropy model-on Chinese speech NER.", "labels": [], "entities": []}, {"text": "We also propose using re-segmentation and postclassification to improve this model.", "labels": [], "entities": []}, {"text": "Finally, we propose to integrate the ASR and NER components to optimize NER performance by making use of the n-best ASR output.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the n-best hypothesis of 1,046 Broadcast News Chinese utterances from the BBN LVCSR system.", "labels": [], "entities": [{"text": "Broadcast News Chinese utterances", "start_pos": 38, "end_pos": 71, "type": "DATASET", "confidence": 0.8856493830680847}, {"text": "BBN LVCSR system", "start_pos": 81, "end_pos": 97, "type": "DATASET", "confidence": 0.9320510029792786}]}, {"text": "n ranges from one to 300, averaging at 68.", "labels": [], "entities": []}, {"text": "Each utterance has a reference transcription with no recognition error.", "labels": [], "entities": []}, {"text": "presents the NER results for the reference sentence, one best hypothesis, and different n-best voting methods.", "labels": [], "entities": []}, {"text": "Results for the reference sentences show the upper bound performance (68% F-measure) of applying a MaxEnt NER system trained from the Chinese text corpus (e.g., PFR) to Chinese speech output (e.g., Broadcast News).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9990052580833435}, {"text": "Broadcast News)", "start_pos": 198, "end_pos": 213, "type": "DATASET", "confidence": 0.9275617996851603}]}, {"text": "From, we can conclude that it is possible to improve NER precision by using n-best hypothesis by finding the optimized combination of different acoustic, language model, NER, and other scores.", "labels": [], "entities": [{"text": "NER", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9696251153945923}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.8899853825569153}]}, {"text": "In particular, since most errors in Chinese ASR seem to be for person names, using NER score on the n-best hypotheses can improve recognition results by a relative 6.7% in precision and 1.7% in F-measure..", "labels": [], "entities": [{"text": "ASR", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.7810628414154053}, {"text": "NER score", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.7156279683113098}, {"text": "precision", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9995216131210327}, {"text": "F-measure.", "start_pos": 194, "end_pos": 204, "type": "METRIC", "confidence": 0.997969925403595}]}, {"text": "n-best weighted voting with NE score gives the best performance.", "labels": [], "entities": [{"text": "NE", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9784483909606934}]}], "tableCaptions": [{"text": " Table 1. NER results on Chinese speech data are worse  than on Chinese text data.", "labels": [], "entities": []}, {"text": " Table 3. The character model is better than the word  model, and one-pass NER is better than two-pass.", "labels": [], "entities": []}, {"text": " Table 4. Next,  we propose a mechanism of weighted voting using  confidence measure for each hypothesis. In one  experiment, we use the MaxEnt NER score as  confidence measure. In another experiment, we use all  the six scores (acoustic, language model, number of  words, number of phones, number of silence, or NER  score) provided by the BBN ASR system as confidence  measure. During implementation, an optimizer based on  Powell's algorithm is used to find the 6 weights (\u03c9 k ) for  each score (S k ). For any given hypothesis, confidence  measure is given by:", "labels": [], "entities": [{"text": "MaxEnt NER score", "start_pos": 137, "end_pos": 153, "type": "DATASET", "confidence": 0.7193766633669535}, {"text": "BBN ASR system", "start_pos": 341, "end_pos": 355, "type": "DATASET", "confidence": 0.9280012448628744}]}, {"text": " Table 4. n-best weighted voting with NE score gives the  best performance.", "labels": [], "entities": [{"text": "NE score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9500260353088379}]}]}