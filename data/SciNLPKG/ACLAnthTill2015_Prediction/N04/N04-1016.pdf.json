{"title": [{"text": "The Web as a Baseline: Evaluating the Performance of Unsupervised Web-based Models fora Range of NLP Tasks", "labels": [], "entities": []}], "abstractContent": [{"text": "Previous work demonstrated that web counts can be used to approximate bigram frequencies , and thus should be useful fora wide variety of NLP tasks.", "labels": [], "entities": []}, {"text": "So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7997283935546875}, {"text": "confusion-set disambiguation", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.6909735798835754}]}, {"text": "The present paper investigates if these results generalize to tasks covering both syntax and semantics , both generation and analysis, and a larger range of n-grams.", "labels": [], "entities": [{"text": "generation and analysis", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.7077061931292216}]}, {"text": "For the majority of tasks, we find that simple, unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus.", "labels": [], "entities": []}, {"text": "However, inmost cases, web-based models fail to outperform more sophisticated state-of-the-art models trained on small corpora.", "labels": [], "entities": []}, {"text": "We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models.", "labels": [], "entities": []}], "introductionContent": [{"text": "investigated the validity of web counts fora range of predicate-argument bigrams (verbobject, adjective-noun, and noun-noun bigrams).", "labels": [], "entities": []}, {"text": "They presented a simple method for retrieving bigram counts from the web by querying a search engine and demonstrated that web counts (a) correlate with frequencies obtained from a carefully edited, balanced corpus such as the 100M words British National Corpus (BNC), (b) correlate with frequencies recreated using smoothing methods in the case of unseen bigrams, (c) reliably predict human plausibility judgments, and (d) yield state-of-the-art performance on pseudo-disambiguation tasks.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 238, "end_pos": 267, "type": "DATASET", "confidence": 0.9733668863773346}]}, {"text": "results suggest that webbased frequencies can be a viable alternative to bigram frequencies obtained from smaller corpora or recreated using smoothing.", "labels": [], "entities": []}, {"text": "However, they do not demonstrate that realistic NLP tasks can benefit from web counts.", "labels": [], "entities": []}, {"text": "In order to show this, web counts would have to be applied to a diverse range of NLP tasks, both syntactic and seman-: Overview of the tasks investigated in this paper (n: size of n-gram; POS: parts of speech; Ling: linguistic knowledge; Type: type of task) tic, involving analysis (e.g., disambiguation) and generation (e.g., selection among competing outputs).", "labels": [], "entities": []}, {"text": "Also, it remains to be shown that the web-based approach scales up to larger n-grams (e.g., trigrams), and to combinations of different parts of speech (Keller and Lapata 2003 only tested bigrams involving nouns, verbs, and adjectives).", "labels": [], "entities": []}, {"text": "Another important question is whether web-based methods, which are by definition unsupervised, can be competitive alternatives to supervised approaches used for most tasks in the literature.", "labels": [], "entities": []}, {"text": "This paper aims to address these questions.", "labels": [], "entities": []}, {"text": "We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation and (b) context sensitive spelling correction (.", "labels": [], "entities": [{"text": "target language candidate selection", "start_pos": 124, "end_pos": 159, "type": "TASK", "confidence": 0.5977292433381081}, {"text": "machine translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.771199107170105}, {"text": "context sensitive spelling correction", "start_pos": 192, "end_pos": 229, "type": "TASK", "confidence": 0.6024977117776871}]}, {"text": "Then we investigate the generality of the web-based approach by applying it to a range of analysis and generations tasks, involving both syntactic and semantic knowledge: (c) ordering of prenominal adjectives, (d) compound noun bracketing, (e) compound noun interpretation, and (f) noun countability detection.", "labels": [], "entities": [{"text": "compound noun bracketing", "start_pos": 214, "end_pos": 238, "type": "TASK", "confidence": 0.619843989610672}, {"text": "compound noun interpretation", "start_pos": 244, "end_pos": 272, "type": "TASK", "confidence": 0.6757611334323883}, {"text": "noun countability detection", "start_pos": 282, "end_pos": 309, "type": "TASK", "confidence": 0.8090528249740601}]}, {"text": "gives an overview of these tasks and their properties.", "labels": [], "entities": []}, {"text": "In all cases, we propose a simple, unsupervised n-gram based model whose parameters are estimated using web counts.", "labels": [], "entities": []}, {"text": "We compare this model both against a baseline (same model, but parameters estimated on the BNC) and against state-of-the-art models from the literature, which are either supervised (i.e., use annotated training data) or unsupervised but rely on taxonomies to recreate missing counts.", "labels": [], "entities": [{"text": "BNC", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.8956107497215271}]}, {"text": "Following, web counts for ngrams were obtained using a simple heuristic based on queries to the search engine Altavista.", "labels": [], "entities": []}, {"text": "In this approach, the web count fora given n-gram is simply the number of hits (pages) returned by the search engine for the queries generated for this n-gram.", "labels": [], "entities": []}, {"text": "Three different types of queries were used for the NLP tasks in the present paper: Literal queries use the quoted n-gram directly as a search term for Altavista (e.g., the bigram history changes expands to the query \"history changes\").", "labels": [], "entities": []}, {"text": "Near queries use Altavista's NEAR operator to expand the n-gram; a NEAR b means that a has to occur in the same ten word window as b; the window is treated as a bag of words (e.g., history changes expands to \"history\" NEAR \"changes\").", "labels": [], "entities": []}, {"text": "Inflected queries are performed by expanding an n-gram into all its morphological forms.", "labels": [], "entities": []}, {"text": "These forms are then submitted as literal queries, and the resulting hits are summed up (e.g., history changes expands to \"history change\", \"histories change\", \"history changed\", etc.).", "labels": [], "entities": []}, {"text": "John Carroll's suite of morphological tools (morpha, morphg, and ana) was used to generate inflected forms of verbs and nouns.", "labels": [], "entities": []}, {"text": "In certain cases (detailed below), determiners were inserted before nouns in order to make it possible to recognize simple NPs.", "labels": [], "entities": []}, {"text": "This insertion was limited to a/an, the, and the empty determiner (for bare plurals).", "labels": [], "entities": []}, {"text": "All queries (other than the ones using the NEAR operator) were performed as exact matches (using quotation marks in Altavista).", "labels": [], "entities": []}, {"text": "All search terms were submitted to the search engine in lowercase.", "labels": [], "entities": []}, {"text": "If a query consists of a single, highly frequent word (such as the), Altavista will return an error message.", "labels": [], "entities": []}, {"text": "In these cases, we set the web count to a large constant ).", "labels": [], "entities": []}, {"text": "This problem is limited to unigrams, which were used in some of the models detailed below.", "labels": [], "entities": []}, {"text": "Sometimes the search engine fails to return a hit fora given n-gram (for any of its morphological variants).", "labels": [], "entities": []}, {"text": "We smooth zero counts by setting them to .5.", "labels": [], "entities": []}, {"text": "For all tasks, the web-based models are compared against identical models whose parameters were estimated from the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.9416508674621582}]}, {"text": "The BNC is a static 100M word corpus of British English, which is about 1000 times smaller than the web.", "labels": [], "entities": [{"text": "BNC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9285061955451965}]}, {"text": "Comparing the performance of the same model on the web and on the BNC allows us to assess how much improvement can be expected simply by using a larger data set.", "labels": [], "entities": [{"text": "BNC", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9243161082267761}]}, {"text": "The BNC counts were retrieved using the Gsearch corpus query tool (); the morphological query expansion was the same as for web queries; the NEAR operator was simulated by assuming a window of five words to the left and five to the right.", "labels": [], "entities": []}, {"text": "# best model on development set * \ud97b\udf59 * (not) sign.", "labels": [], "entities": []}, {"text": "different from best BNC model on test set \u2020 \ud97b\udf59 \u2020 (not) sign.", "labels": [], "entities": []}, {"text": "different from baseline \u2021 \ud97b\udf59 \u2021 (not) sign.", "labels": [], "entities": [{"text": "baseline \u2021 \ud97b\udf59 \u2021", "start_pos": 15, "end_pos": 29, "type": "METRIC", "confidence": 0.8540626168251038}]}, {"text": "different from best model in the literature: Meaning of diacritics indicating statistical significance (\u03c7 2 tests) Gsearch was used to search solely for adjacent words; no POS information was incorporated in the queries, and no parsing was performed.", "labels": [], "entities": [{"text": "Meaning", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9841449856758118}]}, {"text": "For all of our tasks, we have to select either the best of several possible models or the best parameter setting fora single model.", "labels": [], "entities": []}, {"text": "We therefore require a separate development set.", "labels": [], "entities": []}, {"text": "This was achieved by using the gold standard data set from the literature fora given task and randomly dividing it into a development set and a test set (of equal size).", "labels": [], "entities": []}, {"text": "We report the test set performance for all models fora given task, and indicate which model shows optimal performance on the development set (marked by a '#' in all subsequent tables).", "labels": [], "entities": []}, {"text": "We then compare the test set performance of this optimal model to the performance of the models reported in the literature.", "labels": [], "entities": []}, {"text": "It is important to note that the figures taken from the literature were typically obtained on the whole gold standard data set, and hence may differ from the performance on our test set.", "labels": [], "entities": [{"text": "gold standard data set", "start_pos": 104, "end_pos": 126, "type": "DATASET", "confidence": 0.9240446537733078}]}, {"text": "We work on the assumption that such differences are negligible.", "labels": [], "entities": []}, {"text": "We use \u03c7 2 tests to determine whether the performance of the best web model on the test set is significantly different from that of the best BNC model.", "labels": [], "entities": []}, {"text": "We also determine whether both models differ significantly from the baseline and from the best model in the literature.", "labels": [], "entities": []}, {"text": "A set of diacritics is used to indicate significance throughout this paper, see.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Performance of Altavista counts and BNC counts  for candidate selection for MT (data from Prescher et al.  2000)", "labels": [], "entities": [{"text": "BNC counts", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9712794423103333}, {"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.9920143485069275}]}, {"text": " Table 4: Performance comparison with the literature for  candidate selection for MT", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9931483864784241}]}, {"text": " Table 5: Performance of Altavista counts and BNC  counts for context sensitive spelling correction (data from  Cucerzan and Yarowsky 2002)", "labels": [], "entities": [{"text": "BNC  counts", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9718026220798492}, {"text": "context sensitive spelling correction", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.612888291478157}]}, {"text": " Table 6: Performance comparison with the literature for  context sensitive spelling correction", "labels": [], "entities": [{"text": "context sensitive spelling correction", "start_pos": 58, "end_pos": 95, "type": "TASK", "confidence": 0.6714409068226814}]}, {"text": " Table 7: Performance of Altavista counts and BNC counts  for adjective ordering (data from Malouf 2000)", "labels": [], "entities": [{"text": "BNC", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9577956199645996}, {"text": "adjective ordering", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7547063231468201}]}, {"text": " Table 7. We also found that there was no significant  difference between the best Altavista model and the best  model reported by Malouf, a supervised method using  positional probability estimates from the BNC and mor- phological variants.", "labels": [], "entities": [{"text": "BNC", "start_pos": 208, "end_pos": 211, "type": "DATASET", "confidence": 0.8417664766311646}]}, {"text": " Table 8: Performance of Altavista counts and BNC counts  for compound bracketing (data from Lauer 1995)", "labels": [], "entities": [{"text": "BNC counts", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9767985641956329}, {"text": "compound bracketing", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.6808821260929108}]}, {"text": " Table 9: Performance comparison with the literature for  compound bracketing", "labels": [], "entities": [{"text": "compound bracketing", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7599334120750427}]}, {"text": " Table 10: Performance of Altavista counts and BNC  counts for compound interpretation (data from Lauer  1995)", "labels": [], "entities": [{"text": "BNC  counts", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9732747972011566}, {"text": "compound interpretation", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.8159822523593903}]}, {"text": " Table 11: Performance comparison with the literature for  compound interpretation", "labels": [], "entities": [{"text": "compound interpretation", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.9021008908748627}]}, {"text": " Table 12: Performance of Altavista counts and BNC  counts for noun countability detection (data from Bald- win and Bond 2003)", "labels": [], "entities": [{"text": "BNC", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.983911395072937}, {"text": "noun countability detection", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.8553067644437155}, {"text": "Bald- win and Bond 2003", "start_pos": 102, "end_pos": 125, "type": "DATASET", "confidence": 0.8505814373493195}]}, {"text": " Table 13: Performance comparison with the literature for  noun countability detection", "labels": [], "entities": [{"text": "noun countability", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.8313094973564148}]}, {"text": " Table 12. The best Altavista model is the condi- tional det-noun model ( f (det, n)/ f (n)), which achieves  88.38% on countable and 91.22% on uncountable nouns.  On the BNC, the simple unigram model performs best. Its  performance is not statistically different from that of the  best Altavista model. Note that for the BNC models, data  sparseness means the det-noun models perform poorly,  which is why the backoff model was not attempted here.", "labels": [], "entities": [{"text": "BNC", "start_pos": 171, "end_pos": 174, "type": "DATASET", "confidence": 0.9357689619064331}]}]}