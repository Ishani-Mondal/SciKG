{"title": [{"text": "A Salience-Based Approach to Gesture-Speech Alignment", "labels": [], "entities": [{"text": "Salience-Based Approach", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.8794485926628113}, {"text": "Gesture-Speech Alignment", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.8321233093738556}]}], "abstractContent": [{"text": "One of the first steps towards understanding natural multimodal language is aligning gesture and speech, so that the appropriate gestures ground referential pronouns in the speech.", "labels": [], "entities": []}, {"text": "This paper presents a novel technique for gesture-speech alignment, inspired by salience-based approaches to anaphoric pronoun resolution.", "labels": [], "entities": [{"text": "gesture-speech alignment", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.7735225856304169}, {"text": "anaphoric pronoun resolution", "start_pos": 109, "end_pos": 137, "type": "TASK", "confidence": 0.7604581713676453}]}, {"text": "We use a hybrid between data-driven and knowledge-based mtehods: the basic structure is derived from a set of rules about gesture salience, but the salience weights themselves are learned from a corpus.", "labels": [], "entities": []}, {"text": "Our system achieves 95% recall and precision on a corpus of transcriptions of unconstrained multimodal monologues , significantly outperforming a competitive baseline.", "labels": [], "entities": [{"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9995269775390625}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9965924620628357}]}], "introductionContent": [{"text": "In face to face communication, speakers frequently use gesture to supplement speech, using the additional modality to provide unique, non-redundant information.", "labels": [], "entities": []}, {"text": "In the context of pen/speech user interfaces, Oviatt finds that \"multimodal ...", "labels": [], "entities": []}, {"text": "language is briefer, syntactically simpler, and less disfluent than users' unimodal speech.\"", "labels": [], "entities": []}, {"text": "One of the simplest and most direct ways in which gesture can supplement verbal communication is by grounding references, usually through deixis.", "labels": [], "entities": []}, {"text": "For example, it is impossible to extract the semantic content of the verbal utterance \"I'll take this one\" without an accompanying pointing gesture indicating the thing that is desired.", "labels": [], "entities": [{"text": "extract the semantic content of the verbal utterance \"I'll take this one\"", "start_pos": 33, "end_pos": 106, "type": "TASK", "confidence": 0.7880292097727458}]}, {"text": "The problem of gesture-speech alignment involves choosing the appropriate gesture to ground each verbal utterance.", "labels": [], "entities": [{"text": "gesture-speech alignment", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.7675110995769501}]}, {"text": "This paper describes a novel technique for this problem.", "labels": [], "entities": []}, {"text": "We evaluate our system on a corpus of multimodal monologues with no fixed grammar or vocabulary.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our system by testing its performance on a set of 26 transcriptions of unconstrained humanto-human communication, from nine different speak-: Performance of our system versus a baseline ers (.", "labels": [], "entities": []}, {"text": "Of the four women and five men who participated, eight were right-handed, and one was a non-native English speaker.", "labels": [], "entities": []}, {"text": "The participants ranged in age from 22 to 28.", "labels": [], "entities": []}, {"text": "All had extensive computer experience, but none had any experience in the task domain, which required explaining the behavior of simple mechanical devices.", "labels": [], "entities": []}, {"text": "The participants were presented with three conditions, each of which involved describing the operation of a mechanical device based on a computer simulation.", "labels": [], "entities": []}, {"text": "The conditions were shown in order of increasing complexity, as measured by the number of moving parts: a latchbox, a piston, and a pinball machine.", "labels": [], "entities": []}, {"text": "Monologues ranged in duration from 15 to 90 seconds; the number of gestures used ranged from six to 58.", "labels": [], "entities": []}, {"text": "In total, 574 gesture phrases were transcribed, of which 239 participated in gesture-speech bindings.", "labels": [], "entities": [{"text": "gesture-speech bindings", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.7635553777217865}]}, {"text": "In explaining the devices, the participants were allowed -but not instructed -to refer to a predrawn diagram that corresponded to the simulation.", "labels": [], "entities": []}, {"text": "Vocabulary, grammar, and gesture were not constrained in anyway.", "labels": [], "entities": []}, {"text": "The monologues were videotaped, transcribed, and annotated by hand.", "labels": [], "entities": []}, {"text": "No gesture or speech recognition was performed.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.6764716506004333}]}, {"text": "The decision to use transcriptions rather than speech and gesture recognizers will be discussed in detail below.", "labels": [], "entities": [{"text": "speech and gesture recognizers", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6262475326657295}]}], "tableCaptions": [{"text": " Table 1: Performance of our system versus a baseline", "labels": [], "entities": []}]}