{"title": [{"text": "Automatic Question Answering: Beyond the Factoid", "labels": [], "entities": [{"text": "Automatic Question Answering", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6414828101793925}]}], "abstractContent": [{"text": "In this paper we describe and evaluate a Question Answering system that goes beyond answering factoid questions.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.8237949907779694}]}, {"text": "We focus on FAQ-like questions and answers, and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for an-swer/question terms, trained on a corpus of 1 million question/answer pairs collected from the Web.", "labels": [], "entities": [{"text": "FAQ-like questions and answers", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.743177518248558}]}], "introductionContent": [{"text": "The Question Answering (QA) task has received a great deal of attention from the Computational Linguistics research community in the last few years (e.g.,.", "labels": [], "entities": [{"text": "Question Answering (QA) task", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.8973751465479533}]}, {"text": "The definition of the task, however, is generally restricted to answering factoid questions: questions for which a complete answer can be given in 50 bytes or less, which is roughly a few words.", "labels": [], "entities": []}, {"text": "Even with this limitation in place, factoid question answering is by no means an easy task.", "labels": [], "entities": [{"text": "factoid question answering", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.8151242534319559}]}, {"text": "The challenges posed by answering factoid question have been addressed using a large variety of techniques, such as question parsing), question-type determination (), WordNet exploitation (), Web exploitation (), noisy-channel transformations, semantic analysis (), and inferencing ().", "labels": [], "entities": [{"text": "question parsing", "start_pos": 116, "end_pos": 132, "type": "TASK", "confidence": 0.7616015076637268}, {"text": "question-type determination", "start_pos": 135, "end_pos": 162, "type": "TASK", "confidence": 0.8106953799724579}]}, {"text": "The obvious limitation of any factoid QA system is that many questions that people want answers for are not factoid questions.", "labels": [], "entities": [{"text": "factoid QA", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.6683763861656189}]}, {"text": "It is also frequently the case that nonfactoid questions are the ones for which answers cannot as readily be found by simply using a good search engine.", "labels": [], "entities": []}, {"text": "It follows that there is a good economic incentive in moving the QA task to a more general level: it is likely that a system able to answer complex questions of the type people generally and/or frequently ask has greater potential impact than one restricted to answering only factoid questions.", "labels": [], "entities": [{"text": "QA task", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.9090087413787842}]}, {"text": "A natural move is to recast the question answering task to handling questions people frequently ask or want answers for, as seen in Frequently Asked Questions (FAQ) lists.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.8217691779136658}]}, {"text": "These questions are sometimes factoid questions (such as, \"What is Scotland's national costume?\"), but in general are more complex questions (such as, \"How does a film qualify for an Academy Award?\", which requires an answer along the following lines: \"A feature film must screen in a Los Angeles County theater in 35 or 70mm or in a 24-frame progressive scan digital format suitable for exhibiting in existing commercial digital cinema sites for paid admission for seven consecutive days.", "labels": [], "entities": []}, {"text": "The seven day run must begin before midnight, December 31, of the qualifying year.", "labels": [], "entities": []}, {"text": "In this paper, we make a first attempt towards solving a QA problem more generic than factoid QA, for which there are no restrictions on the type of questions that are handled, and there is no assumption that the answers to be provided are factoids.", "labels": [], "entities": []}, {"text": "In our solution to this problem we employ learning mechanisms for questionanswer transformations (), and also exploit large document collections such as the Web for finding answers ().", "labels": [], "entities": [{"text": "questionanswer transformations", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.7719748318195343}]}, {"text": "We build our QA system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms, trained on a corpus of 1 million question/answer pairs collected from the Web.", "labels": [], "entities": []}, {"text": "Our evaluations show that our system achieves reasonable performance in terms of answer accuracy fora large variety of complex, non-factoid questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9181682467460632}]}], "datasetContent": [{"text": "We evaluated our QA system systematically for each module, in order to assess the impact of various algorithms on the overall performance of the system.", "labels": [], "entities": []}, {"text": "The evaluation was done by a human judge on a set of 115 Test questions, which contained a large variety of nonfactoid questions.", "labels": [], "entities": []}, {"text": "Each answer was rated as either correct(C), somehow related(S), wrong(W), or cannot tell(N).", "labels": [], "entities": [{"text": "tell(N)", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9267522543668747}]}, {"text": "The somehow related option allowed the judge to indicate the fact that the answer was only partially correct (for example, because of missing information, or because the answer was more general/specific than required by the question, etc.).", "labels": [], "entities": []}, {"text": "The cannot tell option was used in those cases when the validity of the answer could not be assessed.", "labels": [], "entities": []}, {"text": "Note that the judge did not have access to any reference answers in order to asses the quality of a proposed answer.", "labels": [], "entities": []}, {"text": "Only general knowledge and human judgment were involved when assessing the validity of the proposed answers.", "labels": [], "entities": []}, {"text": "Also note that, mainly because our system's answers were restricted to a maximum of 3 sentences, the evaluation guidelines stated that answers that contained the right information plus other extraneous information were to be rated correct.", "labels": [], "entities": []}, {"text": "For the given set of Test questions, we estimated the performance of the system using the formula (|C|+.5|S|)/(|C|+|S|+|W|).", "labels": [], "entities": []}, {"text": "This formula gives a score of 1 if the questions that are not \"N\" rated are all considered correct, and a score of 0 if they are all considered wrong.", "labels": [], "entities": []}, {"text": "A score of 0.5 means that, in average, 1 out of 2 questions is answered correctly.", "labels": [], "entities": []}, {"text": "We evaluated the Question2Query module while keeping fixed the configuration of the other modules (MSNSearch as the search engine, the top 10 hits in the Filter module), except for the AnswerExtraction module, for which we tested both the N-gram co-occurrence based algorithm (NG-AE) and a Model 1 based algorithm (M1e-AE, see Section 5.4).", "labels": [], "entities": []}, {"text": "The evaluation assessed the impact of the statistical chunker used to transform questions into queries, against the baseline strategy of submitting the question as-is to the search engine.", "labels": [], "entities": []}, {"text": "As illustrated in, the overall performance of the QA system significantly increased when the question was segmented before being submitted to the SearchEngine module, for both AnswerExtraction algorithms.", "labels": [], "entities": []}, {"text": "The score increased from 0.18 to 0.23 when using the NG-AE algorithm, and from 0.34 to 0.38 when using the M1e-AE algorithm.", "labels": [], "entities": []}, {"text": "The evaluation of the SearchEngine module assessed the impact of different search engines on the overall system performance.", "labels": [], "entities": []}, {"text": "We fixed the configurations of the other modules (segmented question for the Question2Query module, top 10 hits in the Filter module), except for the AnswerExtraction module, for which we tested the performance while using for answer extraction the NG-AE, M1e-AE, and ONG-AE algorithms.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 227, "end_pos": 244, "type": "TASK", "confidence": 0.8311001062393188}, {"text": "ONG-AE", "start_pos": 268, "end_pos": 274, "type": "METRIC", "confidence": 0.8763436675071716}]}, {"text": "The later algorithm works exactly like NG-AE, with the exception that the potential answers are compared with a reference answer available to an Oracle, rather than against the question.", "labels": [], "entities": []}, {"text": "The performance obtained using the ONG-AE algorithm can bethought of as indicative of the ceiling in the performance that can be achieved by an AE algorithm given the potential answers available.", "labels": [], "entities": []}, {"text": "As illustrated in, both the MSNSearch and Google search engines achieved comparable performance accuracy.", "labels": [], "entities": [{"text": "MSNSearch", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.91527259349823}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9715174436569214}]}, {"text": "The scores were 0.23 and 0.24 when using the NG-AE algorithm, 0.38 and 0.37 when using the M1e-AE algorithm, and 0.46 and 0.46 when using the ONG-AE algorithm, for MSNSearch and Google, respectively.", "labels": [], "entities": []}, {"text": "As aside note, it is worth mentioning that only 5% of the URLs returned by the two search engines for the entire Test set of questions overlapped.", "labels": [], "entities": [{"text": "Test set of questions", "start_pos": 113, "end_pos": 134, "type": "DATASET", "confidence": 0.907539963722229}]}, {"text": "Therefore, the comparable performance accuracy was not due to the fact that the AnswerExtraction module had access to the same set of potential answers, but rather to the fact that the 10 best hits of both search engines provide similar answering options.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9953180551528931}]}, {"text": "As mentioned in Section 4, the Filter module filters out the low score documents returned by the search engine and provides a set of potential answers extracted from the N-best list of documents.", "labels": [], "entities": []}, {"text": "The evaluation of the Filter module therefore assessed the trade-off between computation time and accuracy of the overall system: the size of the set of potential answers directly influences the accuracy of the system while increasing the computation time of the AnswerExtraction module.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9988258481025696}, {"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9980746507644653}]}, {"text": "The ONG-AE algorithm gives an accurate estimate of the performance ceiling induced by the set of potential answers available to the AnswerExtraction Module.", "labels": [], "entities": [{"text": "ONG-AE", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.69541335105896}]}, {"text": "As illustrated in, there is a significant performance ceiling increase from considering only the document returned as the first hit (0.36) to considering the first 10 hits (0.46).", "labels": [], "entities": []}, {"text": "There is only a slight increase in performance ceiling, however, from considering the first 10 hits to considering the first 50 hits (0.46 to 0.49).", "labels": [], "entities": [{"text": "performance ceiling", "start_pos": 35, "end_pos": 54, "type": "METRIC", "confidence": 0.8822197616100311}]}, {"text": "The Answer-Extraction module was evaluated while fixing all the other module configurations (segmented question for the Question2Query module, MSNSearch as the search engine, and top 10 hits in the Filter module).", "labels": [], "entities": [{"text": "MSNSearch", "start_pos": 143, "end_pos": 152, "type": "DATASET", "confidence": 0.8899908661842346}]}, {"text": "The algorithm based on the BLEU score, NG-AE, and its Oracle-informed variant ONG-AE, do not depend on the amount of training data available, and therefore they performed uniformly at 0.23 and 0.46, respectively).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9696669280529022}, {"text": "NG-AE", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.9082968235015869}, {"text": "ONG-AE", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.5375889539718628}]}, {"text": "The score of 0.46 can be interpreted as a performance ceiling of the AE algorithms given the available set of potential answers.", "labels": [], "entities": [{"text": "AE", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.6533515453338623}]}, {"text": "The algorithms based on the noisy-channel architecture displayed increased performance with the increase in the amount of available training data, reaching as high as 0.38.", "labels": [], "entities": []}, {"text": "An interesting observation is that the extraction algorithm using Model 1 (M1-AE) performed poorer than the extraction algorithm using Model 0 (M0-AE), for the available training data.", "labels": [], "entities": []}, {"text": "Our explanation is that the probability distribution of question terms given answer terms learnt by Model 1 is well informed (many mappings are allowed) but badly distributed, whereas the probability distribution learnt by Model 0 is poorly informed (indeed, only one mapping is allowed), but better distributed.", "labels": [], "entities": []}, {"text": "Note the steep learning curve of Model 1, whose performance gets increasingly better as the distribution probabilities of various answer terms (including the NULL word) become more informed (more mappings are learnt), compared to the gentle learning curve of Model 0, whose performance increases slightly only as more words become known as self-translations to the system (and the distribution of the NULL word gets better approximated).", "labels": [], "entities": []}, {"text": "From the above analysis, it follows that a model whose probability distribution of question terms given answer terms is both well informed and well distributed is likely to outperform both M1-AE and M0-AE.", "labels": [], "entities": []}, {"text": "Such a model was obtained when Model 1 was trained on both the question/answer parallel corpus from Section 3 and an artificially created parallel corpus in which each question had itself as its \"translation\".", "labels": [], "entities": []}, {"text": "This training regime allowed the model to assign high probabilities to identity mappings (and therefore be better distributed), while also distributing some probability mass to other questionanswer term pairs (and therefore be well informed).", "labels": [], "entities": []}, {"text": "We call the extraction algorithm that uses this model M1e-AE, and the top score of 0.38 was obtained by M1e-AE when trained on 1 million question/answer pairs.", "labels": [], "entities": []}, {"text": "Note that the learning curve of algorithm M1e-AE in indeed indicates that this answer extraction procedure is well informed about the distribution probabilities of various answer terms (it has the same steepness in the learning curve as for M1-AE), while at the same time uses a better distribution of the probability mass for each answer term compared to M1-AE (it outperforms M1-AE by roughly a constant amount for each training set size in the evaluation).", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.8293746113777161}]}], "tableCaptions": []}