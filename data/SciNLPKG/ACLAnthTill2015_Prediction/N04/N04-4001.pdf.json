{"title": [{"text": "Using N-Grams to Understand the Nature of Summaries", "labels": [], "entities": [{"text": "Summaries", "start_pos": 42, "end_pos": 51, "type": "TASK", "confidence": 0.6160625219345093}]}], "abstractContent": [{"text": "Although single-document summarization is a well-studied task,", "labels": [], "entities": [{"text": "single-document summarization", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.5948681235313416}]}], "introductionContent": [{"text": "The explosion of available online text has made it necessary to be able to present information in a succinct, navigable manner.", "labels": [], "entities": []}, {"text": "The increased accessibility of worldwide online news sources and the continually expanding size of the worldwide web place demands on users attempting to wade through vast amounts of text.", "labels": [], "entities": []}, {"text": "Document clustering and multi-document summarization technologies working in tandem promise to ease some of the burden on users when browsing related documents.", "labels": [], "entities": [{"text": "Document clustering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.846820741891861}, {"text": "multi-document summarization", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.6449083983898163}]}, {"text": "Summarizing a set of documents brings about challenges that are not present when summarizing a single document.", "labels": [], "entities": []}, {"text": "One might expect that a good multidocument summary will present a synthesis of multiple views of the event being described over different documents, or present a high-level view of an event that is not explicitly reflected in any single document.", "labels": [], "entities": []}, {"text": "A useful multi-document summary may also indicate the presence of new or distinct information contained within a set of documents describing the same topic).", "labels": [], "entities": []}, {"text": "To meet these expectations, a multi-document summary is required to generalize, condense and merge information coming from multiple sources.", "labels": [], "entities": []}, {"text": "Although single-document summarization is a wellstudied task (see for an overview), multi-document summarization is only recently being studied closely.", "labels": [], "entities": [{"text": "single-document summarization", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.5424773693084717}, {"text": "multi-document summarization", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.6984351873397827}]}, {"text": "While close attention has been paid to multi-document summarization technologies (, the inherent properties of humanwritten multi-document summaries have not yet been quantified.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.5543830692768097}]}, {"text": "In this paper, we seek to empirically characterize ideal multi-document summaries in part by attempting to answer the questions: Can multi-document summaries that are written by humans be characterized as extractive or generative?", "labels": [], "entities": []}, {"text": "Are multi-document summaries less extractive than single-document summaries?", "labels": [], "entities": []}, {"text": "Our aim in answering these questions is to discover how the nature of multi-document summaries will impact our system requirements.", "labels": [], "entities": []}, {"text": "We have chosen to focus our experiments on the data provided for summarization evaluation during the Document Understanding Conference (DUC).", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.9449036419391632}, {"text": "Document Understanding Conference (DUC)", "start_pos": 101, "end_pos": 140, "type": "TASK", "confidence": 0.6852350731690725}]}, {"text": "While we recognize that other summarization corpora may exhibit different properties than what we report, the data prepared for DUC evaluations is widely used, and continues to be a powerful force in shaping directions in summarization research and evaluation.", "labels": [], "entities": [{"text": "summarization corpora", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.9078759849071503}, {"text": "summarization", "start_pos": 222, "end_pos": 235, "type": "TASK", "confidence": 0.9879223704338074}]}, {"text": "In the following section we describe previous work related to investigating the potential for extractive summaries.", "labels": [], "entities": [{"text": "extractive summaries", "start_pos": 94, "end_pos": 114, "type": "TASK", "confidence": 0.8261286318302155}]}, {"text": "Section 3 describes anew approach for assessing the degree to which a summary can be described as extractive, and reports our findings for both single and multiple document summarization tasks.", "labels": [], "entities": []}, {"text": "We conclude with a discussion of our findings in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we used data made available from the 2001 Document Understanding Conference (DUC), an annual large-scale evaluation of summarization systems sponsored by the National Institute of Standards and Technology (NIST).", "labels": [], "entities": [{"text": "2001 Document Understanding Conference (DUC)", "start_pos": 57, "end_pos": 101, "type": "TASK", "confidence": 0.6175897887774876}]}, {"text": "In this corpus, NIST has gathered documents describing 60 events, taken from the Associated Press, Wall Street Journal, FBIS San Jose Mercury, and LA Times newswires.", "labels": [], "entities": [{"text": "Wall Street Journal, FBIS San Jose Mercury", "start_pos": 99, "end_pos": 141, "type": "DATASET", "confidence": 0.7652300633490086}]}, {"text": "An event is described by between 3 and 20 separate (but not necessarily unique) documents; on average a cluster contains 10 documents.", "labels": [], "entities": []}, {"text": "Of the 60 available clusters, we used the portion specifically designated for training, which contains a total of 295 documents distributed over 30 clusters.", "labels": [], "entities": []}, {"text": "As part of the DUC 2001 summarization corpus, NIST also provides four hand-written summaries of different lengths for every document cluster, as well as 100-word summaries of each document.", "labels": [], "entities": [{"text": "DUC 2001 summarization corpus", "start_pos": 15, "end_pos": 44, "type": "DATASET", "confidence": 0.8546607941389084}, {"text": "NIST", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.880074679851532}]}, {"text": "Since we wished to collectively compare single-document summaries against multi-document summaries, we used the 100-word multi-document summaries for our analysis.", "labels": [], "entities": []}, {"text": "It is important to note that for each cluster, all summaries (50, 100, 200 and 400-word multi-document and 100-word per-document) have been written by the same author.", "labels": [], "entities": []}, {"text": "NIST used a total often authors, each providing summaries for 3 of the 30 topics.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9812812209129333}]}, {"text": "The instructions provided did not differ per task; in both single and multi-document scenarios, the authors were directed to use complete sentences and told to feel free to use their own words.", "labels": [], "entities": []}, {"text": "To compare the text of human-authored multidocument summaries to the full-text documents describing the events, we automatically broke the documents into sentences, and constructed a minimal tiling of each summary sentence.", "labels": [], "entities": []}, {"text": "Specifically, for each sentence in the summary, we searched for all n-grams that are present in both the summary and the documents, placing no restrictions on the potential size of an n-gram.", "labels": [], "entities": []}, {"text": "We then covered each summary sentence with the ngrams, optimizing to use as few n-grams as possible (i.e. favoring n-grams that are longer in length).", "labels": [], "entities": []}, {"text": "For this experiment, we normalized the data by converting all terms to lowercase and removing punctuation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  summarizes the findings we have presented in this  section.", "labels": [], "entities": []}]}