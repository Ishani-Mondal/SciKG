{"title": [], "abstractContent": [{"text": "This paper describes the application of discrim-inative reranking techniques to the problem of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.8384317457675934}]}, {"text": "For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked-best list of candidate translations in the target language.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.7090851068496704}]}, {"text": "We introduce two novel perceptron-inspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.6970110237598419}, {"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9936586022377014}]}, {"text": "We provide experimental results on the NIST 2003 Chinese-English large data track evaluation.", "labels": [], "entities": [{"text": "NIST 2003 Chinese-English large data track evaluation", "start_pos": 39, "end_pos": 92, "type": "DATASET", "confidence": 0.9630565302712577}]}, {"text": "We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-the-art performance in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.7466611564159393}]}], "introductionContent": [{"text": "The noisy-channel model () has been the foundation for statistical machine translation (SMT) for over ten years.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 55, "end_pos": 92, "type": "TASK", "confidence": 0.8253419597943624}]}, {"text": "Recently so-called reranking techniques, such as maximum entropy models) and gradient methods, have been applied to machine translation (MT), and have provided significant improvements.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.8408390879631042}]}, {"text": "In this paper, we introduce two novel machine learning algorithms specialized for the MT task.", "labels": [], "entities": [{"text": "MT task", "start_pos": 86, "end_pos": 93, "type": "TASK", "confidence": 0.9430274665355682}]}, {"text": "Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.6277077496051788}, {"text": "tagging", "start_pos": 106, "end_pos": 113, "type": "TASK", "confidence": 0.9533417224884033}]}, {"text": "Discriminative reranking algorithms used for these applications include Perceptron, Boosting and Support Vector Machines (SVMs).", "labels": [], "entities": []}, {"text": "In the machine learning community, some novel discriminative ranking (also called ordinal regression) algorithms have been proposed in recent years.", "labels": [], "entities": []}, {"text": "Based on this work, in this paper, we will present some novel discriminative reranking techniques applied to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.8246623873710632}]}, {"text": "The reranking problem for natural language is neither a classification problem nor a regression problem, and under certain conditions MT reranking turns out to be quite different from parse reranking.", "labels": [], "entities": [{"text": "MT reranking", "start_pos": 134, "end_pos": 146, "type": "TASK", "confidence": 0.87462118268013}]}, {"text": "In this paper, we consider the special issues of applying reranking techniques to the MT task and introduce two perceptron-like reranking algorithms for MT reranking.", "labels": [], "entities": [{"text": "MT task", "start_pos": 86, "end_pos": 93, "type": "TASK", "confidence": 0.9388293027877808}, {"text": "MT reranking", "start_pos": 153, "end_pos": 165, "type": "TASK", "confidence": 0.9004526734352112}]}, {"text": "We provide experimental results that show that the proposed algorithms achieve start-of-the-art results on the NIST 2003 Chinese-English large data track evaluation.", "labels": [], "entities": [{"text": "NIST 2003 Chinese-English large data track evaluation", "start_pos": 111, "end_pos": 164, "type": "DATASET", "confidence": 0.9670017617089408}]}], "datasetContent": [{"text": "We provide experimental results on the NIST 2003 Chinese-English large data track evaluation.", "labels": [], "entities": [{"text": "NIST 2003 Chinese-English large data track evaluation", "start_pos": 39, "end_pos": 92, "type": "DATASET", "confidence": 0.9630565302712577}]}, {"text": "We use the data set used in.", "labels": [], "entities": []}, {"text": "The training data consists of about 170M English words, on which the baseline translation system is trained.", "labels": [], "entities": []}, {"text": "The training data is also used to build language models which are used to define feature functions on various syntactic levels.", "labels": [], "entities": []}, {"text": "The development data consists of 993 Chinese sentences.", "labels": [], "entities": []}, {"text": "Each Chinese sentence is associated with 1000-best English translations generated by the baseline MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9516861438751221}]}, {"text": "The development data set is used to estimate the parameters for the feature functions for the purpose of reranking.", "labels": [], "entities": []}, {"text": "The: BLEU scores reported in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9993628859519958}]}, {"text": "Every single feature was combined with the 6 baseline features for the training and test.", "labels": [], "entities": []}, {"text": "The minimum error training were used as baseline features.", "labels": [], "entities": []}, {"text": "Each of the 450 features was evaluated independently by combining it with 6 baseline features and assessing on the test data with the minimum error training.", "labels": [], "entities": [{"text": "minimum error training", "start_pos": 134, "end_pos": 156, "type": "METRIC", "confidence": 0.7234169642130533}]}, {"text": "The baseline BLEU score on the test set is 31.6%.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9661654829978943}]}, {"text": "shows some of the best performing features.", "labels": [], "entities": []}, {"text": "In, aggressive search was used to combine features.", "labels": [], "entities": []}, {"text": "After combining about a dozen features, the BLEU score did not improve anymore, and the score was 32.9%.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9557828605175018}]}, {"text": "It was also noticed that the major improvement came from the Model 1 feature.", "labels": [], "entities": []}, {"text": "By combining the four features, Model 1, matched parentheses, matched quotation marks and POS language model, the system achieved a BLEU score of 32.6%.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 132, "end_pos": 142, "type": "METRIC", "confidence": 0.9871927499771118}]}, {"text": "In our experiments, we will use 4 different kinds of feature combinations: \u009d Baseline: The 6 baseline features used in, such as cost of word penalty, cost of aligned template penalty.", "labels": [], "entities": []}, {"text": "We apply Algorithm 1 and 2 to the four feature sets.", "labels": [], "entities": []}, {"text": "For algorithm 1, the splitting algorithm, we set in the 1000-best translations given by the baseline MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.8184964656829834}]}, {"text": "For algorithm 2, the ordinal regression algorithm, we set the updating condition as , which means one's rank number is at most half of the other's and there are at least 20 ranks in between.", "labels": [], "entities": []}, {"text": "show the results of using Algorithm 1 and 2 with the four feature sets.", "labels": [], "entities": []}, {"text": "The \u00a5 -axis represents the number of iterations in the training.", "labels": [], "entities": [{"text": "\u00a5 -axis", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.935911218325297}]}, {"text": "The left f -axis stands for the BLEU% score on the test data, and the right f -axis stands for log of the loss function on the development data.", "labels": [], "entities": [{"text": "f -axis", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.8704586227734884}, {"text": "BLEU% score", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9780791600545248}, {"text": "f -axis", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.8890393972396851}]}, {"text": "Algorithm 1, the splitting algorithm, converges on the first three feature sets.", "labels": [], "entities": []}, {"text": "The smaller the feature set is, the faster the algorithm converges.", "labels": [], "entities": []}, {"text": "It achieves a BLEU score of 31.7% on the Baseline, 32.8% on the Best Feature, but only 32.6% on the Top Twenty features.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9858453869819641}, {"text": "Baseline", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9044745564460754}]}, {"text": "However it is within the range of 95% confidence.", "labels": [], "entities": []}, {"text": "Unfortunately on the Large Set, Algorithm 1 converges very slowly.", "labels": [], "entities": [{"text": "Large Set", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.9002317190170288}]}, {"text": "In the Top Twenty set there area fewer number of individually non-discriminative feature making the pool of features \"better\".", "labels": [], "entities": [{"text": "Top Twenty set", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.7599226236343384}]}, {"text": "In addition, generalization performance in the Top Twenty set is better than the Large Set due to the smaller set of \"better\" features, cf. ().", "labels": [], "entities": [{"text": "Top Twenty set", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9023755590120951}]}, {"text": "If the number of the non-discriminative features is large enough, the data set becomes unsplittable.", "labels": [], "entities": []}, {"text": "We have tried using the \u00b9 trick as in () to make data separable artificially, but the performance could not be improved with such features.", "labels": [], "entities": []}, {"text": "We achieve similar results with Algorithm 2, the ordinal regression with uneven margin.", "labels": [], "entities": []}, {"text": "It converges on the first 3 feature sets too.", "labels": [], "entities": []}, {"text": "On the Baseline, it achieves 31.4%.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.9543899893760681}]}, {"text": "We notice that the model is over-trained on the development data according to the learning curve.", "labels": [], "entities": []}, {"text": "In the Best Feature category, it achieves 32.7%, and on the Top Twenty features, it achieves 32.9%.", "labels": [], "entities": []}, {"text": "This algorithm does not converge on the Large Set in 10000 iterations.", "labels": [], "entities": []}, {"text": "We compare our perceptron-like algorithms with the minimum error training used in as shown in.", "labels": [], "entities": []}, {"text": "The splitting algorithm achieves slightly better results on the Baseline and the Best Feature set, while the minimum error training and the regression algorithm tie for first place on feature combinations.", "labels": [], "entities": [{"text": "minimum error training", "start_pos": 109, "end_pos": 131, "type": "METRIC", "confidence": 0.8081677357355753}]}, {"text": "However, the differences are not significant.", "labels": [], "entities": []}, {"text": "We notice in those separable feature sets the performance on the development data and the test data are tightly consistent.", "labels": [], "entities": []}, {"text": "Whenever the log-loss on the development set is decreased, and BLEU score on the test set goes up, and vice versa.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9784363508224487}]}, {"text": "This tells us the merit of these two algorithms; By optimizing on the loss function for the development data, we can improve performance on the test data.", "labels": [], "entities": []}, {"text": "This property is guaranteed by the theoretical analysis and is borne out in the experimental results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores reported in", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987533092498779}]}, {"text": " Table 2: Comparison between the minimum error  training with discriminative reranking on the test data  (BLEU%)", "labels": [], "entities": [{"text": "minimum error  training", "start_pos": 33, "end_pos": 56, "type": "METRIC", "confidence": 0.7300071318944296}, {"text": "BLEU%)", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9453405737876892}]}]}