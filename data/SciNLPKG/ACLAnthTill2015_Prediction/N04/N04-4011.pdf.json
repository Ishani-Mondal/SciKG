{"title": [{"text": "Performance Evaluation and Error Analysis for Multimodal Reference Resolution in a Conversation System", "labels": [], "entities": [{"text": "Error Analysis", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.6501718908548355}, {"text": "Multimodal Reference Resolution", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.7297841906547546}]}], "abstractContent": [{"text": "Multimodal reference resolution is a process that automatically identifies what users refer to during multimodal human-machine conversation.", "labels": [], "entities": [{"text": "Multimodal reference resolution", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7845245997111002}]}, {"text": "Given the substantial work on multimodal reference resolution; it is important to evaluate the current state of the art, understand the limitations, and identify directions for future improvement.", "labels": [], "entities": [{"text": "multimodal reference resolution", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.6713682214419047}]}, {"text": "We conducted a series of user studies to evaluate the capability of reference resolution in a multimodal conversation system.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7872121036052704}]}, {"text": "This paper analyzes the main error sources during real-time human-machine interaction and presents key strategies for designing robust multimodal reference resolution algorithms.", "labels": [], "entities": [{"text": "multimodal reference resolution", "start_pos": 135, "end_pos": 166, "type": "TASK", "confidence": 0.62436510125796}]}], "introductionContent": [], "datasetContent": [{"text": "We conducted several user studies to evaluate the performance of real time reference resolution using the graph-based approach.", "labels": [], "entities": [{"text": "real time reference resolution", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.6108609884977341}]}, {"text": "Eleven subjects participated in these studies.", "labels": [], "entities": []}, {"text": "Each of them was asked to interact with the system using both speech and gestures (point and circle) to accomplish five tasks.", "labels": [], "entities": []}, {"text": "For example, one task was to find the least expensive house in the most populated town.", "labels": [], "entities": []}, {"text": "The voice from each subject was trained individually to minimize speech recognition errors.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.6851459890604019}]}, {"text": "summarizes the referring behavior observed in the studies and the performance of the system.", "labels": [], "entities": []}, {"text": "The columns indicate whether there was no gesture, one gesture (point or circle), or multiple gestures involved in the input.", "labels": [], "entities": []}, {"text": "The rows indicate the type of referring expressions in the speech utterances.", "labels": [], "entities": []}, {"text": "Each table entry shows the system performance on resolving a particular combination of speech and gesture inputs.", "labels": [], "entities": []}, {"text": "For example, the entry at <S2, G4> indicates that 35 inputs consist of demonstrative singular noun phrases (as the referring expressions) and a single circle gesture.", "labels": [], "entities": []}, {"text": "Out of these inputs, 27 were correctly recognized and eight were incorrectly recognized by the speech recognizer.", "labels": [], "entities": []}, {"text": "Out of the 27 correctly recognized inputs, 26 were correctly assigned referents by the system.", "labels": [], "entities": []}, {"text": "Out of the eight incorrectly recognized inputs, references in two inputs were correctly resolved.", "labels": [], "entities": []}, {"text": "Consistent with earlier findings, the majority of user references were simple which only involved one referring expression and one gesture as shown in (i.e., S1 to S8, with column G2 and G4).", "labels": [], "entities": []}, {"text": "However, we have also found that 14% (31/219) of the inputs were complex, which involved multiple referring expressions from speech utterances (see the row S9).", "labels": [], "entities": []}, {"text": "Some of these inputs did not have any accompanied gesture (e.g., <S9, G1>).", "labels": [], "entities": []}, {"text": "Some were accompanied by one gesture (e.g., <S9, G4>) or multiple gestures (e.g., <S9, G3> and <S9, G5>).", "labels": [], "entities": []}, {"text": "The referents to these referring expressions could come from user's gestures, or from the conversation context, or from the graphic display.", "labels": [], "entities": []}, {"text": "To resolve these types of references, the graph-based approach is effective by simultaneously considering the semantic, temporal, and", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance evaluation of the graph-matching approach to multimodal reference resolution. In each entry form  \"a(b), c(d)\", \"a\" indicates the number of inputs in which the referring expressions were correctly recognized by the speech  recognizer; \"b\" indicates the number of inputs in which the referring expressions were correctly recognized and were cor- rectly resolved; \"c\" indicates the number of inputs in which the referring expressions were not correctly recognized; \"d\"  indicates the number of inputs in which the referring expressions were not correctly recognized, but were correctly resolved.  The sum of \"a\" and \"c\" gives the total number of inputs with a particular combination of speech and gesture.", "labels": [], "entities": [{"text": "multimodal reference resolution", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.7061742941538492}]}]}