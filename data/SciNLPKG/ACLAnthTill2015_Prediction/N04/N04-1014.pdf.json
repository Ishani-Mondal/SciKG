{"title": [], "abstractContent": [{"text": "Many probabilistic models for natural language are now written in terms of hierarchical tree structure.", "labels": [], "entities": []}, {"text": "Tree-based modeling still lacks many of the standard tools taken for granted in (finite-state) string-based modeling.", "labels": [], "entities": []}, {"text": "The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature.", "labels": [], "entities": []}, {"text": "We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-to-tree and tree-to-string transducers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Much of natural language work over the past decade has employed probabilistic finite-state transducers (FSTs) operating on strings.", "labels": [], "entities": []}, {"text": "This has occurred somewhat under the influence of speech recognition, where transducing acoustic sequences to word sequences is neatly captured by left-to-right stateful substitution.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7258387356996536}]}, {"text": "Many conceptual tools exist, such as Viterbi decoding and forward-backward training, as well as generic software toolkits.", "labels": [], "entities": []}, {"text": "Moreover, a surprising variety of problems are attackable with FSTs, from partof-speech tagging to letter-to-sound conversion to name transliteration.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 63, "end_pos": 67, "type": "TASK", "confidence": 0.9387786984443665}, {"text": "partof-speech tagging", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7955907881259918}, {"text": "letter-to-sound conversion", "start_pos": 99, "end_pos": 125, "type": "TASK", "confidence": 0.7086469233036041}, {"text": "name transliteration", "start_pos": 129, "end_pos": 149, "type": "TASK", "confidence": 0.7332008183002472}]}, {"text": "However, language problems like machine translation break this mold, because they involve massive reordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7676969170570374}]}, {"text": "Recently, specific probabilistic tree-based models have been proposed not only for machine translation, but also for This work was supported by DARPA contract F49620-00-1-0337 and ARDA contract MDA904-02-C-0450.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8078746795654297}, {"text": "DARPA contract F49620-00-1-0337", "start_pos": 144, "end_pos": 175, "type": "DATASET", "confidence": 0.6470751563707987}, {"text": "ARDA contract MDA904-02-C-0450", "start_pos": 180, "end_pos": 210, "type": "DATASET", "confidence": 0.7295138835906982}]}, {"text": "summarization (), paraphrasing, natural language generation (), and language modeling.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.6506711542606354}, {"text": "language modeling", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7331100851297379}]}, {"text": "It is useful to understand generic algorithms that may support all these tasks and more.", "labels": [], "entities": []}, {"text": "( and) independently introduced tree transducers as a generalization of FSTs.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.6113889217376709}]}, {"text": "Rounds was motivated by natural language.", "labels": [], "entities": []}, {"text": "The Rounds tree transducer is very similar to a left-to-right FST, except that it works top-down, pursuing subtrees in parallel, with each subtree transformed depending only on its own passed-down state.", "labels": [], "entities": []}, {"text": "This class of transducer is often nowadays called R, for \"Root-to-frontier\".", "labels": [], "entities": []}, {"text": "Rounds uses a mathematics-oriented example of an R transducer, which we summarize in.", "labels": [], "entities": []}, {"text": "At each point in the top-down traversal, the transducer chooses a production to apply, based only on the current state and the current root symbol.", "labels": [], "entities": []}, {"text": "The traversal continues until there are no more state-annotated nodes.", "labels": [], "entities": []}, {"text": "Nondeterministic transducers may have several productions with the same left-hand side, and therefore some free choices to make during transduction.", "labels": [], "entities": []}, {"text": "An R transducer compactly represents a potentiallyinfinite set of input/output tree pairs: exactly those pairs (T1, T2) for which some sequence of productions applied to T1 (starting in the initial state) results in T2.", "labels": [], "entities": []}, {"text": "This is similar to an FST, which compactly represents a set of input/output string pairs, and in fact, R is a generalization of FST.", "labels": [], "entities": [{"text": "FST", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.5284239053726196}, {"text": "FST", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.8926678895950317}]}, {"text": "If we think of strings written down vertically, as degenerate trees, we can convert any FST into an R transducer by automatically replacing FST transitions with R productions.", "labels": [], "entities": []}, {"text": "R does have some extra power beyond path following: A sample R tree transducer that takes the derivative of its input. and state-based record keeping.", "labels": [], "entities": [{"text": "state-based record keeping", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.6250739097595215}]}, {"text": "It can copy whole subtrees, and transform those subtrees differently.", "labels": [], "entities": []}, {"text": "It can also delete subtrees without inspecting them (imagine by analogy an FST that quits and accepts right in the middle of an input string).", "labels": [], "entities": []}, {"text": "Variants of R that disallow copying and deleting are called RL (for linear) and RN (for nondeleting), respectively.", "labels": [], "entities": []}, {"text": "One advantage of working with tree transducers is the large and useful body of literature about these automata; two excellent surveys are ( and ().", "labels": [], "entities": []}, {"text": "For example, R is not closed under composition, and neither are RL or F (the \"frontier-to-root\" cousin of R), but the non-copying FL is closed under composition.", "labels": [], "entities": []}, {"text": "Many of these composition results are first found in.", "labels": [], "entities": []}, {"text": "R has surprising ability to change the structure of an input tree.", "labels": [], "entities": []}, {"text": "For example, it may not be initially obvious how an R transducer can transform the English structure S(PRO, VP(V, NP)) into the Arabic equivalent S(V, PRO, NP), as it is difficult to move the subject PRO into position between the verb V and the direct object NP.", "labels": [], "entities": []}, {"text": "First, R productions have no lookahead capability-the left-handside of the S production consists only of q S(x0, x1), although we want our English-to-Arabic transformation to apply only when it faces the entire structure q S(PRO, VP(V, NP)).", "labels": [], "entities": []}, {"text": "However, we can simulate lookahead using states, as in these productions: -q S(x0, x1) \u2192 S(qpro x0, qvp.v.np x1) -qpro PRO \u2192 PRO -qvp.v.np VP(x0, x1) \u2192 VP(qv x0, qnp x1) By omitting rules like qpro NP \u2192 ..., we ensure that the entire production sequence will dead-end unless the first child of the input tree is in fact PRO.", "labels": [], "entities": []}, {"text": "So finite lookahead is not a problem.", "labels": [], "entities": []}, {"text": "The next problem is how to get the PRO to appear in between the V and NP, as in Arabic.", "labels": [], "entities": []}, {"text": "This can be carried out using copying.", "labels": [], "entities": []}, {"text": "We make two copies of the English VP, and assign them different states: While general properties of R are understood, there are many algorithmic questions.", "labels": [], "entities": []}, {"text": "In this paper, we take on the problem of training probabilistic R transducers.", "labels": [], "entities": []}, {"text": "For many language problems (machine translation, paraphrasing, text compression, etc.), it is possible to collect training data in the form of tree pairs and to distill linguistic knowledge automatically.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7817582488059998}, {"text": "text compression", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.7158055901527405}]}, {"text": "Our problem statement is: Given (1) a particular transducer with productions P, and (2) a finite training set of sample input/output tree pairs, we want to produce (3) a probability estimate for each production in P such that we maximize the probability of the output trees given the input trees.", "labels": [], "entities": []}, {"text": "As organized in the rest of this paper, we accomplish this by intersecting the given transducer with each input/output pair in turn.", "labels": [], "entities": []}, {"text": "Each such intersection produces a set of weighted derivations that are packed into a regular tree grammar (Sections 3-5), which is equivalent to a tree substitution grammar.", "labels": [], "entities": []}, {"text": "The inside and outside probabilities of this packed derivation structure are used to compute expected counts of the productions from the original, given transducer (Sections 6-7).", "labels": [], "entities": []}, {"text": "Section 9 gives a sample transducer implementing a published machine translation model; some readers may wish to skip to this section directly.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.6855795532464981}]}], "datasetContent": [], "tableCaptions": []}