{"title": [{"text": "Augmenting the kappa statistic to determine interannotator reliability for multiply labeled data points", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a method for evaluating interannotator reliability in an email corpus annotated for type (e.g., question, answer, social chat) when annotators are allowed to assign multiple labels to a message.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reliable annotated data are necessary fora wide variety of natural language processing tasks.", "labels": [], "entities": []}, {"text": "Machine learning algorithms commonly employed to tackle language problems from syntactic parsing to prosodic analysis and information retrieval all require annotated data for training and testing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7310948073863983}, {"text": "prosodic analysis", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.730659157037735}, {"text": "information retrieval", "start_pos": 122, "end_pos": 143, "type": "TASK", "confidence": 0.765557736158371}]}, {"text": "The reliability of these computational solutions is intricately tied to the accuracy of the annotated data used in their development.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.998999297618866}]}, {"text": "Human error and subjectivity make deciding the accuracy of annotations an intractable problem.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9447027444839478}]}, {"text": "While the objective correctness of human annotations cannot be determined algorithmically, the degree to which the annotators agree in their labeling of a corpus can be quickly and simply statistically determined using kappa measure.", "labels": [], "entities": []}, {"text": "Because human artifacts are less likely to co-occur simultaneously in two annotators, the kappa statistic is used to measure interannotator reliability.", "labels": [], "entities": []}, {"text": "This paper will describe an email classification and summarization project which presented a problem for interlabeler reliability computation since annotators were allowed to label data with one or two labels).", "labels": [], "entities": [{"text": "email classification and summarization", "start_pos": 28, "end_pos": 66, "type": "TASK", "confidence": 0.7604403719305992}]}, {"text": "The existing kappa statistic computation does not obviously extend to accommodate the presence of a secondary label.", "labels": [], "entities": []}, {"text": "The augmentation to the algorithm presented in this paper allows for both a more accurate assessment of interannotator reliability and a unique insight into the data and how the annotators have employed the optional second label.", "labels": [], "entities": []}, {"text": "Section 2 will describe the categorization project.", "labels": [], "entities": []}, {"text": "Section 3 will present a description of the annotated corpus.", "labels": [], "entities": []}, {"text": "Section 4 will describe why the kappa statistic for determining interannotator agreement in its basic form cannot effectively be applied to this corpus.", "labels": [], "entities": []}, {"text": "Section 5 will present away to augment the algorithm computing kappa statistic to provide greater insight into user annotations.", "labels": [], "entities": []}, {"text": "Section 6 will analyze the results of applying this new algorithm to the annotated corpus.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 shows a sample set of annotations on 5 mes- sages by annotator A. Table 2 shows the resulting M A  based on the annotation data in Table 1 where p=0.6.", "labels": [], "entities": []}, {"text": " Table 2. M A based on Table 1 data (p=0.6;N=5).", "labels": [], "entities": []}, {"text": " Table 3 shows  Freq A based on M A from Table 2.", "labels": [], "entities": [{"text": "Freq A", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9463487565517426}]}, {"text": " Table 3. Freq A from M A in Table 2 (p=0.6;N=5).", "labels": [], "entities": [{"text": "Freq A", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9433204233646393}]}]}