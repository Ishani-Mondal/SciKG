{"title": [{"text": "Accurate Information Extraction from Research Papers using Conditional Random Fields", "labels": [], "entities": [{"text": "Accurate Information Extraction from Research Papers", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8404581050078074}]}], "abstractContent": [{"text": "With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9991357922554016}]}, {"text": "This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers.", "labels": [], "entities": []}, {"text": "The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration.", "labels": [], "entities": []}, {"text": "This paper makes an empirical exploration of several factors , including variations on Gaussian, exponential and hyperbolic-L 1 priors for improved regularization, and several classes of features and Markov order.", "labels": [], "entities": []}, {"text": "On a standard benchmark data set, we achieve new state-of-the-art performance , reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results.", "labels": [], "entities": [{"text": "error", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9851436018943787}, {"text": "F1", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.8049910664558411}, {"text": "word error rate", "start_pos": 121, "end_pos": 136, "type": "METRIC", "confidence": 0.8118138114611307}]}, {"text": "Accuracy compares even more favorably against HMMs.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9942736029624939}]}], "introductionContent": [{"text": "Research paper search engines, such as CiteSeer () and Cora (), give researchers tremendous power and convenience in their research.", "labels": [], "entities": [{"text": "Cora", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.7862288355827332}]}, {"text": "They are also becoming increasingly used for recruiting and hiring decisions.", "labels": [], "entities": [{"text": "recruiting", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.9641342163085938}]}, {"text": "Thus the information quality of such systems is of significant importance.", "labels": [], "entities": []}, {"text": "This quality critically depends on an information extraction component that extracts meta-data, such as title, author, institution, etc, from paper headers and references, because these meta-data are further used in many component applications such as field-based search, author analysis, and citation analysis.", "labels": [], "entities": [{"text": "author analysis", "start_pos": 272, "end_pos": 287, "type": "TASK", "confidence": 0.7985711693763733}, {"text": "citation analysis", "start_pos": 293, "end_pos": 310, "type": "TASK", "confidence": 0.939612627029419}]}, {"text": "Previous work in information extraction from research papers has been based on two major machine learning techniques.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.8393936455249786}]}, {"text": "The first is hidden Markov models (HMM)).", "labels": [], "entities": []}, {"text": "An HMM learns a generative model over input sequence and labeled sequence pairs.", "labels": [], "entities": []}, {"text": "While enjoying wide historical success, standard HMM models have difficulty modeling multiple non-independent features of the observation sequence.", "labels": [], "entities": []}, {"text": "The second technique is based on discriminatively-trained SVM classifiers ().", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.7921735942363739}]}, {"text": "These SVM classifiers can handle many nonindependent features.", "labels": [], "entities": []}, {"text": "However, for this sequence labeling problem, work in a two stages process: first classifying each line independently to assign it label, then adjusting these labels based on an additional classifier that examines larger windows of labels.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.6147017776966095}]}, {"text": "Solving the information extraction problem in two steps looses the tight interaction between state transitions and observations.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.8481858670711517}]}, {"text": "In this paper, we present results on this research paper meta-data extraction task using a Conditional Random Field (), and explore several practical issues in applying CRFs to information extraction in general.", "labels": [], "entities": [{"text": "meta-data extraction task", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.8245857159296671}, {"text": "information extraction", "start_pos": 177, "end_pos": 199, "type": "TASK", "confidence": 0.8372974693775177}]}, {"text": "The CRF approach draws together the advantages of both finite state HMM and discriminative SVM techniques by allowing use of arbitrary, dependent features and joint inference over entire sequences.", "labels": [], "entities": []}, {"text": "CRFs have been previously applied to other tasks such as name entity extraction, table extraction () and shallow parsing.", "labels": [], "entities": [{"text": "name entity extraction", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.7568954428037008}, {"text": "table extraction", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.8324971497058868}, {"text": "shallow parsing", "start_pos": 105, "end_pos": 120, "type": "TASK", "confidence": 0.7135987281799316}]}, {"text": "The basic theory of CRFs is now well-understood, but the best-practices for applying them to new, real-world data is still in an early-exploration phase.", "labels": [], "entities": []}, {"text": "Here we explore two key practical issues: (1) regularization, with an empirical study of Gaussian), exponential, and hyperbolic-L 1 ( priors; (2) exploration of various families of features, including text, lexicons, and layout, as well as proposing a method for the beneficial use of zero-count features without incurring large memory penalties.", "labels": [], "entities": []}, {"text": "We describe a large collection of experimental results on two traditional benchmark data sets.", "labels": [], "entities": []}, {"text": "Dramatic improvements are obtained in comparison with previous SVM and HMM based results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with two datasets of research paper content.", "labels": [], "entities": []}, {"text": "One consists of the headers of research papers.", "labels": [], "entities": []}, {"text": "The other consists of pre-segmented citations from the reference sections of research papers.", "labels": [], "entities": []}, {"text": "These data sets have been used as standard benchmarks in several previous studies).", "labels": [], "entities": []}, {"text": "The header of a research paper is defined to be all of the words from the beginning of the paper up to either the first section of the paper, usually the introduction, or to the end of the first page, whichever occurs first.", "labels": [], "entities": []}, {"text": "It contains 15 fields to be extracted: title, author, affiliation, address, note, email, date, abstract, introduction, phone, keywords, web, degree, publication number, and page ().", "labels": [], "entities": []}, {"text": "The header dataset contains 935 headers.", "labels": [], "entities": []}, {"text": "Following previous research), for each trial we randomly select 500 for training and the remaining 435 for testing.", "labels": [], "entities": []}, {"text": "We refer this dataset as H.", "labels": [], "entities": []}, {"text": "The reference dataset was created by the Cora project ().", "labels": [], "entities": [{"text": "Cora project", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.9236498177051544}]}, {"text": "It contains 500 references, we use 350 for training and the rest 150 for testing.", "labels": [], "entities": []}, {"text": "References contain 13 fields: author, title, editor, booktitle, date, journal, volume, tech, institution, pages, location, publisher, note.", "labels": [], "entities": []}, {"text": "We refer this dataset as R.", "labels": [], "entities": []}, {"text": "We first report the overall results by comparing CRFs with HMMs, and with the previously best benchmark results obtained by SVMs ( shows the results on dataset R.", "labels": [], "entities": []}, {"text": "SVM results are not available for these datasets.", "labels": [], "entities": [{"text": "SVM", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.4728095531463623}]}], "tableCaptions": [{"text": " Table 1 shows the results on dataset H with the best re- sults in bold; (intro and page fields are not shown, fol- lowing past practice (Seymore et al., 1999; Han et al.,  2003)). The results we obtained with CRFs use second- order state transition features, layout features, as well as  supported and unsupported features. Feature induction  is used in experiments on dataset R; (it didn't improve  accuracy on H). The results we obtained with the HMM  model use a second order model for transitions, and a first  order for observations. The results on SVM is obtained  from (Han et al., 2003) by computing F1 measures from  the precision and recall numbers they report.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 401, "end_pos": 409, "type": "METRIC", "confidence": 0.9962038397789001}, {"text": "precision", "start_pos": 631, "end_pos": 640, "type": "METRIC", "confidence": 0.9846195578575134}, {"text": "recall", "start_pos": 645, "end_pos": 651, "type": "METRIC", "confidence": 0.8536365628242493}]}, {"text": " Table 2: Extraction results for paper references on R", "labels": [], "entities": [{"text": "R", "start_pos": 53, "end_pos": 54, "type": "TASK", "confidence": 0.5817983150482178}]}, {"text": " Table 3: Regularization comparisons: Gaussian infinity is  non-regularized, Gaussian variance = X sets variance to  be X. Gaussian cut 7 refers to the Threshold Cut method,  Gaussian divide count refers to the Divide Count method,  Gaussian bin N refers to the Bin-Based method with bin  size equals N, as described in 2.1.1", "labels": [], "entities": []}, {"text": " Table 4: Effects of using unsupported features and state  transitions on H", "labels": [], "entities": []}, {"text": " Table 5: List of features used", "labels": [], "entities": []}]}