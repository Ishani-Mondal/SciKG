{"title": [{"text": "Identifying Semitic Roots: Machine Learning with Linguistic Constraints", "labels": [], "entities": [{"text": "Identifying Semitic Roots", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9282799959182739}]}], "abstractContent": [{"text": "Words in Semitic languages are formed by combining two morphemes: a root and a pattern.", "labels": [], "entities": []}, {"text": "The root consists of consonants only, by default three, and the pattern is a combination of vowels and consonants, with non-consecutive \"slots\" into which the root consonants are inserted.", "labels": [], "entities": []}, {"text": "Identifying the root of a given word is an important task, considered to bean essential part of the morphological analysis of Semitic languages, and information on roots is important for linguistics research as well as for practical applications.", "labels": [], "entities": [{"text": "Identifying the root of a given word", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.860369052205767}]}, {"text": "We present a machine learning approach, augmented by limited linguistic knowledge, to the problem of identifying the roots of Semitic words.", "labels": [], "entities": [{"text": "identifying the roots of Semitic words", "start_pos": 101, "end_pos": 139, "type": "TASK", "confidence": 0.8196543653806051}]}, {"text": "Although programs exist which can extract the root of words in Arabic and Hebrew, they are all dependent on labor-intensive construction of large-scale lexicons which are components of full-scale morphological analyzers.", "labels": [], "entities": []}, {"text": "The advantage of our method is an automation of this process, avoiding the bottleneck of having to laboriously list the root and pattern of each lexeme in the language.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first application of machine learning to this problem, and one of the few attempts to directly address non-concatenative morphology using machine learning.", "labels": [], "entities": []}, {"text": "More generally, our results shed light on the problem of combining classifiers under (linguistically motivated) constraints.", "labels": [], "entities": []}], "introductionContent": [{"text": "The standard account of word-formation processes in Semitic languages describes words as combinations of two morphemes: a root and a pattern.", "labels": [], "entities": []}, {"text": "The root consists of consonants only, by default three (although longer roots are known), called radicals.", "labels": [], "entities": []}, {"text": "The pattern is a combination of vowels and, possibly, consonants too, with \"slots\" into which the root consonants can be inserted.", "labels": [], "entities": []}, {"text": "Words are created by interdigitating roots of interdependent classifiers in Section 5 and demonstrate the benefits of using limited linguistic knowledge in the inference procedure.", "labels": [], "entities": []}, {"text": "Then, the same technique is applied to Arabic in Section 6 and we demonstrate comparable improvements.", "labels": [], "entities": []}, {"text": "In Section 7 we discuss the influence of global constraints on local classifiers.", "labels": [], "entities": []}, {"text": "We conclude with suggestions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "For training and testing, a Hebrew linguist manually tagged a corpus of 15,000 words (from a set of newspaper articles).", "labels": [], "entities": []}, {"text": "Of these, only 9,752 were annotated; the reason for the gap is that some Hebrew words, mainly borrowed but also some frequent words such as prepositions, are not formed by the root and pattern paradigm.", "labels": [], "entities": []}, {"text": "Such words are excluded from our experiments in this work; in an application, such words have to be identified and handled separately.", "labels": [], "entities": []}, {"text": "This can be rather easily done using simple heuristics and a small list of frequent closed-class words, because words which do not conform to the root and pattern paradigm are either (short, functional) closed-class words, or loan words which tend to be longer and, in many cases, involve \"foreign\" characters (typically proper names).", "labels": [], "entities": []}, {"text": "This problem is orthogonal to the problem of identifying the root, and hence a pipeline approach is reasonable.", "labels": [], "entities": []}, {"text": "We further eliminated 168 roots with more than three consonants and were left with 5,242 annotated word types, exhibiting 1,043 different roots.", "labels": [], "entities": []}, {"text": "shows the distribution of word types according to root ambiguity.", "labels": [], "entities": []}, {"text": "provides the distribution of the roots of the 5,242 word types in our corpus according to root type, where R i is the ith radical (note that some roots may belong to more than one group).", "labels": [], "entities": []}, {"text": "As assurance for statistical reliability, in all the experiments discussed in the sequel (unless otherwise mentioned) we performed 10-fold cross validation runs for every classification task during evaluation.", "labels": [], "entities": [{"text": "reliability", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.8503023982048035}]}, {"text": "We also divided the annotated corpus into two sets: a training set of 4,800 words and a test set of 442 words.", "labels": [], "entities": []}, {"text": "Only the training set was used for most experiments, and the results reported here refer to these data unless stated otherwise.", "labels": [], "entities": []}, {"text": "We used the training set to tune the parameter \u03b4 (see Section 5.4), and once \u03b4 was set we report on results obtained by training on the training set and testing on test data.", "labels": [], "entities": []}, {"text": "A given example is a word type with all its (manually tagged) possible roots.", "labels": [], "entities": []}, {"text": "In the experiments we describe subsequently, our system produces one or more root candidates for each example.", "labels": [], "entities": []}, {"text": "For each example, we define tp as the number of correct candidates produced by the system; fp as the number of candidates which are not correct roots; and fn as the number of roots the system did not produce.", "labels": [], "entities": []}, {"text": "As usual, we define recall as tp tp+fp , precision as tp tp+fn and F-score as 2\u00d7recall\u00d7precision recall+precision ; we then (macro-) average overall words to obtain the system's overall F-score.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9982873797416687}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9992965459823608}, {"text": "F-score", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9984194040298462}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.946365475654602}, {"text": "precision recall", "start_pos": 87, "end_pos": 103, "type": "METRIC", "confidence": 0.8588258028030396}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.7708202004432678}, {"text": "F-score", "start_pos": 186, "end_pos": 193, "type": "METRIC", "confidence": 0.9872322082519531}]}, {"text": "To estimate the difficulty of this task, we asked six human subjects to perform it.", "labels": [], "entities": []}, {"text": "Participants were asked to identify all the possible roots of all the words in a list of 200 words (without context), randomly chosen from the training corpus.", "labels": [], "entities": []}, {"text": "All participants were computer science graduates, native Hebrew speakers with no linguistic background.", "labels": [], "entities": []}, {"text": "The average precision of humans on this task is 83.52%, and with recall at 80.27%, F-score is 81.86%.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9994125366210938}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.999669075012207}, {"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9997391104698181}]}, {"text": "We conjecture that the main reasons for the low performance of our subjects are the lack of context (people tend to pick the most prominent root and ignore the less salient ones) and the ambiguity of some of the weak paradigms (Hebrew speakers are unaware of the correct root in many of the weak paradigms, even when only one exists).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Root ambiguity in the corpus.", "labels": [], "entities": []}, {"text": " Table 2  Distribution of root paradigms.", "labels": [], "entities": []}, {"text": " Table 3  Accuracy of identifying the correct radical.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9930579662322998}]}, {"text": " Table 4  Accuracy of identifying the correct radical, sequential model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9826214909553528}]}, {"text": " Table 5  Recall of identifying the correct radical among top-n candidates", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.49513500928878784}]}, {"text": " Table 6  Results: Combining dependent classifiers.", "labels": [], "entities": []}, {"text": " Table 7  Results: Combining classifiers of root radicals bigram.", "labels": [], "entities": []}, {"text": " Table 8  Results: Using linguistic constraints for inference.", "labels": [], "entities": []}, {"text": " Table 9  Error analysis: Performance of the system on different cases.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9757728576660156}]}, {"text": " Table 10  Error analysis: The weak paradigms.", "labels": [], "entities": [{"text": "Error analysis", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.8926534056663513}]}, {"text": " Table 11  Arabic root ambiguity in the corpus.", "labels": [], "entities": []}, {"text": " Table 12  Accuracy of identifying the correct radical in Arabic.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9861257672309875}]}, {"text": " Table 14  Accuracy of each classifier after applying global constraints.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9639803171157837}]}, {"text": " Table 15  Accuracy of each classifier applying the list of roots as a single constraint.", "labels": [], "entities": []}, {"text": " Table 16  Accuracy of each classifier after applying global constraints (Arabic).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9549816250801086}]}]}