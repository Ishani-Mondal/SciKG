{"title": [], "abstractContent": [{"text": "Most semantic role labeling (SRL) research has been focused on training and evaluating on the same corpus.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 5, "end_pos": 33, "type": "TASK", "confidence": 0.828409363826116}]}, {"text": "This strategy, although appropriate for initiating research, can lead to over-training to the particular corpus.", "labels": [], "entities": []}, {"text": "This article describes the operation of ASSERT, a state-of-the art SRL system, and analyzes the robustness of the system when trained on one genre of data and used to label a different genre.", "labels": [], "entities": [{"text": "ASSERT", "start_pos": 40, "end_pos": 46, "type": "TASK", "confidence": 0.8776550889015198}, {"text": "SRL", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9825795888900757}]}, {"text": "As a starting point, results are first presented for training and testing the system on the PropBank corpus, which is annotated Wall Street Journal (WSJ) data.", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.9754243493080139}, {"text": "Wall Street Journal (WSJ) data", "start_pos": 128, "end_pos": 158, "type": "DATASET", "confidence": 0.9277137262480599}]}, {"text": "Experiments are then presented to evaluate the portability of the system to another source of data.", "labels": [], "entities": []}, {"text": "These experiments are based on comparisons of performance using PropBanked WSJ data and PropBanked Brown Corpus data.", "labels": [], "entities": [{"text": "PropBanked WSJ data", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.8334489663441976}, {"text": "PropBanked Brown Corpus data", "start_pos": 88, "end_pos": 116, "type": "DATASET", "confidence": 0.9451979100704193}]}, {"text": "The results indicate that whereas syntactic parses and argument identification transfer relatively well to anew corpus, argument classification does not.", "labels": [], "entities": [{"text": "syntactic parses", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7420686781406403}, {"text": "argument identification", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7593266367912292}, {"text": "argument classification", "start_pos": 120, "end_pos": 143, "type": "TASK", "confidence": 0.8021388649940491}]}, {"text": "An analysis of the reasons for this is presented and these generally point to the nature of the more lexical/semantic features dominating the classification task where more general structural features are dominant in the argument identification task.", "labels": [], "entities": [{"text": "argument identification task", "start_pos": 221, "end_pos": 249, "type": "TASK", "confidence": 0.7955962717533112}]}], "introductionContent": [{"text": "Automatic, accurate, and wide-coverage techniques that can annotate naturally occurring text with semantic structure can play a key role in NLP applications such as information extraction, question answering (, and summarization.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 165, "end_pos": 187, "type": "TASK", "confidence": 0.8420398831367493}, {"text": "question answering", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.9114209115505219}, {"text": "summarization", "start_pos": 215, "end_pos": 228, "type": "TASK", "confidence": 0.9909629225730896}]}, {"text": "Semantic role labeling (SRL) is one method for producing such semantic structure.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.829739491144816}]}, {"text": "When presented with a sentence, a semantic role labeler should, for each predicate in the sentence, first identify and then label its semantic arguments.", "labels": [], "entities": []}, {"text": "This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning specific labels to them.", "labels": [], "entities": []}, {"text": "In the bulk of recent work, this problem has been cast as a problem in supervised machine learning.", "labels": [], "entities": []}, {"text": "Using these techniques with hand-corrected syntactic parses, it has been possible to achieve accuracies within the range of human inter-annotator agreement.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.9711407423019409}]}, {"text": "More recent approaches have involved using improved features such as n-best parses (; exploiting argument interdependence (Jiang, Li, and Ng 2005); using information from fundamentally different, and complementary syntactic, views (Pradhan,; combining hypotheses from different labeling systems using inference; as well as applying novel learning paradigms () that try to capture more sequence and contextual information.", "labels": [], "entities": []}, {"text": "Some have also tried to jointly decode the syntactic and semantic structures).", "labels": [], "entities": []}, {"text": "This problem has also been the subject of two CoNLL shared tasks.", "labels": [], "entities": [{"text": "CoNLL shared tasks", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.5090831915537516}]}, {"text": "Although all of these systems perform quite well on the standard test data, they show significant performance degradation when applied to test data drawn from a genre different from the data on which the system was trained.", "labels": [], "entities": []}, {"text": "The focus of this article is to present results from an examination into the primary causes of the lack of portability across genres of data.", "labels": [], "entities": []}, {"text": "To set the stage for these experiments we first describe the operation of ASSERT, our state-of-the art SRL system.", "labels": [], "entities": [{"text": "ASSERT", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.6364663243293762}, {"text": "SRL", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9670292139053345}]}, {"text": "Results are presented for training and testing the system on the PropBank corpus, which is annotated Wall Street Journal (WSJ) data.", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.9769242107868195}, {"text": "Wall Street Journal (WSJ) data", "start_pos": 101, "end_pos": 131, "type": "DATASET", "confidence": 0.9242037279265267}]}, {"text": "Experiments are then presented to assess the portability of the system to another genre of data.", "labels": [], "entities": []}, {"text": "These experiments are based on comparisons of performance using PropBanked WSJ data and PropBanked Brown corpus data.", "labels": [], "entities": [{"text": "PropBanked WSJ data", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.840054452419281}, {"text": "PropBanked Brown corpus data", "start_pos": 88, "end_pos": 116, "type": "DATASET", "confidence": 0.9648774713277817}]}, {"text": "The results indicate that whereas syntactic parses and identification of the argument bearing nodes transfer relatively well to anew corpus, role classification does not.", "labels": [], "entities": [{"text": "syntactic parses", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7459375262260437}, {"text": "role classification", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.8406894505023956}]}, {"text": "Analysis of the reasons for this generally point to the nature of the more lexical/semantic features dominating the classification task, as opposed to the more structural features that are relied upon for identifying which constituents are associated with arguments.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present a series of experiments comparing the performance of ASSERT on the WSJ corpus to performance on the Brown corpus.", "labels": [], "entities": [{"text": "ASSERT", "start_pos": 81, "end_pos": 87, "type": "TASK", "confidence": 0.9102583527565002}, {"text": "WSJ corpus", "start_pos": 95, "end_pos": 105, "type": "DATASET", "confidence": 0.97451451420784}, {"text": "Brown corpus", "start_pos": 128, "end_pos": 140, "type": "DATASET", "confidence": 0.9412549436092377}]}, {"text": "The intent is to understand how well the algorithms and features transfer to other sources and to understand the nature of any problems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Performance of ASSERT on WSJ test set (Section 23) using correct Treebank parses as well as  Charniak parses.", "labels": [], "entities": [{"text": "ASSERT", "start_pos": 25, "end_pos": 31, "type": "TASK", "confidence": 0.8080012202262878}, {"text": "WSJ test set", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.9491231838862101}]}, {"text": " Table 4  Number of predicates that have been tagged in the PropBanked portion of the Brown corpus.", "labels": [], "entities": [{"text": "PropBanked portion of the Brown corpus", "start_pos": 60, "end_pos": 98, "type": "DATASET", "confidence": 0.9342066546281179}]}, {"text": " Table 5  Performance on the entire PropBanked Brown corpus when ASSERT is trained on WSJ.", "labels": [], "entities": [{"text": "PropBanked Brown corpus", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.898640771706899}, {"text": "ASSERT", "start_pos": 65, "end_pos": 71, "type": "TASK", "confidence": 0.8179031610488892}, {"text": "WSJ", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9081003665924072}]}, {"text": " Table 6  Deleted/missing argument-bearing constituents in Charniak parses of the WSJ test set  (Section 23) and the entire PropBanked Brown corpus.", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.9326065381368002}, {"text": "PropBanked Brown corpus", "start_pos": 124, "end_pos": 147, "type": "DATASET", "confidence": 0.9316909710566202}]}, {"text": " Table 7  Performance when ASSERT is trained using correct Treebank parses, and is used to classify test  set from either the same genre or another. For each data set, the number of examples used for  training are shown in parentheses.", "labels": [], "entities": [{"text": "ASSERT", "start_pos": 27, "end_pos": 33, "type": "TASK", "confidence": 0.9477072358131409}]}, {"text": " Table 8  Performance on Brown test, using Brown and WSJ training sets, with and without oracle  predicate sense information when using Treebank parses.", "labels": [], "entities": [{"text": "WSJ training sets", "start_pos": 53, "end_pos": 70, "type": "DATASET", "confidence": 0.9542361497879028}]}, {"text": " Table 9  Features seen in training for various test sets.", "labels": [], "entities": []}, {"text": " Table 10  Classification accuracy for each argument type in the WSJ (W) and Brown (B) test sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9476866126060486}, {"text": "WSJ (W) and Brown (B) test sets", "start_pos": 65, "end_pos": 96, "type": "DATASET", "confidence": 0.7179127172990278}]}, {"text": " Table 11  Distribution of the named entities in a 10k data from WSJ and Brown corpora.", "labels": [], "entities": [{"text": "WSJ and Brown corpora", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.8158023953437805}]}, {"text": " Table 13  Performance on WSJ and Brown test sets when ASSERT is trained on features extracted from  automatically generated syntactic parses.", "labels": [], "entities": [{"text": "WSJ and Brown test", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.8453762531280518}, {"text": "ASSERT", "start_pos": 55, "end_pos": 61, "type": "TASK", "confidence": 0.9591689705848694}]}, {"text": " Table 14  Effect of incrementally adding data from a new genre.", "labels": [], "entities": []}]}