{"title": [{"text": "A Global Joint Model for Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7539610068003336}]}], "abstractContent": [{"text": "We present a model for semantic role labeling that effectively captures the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.6748774647712708}]}, {"text": "We show how to incorporate these strong dependencies in a statistical joint model with a rich set of features over multiple argument phrases.", "labels": [], "entities": []}, {"text": "The proposed model substantially outperforms a similar state-of-the-art local model that does not include dependencies among different arguments.", "labels": [], "entities": []}, {"text": "We evaluate the gains from incorporating this joint information on the Propbank corpus, when using correct syntactic parse trees as input, and when using automatically derived parse trees.", "labels": [], "entities": [{"text": "Propbank corpus", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.9846881628036499}]}, {"text": "The gains amount to 24.1% error reduction on all arguments and 36.8% on core arguments for gold-standard parse trees on Propbank.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 26, "end_pos": 41, "type": "METRIC", "confidence": 0.9697490930557251}, {"text": "Propbank", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.9714699387550354}]}, {"text": "For automatic parse trees, the error reductions are 8.3% and 10.3% on all and core arguments, respectively.", "labels": [], "entities": [{"text": "error reductions", "start_pos": 31, "end_pos": 47, "type": "METRIC", "confidence": 0.9764421880245209}]}, {"text": "We also present results on the CoNLL 2005 shared task data set.", "labels": [], "entities": [{"text": "CoNLL 2005 shared task data set", "start_pos": 31, "end_pos": 62, "type": "DATASET", "confidence": 0.9289767742156982}]}, {"text": "Additionally, we explore considering multiple syntactic analyses to cope with parser noise and uncertainty.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since the release of the FrameNet corpora, there has been a large amount of work on statistical models for semantic role labeling.", "labels": [], "entities": [{"text": "FrameNet corpora", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.8650338053703308}, {"text": "semantic role labeling", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.7458149790763855}]}, {"text": "Most of this work relies heavily on local classifiers: ones that decide the semantic role of each phrase independently of the roles of other phrases.", "labels": [], "entities": []}, {"text": "However, linguistic theory tells us that a core argument frame is a joint structure, with strong dependencies between arguments.", "labels": [], "entities": []}, {"text": "For instance, in the sentence", "labels": [], "entities": []}], "datasetContent": [{"text": "Since 2004, there has been a precise, standard evaluation measure for semantic role labeling, formulated by the organizers of the CoNLL shared tasks (Carreras and M` arquez).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.7404530247052511}]}, {"text": "An evaluation script is also distributed as part of the provided software for the shared task and can be used to evaluate systems on Propbank I data.", "labels": [], "entities": [{"text": "Propbank I data", "start_pos": 133, "end_pos": 148, "type": "DATASET", "confidence": 0.921114424864451}]}, {"text": "For papers published between 2000 and 2005, there are several details of the evaluation measures for semantic role labeling that make it difficult to compare results obtained by different researchers, because researchers use their own implementations of evaluation measures, without making all the exact details clear in their papers.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.6784724593162537}]}, {"text": "The first issue is the existence of arguments consisting of multiple constituents.", "labels": [], "entities": []}, {"text": "In this case it is not clear whether partial credit is to be given for guessing only some of the constituents comprising the argument correctly.", "labels": [], "entities": []}, {"text": "The second issue is whether the bracketing of constituents should be required to be recovered correctly, in other words, whether pairs of labelings, such as [the] [man] ARG0 and [the man] ARG0 are to be considered the same or not.", "labels": [], "entities": [{"text": "ARG0", "start_pos": 169, "end_pos": 173, "type": "DATASET", "confidence": 0.7531454563140869}]}, {"text": "If they are considered the same, there are multiple labelings of nodes in a parse tree that are equivalent.", "labels": [], "entities": []}, {"text": "The third issue is that when using automatic parsers, some of the constituents that are fillers of semantic roles are not recovered by the parser.", "labels": [], "entities": []}, {"text": "In this case it is not clear how various research groups have scored their systems (using headword match, ignoring these arguments altogether, or using exact match).", "labels": [], "entities": [{"text": "exact", "start_pos": 152, "end_pos": 157, "type": "METRIC", "confidence": 0.955848753452301}]}, {"text": "If we vary the choice taken for these three issues, we can come up with many (at least eight) different evaluation measures, and these details are important, because different choices can lead to rather large differences in reported performance.", "labels": [], "entities": []}, {"text": "Here we describe in detail our evaluation measures for the results on the February 2004 data reported in this article.", "labels": [], "entities": [{"text": "February 2004 data", "start_pos": 74, "end_pos": 92, "type": "DATASET", "confidence": 0.6818666954835256}]}, {"text": "The measures are similar to the CoNLL evaluation measure, but report a richer set of statistics; the exact differences are discussed at the end of this section.", "labels": [], "entities": [{"text": "CoNLL evaluation measure", "start_pos": 32, "end_pos": 56, "type": "DATASET", "confidence": 0.709428588549296}]}, {"text": "For both gold-standard and automatic parses we use one evaluation measure, which we call argument-based evaluation.", "labels": [], "entities": []}, {"text": "To describe the evaluation measure, we will use as an example the correct and guessed semantic role labelings shown in Figures 2(a) and 2(b).", "labels": [], "entities": []}, {"text": "Both are shown as labelings on parse tree nodes with labels of the form ARGX and C-ARGX.", "labels": [], "entities": []}, {"text": "The label C-ARGX is used to represent multi-constituent arguments.", "labels": [], "entities": []}, {"text": "A constituent labeled C-ARGX is assumed to be a continuation of the closest constituent to the left labeled ARGX.", "labels": [], "entities": []}, {"text": "Our semantic role labeling system produces labelings of this form and the gold standard Propbank annotations are converted to this form as well.", "labels": [], "entities": []}, {"text": "The evaluation is carried out individually for each predicate and its associated argument frame.", "labels": [], "entities": []}, {"text": "If a sentence contains several clauses, the several argument frames are evaluated separately.", "labels": [], "entities": []}, {"text": "Our argument-based measures do not require exact bracketing (if the set of words constituting an argument is correct, there is no need to know how this set is broken into constituents) and do not give partial credit for labeling correctly only some of several constituents in a multi-constituent argument.", "labels": [], "entities": []}, {"text": "For these measures, a semantic role labeling of a sentence is viewed as a labeling on sets of words.", "labels": [], "entities": []}, {"text": "These sets can encompass several non-contiguous spans.", "labels": [], "entities": []}, {"text": "gives the representation of the correct and guessed labelings shown in Figures 2(a) and 2(b), in the first and second rows of the table, respectively.", "labels": [], "entities": []}, {"text": "To convert a labeling on parse tree nodes to this form, we create a labeled set for each possibly multi-constituent argument.", "labels": [], "entities": []}, {"text": "All remaining sets of words are implicitly labeled with NONE.", "labels": [], "entities": [{"text": "NONE", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.7308065891265869}]}, {"text": "We can see that, in this way, exact bracketing is not necessary and also no partial credit is given when only some of several constituents in a multi-constituent argument are labeled correctly.", "labels": [], "entities": [{"text": "exact bracketing", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.6442259252071381}]}, {"text": "We will refer to word sets as \"spans.\"", "labels": [], "entities": []}, {"text": "To compute the measures, we are comparing a guessed set of labeled spans to a correct set of labeled spans.", "labels": [], "entities": []}, {"text": "We briefly define the various measures of comparison used herein, using the example guessed and correct The figure shows performance measures-F-Measure (F1) and Whole Frame Accuracy (Acc.)-across nine different conditions.", "labels": [], "entities": [{"text": "F1)", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.9746704697608948}, {"text": "Whole Frame Accuracy (Acc.)-", "start_pos": 161, "end_pos": 189, "type": "METRIC", "confidence": 0.7870682527621587}]}, {"text": "When the sets of labeled spans are compared directly, we obtain the complete task measures, corresponding to the ID&CLS row and ALL column in.", "labels": [], "entities": []}, {"text": "We also define several other measures to understand the performance of the system on different types of labels.", "labels": [], "entities": []}, {"text": "We measure the performance on identification (ID), classification (CLS), and the complete task (ID&CLS), when considering only the core arguments (CORE), all arguments but with a single ARGM label for the modifier arguments (COARSEARGM), and all arguments (ALL).", "labels": [], "entities": []}, {"text": "This defines nine sub-tasks, which we now describe.", "labels": [], "entities": []}, {"text": "For each of them, we compute the Whole Frame Accuracy and F-Measure as follows: Whole Frame Accuracy (Acc.).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.8394306302070618}, {"text": "F-Measure", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9977476000785828}, {"text": "Accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.8649630546569824}, {"text": "Acc.", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.963916540145874}]}, {"text": "This is the percentage of propositions for which there is an exact match between the proposed and correct labelings.", "labels": [], "entities": []}, {"text": "For example, the whole frame accuracy for ID&CLS and ALL is 0, because the correct and guessed sets of labeled spans shown in(c) do not match exactly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9457383155822754}]}, {"text": "In the figures, \"Acc.\" is always an abbreviation for this whole frame accuracy.", "labels": [], "entities": [{"text": "Acc.", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9744424819946289}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9500696063041687}]}, {"text": "Even though this measure has not been used extensively in previous work, we find it useful to track.", "labels": [], "entities": []}, {"text": "Most importantly, potential applications of role labeling may require correct labeling of all (or at least the core) arguments in a sentence in order to be effective, and partially correct labelings may not be very useful.", "labels": [], "entities": [{"text": "role labeling", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.7325372099876404}]}, {"text": "Moreover, a joint model for semantic role labeling optimizes Whole Frame Accuracy more directly than a local model does.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7139724691708883}, {"text": "Accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.8643157482147217}]}, {"text": "true positve+false negative . This formula uses the number of true positive, false positive, and false negative spans in a given guessed labeling.", "labels": [], "entities": []}, {"text": "True positive is the number of spans whose correct label is one of the core or modifier argument labels (not NONE) and whose guessed label is the same as the correct label.", "labels": [], "entities": [{"text": "NONE", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9399555325508118}]}, {"text": "False positive is the number of spans whose guessed label is non-NONE and whose correct label is different from the guessed label (possibly NONE).", "labels": [], "entities": [{"text": "False positive", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9860014617443085}]}, {"text": "False negative is the number of spans whose correct label is non-NONE and whose guessed label is not the same as the correct one (possibly NONE).", "labels": [], "entities": [{"text": "False negative", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9869951605796814}]}, {"text": "In the figures in this paper we show F-Measure multiplied by 100 so that it is in the same range as Whole Frame Accuracy.", "labels": [], "entities": [{"text": "F-Measure multiplied", "start_pos": 37, "end_pos": 57, "type": "METRIC", "confidence": 0.9367967844009399}, {"text": "Accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.897957444190979}]}, {"text": "Core Argument Measures (CORE).", "labels": [], "entities": [{"text": "Core Argument Measures (CORE)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6487360050280889}]}, {"text": "These measures score the system on core arguments only, without regard to modifier arguments.", "labels": [], "entities": []}, {"text": "They can be obtained by first mapping all non-core argument labels in the guessed and correct labelings to NONE.", "labels": [], "entities": [{"text": "NONE", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.6689376831054688}]}, {"text": "Coarse Modifier Argument Measures (COARSEARGM).", "labels": [], "entities": [{"text": "Coarse Modifier Argument Measures", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5938978344202042}]}, {"text": "Sometimes it is sufficient to know a given span has a modifier role, without knowledge of the specific role label.", "labels": [], "entities": []}, {"text": "In addition, deciding exact modifier argument labels was one of the decisions with highest disagreement among annotators.", "labels": [], "entities": [{"text": "deciding exact modifier argument labels", "start_pos": 13, "end_pos": 52, "type": "TASK", "confidence": 0.6244047284126282}]}, {"text": "To estimate performance under this setting, we relabel all ARGM-X arguments to ARGM in the proposed and correct labeling.", "labels": [], "entities": [{"text": "ARGM", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.5354504585266113}]}, {"text": "Such a performance measure was also used by.", "labels": [], "entities": []}, {"text": "Note that these measures do not exclude the core arguments but instead consider the core plus a coarse version of the modifier arguments.", "labels": [], "entities": []}, {"text": "Thus for COARSEARGM ALL we count {0} as a true positive span, {1, 2} , {3, 4}, and {7, 8, 9} as false positive, and {1, 2, 3, 4} and {7, 8, 9} as false negative.", "labels": [], "entities": [{"text": "COARSEARGM", "start_pos": 9, "end_pos": 19, "type": "DATASET", "confidence": 0.7894319891929626}]}, {"text": "Identification Measures (ID).", "labels": [], "entities": [{"text": "Identification Measures (ID)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8204835176467895}]}, {"text": "These measure how well we do on the ARG vs. NONE distinction.", "labels": [], "entities": [{"text": "ARG vs. NONE distinction", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.555787481367588}]}, {"text": "For the purposes of this evaluation, all spans labeled with a non-NONE label are considered to have the generic label ARG.", "labels": [], "entities": [{"text": "ARG", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.7107820510864258}]}, {"text": "For example, to compute CORE ID, we compare the following sets of labeled spans: The F-Measure is 1.0 and the Whole Frame Accuracy is 100%.", "labels": [], "entities": [{"text": "CORE ID", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.48679710924625397}, {"text": "F-Measure", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9592680335044861}, {"text": "Accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.5826892852783203}]}, {"text": "Classification Measures (CLS).", "labels": [], "entities": [{"text": "Classification Measures (CLS)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9065858483314514}]}, {"text": "These are performance on argument spans which were also guessed to be argument spans (but possibly the exact label was wrong).", "labels": [], "entities": []}, {"text": "In other words, these measures ignore the ARG vs. NONE confusions.", "labels": [], "entities": [{"text": "ARG", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.8925468921661377}, {"text": "NONE", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.5916690230369568}]}, {"text": "They ignore all spans, which were incorrectly labeled NONE, or incorrectly labeled with an argument label, when the correct label was NONE.", "labels": [], "entities": [{"text": "NONE", "start_pos": 134, "end_pos": 138, "type": "DATASET", "confidence": 0.8736637234687805}]}, {"text": "This is different from \"classification accuracy\" used in previous work to mean the accuracy of the system in classifying spans when the correct set of argument spans is given.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.8022319078445435}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9993957281112671}]}, {"text": "To compute CLS measures, we remove all spans from S guessed and S correct that do not occur in both sets, and compare the resulting sets.", "labels": [], "entities": []}, {"text": "For example, to compute the ALL CLS measures, we need to compare the following sets of labeled spans: The rest of the spans were removed from both sets because they were labeled NONE according to one of the labelings and non-NONE according to the other.", "labels": [], "entities": []}, {"text": "The F-Measure is .50 and the Whole Frame Accuracy is 0%.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9990035891532898}, {"text": "Whole Frame Accuracy", "start_pos": 29, "end_pos": 49, "type": "METRIC", "confidence": 0.6712194383144379}]}, {"text": "As we mentioned before, we label and evaluate the semantic frame of every predicate in the sentence separately.", "labels": [], "entities": []}, {"text": "It is possible fora sentence to contain several propositions-annotations of predicates occurring in the sentence.", "labels": [], "entities": []}, {"text": "For example, in the sentence The spacecraft faces a six-year journey to explore Jupiter, there are two propositions, for the verbs faces and explore.", "labels": [], "entities": []}, {"text": "These are: Our evaluation measures compare the guessed and correct set of labeled spans for each proposition.", "labels": [], "entities": []}, {"text": "The CoNLL evaluation measure) is almost the same as our argument-based measure.", "labels": [], "entities": [{"text": "CoNLL evaluation measure", "start_pos": 4, "end_pos": 28, "type": "METRIC", "confidence": 0.6315221687157949}]}, {"text": "The only difference is that the CoNLL measure introduces an additional label type for arguments, of the form R-ARGX, used for referring ex-pressions.", "labels": [], "entities": []}, {"text": "The Propbank distribution contains a specification of which multi-constituent arguments are in a coreference chain.", "labels": [], "entities": [{"text": "Propbank distribution", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.9241988956928253}]}, {"text": "The CoNLL evaluation script considers these multi-constituent arguments as several separate arguments having different labels, where one argument has an ARGX label and the others have R-ARGX labels.", "labels": [], "entities": []}, {"text": "The decision of which constituents were to be labeled with referring labels was made using a set of rules expressed with regular expressions.", "labels": [], "entities": []}, {"text": "5 A script that converts Propbank annotations to CoNLL format is available as part of the shared task software.", "labels": [], "entities": []}, {"text": "For example, in the following sentence, the CoNLL specification annotates the arguments of began as follows: In contrast, we treat all multi-constituent arguments in the same way, and do not distinguish coreferential versus non-coreferential split arguments.", "labels": [], "entities": []}, {"text": "According to our argument-based evaluation, the annotation of the arguments of the verb began is: The difference between our argument based measure and the CoNLL evaluation measure is such that we cannot say that the value of one is always higher than the value of the other.", "labels": [], "entities": []}, {"text": "Either measure could be higher depending on the kinds of errors made.", "labels": [], "entities": []}, {"text": "For example, if the guessed labeling is: [began] PRED enabled shippers to bargain for transportation, the CoNLL script would count the argument that as correct and report precision and recall of .5, whereas our argument-based measure would not count any argument correct and report precision and recall of 0.", "labels": [], "entities": [{"text": "precision", "start_pos": 171, "end_pos": 180, "type": "METRIC", "confidence": 0.8825852870941162}, {"text": "recall", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.9810707569122314}, {"text": "precision", "start_pos": 282, "end_pos": 291, "type": "METRIC", "confidence": 0.8531090021133423}, {"text": "recall", "start_pos": 296, "end_pos": 302, "type": "METRIC", "confidence": 0.858989417552948}]}, {"text": "On the other hand, if the guessed labeling isThe deregulation] ARG1 of railroads [that] C-ARG1 PRED enabled shippers to bargain for transportation, the CoNLL measure would report a precision and recall of 0, whereas our argument-based measure would report precision and recall of 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 181, "end_pos": 190, "type": "METRIC", "confidence": 0.9989967942237854}, {"text": "recall", "start_pos": 195, "end_pos": 201, "type": "METRIC", "confidence": 0.9807541370391846}, {"text": "precision", "start_pos": 256, "end_pos": 265, "type": "METRIC", "confidence": 0.9990260601043701}, {"text": "recall", "start_pos": 270, "end_pos": 276, "type": "METRIC", "confidence": 0.9894037246704102}]}, {"text": "If the guessed labeling isThe deregulation] ARG1 of railroads [that] R-ARG1 PRED enabled shippers to bargain for transportation, both measures would report precision and recall of 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.9996968507766724}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9995958209037781}]}, {"text": "(For our argument-based measure it does not make sense to propose R-ARGX labels and we assume such labels would be converted to C-ARGX labels if they are after the phrase they refer to.)", "labels": [], "entities": []}, {"text": "Nevertheless, overall we expect the two measures to yield very similar results.", "labels": [], "entities": []}, {"text": "The CoNLL 2005 data is derived from Propbank version I, which is the first official release in 2005, whereas the results we have been reporting in the previous sections used the pre-final February 2004 data.", "labels": [], "entities": [{"text": "CoNLL 2005 data", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9448431531588236}, {"text": "Propbank version I", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.9465585947036743}]}, {"text": "Using the CoNLL 2005 evaluation standard ensures that results obtained by different groups are evaluated in exactly the same way.", "labels": [], "entities": [{"text": "CoNLL 2005 evaluation standard", "start_pos": 10, "end_pos": 40, "type": "DATASET", "confidence": 0.9461335241794586}]}], "tableCaptions": []}