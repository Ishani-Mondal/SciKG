{"title": [{"text": "Modeling Local Coherence: An Entity-Based Approach", "labels": [], "entities": [{"text": "Modeling Local Coherence", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8394110997517904}]}], "abstractContent": [{"text": "This article proposes a novel framework for representing and measuring local coherence.", "labels": [], "entities": []}, {"text": "Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text.", "labels": [], "entities": []}, {"text": "The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities.", "labels": [], "entities": []}, {"text": "We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks.", "labels": [], "entities": [{"text": "ranking-based generation", "start_pos": 125, "end_pos": 149, "type": "TASK", "confidence": 0.7259911298751831}, {"text": "text classification tasks", "start_pos": 154, "end_pos": 179, "type": "TASK", "confidence": 0.8164255420366923}]}, {"text": "Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.", "labels": [], "entities": [{"text": "text ordering", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.8056750595569611}]}], "introductionContent": [], "datasetContent": [{"text": "Text structuring algorithms (Lapata 2003;) are commonly evaluated by their performance at information-ordering.", "labels": [], "entities": [{"text": "Text structuring", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7055710107088089}]}, {"text": "The task concerns determining a sequence in which to present a pre-selected set of information-bearing items; this is an essential step in concept-to-text generation, multi-document summarization, and other text-synthesis problems.", "labels": [], "entities": [{"text": "concept-to-text generation", "start_pos": 139, "end_pos": 165, "type": "TASK", "confidence": 0.7502578496932983}, {"text": "multi-document summarization", "start_pos": 167, "end_pos": 195, "type": "TASK", "confidence": 0.6899006366729736}]}, {"text": "The information bearing items can be database entries (), propositions () or sentences (Lapata 2003;).", "labels": [], "entities": []}, {"text": "In sentence ordering, a document is viewed as a bag of sentences and the algorithm's task is to try to find the ordering which maximizes coherence according to some criterion (e.g., the probability of an order).", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7437836527824402}]}, {"text": "As explained previously, we use our coherence model to rank alternative sentence orderings instead of trying to find an optimal ordering.", "labels": [], "entities": []}, {"text": "We do not assume that local coherence is sufficient to uniquely determine a maximally coherent ordering-other constraints clearly play a role here.", "labels": [], "entities": []}, {"text": "It is nevertheless a key property of well-formed text (documents lacking local coherence are naturally globally incoherent), and a model which takes it into account should be able to discriminate coherent from incoherent texts.", "labels": [], "entities": []}, {"text": "In our sentence-ordering task we generate random permutations of a test document and measure how often a permutation is ranked higher than the original document.", "labels": [], "entities": []}, {"text": "A non-deficient model should prefer the original text more frequently than its permutations (see Section 4.2 for details).", "labels": [], "entities": []}, {"text": "We begin by explaining how a ranking function can be learned for the sentence ordering task.", "labels": [], "entities": [{"text": "sentence ordering task", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.8210498889287313}]}, {"text": "Next, we give details regarding the corpus used for our experiments, describe the methods used for comparison with our approach, and note the evaluation metric employed for assessing model performance.", "labels": [], "entities": []}, {"text": "Our results are presented in Section 4.3.", "labels": [], "entities": []}, {"text": "We further test the ability of our method to assess coherence by comparing model induced rankings against rankings elicited by human judges.", "labels": [], "entities": []}, {"text": "Admittedly, the synthetic data used in the ordering task only partially approximates coherence violations that human readers encounter in machine generated texts.", "labels": [], "entities": []}, {"text": "A representative example of such texts are automatically generated summaries which often contain sentences taken out of context and thus display problems with respect to local coherence (e.g., dangling anaphors, thematically unrelated sentences).", "labels": [], "entities": []}, {"text": "A model that exhibits high agreement with human judges not only accurately captures the coherence properties of the summaries in question, but ultimately holds promise for the automatic evaluation of machine-generated texts.", "labels": [], "entities": []}, {"text": "Existing automatic evaluation measures such as BLEU ( and ROUGE (Lin and Hovy 2003) are not designed for the coherence assessment task, because they focus on content similarity between system output and reference texts.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9974496960639954}, {"text": "ROUGE", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9916650652885437}]}, {"text": "So far, our experiments have explored the potential of the proposed discourse representation for coherence modeling.", "labels": [], "entities": [{"text": "coherence modeling", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.7787616550922394}]}, {"text": "We have presented several classes of grid models achieving good performance in discerning coherent from incoherent texts.", "labels": [], "entities": []}, {"text": "Our experiments also reveal a surprising property of grid models: Even though these models are not lexicalized, they are domain-and style-dependent.", "labels": [], "entities": []}, {"text": "In this section, we investigate in detail this feature of grid models.", "labels": [], "entities": []}, {"text": "Here, we move away from the coherence rating task and put the entity-grid representation further to the test by examining whether it can be usefully employed in style classification.", "labels": [], "entities": [{"text": "style classification", "start_pos": 161, "end_pos": 181, "type": "TASK", "confidence": 0.7667355835437775}]}, {"text": "Specifically, we embed our entity grids into a system that assesses document readability.", "labels": [], "entities": []}, {"text": "The term describes the ease with which a document can be read and understood.", "labels": [], "entities": []}, {"text": "The quantitative measurement of readability has attracted considerable interest and debate over the last 70 years (see and Chall for detailed overviews) and has recently benefited from the use of NLP technology (.", "labels": [], "entities": []}, {"text": "A number of readability formulas have been developed with the primary aim of assessing whether texts or books are suitable for students at particular grade levels or ages.", "labels": [], "entities": []}, {"text": "Many readability methods focus on simple approximations of semantic factors concerning the words used and syntactic factors concerning the length or structure of sentences.", "labels": [], "entities": []}, {"text": "Despite their widespread applicability in education and technical, readability formulas are often criticized for being too simplistic; they systematically ignore many important factors that affect readability such as discourse coherence and cohesion, layout and formatting, use of illustrations, the nature of the topic, the characteristics of the readers, and so forth.", "labels": [], "entities": []}, {"text": "Schwarm and Ostendorf (2005) developed a method for assessing readability which addresses some of the shortcomings of previous approaches.", "labels": [], "entities": []}, {"text": "By recasting readability assessment as a classification task, they are able to combine several knowledge sources ranging from traditional reading level measures, to statistical language models, and syntactic analysis.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 198, "end_pos": 216, "type": "TASK", "confidence": 0.7187642008066177}]}, {"text": "Evaluation results show that their system outperforms two commonly used reading level measures (the Flesch-Kincaid Grade Level index and Lexile).", "labels": [], "entities": [{"text": "Flesch-Kincaid Grade Level index", "start_pos": 100, "end_pos": 132, "type": "METRIC", "confidence": 0.8826417773962021}, {"text": "Lexile", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.8357003331184387}]}, {"text": "In the following we build on their approach and examine whether the entity-grid representation introduced in this article contributes to the readability assessment task.", "labels": [], "entities": []}, {"text": "The incorporation of coherence-based information in the measurement of text readability is, to our knowledge, novel.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3  Example of a feature-vector document representation using all transitions of length two given  syntactic categories S, O, X, and -.", "labels": [], "entities": []}, {"text": " Table 4  The size of the training and test instances for the Earthquakes and Accidents corpora (measured  by the number of pairs that contain the original order and a random permutation of this order).", "labels": [], "entities": [{"text": "Earthquakes and Accidents corpora", "start_pos": 62, "end_pos": 95, "type": "DATASET", "confidence": 0.6523290053009987}]}, {"text": " Table 5  Accuracy measured as a fraction of correct pairwise rankings in the test set. Coreference[+/\u2212]  indicates whether coreference information has been used in the construction of the entity grid.  Similarly, Syntax[+/\u2212] and Salience[+/\u2212] reflect the use of syntactic and salience information.  Diacritics ** (p < .01) and * (p < .05) indicate whether differences in accuracy between the full  model (Coreference+Syntax+Salience+) and all other models are significant (using a Fisher  Sign test).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 372, "end_pos": 380, "type": "METRIC", "confidence": 0.9980521202087402}]}, {"text": " Table 7  Accuracy of entity-based model (Coreference+Syntax+Salience+) and HHM-based content  model on out-of-domain texts. Diacritics ** (p < .01) and * (p < .05) indicate whether  performances on in-domain and out-of-domain data are significantly different using a Fisher  Sign Test.", "labels": [], "entities": []}, {"text": " Table 8  Summary ranking accuracy measured as fraction of correct pairwise rankings in the test set.  Coreference[+/\u2212] indicates whether anaphoric information has been used when constructing  the entity grid. Similarly, Syntax[+/\u2212] and Salience[+/\u2212] reflect the use of syntactic and  salience information. Diacritics ** (p < .01) and * (p < .05) indicate whether Coreference\u2212  Syntax+Salience+ is significantly different from all other models (using a Fisher Sign Test).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9840179085731506}]}]}