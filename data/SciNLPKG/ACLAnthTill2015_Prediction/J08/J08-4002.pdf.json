{"title": [{"text": "Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets", "labels": [], "entities": [{"text": "Supervised Learning of Dialogue Policies from Fixed Data Sets", "start_pos": 21, "end_pos": 82, "type": "TASK", "confidence": 0.658939755625195}]}], "abstractContent": [{"text": "We propose a method for learning dialogue management policies from a fixed data set.", "labels": [], "entities": [{"text": "learning dialogue management", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.5933847526709238}]}, {"text": "The method addresses the challenges posed by Information State Update (ISU)-based dialogue systems, which represent the state of a dialogue as a large set of features, resulting in a very large state space and a huge policy space.", "labels": [], "entities": []}, {"text": "To address the problem that any fixed data set will only provide information about small portions of these state and policy spaces, we propose a hybrid model that combines reinforcement learning with supervised learning.", "labels": [], "entities": []}, {"text": "The reinforcement learning is used to optimize a measure of dialogue reward, while the supervised learning is used to restrict the learned policy to the portions of these spaces for which we have data.", "labels": [], "entities": []}, {"text": "We also use linear function approximation to address the need to generalize from a fixed amount of data to large state spaces.", "labels": [], "entities": [{"text": "linear function approximation", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.6342115700244904}]}, {"text": "To demonstrate the effectiveness of this method on this challenging task, we trained this model on the COMMUNICATOR corpus, to which we have added annotations for user actions and Information States.", "labels": [], "entities": [{"text": "COMMUNICATOR corpus", "start_pos": 103, "end_pos": 122, "type": "DATASET", "confidence": 0.8192459344863892}]}, {"text": "When tested with a user simulation trained on a different part of the same data set, our hybrid model outperforms a pure supervised learning model and a pure reinforcement learning model.", "labels": [], "entities": []}, {"text": "It also outperforms the hand-crafted systems on the COMMUNICATOR data, according to automatic evaluation measures, improving over the average COMMUNICATOR system policy by 10%.", "labels": [], "entities": [{"text": "COMMUNICATOR data", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.9407518208026886}]}, {"text": "The proposed method will improve techniques for bootstrapping and automatic optimization of dialogue management policies from limited initial data sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the practical development of dialogue systems it is often the case that an initial corpus of task-oriented dialogues is collected, either using \"Wizard of Oz\" methods or a prototype system deployment.", "labels": [], "entities": []}, {"text": "This data is usually used to motivate and inspire anew hand-built dialogue system or to modify an existing one.", "labels": [], "entities": []}, {"text": "However, given the existence of such data, it should be possible to exploit machine learning methods to automatically build and optimize anew dialogue system.", "labels": [], "entities": []}, {"text": "This objective poses two questions: what machine learning methods are effective for this problem? and how can we encode the task in away which is appropriate for these methods?", "labels": [], "entities": []}, {"text": "For the latter challenge, we exploit the Information State Update (ISU) approach to dialogue systems (, which provides the kind of rich and flexible feature-based representations of context that are used with many recent machine learning methods, including the linear function approximation method we use here.", "labels": [], "entities": [{"text": "linear function approximation", "start_pos": 261, "end_pos": 290, "type": "TASK", "confidence": 0.6285220185915629}]}, {"text": "For the former challenge, we propose a novel hybrid method that combines reinforcement learning (RL) with supervised learning (SL).", "labels": [], "entities": []}, {"text": "The focus of this article is to establish effective methods for using fixed corpora of dialogues to automatically optimize complex dialogue systems.", "labels": [], "entities": []}, {"text": "To avoid the need for extensive hand-crafting, we allow rich representations of context that include all the features that might be relevant to dialogue management decisions, and we allow abroad set of dialogue management decisions with very few constraints on when a decision is applicable.", "labels": [], "entities": []}, {"text": "This flexibility simplifies system design, but it leads to a huge space of possible dialogue management policies, which poses severe difficulties for existing approaches to machine learning for dialogue systems (see Section 1.1).", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7996096312999725}]}, {"text": "Our proposed method addresses these difficulties without the use of user simulations, feature engineering, or further data collections.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of the proposed method on the COMMUNICA-TOR corpora of flight-booking dialogues.", "labels": [], "entities": [{"text": "COMMUNICA-TOR corpora of flight-booking dialogues", "start_pos": 63, "end_pos": 112, "type": "DATASET", "confidence": 0.867581820487976}]}, {"text": "Our method (\"hybrid learning\" with linear function approximation) can learn dialogue strategies that are better than those learned by standard learning methods, and that are better than the (in this case hand-coded) strategies present in the original corpora, according to a variety of metrics.", "labels": [], "entities": []}, {"text": "To evaluate learned strategies we run them with simulated users that are also trained on (different parts of) the COMMUNICATOR corpora, and automatically score the simulated dialogues based on how many information \"slots\" they manage to collect from users (\"filled slots\"), whether those slots were confirmed (\"confirmed slots\"), and how many dialogue turns were required to do so.", "labels": [], "entities": []}, {"text": "Later work has shown these metrics to correlate strongly with task completion for real users of the different policies ).", "labels": [], "entities": []}, {"text": "The main contributions of the work are therefore in empirically demonstrating that: r limited initial data sets can be used to train complex dialogue policies, using a novel combination of supervised and reinforcement learning; and r large, feature-based representations of dialogue context can be used in tractable learning of dialogue policies, using linear function approximation.", "labels": [], "entities": []}, {"text": "In this article, after a discussion of related work, we outline the annotations we have added to the COMMUNICATOR data, then present the proposed learning method, and describe our evaluation method.", "labels": [], "entities": [{"text": "COMMUNICATOR data", "start_pos": 101, "end_pos": 118, "type": "DATASET", "confidence": 0.8315063416957855}]}, {"text": "Finally, we present the evaluation results and discuss their implications.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the trained dialogue management policies by running them against trained user simulations.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7380190491676331}]}, {"text": "The policies and the user simulations were trained using different parts of the annotated COMMUNICATOR data (using two-fold and five-fold cross validation).", "labels": [], "entities": [{"text": "COMMUNICATOR data", "start_pos": 90, "end_pos": 107, "type": "DATASET", "confidence": 0.794808030128479}]}, {"text": "We compare our results against each other and against the performance of the eight COMMUNICATOR systems, using an evaluation metric discussed subsequently.", "labels": [], "entities": []}, {"text": "The Information States for the simulated dialogues were computed with the same rules used to compute the Information States for the annotated data.", "labels": [], "entities": []}, {"text": "To evaluate the success of a dialogue, we take the final state of the dialogue and use it to compute a scoring function.", "labels": [], "entities": []}, {"text": "We want the scoring function to be similar to the reward we compute from the quality measures provided with the COMMUNICATOR data (e.g., the user questionnaires), but because we do not have these quality measures for the simulated dialogues, we cannot use the exact same reward function.", "labels": [], "entities": [{"text": "COMMUNICATOR data", "start_pos": 112, "end_pos": 129, "type": "DATASET", "confidence": 0.7524992525577545}]}, {"text": "When we compare the hybrid policy against the COMMUNICATOR systems, we apply the same scoring function to both types of dialogues so that we have a comparable evaluation metric for both.", "labels": [], "entities": []}, {"text": "Because currently we are only considering users who only want single-leg or return flight bookings, the scoring function only looks at the four essential slots for these bookings: origin city, destination city, departure date, and departure time.", "labels": [], "entities": []}, {"text": "We give 25 points for each slot that is filled, plus another 25 points for each slot that is also confirmed.", "labels": [], "entities": []}, {"text": "We also deduct 1 point for each action performed by the system, to penalize longer dialogues.", "labels": [], "entities": []}, {"text": "Thus the maximum possible score is 197 (i.e., 200 minus 3 system actions: ask for all the user information in one action, then confirm all the four slots in one action and offer a flight).", "labels": [], "entities": []}, {"text": "The motivation behind this evaluation metric is that confirmed slots are more likely to be correct than slots that are just filled.", "labels": [], "entities": []}, {"text": "If we view the score as proportional to the probability that a slot is filled correctly, then this scoring assumes that confirmed slots are twice as likely to be correct.", "labels": [], "entities": []}, {"text": "Although other scoring metrics are clearly possible, this one is a simple and reasonable approximation of the relative expected correctness of confirmed versus non-confirmed information in dialogue systems.", "labels": [], "entities": []}, {"text": "On the other hand, none of our conclusions depend on this exact scoring function, as indicated by results for the \"no-conf\" version of our scoring function (discussed subsequently), which ignores confirmations.", "labels": [], "entities": []}, {"text": "When combining the scores for different slots, we do not try to model the all-ornothing nature of the COMMUNICATOR task-completion quality measures, but instead sum the scores for the individual slots.", "labels": [], "entities": []}, {"text": "This sum makes our scoring metric value partial completions more highly, but inspection of the distributions of scores indicates that this difference does not favor either the hybrid policy or the original COMMUNICATOR systems.", "labels": [], "entities": [{"text": "scoring metric value partial completions", "start_pos": 19, "end_pos": 59, "type": "METRIC", "confidence": 0.5057254254817962}]}, {"text": "Although this evaluation metric could reflect the relative quality of individual dialogues more accurately, we believe it provides a good measure of the relative quality of the systems we wish to compare.", "labels": [], "entities": []}, {"text": "First, the exact same metric is applied to every system.", "labels": [], "entities": []}, {"text": "Additional information that we have for some systems, but not all, is not used (e.g., the COMMUNICATOR user questionnaires, which we do not have for simulated dialogues).", "labels": [], "entities": []}, {"text": "Second, the systems are being run against approximately equivalent users.", "labels": [], "entities": []}, {"text": "The user simulation is trained on exactly the same user actions that are used to evaluate the COMMUNICATOR systems, so the user simulations mimic exactly these users.", "labels": [], "entities": []}, {"text": "In particular, the simulation is able to mimic the effects of speech recognition errors, because it is just as likely as the real users to disagree with a confirmation or provide anew value fora previously filled slot.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7168949842453003}]}, {"text": "The nature of the simulation model may make it systematically different from real users in someway, but we know of no argument for why this would bias our results in favor of one system or another.", "labels": [], "entities": []}, {"text": "One concern about this evaluation metric is that it does not reflect the quality of the speech recognizer being used by the system.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7240030467510223}]}, {"text": "If a system has a good speech recognizer, then it may not be necessary for it to confirm a slot value, but our scoring function will still penalize it for not confirming.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.686360165476799}]}, {"text": "This would certainly be a problem if this metric were to be used to compare different systems within the COMMUNICATOR data set.", "labels": [], "entities": [{"text": "COMMUNICATOR data set", "start_pos": 105, "end_pos": 126, "type": "DATASET", "confidence": 0.8776623606681824}]}, {"text": "However, the intention of the metric is simply to facilitate comparisons between different versions of our proposed system, and between our proposed systems and those in the data.", "labels": [], "entities": []}, {"text": "Because the user simulations are trained on the COMMUNICATOR data, they simulate speech recognition errors at the same rate as the data, thereby controlling for the quality of the speech recognizer.", "labels": [], "entities": [{"text": "COMMUNICATOR data", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.7550062835216522}, {"text": "speech recognition", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.6821126192808151}, {"text": "speech recognizer", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.7152101099491119}]}, {"text": "Nonetheless, it is worth considering another evaluation metric that does not penalize for missing confirmations.", "labels": [], "entities": []}, {"text": "For this reason we also evaluate the different systems based on their scores for only filled slots and length, which we call the \"no-conf\" score.", "labels": [], "entities": [{"text": "length", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9918559789657593}]}], "tableCaptions": [{"text": " Table 1  Statistics for the 2000 and 2001 COMMUNICATOR data.", "labels": [], "entities": [{"text": "COMMUNICATOR data", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.7040733695030212}]}, {"text": " Table 2  The weights used to compute a dialogue's final reward value, multiplied by values between 0  and 1 computed from user responses.", "labels": [], "entities": []}, {"text": " Table 3  The average scores from the different systems for single-leg and return dialogues, the score  excluding confirmations, and the three components of these scores.", "labels": [], "entities": []}, {"text": " Table 4  The average scores after the first flight offer for single-leg and return dialogues, the score  excluding confirmations, and the three components of these scores.", "labels": [], "entities": []}]}