{"title": [{"text": "Constructing Corpora for the Development and Evaluation of Paraphrase Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatic paraphrasing is an important component in many natural language processing tasks.", "labels": [], "entities": [{"text": "Automatic paraphrasing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6887591183185577}]}, {"text": "In this article we present anew parallel corpus with paraphrase annotations.", "labels": [], "entities": []}, {"text": "We adopt a definition of paraphrase based on word alignments and show that it yields high inter-annotator agreement.", "labels": [], "entities": []}, {"text": "As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks.", "labels": [], "entities": []}, {"text": "We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1) and also in developing linguistically rich paraphrase models based on syntactic structure.", "labels": [], "entities": [{"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9955756664276123}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.997763991355896}, {"text": "F1", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.9997187256813049}]}], "introductionContent": [{"text": "The ability to paraphrase text automatically carries much practical import for many NLP applications ranging from summarization (Barzilay 2003;) to question answering () and machine translation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.9875128865242004}, {"text": "question answering", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.8500832319259644}, {"text": "machine translation", "start_pos": 174, "end_pos": 193, "type": "TASK", "confidence": 0.8116495907306671}]}, {"text": "It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora.", "labels": [], "entities": []}, {"text": "These are most often monolingual corpora containing parallel translations of the same source text (.", "labels": [], "entities": []}, {"text": "Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases ( as well as comparable corpora such as collections of articles produced by two different newswire agencies about the same events (.", "labels": [], "entities": []}, {"text": "Although paraphrase induction algorithms differ in many respects-for example, the acquired paraphrases often vary in granularity as they can be lexical (fighting, battle) or structural (last week's fighting, the battle last week), and are represented as words or syntax trees-they all rely on some form of alignment for extracting paraphrase pairs.", "labels": [], "entities": [{"text": "paraphrase induction", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.9133973121643066}]}, {"text": "In its simplest form, the alignment can range over individual words, as is often done in machine translation ).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7414039373397827}]}, {"text": "In other cases, the alignments range over entire trees or sentence clusters (.", "labels": [], "entities": []}, {"text": "The obtained paraphrases are typically evaluated via human judgments.", "labels": [], "entities": []}, {"text": "Paraphrase pairs are presented to judges who are asked to decide whether they are semantically equivalent, that is, whether they can be generally substituted for one another in the same context without great information loss (.", "labels": [], "entities": []}, {"text": "In some cases the automatically acquired paraphrases are compared against manually generated ones ( or evaluated indirectly by demonstrating performance increase fora specific application, such as machine translation).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.7365265637636185}]}, {"text": "Unfortunately, manually evaluating paraphrases in this way has at least three drawbacks.", "labels": [], "entities": []}, {"text": "First, it is infeasible to perform frequent evaluations when assessing incremental system changes or tuning system parameters.", "labels": [], "entities": []}, {"text": "Second, it is difficult to replicate results presented in previous work because there is no standard corpus, and no standard evaluation methodology.", "labels": [], "entities": []}, {"text": "Consequently comparisons across systems are few and far between.", "labels": [], "entities": []}, {"text": "The third drawback concerns the evaluation studies themselves, which primarily focus on precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9975408315658569}]}, {"text": "Recall is almost never evaluated directly in the literature.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.607638955116272}]}, {"text": "And this is fora good reason: There is no guarantee that participants will identify the same set of paraphrases as each other or with a computational model.", "labels": [], "entities": []}, {"text": "The problem relates to the nature of the paraphrasing task, which has so far eluded formal definition (see the discussion in Barzilay).", "labels": [], "entities": []}, {"text": "Such a definition is not so crucial when assessing precision, because subjects are asked to rate the paraphrases without actually having to identify them.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9982755184173584}]}, {"text": "However, recall might be measured with respect to some set of \"goldstandard\" paraphrases which will have to be collected according to some concrete definition.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9991067051887512}]}, {"text": "In this article we present a resource that could potentially be used to address these problems.", "labels": [], "entities": []}, {"text": "Specifically, we create a monolingual parallel corpus with human paraphrase annotations.", "labels": [], "entities": []}, {"text": "Our working definition of paraphrase is based on word and phrase 1 alignments between semantically equivalent sentences.", "labels": [], "entities": []}, {"text": "Other definitions are possible, for instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus.", "labels": [], "entities": []}, {"text": "We chose to work with alignments for two reasons.", "labels": [], "entities": []}, {"text": "First, the notion of alignment appears to be central in paraphrasing-most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units.", "labels": [], "entities": [{"text": "paraphrasing-most", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.9741093516349792}, {"text": "paraphrase induction", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.8053673207759857}]}, {"text": "Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identified reliably by annotators.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7941660583019257}, {"text": "word alignments", "start_pos": 119, "end_pos": 134, "type": "TASK", "confidence": 0.6946832239627838}]}, {"text": "We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.6915749609470367}, {"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7296957969665527}]}, {"text": "Alignment blocks larger than one-to-one are used to specify phrase correspondences.", "labels": [], "entities": []}, {"text": "In the following section we explain how our corpus was created and summarize our annotation guidelines.", "labels": [], "entities": []}, {"text": "Section 3 gives the details of an agreement study, demonstrating that our annotators can identify and align paraphrases reliably.", "labels": [], "entities": []}, {"text": "We measure agreement using alignment overlap measures from the SMT literature, and also introduce a novel agreement statistic for non-enumerable labeling spaces.", "labels": [], "entities": [{"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9896217584609985}]}, {"text": "Section 4 illustrates how the corpus can be used in paraphrase research, for example, as a test set for evaluating the output of automatic systems or as a training set for the development of paraphrase systems.", "labels": [], "entities": []}, {"text": "Discussion of our results concludes the article.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our annotated corpus can be used in a number of ways to help paraphrase research: for example, to inform the linguistic analysis of paraphrases, as a training set for the development of discriminative paraphrase systems, and as a test set for the automatic evaluation of computational models.", "labels": [], "entities": []}, {"text": "Here, we briefly demonstrate some of these uses.", "labels": [], "entities": []}, {"text": "Much previous research has focused on lexical paraphrases (but see Lin and Pantel and Pang, Knight, and Marcu for exceptions).", "labels": [], "entities": []}, {"text": "We argue that our corpus should support a richer range of structural (syntactic) paraphrases.", "labels": [], "entities": []}, {"text": "To demonstrate this we have extracted paraphrase rules from our annotations using the grammar induction algorithm from.", "labels": [], "entities": []}, {"text": "Briefly, the algorithm extracts tree pairs from word-aligned text by choosing aligned constituents in a pair of equivalent sentences.", "labels": [], "entities": []}, {"text": "These pairs are then generalized by factoring out aligned subtrees, thereby resulting in synchronous grammar rules (Aho and Ullman 1969) with variable nodes.", "labels": [], "entities": []}, {"text": "We parsed the MTC corpus with Bikel's (2002) parser and extracted synchronous rules from the gold-standard alignments.", "labels": [], "entities": [{"text": "MTC corpus", "start_pos": 14, "end_pos": 24, "type": "DATASET", "confidence": 0.8116621375083923}]}, {"text": "A sample of these rules are shown in.", "labels": [], "entities": []}, {"text": "Here we see three lexical paraphrases, followed by five structural paraphrases.", "labels": [], "entities": []}, {"text": "In example 4, also is replaced with moreover and is moved to the start of the sentence from the pre-verbal position.", "labels": [], "entities": []}, {"text": "Examples 5-8 show various reordering operations, where the boxed numbers indicate correspondences between non-terminals in the two sides of the rules.", "labels": [], "entities": []}, {"text": "The synchronous rules in provide insight into the process of paraphrasing at the syntactic level, and also a practical means for developing algorithms for paraphrase generation-a task which has received little attention to date.", "labels": [], "entities": [{"text": "paraphrase generation-a task", "start_pos": 155, "end_pos": 183, "type": "TASK", "confidence": 0.9120086828867594}]}, {"text": "For instance, we could envisage a paraphrase model that transforms parse trees of an input sentence into parse trees that represent a sentential paraphrase of that sentence.", "labels": [], "entities": []}, {"text": "Our corpus can be used to learn this mapping using discriminative methods.", "labels": [], "entities": []}, {"text": "As mentioned in Section 1, it is currently difficult to compare competing approaches due to the effort involved in eliciting manual judgments of paraphrase output.", "labels": [], "entities": []}, {"text": "Our corpus could fill the role of a gold-standard test set, allowing for automatic evaluation techniques.", "labels": [], "entities": []}, {"text": "Developing measures for automatic paraphrase evaluation is outside the scope of this article.", "labels": [], "entities": [{"text": "paraphrase evaluation", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.8358680903911591}]}, {"text": "Nevertheless, we illustrate how the corpus can be used for this purpose.", "labels": [], "entities": []}, {"text": "For example we could easily measure the precision and recall of an automatic system against our annotations.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9994770884513855}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9925468564033508}]}, {"text": "Computing precision and recall for an individual system is not perhaps the most meaningful test, considering the large potential for paraphrasing in a given sentence pair.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9938094019889832}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9968175888061523}]}, {"text": "A better evaluation strategy would include a comparison across many systems on the same corpus.", "labels": [], "entities": []}, {"text": "We could then rank these systems without, however, paying so much attention to the absolute precision and recall values.", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9862468242645264}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9976860284805298}]}, {"text": "We expect these comparisons to yield relatively low numbers for many reasons.", "labels": [], "entities": []}, {"text": "First and foremost the task is hard, as shown by our inter-annotator agreement figures in.", "labels": [], "entities": []}, {"text": "Secondly, there maybe valid paraphrases that the systems identify but are not listed in our gold standard.", "labels": [], "entities": []}, {"text": "Thirdly, systems may have different biases, for example, towards producing more lexical or syntactic paraphrases, but our comparison would not take this into account.", "labels": [], "entities": []}, {"text": "Despite all these considerations, we believe that comparison against our corpus would treat these systems on an equal footing against the same materials while factoring out nonessential degrees of freedom inherent inhuman elicitation studies (e.g., attention span, task familiarity, background).", "labels": [], "entities": []}, {"text": "We evaluated the performance of two systems against our corpus.", "labels": [], "entities": []}, {"text": "Our first system is simply Giza++ trained on the 55, 615 sentence pairs described in Section 4.", "labels": [], "entities": []}, {"text": "The second system uses a co-training-based paraphrase extraction algorithm (.", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7537432014942169}]}, {"text": "It was also trained on the MTC part 1 corpus, on the same data set used for Giza++, with its default parameters.", "labels": [], "entities": [{"text": "MTC part 1 corpus", "start_pos": 27, "end_pos": 44, "type": "DATASET", "confidence": 0.9009485989809036}]}, {"text": "For each system, we filtered the predicted paraphrases to just those which match part of a sentence pair in the test set.", "labels": [], "entities": []}, {"text": "These paraphrases were then compared to the sure phrase pairs extracted from our manually aligned corpus.", "labels": [], "entities": []}, {"text": "Giza++'s precision is 55% and recall 49% (see).", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9996531009674072}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9994961023330688}]}, {"text": "The co-training system obtained a precision of 30% and recall of 16%.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9996591806411743}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9998608827590942}]}, {"text": "To confirm the accuracy of the precision estimate, we performed a human evaluation on a sample of 48 of the predicted paraphrases which were treated as errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9994611144065857}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9995357990264893}]}, {"text": "Of these, 63% were confirmed as being incorrect and only 20% were acceptable (the remaining were uncertain).", "labels": [], "entities": []}, {"text": "The interannotator agreement in can be used as an upper bound for precision and recall (precision for Sure phrase pairs is 67% and recall 66%).", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9995349645614624}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9988107681274414}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9989011287689209}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9929137229919434}]}, {"text": "These results seem to suggest that a hypothetical paraphrase extractor based on automatic word alignments would obtain performance superior to the co-training approach.", "labels": [], "entities": [{"text": "paraphrase extractor", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.7206816673278809}]}, {"text": "However, we must bear in mind that the co-training system is highly parametrized and was not specifically tuned to our data set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Phrase pairs are specified by the word alignments from", "labels": [], "entities": [{"text": "Phrase pairs", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9140680134296417}]}, {"text": " Table 4  Inter-annotator agreement using precision, recall, F1, and\u02c6Cand\u02c6 and\u02c6C; the agreement is measured over  atomic phrase pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9992095232009888}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9986030459403992}, {"text": "F1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9967083930969238}]}, {"text": " Table 5  Agreement between automatic Giza++ predicted word alignments and our manually corrected  alignments, measured over atomic phrase pairs.", "labels": [], "entities": [{"text": "Giza++ predicted word alignments", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.5119602680206299}]}]}