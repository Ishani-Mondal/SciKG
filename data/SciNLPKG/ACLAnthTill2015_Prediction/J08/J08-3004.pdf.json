{"title": [], "abstractContent": [{"text": "Many probabilistic models for natural language are now written in terms of hierarchical tree structure.", "labels": [], "entities": []}, {"text": "Tree-based modeling still lacks many of the standard tools taken for granted in (finite-state) string-based modeling.", "labels": [], "entities": []}, {"text": "The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature.", "labels": [], "entities": []}, {"text": "We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-to-tree and tree-to-string transducers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Much natural language work over the past decade has employed probabilistic finitestate transducers (FSTs) operating on strings.", "labels": [], "entities": []}, {"text": "This has occurred somewhat under the influence of speech recognition research, where transducing acoustic sequences to word sequences is neatly captured by left-to-right stateful substitution.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7331475466489792}]}, {"text": "Many conceptual tools exist, such as Viterbi decoding) and forward-backward training, as well as software toolkits like the AT&T FSM Library and USC/ISI's Carmel.", "labels": [], "entities": [{"text": "AT&T FSM Library", "start_pos": 124, "end_pos": 140, "type": "DATASET", "confidence": 0.8937021136283875}, {"text": "USC/ISI's Carmel", "start_pos": 145, "end_pos": 161, "type": "DATASET", "confidence": 0.8832773923873901}]}, {"text": "Moreover, a surprising variety of problems are attackable with FSTs, from part-of-speech tagging to letter-to-sound conversion to name transliteration.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 63, "end_pos": 67, "type": "TASK", "confidence": 0.9395173788070679}, {"text": "part-of-speech tagging", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.6759183406829834}, {"text": "letter-to-sound conversion", "start_pos": 100, "end_pos": 126, "type": "TASK", "confidence": 0.7043605297803879}, {"text": "name transliteration", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.7428194880485535}]}, {"text": "However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7716326713562012}]}, {"text": "Recently, specific probabilistic tree-based models have been proposed not only for machine translation), but also for summarization , paraphrasing, natural language generation (, parsing, and language modeling The Rounds/Thatcher tree transducer is very similar to a left-to-right FST, except that it works top-down, pursuing subtrees independently, with each subtree transformed depending only on its own passed-down state.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7321605235338211}, {"text": "summarization", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.9858084917068481}, {"text": "natural language generation", "start_pos": 148, "end_pos": 175, "type": "TASK", "confidence": 0.6516010761260986}, {"text": "parsing", "start_pos": 179, "end_pos": 186, "type": "TASK", "confidence": 0.961533784866333}, {"text": "language modeling", "start_pos": 192, "end_pos": 209, "type": "TASK", "confidence": 0.6949690133333206}]}, {"text": "This class of transducer, called R in earlier works ( for \"root-to-frontier,\" is often nowadays called T, for \"top-down\".", "labels": [], "entities": []}, {"text": "Rounds uses a mathematics-oriented example of a T transducer, which we repeat in.", "labels": [], "entities": []}, {"text": "At each point in the top-down traversal, the transducer chooses a production to apply, based only on the current state and the current root symbol.", "labels": [], "entities": []}, {"text": "The traversal continues until there are no more state-annotated nodes.", "labels": [], "entities": []}, {"text": "Non-deterministic transducers may have several productions with the same left-hand side, and therefore some free choices to make during transduction.", "labels": [], "entities": []}, {"text": "A T transducer compactly represents a potentially infinite set of input/output tree pairs: exactly those pairs (T1, T2) for which some sequence of productions applied to T1 (starting in the initial state) results in T2.", "labels": [], "entities": []}, {"text": "This is similar to an FST, which compactly represents a set of input/output string pairs; in fact, T is a generalization of FST.", "labels": [], "entities": [{"text": "FST", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.5109924674034119}, {"text": "FST", "start_pos": 124, "end_pos": 127, "type": "DATASET", "confidence": 0.8948653340339661}]}, {"text": "If we think of strings written down vertically, as degenerate trees, we can convert any FST into a T transducer by automatically replacing FST transitions with T productions, as follows: If an FST transition from state q to state r reads input symbol A and outputs symbol B, then the corresponding T production is q A(x0) \u2192 B(r x0).", "labels": [], "entities": []}, {"text": "If the FST transition output is epsilon, then we have instead q A(x0) \u2192 r x0, or if the input is epsilon, then q x0 \u2192 B(r x0).", "labels": [], "entities": [{"text": "FST transition", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.5569305121898651}]}, {"text": "T does have some extra power beyond path following and state-based recordkeeping.", "labels": [], "entities": [{"text": "path following", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.7022222429513931}]}, {"text": "It can copy whole subtrees, and transform those subtrees differently.", "labels": [], "entities": []}, {"text": "It can also delete subtrees without inspecting them (imagine by analogy an FST that quits and accepts right in the middle of an input string).", "labels": [], "entities": []}, {"text": "Variants of T that disallow copying and deleting are called LT (for linear) and NT (for nondeleting), respectively.", "labels": [], "entities": []}, {"text": "One advantage to working with tree transducers is the large and useful body of literature about these automata; two excellent surveys are and.", "labels": [], "entities": []}, {"text": "For example, it is known that T is not closed under composition, and neither are LT or B (the \"bottom-up\" cousin of T), but the noncopying LB is closed under composition.", "labels": [], "entities": []}, {"text": "Many of these composition results are first found in.", "labels": [], "entities": []}, {"text": "The power of T to change the structure of an input tree is surprising.", "labels": [], "entities": []}, {"text": "For example, it may not be initially obvious how a T transducer can transform the English structure S(PRO, VP(V, NP)) into the Arabic equivalent S(V, PRO, NP), as it is difficult to move the subject PRO into position between the verb V and the direct object NP.", "labels": [], "entities": []}, {"text": "First, T productions have no lookahead capability-the left-hand-side of the S production consists only of q S(x0, x1), although we want the English-to-Arabic transformation to apply only when it faces the entire structure q S(PRO, VP(V, NP)).", "labels": [], "entities": []}, {"text": "However, we can simulate lookahead using states, as in these productions: q S(x0, x1) \u2192 S(qpro x0, qvp.v.np x1) qpro PRO \u2192 PRO qvp.v.np VP(x0, x1) \u2192 VP(qv x0, qnp x1)", "labels": [], "entities": []}], "datasetContent": [{"text": "It is possible to cast many current probabilistic natural language models as T-type tree transducers.", "labels": [], "entities": []}, {"text": "In this section, we implement the translation model of and train it using the EM algorithm.", "labels": [], "entities": []}, {"text": "shows a portion of the bilingual English-tree/Japanese-string corpus used in and here.", "labels": [], "entities": []}, {"text": "show the generative model and parameters; the parameter values shown were learned via specialized EM re-estimation formulae described in this article's appendix.", "labels": [], "entities": []}, {"text": "According to the model, an English tree becomes a Japanese string in four steps.", "labels": [], "entities": []}, {"text": "First, every node is re-ordered, that is, its children are permuted probabilistically.", "labels": [], "entities": []}, {"text": "If there are three children, then there are six possible permutations whose probabilities add up to 1.", "labels": [], "entities": []}, {"text": "The re-ordering depends only on the child label sequence, and not on any wider or deeper context.", "labels": [], "entities": []}, {"text": "Note that the English trees in are already flattened in pre-processing because the model cannot perform complex re-orderings such as the one we described in Section 1, S(PRO,VP(V,NP)) \u2192 V, PRO, NP.", "labels": [], "entities": []}, {"text": "In this section, we demonstrate another application of the xTs training algorithm.", "labels": [], "entities": []}, {"text": "We show its generality by applying it to the standard task of training a probabilistic contextfree grammar (PCFG) on string examples.", "labels": [], "entities": []}, {"text": "Consider the following grammar: Viterbi parses for the strings can also be obtained from the derivations forests computed by the SDeriv procedure.", "labels": [], "entities": []}, {"text": "We note that our use of xTs training relies on copying.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  A comparison of the three transducer models used to simulate the model of Yamada and  Knight (2001).", "labels": [], "entities": []}]}