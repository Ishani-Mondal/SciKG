{"title": [{"text": "A Twin-Candidate Model for Learning-Based Anaphora Resolution", "labels": [], "entities": []}], "abstractContent": [{"text": "The traditional single-candidate learning model for anaphora resolution considers the antecedent candidates of an anaphor in isolation, and thus cannot effectively capture the preference relationships between competing candidates for its learning and resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7993124127388}]}, {"text": "To deal with this problem, we propose a twin-candidate model for anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8101257383823395}]}, {"text": "The main idea behind the model is to recast anaphora resolution as a preference classification problem.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7106566876173019}, {"text": "preference classification", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.7414474189281464}]}, {"text": "Specifically, the model learns a classifier that determines the preference between competing candidates, and, during resolution, chooses the antecedent of a given anaphor based on the ranking of the candidates.", "labels": [], "entities": []}, {"text": "We present in detail the framework of the twin-candidate model for anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.793185830116272}]}, {"text": "Further, we explore how to deploy the model in the more complicated coreference resolution task.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.9502869844436646}]}, {"text": "We evaluate the twin-candidate model in different domains using the Automatic Content Extraction data sets.", "labels": [], "entities": [{"text": "Automatic Content Extraction data sets", "start_pos": 68, "end_pos": 106, "type": "DATASET", "confidence": 0.720122092962265}]}, {"text": "The experimental results indicate that our twin-candidate model is superior to the single-candidate model for the task of pronominal anaphora resolution.", "labels": [], "entities": [{"text": "pronominal anaphora resolution", "start_pos": 122, "end_pos": 152, "type": "TASK", "confidence": 0.7332665324211121}]}, {"text": "For the task of coreference resolution, it also performs equally well, or better.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.9747839272022247}]}], "introductionContent": [{"text": "Anaphora is reference to an entity that has been previously introduced into the discourse (Jurafsky and Martin 2000).", "labels": [], "entities": []}, {"text": "The referring expression used is called the anaphor and the expression being referred to is its antecedent.", "labels": [], "entities": []}, {"text": "The anaphor is usually used to refer to the same entity as the antecedent; hence, they are coreferential with each other.", "labels": [], "entities": []}, {"text": "The process of determining the antecedent of an anaphor is called anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.6795691698789597}]}, {"text": "As a key problem in discourse and language understanding, anaphora resolution is crucial in many natural language applications, such as machine translation, text summarization, question answering, information extraction, and soon.", "labels": [], "entities": [{"text": "discourse and language understanding", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.7138525694608688}, {"text": "anaphora resolution", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7935379147529602}, {"text": "machine translation", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.8041014671325684}, {"text": "text summarization", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.7294763326644897}, {"text": "question answering", "start_pos": 177, "end_pos": 195, "type": "TASK", "confidence": 0.8973175287246704}, {"text": "information extraction", "start_pos": 197, "end_pos": 219, "type": "TASK", "confidence": 0.8694440722465515}]}, {"text": "In recent years, supervised learning approaches have been widely applied to anaphora resolution, and they have achieved considerable success ().", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8880891799926758}]}, {"text": "The strength of learning-based anaphora resolution is that resolution regularities can be automatically learned from annotated data.", "labels": [], "entities": [{"text": "learning-based anaphora resolution", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.6551289061705271}]}, {"text": "Traditionally, learning-based approaches to anaphora resolution adopt the single-candidate model, in which the potential antecedents (i.e., antecedent candidates) are considered in isolation for both learning and resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8096003234386444}]}, {"text": "In such a model, the purpose of classification is to determine if a candidate is the antecedent of a given anaphor.", "labels": [], "entities": []}, {"text": "A training or testing instance is formed by an anaphor and each of its candidates, with features describing the properties of the anaphor and the individual candidate.", "labels": [], "entities": []}, {"text": "During resolution, the antecedent of an anaphor is selected based on the classification results for each candidate.", "labels": [], "entities": [{"text": "resolution", "start_pos": 7, "end_pos": 17, "type": "TASK", "confidence": 0.9734209775924683}]}, {"text": "One assumption behind the single-candidate model is that whether a candidate is the antecedent of an anaphor is completely independent of the other competing candidates.", "labels": [], "entities": []}, {"text": "However, anaphora resolution can be more accurately represented as a ranking problem in which candidates are ordered based on their preference and the best one is the antecedent of the anaphor (Jurafsky and Martin 2000).", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.910595178604126}]}, {"text": "The single-candidate model, which only considers the candidates of an anaphor in isolation, is incapable of effectively capturing the preference relationship between candidates for its training.", "labels": [], "entities": []}, {"text": "Consequently, the learned classifier cannot produce reliable results for preference determination during resolution.", "labels": [], "entities": [{"text": "preference determination during resolution", "start_pos": 73, "end_pos": 115, "type": "TASK", "confidence": 0.6178851947188377}]}, {"text": "To deal with this problem, we propose a twin-candidate learning model for anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.8001407384872437}]}, {"text": "The main idea behind the model is to recast anaphora resolution as a preference classification problem.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7106566876173019}, {"text": "preference classification", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.7414474189281464}]}, {"text": "The purpose of the classification is to determine the preference between two competing candidates for the antecedent of a given anaphor.", "labels": [], "entities": []}, {"text": "In the model, an instance is formed by an anaphor and two of its antecedent candidates, with features used to describe their properties and relationships.", "labels": [], "entities": []}, {"text": "The antecedent is selected based on the judged preference among the candidates.", "labels": [], "entities": []}, {"text": "In the article we focus on two issues about the twin-candidate model.", "labels": [], "entities": []}, {"text": "In the first part, we will introduce the framework of the twin-candidate model for anaphora resolution, including detailed training procedures and resolution schemes.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7987608015537262}]}, {"text": "In the second part, we will further explore how to deploy the twin-candidate model in the more complicated task of coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.9778424203395844}]}, {"text": "We will present an empirical evaluation of the twin-candidate model in different domains, using the Automatic Content Extraction (ACE) data sets.", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE) data sets", "start_pos": 100, "end_pos": 144, "type": "DATASET", "confidence": 0.757285550236702}]}, {"text": "The experimental results indicate that the twin-candidate model is superior to the single-candidate model for the task of pronominal anaphora resolution.", "labels": [], "entities": [{"text": "pronominal anaphora resolution", "start_pos": 122, "end_pos": 152, "type": "TASK", "confidence": 0.7396061817804972}]}, {"text": "For the coreference resolution task, it also performs equally well, or better.", "labels": [], "entities": [{"text": "coreference resolution task", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.9478519360224406}]}], "datasetContent": [{"text": "We used the same ACE data sets for coreference resolution evaluation, as described in the previous section for anaphora resolution.", "labels": [], "entities": [{"text": "ACE data sets", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.9583558837572733}, {"text": "coreference resolution evaluation", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.9605528712272644}, {"text": "anaphora resolution", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7964658141136169}]}, {"text": "A raw input document was processed in advance by the same pipeline of NLP modules including POS-tagger, NP chunker, NE recognizer, and soon, to obtain all possible NPs and related information (see Section 3.5.1).", "labels": [], "entities": []}, {"text": "For evaluation, we adopted scoring algorithm in which recall and precision 17 were computed by comparing the key chains (i.e., the annotated \"standard\" coreferential chains) and the response chains (i.e., the chains generated by the coreference resolution system).", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.99806147813797}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9888533353805542}, {"text": "coreference resolution", "start_pos": 233, "end_pos": 255, "type": "TASK", "confidence": 0.8332163095474243}]}, {"text": "As already mentioned, the twin-candidate model described in this section is mainly meant for non-pronouns that are often not anaphoric.", "labels": [], "entities": []}, {"text": "To better examine the utility of the model in our experiments, we first focused on coreference resolution for nonpronominal NPs.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.9623457491397858}]}, {"text": "The recall and precision to be reported were computed based on the response chains and the key chains from which all the pronouns are removed.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9994797110557556}, {"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9995346069335938}]}, {"text": "We will later show the results of overall coreference resolution for whole NPs by combining the resolution of pronouns and non-pronouns.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.9439771771430969}]}, {"text": "In non-pronoun resolution, an anaphor and its antecedent do not often occur a short distance apart as they do in pronoun resolution.", "labels": [], "entities": [{"text": "non-pronoun resolution", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.7437165081501007}, {"text": "pronoun resolution", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7586492896080017}]}, {"text": "For this reason, during training, we took as antecedent candidates all the preceding non-pronominal NPs in the current and previous four sentences; while during testing, we used all the preceding nonpronouns, regardless of distance, as candidates.", "labels": [], "entities": []}, {"text": "The statistics of the training instances for each data set are summarized in.", "labels": [], "entities": []}, {"text": "Again, we examined the three learning algorithms: C5, MaxEnt, and SVM.", "labels": [], "entities": []}, {"text": "20 As both the single-candidate and the twin-candidate models used a threshold to block lowconfidence coreferential pairs, we performed three-fold cross-evaluation on the training data to determine the thresholds for the coreference resolution systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 221, "end_pos": 243, "type": "TASK", "confidence": 0.9345231354236603}]}, {"text": "lists the results for the different systems on the non-pronominal NP coreference resolution.", "labels": [], "entities": [{"text": "NP coreference resolution", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.7562089264392853}]}, {"text": "We used as the baseline the system with the single-candidate model described in Section 4.1.", "labels": [], "entities": []}, {"text": "As mentioned, the system was trained The overall F-measure was defined as 2 * Recall * Precision Recall + Precision 18 As suggested in Ng and Cardie (2002b), we did not include pronouns in the candidate set of a non-pronoun, because a pronoun is usually anaphoric and cannot give much information about the entity to which it refers.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9900181293487549}]}, {"text": "19 Unlike in the case of pronoun resolution, we did not filter candidates that had mismatched number/gender agreement as these constraints are not reliable for non-pronoun resolution (e.g., in our data set, around 15% of coreferential pairs do not agree in number).", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8338358402252197}, {"text": "non-pronoun resolution", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.8687181770801544}]}, {"text": "Instead, we took these factors as features (see) and let the learning algorithm make the preference decision.", "labels": [], "entities": []}, {"text": "20 For SVM, we employed the one-against-all aggregation method for the 3-class learning and testing.", "labels": [], "entities": [{"text": "SVM", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.8693650960922241}]}], "tableCaptions": [{"text": " Table 6  Training instances generated under the twin-candidate model for anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7347205430269241}]}, {"text": " Table 7  Test instances generated under the twin-candidate model with the Tournament Elimination  scheme.", "labels": [], "entities": []}, {"text": " Table 8  Test instances generated under the twin-candidate model with the Round Robin scheme.", "labels": [], "entities": []}, {"text": " Table 10  Statistics for the training and testing data sets.", "labels": [], "entities": []}, {"text": " Table 12  Accuracy in percent for the pronominal anaphora resolution.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9993115663528442}, {"text": "pronominal anaphora resolution", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.583563874165217}]}, {"text": " Table 14  Training instances generated under the single-candidate model for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.9781663715839386}]}, {"text": " Table 15  Feature set for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.9875977337360382}]}, {"text": " Table 16  Training instances generated under the twin-candidate model for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.9779563546180725}]}, {"text": " Table 17  Statistics of the training instances generated for coreference resolution (non-pronoun).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.955073893070221}]}, {"text": " Table 19  Recall (R), Precision (P), and F-Measure (F) in percent for coreference resolution.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9538267850875854}, {"text": "Precision (P)", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9610296189785004}, {"text": "F-Measure (F)", "start_pos": 42, "end_pos": 55, "type": "METRIC", "confidence": 0.9719270467758179}, {"text": "coreference resolution", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.9648866951465607}]}]}