{"title": [{"text": "Survey Article Inter-Coder Agreement for Computational Linguistics", "labels": [], "entities": []}], "abstractContent": [{"text": "This article is a survey of methods for measuring agreement among corpus annotators.", "labels": [], "entities": []}, {"text": "It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff's alpha as well as Scott's pi and Cohen's kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, maybe more appropriate for many corpus annotation tasks-but that their use makes the interpretation of the value of the coefficient even harder.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1  A simple example of agreement on dialogue act tagging.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.6723265846570333}]}, {"text": " Table 3  Agreement table with three coders.", "labels": [], "entities": []}, {"text": " Table 4  An integrated coding example.", "labels": [], "entities": []}, {"text": " Table 5  Unweighted coefficients for the data from", "labels": [], "entities": [{"text": "Unweighted", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9082406759262085}]}, {"text": " Table 8  Fewer boundaries, higher expected agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9573813676834106}]}]}