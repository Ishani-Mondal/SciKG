{"title": [{"text": "The Importance of Syntactic Parsing and Inference in Semantic Role Labeling BBN Technologies", "labels": [], "entities": [{"text": "Syntactic Parsing and Inference", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.796366959810257}, {"text": "Semantic Role Labeling BBN", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.695989340543747}]}], "abstractContent": [{"text": "We present a general framework for semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7550493677457174}]}, {"text": "The framework combines a machine-learning technique with an integer linear programming-based inference procedure, which incorporates linguistic and structural constraints into a global decision process.", "labels": [], "entities": []}, {"text": "Within this framework, we study the role of syntactic parsing information in semantic role labeling.", "labels": [], "entities": [{"text": "syntactic parsing information", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.7955959041913351}, {"text": "semantic role labeling", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.6967317660649618}]}, {"text": "We show that full syntactic parsing information is, by far, most relevant in identifying the argument, especially, in the very first stage-the pruning stage.", "labels": [], "entities": [{"text": "syntactic parsing information", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.7716903289159139}]}, {"text": "Surprisingly, the quality of the pruning stage cannot be solely determined based on its recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9996181726455688}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9966322779655457}]}, {"text": "Instead, it depends on the characteristics of the output candidates that determine the difficulty of the downstream problems.", "labels": [], "entities": []}, {"text": "Motivated by this observation, we propose an effective and simple approach of combining different semantic role labeling systems through joint inference, which significantly improves its performance.", "labels": [], "entities": []}, {"text": "Our system has been evaluated in the CoNLL-2005 shared task on semantic role labeling, and achieves the highest F 1 score among 19 participants.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.6111814181009928}, {"text": "F 1 score", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9919412533442179}]}], "introductionContent": [{"text": "Semantic parsing of sentences is believed to bean important task on the road to natural language understanding, and has immediate applications in tasks such as information extraction and question answering.", "labels": [], "entities": [{"text": "Semantic parsing of sentences", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8620373159646988}, {"text": "natural language understanding", "start_pos": 80, "end_pos": 110, "type": "TASK", "confidence": 0.6769719123840332}, {"text": "information extraction", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.8416866362094879}, {"text": "question answering", "start_pos": 187, "end_pos": 205, "type": "TASK", "confidence": 0.9059244096279144}]}, {"text": "Semantic Role Labeling (SRL) is a shallow semantic parsing task, in which for each predicate in a sentence, the goal is to identify all constituents that fill a semantic role, and to determine their roles (Agent, Patient, Instrument, etc.) and their adjuncts (Locative, Temporal, Manner, etc.).", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8260444502035776}, {"text": "semantic parsing task", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.8019368847211202}]}, {"text": "The PropBank project, which provides a large human-annotated corpus of verb predicates and their arguments, has enabled researchers to apply machine learning techniques to develop SRL systems ().", "labels": [], "entities": [{"text": "SRL", "start_pos": 180, "end_pos": 183, "type": "TASK", "confidence": 0.9836486577987671}]}, {"text": "However, most systems rely heavily on full syntactic parse trees.", "labels": [], "entities": []}, {"text": "Therefore, the overall performance of the system is largely determined by the quality of the automatic syntactic parsers of which the state of the art) is still far from perfect.", "labels": [], "entities": []}, {"text": "Alternatively, shallow syntactic parsers (i.e., chunkers and clausers), although they do not provide as much information as a full syntactic parser, have been shown to be more robust in their specific tasks (.", "labels": [], "entities": []}, {"text": "This raises the very natural and interesting question of quantifying the importance of full parsing information to semantic parsing and whether it is possible to use only shallow syntactic information to build an outstanding SRL system.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 115, "end_pos": 131, "type": "TASK", "confidence": 0.7070737183094025}, {"text": "SRL", "start_pos": 225, "end_pos": 228, "type": "TASK", "confidence": 0.9756757020950317}]}, {"text": "Although PropBank is built by adding semantic annotations to the constituents in the Penn Treebank syntactic parse trees, it is not clear how important syntactic parsing is for an SRL system.", "labels": [], "entities": [{"text": "Penn Treebank syntactic parse trees", "start_pos": 85, "end_pos": 120, "type": "DATASET", "confidence": 0.9576971769332886}, {"text": "syntactic parsing", "start_pos": 152, "end_pos": 169, "type": "TASK", "confidence": 0.7137544751167297}, {"text": "SRL", "start_pos": 180, "end_pos": 183, "type": "TASK", "confidence": 0.960719645023346}]}, {"text": "To the best of our knowledge, this problem was first addressed by.", "labels": [], "entities": []}, {"text": "In their attempt to use limited syntactic information, the parser they used was very shallow-clauses were not available and only chunks were used.", "labels": [], "entities": []}, {"text": "Moreover, the pruning stage there was very strict-only chunks were considered as argument candidates.", "labels": [], "entities": []}, {"text": "This results in over 60% of the actual arguments being ignored.", "labels": [], "entities": []}, {"text": "Consequently, the overall recall in their approach was very low.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9996815919876099}]}, {"text": "The use of only shallow parsing information in an SRL system has largely been ignored until the recent CoNLL-2004 shared task competition.", "labels": [], "entities": [{"text": "SRL", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.8992453217506409}, {"text": "CoNLL-2004 shared task competition", "start_pos": 103, "end_pos": 137, "type": "TASK", "confidence": 0.6243196278810501}]}, {"text": "In that competition, participants were restricted to using only shallow parsing information, which included part-of-speech tags, chunks, and clauses (the definitions of chunks and clauses can be found in Tjong Kim Sang and Buchholz and Carreras et al., respectively).", "labels": [], "entities": [{"text": "Tjong Kim Sang", "start_pos": 204, "end_pos": 218, "type": "DATASET", "confidence": 0.8539337913195292}]}, {"text": "As a result, the performance of the best shallow parsingbased system () in the competition is about 10 points in F 1 below the best system that uses full parsing information ().", "labels": [], "entities": [{"text": "F", "start_pos": 113, "end_pos": 114, "type": "METRIC", "confidence": 0.9955832362174988}]}, {"text": "However, this is not the outcome of a true and fair quantitative comparison.", "labels": [], "entities": []}, {"text": "The CoNLL-2004 shared task used only a subset of the data for training, which potentially makes the problem harder.", "labels": [], "entities": []}, {"text": "Furthermore, an SRL system is usually complicated and consists of several stages.", "labels": [], "entities": [{"text": "SRL", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9914485812187195}]}, {"text": "It was still unclear how much syntactic information helps and precisely where it helps the most.", "labels": [], "entities": []}, {"text": "The goal of this paper is threefold.", "labels": [], "entities": []}, {"text": "First, we describe an architecture for an SRL system that incorporates a level of global inference on top of the relatively common processing steps.", "labels": [], "entities": [{"text": "SRL", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9759783148765564}]}, {"text": "This inference step allows us to incorporate structural and linguistic constraints over the possible outcomes of the argument classifier in an easy way.", "labels": [], "entities": []}, {"text": "The inference procedure is formalized via an Integer Linear Programming framework and is shown to yield state-of-the-art results on this task.", "labels": [], "entities": []}, {"text": "Second, we provide a fair comparison between SRL systems that use full parse trees and systems that only use shallow syntactic information.", "labels": [], "entities": [{"text": "SRL", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9691787958145142}]}, {"text": "As with our full syntactic parse-based SRL system (), our shallow parsing-based SRL system is based on the system that achieves very competitive results and was one of the top systems in the CoNLL-2004 shared task competition).", "labels": [], "entities": [{"text": "shallow parsing-based SRL", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.5660509268442789}]}, {"text": "This comparison brings forward a careful analysis of the significance of full parsing information in the SRL task, and provides an understanding of the stages in the process in which this information makes the most difference.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 105, "end_pos": 113, "type": "TASK", "confidence": 0.9224736988544464}]}, {"text": "Finally, to relieve the dependency of the SRL system on the quality of automatic parsers, we suggest away to improve semantic role labeling significantly by developing a global inference algorithm, which is used to combine several SRL systems based on different state-of-the-art full parsers.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.61381796002388}]}, {"text": "The combination process is done through a joint inference stage, which takes the output of each individual system as input and generates the best predictions, subject to various structural and linguistic constraints.", "labels": [], "entities": []}, {"text": "The underlying system architecture can largely affect the outcome of our study.", "labels": [], "entities": []}, {"text": "Therefore, to make the conclusions of our experimental study as applicable as possible to general SRL systems, the architecture of our SRL system follows the most widely used two-step design.", "labels": [], "entities": [{"text": "SRL", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9604706168174744}, {"text": "SRL", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.8823580741882324}]}, {"text": "In the first step, the system is trained to identify argument candidates fora given verb predicate.", "labels": [], "entities": []}, {"text": "In the second step, the system classifies the argument candidates into their types.", "labels": [], "entities": []}, {"text": "In addition, it is also a simple procedure to prune obvious non-candidates before the first step, and to use post-processing inference to fix inconsistent predictions after the second step.", "labels": [], "entities": []}, {"text": "These two additional steps are also employed by our system.", "labels": [], "entities": []}, {"text": "Our study of shallow and full syntactic information-based SRL systems was done by comparing their impact at each stage of the process.", "labels": [], "entities": [{"text": "SRL", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.8601598143577576}]}, {"text": "Specifically, our goal is to investigate at what stage full parsing information is most helpful relative to a shallow parsingbased system.", "labels": [], "entities": [{"text": "parsing information", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8433325886726379}]}, {"text": "Therefore, our experiments were designed so that the compared systems are as similar as possible, and the addition of the full parse tree-based features is the only difference.", "labels": [], "entities": []}, {"text": "The most interesting result of this comparison is that although each step of the shallow parsing information-based system exhibits very good performance, the overall performance is significantly inferior to the system that uses full parsing information.", "labels": [], "entities": [{"text": "shallow parsing information-based", "start_pos": 81, "end_pos": 114, "type": "TASK", "confidence": 0.6609900593757629}]}, {"text": "Our explanation is that chaining multiple processing stages to produce the final SRL analysis is crucial to understanding this analysis.", "labels": [], "entities": [{"text": "SRL analysis", "start_pos": 81, "end_pos": 93, "type": "TASK", "confidence": 0.8961917757987976}]}, {"text": "Specifically, the quality of the information passed from one stage to the other is a decisive issue, and it is not necessarily judged simply by considering the F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.975475013256073}]}, {"text": "We conclude that, for the system architecture used in our study, the significance of full parsing information comes into play mostly at the pruning stage, where the candidates to be processed later are determined.", "labels": [], "entities": []}, {"text": "In addition, we produce a state-of-the-art SRL system by combining different SRL systems based on two automatic full parsers, which achieves the best result in the.", "labels": [], "entities": [{"text": "SRL", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9106461405754089}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the task of semantic role labeling in more detail.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.760477602481842}]}, {"text": "Section 3 describes the four-stage architecture of our SRL system, which includes pruning, argument identification, argument classification, and inference.", "labels": [], "entities": [{"text": "SRL", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9709811210632324}, {"text": "argument identification", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.7337532490491867}, {"text": "argument classification", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.7190273553133011}]}, {"text": "The features used for building the classifiers and the learning algorithm applied are also explained there.", "labels": [], "entities": []}, {"text": "Section 4 explains why and where full parsing information contributes to SRL by conducting a series of carefully designed experiments.", "labels": [], "entities": [{"text": "parsing information", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8654576539993286}, {"text": "SRL", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9930797219276428}]}, {"text": "Inspired by the result, we examine the effect of inference in a single system and propose an approach that combines different SRL systems based on joint inference in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 presents the empirical evaluation of our system in the CoNLL-2005 shared task competition.", "labels": [], "entities": [{"text": "CoNLL-2005 shared task competition", "start_pos": 65, "end_pos": 99, "type": "DATASET", "confidence": 0.7448986768722534}]}, {"text": "After that, we discuss the related work in Section 7 and conclude this paper in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use PropBank Sections 02 through 21 as training data, Section 23 as testing, and Section 24 as a validation set when necessary.", "labels": [], "entities": []}, {"text": "In order to apply the standard CoNLL shared task evaluation script, our system conforms to both the input and output format defined in the shared task.", "labels": [], "entities": []}, {"text": "The goal of the experiments in this section is to understand the effective contribution of full parsing information versus shallow parsing information (i.e., using only the part-of-speech tags, chunks, and clauses).", "labels": [], "entities": []}, {"text": "In addition, we also compare performance when using the correct (gold-standard) data versus using automatic parse data.", "labels": [], "entities": []}, {"text": "The performance is measured in terms of precision, recall, and the F 1 measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9997581839561462}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9997972846031189}, {"text": "F 1 measure", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.9917877515157064}]}, {"text": "Note that all the numbers reported here do not take into account the V arguments as it is quite trivial to predict V and, hence, this gives overoptimistic overall performance if included.", "labels": [], "entities": []}, {"text": "When doing the comparison, we also compute the 95% confidence interval of F 1 using the bootstrap resampling method, and the difference is considered significant if the compared F 1 lies outside this interval.", "labels": [], "entities": [{"text": "95% confidence interval of F 1", "start_pos": 47, "end_pos": 77, "type": "METRIC", "confidence": 0.7265199422836304}]}, {"text": "The automatic full parse trees are derived using Charniak's parser  In this section, we present the detailed evaluation of our SRL system, in the competition on semantic role labeling-the CoNLL-2005 shared task (Carreras and M` arquez).", "labels": [], "entities": [{"text": "semantic role labeling-the CoNLL-2005 shared task", "start_pos": 161, "end_pos": 210, "type": "TASK", "confidence": 0.7333160440127054}]}, {"text": "The setting of this shared task is basically the same as it was in 2004, with some extensions.", "labels": [], "entities": []}, {"text": "First, it allows much richer syntactic information.", "labels": [], "entities": []}, {"text": "In particular, full parse trees generated using Collins's parser) and Charniak's parser (Charniak 2001) were provided.", "labels": [], "entities": []}, {"text": "Second, the full parsing standard partition was usedthe training set was enlarged and covered Sections 02-21, the development set was Section 24, and the test set was Section 23.", "labels": [], "entities": []}, {"text": "Finally, in addition to the Wall Street Journal (WSJ) data, three sections of the Brown corpus were used to provide cross-corpora evaluation.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) data", "start_pos": 28, "end_pos": 58, "type": "DATASET", "confidence": 0.9474498544420514}, {"text": "Brown corpus", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.9488558173179626}]}, {"text": "The system we used to participate in the CoNLL-2005 shared task is an enhanced version of the system described in Sections 3 and 5.", "labels": [], "entities": []}, {"text": "The main difference was that the joint-inference stage was extended to combine six basic SRL systems instead of two.", "labels": [], "entities": [{"text": "SRL", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.8735226392745972}]}, {"text": "Specifically for this implementation, we first trained two SRL systems that use Collins's parser and Charniak's parser, respectively, because of their noticeably different outputs.", "labels": [], "entities": []}, {"text": "In evaluation, we ran the system that was trained with Charniak's parser five times, with the top-5 parse trees output by Charniak's parser.", "labels": [], "entities": []}, {"text": "Together we have six different outputs per predicate.", "labels": [], "entities": []}, {"text": "For each parse tree output, we ran the first three stages, namely, pruning, argument identification, and argument classification.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.7555800974369049}, {"text": "argument classification", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.7240146994590759}]}, {"text": "Then, a joint-inference stage, where each individual system is weighted equally, was used to resolve the inconsistency of the output of argument classification in these systems.", "labels": [], "entities": [{"text": "argument classification", "start_pos": 136, "end_pos": 159, "type": "TASK", "confidence": 0.7443881928920746}]}, {"text": "shows the overall results on the development set and different test sets; the detailed results on WSJ section 23 are shown in. shows the results of individual systems and the improvement gained by the joint inference procedure on the development set.", "labels": [], "entities": [{"text": "WSJ section 23", "start_pos": 98, "end_pos": 112, "type": "DATASET", "confidence": 0.8942307432492574}]}, {"text": "Our system reached the highest F 1 scores on all the test sets and was the best system among the 19 participating teams.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9863701264063517}]}, {"text": "After the competition, we improved the system slightly by tuning the weights of the individual systems in the joint inference procedure, where the F 1 scores on WSJ test section and the Brown test set are 79.59 points and 67.98 points, respectively.", "labels": [], "entities": [{"text": "F 1", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.9786358773708344}, {"text": "WSJ test section", "start_pos": 161, "end_pos": 177, "type": "DATASET", "confidence": 0.9461276133855184}, {"text": "Brown test set", "start_pos": 186, "end_pos": 200, "type": "DATASET", "confidence": 0.772931824127833}]}, {"text": "In terms of the computation time, for both the argument identifier and the argument classifier, the training of each model, excluding feature extraction, takes 50-70 minutes using less than 1GB memory on a 2.6GHz AMD machine.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.7029513865709305}]}, {"text": "On the same machine, the average test time for each stage, excluding feature extraction, is around 2 minutes.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.6794461011886597}]}], "tableCaptions": [{"text": " Table 2  The accuracy of argument classification when argument boundaries are known.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996383190155029}, {"text": "argument classification", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7204058766365051}]}, {"text": " Table 3  The overall system performance when argument boundaries are known.", "labels": [], "entities": []}, {"text": " Table 4  The performance of argument identification after pruning (based on the gold standard full parse  trees).", "labels": [], "entities": [{"text": "argument identification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.790303111076355}]}, {"text": " Table 5  The performance of argument identification after pruning (based on the gold-standard full parse  trees) and with threshold = 0.1.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7689894735813141}]}, {"text": " Table 6  The overall system performance using the output from the pruning heuristics, applied on the  gold-standard full parse trees.", "labels": [], "entities": []}, {"text": " Table 7  The performance of pruning using heuristics and classifiers.", "labels": [], "entities": []}, {"text": " Table 8  Statistics of the training and test examples for the pruning stage.", "labels": [], "entities": []}, {"text": " Table 9  The overall system performance.", "labels": [], "entities": []}, {"text": " Table 10  The impact of removing most constraints in overall system performance.", "labels": [], "entities": []}, {"text": " Table 11  The performance of individual and combined SRL systems.", "labels": [], "entities": [{"text": "SRL", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.971561849117279}]}, {"text": " Table 12  Overall CoNLL-2005 shared task results.", "labels": [], "entities": [{"text": "CoNLL-2005 shared task", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.4791862368583679}]}, {"text": " Table 13  Detailed CoNLL-2005 shared task results on the WSJ test set.", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9807477990786234}]}, {"text": " Table 14  The results of individual systems and the result with joint inference on the development set.", "labels": [], "entities": []}]}