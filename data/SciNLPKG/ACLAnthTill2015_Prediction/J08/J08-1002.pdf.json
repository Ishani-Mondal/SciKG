{"title": [{"text": "Feature Forest Models for Probabilistic HPSG Parsing", "labels": [], "entities": [{"text": "HPSG Parsing", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.520292192697525}]}], "abstractContent": [{"text": "Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures.", "labels": [], "entities": []}, {"text": "This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into sub-structures under the assumption of statistical independence among sub-structures.", "labels": [], "entities": []}, {"text": "For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules.", "labels": [], "entities": [{"text": "part-of-speech tagging of a sentence", "start_pos": 13, "end_pos": 49, "type": "TASK", "confidence": 0.8505926728248596}, {"text": "CFG parsing", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.7884138822555542}]}, {"text": "These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures.", "labels": [], "entities": []}, {"text": "This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures.", "labels": [], "entities": []}, {"text": "The feature forest model provides a method for probabilistic modeling without the independence assumption when prob-abilistic events are represented with feature forests.", "labels": [], "entities": []}, {"text": "Feature forests are generic data structures that represent ambiguous trees in a packed forest structure.", "labels": [], "entities": []}, {"text": "Feature forest models are maximum entropy models defined over feature forests.", "labels": [], "entities": []}, {"text": "A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests.", "labels": [], "entities": [{"text": "maximum entropy estimation", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.6487913032372793}]}, {"text": "Thus probabilistic modeling of any data structures is possible when they are represented by feature forests.", "labels": [], "entities": []}, {"text": "This article also describes methods for representing HPSG syntactic structures and predicate-argument structures with feature forests.", "labels": [], "entities": []}, {"text": "Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.7432757914066315}]}, {"text": "The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9962612390518188}, {"text": "parsing of real-world sentences", "start_pos": 153, "end_pos": 184, "type": "TASK", "confidence": 0.8775387853384018}]}], "introductionContent": [{"text": "Following the successful development of wide-coverage lexicalized grammars (, statistical modeling of these grammars is attracting considerable attention.", "labels": [], "entities": []}, {"text": "This is because natural language processing applications usually require disambiguated or ranked parse results, and statistical modeling of syntactic/semantic preference is one of the most promising methods for disambiguation.", "labels": [], "entities": []}, {"text": "The focus of this article is the problem of probabilistic modeling of wide-coverage HPSG parsing.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 84, "end_pos": 96, "type": "TASK", "confidence": 0.6800170242786407}]}, {"text": "Although previous studies have proposed maximum entropy models of HPSG-style parse trees), the straightforward application of maximum entropy models to wide-coverage HPSG parsing is infeasible because estimation of maximum entropy models is computationally expensive, especially when targeting wide-coverage parsing.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 166, "end_pos": 178, "type": "TASK", "confidence": 0.48281680047512054}]}, {"text": "In general, complete structures, such as transition sequences in Markov models and parse trees, have an exponential number of ambiguities.", "labels": [], "entities": []}, {"text": "This causes an exponential explosion when estimating the parameters of maximum entropy models.", "labels": [], "entities": []}, {"text": "We therefore require solutions to make model estimation tractable.", "labels": [], "entities": []}, {"text": "This article first proposes feature forest models, which area general solution to the problem of maximum entropy modeling of tree structures.", "labels": [], "entities": []}, {"text": "Our algorithm avoids exponential explosion by representing probabilistic events with feature forests, which are packed representations of tree structures.", "labels": [], "entities": []}, {"text": "When complete structures are represented with feature forests of a tractable size, the parameters of maximum entropy models are efficiently estimated without unpacking the feature forests.", "labels": [], "entities": []}, {"text": "This is due to dynamic programming similar to the algorithm for computing inside/outside probabilities in PCFG parsing.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 106, "end_pos": 118, "type": "TASK", "confidence": 0.7950925827026367}]}, {"text": "The latter half of this article (Section 4) is on the application of feature forest models to disambiguation in wide-coverage HPSG parsing.", "labels": [], "entities": [{"text": "wide-coverage HPSG parsing", "start_pos": 112, "end_pos": 138, "type": "TASK", "confidence": 0.6412742137908936}]}, {"text": "We describe methods for representing HPSG parse trees and predicate-argument structures using feature forests).", "labels": [], "entities": [{"text": "HPSG parse trees", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.6436984340349833}]}, {"text": "Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing.", "labels": [], "entities": [{"text": "wide-coverage HPSG parsing", "start_pos": 156, "end_pos": 182, "type": "TASK", "confidence": 0.6429597437381744}]}, {"text": "The methods we propose here were applied to an English HPSG parser, Enju).", "labels": [], "entities": []}, {"text": "We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (.", "labels": [], "entities": [{"text": "Wall Street Journal portion of the Penn Treebank", "start_pos": 86, "end_pos": 134, "type": "DATASET", "confidence": 0.9584506675601006}]}, {"text": "The content of this article is an extended version of our earlier work reported in) and Miyao,.", "labels": [], "entities": []}, {"text": "The major contribution of this article is a strict mathematical definition of the feature forest model and the parameter estimation algorithm, which are substantially refined and extended from.", "labels": [], "entities": []}, {"text": "Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 137, "end_pos": 149, "type": "TASK", "confidence": 0.8239179253578186}]}, {"text": "We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.7518353760242462}]}, {"text": "Section 2 discusses a problem of conventional probabilistic models for lexicalized grammars.", "labels": [], "entities": []}, {"text": "Section 3 proposes feature forest models for solving this problem.", "labels": [], "entities": []}, {"text": "Section 4 describes the application of feature forest models to probabilistic HPSG parsing.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 78, "end_pos": 90, "type": "TASK", "confidence": 0.7689516842365265}]}, {"text": "Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.72378870844841}]}], "datasetContent": [{"text": "This section presents experimental results on the parsing accuracy attained by the feature forest models.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9735779166221619}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.826848030090332}]}, {"text": "In all of the following experiments, we use the HPSG grammar developed by the method of Miyao, Ninomiya, and . Section 5.1 describes how this grammar was developed.", "labels": [], "entities": [{"text": "HPSG grammar", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.8971111476421356}]}, {"text": "Section 5.2 explains other aspects of the experimental settings.", "labels": [], "entities": []}, {"text": "In Sections 5.3 to 5.7, we report results of the experiments on HPSG parsing.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 64, "end_pos": 76, "type": "TASK", "confidence": 0.8213880062103271}]}, {"text": "The data for the training of the disambiguation models was the HPSG treebank derived from Sections 02-21 of the Wall Street Journal portion of the Penn Treebank, that is, the same set used for lexicon extraction.", "labels": [], "entities": [{"text": "HPSG treebank derived from Sections 02-21 of the Wall Street Journal portion of the Penn Treebank", "start_pos": 63, "end_pos": 160, "type": "DATASET", "confidence": 0.8332569934427738}, {"text": "lexicon extraction", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.7178770750761032}]}, {"text": "For training of the disambiguation models, we eliminated sentences of 40 words or more and sentences for which the parser could not produce the correct parses.", "labels": [], "entities": []}, {"text": "The resulting training set consists of 33,604 sentences (when n = 10 and \ud97b\udf59 = 0.95; see Section 5.4 for details).", "labels": [], "entities": []}, {"text": "The treebanks derived from Sections 22 and 23 were used as the development and final test sets, respectively.", "labels": [], "entities": []}, {"text": "Following previous studies on parsing with PCFG-based models, accuracy is measured for sentences of less than 40 words and for those with less than 100 words.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.973711371421814}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9994751811027527}]}, {"text": "shows the specifications of the test data.", "labels": [], "entities": []}, {"text": "The measure for evaluating parsing accuracy is precision/recall of predicateargument dependencies output by the parser.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9697778820991516}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.8684138655662537}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9994043111801147}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9739605188369751}]}, {"text": "A predicate-argument dependency is defined as a tuple \ud97b\udf59w h , w n , \u03c0, \u03c1\ud97b\udf59, where w h is the headword of the predicate, w n is the headword of the argument, \u03c0 is the type of the predicate (e.g., adjective, intransitive verb), and \u03c1 is an argument label (MODARG, ARG1, . . ., ARG4).", "labels": [], "entities": [{"text": "MODARG", "start_pos": 252, "end_pos": 258, "type": "DATASET", "confidence": 0.8784207701683044}, {"text": "ARG1", "start_pos": 260, "end_pos": 264, "type": "DATASET", "confidence": 0.8224576115608215}, {"text": "ARG4", "start_pos": 273, "end_pos": 277, "type": "DATASET", "confidence": 0.8708576560020447}]}, {"text": "For example, He tried running has three dependencies as follows: r \ud97b\udf59tried, he, transitive verb, ARG1\ud97b\udf59 Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified by the parser, and unlabeled precision/recall (UP/UR) is the ratio of w hand w n correctly identified regardless of \u03c0 and \u03c1.", "labels": [], "entities": [{"text": "ARG1\ud97b\udf59 Labeled precision/recall (LP/LR)", "start_pos": 96, "end_pos": 134, "type": "METRIC", "confidence": 0.8678084015846252}, {"text": "precision/recall (UP/UR)", "start_pos": 208, "end_pos": 232, "type": "METRIC", "confidence": 0.8212704993784428}]}, {"text": "F-score is the harmonic mean of LP and LR.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9818319082260132}, {"text": "LP", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9559939503669739}]}, {"text": "Sentence accuracy is the exact match accuracy of complete predicate-argument relations in a sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.6161666512489319}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.7784507870674133}]}, {"text": "These measures correspond to those used in other studies measuring the accuracy of predicate-argument dependencies in CCG parsing and LFG parsing (), although exact figures cannot be compared directly because the definitions of dependencies are different.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.997456967830658}, {"text": "CCG parsing", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.6385108381509781}, {"text": "LFG parsing", "start_pos": 134, "end_pos": 145, "type": "TASK", "confidence": 0.6421000063419342}]}, {"text": "All predicate-argument dependencies in a sentence are the target of evaluation except quotation marks and periods.", "labels": [], "entities": []}, {"text": "The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995250701904297}, {"text": "Penn Treebank", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.9944571852684021}]}, {"text": "The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 131, "end_pos": 138, "type": "METRIC", "confidence": 0.9940569400787354}]}, {"text": "The algorithm for parameter estimation was the limited-memory BFGS method).", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.6997550427913666}, {"text": "BFGS", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.4690672755241394}]}, {"text": "The parser was implemented in C++ with the LiLFeS library (, and various speed-up techniques for HPSG parsing were used such as quick check and iterative beam search).", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 97, "end_pos": 109, "type": "TASK", "confidence": 0.7267275154590607}, {"text": "beam search", "start_pos": 154, "end_pos": 165, "type": "TASK", "confidence": 0.7281064391136169}]}, {"text": "Other efficient parsing techniques, including global thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were not used.", "labels": [], "entities": []}, {"text": "The results obtained using these techniques are given in Ninomiya et al.", "labels": [], "entities": []}, {"text": "A limit on the number of constituents was set for time-out; the parser stopped parsing when the number of constituents created during parsing exceeded 50,000.", "labels": [], "entities": [{"text": "parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.9585879445075989}]}, {"text": "In such a case, the parser output nothing, and the recall was computed as zero.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9987168312072754}]}, {"text": "Features occurring more than twice were included in the probabilistic models.", "labels": [], "entities": []}, {"text": "A method of filtering lexical entries was applied to the parsing of training data (Section 4.4).", "labels": [], "entities": [{"text": "parsing of training", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8799873193105062}]}, {"text": "Unless otherwise noted, parameters for filtering were n = 10 and \ud97b\udf59 = 0.95, and a reference distribution method was applied.", "labels": [], "entities": []}, {"text": "The unigram model, p 0 (t|s), for filtering is a maximum entropy model with two feature templates, \ud97b\udf59WORD, POS, LE\ud97b\udf59 and \ud97b\udf59POS, LE\ud97b\udf59.", "labels": [], "entities": [{"text": "\ud97b\udf59WORD, POS, LE", "start_pos": 99, "end_pos": 113, "type": "METRIC", "confidence": 0.6878535250822703}]}, {"text": "The model includes 24,847 features.", "labels": [], "entities": []}, {"text": "show parsing accuracy for the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9898195862770081}]}, {"text": "In the tables, \"Syntactic features\" denotes a model with syntactic features, that is, f binary , f unary , and f root introduced  in Section 4.5.", "labels": [], "entities": []}, {"text": "\"Semantic features\" represents a model with features on predicateargument structures, that is, f pa given in.", "labels": [], "entities": []}, {"text": "\"All\" is a model with both syntactic and semantic features.", "labels": [], "entities": []}, {"text": "The \"Baseline\" row shows the results for the reference model, p 0 (t|s), used for lexical entry filtering in the estimation of the other models.", "labels": [], "entities": [{"text": "lexical entry filtering", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.6813427110513052}]}, {"text": "This model is considered as a simple application of a traditional PCFG-style model; that is, p(r) = 1 for any ruler in the construction rules of the HPSG grammar.", "labels": [], "entities": [{"text": "HPSG grammar", "start_pos": 149, "end_pos": 161, "type": "DATASET", "confidence": 0.9005654454231262}]}], "tableCaptions": [{"text": " Table 5  Specification of test data for the evaluation of parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 59, "end_pos": 66, "type": "TASK", "confidence": 0.9802982211112976}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9417093992233276}]}, {"text": " Table 6  Accuracy of predicate-argument relations (test set, <40 words).", "labels": [], "entities": []}, {"text": " Table 7  Accuracy of predicate-argument relations (test set, <100 words).", "labels": [], "entities": []}, {"text": " Table 8  Computation/space costs of model estimation.", "labels": [], "entities": [{"text": "model estimation", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.6615115404129028}]}, {"text": " Table 9  Estimation method vs. accuracy and estimation time.", "labels": [], "entities": [{"text": "Estimation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9594615697860718}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9995668530464172}, {"text": "estimation time", "start_pos": 45, "end_pos": 60, "type": "METRIC", "confidence": 0.9243527054786682}]}, {"text": " Table 10  Filtering threshold vs. accuracy.", "labels": [], "entities": [{"text": "Filtering threshold", "start_pos": 11, "end_pos": 30, "type": "METRIC", "confidence": 0.8815888166427612}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9983069896697998}]}, {"text": " Table 11  Filtering threshold vs. estimation cost.", "labels": [], "entities": []}, {"text": " Table 12  Accuracy with different feature sets.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9907671213150024}]}, {"text": " Table 13  Accuracy for covered/uncovered sentences.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9978943467140198}]}, {"text": " Table 14  Accuracy with automatic parts-of-speech tags (test set).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9833722710609436}]}, {"text": " Table 15  Classification of disambiguation errors.", "labels": [], "entities": []}]}