{"title": [{"text": "Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation", "labels": [], "entities": [{"text": "Wide-Coverage Deep Statistical Parsing", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.4852968826889992}]}], "abstractContent": [{"text": "A number of researchers have recently conducted experiments comparing \"deep\" hand-crafted wide-coverage with \"shallow\" treebank-and machine-learning-based parsers at the level of dependencies, using simple and automatic methods to convert tree output generated by the shallow parsers into dependencies.", "labels": [], "entities": []}, {"text": "In this article, we revisit such experiments, this time using sophisticated automatic LFG f-structure annotation methodologies with surprising results.", "labels": [], "entities": []}, {"text": "We compare various PCFG and history-based parsers to find a baseline parsing system that fits best into our automatic dependency structure annotation technique.", "labels": [], "entities": []}, {"text": "This combined system of syntactic parser and dependency structure annotation is compared to two hand-crafted, deep constraint-based parsers, RASP and XLE.", "labels": [], "entities": []}, {"text": "We evaluate using dependency-based gold standards Computational Linguistics Volume 34, Number 1 and use the Approximate Randomization Test to test the statistical significance of the results.", "labels": [], "entities": [{"text": "Computational Linguistics Volume 34", "start_pos": 50, "end_pos": 85, "type": "DATASET", "confidence": 0.6277565658092499}, {"text": "Approximate Randomization Test", "start_pos": 108, "end_pos": 138, "type": "METRIC", "confidence": 0.8976702094078064}]}, {"text": "Our experiments show that machine-learning-based shallow grammars augmented with sophisticated automatic dependency annotation technology outperform hand-crafted, deep, wide-coverage constraint grammars.", "labels": [], "entities": []}, {"text": "Currently our best system achieves an f-score of 82.73% against the PARC 700 Dependency Bank, a statistically significant improvement of 2.18% over the most recent results of 80.55% for the hand-crafted LFG grammar and XLE parsing system and an f-score of 80.23% against the CBS 500 Dependency Bank, a statistically significant 3.66% improvement over the 76.57% achieved by the hand-crafted RASP grammar and parsing system.", "labels": [], "entities": [{"text": "f-score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9901665449142456}, {"text": "PARC 700 Dependency Bank", "start_pos": 68, "end_pos": 92, "type": "DATASET", "confidence": 0.9878981113433838}, {"text": "LFG grammar and XLE parsing", "start_pos": 203, "end_pos": 230, "type": "TASK", "confidence": 0.6634301781654358}, {"text": "f-score", "start_pos": 245, "end_pos": 252, "type": "METRIC", "confidence": 0.9912251830101013}, {"text": "CBS 500 Dependency Bank", "start_pos": 275, "end_pos": 298, "type": "DATASET", "confidence": 0.9804777801036835}, {"text": "RASP grammar and parsing", "start_pos": 391, "end_pos": 415, "type": "TASK", "confidence": 0.6440639197826385}]}], "introductionContent": [{"text": "Wide-coverage parsers are often evaluated against gold-standard CFG trees (e.g., Penn-II WSJ Section 23 trees) reporting traditional PARSEVAL metrics of labeled and unlabeled bracketing precision, recall and f-score measures, number of crossing brackets, complete matches, and so forth.", "labels": [], "entities": [{"text": "CFG trees", "start_pos": 64, "end_pos": 73, "type": "DATASET", "confidence": 0.9053798615932465}, {"text": "Penn-II WSJ Section 23 trees", "start_pos": 81, "end_pos": 109, "type": "DATASET", "confidence": 0.9606294870376587}, {"text": "precision", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.9286776781082153}, {"text": "recall and f-score measures", "start_pos": 197, "end_pos": 224, "type": "METRIC", "confidence": 0.7944962829351425}]}, {"text": "Although tree-based parser evaluation provides valuable insights into the performance of grammars and parsing systems, it is subject to a number of (related) drawbacks: 1.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8459044098854065}]}, {"text": "Bracketed trees do not always provide NLP applications with enough information to carryout the required tasks: Many applications involve a deeper analysis of the input in the form of semantically motivated information such as deep dependency relations, predicate-argument structures, or simple logical forms.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we compare four history-based parsers for integration into the pipeline parsing architecture described in Section 2.3: parser which retains Penn-II functional tags Input for Collins's and Bikel's parsers was pre-tagged using the MXPOST POS tagger).", "labels": [], "entities": [{"text": "pipeline parsing", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.6987860798835754}]}, {"text": "Charniak's parser provides its own POS tagger.", "labels": [], "entities": []}, {"text": "The combined system of best history-based parser and automatic f-structure annotation is compared to two probabilistic parsing systems based on hand-crafted, wide-coverage, constraintbased, deep grammars: r the RASP parsing system r the XLE parsing system () Both hand-crafted grammars perform their own POS tagging, resolve LDDs, and associate strings with dependency relations (in the form of grammatical relations or LFG f-structures).", "labels": [], "entities": [{"text": "RASP parsing", "start_pos": 211, "end_pos": 223, "type": "TASK", "confidence": 0.6434828341007233}, {"text": "POS tagging", "start_pos": 304, "end_pos": 315, "type": "TASK", "confidence": 0.7799407839775085}]}, {"text": "We evaluate the parsers against a number of gold-standard dependency banks.", "labels": [], "entities": []}, {"text": "We use the DCU 105 Dependency Bank () as our development set for the treebank-based LFG parsers.", "labels": [], "entities": [{"text": "DCU 105 Dependency Bank", "start_pos": 11, "end_pos": 34, "type": "DATASET", "confidence": 0.955224871635437}]}, {"text": "We use the f-structure annotation algorithm to automatically generate a gold-standard test set from the original Section 22 treebank trees (the f-structure annotated WSJ Section 22 Dependency Bank) to choose the best treebank-based LFG parsing systems for the PARC 700 and CBS 500 experiments.", "labels": [], "entities": [{"text": "Section 22 treebank trees", "start_pos": 113, "end_pos": 138, "type": "DATASET", "confidence": 0.9123631864786148}, {"text": "WSJ Section 22 Dependency Bank", "start_pos": 166, "end_pos": 196, "type": "DATASET", "confidence": 0.9308006763458252}, {"text": "PARC 700 and CBS 500 experiments", "start_pos": 260, "end_pos": 292, "type": "DATASET", "confidence": 0.9279087583223978}]}, {"text": "Following the experimental setup in, we use the Penn-II Section 23-based PARC 700 Dependency Bank () to evaluate the treebank-induced LFG resources against the hand-crafted XLE grammar and parsing system of and, we use the SUSANNE Based CBS 500 Dependency Bank to evaluate the treebankinduced LFG resources against the hand-crafted RASP grammar and parsing system as well as against the XLE system (.", "labels": [], "entities": [{"text": "Penn-II Section 23-based PARC 700 Dependency Bank", "start_pos": 48, "end_pos": 97, "type": "DATASET", "confidence": 0.9301244786807469}, {"text": "SUSANNE Based CBS 500 Dependency Bank", "start_pos": 223, "end_pos": 260, "type": "DATASET", "confidence": 0.7753072877724966}, {"text": "RASP grammar and parsing", "start_pos": 332, "end_pos": 356, "type": "TASK", "confidence": 0.7135390788316727}]}, {"text": "For each gold standard, our experiment design is as follows: We parse automatically tagged input 14 sentences with the treebank-and machine-learning-based parsers trained on WSJ Sections 02-21 in the pipeline architecture, pass the resulting parse trees to our automatic f-structure annotation algorithm, collect the f-structure equations, pass them to a constraint-solver which generates an f-structure, resolve long-distance dependencies at f-structure following and convert the resulting LDDresolved f-structures into dependency representations using the formats and software of  (for the DCU 105, PARC 700, and WSJ Section 22 evaluations) and the formats and software of Carroll, Briscoe, and Sanfilippo (1998) (for the CBS 500 evaluation).", "labels": [], "entities": [{"text": "WSJ Sections 02-21", "start_pos": 174, "end_pos": 192, "type": "DATASET", "confidence": 0.9189630150794983}, {"text": "DCU 105, PARC 700", "start_pos": 592, "end_pos": 609, "type": "DATASET", "confidence": 0.8341474413871766}, {"text": "WSJ Section 22 evaluations", "start_pos": 615, "end_pos": 641, "type": "DATASET", "confidence": 0.9213595241308212}, {"text": "CBS 500 evaluation", "start_pos": 724, "end_pos": 742, "type": "DATASET", "confidence": 0.920678456624349}]}, {"text": "In the experiments we did not use any additional annotations such as -A (for argument) that can be generated by some of the history-based parsers) as the f-structure annotation algorithm is designed for Penn-II trees (which do not contain such annotations).", "labels": [], "entities": []}, {"text": "We also did not use the limited LDD resolution for whrelative clauses provided by Collins's Model 3 as better results are achieved by LDD resolution on f-structure ().", "labels": [], "entities": [{"text": "Collins's Model 3", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.9100120514631271}]}, {"text": "A complete set of parameter settings for the parsers is provided in the Appendix.", "labels": [], "entities": []}, {"text": "In order to evaluate the treebank-induced LFG resources against the PARC 700 and the CBS 500 dependency banks, a certain amount of automatic mapping is required to account for systematic differences in linguistic analysis, feature geometry, and nomenclature at the level of dependencies.", "labels": [], "entities": [{"text": "PARC 700", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.968063473701477}, {"text": "CBS 500 dependency banks", "start_pos": 85, "end_pos": 109, "type": "DATASET", "confidence": 0.9468591511249542}, {"text": "feature geometry", "start_pos": 223, "end_pos": 239, "type": "TASK", "confidence": 0.7067253291606903}]}, {"text": "This is discussed in Sections 5.1 and 5.2.", "labels": [], "entities": []}, {"text": "Throughout, we use the Approximate Randomization Test to test the statistical significance of the results.", "labels": [], "entities": [{"text": "Approximate Randomization Test", "start_pos": 23, "end_pos": 53, "type": "METRIC", "confidence": 0.9132717450459799}]}, {"text": "For reference, we include the traditional CFG-tree-based comparison for treebankinduced parsers.", "labels": [], "entities": []}, {"text": "The parsers are trained on Sections 02 to 21 of the Penn-II Treebank and tested on Section 23.", "labels": [], "entities": [{"text": "Penn-II Treebank", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.988051563501358}]}, {"text": "The published results 15 on these experiments for the history-based parsers are given in.", "labels": [], "entities": []}, {"text": "We also include figures fora PCFG and a Parent-PCFG (a PCFG which has undergone the parent transformation).", "labels": [], "entities": [{"text": "PCFG", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.8668725490570068}]}, {"text": "These PCFGs are induced following standard treebank preprocessing steps, including elimination of empty nodes, but following, they do include Penn-II functional tags, as these tags contain valuable information for the automatic f-structure annotation algorithm (Section 2.2).", "labels": [], "entities": [{"text": "Penn-II functional tags", "start_pos": 142, "end_pos": 165, "type": "DATASET", "confidence": 0.8789199789365133}]}, {"text": "These tags are removed for the tree-based evaluation.", "labels": [], "entities": []}, {"text": "The results show that the history-based parsers produce considerably better trees than the more basic PCFGs (with and without parent transformations).", "labels": [], "entities": []}, {"text": "Charniak's (2000) parser scores best with an f-score of 89.73% on all sentences in Section 23.", "labels": [], "entities": [{"text": "f-score", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9903005361557007}]}, {"text": "The vanilla PCFG achieves the lowest f-score of 73.03%, a difference of 16.7 percentage points.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.6346162557601929}, {"text": "f-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9884278178215027}]}, {"text": "The hand-crafted XLE and RASP grammars achieve around 80% coverage (measured in terms of complete spanning parse) on Section 23 and use a variety of (longest) fragments combining techniques to generate dependency representations for the remaining 20% of Section 23 strings.", "labels": [], "entities": [{"text": "RASP grammars", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.6043716669082642}, {"text": "coverage", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9798423647880554}]}, {"text": "By contrast, the treebank-induced PCFGs and history-based parsers all achieve coverage of over 99.9%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9936246871948242}]}, {"text": "Given that the history-based parsers score considerably better than PCFGs on trees, we would also expect them to produce dependency structures of substantially higher quality.", "labels": [], "entities": []}, {"text": "In an experimental setup similar to that of, we evaluate each parser against a large automatically generated gold standard.", "labels": [], "entities": []}, {"text": "The goldstandard dependency bank is automatically generated by annotating the original 1,700 treebank trees from WSJ Section 22 of the Penn-II Treebank with our f-structure annotation algorithm.", "labels": [], "entities": [{"text": "WSJ Section 22 of the Penn-II Treebank", "start_pos": 113, "end_pos": 151, "type": "DATASET", "confidence": 0.8849894489560809}]}, {"text": "We then evaluate the f-structures generated from the tree output of the six parsers trained on Sections 02 to 21 resulting from parsing the Section 22 strings against the automatically produced f-structures for the original Section 22 Penn-II treebank trees.", "labels": [], "entities": [{"text": "Section 22 Penn-II treebank trees", "start_pos": 224, "end_pos": 257, "type": "DATASET", "confidence": 0.8010350346565247}]}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "Compared to for the DCU 105 gold standard, most scores are up, particularly so for the history-based parsers.", "labels": [], "entities": [{"text": "DCU 105 gold standard", "start_pos": 20, "end_pos": 41, "type": "DATASET", "confidence": 0.9405982792377472}]}, {"text": "This trend is possibly due to the fact that the WSJ The annotation algorithm relies on Penn-II-style punctuation patterns where an   Section 22 gold standard is generated automatically from the original \"perfect\" Penn-II treebank trees using the automatic f-structure annotation algorithm, whereas the DCU 105 has been created manually without regard as to whether or not the f-structure annotation algorithm could ever generate the f-structures, even given the \"perfect\" trees.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.9006688594818115}, {"text": "Section 22 gold standard", "start_pos": 133, "end_pos": 157, "type": "DATASET", "confidence": 0.843939408659935}, {"text": "Penn-II treebank trees", "start_pos": 213, "end_pos": 235, "type": "DATASET", "confidence": 0.9541645248730978}, {"text": "DCU 105", "start_pos": 302, "end_pos": 309, "type": "DATASET", "confidence": 0.9554023146629333}]}, {"text": "The LFG system based on Bikel's retrained parser achieves the highest f-score of 83.06% preds-only and 87.63% all GFs.", "labels": [], "entities": [{"text": "f-score", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9820535182952881}, {"text": "GFs", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9418331384658813}]}, {"text": "Parent-PCFG achieves an f-score of 74.92% preds-only and 83.04% all GFs.", "labels": [], "entities": [{"text": "f-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9984825253486633}, {"text": "GFs", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9500283598899841}]}, {"text": "provides a breakdown by feature of the predsonly evaluation.", "labels": [], "entities": [{"text": "predsonly", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.8868700265884399}]}, {"text": "shows that, once again, the automatic f-structure annotation algorithm is notable to identify any cases of apposition from the output of Collins's Model 3 parser.", "labels": [], "entities": [{"text": "Collins's Model 3 parser", "start_pos": 137, "end_pos": 161, "type": "DATASET", "confidence": 0.9130476593971253}]}, {"text": "Apart from Bikel's retrained parser, none of the history-based parsers are able to identify  The PARC 700 Dependency Bank () provides dependency relations (including LDD relations) for 700 sentences randomly selected from WSJ Section 23 of the Penn-II Treebank.", "labels": [], "entities": [{"text": "PARC 700 Dependency Bank", "start_pos": 97, "end_pos": 121, "type": "DATASET", "confidence": 0.9443232864141464}, {"text": "WSJ Section 23 of the Penn-II Treebank", "start_pos": 222, "end_pos": 260, "type": "DATASET", "confidence": 0.8991025686264038}]}, {"text": "In order to evaluate the parsers, we follow the experimental setup of with a split of 560 dependency structures for the test set and 140 for the development set.", "labels": [], "entities": []}, {"text": "The set of features, later in this article) evaluated in the experiment form a proper superset of preds-only, but a proper subset of all grammatical functions (preds-only \u2282 PARC \u2282 all GFs).", "labels": [], "entities": [{"text": "PARC", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.8405205607414246}]}, {"text": "This feature set was selected in Kaplan et al. because the features carry important semantic information.", "labels": [], "entities": []}, {"text": "There are systematic differences between the PARC 700 dependencies and the f-structures generated in our approach as regards feature geometry, feature nomenclature, and the treatment of named entities.", "labels": [], "entities": [{"text": "PARC 700 dependencies", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.9312360286712646}, {"text": "feature geometry", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.7086056768894196}, {"text": "feature nomenclature", "start_pos": 143, "end_pos": 163, "type": "TASK", "confidence": 0.6791893690824509}]}, {"text": "In order to evaluate against the PARC 700 test set, we automatically map the f-structures produced by our parsers to a format similar to that of the PARC 700 Dependency Bank.", "labels": [], "entities": [{"text": "PARC 700 test set", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.9624616503715515}, {"text": "PARC 700 Dependency Bank", "start_pos": 149, "end_pos": 173, "type": "DATASET", "confidence": 0.9672936946153641}]}, {"text": "This is done with conversion software in a post-processing stage on the f-structure annotated trees (.", "labels": [], "entities": []}, {"text": "The conversion software is developed on the 140-sentence development set of the PARC 700, except for the Multi-Word Expressions section.", "labels": [], "entities": [{"text": "140-sentence development set of the PARC 700", "start_pos": 44, "end_pos": 88, "type": "DATASET", "confidence": 0.7525396176746914}]}, {"text": "Following the experimental setup of, we markup multi-word expression predicates based on the gold-standard PARC 700 Dependency Bank.", "labels": [], "entities": [{"text": "PARC 700 Dependency Bank", "start_pos": 107, "end_pos": 131, "type": "DATASET", "confidence": 0.9510102421045303}]}, {"text": "We also compare the hand-crafted, deep, probabilistic unification grammar-based RASP parsing system of to our treebank-and retrained Bikel parser-based LFG system.", "labels": [], "entities": [{"text": "RASP parsing", "start_pos": 80, "end_pos": 92, "type": "TASK", "confidence": 0.8003138601779938}]}, {"text": "The RASP parsing system is a domain-independent, robust statistical parsing system for English, based on a hand-written, feature-based unification grammar.", "labels": [], "entities": [{"text": "RASP parsing", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.7616628706455231}]}, {"text": "A probabilistic parse selection model conditioned on the structural parse context, degree of support fora subanalysis in the parse forest, and lexical information (when available) chooses the most likely parses.", "labels": [], "entities": []}, {"text": "For this experiment, we evaluate against the CBS 500, 25 developed by in order to evaluate a precursor of the RASP parsing resources.", "labels": [], "entities": [{"text": "CBS 500", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.8747382462024689}, {"text": "RASP parsing", "start_pos": 110, "end_pos": 122, "type": "TASK", "confidence": 0.8144433200359344}]}, {"text": "The CBS 500 contains dependency structures (including some long distance dependencies 26 ) for 500 sentences chosen at random from the SUSANNE corpus (Sampson 1995), but subject to the constraint that they are parsable by the parser in Carroll, Briscoe, and Sanfilippo.", "labels": [], "entities": [{"text": "SUSANNE corpus", "start_pos": 135, "end_pos": 149, "type": "DATASET", "confidence": 0.7718579769134521}]}, {"text": "As with the PARC 700, there are systematic differences between the f-structures produced by our methodology and the dependency structures of the CBS 500.", "labels": [], "entities": [{"text": "PARC 700", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.932786375284195}]}, {"text": "In order to be able to evaluate against the CBS 500, we automatically map our f-structures into a format similar to theirs.", "labels": [], "entities": [{"text": "CBS 500", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.989465057849884}]}, {"text": "We did not split the data into a heldout and a test set when developing the mapping, so that a comparison could be made with other systems that report evaluations against the CBS 500.", "labels": [], "entities": [{"text": "CBS 500", "start_pos": 175, "end_pos": 182, "type": "DATASET", "confidence": 0.9825930297374725}]}, {"text": "The following CBS 500-style grammatical relations are produced from the f-structure in: Some mapping is carried out (as in the evaluation against the PARC 700) on the fstructure annotated trees, and the remaining mapping is carried out on the f-structures ().", "labels": [], "entities": [{"text": "PARC 700", "start_pos": 150, "end_pos": 158, "type": "DATASET", "confidence": 0.9224759936332703}]}, {"text": "As with the PARC 700 mapping, all mappings are carried out automatically.", "labels": [], "entities": [{"text": "PARC 700 mapping", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.9376771847407023}]}, {"text": "The following phenomena were dealt with on the f-structure annotated trees: Auxiliary verbs (xcomp flattening) XCOMPS were flattened to promote the main verb to the top level, while maintaining a list of auxiliary and modal verbs and their relation to one another.", "labels": [], "entities": [{"text": "XCOMPS", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.7449572086334229}]}, {"text": "Treatment of topicalized sentences The predicate of the topicalized sentence became the main predicate and any other top level material became an adjunct.", "labels": [], "entities": []}, {"text": "Multi-word expressions Multi-word expressions (such as according to) were not marked up in the parser input, but captured in the annotated trees and the annotations adjusted accordingly.", "labels": [], "entities": []}, {"text": "Treatment of the verbs be and become Our automatic annotation algorithm does not treat the verbs be and become differently from any other verbs when they are used transitively.", "labels": [], "entities": []}, {"text": "This analysis conflicted with the CBS 500 analysis, so was changed to match theirs.", "labels": [], "entities": [{"text": "CBS 500 analysis", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9762338002522787}]}], "tableCaptions": [{"text": " Table 3  Most frequent wh-TOPICREL paths.", "labels": [], "entities": []}, {"text": " Table 4  Most frequent semantic forms for active and passive (p) occurrences of the verb want and  reward.", "labels": [], "entities": []}, {"text": " Table 5  Results of tree-based evaluation on all sentences WSJ section 23, Penn-II.", "labels": [], "entities": [{"text": "WSJ section 23", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.8615074753761292}, {"text": "Penn-II", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.8139781951904297}]}, {"text": " Table 6  Treebank-induced parsers: results of dependency-based evaluation against DCU 105.", "labels": [], "entities": [{"text": "DCU 105", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9512873291969299}]}, {"text": " Table 7. The system  based on the retrained parser is now much better able to identify oblique arguments and  overall preds-only accuracy has improved by 3.53% over the original Bikel experiment  and 3.31% over Charniak's parser, even though Charniak's parser performs more than  2% better on the tree-based scores in", "labels": [], "entities": [{"text": "preds-only", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.9243075847625732}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.8974195122718811}]}, {"text": " Table 8  Results of dependency-based evaluation against the automatically generated gold standard for  WSJ Section 22.", "labels": [], "entities": [{"text": "WSJ Section 22", "start_pos": 104, "end_pos": 118, "type": "DATASET", "confidence": 0.8355739514032999}]}, {"text": " Table 9  Breakdown by dependency of results of preds-only evaluation against the automatically  generated Section 22 gold standard.", "labels": [], "entities": [{"text": "Section 22 gold standard", "start_pos": 107, "end_pos": 131, "type": "DATASET", "confidence": 0.9140908271074295}]}, {"text": " Table 10  Comparing parsers evaluated against Section 22 dependencies (preds-only): p-values for  approximate randomization test for 10,000,000 randomizations.", "labels": [], "entities": []}, {"text": " Table 11  Results of evaluation against the PARC 700 Dependency Bank following the experimental setup  of Kaplan et al. (2004).", "labels": [], "entities": [{"text": "PARC 700 Dependency Bank", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.9708250164985657}]}, {"text": " Table 12  Breakdown by dependency relation of results of evaluation against PARC 700.", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.8382788896560669}, {"text": "PARC 700", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9228289425373077}]}, {"text": " Table 14  Breakdown by grammatical relation for results of evaluation against CBS 500.", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.8878637552261353}, {"text": "CBS 500", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.9701365232467651}]}, {"text": " Table 15  Evaluation and significance testing of sentences length \u226440 against the PARC 700.", "labels": [], "entities": [{"text": "significance", "start_pos": 26, "end_pos": 38, "type": "METRIC", "confidence": 0.9214962720870972}, {"text": "PARC 700", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.9489718675613403}]}, {"text": " Table 16  Evaluation and significance testing of sentences length \u226440 against the CBS 500.", "labels": [], "entities": [{"text": "significance", "start_pos": 26, "end_pos": 38, "type": "METRIC", "confidence": 0.9003745913505554}, {"text": "CBS 500", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9481625556945801}]}, {"text": " Table 17  Preds-only evaluation against the PARC 700 Dependency Bank.", "labels": [], "entities": [{"text": "Preds-only", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9726321697235107}, {"text": "PARC 700 Dependency Bank", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.9438465088605881}]}]}