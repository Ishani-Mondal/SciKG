{"title": [{"text": "Computing Word Similarity and Identifying Cognates with Pair Hidden Markov Models", "labels": [], "entities": [{"text": "Computing Word Similarity", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5793259640534719}]}], "abstractContent": [{"text": "We present a system for computing similarity between pairs of words.", "labels": [], "entities": []}, {"text": "Our system is based on Pair Hidden Markov Models , a variation on Hidden Markov Models that has been used successfully for the alignment of biological sequences.", "labels": [], "entities": [{"text": "alignment of biological sequences", "start_pos": 127, "end_pos": 160, "type": "TASK", "confidence": 0.8599095940589905}]}, {"text": "The parameters of the model are automatically learned from training data that consists of word pairs known to be similar.", "labels": [], "entities": []}, {"text": "Our tests focus on the identification of cog-nates-words of common origin in related languages.", "labels": [], "entities": [{"text": "identification of cog-nates-words of common origin in related languages", "start_pos": 23, "end_pos": 94, "type": "TASK", "confidence": 0.7442289061016507}]}, {"text": "The results show that our system outperforms previously proposed techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "The computation of surface similarity between pairs of words is an important task in many areas of natural language processing.", "labels": [], "entities": [{"text": "computation of surface similarity between pairs of words", "start_pos": 4, "end_pos": 60, "type": "TASK", "confidence": 0.79484923183918}, {"text": "natural language processing", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.6550810833772024}]}, {"text": "In historical linguistics phonetic similarity is one of the clues for identifying cognates, that is, words that share a common origin).", "labels": [], "entities": [{"text": "historical linguistics phonetic similarity", "start_pos": 3, "end_pos": 45, "type": "TASK", "confidence": 0.6549311131238937}]}, {"text": "In statistical machine translation, cognates are helpful in inducing translation lexicons), sentence alignment, and word alignment.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6436401704947153}, {"text": "sentence alignment", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7749781906604767}, {"text": "word alignment", "start_pos": 116, "end_pos": 130, "type": "TASK", "confidence": 0.8075538277626038}]}, {"text": "In dialectology, similarity is used for estimating distance between dialects.", "labels": [], "entities": [{"text": "similarity", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9689977765083313}]}, {"text": "Other applications include cross-lingual information retrieval (, detection of confusable drug names (, and lexicography (.", "labels": [], "entities": [{"text": "cross-lingual information retrieval", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.7441057960192362}, {"text": "detection of confusable drug names", "start_pos": 66, "end_pos": 100, "type": "TASK", "confidence": 0.8661634683609009}]}, {"text": "Depending on the context, strong word similarity may indicate either that words share a common origin (cognates), a common meaning (synonyms), or are related in someway (e.g. spelling variants).", "labels": [], "entities": []}, {"text": "In this paper, we focus on cognates.", "labels": [], "entities": []}, {"text": "Genetic cognates are well-suited for testing measures of word similarity because they arise by evolving from a single word in a proto-language.", "labels": [], "entities": []}, {"text": "Unlike rather indefinite concepts like synonymy or confusability, cognation is a binary notion, which inmost cases can be reliably determined.", "labels": [], "entities": []}, {"text": "Methods that are normally used for computing word similarity can be divided into orthographic and phonetic.", "labels": [], "entities": [{"text": "computing word similarity", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.7264039715131124}]}, {"text": "The former includes string edit distance (, longest common subsequence ratio, and measures based on shared character n-grams.", "labels": [], "entities": [{"text": "string edit distance", "start_pos": 20, "end_pos": 40, "type": "METRIC", "confidence": 0.5376443068186442}, {"text": "longest common subsequence ratio", "start_pos": 44, "end_pos": 76, "type": "METRIC", "confidence": 0.7430275976657867}]}, {"text": "These usually employ a binary identity function on the level of character comparison.", "labels": [], "entities": []}, {"text": "The phonetic approaches, such as Soundex ( and, attempt to take advantage of the phonetic characteristics of individual characters in order to estimate their similarity.", "labels": [], "entities": [{"text": "Soundex", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.8440065383911133}]}, {"text": "All of the above methods are static, in the sense of having a fixed definition that leaves little room for adaptation to a specific context.", "labels": [], "entities": []}, {"text": "In contrast, the methods proposed by automatically construct weighted string similarity measures on the basis of string segmentation and bitext co-occurrence statistics.", "labels": [], "entities": []}, {"text": "We have created a system for determining word similarity based on a Pair Hidden Markov Model.", "labels": [], "entities": []}, {"text": "The parameters of the model are automatically learned from training data that consists of word pairs that are known to be similar.", "labels": [], "entities": []}, {"text": "The model is trained using the Baum-Welch algorithm (.", "labels": [], "entities": []}, {"text": "We examine several variants of the model, which are characterized by different training techniques, number of parameters, and word length correction method.", "labels": [], "entities": [{"text": "word length correction", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.47744515538215637}]}, {"text": "The models are tested on a cognate recognition task across word lists representing several Indo-European languages.", "labels": [], "entities": [{"text": "cognate recognition task", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8141820232073466}]}, {"text": "The experiments indicate that our system substantially outperforms the most commonly used approaches.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a more detailed description of the problem of word similarity.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.7370540797710419}]}, {"text": "Section 3 contains an introduction to Pair Hidden Markov Models, while section 4 describes their adaptation to our domain.", "labels": [], "entities": []}, {"text": "Sections 5 and 6 report experimental set-up and results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our word similarity system on the task of the identification of cognates.", "labels": [], "entities": []}, {"text": "The input consists of pairs of words that have the same meaning in distinct languages.", "labels": [], "entities": []}, {"text": "For each pair, the system produces a score representing the likelihood that the words are cognate.", "labels": [], "entities": []}, {"text": "Ideally, the scores for true cognate pairs should always be higher than scores assigned to unrelated pairs.", "labels": [], "entities": []}, {"text": "For binary classification, a specific score threshold could be applied, but we defer the decision on the precision-recall trade-off to downstream applications.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7925014793872833}, {"text": "precision-recall", "start_pos": 105, "end_pos": 121, "type": "METRIC", "confidence": 0.9892431497573853}]}, {"text": "Instead, we order the candidate pairs by their scores, and evaluate the ranking using 11-point interpolated average precision).", "labels": [], "entities": [{"text": "11-point interpolated average precision", "start_pos": 86, "end_pos": 125, "type": "METRIC", "confidence": 0.6518748328089714}]}, {"text": "Word similarity is not always a perfect indicator of cognation because it can also result from lexical borrowing and random chance.", "labels": [], "entities": []}, {"text": "It is also possible that two words are cognates and yet exhibit little surface similarity.", "labels": [], "entities": []}, {"text": "Therefore, the upper bound for average precision is likely to be substantially lower than 100%.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9605239033699036}]}, {"text": "In this section, we first report on the effect of model variations on the overall performance, and then we compare the best results for each algorithm.", "labels": [], "entities": []}, {"text": "shows the average cognate recognition precision on the test set fora number of model variations combined with four basic algorithms, VIT, FOR, LOG, and FLO, which were introduced in Section 4.: Average cognate recognition precision for each model and algorithm combination.", "labels": [], "entities": [{"text": "VIT", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9563969373703003}, {"text": "FOR", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.986641526222229}, {"text": "LOG", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.9499971866607666}, {"text": "FLO", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.9985975623130798}]}, {"text": "The remaining rows contain the results for the model variations described in Section 4.2.", "labels": [], "entities": []}, {"text": "In all cases, the simplifications are in effect during testing only, after the full model had been trained.", "labels": [], "entities": []}, {"text": "We also performed experiments with the model simplified prior to training but their results were consistently lower than the results presented here.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average cognate recognition precision for various models and algorithms.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9030165076255798}]}]}