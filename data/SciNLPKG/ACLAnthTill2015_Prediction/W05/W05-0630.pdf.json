{"title": [], "abstractContent": [{"text": "We present a four-step hierarchical SRL strategy which generalizes the classical two-level approach (boundary detection and classification).", "labels": [], "entities": [{"text": "SRL", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9560728669166565}, {"text": "boundary detection and classification", "start_pos": 101, "end_pos": 138, "type": "TASK", "confidence": 0.7325200885534286}]}, {"text": "To achieve this, we have split the classification step by grouping together roles which share linguistic properties (e.g. Core Roles versus Adjuncts).", "labels": [], "entities": []}, {"text": "The results show that the non-optimized hierarchical approach is com-putationally more efficient than the traditional systems and it preserves their accuracy .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9968189001083374}]}], "introductionContent": [{"text": "For accomplishing the CoNLL 2005 Shared Task on Semantic Role Labeling (), we capitalized on our experience on the semantic shallow parsing by extending our system, widely experimented on PropBank and FrameNet () data, with a twostep boundary detection and a hierarchical argument classification strategy.", "labels": [], "entities": [{"text": "CoNLL 2005 Shared Task on Semantic Role Labeling", "start_pos": 22, "end_pos": 70, "type": "TASK", "confidence": 0.7265603467822075}, {"text": "semantic shallow parsing", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.6641241212685903}, {"text": "twostep boundary detection", "start_pos": 226, "end_pos": 252, "type": "TASK", "confidence": 0.7179965774218241}, {"text": "argument classification", "start_pos": 272, "end_pos": 295, "type": "TASK", "confidence": 0.7303818762302399}]}, {"text": "Currently, the system can work in both basic and enhanced configuration.", "labels": [], "entities": []}, {"text": "Given the parse tree of an input sentence, the basic system applies (1) a boundary classifier to select the nodes associated with correct arguments and (2) a multi-class labeler to assign the role type.", "labels": [], "entities": []}, {"text": "For such models, we used some of the linear (e.g. () and structural features developed in previous studies.", "labels": [], "entities": []}, {"text": "In the enhanced configuration, the boundary annotation is subdivided in two steps: a first pass in which we label argument boundary and a second pass in which we apply a simple heuristic to eliminate the argument overlaps.", "labels": [], "entities": []}, {"text": "We have also tried some strategies to learn such heuristics automatically.", "labels": [], "entities": []}, {"text": "In order to do this we used a tree kernel to classify the subtrees associated with correct predicate argument structures (see).", "labels": [], "entities": []}, {"text": "The rationale behind such an attempt was to exploit the correlation among potential arguments.", "labels": [], "entities": []}, {"text": "Also, the role labeler is divided into two steps: (1) we assign to the arguments one out of four possible class labels: Core Roles, Adjuncts, Continuation Arguments and Co-referring Arguments, and (2) in each of the above class we apply the set of its specific classifiers, e.g. A0,..,A5 within the Core Role class.", "labels": [], "entities": []}, {"text": "As such grouping is relatively new, the traditional features may not be sufficient to characterize each class.", "labels": [], "entities": []}, {"text": "Thus, to generate a large set of features automatically, we again applied tree kernels.", "labels": [], "entities": []}, {"text": "Since our SRL system exploits the PropBank formalism for internal data representation, we developed ad-hoc procedures to convert back and forth to the CoNLL Shared Task format.", "labels": [], "entities": []}, {"text": "This conversion step gave us useful information about the amount and the nature of the parsing errors.", "labels": [], "entities": [{"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9797114729881287}]}, {"text": "Also, we could measure the frequency of the mismatches between syntax and role annotation.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, Section 2 describes the basic system configuration whereas Section 3 illustrates its enhanced properties and the hierarchical structure.", "labels": [], "entities": []}, {"text": "Section 4 describes the experimental setting and the results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 summarizes our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented our approach with the CoNLL 2005 Shared Task standard dataset, i.e. the PennTree Bank, where sections from 02 to 21 are used as training set, Section 24 as development set (Dev) and Section 23 as the test set (WSJ).", "labels": [], "entities": [{"text": "CoNLL 2005 Shared Task standard dataset", "start_pos": 38, "end_pos": 77, "type": "DATASET", "confidence": 0.9404854774475098}, {"text": "PennTree Bank", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.9877974689006805}]}, {"text": "Additionally, the Brown corpus' sentences were also used as the test set (Brown).", "labels": [], "entities": [{"text": "Brown corpus' sentences", "start_pos": 18, "end_pos": 41, "type": "DATASET", "confidence": 0.9246538281440735}]}, {"text": "As input for our feature extractor we used only the Charniak's parses with their POSs.", "labels": [], "entities": [{"text": "feature extractor", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.6778510063886642}]}, {"text": "The evaluations were carried outwith the SVMlight-TK software) available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes the tree kernels in the SVM-light software.", "labels": [], "entities": []}, {"text": "We used the default polynomial kernel (degree=3) for the linear feature representations and the tree kernels for the structural feature processing.", "labels": [], "entities": []}, {"text": "As our feature extraction module was designed to work on the PropBank project annotation format (i.e. the prop.txt index file), we needed to generate it from the CoNLL data.", "labels": [], "entities": [{"text": "PropBank project annotation format", "start_pos": 61, "end_pos": 95, "type": "DATASET", "confidence": 0.9452482759952545}, {"text": "CoNLL data", "start_pos": 162, "end_pos": 172, "type": "DATASET", "confidence": 0.9693966209888458}]}, {"text": "Each PropBank annotation refers to a parse tree node which exactly covers the target argument but when using automatic parses such node may not exist.", "labels": [], "entities": []}, {"text": "For example, on the CoNLL Charniak's parses, (sections 02-21 and 24), we discovered that this problem affects 10,293 out of the 241,121 arguments (4.3%) and 9,741 sentences out of 87,257 (11.5%).", "labels": [], "entities": [{"text": "CoNLL Charniak's parses", "start_pos": 20, "end_pos": 43, "type": "DATASET", "confidence": 0.9374806880950928}]}, {"text": "We have found out that most of the errors are due to wrong parsing attachments.", "labels": [], "entities": []}, {"text": "This observation suggests that the capability of discriminating between correct and incorrect parse trees is a key issue in the boundary detection phase and it must be properly taken into account.", "labels": [], "entities": [{"text": "boundary detection", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.7610492408275604}]}, {"text": "For the boundary classifier we used a SVM with the polynomial kernel of degree 3.", "labels": [], "entities": []}, {"text": "We set the regularization parameter, c, to 1 and the cost factor, j to 7 (to have a slightly higher recall).", "labels": [], "entities": [{"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9995580315589905}]}, {"text": "To reduce the learning time, we applied a simple heuristic which removes the nodes covering the target predicate node.", "labels": [], "entities": []}, {"text": "From the initial 4,683,777 nodes (of sections 02-21), the heuristic removed 1,503,100 nodes with a loss of 2.6% of the total arguments.", "labels": [], "entities": []}, {"text": "However, as we started the experiments in late, we used only the 992,819 nodes from the sections 02-08.", "labels": [], "entities": []}, {"text": "The classifier took about two days and half to converge on a 64 bits machine (2.4 GHz and 4Gb Ram).", "labels": [], "entities": []}, {"text": "The multiclassifier was built with 52 binary argument classifiers.", "labels": [], "entities": []}, {"text": "Their training on all arguments from sec 02-21, (i.e. 242,957), required about a half day on a machine with 8 processors (32 bits, 1.7 GHz and overll 4Gb Ram).", "labels": [], "entities": []}, {"text": "We run the role multiclassifier on the output of the boundary classifier.", "labels": [], "entities": []}, {"text": "The results on the Dev, WSJ and Brown test data are shown in.", "labels": [], "entities": [{"text": "Dev", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.8865647912025452}, {"text": "WSJ and Brown test data", "start_pos": 24, "end_pos": 47, "type": "DATASET", "confidence": 0.7255704343318939}]}, {"text": "Note that, the overlapping nodes cause the generation of overlapping constituents in the sentence annotation.", "labels": [], "entities": []}, {"text": "This prevents us to use the CoNLL evaluator.", "labels": [], "entities": [{"text": "CoNLL evaluator", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9245570302009583}]}, {"text": "Thus, we used the overlap resolution algorithm also for the basic system.", "labels": [], "entities": [{"text": "overlap resolution", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.5293137729167938}]}, {"text": "As the first two phases of the hierarchical labeler are identical to the basic system, we focused on the last two phases.", "labels": [], "entities": []}, {"text": "We carried out our studies over the Gold Standard boundaries in the presence of arguments that do not have a perfect-covering node in the Charniak trees.", "labels": [], "entities": [{"text": "Gold Standard boundaries", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.9653565883636475}]}, {"text": "To accomplish the third phase, we re-organized the flat arguments into the AX, AM, CX and RX classes and we built a single multi-classifier.", "labels": [], "entities": []}, {"text": "For the fourth phase, we built a multi-classifier for each of the above classes: only the examples related to the target class were used, e.g. the AX mutliclassifier was designed with the A0,..,A5 ONE-vs-ALL binary classifiers.", "labels": [], "entities": []}, {"text": "In rows 2 and 3, shows the numbers of training and development set instances.", "labels": [], "entities": []}, {"text": "Row 4 contains the F 1 of the binary classifiers of the third phase whereas Row 5 reports the F 1 of the resulting multi-classifier.", "labels": [], "entities": [{"text": "F 1", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9825289249420166}, {"text": "F 1", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9829629063606262}]}, {"text": "Row 6 presents the F 1 s of the multi-classifiers of the fourth phase.", "labels": [], "entities": [{"text": "F 1 s", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9643995563189188}]}, {"text": "Row 7 illustrates the F 1 measure of the fourth phase classifier applied to the third phase output.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9743057290712992}]}, {"text": "Fi- nally, in Row 8, we report the F 1 of the basic system on the gold boundary nodes.", "labels": [], "entities": [{"text": "F 1", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9909766614437103}]}, {"text": "We note that the basic system shows a slightly higher F 1 but is less computational efficient than the hierarchical approach.", "labels": [], "entities": [{"text": "F 1", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9953675866127014}]}], "tableCaptions": [{"text": " Table 1: Overall results (top) and detailed results on  the WSJ test (bottom).", "labels": [], "entities": [{"text": "WSJ test", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.8127253949642181}]}, {"text": " Table 2: Hierarchical Semantic Role Labeler Results", "labels": [], "entities": [{"text": "Hierarchical Semantic Role Labeler", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.6867107450962067}]}]}