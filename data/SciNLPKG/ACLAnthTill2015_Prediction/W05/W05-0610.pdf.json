{"title": [{"text": "Using Uneven Margins SVM and Perceptron for Information Extraction", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7469440698623657}]}], "abstractContent": [{"text": "The classification problem derived from information extraction (IE) has an imbal-anced training set.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.8396054625511169}]}, {"text": "This is particularly true when learning from smaller datasets which often have a few positive training examples and many negative ones.", "labels": [], "entities": []}, {"text": "This paper takes two popular IE algorithms-SVM and Perceptron-and demonstrates how the introduction of an uneven margins parameter can improve the results on im-balanced training data in IE.", "labels": [], "entities": [{"text": "IE", "start_pos": 187, "end_pos": 189, "type": "TASK", "confidence": 0.9359198212623596}]}, {"text": "Our experiments demonstrate that the uneven margin was indeed helpful, especially when learning from few examples.", "labels": [], "entities": []}, {"text": "Essentially, the smaller the training set is, the more beneficial the uneven margin can be.", "labels": [], "entities": [{"text": "uneven margin", "start_pos": 70, "end_pos": 83, "type": "METRIC", "confidence": 0.8493611514568329}]}, {"text": "We also compare our systems to other state-of-the-art algorithms on several benchmarking corpora for IE.", "labels": [], "entities": [{"text": "IE", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.9634326696395874}]}], "introductionContent": [{"text": "Information Extraction (IE) is the process of automatic extraction of information about pre-specified types of events, entities or relations from text such as newswire articles or Web pages.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8598442673683167}, {"text": "automatic extraction of information about pre-specified types of events, entities or relations from text such as newswire articles or Web pages", "start_pos": 46, "end_pos": 189, "type": "TASK", "confidence": 0.7145080674778331}]}, {"text": "IE is useful in many applications, such as information gathering in a variety of domains, automatic annotations of web pages for Semantic Web, and knowledge management.", "labels": [], "entities": [{"text": "IE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9067654013633728}, {"text": "information gathering", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.8030443787574768}, {"text": "knowledge management", "start_pos": 147, "end_pos": 167, "type": "TASK", "confidence": 0.8246925473213196}]}, {"text": "A wide range of machine learning techniques have been used for IE and achieved state-of-the-art results, comparable to manually engineered IE systems.", "labels": [], "entities": [{"text": "IE", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9949967861175537}]}, {"text": "A learning algorithm usually learns a model from a set of documents which have been manually annotated by the user.", "labels": [], "entities": []}, {"text": "Then the model can be used to extract information from new documents.", "labels": [], "entities": []}, {"text": "Manual annotation is a time-consuming process.", "labels": [], "entities": [{"text": "Manual annotation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7591604292392731}]}, {"text": "Hence, in many cases learning from small data sets is highly desirable.", "labels": [], "entities": []}, {"text": "Therefore in this paper we also evaluate the performance of our algorithms on small amounts of training data and show their learning curve.", "labels": [], "entities": []}, {"text": "The learning algorithms for IE can be classified broadly into two main categories: rule learning and statistical learning.", "labels": [], "entities": [{"text": "IE", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9907941222190857}]}, {"text": "The former induces a set of rules from training examples.", "labels": [], "entities": []}, {"text": "There are many rule based learning systems, e.g. SRV, RAPIER, WHISK, BWI), and (LP ) 2.", "labels": [], "entities": [{"text": "SRV", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.833226203918457}, {"text": "RAPIER", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.8493609428405762}, {"text": "BWI", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9400517344474792}]}, {"text": "Statistical systems learn a statistical model or classifiers, such as HMMs (), Maximal Entropy (), the SVM (, and Perceptron (.", "labels": [], "entities": []}, {"text": "IE systems also differ from each other in the NLP features that they use.", "labels": [], "entities": [{"text": "IE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9074031114578247}]}, {"text": "These include simple features such as token form and capitalisation information, linguistic features such as part-ofspeech, semantic information from gazetteer lists, and genre-specific information such as document structure.", "labels": [], "entities": []}, {"text": "In general, the more features the system uses, the better performance it can achieve.", "labels": [], "entities": []}, {"text": "This paper concentrates on classifier-based learning for IE, which typically converts the recognition of each information entity into a set of classification problems.", "labels": [], "entities": [{"text": "IE", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9823300838470459}]}, {"text": "In the framework discussed here, two binary classifiers are trained for each type of information entity.", "labels": [], "entities": []}, {"text": "One classifier is used for recognising the entity's start token and the other -the entity's end token.", "labels": [], "entities": []}, {"text": "The classification problem derived from IE usually has imbalanced training data, in which positive training examples are vastly outnumbered by negative ones.", "labels": [], "entities": [{"text": "IE", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9724497199058533}]}, {"text": "This is particularly true for smaller data sets where often there are hundreds of negative training examples and only few positive ones.", "labels": [], "entities": []}, {"text": "Two approaches have been studied so far to deal with imbalanced data in IE.", "labels": [], "entities": [{"text": "IE", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9837884902954102}]}, {"text": "One approach is to under-sample majority class or over-sample minority class in order to obtain a relatively balanced training data.", "labels": [], "entities": []}, {"text": "However, under-sampling can potentially remove certain important examples, and over-sampling can lead to over-fitting and a larger training set.", "labels": [], "entities": []}, {"text": "Another approach is to divide the problem into several sub-problems in two layers, each of which has less imbalanced training set than the original one.", "labels": [], "entities": []}, {"text": "The output of the classifier in the first layer is used as the input to the classifiers in the second layer.", "labels": [], "entities": []}, {"text": "As a result, this approach needs more classifiers than the original problem.", "labels": [], "entities": []}, {"text": "Moreover, the classification errors in the first layer will affect the performance of the second one.", "labels": [], "entities": []}, {"text": "In this paper we explore another approach to handle the imbalanced data in IE, namely, adapting the learning algorithms for balanced classification to imbalanced data.", "labels": [], "entities": [{"text": "IE", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9686464667320251}]}, {"text": "We particularly study two popular classification algorithms in IE, Support Vector Machines (SVM) and Perceptron.", "labels": [], "entities": [{"text": "IE", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9497037529945374}]}, {"text": "SVM is a general supervised machine learning algorithm, that has achieved state of the art performance on many classification tasks, including NE recognition.", "labels": [], "entities": [{"text": "NE recognition", "start_pos": 143, "end_pos": 157, "type": "TASK", "confidence": 0.9529948234558105}]}, {"text": "compared three commonly used methods for named entity recognition -the SVM with quadratic kernel, maximal entropy method, and a rule based learning system, and showed that the SVM-based system performed better than the other two.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.6171316107114156}]}, {"text": "used a lattice-based approach to named entity recognition and employed the SVM with cubic kernel to compute transition probabilities in a lattice.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.6688602467377981}]}, {"text": "Their results on CoNLL2003 shared task were comparable to other systems but were not the best ones.", "labels": [], "entities": [{"text": "CoNLL2003 shared task", "start_pos": 17, "end_pos": 38, "type": "DATASET", "confidence": 0.7196608384450277}]}, {"text": "Previous research on using SVMs for IE adopts the standard form of the SVM, which treats positive and negative examples equally.", "labels": [], "entities": [{"text": "IE", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9540782570838928}]}, {"text": "As a result, they did not consider the difference between the balanced classification problems, where the SVM performs quite well, and the imbalanced ones.", "labels": [], "entities": []}, {"text": "proposes an uneven margins version of the SVM and shows that the SVM with uneven margins performs significantly better than the standard SVM on document classification problems with imbalanced training data.", "labels": [], "entities": [{"text": "document classification", "start_pos": 144, "end_pos": 167, "type": "TASK", "confidence": 0.698038786649704}]}, {"text": "Since the classification problem for IE is also imbalanced, this paper investigates the SVM with uneven margins for IE tasks and demonstrates empirically that the uneven margins SVM does have better performance than the standard SVM.", "labels": [], "entities": [{"text": "IE", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.954248309135437}]}, {"text": "Perceptron is a simple, fast and effective learning algorithm, which has successfully been applied to named entity recognition (.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.6681077977021536}]}, {"text": "The system uses a two-layer structure of classifiers to handle the imbalanced data.", "labels": [], "entities": []}, {"text": "The first layer classifies each word as entity or non-entity.", "labels": [], "entities": []}, {"text": "The second layer classifies the named entities identified by the first layer in the respective entity classes.", "labels": [], "entities": []}, {"text": "proposed another variant of Perceptron, the Perceptron algorithm with uneven margins (PAUM), designed especially for imbalanced data.", "labels": [], "entities": [{"text": "Perceptron algorithm with uneven margins (PAUM)", "start_pos": 44, "end_pos": 91, "type": "METRIC", "confidence": 0.6717680171132088}]}, {"text": "In this paper we explore the application of PAUM to IE.", "labels": [], "entities": [{"text": "IE", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9429806470870972}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the uneven margins SVM and Perceptron algorithms.", "labels": [], "entities": []}, {"text": "Sections 3.1 and 3.2 discuss the classifier-based framework for IE and the experimental datasets we used, respectively.", "labels": [], "entities": [{"text": "IE", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.982975423336029}]}, {"text": "We compare our systems to other state-of-the-art systems on three benchmark datasets in Section 3.3.", "labels": [], "entities": []}, {"text": "Section 3.4 discusses the effects of the uneven margins parameter on the SVM and Perceptron's performances.", "labels": [], "entities": []}, {"text": "Finally, Section 4 provides some conclusions.", "labels": [], "entities": []}, {"text": "introduced an uneven margins parameter into the SVM to deal with imbalanced classification problems.", "labels": [], "entities": [{"text": "SVM", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.7372700572013855}]}, {"text": "They showed that the SVM with uneven margins outperformed the standard SVM on document classification problem with imbalanced training data.", "labels": [], "entities": [{"text": "document classification", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.6963956356048584}]}, {"text": "Formally, given a training set Z = ((x 1 , y 1 ), . .", "labels": [], "entities": []}, {"text": ", (x m , y m )),where xi is the ndimensional input vector and y i (= +1 or \u22121) its label, the SVM with uneven margins is obtained by solving the quadratic optimisation problem:", "labels": [], "entities": []}], "datasetContent": [{"text": "The paper reports evaluation results on three corpora covering different IE tasks -named entity recognition (CoNLL-2003) and template filling or scenario templates in different domains (Jobs and CFP).", "labels": [], "entities": [{"text": "IE tasks -named entity recognition", "start_pos": 73, "end_pos": 107, "type": "TASK", "confidence": 0.6727799574534098}, {"text": "template filling or scenario templates", "start_pos": 125, "end_pos": 163, "type": "TASK", "confidence": 0.8330438017845154}]}, {"text": "The CoNLL-2003 3 provides the most recent evaluation results of many learning algorithms on named entity recognition.", "labels": [], "entities": [{"text": "CoNLL-2003 3", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9027171432971954}, {"text": "named entity recognition", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.6410250763098398}]}, {"text": "The Jobs corpus 4 has also been used recently by several learning systems.", "labels": [], "entities": [{"text": "Jobs corpus 4", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7740583419799805}]}, {"text": "The CFP corpus was created as part of the recent Pascal Challenge for evaluation of machine learning methods for IE 5 . In detail, we used the English part of the CoNLL-2003 shared task dataset, which consists of 946 documents for training, 216 document for development (e.g., tuning the parameters in learning algorithm), and 231 documents for evaluation (i.e., testing), all of which are news articles taken from the Reuters English corpus (RCV1).", "labels": [], "entities": [{"text": "CFP corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9634186327457428}, {"text": "CoNLL-2003 shared task dataset", "start_pos": 163, "end_pos": 193, "type": "DATASET", "confidence": 0.8386471122503281}, {"text": "Reuters English corpus (RCV1)", "start_pos": 419, "end_pos": 448, "type": "DATASET", "confidence": 0.9642380972703298}]}, {"text": "The corpus contains four types of named entities -person, location, organisation and miscellaneous names.", "labels": [], "entities": []}, {"text": "In the other two corpora domain-specific information was extracted into a number of slots.", "labels": [], "entities": []}, {"text": "The Job corpus includes 300 computer related job advertisements and 17 slots encoding job details, such as title, salary, recruiter, computer language, application, and platform.", "labels": [], "entities": [{"text": "Job corpus includes 300 computer related job advertisements and 17 slots encoding job details, such as title, salary, recruiter, computer language, application, and platform", "start_pos": 4, "end_pos": 177, "type": "Description", "confidence": 0.7373906830946605}]}, {"text": "The CFP corpus consists of 1100 conference or workshop call for papers (CFP), of which 600 were annotated.", "labels": [], "entities": [{"text": "CFP corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9539654552936554}]}, {"text": "The corpus includes 11 slots such as workshop and conference names and acronyms, workshop date, location and homepage.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison to other systems on CoNLL-2003 corpus: F -measure(%) on each entity type and the  overall micro-averaged F-measure. The 90% confidence intervals for results of other three systems are also  presented. The best performance figures for each entity type and overall appear in bold.", "labels": [], "entities": [{"text": "CoNLL-2003 corpus", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9788461625576019}, {"text": "F -measure", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9940045277277628}, {"text": "F-measure", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.8745832443237305}]}, {"text": " Table 2: Comparison to other systems on the jobs corpus: F 1 (%) on each entity type and overall perfor- mance as macro-averaged F 1 . Standard deviations for the MA F 1 of our systems are presented in parenthe- sis. The highest score on each slot and overall performance appears in bold.", "labels": [], "entities": [{"text": "F 1", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9921902418136597}, {"text": "MA F 1", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.8601659536361694}]}, {"text": " Table 3: Results of our SVM and PAUM systems  on CFP corpus: F-measures(%) on individual entity  type and the overall figures, together with the system  with the highest overall score. The highest score on  each slot appears in bold.", "labels": [], "entities": [{"text": "CFP corpus", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.9796375632286072}, {"text": "F-measures", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.998467743396759}]}, {"text": " Table 4: The effects of uneven margins parameter  of the SVM and Perceptron, respectively: macro av- eraged F 1 (%) on the two datasets CoNLL-2003 (de- velopment set) and Jobs. The standard deviations for  the Jobs dataset show the statistical significances of  the results. In bold are the best performance figures  for each dataset and each system.", "labels": [], "entities": [{"text": "macro av- eraged F 1", "start_pos": 92, "end_pos": 112, "type": "METRIC", "confidence": 0.8562154173851013}, {"text": "CoNLL-2003", "start_pos": 137, "end_pos": 147, "type": "DATASET", "confidence": 0.9236316084861755}]}]}