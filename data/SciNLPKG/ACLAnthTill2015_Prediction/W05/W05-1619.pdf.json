{"title": [{"text": "The Types and Distributions of Errors in a Wide Coverage Surface Realizer Evaluation", "labels": [], "entities": [{"text": "Wide Coverage Surface Realizer", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.6997634619474411}]}], "abstractContent": [{"text": "Recent empirical experiments on surface realizers have shown that grammars for generation can be effectively evaluated using large corpora.", "labels": [], "entities": []}, {"text": "Evaluation metrics are usually reported as single averages across all possible types of errors and syntactic forms.", "labels": [], "entities": []}, {"text": "But the causes of these errors are diverse, and the extent to which the accuracy of generation over individual syntactic phenomena is unknown.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9989544153213501}]}, {"text": "This article explores the types of errors, both computational and linguistic, inherent in the evaluation of a surface realizer when using large corpora.", "labels": [], "entities": []}, {"text": "We analyze data from an earlier wide coverage experiment on the FUF/SURGE surface realizer with the Penn TreeBank in order to empirically classify the sources of errors and describe their frequency and distribution.", "labels": [], "entities": [{"text": "FUF", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.9161374568939209}, {"text": "SURGE surface realizer", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.8117179075876871}, {"text": "Penn TreeBank", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.9942179024219513}]}, {"text": "This both provides a baseline for future evaluations and allows designers of NLG applications needing off-the-shelf surface realizers to choose on a quantitative basis.", "labels": [], "entities": []}], "introductionContent": [{"text": "Surface realization is the process of converting the semantic and syntactic representation of a sentence or series of sentences into a surface form fora particular language.", "labels": [], "entities": [{"text": "Surface realization is the process of converting the semantic and syntactic representation of a sentence or series of sentences into a surface form fora particular language", "start_pos": 0, "end_pos": 172, "type": "Description", "confidence": 0.8151840086166675}]}, {"text": "Most reusable deep surface realizers ] have been symbolic, hand-written grammar-based systems, often based on syntactic linguistic theories such as Halliday's systemic functional theory (FUF/SURGE and KPML) or Mel'cuk's Meaning-Text Theory (REALPRO).", "labels": [], "entities": []}, {"text": "However, corpus-based components, and in particular statistical surface realizers have focused attention on a number of problems facing symbolic NLG systems that until now have been generally considered future work: large-scale, data-robust and language-and domain-independent generation.", "labels": [], "entities": [{"text": "statistical surface realizers", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.6284012198448181}]}, {"text": "In each case, empirical evaluation plays a fundamental role in determining performance at the task of surface realization and setting baselines for future performance evaluation.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7638312876224518}]}, {"text": "For instance, the HALOGEN statistical realizer] underwent the most comprehensive evaluation of any surface realizer, which was conducted by measuring sentences extracted from the Penn TreeBank, converting them into its input formalism, and then producing output strings.", "labels": [], "entities": [{"text": "HALOGEN statistical realizer", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.6264035701751709}, {"text": "Penn TreeBank", "start_pos": 179, "end_pos": 192, "type": "DATASET", "confidence": 0.9938220977783203}]}, {"text": "Using automatic metrics from machine translation then quickly produces figures for global characteristics of traits such as accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9977254271507263}]}, {"text": "Ina recent experiment, we compared the performance of HALOGEN relative to the grammar-based FUF/SURGE surface realizer on the identical corpus and with a similar methodology.", "labels": [], "entities": [{"text": "FUF/SURGE surface realizer", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.5273137211799621}]}, {"text": "Although FUF/SURGE scored higher than the HALOGEN realizer, we were interested in absolute as well as relative performance: e.g., what particular grammatical rules are not well-covered by SURGE's grammar?", "labels": [], "entities": [{"text": "FUF", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.8489387035369873}]}, {"text": "While the above methodology gives an average figure for what coverage and accuracy are on a corpus like the Penn TreeBank, it describes a very wide array of errors as simple numerical averages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9978896975517273}, {"text": "Penn TreeBank", "start_pos": 108, "end_pos": 121, "type": "DATASET", "confidence": 0.9945851266384125}]}, {"text": "Thus it is impossible to know without working experience which types of realizer errors HALOGEN is likely to make.", "labels": [], "entities": []}, {"text": "From the perspective of syntactic analysis, statistical realizers behave as black boxes and thus there is little or no attempt made to look at incorrect sentences to trace back exactly what caused a particular error.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7070349156856537}]}, {"text": "Instead, either the language model is adjusted or anew corpus is obtained.", "labels": [], "entities": []}, {"text": "However, the grammars in symbolic realizers can also be evaluated as glass boxes, allowing the improvement of an incorrect or missing grammatical rule to also improve the generation of all other instances of the same grammatical rule in the corpus.", "labels": [], "entities": []}, {"text": "Those who are looking to use a surface realizer typically fall into three cases: (1) those employing one for the first time, often looking to use a well-known system, (2) those seeking to use a different surface realizer because their current realizer has undesirable operational parameters such as slowness or lack of multilingual support, and (3) those seeking to change realizers due to lack of grammatical coverage for their domain.", "labels": [], "entities": [{"text": "surface realizer", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7890298664569855}]}, {"text": "Those falling in this latter category are not interested in a general quantitative measure for coverage, but rather in finding out, without expending too much effort, whether a given surface realizer will generate the types of sentences they need.", "labels": [], "entities": []}, {"text": "We were thus interested in a range of questions about the evaluation itself as well as the results of the evaluation, which would allow a number of questions to be answered: \u00af What types of problems might be encountered during the experiment itself, and what are their sources?", "labels": [], "entities": []}, {"text": "\u00af What kinds of constructions are difficult to handle in general, such as adverb positioning, verb argument ordering, or very long noun phrases?", "labels": [], "entities": [{"text": "adverb positioning", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.8027920126914978}, {"text": "verb argument ordering", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.6623713572820028}]}, {"text": "\u00af What coverage does FUF/SURGE have for specific types of infrequent syntactic phenomena, like indirect questions or topicalizations?", "labels": [], "entities": [{"text": "FUF/SURGE", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.6403350830078125}]}, {"text": "\u00af What is a reasonable distribution of these errors?", "labels": [], "entities": []}, {"text": "\u00af What data is necessary to allow one to predict if a particular surface realizer will match the expectations of anew application?", "labels": [], "entities": []}, {"text": "We look at these questions from two perspectives: (1) establishing baselines to inform future, more detailed evaluations, and (2) providing information about the current state of the FUF/SURGE grammar and morphology for those who may wish to use it as a surface realizer in anew project and need to know whether it will support the types of syntactic constructions needed in their domain.", "labels": [], "entities": [{"text": "FUF/SURGE grammar", "start_pos": 183, "end_pos": 200, "type": "DATASET", "confidence": 0.8469216972589493}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparing two SURGE versions with HALOGEN on 2416 sentences from Section 23 of the Penn TreeBank.", "labels": [], "entities": [{"text": "HALOGEN", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9947750568389893}, {"text": "Penn TreeBank", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.8941666185855865}]}, {"text": " Table 2: Distribution of 629 high-level errors in the 4,240 tested sentences from Sections 20-22.", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.8562571406364441}]}, {"text": " Table 3: Distribution of 193 major syntactic errors in the 4,240 tested sentences from Sections 20-22.", "labels": [], "entities": []}]}