{"title": [{"text": "Weakly Supervised Learning Methods for Improving the Quality of Gene Name Normalization Data", "labels": [], "entities": [{"text": "Improving", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.9611048698425293}, {"text": "Gene Name Normalization", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.5458464324474335}]}], "abstractContent": [{"text": "A pervasive problem facing many bio-medical text mining applications is that of correctly associating mentions of entities in the literature with corresponding concepts in a database or ontology.", "labels": [], "entities": [{"text": "bio-medical text mining", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.6365415155887604}]}, {"text": "Attempts to build systems for automating this process have shown promise as demonstrated by the recent BioCreAtIvE Task 1B evaluation.", "labels": [], "entities": [{"text": "BioCreAtIvE Task 1B evaluation", "start_pos": 103, "end_pos": 133, "type": "DATASET", "confidence": 0.5943791568279266}]}, {"text": "A significant obstacle to improved performance for this task, however , is alack of high quality training data.", "labels": [], "entities": []}, {"text": "In this work, we explore methods for improving the quality of (noisy) Task 1B training data using variants of weakly supervised learning methods.", "labels": [], "entities": []}, {"text": "We present positive results demonstrating that these methods result in an improvement in training data quality as measured by improved system performance over the same system using the originally labeled data.", "labels": [], "entities": []}], "introductionContent": [{"text": "A primary set of tasks facing biomedical text processing systems is that of categorizing, identifying and classifying entities within the literature.", "labels": [], "entities": []}, {"text": "A key step in this process involves grouping mentions of entities together into equivalence classes that denote some underlying entity.", "labels": [], "entities": []}, {"text": "In the biomedical domain, however, we are fortunate to have structured data resources such as databases and ontologies with entries denoting these equivalence classes.", "labels": [], "entities": []}, {"text": "In biomedical text mining, then, this process involves associating mentions of entities with known, existing unique identifiers for those entities in databases or ontologies -a process referred to as normalization.", "labels": [], "entities": [{"text": "biomedical text mining", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.6154886980851492}]}, {"text": "This ability is required for text processing systems to associate descriptions of concepts in free text with a grounded, organized system of knowledge more readily amenable to machine processing.", "labels": [], "entities": []}, {"text": "The recent BioCreAtIvE Task 1B evaluation challenged a number of systems to identify genes associated with abstracts for three different organisms: mouse, fly and yeast.", "labels": [], "entities": [{"text": "BioCreAtIvE Task 1B", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.5000353157520294}]}, {"text": "The participants were provided with a large set of noisy training data and a smaller set of higher quality development test data.", "labels": [], "entities": []}, {"text": "They were also provided with a lexicon containing all the potential gene identifiers that might occur and a list of known, though incomplete, names and synonyms that refer to each of them.", "labels": [], "entities": []}, {"text": "To prepare the training data, the list of unique gene identifiers associated with each full text article was obtained from the appropriate model organism database.", "labels": [], "entities": []}, {"text": "However, the list had to be pruned to correspond to the genes mentioned in the abstract.", "labels": [], "entities": []}, {"text": "This was done by searching the abstract for each gene on the list or its synonyms, using exact string matching.", "labels": [], "entities": []}, {"text": "This process has the potential to miss genes that were referred to in the abstract using a phrase that does not appear in the synonym list.", "labels": [], "entities": []}, {"text": "Additionally, the list maybe incomplete, because not all genes mentioned in the article were curated, so there are mentions of genes in an abstract that did not have a corresponding identifier on the gene list.", "labels": [], "entities": []}, {"text": "This paper explores a series of methods for attempting to recover some of these missing gene identifiers from the Task 1B training data abstracts.", "labels": [], "entities": [{"text": "Task 1B training data abstracts", "start_pos": 114, "end_pos": 145, "type": "DATASET", "confidence": 0.683266419172287}]}, {"text": "We start with a robust, machine learning-based baseline system: a reimplementation of the system in.", "labels": [], "entities": []}, {"text": "Briefly, this system utilizes a classifier to selector filter matches made against the synonym list with a loose matching criterion.", "labels": [], "entities": []}, {"text": "From this baseline, we explore various methods for relabeling the noisy training data, resulting in improved scores on the overall Task 1B development test and evaluation data.", "labels": [], "entities": []}, {"text": "Our methods are based on weakly supervised learning techniques such as cotraining and self-training for learning with both labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "The setting here is different than the typical setting for weakly supervised learning, however, in that we have a large amount of noisily labeled data, as opposed to completely unlabeled data.", "labels": [], "entities": []}, {"text": "The main contribution of this work is a framework for applying weakly supervised methods to this problem of re-labeling noisy training data.", "labels": [], "entities": []}, {"text": "Our approach is based on partitioning the training data into two sets and viewing the problem as two mutually supporting weakly supervised learning problems.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that these methods, carefully tuned, improve performance for the gene name normalization task over those previously reported using machine learning-based techniques.", "labels": [], "entities": [{"text": "gene name normalization task", "start_pos": 98, "end_pos": 126, "type": "TASK", "confidence": 0.798359140753746}]}], "datasetContent": [{"text": "The main goal of our experiments was to demonstrate the benefits of re-labeling potentially noisy training instances in the task 1B training data.", "labels": [], "entities": []}, {"text": "In this work we focus the weakly supervised relabeling experiments on the mouse data set.", "labels": [], "entities": [{"text": "mouse data set", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.7760966122150421}]}, {"text": "In the mouse data there is a strong bias towards false negatives in the training data -i.e. many training instances have a negative label and should have a positive one.", "labels": [], "entities": []}, {"text": "Our reasons for focusing on this data are twofold: 1) we believe this situation is likely to be more common in practice since an organism may have impoverished synonym lists or \"gaps\" in the curated databases and 2) the experiments and resulting analyses are made clearer by focusing on re-labeling instances in one direction only (i.e. from negative to positive).", "labels": [], "entities": []}, {"text": "In this section, we first describe an initial experiment comparing the baseline system (described above) using the original training data with aversion trained with an augmented data set where labels changed based on a simple heuristic.", "labels": [], "entities": []}, {"text": "We then describe our main body of experiments using various weakly supervised learning methods for relabeling the data.", "labels": [], "entities": []}, {"text": "Finally, we report our overall scores on the evaluation data for all three organisms using the best system configurations derived from the development test data.", "labels": [], "entities": []}, {"text": "Our first set of experiments uses the baseline system described earlier.", "labels": [], "entities": []}, {"text": "We compare the results of this system using the Task 1B training data \"as provided\" with the results obtained by re-labeling some of the negative instances provided to the classifier as positive instances.", "labels": [], "entities": []}, {"text": "We re-labeled any instances as positive that matched a gene identifier associated with the abstract regardless of the (potentially incorrect) label associated with the identifier.", "labels": [], "entities": []}, {"text": "The Task 1B dataset creators marked an identifier \"no\" if an exact lexicon match wasn't found in the abstract.", "labels": [], "entities": []}, {"text": "As our system matching phase is a bit different (i.e. we remove punctuation and ignore case), this amounts to re-labeling the training data using this looser criterion.", "labels": [], "entities": [{"text": "system matching", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.6688342690467834}]}, {"text": "The results of this match-based re-labeling are shown in Balanced F-measure scores comparing the baseline vs. a system trained with the match-based re-labeled instances on the development test data.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.5651164650917053}]}, {"text": "In our next set of experiments we tested a number of different weakly supervised learning configurations.", "labels": [], "entities": []}, {"text": "These different methods simply amount to different rankings of the instances to re-label (based on confidence and the gene name tags).", "labels": [], "entities": []}, {"text": "The basic algorithm (outlined in remains the same in all cases.", "labels": [], "entities": []}, {"text": "Specifically, we investigated three methods for ranking the instances to re-label: 1) na\u00efve self-training, 2) self-training with bagging, and 3) co-training.", "labels": [], "entities": []}, {"text": "Na\u00efve self-training consisted of training a single maximum entropy classifier with the full feature set on each partition and using it to re-label instances from the other partition based on confidence.", "labels": [], "entities": []}, {"text": "Self training with bagging followed the same idea but used bagging.", "labels": [], "entities": []}, {"text": "For each partition, we trained 20 separate classifiers on random subsets of the training data using the full feature set.", "labels": [], "entities": []}, {"text": "The confidence assigned to a test instance was then defined as the product of the confidences of the individual classifiers.", "labels": [], "entities": []}, {"text": "Co-training involved training two classifiers for each partition with feature split.", "labels": [], "entities": []}, {"text": "We split the features into context-based features such as the surrounding words and the number of gene ids matching the current phrase, and lexically-based features that included the phrase itself, affixes, the number of tokens in the phrase, etc.", "labels": [], "entities": []}, {"text": "We computed the aggregated confidences for each instance as the product of the confidences assigned by the resulting context-based and lexically-based classifiers.", "labels": [], "entities": []}, {"text": "We ran experiments for each of these three options both with the gene tagger and without the gene tagger.", "labels": [], "entities": []}, {"text": "The systems that included the gene tagger ranked all instances derived from tagged phrases above all instances derived from phrases that were not tagged regardless of the classifier confidence.", "labels": [], "entities": []}, {"text": "A final experimental condition we explored was comparing batch re-labeling vs. incremental relabeling.", "labels": [], "entities": []}, {"text": "Batch re-labeling involved training the classifiers once and re-labeling all k instances using the same classifier.", "labels": [], "entities": []}, {"text": "Incremental re-labeling consisted of iteratively re-labeling n instances over k/n epochs where the classifiers were re-trained on each epoch with the newly re-labeled training data.", "labels": [], "entities": []}, {"text": "Interestingly, incremental re-labeling did not perform better than batch re-labeling in our experiments.", "labels": [], "entities": []}, {"text": "All results reported here, therefore, used batch re-labeling.", "labels": [], "entities": []}, {"text": "After the training data was re-labeled, a single maximum entropy classifier was trained on the entire (now re-labeled) training set.", "labels": [], "entities": []}, {"text": "This resulting classifier was then applied to the development set in the manner described in Section 3.", "labels": [], "entities": []}, {"text": "We tested each of these six configurations for different values of k, where k is the total number of instances re-labeled 3 . highlights the maximum and average balanced f-measure scores across all values of k for the different system configurations.", "labels": [], "entities": []}, {"text": "Both the maximum and averaged scores appear noticeably higher when constraining the instances to re-label with the tagger.", "labels": [], "entities": []}, {"text": "The three weakly supervised methods perform comparably with bagging performing slightly better.", "labels": [], "entities": []}, {"text": "In order to gain further insight into re-labeling instances, we have plotted the balanced F-measure performance on the development test for various values of k.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9757519960403442}]}, {"text": "The upper graph indicates that the three different methods correlate strongly.", "labels": [], "entities": []}, {"text": "The bottom graph makes apparent the benefits of tagging as a constraint.", "labels": [], "entities": []}, {"text": "It also points to the weakness of the tagger, however.", "labels": [], "entities": []}, {"text": "At k=7000 and k=8000, the system tends to perform worse when using the tags as a constraint.", "labels": [], "entities": []}, {"text": "This indicates that tagger recall errors have the potential to filter out good candidates for re-labeling.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.8580142259597778}]}, {"text": "We report our results using the best overall system configurations on the Task 1B evaluation data.", "labels": [], "entities": [{"text": "Task 1B evaluation data", "start_pos": 74, "end_pos": 97, "type": "DATASET", "confidence": 0.7740427032113075}]}, {"text": "We \"submitted\" 3 runs for two different mouse configurations and one for both fly and yeast.", "labels": [], "entities": []}, {"text": "The highest scores over the 3 runs are reported in.", "labels": [], "entities": []}, {"text": "MouseWS used the best weakly supervised method as determined on the development test data: bagging with k=4000.", "labels": [], "entities": [{"text": "MouseWS", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9577056765556335}]}, {"text": "MouseMBR, YeastMBR and FlyMBR used match-based re-labeling described in Section 5.2.", "labels": [], "entities": [{"text": "MouseMBR", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9409266710281372}, {"text": "YeastMBR", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.9679762721061707}, {"text": "FlyMBR", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.8078364133834839}]}, {"text": "The Gaussian prior was set to 2.0 for all runs and the 3 submissions for each configuration only varied in the threshold value T.", "labels": [], "entities": [{"text": "Gaussian prior", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.962191104888916}]}, {"text": "These results are competitive compared with the BioCreAtIvE Task 1B results where the highest F-measures for mouse, fly and yeast were 79.1, 81.5 and 92.1 with the medians at 73.8, 66.1 and 85.8, respectively.", "labels": [], "entities": [{"text": "BioCreAtIvE Task 1B", "start_pos": 48, "end_pos": 67, "type": "DATASET", "confidence": 0.6274267037709554}, {"text": "F-measures", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9977637529373169}]}, {"text": "The results for mouse and fly improve upon previous best reported results with an organism invariant, automatic system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 Balanced F-measure scores comparing the  baseline vs. a system trained with the match-based  re-labeled instances on the development test data.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9572099447250366}]}, {"text": " Table 2. Maximum and average balanced f-measure  scores on the mouse data set for each of the six sys- tem configurations for all values of k -the number of  instances re-labeled. The numbers in parentheses  indicate for which value of k the maximum value was  achieved.", "labels": [], "entities": [{"text": "mouse data set", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.800652543703715}]}, {"text": " Table  3. MouseWS used the best weakly supervised  method as determined on the development test  data: bagging with k=4000. MouseMBR, Ye- astMBR and FlyMBR used match-based re-labeling  described in Section 5.2. The Gaussian prior was  set to 2.0 for all runs and the 3 submissions for  each configuration only varied in the threshold  value T.", "labels": [], "entities": [{"text": "MouseWS", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.9027442932128906}, {"text": "MouseMBR", "start_pos": 125, "end_pos": 133, "type": "DATASET", "confidence": 0.927969753742218}, {"text": "FlyMBR", "start_pos": 150, "end_pos": 156, "type": "DATASET", "confidence": 0.5541864037513733}]}, {"text": " Table 3. Final evaluation results.", "labels": [], "entities": []}]}