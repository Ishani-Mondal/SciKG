{"title": [], "abstractContent": [{"text": "Distributional similarity requires large volumes of data to accurately represent infrequent words.", "labels": [], "entities": [{"text": "Distributional similarity", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7968388497829437}]}, {"text": "However, the nearest-neighbour approach to finding synonyms suffers from poor scalability.", "labels": [], "entities": []}, {"text": "The Spatial Approximation Sample Hierarchy (SASH), proposed by Houle (2003b), is a data structure for approximate nearest-neighbour queries that balances the effi-ciency/approximation trade-off.", "labels": [], "entities": [{"text": "Spatial Approximation Sample Hierarchy (SASH)", "start_pos": 4, "end_pos": 49, "type": "TASK", "confidence": 0.7935362075056348}]}, {"text": "We have intergrated this into an existing distribu-tional similarity system, tripling efficiency with a minor accuracy penalty.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9993602633476257}]}], "introductionContent": [{"text": "With the development of WordNet and large electronic thesauri, information from lexical semantic resources is regularly used to solve NLP problems.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9363642334938049}]}, {"text": "These problems include collocation discovery), smoothing and estimation) and question answering).", "labels": [], "entities": [{"text": "collocation discovery", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.7073678970336914}, {"text": "question answering", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8991208672523499}]}, {"text": "Unfortunately, these resources are expensive and time-consuming to create manually, and tend to suffer from problems of bias, inconsistency, and limited coverage.", "labels": [], "entities": []}, {"text": "In addition, lexicographers cannot keep up with constantly evolving language use and cannot afford to build new resources for the many subdomains that NLP techniques are being applied to.", "labels": [], "entities": []}, {"text": "There is a clear need for methods to extract lexical semantic resources automatically or tools that assist in their manual creation and maintenance.", "labels": [], "entities": []}, {"text": "Much of the existing work on automatically extracting resources is based on the distributional hypothesis that similar words appear in similar contexts.", "labels": [], "entities": []}, {"text": "Existing approaches differ primarily in their definition of \"context\", e.g. the surrounding words or the entire document, and their choice of distance metric for calculating similarity between the vector of contexts representing each term.", "labels": [], "entities": []}, {"text": "Finding synonyms using distributional similarity involves performing a nearest-neighbour search over the context vectors for each term.", "labels": [], "entities": []}, {"text": "This is very computationally intensive and scales according to the vocabulary size and the number of contexts for each term.", "labels": [], "entities": []}, {"text": "have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality.", "labels": [], "entities": []}, {"text": "Unfortunately, this also increases the vocabulary size and the number of contexts for each term, making the use of huge datasets infeasible.", "labels": [], "entities": []}, {"text": "There have been many data structures and approximation algorithms proposed to reduce the computational complexity of nearest-neighbour search ().", "labels": [], "entities": []}, {"text": "Many of these approaches reduce the search space by using clustering techniques to generate an index of near-neighbours.", "labels": [], "entities": []}, {"text": "We use the Spacial Approximation Sample Hierarchy (SASH) data structure developed by as it allows more control over the efficiency-approximation trade-off than other approximation methods.", "labels": [], "entities": [{"text": "Spacial Approximation Sample Hierarchy (SASH)", "start_pos": 11, "end_pos": 56, "type": "TASK", "confidence": 0.7806311930928912}]}, {"text": "This paper describes integrating the SASH into an existing distributional similarity system).", "labels": [], "entities": [{"text": "SASH", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.7780709266662598}]}, {"text": "We show that replacing the nearestneighbour search improves efficiency by a factor of three with only a minor accuracy penalty.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9993060827255249}]}], "datasetContent": [{"text": "The simplest method of evaluation is direct comparison of the extracted synonyms with a manuallycreated gold standard.", "labels": [], "entities": []}, {"text": "However, on small corpora, rare direct matches provide limited information for evaluation, and thesaurus coverage is a problem.", "labels": [], "entities": [{"text": "thesaurus coverage", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.47013235092163086}]}, {"text": "Our evaluation uses a combination of three electronic thesauri: the Macquarie (Bernard, 1990), Roget's (Roget, 1911) and Moby thesauri.", "labels": [], "entities": [{"text": "Macquarie (Bernard, 1990)", "start_pos": 68, "end_pos": 93, "type": "DATASET", "confidence": 0.9151856601238251}]}, {"text": "With this gold standard in place, it is possible to use precision and recall measures to evaluate the quality of the extracted thesaurus.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9994786381721497}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9983261227607727}]}, {"text": "To help overcome the problems of direct comparisons we use several measures of system performance: direct matches (DIRECT), inverse rank (INVR), and precision of the top n synonyms (P(n)), for n = 1, 5 and 10.", "labels": [], "entities": [{"text": "direct matches (DIRECT)", "start_pos": 99, "end_pos": 122, "type": "METRIC", "confidence": 0.6181816458702087}, {"text": "inverse rank (INVR)", "start_pos": 124, "end_pos": 143, "type": "METRIC", "confidence": 0.9435735940933228}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9989221096038818}]}, {"text": "INVR is the sum of the inverse rank of each matching synonym, e.g. matching synonyms at ranks 3, 5 and 28 give an inverse rank score of 1 3 + 1 5 + 1 28 , and with at most 100 synonyms, the maximum INVR score is 5.187.", "labels": [], "entities": [{"text": "INVR", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7912251949310303}, {"text": "INVR score", "start_pos": 198, "end_pos": 208, "type": "METRIC", "confidence": 0.9637533128261566}]}, {"text": "Precision of the top n is the percentage of matching synonyms in the top n extracted synonyms.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9818052649497986}]}, {"text": "The same 70 single-word nouns were used for the evaluation as in.", "labels": [], "entities": []}, {"text": "These were chosen randomly from WordNet such that they covered a range over the following properties: For each of these terms, the closest 100 terms and their similarity score were extracted.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9627779126167297}]}, {"text": "The contexts were extracted from the non-speech portion of the British National Corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 63, "end_pos": 86, "type": "DATASET", "confidence": 0.9476368625958761}]}, {"text": "All experiments used the JACCARD measure function, the TTEST weight function and a cutoff frequency of 5.", "labels": [], "entities": [{"text": "JACCARD measure function", "start_pos": 25, "end_pos": 49, "type": "METRIC", "confidence": 0.8168652057647705}, {"text": "TTEST weight function", "start_pos": 55, "end_pos": 76, "type": "METRIC", "confidence": 0.9532989859580994}]}, {"text": "The SASH was constructed using the geometric equation fork i described in Section 4.4.", "labels": [], "entities": [{"text": "SASH", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.8311375379562378}]}, {"text": "When the heuristic was applied, the TTESTLOG weight function was used with a canonical set size of 100 and a maximum frequency cutoff of 10,000.", "labels": [], "entities": [{"text": "TTESTLOG weight function", "start_pos": 36, "end_pos": 60, "type": "METRIC", "confidence": 0.9462107419967651}]}, {"text": "The values 4-16, 8-32, 16-64, and 32-128 were chosen for p and c.", "labels": [], "entities": []}, {"text": "This gives a range of branching factors to test the balance between sparseness, where there is potential for erroneous fragmentation of large clusters, and bushiness, where more tests must be made to find near children.", "labels": [], "entities": []}, {"text": "The c = 4p relationship is derived from the simple hashing rule of thumb that says that a hash table should have roughly twice the size required to store all its elements   Our initial experiments showed that the random distribution of nodes (RANDOM) in SASH construction caused the nearest-neighbour approximation to be very inaccurate for distributional similarity.", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 243, "end_pos": 249, "type": "METRIC", "confidence": 0.8400118947029114}, {"text": "SASH construction", "start_pos": 254, "end_pos": 271, "type": "TASK", "confidence": 0.8519065082073212}]}, {"text": "Although the speed was improved by two orders of magnitude when c = 16, it achieved only 13% of the INVR of the na\u00a8\u0131vena\u00a8\u0131ve implementation.", "labels": [], "entities": [{"text": "speed", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9889611601829529}, {"text": "INVR", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9973351359367371}]}, {"text": "The best RAN-DOM result was less than three times faster then the na\u00a8\u0131vena\u00a8\u0131ve solution and only 60% INVR.", "labels": [], "entities": [{"text": "RAN-DOM", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9028170108795166}, {"text": "INVR", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.996620774269104}]}, {"text": "In accordance with Zipf's law the majority of terms have very low frequencies.", "labels": [], "entities": []}, {"text": "Similarity measurements made against these low frequency terms are less reliable, as accuracy increases with the number of relations and their frequencies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9989911913871765}]}, {"text": "This led to the idea that ordering the nodes by frequency before generating the SASH would improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9958269000053406}]}, {"text": "The SASH was then generated with the highest frequency terms were near the root so that the initial search paths would be more accurate.", "labels": [], "entities": [{"text": "SASH", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.6542039513587952}]}, {"text": "This has the unfortunate side-effect of slowing search by up to four times because comparisons with high frequency terms take longer than with low frequency terms as they have a larger number of relations.", "labels": [], "entities": []}, {"text": "DIST c DIRECT P(1) P   This led to updating our original frequency ordering idea by recognising that we did not need the most accurately comparable terms at the top of the SASH, only more accurately comparable terms than those randomly selected.", "labels": [], "entities": []}, {"text": "As a first attempt, we constructed SASHs with frequency orderings that were folded about a chosen number of relations M.", "labels": [], "entities": [{"text": "SASHs", "start_pos": 35, "end_pos": 40, "type": "TASK", "confidence": 0.9051477313041687}]}, {"text": "For each term, if its number of relations mi was greater than M, it was given anew ranking based on the score M 2 mi . In this way, very high and very low frequency terms were pushed away from the root.", "labels": [], "entities": []}, {"text": "The folding points this was tested for were 500, 1000 and 1500.", "labels": [], "entities": [{"text": "folding", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9653310179710388}]}, {"text": "There are many other node organising schemes we are yet to explore.", "labels": [], "entities": [{"text": "node organising", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7082638144493103}]}, {"text": "The frequency distributions over the top three levels for each ordering scheme are shown in.", "labels": [], "entities": []}, {"text": "Zipf's law results in a large difference between the mean and median frequency values in the RANDOM results: most of the nodes have low frequency, but some high frequency results push the mean up.", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 93, "end_pos": 99, "type": "TASK", "confidence": 0.42463046312332153}]}, {"text": "The four-fold reduction in efficiency for SORT (see) is a result of the mean number of relations being over 65 times that of RANDOM.", "labels": [], "entities": [{"text": "SORT", "start_pos": 42, "end_pos": 46, "type": "TASK", "confidence": 0.8698874115943909}, {"text": "RANDOM", "start_pos": 125, "end_pos": 131, "type": "DATASET", "confidence": 0.850404679775238}]}, {"text": "Experiments covering the full set of permutations of these parameters were run, with and without the heuristic applied.", "labels": [], "entities": []}, {"text": "In the cases where the heuristic rejected pairs of terms, the SASH treated the rejected pairs as being as infinitely far apart.", "labels": [], "entities": [{"text": "SASH", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.48636531829833984}]}, {"text": "In addition, the brute force solutions were generated with (NAIVE HEURISTIC) and without (NAIVE) the heuristic.", "labels": [], "entities": [{"text": "NAIVE", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.5052399635314941}, {"text": "HEURISTIC", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.6096891760826111}]}, {"text": "We have assumed that all weights and measures introduce similar distribution properties into the SASH, so that the best weight and measure when performing a brute-force search will also produce the best results when combined with the SASH.", "labels": [], "entities": [{"text": "SASH", "start_pos": 97, "end_pos": 101, "type": "TASK", "confidence": 0.5880522727966309}, {"text": "SASH", "start_pos": 234, "end_pos": 238, "type": "DATASET", "confidence": 0.801957905292511}]}, {"text": "Future experiments will explore SASH behaviour with other similarity measures.", "labels": [], "entities": [{"text": "SASH", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.9792516827583313}]}, {"text": "presents the results for the initial experiments.", "labels": [], "entities": []}, {"text": "SORT was consistently more accurate than RANDOM, and when c = 128, performed as well as NAIVE for all evaluation measures except for direct matches.", "labels": [], "entities": [{"text": "SORT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9357807040214539}, {"text": "accurate", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9825302958488464}, {"text": "RANDOM", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.646889865398407}, {"text": "NAIVE", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9645030498504639}]}, {"text": "Both SASH solutions outperformed NAIVE in efficiency.", "labels": [], "entities": [{"text": "NAIVE", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.7142754197120667}]}], "tableCaptions": [{"text": " Table 1: Load time distributions and values of c", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of different random and fully sorted distributions", "labels": [], "entities": []}, {"text": " Table 5: Evaluation of different distributions using the approximation", "labels": [], "entities": []}]}