{"title": [{"text": "Investigating the Effects of Selective Sampling on the Annotation Task", "labels": [], "entities": [{"text": "Selective Sampling", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.6658524572849274}]}], "abstractContent": [{"text": "We report on an active learning experiment for named entity recognition in the astronomy domain.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.6247170368830363}]}, {"text": "Active learning has been shown to reduce the amount of labelled data required to train a supervised learner by selectively sampling more informative data points for human annotation.", "labels": [], "entities": []}, {"text": "We inspect double annotation data from the same domain and quantify potential problems concerning annotators' performance.", "labels": [], "entities": []}, {"text": "For data selectively sampled according to different selection metrics, we find lower inter-annotator agreement and higher per token annotation times.", "labels": [], "entities": []}, {"text": "However, overall results confirm the utility of active learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised training of named entity recognition (NER) systems requires large amounts of manually annotated data.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.7892944465080897}]}, {"text": "However, human annotation is typically costly and time-consuming.", "labels": [], "entities": []}, {"text": "Active learning promises to reduce this cost by requesting only those data points for human annotation which are highly informative.", "labels": [], "entities": []}, {"text": "Example informativity can be estimated by the degree of uncertainty of a single learner as to the correct label of a data point or in terms of the disagreement of a committee of learners (.", "labels": [], "entities": []}, {"text": "Active learning has been successfully applied to a variety of tasks such as document classification, part-of-speech tagging, and parsing ().", "labels": [], "entities": [{"text": "document classification", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.8227665722370148}, {"text": "part-of-speech tagging", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.7291450500488281}, {"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9689130783081055}]}, {"text": "We employ a committee-based method where the degree of deviation of different classifiers with respect to their analysis can tell us if an example is potentially useful.", "labels": [], "entities": []}, {"text": "Ina companion paper), we present active learning experiments for NER in radio-astronomical texts following this approach.", "labels": [], "entities": [{"text": "NER", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9794384241104126}]}, {"text": "1 These experiments prove the utility of selective sampling and suggest that parameters fora new domain can be optimised in another domain for which annotated data is already available.", "labels": [], "entities": []}, {"text": "However there are some provisos for active learning.", "labels": [], "entities": []}, {"text": "An important point to consider is what effect informative examples have on the annotators.", "labels": [], "entities": []}, {"text": "Are these examples more difficult?", "labels": [], "entities": []}, {"text": "Will they affect the annotators' performance in terms of accuracy?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9979404807090759}]}, {"text": "Will they affect the annotators performance in terms of time?", "labels": [], "entities": []}, {"text": "In this paper, we explore these questions using doubly annotated data.", "labels": [], "entities": []}, {"text": "We find that selective sampling does have an adverse effect on annotator accuracy and efficiency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9838789701461792}]}, {"text": "In section 2, we present standard active learning results showing that good performance can be achieved using fewer examples than random sampling.", "labels": [], "entities": []}, {"text": "Then, in section 3, we address the questions above, looking at the relationship between interannotator agreement and annotation time and the examples that are selected by active learning.", "labels": [], "entities": []}, {"text": "Finally, section 4 presents conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To tune the active learning parameters discussed in section 2.3, we ran detailed simulated experiments on the named entity data from the BioNLP shared task of the COLING 2004 International Joint Workshop on Natural Language Processing in Biomedicine and its Applications ().", "labels": [], "entities": [{"text": "BioNLP shared task of the COLING 2004 International Joint Workshop on Natural Language Processing", "start_pos": 137, "end_pos": 234, "type": "TASK", "confidence": 0.6327184438705444}]}, {"text": "These results are treated in detail in the companion paper).", "labels": [], "entities": []}, {"text": "We used the CMM tagger to train two different models by splitting the feature set to give multiple views of the same data.", "labels": [], "entities": [{"text": "CMM tagger", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.6437425911426544}]}, {"text": "The feature set was handcrafted such that it comprises different views while empirically ensuring that performance is sufficiently similar.", "labels": [], "entities": []}, {"text": "On the basis of the findings of the simulation experiments we setup the real active learning annotation experiment using: average KL-divergence as the selection metric and a feature split that divides the full feature set roughly into features of words and features derived from external resources.", "labels": [], "entities": []}, {"text": "As smaller batch sizes require more retraining iterations and larger batch sizes increase the amount of annotation necessary at each round and could lead to unnecessary strain for the annotators, we settled on a batch size of 50 sentences for the real AL experiment as a compromise between computational cost and workload for the annotator.", "labels": [], "entities": []}, {"text": "We developed an active annotation tool and ran real annotation experiments on the astronomy abstracts described in section 2.1.", "labels": [], "entities": []}, {"text": "The tool was given to the same astronomy PhD students for annotation who were responsible for the seed and test data.", "labels": [], "entities": []}, {"text": "The learning curve for selective sampling is plotted in.", "labels": [], "entities": []}, {"text": "The randomly sampled data was doubly annotated and the learning curve is averaged between the two annotators.", "labels": [], "entities": []}, {"text": "Comparing the selective sampling performance to the baseline, we confirm that active learning provides a significant reduction in the number of examples that need annotating.", "labels": [], "entities": []}, {"text": "In fact, the random curve reaches an f-score of 76 after approximately 39000 tokens have been annotated while the selective sampling curve reaches this level of performance after only \u2248 24000 tokens.", "labels": [], "entities": [{"text": "f-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9918618202209473}]}, {"text": "This represents a substantial reduction in tokens annotated of 38.5%.", "labels": [], "entities": []}, {"text": "In addition, at 39000 tokens, selectively sampling offers an error reduction of 21.4% with a 3 point improvement in f-score.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 61, "end_pos": 76, "type": "METRIC", "confidence": 0.9673571586608887}, {"text": "f-score", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.9649757146835327}]}], "tableCaptions": [{"text": " Table 1: Phrasal confusion matrices for document  sub-set of 100 sentences sorted by average KL- divergence and for full random document sub-set of  1000 sentences (A1: Annotator 1, A2: Annotator 2).", "labels": [], "entities": [{"text": "Phrasal confusion", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.5561732947826385}, {"text": "KL- divergence", "start_pos": 94, "end_pos": 108, "type": "METRIC", "confidence": 0.8700052698453268}]}]}