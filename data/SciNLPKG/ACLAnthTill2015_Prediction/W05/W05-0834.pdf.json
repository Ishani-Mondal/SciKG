{"title": [{"text": "Word Graphs for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.8269635041554769}]}], "abstractContent": [{"text": "Word graphs have various applications in the field of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7897293567657471}]}, {"text": "Therefore it is important for machine translation systems to produce compact word graphs of high quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7770738303661346}]}, {"text": "We will describe the generation of word graphs for state of the art phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.5979212621847788}]}, {"text": "We will use these word graph to provide an analysis of the search process.", "labels": [], "entities": []}, {"text": "We will evaluate the quality of the word graphs using the well-known graph word error rate.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 75, "end_pos": 90, "type": "METRIC", "confidence": 0.654079924027125}]}, {"text": "Additionally, we introduce the two novel graph-to-string criteria: the position-independent graph word error rate and the graph BLEU score.", "labels": [], "entities": [{"text": "position-independent graph word error rate", "start_pos": 71, "end_pos": 113, "type": "METRIC", "confidence": 0.6170331954956054}, {"text": "BLEU score", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9774574339389801}]}, {"text": "Experimental results are presented for two Chinese-English tasks: the small IWSLT task and the NIST large data track task.", "labels": [], "entities": [{"text": "NIST large data track task", "start_pos": 95, "end_pos": 121, "type": "DATASET", "confidence": 0.7802434206008911}]}, {"text": "For both tasks, we achieve significant reductions of the graph error rate already with compact word graphs.", "labels": [], "entities": [{"text": "graph error rate", "start_pos": 57, "end_pos": 73, "type": "METRIC", "confidence": 0.7150384982426962}]}], "introductionContent": [{"text": "A statistical machine translation system usually produces the single-best translation hypotheses fora source sentence.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 2, "end_pos": 33, "type": "TASK", "confidence": 0.6275889078776041}]}, {"text": "For some applications, we are also interested in alternative translations.", "labels": [], "entities": []}, {"text": "The simplest way to represent these alternatives is a list with the N -best translation candidates.", "labels": [], "entities": []}, {"text": "These N -best lists have one major disadvantage: the high redundancy.", "labels": [], "entities": []}, {"text": "The translation alternatives may differ only by a single word, but still both are listed completely.", "labels": [], "entities": []}, {"text": "Usually, the size of the N -best list is in the range of a few hundred up to a few thousand candidate translations per source sentence.", "labels": [], "entities": []}, {"text": "If we want to use larger N -best lists the processing time gets very soon infeasible.", "labels": [], "entities": []}, {"text": "Word graphs area much more compact representation that avoid these redundancies as much as possible.", "labels": [], "entities": []}, {"text": "The number of alternatives in a word graph is usually an order of magnitude larger than in an Nbest list.", "labels": [], "entities": []}, {"text": "The graph representation avoids the combinatorial explosion that make large N -best lists infeasible.", "labels": [], "entities": []}, {"text": "Word graphs are an important data structure with various applications: \u2022 Word Filter.", "labels": [], "entities": []}, {"text": "The word graph is used as a compact representation of a large number of sentences.", "labels": [], "entities": []}, {"text": "The score information is not contained.", "labels": [], "entities": []}, {"text": "We can use word graphs for rescoring with more sophisticated models, e.g. higher-order language models.", "labels": [], "entities": []}, {"text": "The training of the model scaling factors as described in ) was done on N -best lists.", "labels": [], "entities": []}, {"text": "Using word graphs instead could further improve the results.", "labels": [], "entities": []}, {"text": "Also, the phrase translation probabilities could be trained discrimatively, rather than only the scaling factors.", "labels": [], "entities": [{"text": "phrase translation probabilities", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7870667974154154}]}, {"text": "Word graphs can be used to derive confidence measures, such as the posterior probability ().", "labels": [], "entities": [{"text": "posterior probability", "start_pos": 67, "end_pos": 88, "type": "METRIC", "confidence": 0.9358816742897034}]}, {"text": "Some interactive machine translation systems make use of word graphs, e.g. ( ).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7359679639339447}]}, {"text": "Although there are these many applications, there are only few publications directly devoted to word graphs.", "labels": [], "entities": [{"text": "word graphs", "start_pos": 96, "end_pos": 107, "type": "TASK", "confidence": 0.727260410785675}]}, {"text": "The only publication, we are aware of, is).", "labels": [], "entities": []}, {"text": "The shortcomings of) are: \u2022 They use single-word based models only.", "labels": [], "entities": []}, {"text": "Current state of the art statistical machine translation systems are phrase-based.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6377337475617727}]}, {"text": "\u2022 Their graph pruning method is suboptimal as it considers only partial scores and not full path scores.", "labels": [], "entities": []}, {"text": "\u2022 The N -best list extraction does not eliminate duplicates, i.e. different paths that represent the same translation candidate.", "labels": [], "entities": []}, {"text": "\u2022 The rest cost estimation is not efficient.", "labels": [], "entities": []}, {"text": "It has an exponential worst-case time complexity.", "labels": [], "entities": []}, {"text": "We will describe an algorithm with linear worstcase complexity.", "labels": [], "entities": []}, {"text": "Apart from (), publications on weighted finite state transducer approaches to machine translation, e.g. (, deal with word graphs.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7641648054122925}]}, {"text": "But to our knowledge, there are no publications that give a detailed analysis and evaluation of the quality of word graphs for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7363958954811096}]}, {"text": "We will fill this gap and give a systematic description and an assessment of the quality of word graphs for phrase-based machine translation.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 108, "end_pos": 140, "type": "TASK", "confidence": 0.6666606465975443}]}, {"text": "We will show that even for hard tasks with very large vocabulary and long sentences the graph error rate drops significantly.", "labels": [], "entities": [{"text": "graph error rate", "start_pos": 88, "end_pos": 104, "type": "METRIC", "confidence": 0.7050349116325378}]}, {"text": "The remaining part is structured as follows: first we will give a brief description of the translation system in Section 2.", "labels": [], "entities": [{"text": "translation", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.9622895121574402}]}, {"text": "In Section 3, we will give a definition of word graphs and describe the generation.", "labels": [], "entities": []}, {"text": "We will also present efficient pruning and N -best list extraction techniques.", "labels": [], "entities": [{"text": "list extraction", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.74579057097435}]}, {"text": "In Section 4, we will describe evaluation criteria for word graphs.", "labels": [], "entities": []}, {"text": "We will use the graph word error rate, which is well known from speech recognition.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 22, "end_pos": 37, "type": "METRIC", "confidence": 0.6382349828879038}, {"text": "speech recognition", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7285045683383942}]}, {"text": "Additionally, we introduce the novel position-independent word graph error rate and the graph BLEU score.", "labels": [], "entities": [{"text": "position-independent word graph error rate", "start_pos": 37, "end_pos": 79, "type": "METRIC", "confidence": 0.5358712494373321}, {"text": "BLEU score", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9792669713497162}]}, {"text": "These are generalizations of the commonly used string-to-string evaluation criteria in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7282274663448334}]}, {"text": "We will present experimental results in Section 5 for two ChineseEnglish tasks: the first one, the IWSLT task, is in the domain of basic travel expression found in phrasebooks.", "labels": [], "entities": []}, {"text": "The vocabulary is limited and the sentences are short.", "labels": [], "entities": []}, {"text": "The second task is the NIST ChineseEnglish large data track task.", "labels": [], "entities": [{"text": "NIST ChineseEnglish large data track task", "start_pos": 23, "end_pos": 64, "type": "DATASET", "confidence": 0.8468777537345886}]}, {"text": "Here, the domain is news and therefore the vocabulary is very large and the sentences are with an average of 30 words quite long.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: IWSLT Chinese-English Task: corpus  statistics of the bilingual training data.  Chinese English  Train Sentences  20 000  Running Words 182 904 160 523  Vocabulary  7 643  6 982  Test  Sentences  506  Running Words  3 515  3 595  avg. SentLen  6.9  7.1", "labels": [], "entities": [{"text": "IWSLT Chinese-English Task", "start_pos": 10, "end_pos": 36, "type": "DATASET", "confidence": 0.7197397152582804}]}, {"text": " Table 2: NIST Chinese English task: corpus statis- tics of the bilingual training data.", "labels": [], "entities": [{"text": "NIST Chinese English task", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.8856444805860519}]}, {"text": " Table 3: IWSLT Chinese-English: Word graph densities for different window sizes and different stages of  the search process.  language level graph type  window size  1  2  3  4  5  source  word reordering  1.0  2.7  6.2  12.8  24.4  phrase segmented  2.0  5.0  12.1  26.8  55.6  target  translated  40.8  99.3  229.0  479.9  932.8  word TM scores  78.6  184.6  419.2  869.1  1 670.4  + LM scores 958.2 2874.2 7649.7 18 029.7 39 030.1", "labels": [], "entities": [{"text": "IWSLT Chinese-English", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.8611665070056915}]}]}