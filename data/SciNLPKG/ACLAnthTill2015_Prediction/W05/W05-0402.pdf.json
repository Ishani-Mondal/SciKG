{"title": [{"text": "Feature Engineering and Post-Processing for Temporal Expression Recognition Using Conditional Random Fields", "labels": [], "entities": [{"text": "Temporal Expression Recognition", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.8640274802843729}]}], "abstractContent": [{"text": "We present the results of feature engineering and post-processing experiments conducted on a temporal expression recognition task.", "labels": [], "entities": [{"text": "temporal expression recognition task", "start_pos": 93, "end_pos": 129, "type": "TASK", "confidence": 0.7187695577740669}]}, {"text": "The former explores the use of different kinds of tagging schemes and of exploiting a list of core temporal expressions during training.", "labels": [], "entities": []}, {"text": "The latter is concerned with the use of this list for post-processing the output of a system based on conditional random fields.", "labels": [], "entities": []}, {"text": "We find that the incorporation of knowledge sources both for training and post-processing improves recall, while the use of extended tagging schemes may help to offset the (mildly) negative impact on precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9987660646438599}, {"text": "precision", "start_pos": 200, "end_pos": 209, "type": "METRIC", "confidence": 0.9970598816871643}]}, {"text": "Each of these approaches addresses a different aspect of the overall recognition performance.", "labels": [], "entities": []}, {"text": "Taken separately , the impact on the overall performance is low, but by combining the approaches we achieve both high precision and high recall scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9993096590042114}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9991509914398193}]}], "introductionContent": [{"text": "Temporal expressions (timexes) are natural language phrases that refer directly to time points or intervals.", "labels": [], "entities": [{"text": "Temporal expressions (timexes) are natural language phrases", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7663050558831956}]}, {"text": "They not only convey temporal information on their own but also serve as anchors for locating events referred to in a text.", "labels": [], "entities": []}, {"text": "Timex recognition is a named entity recognition (NER) task to which a variety of natural language processing and machine learning techniques have been applied.", "labels": [], "entities": [{"text": "Timex recognition", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6910717040300369}, {"text": "named entity recognition (NER) task", "start_pos": 23, "end_pos": 58, "type": "TASK", "confidence": 0.8071891622883933}]}, {"text": "As with other NER tasks, timex recognition is naturally viewed as a sequence labeling task, easily lending itself to machine learning techniques such as conditional random fields (CRFs) (.", "labels": [], "entities": [{"text": "NER tasks", "start_pos": 14, "end_pos": 23, "type": "TASK", "confidence": 0.916483461856842}, {"text": "timex recognition", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7040959894657135}, {"text": "sequence labeling task", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7358805139859518}]}, {"text": "A preliminary experiment showed that, using CRFs, a respectable recognition performance can easily be achieved with a straightforward baseline system that is based on a simple tagging scheme and requires very little tuning, yielding F-scores around 0.78 (exact match) or even 0.90 (partial match).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 233, "end_pos": 241, "type": "METRIC", "confidence": 0.9896200299263}]}, {"text": "Interestingly, these high scores are mainly due to high or even very high precision scores, while recall leaves much to be improved.", "labels": [], "entities": [{"text": "precision scores", "start_pos": 74, "end_pos": 90, "type": "METRIC", "confidence": 0.9755309224128723}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9994063377380371}]}, {"text": "The main focus of this paper is on boosting recall while maintaining precision at an acceptable (i.e., high) level.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9991766810417175}, {"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9992475509643555}]}, {"text": "We report on two types of experiments aimed at achieving this goal.", "labels": [], "entities": []}, {"text": "One type concerns feature engineering and the other concerns post-processing the output of a machine learner.", "labels": [], "entities": []}, {"text": "While we do exploit the special nature of timexes, for portability reasons we avoid using task-specific and richer linguistic features.", "labels": [], "entities": []}, {"text": "Instead, we focus on features and techniques that can readily be applied to other NER tasks.", "labels": [], "entities": [{"text": "NER tasks", "start_pos": 82, "end_pos": 91, "type": "TASK", "confidence": 0.8927057981491089}]}, {"text": "Specifically, our feature engineering experiments have two facets.", "labels": [], "entities": []}, {"text": "The first concerns identification of a set of simple features that results in high generalization ability (accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9959926009178162}]}, {"text": "Here, particular emphasis will be placed on the use of a list of core timexes as a feature.", "labels": [], "entities": []}, {"text": "The assumption is that the performance of data-driven approaches for timex recognition can be improved by taking into account the peculiar properties of timexes.", "labels": [], "entities": [{"text": "timex recognition", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7252774238586426}]}, {"text": "Timexes exhibit various patterns, ranging from regular patterns that can easily be captured using simple regular expressions to complex linguistic forms (phrases).", "labels": [], "entities": []}, {"text": "While timexes are real-ized in different phrase types, the core lexical items of timexes are restricted.", "labels": [], "entities": []}, {"text": "This suggests that a list of core timexes can easily be compiled and used in machine learning-based timex recognition.", "labels": [], "entities": [{"text": "machine learning-based timex recognition", "start_pos": 77, "end_pos": 117, "type": "TASK", "confidence": 0.624605730175972}]}, {"text": "One approach of integrating such a list is using them to generate features, but the availability of such a list also opens up other possibilities in feature design that we present in later sections.", "labels": [], "entities": []}, {"text": "The second aspect concerns the tagging scheme.", "labels": [], "entities": [{"text": "tagging", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9810702800750732}]}, {"text": "As inmost NER experiments, the task of recognizing timexes is reduced to tagging.", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9453471899032593}]}, {"text": "Commonly used tagging schemes are Inside-Outside (IO) and BeginContinue-End-Unique-Negative (BCEUN)).", "labels": [], "entities": []}, {"text": "The IO tagging scheme, which we use as a baseline, assigns the tag Ito a token if it is part of a timex and O otherwise.", "labels": [], "entities": [{"text": "IO tagging", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.6613353341817856}]}, {"text": "The richer BCEUN scheme assigns the five tags B, C, E, U, and N to tokens depending on whether the token is single-token timex (U), a non-timex (N), appears at the beginning (B), at the end (E) or inside a timex boundary (C).", "labels": [], "entities": []}, {"text": "In this paper, we compare the IO, BCEUN and an extended form of the BCEUN tagging scheme.", "labels": [], "entities": [{"text": "IO", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.7349565625190735}, {"text": "BCEUN", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.7656566500663757}, {"text": "BCEUN tagging", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.6062328219413757}]}, {"text": "The extended scheme adds two tags, PRE and POST, to the BCEUN scheme, which correspond to tokens appearing to the left and to the right of a timex.", "labels": [], "entities": [{"text": "PRE", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9314553737640381}, {"text": "POST", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9706529378890991}, {"text": "BCEUN", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.6068034768104553}]}, {"text": "In contrast, our post-processing experiments investigate the application of the list of core timexes for filtering the output of a machine learner.", "labels": [], "entities": []}, {"text": "The incorporation into the recognition process of explicit knowledge in the form of a list for post-processing requires a carefully designed strategy to ensure that the important properties of the trained model are kept intact as much as possible while at the sametime improving overall results.", "labels": [], "entities": []}, {"text": "We present an approach for using a list for post-processing that exploits the knowledge embodied in the trained model.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we provide background material, both on the timex extraction task ( \u00a72.1) and on the machine learning techniques on which we build in this paper, conditional random fields ( \u00a72.2).", "labels": [], "entities": [{"text": "timex extraction task", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.7930271625518799}]}, {"text": "Our ideas on engineering feature sets and tagging schemes are presented in Section 3, while we describe our method for exploiting the explicit knowledge contained in a list in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we describe the experimental setup and present the results of our experiments.", "labels": [], "entities": []}, {"text": "Related work is briefly reviewed in Section 6, and we conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we provide an experimental assessment of the feature engineering and post-processing methods introduced in Sections 3 and 4.", "labels": [], "entities": []}, {"text": "Specifically, we want to determine what their impact is on the precision and recall scores of the baseline system, and how they can be combined to boost recall while keeping precision at an acceptable level.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9990609288215637}, {"text": "recall scores", "start_pos": 77, "end_pos": 90, "type": "METRIC", "confidence": 0.9661083519458771}, {"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9989294409751892}, {"text": "precision", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9986023306846619}]}, {"text": "The training data consists of 511 files, and the test data consists of 192 files; these files were made available in the 2004 Temporal Expression Recognition and Normalization Evaluation.", "labels": [], "entities": [{"text": "Temporal Expression Recognition and Normalization Evaluation", "start_pos": 126, "end_pos": 186, "type": "TASK", "confidence": 0.8215887546539307}]}, {"text": "The temporal expressions in the training files are marked with XML tags.", "labels": [], "entities": []}, {"text": "The minorThird system takes care of automatically converting from XML format to the corresponding tagging schemes.", "labels": [], "entities": []}, {"text": "A temporal expression enclosed by <TIMEX2> tags constitutes a span.", "labels": [], "entities": []}, {"text": "The features in the training instances are generated by looking at the surface forms of the tokens in the spans and their surrounding contexts.", "labels": [], "entities": []}, {"text": "lists the results of the first part of our experiments.", "labels": [], "entities": []}, {"text": "Specifically, for every tagging scheme, there are two sets of features, basic and list.", "labels": [], "entities": []}, {"text": "The results are based on both exact-match and partial match between the spans in the gold standard and the spans in the output of the systems, as explained in Subsection 2.1.", "labels": [], "entities": [{"text": "exact-match", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.986845850944519}, {"text": "gold standard", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.8265820145606995}]}, {"text": "In both the exact and partial match criteria, the addition of the list features leads to an improvement in recall, and no change or a decrease in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9996757507324219}, {"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9978826642036438}]}], "tableCaptions": [{"text": " Table 2: Timex: Results of applying post-processing on the systems in Table 1. The baseline (from Table 1)  is repeated for ease of reference; it does not use post-processing. Highest scores (Precision, Recall, F- measure) are in bold face.", "labels": [], "entities": [{"text": "Timex", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9795563220977783}, {"text": "Precision", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.9876382350921631}, {"text": "Recall", "start_pos": 204, "end_pos": 210, "type": "METRIC", "confidence": 0.9760894179344177}, {"text": "F- measure)", "start_pos": 212, "end_pos": 223, "type": "METRIC", "confidence": 0.9863840639591217}]}]}