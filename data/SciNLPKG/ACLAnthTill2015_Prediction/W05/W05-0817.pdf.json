{"title": [], "abstractContent": [{"text": "We briefly describe a word alignment system that combines two different methods in bitext correspondences identification.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7633056044578552}, {"text": "bitext correspondences identification", "start_pos": 83, "end_pos": 120, "type": "TASK", "confidence": 0.6035896440347036}]}, {"text": "The first one is a hypotheses testing approach (Gale and Church, 1991; Melamed, 2001; Tufi\u015f 2002) while the second one is closer to a model estimating approach (Brown et al., 1993; Och and Ney, 2000).", "labels": [], "entities": []}, {"text": "We show that combining the two aligners the results are significantly improved as compared to each individual aligner.", "labels": [], "entities": []}], "introductionContent": [{"text": "In) we described a translation equivalence extraction program called TREQ the development of which was twofold motivated: to help enriching the synsets of the Romanian wordnet () with new literals based on bilingual corpora evidence and to check the interlingual alignment of our wordnet against the Princeton Wordnet.", "labels": [], "entities": [{"text": "translation equivalence extraction", "start_pos": 19, "end_pos": 53, "type": "TASK", "confidence": 0.8999871810277303}, {"text": "TREQ", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9326645731925964}, {"text": "Princeton Wordnet", "start_pos": 300, "end_pos": 317, "type": "DATASET", "confidence": 0.9552838206291199}]}, {"text": "The translation equivalence extractor has been also incorporated into a WSD system () part of a semantic web annotation platform.", "labels": [], "entities": [{"text": "translation equivalence extractor", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.8796494007110596}]}, {"text": "It also constituted the backbone of our TREQ-AL word aligner which successfully participated in the previous HLT-NAACL 2003 Shared Task 1 on word alignment for RomanianEnglish parallel texts.", "labels": [], "entities": [{"text": "HLT-NAACL 2003 Shared Task 1", "start_pos": 109, "end_pos": 137, "type": "TASK", "confidence": 0.6316912412643433}, {"text": "word alignment for RomanianEnglish parallel texts", "start_pos": 141, "end_pos": 190, "type": "TASK", "confidence": 0.6808617413043976}]}, {"text": "A detailed description of TREQ&TREQ-AL is given in () and it will be very shortly overviewed.", "labels": [], "entities": [{"text": "TREQ", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.8093866109848022}, {"text": "TREQ-AL", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9104646444320679}]}, {"text": "A quite different approach from our hypotheses testing implemented in the TREQ-AL aligner is taken by the model-estimating aligners, most of them relying on the IBM models (1 to 5) described in the () seminal paper.", "labels": [], "entities": [{"text": "TREQ-AL aligner", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.8922822773456573}]}, {"text": "The first wide-spread and publicly available implementation of the IBM models was the GIZA program, which itself was part of the SMT toolkit EGYPT ().", "labels": [], "entities": [{"text": "SMT toolkit EGYPT", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.818898061911265}]}, {"text": "GIZA has been superseded by its recent extension GIZA++ little more details in a subsequent section.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9212767481803894}]}, {"text": "The alignments produced by MEBA were compared to the ones produced by TREQ-AL.", "labels": [], "entities": [{"text": "MEBA", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.7743542790412903}, {"text": "TREQ-AL", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.806165874004364}]}, {"text": "We used for comparison the Gold Standard 3 annotation from the HLT-NAACL 2003 Shared Task.", "labels": [], "entities": [{"text": "Gold Standard 3 annotation", "start_pos": 27, "end_pos": 53, "type": "DATASET", "confidence": 0.9315568953752518}, {"text": "HLT-NAACL 2003 Shared Task", "start_pos": 63, "end_pos": 89, "type": "DATASET", "confidence": 0.8500104993581772}]}, {"text": "In order to combine the two aligners we had to check whether their accuracy was comparable and that when they are wrong the set of mistakes made by one aligner is not a proper set of the errors made by the second one.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9991900324821472}]}, {"text": "The first check was performed by using McNamer's test and for the second we used Brill &Wu test).", "labels": [], "entities": [{"text": "McNamer's test", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.7240780393282572}, {"text": "Brill &Wu test", "start_pos": 81, "end_pos": 95, "type": "METRIC", "confidence": 0.8489105999469757}]}, {"text": "Both tests confirmed that the conditions for combining were ensured so, we built the combiner.", "labels": [], "entities": [{"text": "combining", "start_pos": 45, "end_pos": 54, "type": "TASK", "confidence": 0.9436494708061218}]}, {"text": "The Combined Word Aligner, COWAL, is a wrapper of the two aligners (TREQ-AL and MEBA) ensuring the pre-and post-processing.", "labels": [], "entities": [{"text": "Word Aligner", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.6219541728496552}, {"text": "TREQ-AL", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.9849051833152771}, {"text": "MEBA", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.7983450889587402}]}, {"text": "It is complemented by a graphical user interface that allows for the visualisation of the alignments (intermediary and the final ones) as well as for their editing.", "labels": [], "entities": []}, {"text": "We should note that the corrections made by the user are stored by COWAL as positive and negative examples for word dependencies (in the monolingual context) and translation equivalencies (in the bilingual context).", "labels": [], "entities": [{"text": "COWAL", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.8734299540519714}, {"text": "translation equivalencies", "start_pos": 162, "end_pos": 187, "type": "TASK", "confidence": 0.9053764939308167}]}, {"text": "In the current version the editorial logs are used by the human developers but we plan to further extend COWAL for automatic learning from this extremely valuable kind of data.", "labels": [], "entities": [{"text": "COWAL", "start_pos": 105, "end_pos": 110, "type": "DATASET", "confidence": 0.5491997003555298}]}], "datasetContent": [{"text": "Neither TREQ-AL nor MEBA needs an a priori bilingual dictionary, as this will be automatically extracted by the TREQ or GIZA++.", "labels": [], "entities": [{"text": "MEBA", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.8877036571502686}, {"text": "TREQ", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.4871649444103241}]}, {"text": "We made evaluation of the individual alignments in both experimental settings: without a startup bilingual lexicon and with an initial mid-sized bilingual lexicon.", "labels": [], "entities": []}, {"text": "Surprisingly enough, we found that while the performance of TREQ-AL increases a little bit (approx. 1% increase of the F-measure) MEBA is doing better without an additional lexicon.", "labels": [], "entities": [{"text": "TREQ-AL", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.93146812915802}, {"text": "F-measure", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9871411323547363}]}, {"text": "So, in the evaluation below MEBA uses only the training data vocabulary.", "labels": [], "entities": [{"text": "MEBA", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.714250922203064}]}, {"text": "After the release of the official Gold Standard we noticed and corrected some obvious errors and also removed the controversial links of the type c) discussed in the previous section.", "labels": [], "entities": [{"text": "Gold Standard", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.850766509771347}]}, {"text": "The evaluations against this new \"Gold Standard\" showed, on average, 3.5% better figures (precision, recall, F-measure and AER) for the individual aligners, while for the combined classifiers, the performance scores were about 4% better.", "labels": [], "entities": [{"text": "Gold Standard\"", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.6690012315909067}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9995657801628113}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.996428906917572}, {"text": "F-measure", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9954164028167725}, {"text": "AER", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9969377517700195}]}, {"text": "MEBA is very sensitive to the values of the parameters which control its behavior.", "labels": [], "entities": [{"text": "MEBA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7566168308258057}]}, {"text": "Currently they are set according to the developers' intuition and after the analysis of the results from several trials.", "labels": [], "entities": []}, {"text": "Since this activity is pretty time consuming (human analysis plus re-training might take a couple of hours) we plan to extend MEBA with a supervised learning module, which would automatically determine the \"optimal\" parameters (thresholds and weights) values.", "labels": [], "entities": []}], "tableCaptions": []}