{"title": [{"text": "A Connectionist Model of Language-Scene Interaction", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent \"visual worlds\" studies, wherein researchers study language in context by monitoring eye-movements in a visual scene during sentence processing, have revealed much about the interaction of diverse information sources and the time course of their influence on comprehension.", "labels": [], "entities": []}, {"text": "In this study, five experiments that trade off scene context with a variety of linguistic factors are modelled with a Simple Recurrent Network modified to integrate a scene representation with the standard incremental input of a sentence.", "labels": [], "entities": []}, {"text": "The results show that the model captures the qualitative behavior observed during the experiments, while retaining the ability to develop the correct interpretation in the absence of visual input.", "labels": [], "entities": []}], "introductionContent": [{"text": "People learn language within the context of the surrounding world, and use it to refer to objects in that world, as well as relationships among those objects (e.g.,).", "labels": [], "entities": []}, {"text": "Recent research in the visual worlds paradigm, wherein participants' gazes in a scene while listening to an utterance are monitored, has yielded a number of insights into the time course of sentence comprehension.", "labels": [], "entities": []}, {"text": "The careful manipulation of information sources in this experimental setting has begun to reveal important characteristics of comprehension such as incrementality and anticipation.", "labels": [], "entities": [{"text": "anticipation", "start_pos": 167, "end_pos": 179, "type": "METRIC", "confidence": 0.9706537127494812}]}, {"text": "For example, people's attention to objects in a scene closely tracks their mention in a spoken sentence (, and world and linguistic knowledge seem to be factors that facilitate object identification (.", "labels": [], "entities": [{"text": "object identification", "start_pos": 177, "end_pos": 198, "type": "TASK", "confidence": 0.723700612783432}]}, {"text": "More recently, have shown that when scenes include depicted events, such visual information helps to establish important relations between the entities, such as role relations.", "labels": [], "entities": []}, {"text": "Models of sentence comprehension to date, however, continue to focus on modelling reading behavior.", "labels": [], "entities": []}, {"text": "No model, to our knowledge, attempts to account for the use of immediate (non-linguistic) context.", "labels": [], "entities": []}, {"text": "In this paper we present results from two simulations using a Simple Recurrent Network (SRN;) modified to integrate input from a scene with the characteristic incremental processing of such networks in order to model people's ability to adaptively use the contextual information in visual scenes to more rapidly interpret and disambiguate a sentence.", "labels": [], "entities": []}, {"text": "In the modelling of five visual worlds experiments reported here, accurate sentence interpretation hinges on proper case-role assignment to sentence referents.", "labels": [], "entities": [{"text": "sentence interpretation", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.7084568589925766}]}, {"text": "In particular, modelling is focussed on the following aspects of sentence processing: \u2022 anticipation of upcoming arguments and their roles in a sentence \u2022 adaptive use of the visual scene as context fora spoken utterance \u2022 influence of depicted events on developing interpretation \u2022 multiple/conflicting information sources and their relative importance on whether the hare is the subject or object of the sentence, as well as the thematic role structure of the verb.", "labels": [], "entities": []}, {"text": "These gaze fixations reveal that people use linguistic and world knowledge to anticipate upcoming arguments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained the network to correctly handle sentences involving non-stereotypical events as well as stereotypical ones, both when visual context was present and when it was absent.", "labels": [], "entities": []}, {"text": "As over half a billion sentence/scene combinations were possible for all of the experiments, we adopted a grammar-based approach to exhaustively generate sentences and scenes based on the experimental materials while holding out the actual materials to be used for testing.", "labels": [], "entities": []}, {"text": "In order to accurately model the first two experiments involving selectional restrictions on verbs, two additional words were added to the lexicon for each character selected by a verb.", "labels": [], "entities": []}, {"text": "For example, in the sentence Der Hase frisst gleich den Kohl, the nouns Hase1, Hase2, Kohl1, and Kohl2 were used to develop training sentences.", "labels": [], "entities": []}, {"text": "These were meant to represent, for example, words such as \"rabbit\" and \"jackrabbit\" or \"carrot\" and \"lettuce\" in the lexicon that have the same distributional properties as the original words \"hare\" and \"cabbage\".", "labels": [], "entities": []}, {"text": "With these extra tokens the network could learn that Hase, frisst, and Kohl were correlated without ever encountering all three words in the same training sentence.", "labels": [], "entities": []}, {"text": "The experiments involving non-stereotypicality did not pose this constraint, so training sentences were simply generated to avoid presenting experimental items.", "labels": [], "entities": []}, {"text": "Some standard simplifications to the words have been made to facilitate modelling.", "labels": [], "entities": []}, {"text": "For example, multi-word adverbs such as fast immer were treated as one word through hyphenation so that sentence length within a given experimental setup is maintained.", "labels": [], "entities": []}, {"text": "Nominal case markings such as -n in Hasen were removed to avoid sparse data as these markings are idiosyncratic, while the case markings on the determiners are more informative overall.", "labels": [], "entities": []}, {"text": "More importantly, morphemes such as the infinitive marker -en and past participle ge-were removed, because, for example, the verb forms malt, malen, and gemalt, would all be treated as unrelated tokens, again contributing unnecessarily to the problem with sparse data.", "labels": [], "entities": []}, {"text": "The result is that one verb form is used, and to perform accurately, the network must rely on its position in the sentence (either second or sentencefinal), as well as whether the word von occurs to indicate a participial reading rather than infinitival.", "labels": [], "entities": []}, {"text": "All 326 words in the lexicon for the first four exper- iments were given random representations over the vertices of a 100-dimensional hypercube, which resulted in marked improvement oversampling from within the hypercube ().", "labels": [], "entities": []}, {"text": "We trained the network by repeatedly presenting the model with 1000 randomly generated sentences from each experiment (constituting one epoch) and testing every 100 epochs against the held-out test materials for each of the four experiments.", "labels": [], "entities": []}, {"text": "Scenes were provided half of the time to provide an unbiased approximation to linguistic experience.", "labels": [], "entities": []}, {"text": "The network was initialized with weights between -0.01 and 0.01.", "labels": [], "entities": []}, {"text": "The learning rate was initially set to 0.05 and gradually reduced to 0.002 over the course of 15000 epochs.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.969096839427948}]}, {"text": "Ten splits were run on 1.6Ghz PCs and took a little over two weeks to complete.", "labels": [], "entities": []}, {"text": "reports the percentage of targets at the network's output layer that the model correctly matches, both as measured at the adverb and at the end of the sentence.", "labels": [], "entities": []}, {"text": "The model clearly demonstrates the qualitative behavior observed in all four experiments in that it is able to access the information from the encoded scene or stereotypicality and combine it with the incrementally presented sentence to anticipate forthcoming arguments.", "labels": [], "entities": []}], "tableCaptions": []}