{"title": [{"text": "Combining Linguistic Data Views for Phrase-based SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.7715967893600464}]}], "abstractContent": [{"text": "We describe the Spanish-to-English LDV-COMBO system for the Shared Task 2: \"Exploiting Parallel Texts for Statistical Machine Translation\" of the ACL-2005 Workshop on \"Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond\".", "labels": [], "entities": [{"text": "Statistical Machine Translation\" of the ACL-2005 Workshop on \"Building and Using Parallel Texts: Data-Driven Machine Translation", "start_pos": 106, "end_pos": 234, "type": "TASK", "confidence": 0.6791196465492249}]}, {"text": "Our approach explores the possibility of working with alignments at different levels of abstraction , using different degrees of linguistic annotation.", "labels": [], "entities": []}, {"text": "Several phrase-based translation models are built out from these alignments.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.6811406314373016}]}, {"text": "Their combination significa-tively outperforms any of them in isolation.", "labels": [], "entities": []}, {"text": "Moreover, we have built a word-based translation model based on Word-Net which is used for unknown words.", "labels": [], "entities": [{"text": "word-based translation", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.6625913381576538}, {"text": "Word-Net", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9653501510620117}]}], "introductionContent": [{"text": "The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments.", "labels": [], "entities": [{"text": "word and phrase alignments", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.6502573117613792}]}, {"text": "Many other authors have tried to do so.", "labels": [], "entities": []}, {"text": "See),), (), (, and.", "labels": [], "entities": []}, {"text": "Far from full syntactic complexity, we suggest to go back to the simpler alignment methods first described by.", "labels": [], "entities": []}, {"text": "Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks).", "labels": [], "entities": []}, {"text": "In order to avoid confusion so forth we will talk about tokens instead of words as the minimal alignment unit.", "labels": [], "entities": []}, {"text": "Apart from redefining the scope of the alignment unit, we may use different degrees of linguistic annotation.", "labels": [], "entities": []}, {"text": "We introduce the general concept of data view, which is defined as any possible representation of the information contained in a bitext.", "labels": [], "entities": []}, {"text": "We enrich data view tokens with features further than lexical such as PoS, lemma, and chunk label.", "labels": [], "entities": [{"text": "PoS", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8379703760147095}]}, {"text": "As an example of the applicability of data views, suppose the case of the word 'plays' being seen in the training data acting as a verb.", "labels": [], "entities": []}, {"text": "Representing this information as 'plays V BZ ' would allow us to distinguish it from its homograph 'plays N NS ' for 'plays' as a noun.", "labels": [], "entities": []}, {"text": "Ideally, one would wish to have still deeper information, moving through syntax onto semantics, such as word senses.", "labels": [], "entities": []}, {"text": "Therefore, it would be possible to distinguish for instance between two realizations of 'plays' with different meanings: 'he P RP plays V BG guitar N N ' and 'he P RP plays V BG basketball N N '.", "labels": [], "entities": []}, {"text": "Of course, there is a natural trade-off between the use of data views and data sparsity.", "labels": [], "entities": []}, {"text": "Fortunately, we hava data enough so that statistical parameter estimation remains reliable.", "labels": [], "entities": [{"text": "statistical parameter estimation", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.7003719806671143}]}], "datasetContent": [{"text": "We have used the data sets and language model provided by the organization.", "labels": [], "entities": []}, {"text": "No extra training or development data were used in our experiments.", "labels": [], "entities": []}, {"text": "We evaluate results with 3 different metrics: GTM F 1 -measure (e = 1, 2), BLEU score (n = 4) as provided by organizers, and NIST score (n = 5).", "labels": [], "entities": [{"text": "GTM F 1 -measure", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.7796706855297089}, {"text": "BLEU score (n = 4)", "start_pos": 75, "end_pos": 93, "type": "METRIC", "confidence": 0.9390266197068351}, {"text": "NIST score", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.58204585313797}]}, {"text": "presents MT results for the 10 elementary data views devised in Section 2.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.8804509043693542}]}, {"text": "Default parameters are used for \u03bb tm , \u03bb lm , and \u03bb w . No tuning has been performed.", "labels": [], "entities": [{"text": "tuning", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9419487118721008}]}, {"text": "As expected, word-based views obtain significatively higher results than chunk-based.", "labels": [], "entities": []}, {"text": "All data views at the same level of granularity obtain comparable results.", "labels": [], "entities": []}, {"text": "In MT results for different data view combinations are showed.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.962583065032959}]}, {"text": "Merged model weights are set equiprobable, and no phrase-pair score filtering  is performed.", "labels": [], "entities": []}, {"text": "We refer to the W model as our baseline.", "labels": [], "entities": []}, {"text": "In this view, only words are used.", "labels": [], "entities": []}, {"text": "The 5W-MRG and 5W-GPHEX models use a combination of the 5 word-based data views, as in MRG and GPHEX, respectively.", "labels": [], "entities": [{"text": "MRG", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.8414729237556458}, {"text": "GPHEX", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.904431939125061}]}, {"text": "The 5C-MRG and 5C-GPHEX system use a combination of the 5 chunk based data views, as in MRG and GPHEX, respectively.", "labels": [], "entities": [{"text": "MRG", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.8342922925949097}, {"text": "GPHEX", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.9188482761383057}]}, {"text": "The 10-MRG system uses all 10 data views combined as in MRG.", "labels": [], "entities": [{"text": "MRG", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8654685020446777}]}, {"text": "The 10-GPHEX/MRG system uses the 5 word based views combined as in GPHEX, the 5 chunk based views combined as in GPHEX, and then a combination of these two combo-models as in MRG.", "labels": [], "entities": [{"text": "GPHEX", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9769977331161499}, {"text": "GPHEX", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.9740513563156128}, {"text": "MRG", "start_pos": 175, "end_pos": 178, "type": "DATASET", "confidence": 0.9392029643058777}]}, {"text": "It can be seen that results improve by combining several data views.", "labels": [], "entities": []}, {"text": "Furthermore, global phrase extraction (GPHEX) seems to work much finer than local phrase extraction (LPHEX).", "labels": [], "entities": [{"text": "global phrase extraction", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.6216474870840708}, {"text": "phrase extraction", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7648045122623444}]}, {"text": "shows MT results after optimizing \u03bb tm , \u03bb lm , \u03bb w , and the weights for the MRG operation, by means of the Downhill Simplex Method in Multidimensions.", "labels": [], "entities": [{"text": "MT", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.5904704332351685}, {"text": "MRG", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.6158038377761841}]}, {"text": "Observe that tuning the system improves the performance considerably.", "labels": [], "entities": []}, {"text": "The \u03bb w parameter is particularly sensitive to tuning.", "labels": [], "entities": []}, {"text": "Even though the performance of chunk-based models is poor, the best results are obtained by combinining the two levels of abstraction, thus proving that syntactically motivated phrases may help.", "labels": [], "entities": []}, {"text": "10-MRG and 10-GPHEX models achieve a similar performance.", "labels": [], "entities": []}, {"text": "The 10-MRG-best W N system corresponds to the 10-MRG model using WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9775031805038452}]}, {"text": "The 10-MRGsub W N system is this same system at the time of submission.", "labels": [], "entities": []}, {"text": "Results using WordNet, taking into account that the number of unknown 4 words in the development set was very small, are very promising.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.976081132888794}]}], "tableCaptions": [{"text": " Table 1: An example of 2 rich data views: (WPC) word, PoS and IOB chunk label (Cwpc) chunk of word, PoS and chunk label.", "labels": [], "entities": []}, {"text": " Table 2: MT Results for the 10 elementary data views on the", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9232438802719116}]}, {"text": " Table 3: MT Results without tuning, for some data view com-", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9632126092910767}]}, {"text": " Table 4: MT Results for some data view combinations after", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9669403433799744}]}]}