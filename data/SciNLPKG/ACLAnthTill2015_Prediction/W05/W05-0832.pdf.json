{"title": [{"text": "Gaming Fluency: Evaluating the Bounds and Expectations of Segment-based Translation Memory", "labels": [], "entities": [{"text": "Gaming Fluency", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7139989584684372}, {"text": "Segment-based Translation Memory", "start_pos": 58, "end_pos": 90, "type": "TASK", "confidence": 0.871390700340271}]}], "abstractContent": [{"text": "Translation memories provide assistance to human translators in production settings, and are sometimes used as first-pass machine translation in assimilation settings because they produce highly fluent output very rapidly.", "labels": [], "entities": [{"text": "Translation memories", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8602162301540375}, {"text": "first-pass machine translation", "start_pos": 111, "end_pos": 141, "type": "TASK", "confidence": 0.641430139541626}]}, {"text": "In this paper, we describe and evaluate a simple whole-segment translation memory, placing it as anew lower bound in the well-populated space of machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.6927499622106552}]}, {"text": "The result is anew way to gauge how far machine translation has progressed compared to an easily understood baseline system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7693469226360321}]}, {"text": "The evaluation also sheds light on the evaluation metric and gives evidence showing that gaming translation with perfect fluency does not fool bleu the way it fools people.", "labels": [], "entities": [{"text": "gaming translation", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7407514154911041}]}], "introductionContent": [], "datasetContent": [{"text": "We performed several experiments in the course of optimizing this TM, all using the same set of parallel texts for the TM database and multiple-reference translation corpus for evalutation.", "labels": [], "entities": []}, {"text": "The parallel texts for the TM come from several Chinese-English parallel corpora, all available from the Linguistic Data Consortium (LDC).", "labels": [], "entities": [{"text": "TM", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.91718590259552}, {"text": "Linguistic Data Consortium (LDC)", "start_pos": 105, "end_pos": 137, "type": "DATASET", "confidence": 0.8281699319680532}]}, {"text": "These corpora are described in Table 2.", "labels": [], "entities": []}, {"text": "We discarded any sentence pairs that seemed trivially incomplete, corrupt, or otherwise invalid.", "labels": [], "entities": []}, {"text": "In the case of LDC2002E18, in which sentences were aligned automatically and confidence scores produced for each alignment, we dropped all pairs with scores above 9, indicating poor alignment.", "labels": [], "entities": []}, {"text": "No duplication checks were performed.", "labels": [], "entities": []}, {"text": "Our final corpus contained approximately 7 million sentence pairs and contained 3.2 GB of UTF-8 data.", "labels": [], "entities": []}, {"text": "Our evaluation corpus and reference corpus come from the data used in the).", "labels": [], "entities": []}, {"text": "The evaluation corpus is 878 segments of Chinese source text.", "labels": [], "entities": []}, {"text": "The reference corpus consists of four independent human-generated reference English translations of the evaluation corpus.", "labels": [], "entities": []}, {"text": "All performance measurements were made using a fast reimplementation of NIST's bleu.", "labels": [], "entities": [{"text": "NIST's bleu", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.9458433190981547}]}, {"text": "bleu exhibits a high correlation with human judgments of translation quality when measuring on large sections of text ().", "labels": [], "entities": []}, {"text": "Furthermore, using bleu allowed us to compare our performance to that of other systems that have been tested with the same evaluation data.", "labels": [], "entities": []}, {"text": "The fourth experiment was to determine whether we could improve upon tf-idf by applying automated MT metrics to pick the best sentence from the top n translation pairs returned by the IR engine.", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9523004293441772}]}, {"text": "We compared a variety of metrics from MT evaluation literatures.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.9191522002220154}]}, {"text": "All of these were run on the tokens in the source language side of the IR result, comparing against the single pseudo-reference, the original source language segment.", "labels": [], "entities": [{"text": "IR result", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.7830761969089508}]}, {"text": "While many of these metrics aren't designed to perform well with one reference, they stand in as good approximate string matching algorithms.", "labels": [], "entities": [{"text": "approximate string matching", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.6813147167364756}]}, {"text": "The score that the IR engine associates with each segment is retained and marked as tf-idf in this experiment.", "labels": [], "entities": []}, {"text": "Naturally, bleu () was the first choice metric, as it was well-matched to the target language evaluation function.", "labels": [], "entities": [{"text": "bleu", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9782010316848755}]}, {"text": "rouge was a reimplementation of ROUGE-L from ().", "labels": [], "entities": [{"text": "rouge", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6105074286460876}, {"text": "ROUGE-L", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9822580218315125}]}, {"text": "It computes an F-measure from precision and recall that are both based on the longest common subsequence of the hypothesis and reference strings.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9978285431861877}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9984930753707886}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9986187219619751}]}, {"text": "wer-g is a variation on traditional word error rate that was found to correlate very well with human judgments, and per is the traditional position-independent error rate that was also shown to correlate well with human judgments ().", "labels": [], "entities": [{"text": "word error rate", "start_pos": 36, "end_pos": 51, "type": "METRIC", "confidence": 0.6362664500872294}]}, {"text": "Finally, a random metric was added to show the bleu value one could achieve by selecting from the top n strictly by chance.", "labels": [], "entities": []}, {"text": "After the individual metrics are calculated for these segments, a uniform-weight log-linear combination of the metrics is calculated and used to produce anew rank ordering under the belief that the different metrics will make predictions that are constructive in aggregate.", "labels": [], "entities": []}, {"text": "shows the maximum possible bleu score that can an oracle can achieve by selecting the best English-side segment from the parallel text.", "labels": [], "entities": []}, {"text": "The upper bound achieved here is a bleu score of 17.7, and this number is higher than the best performing system in the corresponding NIST evaluation.", "labels": [], "entities": [{"text": "bleu score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9641029536724091}, {"text": "NIST evaluation", "start_pos": 134, "end_pos": 149, "type": "DATASET", "confidence": 0.9224372804164886}]}, {"text": "Note the log-linear growth in the resulting   bleu score of the TM with increasing database size.", "labels": [], "entities": []}, {"text": "As the database is increased by a factor often, the TM gains approximately 5 points of bleu.", "labels": [], "entities": []}, {"text": "While this trend has a natural limit at 20 orders of magnitude, it is unlikely that this amount of text, let alone parallel text, will be a indexed in the foreseeable future.", "labels": [], "entities": []}, {"text": "This rate is more useful in interpolation, giving an idea of how much could be gained from adding to corpora that are smaller than 7.5 million segments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Sentence-aligned parallel corpora used for the creation of the translation memory. The  \"pairs\" column gives the number of translation pairs available after trivial pruning.", "labels": [], "entities": []}]}