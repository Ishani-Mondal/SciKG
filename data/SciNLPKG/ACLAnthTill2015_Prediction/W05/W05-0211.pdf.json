{"title": [{"text": "Evaluating State-of-the-Art Treebank-style Parsers for Coh-Metrix and Other Learning Technology Environments", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper evaluates a series of freely available, state-of-the-art parsers on a standard benchmark as well as with respect to a set of data relevant for measuring text cohesion.", "labels": [], "entities": []}, {"text": "We outline advantages and disadvantages of existing technologies and make recommendations.", "labels": [], "entities": []}, {"text": "Our performance report uses traditional measures based on a gold standard as well as novel dimensions for parsing evaluation.", "labels": [], "entities": [{"text": "parsing evaluation", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.8797383308410645}]}, {"text": "To our knowledge this is the first attempt to evaluate parsers accross genres and grade levels for the implementation in learning technology.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of syntactic parsing is valuable to most natural language understanding applications, e.g., anaphora resolution, machine translation, or question answering.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.8636742532253265}, {"text": "natural language understanding", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.7016522487004598}, {"text": "anaphora resolution", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7238303571939468}, {"text": "machine translation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7839719355106354}, {"text": "question answering", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.91260826587677}]}, {"text": "Syntactic parsing in its most general definition maybe viewed as discovering the underlying syntactic structure of a sentence.", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9248173832893372}]}, {"text": "The specificities include the types of elements and relations that are retrieved by the parsing process and the way in which they are represented.", "labels": [], "entities": []}, {"text": "For example, Treebank-style parsers retrieve a bracketed form that encodes a hierarchical organization (tree) of smaller elements (called phrases), while GrammaticalRelations(GR)-style parsers explicitly output relations together with elements involved in the relation (subj).", "labels": [], "entities": []}, {"text": "The present paper presents an evaluation of parsers for the Coh-Metrix project () at the Institute for Intelligent Systems of the University of Memphis.", "labels": [], "entities": []}, {"text": "Coh-Metrix is a text-processing tool that provides new methods of automatically assessing text cohesion, readability, and difficulty.", "labels": [], "entities": []}, {"text": "In its present form, v1.1, few cohesion measures are based on syntactic information, but its next incarnation, v2.0, will depend more heavily on hierarchical syntactic information.", "labels": [], "entities": []}, {"text": "We are developing these measures.", "labels": [], "entities": []}, {"text": "Thus, our current goal is to provide the most reliable parser output available for them, while still being able to process larger texts in real time.", "labels": [], "entities": []}, {"text": "The usual trade-off between accuracy and speed has to betaken into account.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9994865655899048}, {"text": "speed", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9952031373977661}]}, {"text": "In the first part of the evaluation, we adopt a constituent-based approach for evaluation, as the output parses are all derived in one way or another from the same data and generate similar, bracketed output.", "labels": [], "entities": []}, {"text": "The major goal is to consistently evaluate the freely available state-ofthe-art parsers on a standard data set and across genre on corpora typical for learning technology environments.", "labels": [], "entities": []}, {"text": "We report parsers' competitiveness along an array of dimensions including performance, robustness, tagging facility, stability, and length of input they can handle.", "labels": [], "entities": []}, {"text": "Next, we briefly address particular types of misparses and mistags in their relation to measures planned for Coh-Metrix 2.0 and assumed to be typical for learning technology applications.", "labels": [], "entities": []}, {"text": "Coh-Metrix 2.0 measures that centrally rely on good parses include: causal and intentional cohesion, for which the main verb and its subject must be identified; anaphora resolution, for which the syntactic relations of pronoun and referent must be identified; temporal cohesion, for which the main verb and its tense/aspect must be identified.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.6967907249927521}]}, {"text": "These measures require complex algorithms operating on the cleanest possible sentence parse, as a faulty parse will lead to a cascading error effect.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation methods can be broadly divided into non-corpus-and corpus-based methods with the latter subdivided into unannotated and annotated corpus-based methods).", "labels": [], "entities": []}, {"text": "The non-corpus method simply lists linguistic constructions covered by the parser/grammar.", "labels": [], "entities": []}, {"text": "It is well-suited for handbuilt grammars because during the construction phase the covered cases can be recorded.", "labels": [], "entities": []}, {"text": "However, it has problems with capturing complexities occuring from the interaction of covered cases.", "labels": [], "entities": []}, {"text": "The most widely used corpus-based evaluation methods are: (1) the constituentbased (phrase structure) method, and (2) the dependency/GR-based method.", "labels": [], "entities": []}, {"text": "The former has its roots in the Grammar Evaluation Interest Group (GEIG) scheme) developed to compare parsers with different underlying grammatical formalisms.", "labels": [], "entities": []}, {"text": "It promoted the use of phrase-structure bracketed information and defined Precision, Recall, and Crossing Brackets measures.", "labels": [], "entities": [{"text": "Precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9953134059906006}, {"text": "Recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.8626804351806641}, {"text": "Crossing Brackets measures", "start_pos": 97, "end_pos": 123, "type": "METRIC", "confidence": 0.8223216533660889}]}, {"text": "The GEIG measures were extended later to constituent information (bracketing information plus label) and have since become the standard for reporting automated syntactic parsing performance.", "labels": [], "entities": [{"text": "GEIG", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.5645906925201416}, {"text": "reporting automated syntactic parsing", "start_pos": 140, "end_pos": 177, "type": "TASK", "confidence": 0.6509256586432457}]}, {"text": "Among the advantages of constituent-based evaluation are generality (less parser specificity) and fine grain size of the measures.", "labels": [], "entities": [{"text": "fine grain size", "start_pos": 98, "end_pos": 113, "type": "METRIC", "confidence": 0.9097566803296407}]}, {"text": "On the other hand, the measures of the method are weaker than exact sentence measures (full identity), and it is not clear if they properly measure how well a parser identifies the true structure of a sentence.", "labels": [], "entities": []}, {"text": "Many phrase boundary mismatches spawn from differences between parsers/grammars and corpus annotation schemes.", "labels": [], "entities": []}, {"text": "Usually, treebanks are constructed with respect to informal guidelines.", "labels": [], "entities": []}, {"text": "Annotators often interpret them differently leading to a large number of different structural configurations.", "labels": [], "entities": []}, {"text": "There are two major approaches to evaluate parsers using the constituent-based method.", "labels": [], "entities": []}, {"text": "On the one hand, there is the expert-only approach in which an expert looks at the output of a parser, counts errors, and reports different measures.", "labels": [], "entities": []}, {"text": "We use a variant of this approach for the directed parser evaluation (see next section).", "labels": [], "entities": []}, {"text": "Using a gold standard, on the other hand, is a method that can be automated to a higher degree.", "labels": [], "entities": []}, {"text": "It replaces the counting part of the former method with a software system that compares the output of the parser to the gold standard, highly accurate data, manually parsed \u2212 or automatically parsed and manually corrected \u2212 by human experts.", "labels": [], "entities": []}, {"text": "The latter approach is more useful for scaling up evaluations to large collections of data while the expert-only approach is more flexible, allowing for evaluation of parsers from new perspectives and with a view to special applications, e.g., in learning technology environments.", "labels": [], "entities": []}, {"text": "In the first part of this work we use the gold standard approach for parser evaluation.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.9344702661037445}]}, {"text": "The evaluation is done from two different points of view.", "labels": [], "entities": []}, {"text": "First, we offer a uniform evaluation for the parsers on section 23 from the Wall Street Journal (WSJ) section of PTB, the community norm for reporting parser performance.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) section of PTB", "start_pos": 76, "end_pos": 116, "type": "DATASET", "confidence": 0.9458439085218642}]}, {"text": "The goal of this first evaluation is to offer a good estimation of the parsers when evaluated in identical environments (same configuration parameters for the evaluator software).", "labels": [], "entities": []}, {"text": "We also observe the following features which are extremely important for using the parsers in large-scale text processing and to embed them as components in larger systems.", "labels": [], "entities": []}, {"text": "Self-tagging: whether or not the parser does tagging itself.", "labels": [], "entities": []}, {"text": "It is advantageous to take in raw text since it eliminates the need for extra modules.", "labels": [], "entities": []}, {"text": "Performance: if the performance is in the mid and upper 80th percentiles.", "labels": [], "entities": []}, {"text": "Long sentences: the ability of the parser to handle sentences longer than 40 words.", "labels": [], "entities": []}, {"text": "Robustness: relates to the property of a parser to handle any type of input sentence and return a reasonable output for it and not an empty line or some other useless output.", "labels": [], "entities": []}, {"text": "Second, we evaluate the parsers on narrative and expository texts to study their performance across the two genres.", "labels": [], "entities": []}, {"text": "This second evaluation step will provide additional important results for learning technology projects.", "labels": [], "entities": []}, {"text": "We use evalb (http://nlp.cs.nyu.edu/evalb/) to evaluate the bracketing performance of the output of a parser against a gold standard.", "labels": [], "entities": []}, {"text": "The software evaluator reports numerous measures of which we only report the two most important: labelled precision (LR), labelled recall (LR) which are discussed in more detail below.", "labels": [], "entities": [{"text": "labelled precision (LR)", "start_pos": 97, "end_pos": 120, "type": "METRIC", "confidence": 0.8347543478012085}, {"text": "recall (LR)", "start_pos": 131, "end_pos": 142, "type": "METRIC", "confidence": 0.9534759372472763}]}, {"text": "For the third step of this evaluation we looked for specific problems that will affect Coh-Metrix 2.0, and presumably learning technology applications in general, with a view to amending them by postprocessing the parser output.", "labels": [], "entities": []}, {"text": "The following four classes of problems in a sentence's parse were distinguished: None: The parse is generally correct, unambiguous, poses no problem for Coh-Metrix 2.0.", "labels": [], "entities": []}, {"text": "One: There was one minor problem, e.g., a mislabeled terminal or a wrong scope of an adverbial or prepositional phrase (wrong attachment site) that did not affect the overall parse of the sentence, which is therefore still usable for CohMetrix 2.0 measures.", "labels": [], "entities": []}, {"text": "Two: There were two or three problems of the type one, or a problem with the tree structure that affected the overall parse of the sentence, but not in a fatal manner, e.g., a wrong phrase boundary, or a mislabelled higher constituent.", "labels": [], "entities": []}, {"text": "Three: There were two or more problems of the type two, or two or more of the type one as well as one or more of the type two, or another fundamental problem that made the parse of the sentence completely useless, unintelligible, e.g., an omitted sentence or a sentence split into two, because a sentence boundary was misidentified.", "labels": [], "entities": []}, {"text": "This section reports the results of expert rating of texts for specific problems (see Section 1.3).", "labels": [], "entities": []}, {"text": "The best results are produced by CP with an average of 88.69% output useable for Coh-Metrix 2.0).", "labels": [], "entities": []}, {"text": "CP also produces good output     most consistently at a standard deviation over the seven texts of 8.86%.", "labels": [], "entities": [{"text": "CP", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.6023150682449341}]}, {"text": "The other three candidates are clearly trailing behing, namely by between 5% (SP) and 11% (AP).", "labels": [], "entities": [{"text": "AP", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9982761144638062}]}, {"text": "The distribution of severe problems is comparable for all parsers.", "labels": [], "entities": []}, {"text": "As expected, longer sentences are more problematic for all parsers, as can be seen in Table 7.", "labels": [], "entities": []}, {"text": "No significant trends in performance differences with respect to genre difference, narrative (Orlando, Moving, Betty03) vs. expository texts (Heat, Plants, Barron17, Olga91), were detected (cf. also speed results in).", "labels": [], "entities": [{"text": "Barron17", "start_pos": 156, "end_pos": 164, "type": "DATASET", "confidence": 0.9560627937316895}, {"text": "Olga91", "start_pos": 166, "end_pos": 172, "type": "DATASET", "confidence": 0.6710875034332275}, {"text": "speed", "start_pos": 199, "end_pos": 204, "type": "METRIC", "confidence": 0.9818078875541687}]}, {"text": "But we assume that the difference in average sentence length obscures any genre differences in our small sample.", "labels": [], "entities": []}, {"text": "The most common non-fatal problems (type one) involved the well-documented adjunct attachment site issue, in particular for prepositional phrases),,) as well as adjectival phrases) . Similar misattachment issues for adjuncts are encountered with adverbial phrases, but they were rare in our corpus.", "labels": [], "entities": []}, {"text": "Another common problem are deverbal nouns and denominal verbs, as well as -ing/VBG forms.", "labels": [], "entities": []}, {"text": "They share surface forms leading to ambiguous part of speech assignments.", "labels": [], "entities": []}, {"text": "For many Coh-Metrix 2.0 measures, most obviously temporal cohesion, it is necessary to be able to distinguish gerunds from gerundives and deverbal adjectives and deverbal nouns.", "labels": [], "entities": []}, {"text": "Problems with NP misidentification are particularly detrimental in view of the important role of NPs in Coh-Metrix 2.0 measures.", "labels": [], "entities": [{"text": "NP misidentification", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.7125503718852997}]}, {"text": "This pertains in particular to the mistagging/misparsing of complex NPs and the coordination of NPs.", "labels": [], "entities": [{"text": "mistagging/misparsing of complex NPs", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.8344205816586813}]}, {"text": "Parses with fatal problems are expected to produce useless results for algorithms operating with them.", "labels": [], "entities": []}, {"text": "Wrong coordination is another notorious problem of parsers (cf.,).", "labels": [], "entities": []}, {"text": "In our corpus we found 33 instances of miscoordination, of which 23 involved NPs.", "labels": [], "entities": []}, {"text": "Postprocessing approaches that address these issues are currently under investigation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of Parsers.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9876367449760437}]}, {"text": " Table 2: Performance of parsers on the narrative and expository text (average against CP-based  and SP-based gold standard).", "labels": [], "entities": []}, {"text": " Table 4: Average Performance of Parsers.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9932156205177307}]}, {"text": " Table 5: Parser Speed in Seconds.", "labels": [], "entities": [{"text": "Parser Speed", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.8900826275348663}]}, {"text": " Table 6: Average Performance of Parsers over  all Texts (Directed Evaluation).  Ave. (%) S.D. (%)  AP  77.31  15.00  CP  88.69  8.86  CBP  79.82  18.94  SP  83.43  11.42", "labels": [], "entities": [{"text": "AP  77.31  15.00  CP  88.69  8.86  CBP  79.82  18.94  SP  83.43  11.42", "start_pos": 100, "end_pos": 170, "type": "DATASET", "confidence": 0.8231223424275717}]}, {"text": " Table 7: Correlation of Average Performance  per Text for all Parsers and Average Sentence  Length (Directed Evaluation).  Text  perf. (%) length (#words)  Heat  92.31  7.54  Plants  90.76  9.96  Orlando  93.46  6.86  Moving  90.91  13.12  Barron17  76.92  22.15  Betty03  71.43  18.21  Olga91  60.42  25.92", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9780494570732117}, {"text": "Heat  92.31  7.54  Plants  90.76  9.96  Orlando  93.46  6.86  Moving  90.91  13.12  Barron17  76.92  22.15  Betty03  71.43  18.21  Olga91  60.42  25.92", "start_pos": 157, "end_pos": 308, "type": "DATASET", "confidence": 0.866104645388467}]}, {"text": " Table 8: Specific Problems by Parser.  PP ADV cNP &X  AP  13  10  8  9  CP  15  1  2  7  CBP  10  0  0  13  SP  22  6  3  4  Sum  60  17  13  33", "labels": [], "entities": [{"text": "PP ADV cNP &X  AP  13", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.8376970546586173}, {"text": "Sum  60", "start_pos": 126, "end_pos": 133, "type": "TASK", "confidence": 0.8902059197425842}]}]}