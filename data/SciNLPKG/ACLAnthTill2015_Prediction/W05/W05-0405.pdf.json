{"title": [], "abstractContent": [{"text": "In this paper we examine topic segmen-tation of narrative documents, which are characterized by long passages of text with few headings.", "labels": [], "entities": []}, {"text": "We first present results suggesting that previous topic segmenta-tion approaches are not appropriate for narrative text.", "labels": [], "entities": []}, {"text": "We then present a feature-based method that combines features from diverse sources as well as learned features.", "labels": [], "entities": []}, {"text": "Applied to narrative books and encyclopedia articles, our method shows results that are significantly better than previous seg-mentation approaches.", "labels": [], "entities": []}, {"text": "An analysis of individual features is also provided and the benefit of generalization using outside resources is shown.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many long text documents, such as magazine articles, narrative books and news articles contain few section headings.", "labels": [], "entities": []}, {"text": "The number of books in narrative style that are available in digital form is rapidly increasing through projects such as Project Gutenberg and the Million Book Project at Carnegie Mellon University.", "labels": [], "entities": []}, {"text": "Access to these collections is becoming easier with directories such as the Online Books Page at the University of Pennsylvania.", "labels": [], "entities": [{"text": "Online Books Page at the University of Pennsylvania", "start_pos": 76, "end_pos": 127, "type": "DATASET", "confidence": 0.9112097918987274}]}, {"text": "As text analysis and retrieval moves from retrieval of documents to retrieval of document passages, the ability to segment documents into smaller, coherent regions enables more precise retrieval of meaningful portions of text and improved question answering.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.7039889544248581}, {"text": "question answering", "start_pos": 239, "end_pos": 257, "type": "TASK", "confidence": 0.8624100387096405}]}, {"text": "Segmentation also has applications in other areas of information access, including document navigation), anaphora and ellipsis resolution, and text summarization.", "labels": [], "entities": [{"text": "document navigation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7498623728752136}, {"text": "anaphora and ellipsis resolution", "start_pos": 105, "end_pos": 137, "type": "TASK", "confidence": 0.6021376475691795}, {"text": "text summarization", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.7611759305000305}]}, {"text": "Research projects on text segmentation have focused on broadcast news stories), expository texts and synthetic texts).", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7386244833469391}]}, {"text": "Broadcast news stories contain cues that are indicative of anew story, such as \"coming up\", or phrases that introduce a reporter, which are not applicable to written text.", "labels": [], "entities": []}, {"text": "In expository texts and synthetic texts, there is repetition of terms within a topical segment, so that the similarity of \"blocks\" of text is a useful indicator of topic change.", "labels": [], "entities": []}, {"text": "Synthetic texts are created by concatenating stories, and exhibit stronger topic changes than the subtopic changes within a document; consequently, algorithms based on the similarity of text blocks work well on these texts.", "labels": [], "entities": []}, {"text": "In contrast to these earlier works, we present a method for segmenting narrative documents.", "labels": [], "entities": [{"text": "segmenting narrative documents", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.8493406971295675}]}, {"text": "In this domain there is little repetition of words and the segmentation cues are weaker than in broadcast news stories, resulting in poor performance from previous methods.", "labels": [], "entities": []}, {"text": "We present a feature-based approach, where the features are more strongly engineered using linguistic knowledge than in earlier approaches.", "labels": [], "entities": []}, {"text": "The key to most feature-based approaches, particularly in NLP tasks where there is abroad range of possible feature sources, is identifying appropriate features.", "labels": [], "entities": []}, {"text": "Selecting features in this domain presents a number of interesting challenges.", "labels": [], "entities": []}, {"text": "First, features used in previous methods are not sufficient for solving this problem.", "labels": [], "entities": []}, {"text": "We explore a number of different sources of information for extracting features, many previously unused.", "labels": [], "entities": []}, {"text": "Second, the sparse nature of text and the high cost of obtaining training data requires generalization using outside resources.", "labels": [], "entities": [{"text": "generalization", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.9633060693740845}]}, {"text": "Finally, we incorporate features from non-traditional resources such as lexical chains where features must be extracted from the underlying knowledge representation.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we examine a number of narrative segmentation tasks with different segmentation methods.", "labels": [], "entities": [{"text": "narrative segmentation tasks", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.8030003706614176}]}, {"text": "The only data used during development was the first two thirds from Biohazard (exp1 and exp2).", "labels": [], "entities": [{"text": "Biohazard", "start_pos": 68, "end_pos": 77, "type": "DATASET", "confidence": 0.9016956686973572}]}, {"text": "All other data sets were only examined after the algorithm was developed and were used for testing purposes.", "labels": [], "entities": []}, {"text": "Unless stated otherwise, results for the feature based method are using the SVM classifier.", "labels": [], "entities": []}, {"text": "1  We use three segmentation evaluation metrics that have been recently developed to account for \"close but not exact\" placement of hypothesized boundaries: word error probability, sentence error probability, and WindowDiff.", "labels": [], "entities": [{"text": "sentence error probability", "start_pos": 181, "end_pos": 207, "type": "METRIC", "confidence": 0.6261971394220988}]}, {"text": "Word error probability () estimates the probability that a randomly chosen pair of words k words apart is incorrectly classified, i.e. a false positive or false negative of being in the same segment.", "labels": [], "entities": [{"text": "Word error probability", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.6315451562404633}]}, {"text": "In contrast to the standard classification measures of precision and recall, which would consider a \"close\" hypothesized boundary (e.g., off by one sentence) to be incorrect, word error probability gently penalizes \"close\" hypothesized boundaries.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.999180257320404}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9955442547798157}]}, {"text": "We also compute the sentence error probability, which estimates the probability that a randomly chosen pair of sentences s sentences apart is incorrectly classified.", "labels": [], "entities": [{"text": "sentence error probability", "start_pos": 20, "end_pos": 46, "type": "METRIC", "confidence": 0.6819853683312734}]}, {"text": "k and s are chosen to behalf the average length of a section in the test data.) uses a sliding window over the data and measures the difference between the number of hypothesized boundaries and the actual boundaries within the window.", "labels": [], "entities": []}, {"text": "This metric handles several criticisms of the word error probability metric.", "labels": [], "entities": []}, {"text": "shows the results of the SVM-segmenter on Biohazard and Demon in the Freezer.", "labels": [], "entities": [{"text": "SVM-segmenter", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8380830883979797}]}, {"text": "A baseline performance for segmentation algorithms is whether the algorithm performs better than naive segmenting algorithms: choose no boundaries, choose all boundaries and choose randomly.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.9699580073356628}]}, {"text": "Choosing all boundaries results in word and sentence error probabilities of approximately 55%.", "labels": [], "entities": [{"text": "sentence error probabilities", "start_pos": 44, "end_pos": 72, "type": "METRIC", "confidence": 0.6252435942490896}]}, {"text": "Choosing no boundaries is about 45%.", "labels": [], "entities": []}, {"text": "also shows the results for random placement of the correct number of segments.", "labels": [], "entities": []}, {"text": "Both random boundaries at sentence locations and random boundaries at paragraph locations are shown (values shown are the averages of 500 random runs).", "labels": [], "entities": []}, {"text": "Similar results were obtained for random segmentation of the Demon data.", "labels": [], "entities": [{"text": "Demon data", "start_pos": 61, "end_pos": 71, "type": "DATASET", "confidence": 0.6824591308832169}]}, {"text": "For Biohazard the holdout set was not used during development.", "labels": [], "entities": [{"text": "Biohazard", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.6363221406936646}]}, {"text": "When trained on either of the development thirds of the text (i.e., exp1 or exp2) and tested on the test set, a substantial improvement is seen over random.", "labels": [], "entities": []}, {"text": "3-fold cross validation was done by training on two-thirds of the data and testing on the other third.", "labels": [], "entities": []}, {"text": "Recalling from that both PLSA and TextTiling result in performance similar to random even when given the correct number of segments, we note that all of the single train/test splits performed better than any of the naive algorithms and previous methods examined.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Previous approaches evaluated on narrative  data from Biohazard", "labels": [], "entities": [{"text": "Biohazard", "start_pos": 64, "end_pos": 73, "type": "DATASET", "confidence": 0.5343165993690491}]}, {"text": " Table 2: Experiments with Biohazard", "labels": [], "entities": []}, {"text": " Table 3: Performance on Groliers articles", "labels": [], "entities": [{"text": "Groliers", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8918446898460388}]}, {"text": " Table 4: Ave. human performance (Hearst, 1994)", "labels": [], "entities": [{"text": "Ave. human performance (Hearst, 1994)", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.627621226840549}]}, {"text": " Table 5: Feature occurrences at boundary and non- boundary locations", "labels": [], "entities": []}]}