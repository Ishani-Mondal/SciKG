{"title": [], "abstractContent": [{"text": "Although state-of-the-art parsers for natural language are lexicalized, it was recently shown that an accurate unlexical-ized parser for the Penn tree-bank can be simply read off a manually refined tree-bank.", "labels": [], "entities": [{"text": "Penn tree-bank", "start_pos": 141, "end_pos": 155, "type": "DATASET", "confidence": 0.9800548553466797}]}, {"text": "While lexicalized parsers often suffer from sparse data, manual markup is costly and largely based on individual linguistic intuition.", "labels": [], "entities": []}, {"text": "Thus, across domains, languages, and tree-bank annotations, a fundamental question arises: Is it possible to automatically induce an accurate parser from a tree-bank without resorting to full lexicalization?", "labels": [], "entities": []}, {"text": "In this paper, we show how to induce head-driven probabilistic parsers with latent heads from a tree-bank.", "labels": [], "entities": []}, {"text": "Our automatically trained parser has a performance of 85.7% (LP/LR F 1), which is already better than that of early lexicalized ones.", "labels": [], "entities": [{"text": "LP/LR F 1)", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9507454534371694}]}], "introductionContent": [{"text": "State-of-the-art statistical parsers for natural language are based on probabilistic grammars acquired from transformed tree-banks.", "labels": [], "entities": []}, {"text": "The method of transforming the tree-bank is of major influence on the accuracy and coverage of the statistical parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9992917776107788}, {"text": "coverage", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9747700691223145}]}, {"text": "The most important tree-bank transformation in the literature is lexicalization: Each node in a tree is labeled with its headword, the most important word of the constituent under the node,,,,, etc.).", "labels": [], "entities": []}, {"text": "It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carryover across different tree-banks for other languages, annotations or domains (.", "labels": [], "entities": []}, {"text": "Second, full lexicalization leads to a serious sparse-data problem, which can only be solved by sophisticated smoothing and pruning techniques.", "labels": [], "entities": []}, {"text": "Recently, showed that a carefully performed linguistic mark-up of the treebank leads to almost the same performance results as lexicalization.", "labels": [], "entities": []}, {"text": "This result is attractive since unlexicalized grammars are easy to estimate, easy to parse with, and time-and space-efficient: do not smooth grammar-rule probabilities, except unknown-word probabilities, and they do not prune since they are able to determine the most probable parse of each full parse forest.", "labels": [], "entities": []}, {"text": "Both facts are noteworthy in the context of statistical parsing with a tree-bank grammar.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.79084712266922}]}, {"text": "A drawback of their method is, however, that manual linguistic mark-up is not based on abstract rules but rather on individual linguistic intuition, which makes it difficult to repeat their experiment and to generalize their findings to languages other than English.", "labels": [], "entities": []}, {"text": "Is it possible to automatically acquire a more refined probabilistic grammar from a given tree-bank without resorting to full lexicalization?", "labels": [], "entities": []}, {"text": "We present a novel method that is able to induce a parser that is located between two extremes: a fully-lexicalized parser on one side versus an accurate unlexicalized parser based on a manually refined tree-bank on the other side.", "labels": [], "entities": []}, {"text": "In short, our method is based on the same linguistic principles of headedness as other methods: We do believe that lexical information represents an important knowledge source.", "labels": [], "entities": []}, {"text": "To circumvent data sparseness resulting from full lexicalization with words, we simply follow the suggestion of various advanced linguistic theories, e.g., where more complex categories based on feature combinations represent the lexical effect.", "labels": [], "entities": []}, {"text": "We complement this by a learning paradigm: lexical entries carry latent information to be used as head information, and this head information is induced from the tree-bank.", "labels": [], "entities": []}, {"text": "In this paper, we study two different latent-head models, as well as two different estimation methods: The first model is built around completely hidden heads, whereas the second one uses relatively fine-grained combinations of Part-Of-Speech (POS) tags with hidden extra-information; The first estimation method selects a head-driven probabilistic context-free grammar (PCFG) by exploiting latenthead distributions for each node in the tree-bank, whereas the second one is more traditional, reading off the grammar from the tree-bank annotated with the most probable latent heads only.", "labels": [], "entities": []}, {"text": "In other words, both models and estimation methods differ in the degree of information incorporated into them as prior knowledge.", "labels": [], "entities": [{"text": "estimation", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.9548367857933044}]}, {"text": "In general, it can be expected that the better (sharper or richer, or more accurate) the information is, the better the induced grammar will be.", "labels": [], "entities": []}, {"text": "Our empirical results, however, are surprising: First, estimation with latent-head distributions outperforms estimation with most-probable-head annotation.", "labels": [], "entities": []}, {"text": "Second, modeling with completely hidden heads is almost as good as modeling with latent heads based on POS tags, and moreover, results in much smaller grammars.", "labels": [], "entities": []}, {"text": "We emphasize that our task is to automatically induce a more refined grammar based on a few linguistic principles.", "labels": [], "entities": []}, {"text": "With automatic refinement it is harder to guarantee improved performance than with manual refinements or with refinements based on direct lexicalization,,, etc.).", "labels": [], "entities": []}, {"text": "If, however, our refinement provides improved performance then it has a clear advantage: it is automatically induced, which suggests that it is applicable across different domains, languages and tree-bank annotations.", "labels": [], "entities": []}, {"text": "Applying our method to the benchmark Penn treebank Wall-Street Journal, we obtain a refined probabilistic grammar that significantly improves over the original tree-bank grammar and that shows performance that is on par with early work on lexicalized probabilistic grammars.", "labels": [], "entities": [{"text": "Penn treebank Wall-Street Journal", "start_pos": 37, "end_pos": 70, "type": "DATASET", "confidence": 0.9607657790184021}]}, {"text": "This is a promising result given the hard task of automatic induction of improved probabilistic grammars.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents empirical results across our models and estimation methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Parsing results in LP/LR F 1 (the baseline is L = 1)", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.8399301171302795}, {"text": "LP/LR F 1", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.70308358669281}]}, {"text": " Table 3: Comparison with other parsers (sentences of length \u2264 40)", "labels": [], "entities": []}]}