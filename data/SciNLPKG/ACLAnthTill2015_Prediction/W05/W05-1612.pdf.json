{"title": [], "abstractContent": [{"text": "Sentence fusion is a text-to-text (revision-like) generation task which takes related sentences as input and merges these into a single output sentence.", "labels": [], "entities": [{"text": "Sentence fusion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9141546487808228}, {"text": "text-to-text (revision-like) generation task", "start_pos": 21, "end_pos": 65, "type": "TASK", "confidence": 0.6799002289772034}]}, {"text": "In this paper we describe our ongoing work on developing a sentence fusion module for Dutch.", "labels": [], "entities": [{"text": "sentence fusion module for Dutch", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.7771632552146912}]}, {"text": "We propose a generalized version of alignment which not only indicates which words and phrases should be aligned but also labels these in terms of a small set of primitive semantic relations, indicating how words and phrases from the two input sentences relate to each other.", "labels": [], "entities": []}, {"text": "It is shown that human label-ers can perform this task with a high agreement (F-score of .95).", "labels": [], "entities": [{"text": "agreement", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9952278137207031}, {"text": "F-score", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9941135048866272}]}, {"text": "We then describe and evaluate our adaptation of an existing automatic alignment algorithm , and use the resulting alignments, plus the semantic labels, in a generalized fusion and generation algorithm.", "labels": [], "entities": []}, {"text": "A small-scale evaluation study reveals that most of the resulting sentences are adequate to good.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditionally, Natural Language Generation (NLG) is defined as the automatic production of \"meaningful texts in (...) human language from some underlying non-linguistic representation of information\".", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.8325027823448181}]}, {"text": "Recently, there is an increased interest in NLG applications that produce meaningful text from meaningful text rather than from abstract meaning representations.", "labels": [], "entities": []}, {"text": "Such applications are sometimes referred to as text-to-text generation applications (e.g.,,,), and maybe likened to earlier revision-based generation strategies, e.g. [Callaway and.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.7169249355792999}, {"text": "revision-based generation", "start_pos": 124, "end_pos": 149, "type": "TASK", "confidence": 0.705979660153389}, {"text": "Callaway", "start_pos": 168, "end_pos": 176, "type": "DATASET", "confidence": 0.9623100757598877}]}, {"text": "Text-to-text generation is often motivated from practical applications such as summarization, sentence simplification, and sentence compression.", "labels": [], "entities": [{"text": "Text-to-text generation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7093202769756317}, {"text": "summarization", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.9827643036842346}, {"text": "sentence simplification", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.7524081766605377}, {"text": "sentence compression", "start_pos": 123, "end_pos": 143, "type": "TASK", "confidence": 0.7874916195869446}]}, {"text": "One reason for the interest in such generation systems is the possibility to automatically learn text-to-text generation strategies from corpora of parallel text.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 97, "end_pos": 120, "type": "TASK", "confidence": 0.7465565502643585}]}, {"text": "In this paper, we take a closer look at sentence fusion[, one of the interesting variants in text-to-text generation.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7594158947467804}, {"text": "text-to-text generation", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.7287682443857193}]}, {"text": "A sentence fusion module takes related sentences as input, and generates a single sentence summarizing the input sentences.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 2, "end_pos": 17, "type": "TASK", "confidence": 0.7369115054607391}]}, {"text": "The general strategy described in is to first align the dependency structures of the two input sentences to find the common information in both sentences.", "labels": [], "entities": []}, {"text": "On the basis of this alignment, the common information is framed into an fusion tree (i.e., capturing the shared information), which is subsequently realized in natural language by generating all traversals of the fusion tree and scoring their probability using an n-gram language model.", "labels": [], "entities": []}, {"text": "Of the sentences thus generated the one with the lowest (length normalized) entropy is selected.", "labels": [], "entities": []}, {"text": "Barzilay and co-workers apply sentence fusion in the context of multi-document summarization, where the input sentences typically come from multiple documents describing the same event, but sentence fusion seems to be useful for other applications as well.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7393341064453125}, {"text": "multi-document summarization", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.5974559783935547}, {"text": "sentence fusion", "start_pos": 190, "end_pos": 205, "type": "TASK", "confidence": 0.7325419485569}]}, {"text": "In question-answering, for instance, sentence fusion could be used to generate more complete answers.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.7178379744291306}]}, {"text": "Many current QA systems use various parallel answer-finding strategies, each of which may produce an Nbest list of answers (e.g.,) In response to a question like \"What causes RSI?\" one potential answer sentence could be: RSI can be caused by repeating the same sequence of movements many times an hour or day.", "labels": [], "entities": []}, {"text": "And another might be: RSI is generally caused by a mixture of poor ergonomics, stress and poor posture.", "labels": [], "entities": [{"text": "RSI", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9246084094047546}]}, {"text": "These two incomplete answers might be fused into a more complete answer such as: RSI can be caused by a mixture of poor ergonomics, stress, poor posture and by repeating the same sequence of movements many times an hour or day.", "labels": [], "entities": [{"text": "RSI", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.8742505311965942}]}, {"text": "The same process of sentence fusion can of course be applied to the whole list of N-best answers in order to derive a more specific, or even the most specific, answer, akin to taking the union of a number of sets.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7178162038326263}]}, {"text": "Likewise, we can rely on sentence fusion to derive a more general answer, or even the most general one (cf. intersection), in the hope that this will filter out irrelevant parts of the answer.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7214482128620148}]}, {"text": "Arguably, such applications call fora generalized version of sentence fusion, which may have consequences for the various components (alignment, fusion and generation) of the sentence fusion pipeline.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.7565441727638245}, {"text": "sentence fusion pipeline", "start_pos": 175, "end_pos": 199, "type": "TASK", "confidence": 0.7857364018758138}]}, {"text": "At the alignment level, we would like to have a better understanding of how words and phrases in the input sentences relate to each other.", "labels": [], "entities": []}, {"text": "Rather than a binary choice (align or not), one might want to distinguish more fine-grained relations such as overlap (if two phrases share some but not all of their content), paraphrases (if two phrases express the same information in different ways), entailments (if one phrase entails the other, but not vice versa), etc.", "labels": [], "entities": []}, {"text": "Such an alignment strategy would be especially useful for applications such as question answering and information extraction, where it is often important to know whether two sentences are paraphrases or stand in an entailment relation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.926196277141571}, {"text": "information extraction", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.8420986533164978}]}, {"text": "In the fusion module, we are interested in the possibilities to generate various kinds of fusions depending on the relations between the respective sentences, e.g., selecting the more specific or the more general phrase depending on whether the fusion tree is an intersection or a union one.", "labels": [], "entities": []}, {"text": "Finally, the generation maybe more complicated in the generalized version, and it is an interesting question whether the use of language models is equally suitable for different kinds of fusion.", "labels": [], "entities": []}, {"text": "In this paper, we will explore some of these issues related to a generalized version of sentence fusion.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.7921968400478363}]}, {"text": "We start with the basic question whether it is possible at all to reliably align sentences, including different potential relations between words and phrases (section 2).", "labels": [], "entities": []}, {"text": "We then present our ongoing work on sentence fusion, describing the current status and performance of the alignment algorithm (section 3), as well as the fusion and generation components (section 4).", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7794677913188934}]}, {"text": "We end with discussion and description of future plans in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the automatic alignment of nodes, abstracting from relation labels, as we have no algorithm for automatic labeling of these relations yet.", "labels": [], "entities": []}, {"text": "The baseline is achieved by aligning those nodes withstand in an equals relation to each other, i.e., anode v in Dis aligned to anode v in D iff STR(v) =STR(v ).", "labels": [], "entities": []}, {"text": "This alignment can be constructed relatively easy.", "labels": [], "entities": []}, {"text": "The alignment algorithm is tested with the following NODEMATCH function: hyperonym or hyponym of LABEL(v ) 0 otherwise 1 In the original formulation of the algorithm by, there is a penalty for skipping edges.: Precision, recall and F-score on automatic alignment It reserves the highest value fora literal string match, a somewhat lower value for matching lemmas, and an even lower value in case of a synonym, hyperonym or hyponym relation.", "labels": [], "entities": [{"text": "NODEMATCH", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.8823840618133545}, {"text": "LABEL", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.8406872153282166}, {"text": "Precision", "start_pos": 210, "end_pos": 219, "type": "METRIC", "confidence": 0.998167872428894}, {"text": "recall", "start_pos": 221, "end_pos": 227, "type": "METRIC", "confidence": 0.9990105628967285}, {"text": "F-score", "start_pos": 232, "end_pos": 239, "type": "METRIC", "confidence": 0.9985796213150024}]}, {"text": "The latter relations are retrieved from the Dutch part of EuroWordnet].", "labels": [], "entities": [{"text": "Dutch part of EuroWordnet", "start_pos": 44, "end_pos": 69, "type": "DATASET", "confidence": 0.6667452156543732}]}, {"text": "For the RELMATCH function, we simply used a value of 1 for identical dependency relations, and 0 otherwise.", "labels": [], "entities": [{"text": "RELMATCH", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.7733052372932434}]}, {"text": "These values were found to be adequate in a number of test runs on two other, manually aligned chapters (these chapters were not used for the actual evaluation).", "labels": [], "entities": []}, {"text": "In the future we intend to experiment with automatic optimizations.", "labels": [], "entities": []}, {"text": "We measured the alignment accuracy defined as the percentage of correctly aligned node pairs, where the consensus alignment of the first chapter served as the golden standard.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.8790944218635559}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "In order to test the contribution of synonym and hyperonym information for node matching, performance is measured with and without the use of Eurowordnet.", "labels": [], "entities": [{"text": "node matching", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.7004090249538422}, {"text": "Eurowordnet", "start_pos": 142, "end_pos": 153, "type": "DATASET", "confidence": 0.991715133190155}]}, {"text": "The results show that the algorithm improves substantially on the baseline.", "labels": [], "entities": []}, {"text": "The baseline already achieves a relatively high score (an F-score of .56), which maybe attributed to the nature of our material: the translated sentence pairs are relatively close to each other and may show a sizeable amount of literal string overlap.", "labels": [], "entities": [{"text": "F-score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9987857937812805}]}, {"text": "The alignment algorithm (without use of EuroWordnet) loses a few points on precision, but improves a lot on recall (a 200% increase with respect to the baseline), which in turn leads to a substantial improvement on the overall F-score.", "labels": [], "entities": [{"text": "EuroWordnet", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.9825431108474731}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9994046688079834}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9995823502540588}, {"text": "F-score", "start_pos": 227, "end_pos": 234, "type": "METRIC", "confidence": 0.9945762157440186}]}, {"text": "The use of Eurowordnet leads to a small increase (two points) on both precision and recall (and thus to small increase on F-score).", "labels": [], "entities": [{"text": "Eurowordnet", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.9743337035179138}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9995529055595398}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9984686970710754}, {"text": "F-score", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9990043044090271}]}, {"text": "Yet, in comparison with the gold standard human score for this task (.95), there is clearly room for further improvement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Interannotator agreement with respect to align- ment between annotators 1 and 2 before (A 1 , A 2 ) and after  (A 1 , A 2 ) revision , and between the consensus and annota- tor 1 (A c , A 1 ) and annotator 2 (A c , A 2 ) respectively.", "labels": [], "entities": []}, {"text": " Table 2: Inter-annotator agreement with respect to alignment  relation labeling between annotators 1 and 2 before (A 1 , A 2 )  and after (A 1 , A 2 ) revision , and between the consensus and  annotator 1 (A c , A 1 ) and annotator 2 (A c , A 2 ) respectively.", "labels": [], "entities": [{"text": "alignment  relation labeling", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.8491227229436239}]}, {"text": " Table 3: Precision, recall and F-score on automatic alignment", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.999082088470459}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.999559223651886}, {"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9993069171905518}, {"text": "alignment", "start_pos": 53, "end_pos": 62, "type": "TASK", "confidence": 0.6473392248153687}]}, {"text": " Table 4: Results of the evaluation of the sentence fusion out- put as the number of sentences in each of the three categories  perfect, acceptable and nonsense per judge (J1 and J2), bro- ken down in restatements, generalizations and specifications.", "labels": [], "entities": []}]}