{"title": [{"text": "Using Semantic and Syntactic Graphs for Call Classification", "labels": [], "entities": [{"text": "Call Classification", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.9660809338092804}]}], "abstractContent": [{"text": "In this paper, we introduce anew data representation format for language processing , the syntactic and semantic graphs (SSGs), and show its use for call classification in spoken dialog systems.", "labels": [], "entities": [{"text": "call classification", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.7829605937004089}]}, {"text": "For each sentence or utterance, these graphs include lexical information (words), syntactic information (such as the part of speech tags of the words and the syntactic parse of the utterance), and semantic information (such as the named entities and semantic role labels).", "labels": [], "entities": []}, {"text": "In our experiments, we used written language as the training data while computing SSGs and tested on spoken language.", "labels": [], "entities": []}, {"text": "In spite of this mismatch, we have shown that this is a very promising approach for classifying complex examples , and by using SSGs it is possible to reduce the call classification error rate by 4.74% relative.", "labels": [], "entities": [{"text": "call classification error rate", "start_pos": 162, "end_pos": 192, "type": "TASK", "confidence": 0.7537572830915451}]}], "introductionContent": [{"text": "Goal-oriented spoken dialog systems aim to identify intents of humans, expressed in natural language, and take actions accordingly to satisfy their requests.", "labels": [], "entities": []}, {"text": "The intent of each speaker is identified using a natural language understanding component.", "labels": [], "entities": []}, {"text": "This step can be seen as a multi-label, multi-class call classification problem for customer care applications (; Gupta et al., To appear, among others).", "labels": [], "entities": [{"text": "multi-class call classification", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.6651496191819509}]}, {"text": "As an example, consider the utterance I would like to know my account balance, from a financial domain customer care application.", "labels": [], "entities": []}, {"text": "Assuming that the utterance is recognized correctly by the automatic speech recognizer (ASR), the corresponding intent (call-type) would be Request(Balance) and the action would be telling the balance to the user after prompting for the account number or routing this call to the billing department.", "labels": [], "entities": [{"text": "automatic speech recognizer (ASR)", "start_pos": 59, "end_pos": 92, "type": "TASK", "confidence": 0.7339262664318085}, {"text": "Request(Balance)", "start_pos": 140, "end_pos": 156, "type": "METRIC", "confidence": 0.9469525963068008}]}, {"text": "Typically these application specific call-types are pre-designed and large amounts of utterances manually labeled with call-types are used for training call classification systems.", "labels": [], "entities": [{"text": "training call classification", "start_pos": 143, "end_pos": 171, "type": "TASK", "confidence": 0.6011204918225607}]}, {"text": "For classification, generally word \u00a2 -grams are used as features: In the How May I Help You?\u00a3 \u00a5 \u00a4 (HMIHY) call routing system, selected word \u00a2 -grams, namely salient phrases, which are salient to certain call-types play an important role ().", "labels": [], "entities": [{"text": "HMIHY) call routing", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7526600360870361}]}, {"text": "For instance, for the above example, the salient phrase \"account balance\" is strongly associated with the call-type Request(Balance).", "labels": [], "entities": [{"text": "call-type Request(Balance)", "start_pos": 106, "end_pos": 132, "type": "TASK", "confidence": 0.596238112449646}]}, {"text": "Instead of using salient phrases, one can leave the decision of determining useful features (word \u00a2 -grams) to a classification algorithm used as described in) and (Gupta et al., To appear).", "labels": [], "entities": []}, {"text": "An alternative would be using a vector space model for classification where calltypes and utterances are represented as vectors including word \u00a2 -grams.", "labels": [], "entities": []}, {"text": "Call classification is similar to text categorization, except the following: The utterances are much shorter than typical documents used for text categorization (such as broadcast news or newspaper articles).", "labels": [], "entities": [{"text": "Call classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8560351431369781}]}, {"text": "Since it deals with spontaneous speech, the utterances frequently include disfluencies or are ungrammatical, and \u00a6 ASR output is very noisy, typically one out of every four words is misrecognized ().", "labels": [], "entities": [{"text": "\u00a6 ASR output", "start_pos": 113, "end_pos": 125, "type": "METRIC", "confidence": 0.6819136142730713}]}, {"text": "Even though the shortness of the utterances may imply the easiness of the call classification task, unfortunately this is not the case.", "labels": [], "entities": [{"text": "call classification task", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.8946008880933126}]}, {"text": "The call classification error rates typically range between 15% to 30% depending on the application (Gupta et al., To appear).", "labels": [], "entities": [{"text": "call classification error rates", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.7882103249430656}]}, {"text": "This is mainly due to the data sparseness problem because of the nature of the input.", "labels": [], "entities": []}, {"text": "Even for simple call-types like Request(Balance), there are many ways of uttering the same intent.", "labels": [], "entities": [{"text": "Request(Balance)", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7748575806617737}]}, {"text": "For instance, in one of the applications we used in our experiments, as a response to the greeting prompt, there are 2,697 unique utterances out of 3,547 utterances for that call-type.", "labels": [], "entities": []}, {"text": "Some examples include: Given this data sparseness, current classification approaches require an extensive amount of labeled data in order to train a call classification system with a reasonable performance.", "labels": [], "entities": [{"text": "call classification", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.8102727234363556}]}, {"text": "In this paper, we present methods for extending the classifier's feature set by generalizing word sequences using syntactic and semantic information represented in compact graphs, called syntactic and semantic graphs (SSGs).", "labels": [], "entities": []}, {"text": "For each sentence or utterance, these graphs include lexical information (words), syntactic information (such as the part of speech tags of the words and the syntactic parse of the utterance), and semantic information (such as the named entities and semantic role labels).", "labels": [], "entities": []}, {"text": "The generalization is expected to help reduce the data sparseness problem by applying various groupings on word sequences.", "labels": [], "entities": []}, {"text": "Furthermore, the classifier is provided with additional syntactic and semantic information which might be useful for the call classification task.", "labels": [], "entities": [{"text": "call classification task", "start_pos": 121, "end_pos": 145, "type": "TASK", "confidence": 0.8886425892512003}]}, {"text": "In the following section, we describe the syntactic and semantic graphs.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our approach for call classification using SSGs.", "labels": [], "entities": [{"text": "call classification", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.9459647536277771}]}, {"text": "In Section 4, we present the computation of syntactic and semantic information for SSGs.", "labels": [], "entities": [{"text": "SSGs", "start_pos": 83, "end_pos": 87, "type": "TASK", "confidence": 0.9368894100189209}]}, {"text": "In the last Section, we present our experiments and results using a spoken dialog system AT&T VoiceTone R \u00a7 Spoken Dialog System (Gupta et al., To appear).", "labels": [], "entities": [{"text": "AT&T VoiceTone R \u00a7 Spoken Dialog System", "start_pos": 89, "end_pos": 128, "type": "DATASET", "confidence": 0.9521441989474826}]}], "datasetContent": [{"text": "In order to evaluate our approach, we carried out call classification experiments using human-machine dialogs collected by the spoken dialog system used for customer care.", "labels": [], "entities": [{"text": "call classification", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8411067128181458}]}, {"text": "We have only considered utterances which are responses to the greeting prompt How may I help you?", "labels": [], "entities": []}, {"text": "in order not to deal with confirmation and clarification utterances.", "labels": [], "entities": [{"text": "confirmation and clarification utterances", "start_pos": 26, "end_pos": 67, "type": "TASK", "confidence": 0.8517759740352631}]}, {"text": "We first describe this data, and then give the results obtained by the semantic classifier.", "labels": [], "entities": []}, {"text": "We have performed our tests using the Boostexter tool, an implementation of the Boosting algorithm, which iteratively selects the most discriminative features fora given task).", "labels": [], "entities": []}, {"text": "summarizes the characteristics of our application including the amount of training and test data, total number of call-types, average utterance length, and call-type perplexity.", "labels": [], "entities": []}, {"text": "Perplexity is computed using the prior distribution overall the call-types in the training data.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9528940320014954}]}], "tableCaptions": [{"text": " Table 1: F-Measure results for named entity extrac- tion with various approaches. HMM is the sim- ple HMM-based approach, IF is the simplified ver- sion of BBN's name finder with an unknown words  model.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9272370338439941}, {"text": "BBN's name finder", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.7836742699146271}]}, {"text": " Table 2: Characteristics of the data used in the ex- periments.", "labels": [], "entities": []}, {"text": " Table 3: A comparison of number of features.", "labels": [], "entities": []}, {"text": " Table 3. The clas- sifier has now 15 times more features to work with.  Although one can apply a feature selection approach  before classification as frequently done in the ma- chine learning community, we left the burden of an- alyzing 825,201 features to the classifier.", "labels": [], "entities": []}, {"text": " Table 4: The percentage of the features selected by  the classifier for each information category", "labels": [], "entities": []}, {"text": " Table 5: Call classification error rates using words  and SSGs.", "labels": [], "entities": [{"text": "Call classification error", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7953389485677084}]}]}