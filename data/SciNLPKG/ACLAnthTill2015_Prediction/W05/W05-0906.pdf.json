{"title": [{"text": "Evaluating Summaries and Answers: Two Sides of the Same Coin?", "labels": [], "entities": [{"text": "Evaluating Summaries and Answers", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.879943311214447}]}], "abstractContent": [{"text": "This paper discusses the convergence between question answering and multi-document summarization, pointing out implications and opportunities for knowledge transfer in both directions.", "labels": [], "entities": [{"text": "question answering", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8639246225357056}, {"text": "multi-document summarization", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.613405391573906}, {"text": "knowledge transfer", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.7185380458831787}]}, {"text": "As a case study in one direction, we discuss the recent development of an automatic method for evaluating definition questions based on n-gram overlap, a commonly-used technique in summarization evaluation.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 181, "end_pos": 205, "type": "TASK", "confidence": 0.972527951002121}]}, {"text": "In the other direction, the move towards topic-oriented summaries requires an understanding of relevance and topi-cality, issues which have received attention in the question answering literature.", "labels": [], "entities": [{"text": "question answering", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.8104741275310516}]}, {"text": "It is our opinion that question answering and multi-document summarization represent two complementary approaches to the same problem of satisfying complex user information needs.", "labels": [], "entities": [{"text": "question answering", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.8698210418224335}, {"text": "multi-document summarization", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.6648693084716797}]}, {"text": "Although this points to many exciting opportunities for system-building, here we primarily focus on implications for system evaluation.", "labels": [], "entities": [{"text": "system evaluation", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.7296614348888397}]}], "introductionContent": [{"text": "Recent developments in question answering (QA) and multi-document summarization point to many interesting convergences that present exciting opportunities for collaboration and cross-fertilization between these largely independent communities.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8884950518608093}, {"text": "multi-document summarization", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.6861689388751984}]}, {"text": "This position paper attempts to draw connections between the task of answering complex natural language questions and the task of summarizing multiple documents, the boundaries between which are beginning to blur, as anticipated half a decade ago ( ).", "labels": [], "entities": [{"text": "answering complex natural language questions", "start_pos": 69, "end_pos": 113, "type": "TASK", "confidence": 0.790306282043457}, {"text": "summarizing multiple documents", "start_pos": 130, "end_pos": 160, "type": "TASK", "confidence": 0.8907179037729899}]}, {"text": "Although the complementary co-evolution of question answering and document summarization presents new directions for system-building, this paper primarily focuses on implications for evaluation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.8005682528018951}]}, {"text": "Although assessment of answer and summary quality employs different methodologies, there are many lessons that each community can learn from the other.", "labels": [], "entities": []}, {"text": "The summarization community has extensive experience in intrinsic metrics based on n-gram overlap for automatically scoring system outputs against human-generated reference textsthese techniques would help streamline aspects of question answering evaluation.", "labels": [], "entities": [{"text": "question answering evaluation", "start_pos": 228, "end_pos": 257, "type": "TASK", "confidence": 0.9236359397570292}]}, {"text": "In the other direction, because question answering has its roots in information retrieval, much work has focused on extrinsic metrics based on relevance and topicality, which maybe valuable to summarization researchers.", "labels": [], "entities": [{"text": "question answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8942795693874359}, {"text": "information retrieval", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.7331119179725647}, {"text": "summarization", "start_pos": 193, "end_pos": 206, "type": "TASK", "confidence": 0.979977548122406}]}, {"text": "This paper is organized as follows: In Section 2, we discuss the evolution of question answering research and how recent trends point to the convergence of question answering and multi-document summarization.", "labels": [], "entities": [{"text": "question answering", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.8206941783428192}, {"text": "question answering", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.8672186732292175}]}, {"text": "In Section 3, we present a case study of automatically evaluating definition questions by employing metrics based on n-gram overlap, a general technique widely used in summarization and machine translation evaluations.", "labels": [], "entities": [{"text": "summarization", "start_pos": 168, "end_pos": 181, "type": "TASK", "confidence": 0.9870783090591431}, {"text": "machine translation evaluations", "start_pos": 186, "end_pos": 217, "type": "TASK", "confidence": 0.8103012839953104}]}, {"text": "Section 4 highlights some opportunities for knowledge transfer in the other direction: how the notions of rele-vance and topicality, well-studied in the information retrieval literature, can guide the evaluation of topicoriented summaries.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7399805784225464}]}, {"text": "We conclude with thoughts about the future in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}