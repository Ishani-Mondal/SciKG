{"title": [{"text": "Definition and Analysis of Intermediate Entailment Levels", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we define two intermediate models of textual entailment, which correspond to lexical and lexical-syntactic levels of representation.", "labels": [], "entities": []}, {"text": "We manually annotated a sample from the RTE dataset according to each model, compared the outcome for the two models, and explored how well they approximate the notion of entailment.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.911099910736084}]}, {"text": "We show that the lexical-syntactic model outperforms the lexical model, mainly due to a much lower rate of false-positives, but both models fail to achieve high recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9977213740348816}]}, {"text": "Our analysis also shows that paraphrases standout as a dominant contributor to the entailment task.", "labels": [], "entities": []}, {"text": "We suggest that our models and annotation methods can serve as an evaluation scheme for entailment at these levels.", "labels": [], "entities": []}], "introductionContent": [{"text": "Textual entailment has been proposed recently as a generic framework for modeling semantic variability in many Natural Language Processing applications, such as Question Answering, Information Extraction, Information Retrieval and Document Summarization.", "labels": [], "entities": [{"text": "Textual entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7114208340644836}, {"text": "Question Answering", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.8278924822807312}, {"text": "Information Extraction", "start_pos": 181, "end_pos": 203, "type": "TASK", "confidence": 0.7945362329483032}, {"text": "Information Retrieval", "start_pos": 205, "end_pos": 226, "type": "TASK", "confidence": 0.8062987923622131}, {"text": "Document Summarization", "start_pos": 231, "end_pos": 253, "type": "TASK", "confidence": 0.9227874577045441}]}, {"text": "The textual entailment relationship holds between two text fragments, termed text and hypothesis, if the truth of the hypothesis can be inferred from the text.", "labels": [], "entities": []}, {"text": "Identifying entailment is a complex task that incorporates many levels of linguistic knowledge and inference.", "labels": [], "entities": [{"text": "Identifying entailment", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9736436903476715}]}, {"text": "The complexity of modeling entailment was demonstrated in the first PASCAL Challenge Workshop on Recognizing Textual Entailment (RTE) ().", "labels": [], "entities": [{"text": "PASCAL Challenge Workshop on Recognizing Textual Entailment (RTE)", "start_pos": 68, "end_pos": 133, "type": "TASK", "confidence": 0.6848686397075653}]}, {"text": "Systems that participated in the challenge used various combinations of NLP components in order to perform entailment inferences.", "labels": [], "entities": []}, {"text": "These components can largely be classified as operating at the lexical, syntactic and semantic levels (see in).", "labels": [], "entities": []}, {"text": "However, only little research was done to analyze the contribution of each inference level, and on the contribution of individual inference mechanisms within each level.", "labels": [], "entities": []}, {"text": "This paper suggests that decomposing the complex task of entailment into subtasks, and analyzing the contribution of individual NLP components for these subtasks would make a step towards better understanding of the problem, and for pursuing better entailment engines.", "labels": [], "entities": []}, {"text": "We set three goals in this paper.", "labels": [], "entities": []}, {"text": "First, we consider two modeling levels that employ only part of the inference mechanisms, but perform perfectly at each level.", "labels": [], "entities": []}, {"text": "We explore how well these models approximate the notion of entailment, and analyze the differences between the outcome of the different levels.", "labels": [], "entities": []}, {"text": "Second, for each of the presented levels, we evaluate the distribution (and contribution) of each of the inference mechanisms typically associated with that level.", "labels": [], "entities": []}, {"text": "Finally, we suggest that the definitions of entailment at different levels of inference, as proposed in this paper, can serve as guidelines for manual annotation of a \"gold standard\" for evaluating systems that operate at a particular level.", "labels": [], "entities": []}, {"text": "Altogether, we set forth a possible methodology for annotation and analysis of entail-ment datasets.", "labels": [], "entities": []}, {"text": "We introduce two levels of entailment: Lexical and Lexical-Syntactic.", "labels": [], "entities": []}, {"text": "We propose these levels as intermediate stages towards a complete entailment model.", "labels": [], "entities": []}, {"text": "We define an entailment model for each level and manually evaluate its performance over a sample from the RTE test-set.", "labels": [], "entities": [{"text": "RTE test-set", "start_pos": 106, "end_pos": 118, "type": "DATASET", "confidence": 0.9078894853591919}]}, {"text": "We focus on these two levels as they correspond to well-studied NLP tasks, for which robust tools and resources exist, e.g. parsers, part of speech taggers and lexicons.", "labels": [], "entities": [{"text": "speech taggers", "start_pos": 141, "end_pos": 155, "type": "TASK", "confidence": 0.7285545468330383}]}, {"text": "At each level we included inference types that represent common practice in the field.", "labels": [], "entities": []}, {"text": "More advanced processing levels which involve logical/semantic inference are less mature and were left beyond the scope of this paper.", "labels": [], "entities": []}, {"text": "We found that the main difference between the lexical and lexical-syntactic levels is that the lexicalsyntactic level corrects many false-positive inferences done at the lexical level, while introducing only a few false-positives of its own.", "labels": [], "entities": []}, {"text": "As for identifying positive cases (recall), both systems exhibit similar performance, and were found to be complementary.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.998868465423584}]}, {"text": "Neither of the levels was able to identify more than half of the positive cases, which emphasizes the need for deeper levels of analysis.", "labels": [], "entities": []}, {"text": "Among the different inference components, paraphrases standout as a dominant contributor to the entailment task, while synonyms and derivational transformations were found to be the most frequent at the lexical level.", "labels": [], "entities": []}, {"text": "Using our definitions of entailment models as guidelines for manual annotation resulted in a high level of agreement between two annotators, suggesting that the proposed models are well-defined.", "labels": [], "entities": []}, {"text": "Our study follows on previous work), which analyzed the RTE Challenge test-set to find the percentage of cases in which syntactic analysis alone (with optional use of thesaurus for the lexical level) suffices to decide whether or not entailment holds.", "labels": [], "entities": [{"text": "RTE Challenge test-set", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.895775298277537}]}, {"text": "Our study extends this work by considering a broader range of inference levels and inference mechanisms and providing a more detailed view.", "labels": [], "entities": []}, {"text": "A fundamental difference between the two works is that while Vanderwende et al. did not make judgements on cases where additional knowledge was required beyond syntax, our entailment models were evaluated overall of the cases, including those that require higher levels of inference.", "labels": [], "entities": []}, {"text": "This allows us to view the entailment model at each level as an idealized system approximating full entailment, and to evaluate its overall success.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: section 2 provides definitions for the two entailment levels; section 3 describes the annotation experiment we performed, its results and analysis; section 4 concludes and presents planned future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results per level of entailment.", "labels": [], "entities": []}, {"text": " Table 3: Correlation between the entailment lev- els. (a) includes only the positive examples from  the RTE dataset sample, and (b) includes only the  negative examples.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9613320827484131}, {"text": "RTE dataset sample", "start_pos": 105, "end_pos": 123, "type": "DATASET", "confidence": 0.9581550558408102}]}, {"text": " Table 4: The frequency (f ), contribution to recall  (R) and percentage (%), within the gold standard  positive examples, of the various inference mecha- nisms at each level, ordered by their significance.", "labels": [], "entities": [{"text": "recall  (R)", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9515135437250137}]}]}