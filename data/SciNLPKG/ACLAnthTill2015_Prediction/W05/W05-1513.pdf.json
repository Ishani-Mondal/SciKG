{"title": [{"text": "A Classifier-Based Parser with Linear Run-Time Complexity", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a classifier-based parser that produces constituent trees in linear time.", "labels": [], "entities": []}, {"text": "The parser uses a basic bottom-up shift-reduce algorithm, but employs a classifier to determine parser actions instead of a grammar.", "labels": [], "entities": []}, {"text": "This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing.", "labels": [], "entities": [{"text": "full constituent parsing", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.6625327070554098}]}, {"text": "We show that, with an appropriate feature set used in classification , a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9976686835289001}]}, {"text": "We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.", "labels": [], "entities": [{"text": "WSJ section of the Penn Treebank", "start_pos": 44, "end_pos": 76, "type": "DATASET", "confidence": 0.9143324196338654}, {"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.999842643737793}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9997304081916809}]}], "introductionContent": [{"text": "Two classifier-based deterministic dependency parsers for English have been proposed recently (.", "labels": [], "entities": [{"text": "classifier-based deterministic dependency parsers", "start_pos": 4, "end_pos": 53, "type": "TASK", "confidence": 0.6130997985601425}]}, {"text": "Although they use different parsing algorithms, and differ on whether or not dependencies are labeled, they share the idea of greedily pursuing a single path, following parsing decisions made by a classifier.", "labels": [], "entities": []}, {"text": "Despite their greedy nature, these parsers achieve high accuracy in determining dependencies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9988911747932434}]}, {"text": "Although state-of-the-art statistical parsers) are more accurate, the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast, light-weight parsing, or parsing of large amounts of data.", "labels": [], "entities": []}, {"text": "However, dependency analyses lack important information contained in constituent structures.", "labels": [], "entities": []}, {"text": "For example, the tree-path feature has been shown to be valuable in semantic role labeling ().", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7554169495900472}]}, {"text": "We present a parser that shares much of the simplicity and efficiency of the deterministic dependency parsers, but produces both dependency and constituent structures simultaneously.", "labels": [], "entities": []}, {"text": "Like the parser of, it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.", "labels": [], "entities": []}, {"text": "While it may seem that the larger search space of constituent trees (compared to the space of dependency trees) would make it unlikely that accurate parse trees could be built deterministically, we show that the precision and recall of constituents produced by our parser are close to those produced by statistical parsers with higher run-time complexity.", "labels": [], "entities": [{"text": "precision", "start_pos": 212, "end_pos": 221, "type": "METRIC", "confidence": 0.998180627822876}, {"text": "recall", "start_pos": 226, "end_pos": 232, "type": "METRIC", "confidence": 0.9956002235412598}]}, {"text": "One desirable characteristic of our parser is its simplicity.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9822567105293274}]}, {"text": "Compared to other successful approaches to corpus-based constituent parsing, ours is remarkably simple to understand and implement.", "labels": [], "entities": [{"text": "corpus-based constituent parsing", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.6148058871428171}]}, {"text": "An additional feature of our approach is its modularity with regard to the algorithm and the classifier that determines the parser's actions.", "labels": [], "entities": []}, {"text": "This makes it very simple for different classifiers and different sets of features to be used with the same parser with very minimal work.", "labels": [], "entities": []}, {"text": "Finally, its linear runtime complexity allows our parser to be considerably faster than lexicalized PCFG-based parsers.", "labels": [], "entities": []}, {"text": "On the other hand, a major drawback of the classifier-based parsing framework is that, depending on the classifier used, its training time can be much longer than that of other approaches.", "labels": [], "entities": [{"text": "classifier-based parsing", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.6576181054115295}]}, {"text": "Like other deterministic parsers (and unlike many statistical parsers), our parser considers the problem of syntactic analysis separately from partof-speech (POS) tagging.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7285958528518677}, {"text": "partof-speech (POS) tagging", "start_pos": 143, "end_pos": 170, "type": "TASK", "confidence": 0.6023723304271698}]}, {"text": "Because the parser greedily builds trees bottom-up in one pass, considering only one path at any point in the analysis, the task of assigning POS tags to words is done before other syntactic analysis.", "labels": [], "entities": []}, {"text": "In this work we focus only on the processing that occurs once POS tagging is completed.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.7702029645442963}]}, {"text": "In the sections that follow, we assume that the input to the parser is a sentence with corresponding POS tags for each word.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments with the parser described in section 2 using two different classifiers: TinySVM (a support vector machine implementation by Taku Kudo) 2 , and the memory-based learner TiMBL ().", "labels": [], "entities": []}, {"text": "We trained and tested the parser on the Wall Street Journal corpus of the Penn Treebank () using the standard split: sections 2-21 were used for training, section 22 was used for development and tuning of parameters and features, and section 23 was used for testing.", "labels": [], "entities": [{"text": "Wall Street Journal corpus of the Penn Treebank", "start_pos": 40, "end_pos": 87, "type": "DATASET", "confidence": 0.9582353234291077}]}, {"text": "Every experiment reported here was performed on a Pentium IV 1.8GHz with 1GB of RAM.", "labels": [], "entities": []}, {"text": "Each tree in the training set had empty-node and function tag information removed, and the 2 http://chasen.org/~taku/software/TinySVM trees were lexicalized using similar head-table rules as those mentioned in.", "labels": [], "entities": []}, {"text": "The trees were then converted into trees containing only unary and binary branching, using the binarization transform described in section 2.", "labels": [], "entities": []}, {"text": "Classifier training instances of features paired with classes (parser actions) were extracted from the trees in the training set, as described in section 2.3.", "labels": [], "entities": []}, {"text": "The total number of training instances was about 1.5 million.", "labels": [], "entities": []}, {"text": "The classifier in the SVM-based parser (denoted by SVMpar) uses the polynomial kernel with degree 2, following the work of Yamada and Matsumoto (2003) on SVM-based deterministic dependency parsing, and a one-against-all scheme for multi-class classification.", "labels": [], "entities": [{"text": "SVM-based deterministic dependency parsing", "start_pos": 154, "end_pos": 196, "type": "TASK", "confidence": 0.6814572215080261}, {"text": "multi-class classification", "start_pos": 231, "end_pos": 257, "type": "TASK", "confidence": 0.7327394783496857}]}, {"text": "Because of the large number of training instances, we used Yamada and Matsumoto's idea of splitting the training instances into several parts according to POS tags, and training classifiers on each part.", "labels": [], "entities": []}, {"text": "This greatly reduced the time required to train the SVMs, but even with the splitting of the training set, total training time was about 62 hours.", "labels": [], "entities": []}, {"text": "Training set splitting comes with the cost of reduction inaccuracy of the parser, but training a single SVM would likely take more than one week.", "labels": [], "entities": []}, {"text": "Yamada and Matsumoto experienced a reduction of slightly more than 1% in de-  pendency accuracy due to training set splitting, and we expect that a similar loss is incurred here.", "labels": [], "entities": [{"text": "de-  pendency", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.7188987135887146}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.6137933135032654}]}, {"text": "When given perfectly tagged text (gold tags extracted from the Penn Treebank), SVMpar has labeled constituent precision and recall of 87.54% and 87.61%, respectively, and dependency accuracy of 90.3% overall sentences in the test set.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9958559572696686}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.6210814714431763}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9985694885253906}, {"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.5554978251457214}]}, {"text": "The total time required to parse the entire test set was 11 minutes.", "labels": [], "entities": [{"text": "parse", "start_pos": 27, "end_pos": 32, "type": "TASK", "confidence": 0.9690659642219543}]}, {"text": "Out of more than 2,400 sentences, only 26 were rejected by the parser (about 1.1%).", "labels": [], "entities": []}, {"text": "For these sentences, partial analyses were created by combining the items in the stack in flat structures, and these were included in the evaluation.", "labels": [], "entities": []}, {"text": "Predictably, the labeled constituent precision and recall obtained with automatically POS-tagged sentences were lower, at 86.01% and 86.15%.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9099022150039673}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9995835423469543}]}, {"text": "The part-of-speech tagger used in our experiments was SVMTool (, and its accuracy on the test set is 97%.", "labels": [], "entities": [{"text": "SVMTool", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.7359114289283752}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.999535322189331}]}, {"text": "The MBL-based parser (denoted by MBLpar) uses the IB1 algorithm, with five nearest neighbors, and the modified value difference metric (MVDM), following the work of on MBL-based deterministic dependency parsing.", "labels": [], "entities": [{"text": "value difference metric (MVDM)", "start_pos": 111, "end_pos": 141, "type": "METRIC", "confidence": 0.8394554058710734}, {"text": "MBL-based deterministic dependency parsing", "start_pos": 168, "end_pos": 210, "type": "TASK", "confidence": 0.5747278109192848}]}, {"text": "MBLpar was trained with all training instances in under 15 minutes, but its accuracy on the test set was much lower than that of SVMpar, with constituent precision and recall of 80.0% and 80.2%, and dependency accuracy of 86.3% (24 sentences were rejected).", "labels": [], "entities": [{"text": "MBLpar", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6560556888580322}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9996204376220703}, {"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9300441741943359}, {"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.9987621307373047}, {"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.7802644371986389}]}, {"text": "It was also much slower than SVMpar in parsing the test set, taking 127 minutes.", "labels": [], "entities": [{"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9806250333786011}]}, {"text": "In addition, the total memory required for running MBLpar (including the classifier) was close to 1 gigabyte (including the trained classifier), while SVMpar required only about 200 megabytes (including all the classifiers).", "labels": [], "entities": []}, {"text": "shows a summary of the results of our experiments with SVMpar and MBLpar, and also results obtained with the Charniak (2000) parser, the Bikel (2003) implementation of the Collins (1997) parser, and the Ratnaparkhi (1997) parser.", "labels": [], "entities": []}, {"text": "We also include the dependency accuracy from Yamada and Matsumoto's (2003) SVM-based dependency parser, and Nivre and MBL-based dependency parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9525880217552185}, {"text": "SVM-based dependency parser", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.5800373256206512}, {"text": "Nivre and MBL-based dependency parser", "start_pos": 108, "end_pos": 145, "type": "TASK", "confidence": 0.5221499979496003}]}, {"text": "These results show that the choice of classifier is extremely important in this task.", "labels": [], "entities": []}, {"text": "SVMpar and MBLpar use the same algorithm and features, and differ only on the classifiers used to make parsing decisions.", "labels": [], "entities": [{"text": "SVMpar", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8948626518249512}]}, {"text": "While in many natural language processing tasks different classifiers perform at similar levels of accuracy, we have observed a dramatic difference between using support vector machines and a memory-based learner.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9982166886329651}]}, {"text": "Although the reasons for such a large disparity in results is currently the subject of further investigation, we speculate that a relatively small difference in initial classifier accuracy results in larger differences in parser performance, due to the deterministic nature of the parser (certain errors may lead to further errors).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.8684526085853577}]}, {"text": "We also believe classifier choice to be one major source of the difference inaccuracy between Nivre and Scholz's parser and Yamada and Matsumoto's parser.", "labels": [], "entities": []}, {"text": "While the accuracy of SVMpar is below that of lexicalized PCFG-based statistical parsers, it is surprisingly good fora greedy parser that runs in linear time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995898604393005}]}, {"text": "Additionally, it is considerably faster than lexicalized PCFG-based parsers, and offers a good alternative for when fast parsing is needed.", "labels": [], "entities": []}, {"text": "MBLpar, on the other hand, performed poorly in terms of accuracy and speed.", "labels": [], "entities": [{"text": "MBLpar", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9327743053436279}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9995336532592773}, {"text": "speed", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9838001132011414}]}], "tableCaptions": [{"text": " Table 1: Summary of results on labeled precision and recall of constituents, dependency accu- racy, and time required to parse the test set. The parsers of Yamada and Matsumoto (Y&M) and  Nivre and Scholz (N&S) do not produce constituent structures, only dependencies. \"unk\" indi- cates unknown values. Results for MBLpar and SVMpar using correct POS tags (if automatically  produced POS tags are used, accuracy figures drop about 1.5% over all metrics).", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9290080070495605}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9849912524223328}, {"text": "accuracy", "start_pos": 404, "end_pos": 412, "type": "METRIC", "confidence": 0.9988161325454712}]}]}