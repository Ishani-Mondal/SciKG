{"title": [{"text": "Syntactic Features for Evaluation of Machine Translation", "labels": [], "entities": [{"text": "Evaluation of Machine Translation", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.6283497661352158}]}], "abstractContent": [{"text": "Automatic evaluation of machine translation , based on computing n-gram similarity between system output and human reference translations, has revolutionized the development of MT systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7536044716835022}, {"text": "MT", "start_pos": 177, "end_pos": 179, "type": "TASK", "confidence": 0.9960795044898987}]}, {"text": "We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.", "labels": [], "entities": []}, {"text": "Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations fora given sentence.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7320820093154907}]}, {"text": "Human evaluation of system output is costly in both time and money, leading to the rise of automatic evaluation metrics in recent years.", "labels": [], "entities": []}, {"text": "The most commonly used automatic evaluation metrics, BLEU () and NIST, are based on the assumption that \"The closer a machine translation is to a professional human translation, the better it is\" ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9965124726295471}, {"text": "NIST", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.8190652132034302}]}, {"text": "For every hypothesis, BLEU computes the fraction of n-grams which also appear in the reference sentences, as well as a brevity penalty.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9986452460289001}]}, {"text": "NIST uses a similar strategy to BLEU but further considers that n-grams with different frequency should be treated differently in the evaluation.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9790297746658325}, {"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9828673601150513}]}, {"text": "It introduces the notion of information weights, which indicate that rarely occurring n-grams count more than those frequently occurring ones in the evaluation).", "labels": [], "entities": []}, {"text": "BLEU and NIST have been shown to correlate closely with human judgments in ranking MT systems with different qualities ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9787636995315552}, {"text": "NIST", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.8584126830101013}, {"text": "MT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.9207968711853027}]}, {"text": "In the 2003 Johns Hopkins Workshop on Speech and Language Engineering, experiments on MT evaluation showed that BLEU and NIST do not correlate well with human judgments at the sentence level, even when they correlate well overlarge test sets ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.9736421704292297}, {"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9976453185081482}]}, {"text": "use a machine learning approach to improve the correlation at the sentence level.", "labels": [], "entities": [{"text": "correlation", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9644811153411865}]}, {"text": "Their method, based on the assumption that higher classification accuracy in discriminating human-from machine-generated translations will yield closer correlation with human judgments, uses support vector machine (SVM) based learning to weight multiple metrics such as BLEU, NIST, and WER (minimal word error rate).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.8576577305793762}, {"text": "BLEU", "start_pos": 270, "end_pos": 274, "type": "METRIC", "confidence": 0.9975645542144775}, {"text": "WER (minimal word error rate", "start_pos": 286, "end_pos": 314, "type": "METRIC", "confidence": 0.6997051338354746}]}, {"text": "The SVM is trained for differentiating the MT hypothesis and the professional human translations, and then the distance from the hypothesis's metric vector to the hyper-plane of the trained SVM is taken as the final score for the hypothesis.", "labels": [], "entities": [{"text": "MT hypothesis", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.8640744984149933}]}, {"text": "While the machine learning approach improves correlation with human judgments, all the metrics discussed are based on the same type of information: n-gram subsequences of the hypothesis translations.", "labels": [], "entities": []}, {"text": "This type of feature cannot capture the grammaticality of the sentence, in part because they do not take into account sentence-level information.", "labels": [], "entities": []}, {"text": "For example, a sentence can achieve an excellent BLEU score without containing a verb.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9844854772090912}]}, {"text": "As MT systems improve, the shortcomings of n-gram based evaluation are becoming more apparent.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9921237826347351}]}, {"text": "State-of-the-art MT output often contains roughly the correct words and concepts, but does not form a coherent sentence.", "labels": [], "entities": [{"text": "MT output", "start_pos": 17, "end_pos": 26, "type": "TASK", "confidence": 0.8954334259033203}]}, {"text": "Often the intended meaning can be inferred; often it cannot.", "labels": [], "entities": []}, {"text": "Evidence that we are reaching the limits of ngram based evaluation was provided by, who found that a syntax-based language model improved the fluency and semantic accuracy of their system, but lowered their BLEU score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9443796277046204}, {"text": "BLEU score", "start_pos": 207, "end_pos": 217, "type": "METRIC", "confidence": 0.9848771095275879}]}, {"text": "With the progress of MT research in recent years, we are not satisfied with the getting correct words in the translations; we also expect them to be wellformed and more readable.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9897524118423462}]}, {"text": "This presents new challenges to MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9807349443435669}]}, {"text": "As discussed above, the existing word-based metrics cannot give a clear evaluation for the hypothesis' fluency.", "labels": [], "entities": []}, {"text": "For example, in the BLEU metric, the overlapping fractions of n-grams with more than one word are considered as a kind of metric for the fluency of the hypothesis.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9569582343101501}]}, {"text": "Consider the following simple example: Reference: I had a dog.", "labels": [], "entities": []}, {"text": "Hypothesis 1: I have the dog.", "labels": [], "entities": []}, {"text": "Hypothesis 2: A dog I had.", "labels": [], "entities": []}, {"text": "If we use BLEU to evaluate the two sentences, hypothesis 2 has two bigrams a dog and I had which are also found in the reference, and hypothesis 1 has no bigrams in common with the reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9980334639549255}]}, {"text": "Thus hypothesis 2 will get a higher score than hypothesis 1.", "labels": [], "entities": []}, {"text": "The result is obviously incorrect.", "labels": [], "entities": []}, {"text": "However, if we evaluate their fluency based on the syntactic similarity with the reference, we will get our desired results.", "labels": [], "entities": []}, {"text": "shows syntactic trees for the example sentences, from which we can see that hypothesis 1 has exactly the same syntactic structure with the reference, while hypothesis 2 has a very different one.", "labels": [], "entities": []}, {"text": "Thus the evaluation of fluency can be transformed as computing the syntactic similarity of the hypothesis and the references.", "labels": [], "entities": []}, {"text": "This paper develops a number of syntactically motivated evaluation metrics computed by automatically parsing both reference and hypothesis sentences.", "labels": [], "entities": []}, {"text": "Our experiments measure how well these metrics correlate with human judgments, both for individual sentences and over a large test set translated by MT systems of varying quality.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our testing data contains two parts.", "labels": [], "entities": []}, {"text": "One part is a set of 665 English sentences generated by a ChineseEnglish MT system.", "labels": [], "entities": [{"text": "ChineseEnglish MT system", "start_pos": 58, "end_pos": 82, "type": "DATASET", "confidence": 0.7212171753247579}]}, {"text": "And for each MT hypothesis, three reference translations are associated with it.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9707810282707214}]}, {"text": "The translations were generated by the alignment template system of.", "labels": [], "entities": []}, {"text": "This testing set is called JHU testing set in this paper.", "labels": [], "entities": [{"text": "JHU testing set", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.9225054979324341}]}, {"text": "The other set of testing data is from MT evaluation workshop at ACL05.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.7324828803539276}, {"text": "ACL05", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.5478450059890747}]}, {"text": "Three sets of human translations (E01, E03, E04) are selected as the references, and the outputs of seven MT systems (E9 E11 E12 E14 E15 E17 E22) are used for testing the performance of our syntactic metrics.", "labels": [], "entities": []}, {"text": "Each set of MT translations contains 929 English sentences, each of which is associated with human judgments for its fluency and adequacy.", "labels": [], "entities": [{"text": "MT translations", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.9260377287864685}]}, {"text": "The fluency and adequacy scores both range from 1 to 5.", "labels": [], "entities": []}, {"text": "Our syntactic metrics are motivated by a desire to better capture grammaticality in MT evaluation, and thus we are most interested in how well they correlate with human judgments of sentences' fluency, rather than the adequacy of the translation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.9639362394809723}]}, {"text": "To do this, the syntactic metrics (computed with the Collins (1999) parser) as well as BLEU were used to evaluate hypotheses in the test set from ACL05 MT workshop, which provides both fluency and adequacy scores for each sentence, and their Pearson coefficients of correlation with the human fluency scores were computed.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9992656111717224}, {"text": "ACL05 MT workshop", "start_pos": 146, "end_pos": 163, "type": "DATASET", "confidence": 0.8683056632677714}, {"text": "Pearson", "start_pos": 242, "end_pos": 249, "type": "METRIC", "confidence": 0.9817453026771545}]}, {"text": "For BLEU and HWCM, in order to avoid assigning zero scores to individual  sentences, when precision for n-grams of a particular length is zero we replace it with an epsilon value of 10 \u22123 . We choose E14 and E15 as two representative MT systems in the ACL05 MT workshop data set, which have relatively high human scores and low human scores respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9587610363960266}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9979369640350342}, {"text": "MT", "start_pos": 234, "end_pos": 236, "type": "TASK", "confidence": 0.9574352502822876}, {"text": "ACL05 MT workshop data set", "start_pos": 252, "end_pos": 278, "type": "DATASET", "confidence": 0.8961212754249572}]}, {"text": "The results are shown in, with every metric indexed by the maximum n-gram length or subtree depth.", "labels": [], "entities": []}, {"text": "The last row of the each table shows the treekernel-based measures, which have no depth parameter to adjust, but implicitly consider all depths.", "labels": [], "entities": []}, {"text": "The results show that in both systems our syntactic metrics all achieve a better performance in the correlation with human judgments of fluency.", "labels": [], "entities": []}, {"text": "We also notice that with the increasing of the maximum length of n-grams, the correlation of BLEU with human judgments does not necessarily increase, but decreases inmost cases.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9956615567207336}]}, {"text": "This is contrary to the argument in BLEU which says that longer n-grams better represent the sentences' fluency than the shorter  ones.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.6643277406692505}]}, {"text": "The problem can be explained by the limitation of the reference translations.", "labels": [], "entities": []}, {"text": "In our experiments, every hypothesis is evaluated by referring to three human translations.", "labels": [], "entities": []}, {"text": "Since the three human translations can only cover a small set of possible translations, with the increasing of n-gram length, more and more correct n-grams might not be found in the references, so that the fraction of longer ngrams turns to be less reliable than the short ones and hurts the final scores.", "labels": [], "entities": []}, {"text": "In the the corpus-level evaluation of a MT system, the sparse data problem will be less serious than in the sentence-level evaluation, since the overlapping n-grams of all the sentences and their references will be summed up.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9851507544517517}]}, {"text": "So in the traditional BLEU algorithm used for corpuslevel evaluation, a maximum n-gram of length 4 or 5 is usually used.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9599777460098267}, {"text": "corpuslevel evaluation", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.8004249334335327}]}, {"text": "A similar trend can be found in syntax tree and dependency tree based metrics, but the decreasing ratios are much lower than BLEU, which indicates that the syntactic metrics are less affected by the sparse data problem.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9990817308425903}]}, {"text": "The poor performance of tree-kernel based metrics also confirms our arguments on the sparse data problem, since the kernel measures implicitly consider the overlapping ratios of the sub-trees of all shapes, and thus will be very much affected by the sparse data problem.", "labels": [], "entities": []}, {"text": "Though our syntactic metrics are proposed for evaluating the sentences' fluency, we are curious how well they do in the overall evaluation of sentences.", "labels": [], "entities": []}, {"text": "Thus we also computed each metric's correlation with human overall judgments in E14, E15 and JHU testing set.", "labels": [], "entities": [{"text": "E14", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.9178144335746765}, {"text": "E15", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.7759154438972473}, {"text": "JHU testing set", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.9213123718897501}]}, {"text": "The overall human score for each sentence in E14 and E15 is computed by summing up its fluency score and adequacy score.", "labels": [], "entities": []}, {"text": "The results are shown in, and   competitive correlations in the test, among which HWCM, based on headword chains, gives better performances in evaluation of E14 and E15, and a slightly worse performance in JHU testing set than BLEU.", "labels": [], "entities": [{"text": "HWCM", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.724216639995575}, {"text": "JHU testing set", "start_pos": 206, "end_pos": 221, "type": "DATASET", "confidence": 0.9389068484306335}, {"text": "BLEU", "start_pos": 227, "end_pos": 231, "type": "METRIC", "confidence": 0.9931117296218872}]}, {"text": "Just as with the fluency evaluation, HWCM and other syntactic metrics present more stable performance as the n-gram's length (subtree's depth) increases.", "labels": [], "entities": []}, {"text": "MT systems and guiding their development.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.968166708946228}]}, {"text": "Does higher sentence-level correlation necessarily indicate higher correlation in corpus-level evaluation?", "labels": [], "entities": []}, {"text": "To answer this question, we used our syntactic metrics and BLEU to evaluate all the human-scored MT systems (E9 E11 E12 E14 E15 E17 E22) in the ACL05 MT workshop test set, and computed the correlation with human overall judgments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9993583559989929}, {"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.951850950717926}, {"text": "ACL05 MT workshop test set", "start_pos": 144, "end_pos": 170, "type": "DATASET", "confidence": 0.8195685029029847}]}, {"text": "The human judgments for an MT system are estimated by summing up each sentence's human overall score.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9907800555229187}]}, {"text": "shows the results indexed by different ngrams and tree depths.", "labels": [], "entities": []}, {"text": "We can see that the corpus-level correlation and the sentence-level correlation don't always correspond.", "labels": [], "entities": []}, {"text": "For example, the kernel dependency subtree metric achieves a very good performance in corpuslevel evaluation, but it has a poor performance in sentence-level evaluation.", "labels": [], "entities": []}, {"text": "Sentence-level correlation reflects the relative qualities of different hypotheses in a MT system, which does not indicate any information for the relative qualities of different systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9731962084770203}]}, {"text": "If we uniformly decrease or increase every hypothesis's automatic score in a MT system, the sentence-level correlation with human judgments will remain the same, but the corpus-level correlation will be changed.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9731595516204834}]}, {"text": "So we might possibly get inconsistent corpus-level and sentence-level correlations.", "labels": [], "entities": []}, {"text": "From the results, we can see that with the increase of n-grams length, the performance of BLEU and HWCM will first increase up to length 5, and then starts decreasing, where the optimal n-gram length of 5 corresponds to our usual setting for BLEU algorithm.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9720413684844971}]}, {"text": "This shows that corpus-level evaluation, compared with the sentence-level evaluation, is much less sensitive to the sparse data problem and thus leaves more space for making use of comprehensive evaluation metrics.", "labels": [], "entities": []}, {"text": "We speculate this is why the kernel dependency subtree metric achieves the best performance among all the metrics.", "labels": [], "entities": []}, {"text": "We can also see that HWCM and DSTM beat BLEU inmost cases and exhibit more stable performance.", "labels": [], "entities": [{"text": "HWCM", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.763390302658081}, {"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9943056702613831}]}, {"text": "An example hypothesis which was assigned a high score by HWCM but a low score by BLEU is shown in.", "labels": [], "entities": [{"text": "HWCM", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.8691277503967285}, {"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9979810118675232}]}, {"text": "In this particular sentence, the common head-modifier relations \"aboard \u2190 plane\" and \"plane \u2190 the\" caused a high headword chain overlap, but did not appear as common n-grams counted by BLEU.", "labels": [], "entities": [{"text": "headword chain overlap", "start_pos": 113, "end_pos": 135, "type": "METRIC", "confidence": 0.5553848445415497}, {"text": "BLEU", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.9578284621238708}]}, {"text": "The hypothesis is missing the word \"fifth\", but was nonetheless assigned a high score by human judges.", "labels": [], "entities": []}, {"text": "This is probably due to its fluency, which HWCM seems to capture better than BLEU.", "labels": [], "entities": [{"text": "HWCM", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8432671427726746}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9756510853767395}]}], "tableCaptions": [{"text": " Table 1: Correlation with Human Fluency Judg- ments for E14", "labels": [], "entities": [{"text": "Fluency Judg- ments", "start_pos": 33, "end_pos": 52, "type": "METRIC", "confidence": 0.8258010596036911}, {"text": "E14", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.5715193748474121}]}, {"text": " Table 2: Correlation with Human Fluency Judg- ments for E15", "labels": [], "entities": [{"text": "Human Fluency Judg- ments", "start_pos": 27, "end_pos": 52, "type": "METRIC", "confidence": 0.735765027999878}, {"text": "E15", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.5924579501152039}]}, {"text": " Table  5. We can see that the syntactic metrics achieve", "labels": [], "entities": []}, {"text": " Table 3: Correlation with Human Overall Judgments  for E14", "labels": [], "entities": [{"text": "E14", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.5773739218711853}]}, {"text": " Table 4: Correlation with Human Overall Judgments  for E15", "labels": [], "entities": [{"text": "E15", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.6229776740074158}]}, {"text": " Table 6: Corpus-level Correlation with Human  Overall Judgments (E9 E11 E12 E14 E15 E17 E22)", "labels": [], "entities": []}]}