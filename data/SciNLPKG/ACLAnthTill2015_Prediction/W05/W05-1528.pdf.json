{"title": [{"text": "k-NN for Local Probability Estimation in Generative Parsing Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a history-based generative parsing model which uses a k-nearest neighbour (k-NN) technique to estimate the model's parameters.", "labels": [], "entities": [{"text": "generative parsing", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.9090199172496796}]}, {"text": "Taking the output of abase n-best parser we use our model to re-estimate the log probability of each parse tree in the n-best list for sentences from the Penn Wall Street Journal treebank.", "labels": [], "entities": [{"text": "Penn Wall Street Journal treebank", "start_pos": 154, "end_pos": 187, "type": "DATASET", "confidence": 0.9713636755943298}]}, {"text": "By further decomposing the local probability distributions of the base model, enriching the set of conditioning features used to estimate the model's parameters, and using k-NN as opposed to the Witten-Bell estimation of the base model, we achieve an f-score of 89.2%, representing a 4% relative decrease in f-score error over the 1-best output of the base parser.", "labels": [], "entities": [{"text": "f-score error", "start_pos": 308, "end_pos": 321, "type": "METRIC", "confidence": 0.9059344530105591}]}], "introductionContent": [{"text": "This paper describes a generative probabilistic model for parsing, based on Collins (1999), which re-estimates the probability of each parse generated by an initial base parser) using memory-based techniques to estimate local probabilities.", "labels": [], "entities": [{"text": "parsing", "start_pos": 58, "end_pos": 65, "type": "TASK", "confidence": 0.9730647206306458}]}, {"text": "We used Bikel's re-implementation of the Collins parser) to produce the n-best parses of sentences from the Penn treebank.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 108, "end_pos": 121, "type": "DATASET", "confidence": 0.9874562919139862}]}, {"text": "We then recalculated the probability of each parse tree using a probabilistic model very similar to Collins (1999) Model 1.", "labels": [], "entities": []}, {"text": "In addition to the local estimation technique used, our model differs from Collins (1999) Model 1 in that we extend the feature sets used to predict parse structure to include more features from the parse history, and we further decompose some of the model's parameter classes.", "labels": [], "entities": [{"text": "Collins (1999) Model 1", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.7961533963680267}]}], "datasetContent": [{"text": "Our model is trained on sections 2 to 21 inclusive of the Penn WSJ treebank and tested on section 23.", "labels": [], "entities": [{"text": "Penn WSJ treebank", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.9368377526601156}]}, {"text": "We used sections 0, 1, 22 and 24 for validation.", "labels": [], "entities": [{"text": "validation", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.9279727339744568}]}, {"text": "We re-estimated the probability of each parse using our own baseline model, which is a replication of Collins Model 1.", "labels": [], "entities": [{"text": "Collins Model 1", "start_pos": 102, "end_pos": 117, "type": "DATASET", "confidence": 0.9599533081054688}]}, {"text": "We tested k-NN estimation on the head generation parameter class and the parameter classes for generating modifying nonterminals.", "labels": [], "entities": []}, {"text": "We further decomposed the two modifying nonterminal parameter classes.", "labels": [], "entities": []}, {"text": "outlines the parameter classes estimated using k-NN in the final model settings and shows the feature sets used for each parameter class as well as the constraint feature settings.: The parameter classes estimated using k-NN in the final model.", "labels": [], "entities": []}, {"text": "C H is the head child label, C p the parent constituent label, w p the headword, t p the head part-ofspeech (POS) tag.", "labels": [], "entities": []}, {"text": "Ci , w i and ti are the modifier's label, headword and head POS tag.", "labels": [], "entities": []}, {"text": "t gp is the grand-parent POS tag, C gp , C ggp , C gggp are the labels of the grandparent, great-grandparent and great-great-grandparent nodes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for sentences of less than or equal to  40 words, from section 23 of the Penn treebank. LP/LR  =Labelled Precision/Recall. CO99 M1 and M2 are  (Collins 1999) Models 1 and 2 respectively. Bikel 1- best is (Bikel, 2004). k-NN is our final k-NN model.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.9942601919174194}, {"text": "Labelled Precision/Recall", "start_pos": 114, "end_pos": 139, "type": "METRIC", "confidence": 0.8269586116075516}, {"text": "Bikel 1- best", "start_pos": 205, "end_pos": 218, "type": "METRIC", "confidence": 0.893626943230629}]}]}