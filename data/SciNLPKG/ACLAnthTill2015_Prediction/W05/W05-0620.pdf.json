{"title": [{"text": "Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.6321698824564616}]}], "abstractContent": [{"text": "In this paper we describe the CoNLL-2005 shared task on Semantic Role Labeling.", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7586198051770529}]}, {"text": "We introduce the specification and goals of the task, describe the data sets and evaluation methods, and present a general overview of the 19 systems that have contributed to the task, providing a comparative description and results.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the few last years there has been an increasing interest in shallow semantic parsing of natural language, which is becoming an important component in all kind of NLP applications.", "labels": [], "entities": [{"text": "shallow semantic parsing of natural language", "start_pos": 63, "end_pos": 107, "type": "TASK", "confidence": 0.7795496980349222}]}, {"text": "As a particular case, Semantic Role Labeling (SRL) is currently a welldefined task with a substantial body of work and comparative evaluation.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.8502715229988098}]}, {"text": "Given a sentence, the task consists of analyzing the propositions expressed by some target verbs of the sentence.", "labels": [], "entities": []}, {"text": "In particular, for each target verb all the constituents in the sentence which fill a semantic role of the verb have to be recognized.", "labels": [], "entities": []}, {"text": "Typical semantic arguments include Agent, Patient, Instrument, etc. and also adjuncts such as Locative, Temporal, Manner, Cause, etc.", "labels": [], "entities": [{"text": "Manner", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9256170392036438}]}, {"text": "Last year, the CoNLL-2004 shared task aimed at evaluating machine learning SRL systems based only on partial syntactic information.", "labels": [], "entities": [{"text": "machine learning SRL", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.5405305425326029}]}, {"text": "In) one may find a detailed review of the task and also a brief state-of-the-art on SRL previous to 2004.", "labels": [], "entities": [{"text": "SRL", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.74565589427948}]}, {"text": "Ten systems contributed to the task, which was evaluated using the PropBank corpus ( ).", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.9797048270702362}]}, {"text": "The best results were around 70 in F 1 measure.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9788696964581808}]}, {"text": "Though not directly comparable, these figures are substantially lower than the best results published up to date using full parsing as input information (F 1 slightly over 79).", "labels": [], "entities": [{"text": "F 1", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.9936947226524353}]}, {"text": "In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop).", "labels": [], "entities": []}, {"text": "Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus ().", "labels": [], "entities": [{"text": "FrameNet corpus", "start_pos": 89, "end_pos": 104, "type": "DATASET", "confidence": 0.9446329772472382}]}, {"text": "From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works.", "labels": [], "entities": []}, {"text": "Following last year's initiative, the CoNLL-2005 shared task 1 will concern again the recognition of semantic roles for the English language.", "labels": [], "entities": [{"text": "recognition of semantic roles", "start_pos": 86, "end_pos": 115, "type": "TASK", "confidence": 0.8078159391880035}]}, {"text": "Compared to the shared task of CoNLL-2004, the novelties introduced in the 2005 edition are: \u2022 Aiming at evaluating the contribution of full parsing in SRL, the complete syntactic trees given by two alternative parsers have been provided as input information for the task.", "labels": [], "entities": []}, {"text": "The rest of input information does not vary and corresponds to the levels of processing treated in the previous editions of the CoNLL shared task, i.e., words, PoS tags, base chunks, clauses, and named entities.", "labels": [], "entities": []}, {"text": "\u2022 The training corpus has been substantially enlarged.", "labels": [], "entities": []}, {"text": "This allows to test the scalability of learning-based SRL systems to big datasets and to compute learning curves to see how much data is necessary to train.", "labels": [], "entities": [{"text": "SRL", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9169954061508179}]}, {"text": "Again, we concentrate on the PropBank corpus ( ), which is the Wall Street Journal part of the Penn TreeBank corpus enriched with predicate-argument structures.", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.9425259530544281}, {"text": "Wall Street Journal part", "start_pos": 63, "end_pos": 87, "type": "DATASET", "confidence": 0.9591120481491089}, {"text": "Penn TreeBank corpus", "start_pos": 95, "end_pos": 115, "type": "DATASET", "confidence": 0.9023979107538859}]}, {"text": "\u2022 In order to test the robustness of the presented systems, a cross-corpora evaluation is performed using afresh test set from the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 131, "end_pos": 143, "type": "DATASET", "confidence": 0.8947731852531433}]}, {"text": "Regarding evaluation, two different settings were devised depending if the systems use the information strictly contained in the training data (closed challenge) or they make use of external sources of information and/or tools (open challenge).", "labels": [], "entities": []}, {"text": "The closed setting allows to compare systems under strict conditions, while the open setting aimed at exploring the contributions of other sources of information and the limits of the current learning-based systems on the SRL task.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 222, "end_pos": 230, "type": "TASK", "confidence": 0.928857147693634}]}, {"text": "At the end, all 19 systems took part in the closed challenge and none of them in the open challenge.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the general setting of the task.", "labels": [], "entities": []}, {"text": "Section 3 provides a detailed description of training, development and test data.", "labels": [], "entities": []}, {"text": "Participant systems are described and compared in section 4.", "labels": [], "entities": []}, {"text": "In particular, information about learning techniques, SRL strategies, and feature development is provided, together with performance results on the development and test sets.", "labels": [], "entities": [{"text": "SRL", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9917640089988708}]}, {"text": "Finally, section 5 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "A baseline rate was computed for the task.", "labels": [], "entities": []}, {"text": "It was produced using a system developed in the past shared task edition by Erik Tjong Kim Sang, from the University of Amsterdam, The Netherlands.", "labels": [], "entities": []}, {"text": "The baseline processor finds semantic roles based on the following seven rules: \u2022 Tag target verb and successive particles as V.", "labels": [], "entities": []}, {"text": "\u2022 Tag modal verbs in target verb chunk as AM-MOD.", "labels": [], "entities": []}, {"text": "\u2022 Tag first NP before target verb as A0.", "labels": [], "entities": []}, {"text": "\u2022 Tag first NP after target verb as A1.", "labels": [], "entities": [{"text": "Tag", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9419323205947876}]}, {"text": "\u2022 Tag that, which and who before target verb as R-A0.", "labels": [], "entities": []}, {"text": "\u2022 Switch A0 and A1, and R-A0 and R-A1 if the target verb is part of a passive VP chunk.", "labels": [], "entities": []}, {"text": "A VP chunk is considered in passive voice if it contains a form of to be and the verb does not end in ing.", "labels": [], "entities": []}, {"text": "presents the overall results obtained by the nineteen systems plus the baseline, on the development and test sets (i.e., Development, Test WSJ, Test Brown, and Test WSJ+Brown).", "labels": [], "entities": []}, {"text": "The systems are sorted by the performance on the combined WSJ+Brown test set.", "labels": [], "entities": [{"text": "WSJ+Brown test set", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.91262868642807}]}, {"text": "As it can be observed, all systems clearly outperformed the baseline.", "labels": [], "entities": []}, {"text": "There are seven systems with a final F 1 performance in the 75-78 range, seven more with performances in the 70-75 range, and five with a performance between 65 and 70.", "labels": [], "entities": [{"text": "F 1", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9318090677261353}]}, {"text": "The best performance was obtained by, which almost reached an F 1 at 80 in the WSJ test set and almost 78 in the combined test.", "labels": [], "entities": [{"text": "F 1", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9947274029254913}, {"text": "WSJ test set", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.9479368925094604}]}, {"text": "Their results on the WSJ test equal the best results published so far on this task and datasets), though they are not directly comparable due to a different setting in defining arguments not perfectly matching the predicted parse constituents.", "labels": [], "entities": [{"text": "WSJ test", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.8942283093929291}]}, {"text": "Since the evaluation in the shared task setting is more strict, we believe that the best results obtained in the shared task represent anew breakthrough in the SRL task.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 160, "end_pos": 168, "type": "TASK", "confidence": 0.8998217284679413}]}, {"text": "It is also quite clear that the systems using combination are better than the individuals.", "labels": [], "entities": []}, {"text": "It is worth noting that the first 4 systems are combined.", "labels": [], "entities": []}, {"text": "The  The best results in the CoNLL-2005 shared task are 10 points better than those of last year edition.", "labels": [], "entities": [{"text": "CoNLL-2005 shared task", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.701623280843099}]}, {"text": "This increase in performance should be attributed to a combination of the following factors: 1) training sets have been substantially enlarged; 2) predicted parse trees are available as input information; and 3) more sophisticated combination schemes have been implemented.", "labels": [], "entities": []}, {"text": "In order to have a more clear idea of the impact of enriching the syntactic information, we refer to), who developed an individual system based only on partial parsing (\"upc\" input information).", "labels": [], "entities": []}, {"text": "That system performed F 1 =73.57 on the development set, which is 2.18 points below the F 1 =75.75 obtained by the same architecture using full parsing, and 4.47 points below the best performing combined system on the development set ().", "labels": [], "entities": [{"text": "F 1", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9889906942844391}, {"text": "F 1 =75.75", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9636984467506409}]}, {"text": "Comparing the results across development and WSJ test corpora, we find that, with two exceptions, all systems experienced a significant increase in performance (normally between 1 and 2 F 1 points).", "labels": [], "entities": [{"text": "WSJ test corpora", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.7729901274045309}]}, {"text": "This fact maybe attributed to the different levels of difficulty found across WSJ sections.", "labels": [], "entities": []}, {"text": "The linguistic processors and parsers perform slightly worse in the development set.", "labels": [], "entities": []}, {"text": "As a consequence, the matching between parse nodes and actual arguments is lower.", "labels": [], "entities": []}, {"text": "Regarding the evaluation using the Brown test set, all systems experienced a severe drop in performance (about 10 F 1 points), even though the baseline on the Brown test set is higher than that of the WSJ test set.", "labels": [], "entities": [{"text": "Brown test set", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.8734775980313619}, {"text": "F 1 points", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.9672192931175232}, {"text": "Brown test set", "start_pos": 159, "end_pos": 173, "type": "DATASET", "confidence": 0.9108214577039083}, {"text": "WSJ test set", "start_pos": 201, "end_pos": 213, "type": "DATASET", "confidence": 0.9713718096415201}]}, {"text": "As already said in previous sections, all the linguistic processors, from PoS tagging to full parsing, showed a much lower performance than in the WSJ test set, evincing that their performance cannot be extrapolated across corpora.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.6665014922618866}, {"text": "WSJ test set", "start_pos": 147, "end_pos": 159, "type": "DATASET", "confidence": 0.951225737730662}]}, {"text": "Presumably, this fact is the main responsible of the performace drop, though we do not discard an additional overfitting effect due to the design of specific features that do not generalize well.", "labels": [], "entities": []}, {"text": "More im-portantly, this results impose (again) a severe criticism on the current pipelined architecture for Natural Language Processing.", "labels": [], "entities": []}, {"text": "Error propagation and amplification through the chained modules make the final output generalize very badly when changing the domain of application.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy (%) of PoS taggers.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991434812545776}, {"text": "PoS taggers", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.7472909688949585}]}, {"text": " Table 3: Results of the syntactic parsers on the development, and WSJ and Brown test sets. Unlike in full  parsing, the figures have been computed on a strict evaluation basis with respect to punctuation.", "labels": [], "entities": [{"text": "WSJ and Brown test sets", "start_pos": 67, "end_pos": 90, "type": "DATASET", "confidence": 0.7244354546070099}]}, {"text": " Table 6: Overall precision, recall and F 1 rates obtained by the 19 participating systems in the CoNLL-2005  shared task on the development and test sets. Systems sorted by F 1 score on the WSJ+Brown test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9996440410614014}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996628761291504}, {"text": "F 1 rates", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9904686212539673}, {"text": "F 1 score", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9748386939366659}, {"text": "WSJ+Brown test set", "start_pos": 191, "end_pos": 209, "type": "DATASET", "confidence": 0.9430073499679565}]}]}