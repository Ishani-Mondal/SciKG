{"title": [{"text": "On Some Pitfalls in Automatic Evaluation and Significance Testing for MT", "labels": [], "entities": [{"text": "Automatic Evaluation and Significance Testing", "start_pos": 20, "end_pos": 65, "type": "TASK", "confidence": 0.6074354588985443}, {"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9217726588249207}]}], "abstractContent": [{"text": "We investigate some pitfalls regarding the discriminatory power of MT evaluation metrics and the accuracy of statistical significance tests.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9064684212207794}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9986206293106079}]}, {"text": "Ina discriminative rerank-ing experiment for phrase-based SMT we show that the NIST metric is more sensitive than BLEU or F-score despite their incorporation of aspects of fluency or meaning adequacy into MT evaluation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.8131502866744995}, {"text": "NIST metric", "start_pos": 79, "end_pos": 90, "type": "DATASET", "confidence": 0.7630847096443176}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9985309839248657}, {"text": "F-score", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9390267729759216}, {"text": "MT evaluation", "start_pos": 205, "end_pos": 218, "type": "TASK", "confidence": 0.907103568315506}]}, {"text": "In an experimental comparison of two statistical significance tests we show that p-values are estimated more conservatively by approximate randomization than by boot-strap tests, thus increasing the likelihood of type-I error for the latter.", "labels": [], "entities": []}, {"text": "We point out a pitfall of randomly assessing significance in multiple pairwise comparisons, and conclude with a recommendation to combine NIST with approximate random-ization, at more stringent rejection levels than is currently standard.", "labels": [], "entities": []}], "introductionContent": [{"text": "Rapid and accurate detection of result differences is crucial in system development and system benchmarking.", "labels": [], "entities": []}, {"text": "In both situations a multitude of systems or system variants has to be evaluated, so it is highly desirable to employ automatic evaluation measures for detection of result differences, and statistical hypothesis tests to assess the significance of the detected differences.", "labels": [], "entities": []}, {"text": "When evaluating subtle differences between system variants in development, or when benchmarking multiple systems, result differences maybe very small in magnitude.", "labels": [], "entities": []}, {"text": "This imposes strong requirements on both automatic evaluation measures and statistical significance tests: Evaluation measures are needed that have high discriminative power and yet are sensitive to the interesting aspects of the evaluation task.", "labels": [], "entities": []}, {"text": "Significance tests are required to be powerful and yet accurate, i.e., if there are significant differences they should be able to assess them, but not if there are none.", "labels": [], "entities": []}, {"text": "In the area of statistical machine translation (SMT), recently a combination of the BLEU evaluation metric () and the bootstrap method for statistical significance testing has become popular).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 15, "end_pos": 52, "type": "TASK", "confidence": 0.8493564824263254}, {"text": "BLEU evaluation metric", "start_pos": 84, "end_pos": 106, "type": "METRIC", "confidence": 0.9175426165262858}]}, {"text": "Given the current practice of reporting result differences as small as .3% in BLEU score, assessed at confidence levels as low as 70%, questions arise concerning the sensitivity of the employed evaluation metrics and the accuracy of the employed significance tests, especially when result differences are small.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9728359580039978}, {"text": "accuracy", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.9982153177261353}]}, {"text": "We believe that is important to accurately detect such small-magnitude differences in order to understand how to improve systems and technologies, even though such differences may not matter in current applications.", "labels": [], "entities": []}, {"text": "In this paper we will investigate some pitfalls that arise in automatic evaluation and statistical significance testing in MT research.", "labels": [], "entities": [{"text": "MT research", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.9403642416000366}]}, {"text": "The first pitfall concerns the discriminatory power of automatic evaluation measures.", "labels": [], "entities": []}, {"text": "In the following, we compare the sensitivity of three intrinsic evaluation measures that differ with respect to their focus on different aspects of translation.", "labels": [], "entities": []}, {"text": "We consider the well-known BLEU score () which emphasizes fluency by incorporating matches of high n-grams.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9824126958847046}]}, {"text": "Furthermore, we consider an F-score measure that is adapted from dependency-based parsing) and sentence-condensation (.", "labels": [], "entities": [{"text": "F-score measure", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.9697826206684113}, {"text": "dependency-based parsing", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.6395705938339233}]}, {"text": "This measure matches grammatical dependency relations of parses for system output and reference translations, and thus emphasizes semantic aspects of translational adequacy.", "labels": [], "entities": []}, {"text": "As a third measure we consider NIST, which favors lexical choice over word order and does not take structural information into account.", "labels": [], "entities": []}, {"text": "On an experimental evaluation on a reranking experiment we found that only NIST was sensitive enough to detect small result differences, whereas BLEU and Fscore produced result differences that were statistically not significant.", "labels": [], "entities": [{"text": "NIST", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.8739849328994751}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9958482980728149}, {"text": "Fscore", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9499287605285645}]}, {"text": "A second pitfall addressed in this paper concerns the relation of power and accuracy of significance tests.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9992475509643555}]}, {"text": "In situations where the employed evaluation measure produces small result differences, the most powerful significance testis demanded to assess statistical significance of the results.", "labels": [], "entities": []}, {"text": "However, accuracy of the assessments of significance is seldom questioned.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9991227984428406}]}, {"text": "In the following, we will take a closer look at the bootstrap test and compare it with the related technique of approximate randomization).", "labels": [], "entities": []}, {"text": "In an experimental evaluation on our reranking data we found that approximate randomization estimated p-values more conservatively than the bootstrap, thus increasing the likelihood of type-I error for the latter test.", "labels": [], "entities": []}, {"text": "Lastly, we point out a common mistake of randomly assessing significance in multiple pairwise comparisons.", "labels": [], "entities": []}, {"text": "This is especially relevant in k-fold pairwise comparisons of systems or system variants where k is high.", "labels": [], "entities": []}, {"text": "Taking this multiplicity problem into account, we conclude with a recommendation of a combination of NIST for evaluation and the approximate randomization test for significance testing, at more stringent rejection levels than is currently standard in the MT literature.", "labels": [], "entities": [{"text": "NIST", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.8358021974563599}, {"text": "MT", "start_pos": 255, "end_pos": 257, "type": "TASK", "confidence": 0.9453075528144836}]}, {"text": "This is especially important in situations where multiple pairwise comparisons are conducted, and small result differences are expected.", "labels": [], "entities": []}], "datasetContent": [{"text": "Reranking for Phrase-Based SMT The experimental setup we employed to compare evaluation measures and significance tests is a discriminative reranking experiment on 1000-best lists of a phrase-based SMT system.", "labels": [], "entities": [{"text": "Phrase-Based SMT", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.5099675059318542}, {"text": "SMT", "start_pos": 198, "end_pos": 201, "type": "TASK", "confidence": 0.806125819683075}]}, {"text": "Our system is a re-implementation of the phrase-based system described in, and uses publicly available components for word alignment  Phrase-extraction follows and was implemented by the authors: First, the word aligner is applied in both translation directions, and the intersection of the alignment matrices is built.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 118, "end_pos": 132, "type": "TASK", "confidence": 0.7364606261253357}]}, {"text": "Then, the alignment is extended by adding immediately adjacent alignment points and alignment points that align previously unaligned words.", "labels": [], "entities": []}, {"text": "From this many-to-many alignment matrix, phrases are extracted according to a contiguity requirement that states that words in the source phrase are aligned only with words in the target phrase, and vice versa.", "labels": [], "entities": []}, {"text": "Discriminative reranking on a 1000-best list of translations of the SMT system uses an 1 regularized log-linear model that combines a standard maximum-entropy estimator with an efficient, incremental feature selection technique for 1 regularization ().", "labels": [], "entities": [{"text": "Discriminative reranking", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7576134502887726}, {"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9780958890914917}]}, {"text": "Training data are defined as pairs {(s j , t j )} m j=1 of source sentences s j and gold-standard translations t j that are determined as the translations in the 1000-best list that best match a given reference translation.", "labels": [], "entities": []}, {"text": "The objective function to be minimized is the conditional log-likelihood L(\u03bb) subject to a regularization term R(\u03bb), where T (s) is the set of 1000-best translations for sentence s, \u03bb is a vector or log-parameters, and The features employed in our experiments consist of 8 features corresponding to system components (distortion model, language model, phrasetranslations, lexical weights, phrase penalty, word penalty) as provided by PHARAOH, together with a multitude of overlapping phrase features.", "labels": [], "entities": [{"text": "PHARAOH", "start_pos": 434, "end_pos": 441, "type": "DATASET", "confidence": 0.8511890172958374}]}, {"text": "For example, fora phrase-table of phrases consisting of maximally 3 words, we allow all 3-word phrases and 2-word phrases as features.", "labels": [], "entities": []}, {"text": "Since bigram features can overlap, information about trigrams can be gathered by composing bigram features even if the actual trigram is not seen in the training data.", "labels": [], "entities": []}, {"text": "Feature selection makes it possible to employ and evaluate a large number of features, without concerns about redundant or irrelevant features hampering generalization performance.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.72977015376091}]}, {"text": "The 1 regularizer is defined by the weighted 1 -norm of the parameters where \u03b3 is a regularization coefficient, and n is number of parameters.", "labels": [], "entities": []}, {"text": "This regularizer penalizes overly large parameter values in their absolute values, and tends to force a subset of the parameters to be exactly zero at the optimum.", "labels": [], "entities": []}, {"text": "This fact leads to a natural integration of regularization into incremental feature selection as follows: Assuming a tendency of the 1 regularizer to produce a large number of zero-valued parameters at the function's optimum, we start with all-zero weights, and incrementally add features to the model only if adjusting their parameters away from zero sufficiently decreases the optimization criterion.", "labels": [], "entities": []}, {"text": "Since every non-zero weight added to the model incurs a regularizer penalty of \u03b3|\u03bb i |, it only makes sense to add a feature to the model if this penalty is outweighed by the reduction in negative log-likelihood.", "labels": [], "entities": []}, {"text": "Thus features considered for selection have to pass the following test: This gradient testis applied to each feature and at each step the features that pass the test with maximum magnitude are added to the model.", "labels": [], "entities": []}, {"text": "This provides both efficient and accurate estimation with large feature sets.", "labels": [], "entities": []}, {"text": "Work on discriminative reranking has been reported before by, , and . The main purpose of our reranking experiments is to have a system that can easily be adjusted to yield system variants that differ at controllable amounts.", "labels": [], "entities": []}, {"text": "For quick experimental turnaround we selected the training and test data from sentences with 5 to 15 words, resulting in a training set of 160,000 sentences, and a development set of 2,000 sentences.", "labels": [], "entities": []}, {"text": "The phrase-table employed was restricted to phrases of maximally 3 words, resulting in 200,000 phrases.", "labels": [], "entities": []}, {"text": "The intrinsic evaluation measures used in our experiments are the well-known BLEU () and NIST) metrics, and an F-score measure that adapts evaluation techniques from dependency-based parsing) and sentence-condensation () to machine translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9965217113494873}, {"text": "F-score measure", "start_pos": 111, "end_pos": 126, "type": "METRIC", "confidence": 0.9619086980819702}, {"text": "machine translation", "start_pos": 224, "end_pos": 243, "type": "TASK", "confidence": 0.7504337728023529}]}, {"text": "All of these measures Set c = 0 Compute actual statistic of score differences |S X \u2212 S Y | on test data For random shuffles r = 0, . .", "labels": [], "entities": []}, {"text": ", R For sentences in test set Shuffle variable tuples between system X and Y with probability 0.5 Reject null hypothesis if p is less than or equal to specified rejection level.", "labels": [], "entities": []}, {"text": "BLEU and NIST both consider n-grams in source and reference strings as matching entities.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7071393728256226}, {"text": "NIST", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.9484362006187439}]}, {"text": "BLEU weighs all n-grams equally whereas NIST puts more weight on n-grams that are more informative, i.e., occur less frequently.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9828220009803772}, {"text": "NIST", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.8783706426620483}]}, {"text": "This results in BLEU favoring matches in larger n-grams, corresponding to giving more credit to correct word order.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9978358149528503}]}, {"text": "NIST weighs lower n-grams more highly, thus it gives more credit to correct lexical choice than to word order.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8389496207237244}]}, {"text": "F-score is computed by parsing reference sentences and SMT outputs, and matching grammatical dependency relations.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9234143495559692}, {"text": "parsing reference sentences and SMT outputs", "start_pos": 23, "end_pos": 66, "type": "TASK", "confidence": 0.7974364558855692}]}, {"text": "The reported value is the harmonic mean of precision and recall, which is defined as (2 \u00d7 precision \u00d7 recall )/( precision + recall ).", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9994055032730103}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9982788562774658}, {"text": "precision \u00d7 recall )/( precision + recall )", "start_pos": 90, "end_pos": 133, "type": "METRIC", "confidence": 0.80747240036726}]}, {"text": "Precision is the ratio of matching dependency relations to the total number of dependency relations in the parse for the system translation, and recall is the ratio of matches to the total number of dependency relations in the parse for the reference translation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.985978901386261}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9993569254875183}]}, {"text": "The goal of this measure is to focus on aspects of meaning in measuring similarity of system translations to reference translations, and to allow for meaning-preserving word order variation.", "labels": [], "entities": []}, {"text": "Evaluation results fora comparison of reranking against a baseline model that only includes features corresponding to the 8 system components are shown in.", "labels": [], "entities": []}, {"text": "Since the task is a comparison of system variants for development, all results are reported on the development set of 2,000 examples of length 5-15.", "labels": [], "entities": []}, {"text": "The reranking model achieves an increase in NIST score of .15 units, whereas BLEU and F-score decrease by .3% and .2% respectively.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.8181682527065277}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9995037317276001}, {"text": "F-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9951600432395935}]}, {"text": "However, as measured by the statistical significance tests described below, the differences in BLEU and F-scores are not statistically significant with p-values exceeding the standard rejection level of .05.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9992448091506958}, {"text": "F-scores", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9926405549049377}]}, {"text": "In contrast, the differences in NIST score are highly significant.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.5685209035873413}]}, {"text": "These findings correspond to results reported in showing a higher sensitivity of NIST versus BLEU to small result differences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9976444840431213}]}, {"text": "Taking also the results from F-score matching in account, we can conclude that similarity measures that are based on matching more complex entities (such as BLEU's higher n-grams or F's grammatical relations) are not as sensitive to small result differences as scoring techniques that are able to distinguish models by matching simpler entities (such as NIST's focus on lexical choice).", "labels": [], "entities": [{"text": "F-score matching", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.6683006286621094}, {"text": "BLEU", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.9691875576972961}]}, {"text": "Furthermore, we get an indication that differences of .3% in BLEU score or .2% in F-score might not be large enough to conclude statistical significance of result differences.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9813069999217987}, {"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9982056617736816}]}, {"text": "This leads to questions of power and accuracy of the employed statistical significance tests which will be addressed in the next section.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9995169639587402}]}], "tableCaptions": [{"text": " Table 1: NIST, BLEU, F-scores for reranker and baseline on development set", "labels": [], "entities": [{"text": "NIST", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.6122074127197266}, {"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9990848302841187}, {"text": "F-scores", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9918885827064514}]}, {"text": " Table 2: NIST scores for equivalent systems under bootstrap and approximate randomization tests.", "labels": [], "entities": []}]}