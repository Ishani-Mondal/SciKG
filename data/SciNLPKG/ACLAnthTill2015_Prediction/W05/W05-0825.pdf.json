{"title": [], "abstractContent": [{"text": "In this paper, we present a phrase extraction algorithm using a translation lexicon , a fertility model, and a simple distortion model.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8488353788852692}]}, {"text": "Except these models, we do not need explicit word alignments for phrase extraction.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.8266501128673553}]}, {"text": "For each phrase pair (a block), a bilingual lexicon based score is computed to estimate the translation quality between the source and target phrase pairs; a fertility score is computed to estimate how good the lengths are matched between phrase pairs; a center distortion score is computed to estimate the relative position divergence between the phrase pairs.", "labels": [], "entities": [{"text": "fertility score", "start_pos": 158, "end_pos": 173, "type": "METRIC", "confidence": 0.970363587141037}, {"text": "center distortion score", "start_pos": 255, "end_pos": 278, "type": "METRIC", "confidence": 0.8582130670547485}]}, {"text": "We presented the results and our experience in the shared tasks on French-English.", "labels": [], "entities": []}], "introductionContent": [{"text": "Phrase extraction becomes a key component in today's state-of-the-art statistical machine translation systems.", "labels": [], "entities": [{"text": "Phrase extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9493127763271332}, {"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.625024676322937}]}, {"text": "With a longer context than unigram, phrase translation models have flexibilities of modelling local word-reordering, and are less sensitive to the errors made from preprocessing steps including word segmentations and tokenization.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8321196138858795}, {"text": "word segmentations", "start_pos": 194, "end_pos": 212, "type": "TASK", "confidence": 0.7017492651939392}]}, {"text": "However, most of the phrase extraction algorithms rely on good word alignments.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8042578101158142}, {"text": "word alignments", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7479117810726166}]}, {"text": "A widely practiced approach explained in details in, and is to get word alignments from two directions: source to target and target to source; the intersection or union operation is applied to get refined word alignment with pre-designed heuristics fixing the unaligned words.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7043132334947586}, {"text": "word alignment", "start_pos": 205, "end_pos": 219, "type": "TASK", "confidence": 0.7256239503622055}]}, {"text": "With this refined word alignment, the phrase extraction fora given source phrase is essentially to extract the target candidate phrases in the target sentence by searching the left and right projected boundaries.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7024857550859451}]}, {"text": "In (), they treat phrase alignment as a sentence splitting problem: given a source phrase, find the boundaries of the target phrase such that the overall sentence alignment lexicon probability is optimal.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.7838003635406494}, {"text": "sentence splitting", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7402470409870148}, {"text": "sentence alignment lexicon", "start_pos": 154, "end_pos": 180, "type": "TASK", "confidence": 0.7780998547871908}]}, {"text": "We generalize it in various ways, esp. by using a fertility model to get a better estimation of phrase lengths, and a phrase level distortion model.", "labels": [], "entities": []}, {"text": "In our proposed algorithm, we do not need explicit word alignment for phrase extraction.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.7450985014438629}, {"text": "phrase extraction", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.8292428255081177}]}, {"text": "Thereby it avoids the burden of testing and comparing different heuristics especially for some language specific ones.", "labels": [], "entities": []}, {"text": "On the other hand, the algorithm has such flexibilities that one can incorporate word alignment and heuristics in several possible stages within this proposed framework to further improve the quality of phrase pairs.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.7355422079563141}]}, {"text": "In this way, our proposed algorithm is more generalized than the usual word alignment based phrase extraction algorithms.", "labels": [], "entities": [{"text": "word alignment based phrase extraction", "start_pos": 71, "end_pos": 109, "type": "TASK", "confidence": 0.8252427279949188}]}, {"text": "The paper is structured as follows: in section 2, The concept of blocks is explained; in section 3, a dynamic programming approach is model the width of the block; in section 4, a simple center distortion of the block; in section 5, the lexicon model; the complete algorithm is in section 6; in section 7, our experience and results using the proposed approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our system is based on the IBM Model-4 parameters.", "labels": [], "entities": [{"text": "IBM Model-4 parameters", "start_pos": 27, "end_pos": 49, "type": "DATASET", "confidence": 0.8918315172195435}]}, {"text": "We train IBM Model 4 with a scheme of 1 7 2 0 h 7 3 0 4 3 using GIZA++).", "labels": [], "entities": [{"text": "IBM Model 4", "start_pos": 9, "end_pos": 20, "type": "DATASET", "confidence": 0.9427317579587301}]}, {"text": "The maximum fertility for an English word is 3.", "labels": [], "entities": []}, {"text": "All the data is used as given, i.e. we do not have any preprocessing of the English-French data.", "labels": [], "entities": []}, {"text": "The word alignment provided in the workshop is not used in our evaluations.", "labels": [], "entities": []}, {"text": "The language model is provided by the workshop, and we do not use other language models.", "labels": [], "entities": []}, {"text": "The French phrases up to 8-gram in the development and test sets are extracted with top-3 candidate English phrases.", "labels": [], "entities": []}, {"text": "There are in total 2.6 million phrase pairs 1 extracted for both development set and the unseen test set.", "labels": [], "entities": []}, {"text": "We did minimal tuning of the parameters in the pharaoh decoder) settings, simply to balance the length penalty for Bleu score.", "labels": [], "entities": [{"text": "length", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9894486665725708}, {"text": "Bleu score", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9560982286930084}]}, {"text": "Most of the weights are left as they are given: [ttable-limit]=20, [ttable-threshold]=0.01, Our phrase table is to be released to public in this workshop  In, setting s 1 was our submission without using the inverse relative frequency of P rf (e i+k i |f j+l j ).", "labels": [], "entities": []}, {"text": "s 2 is using all the seven scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 shows the algorithm's performance  on several settings for the seven basic scores pro- vided in section 6.", "labels": [], "entities": []}]}