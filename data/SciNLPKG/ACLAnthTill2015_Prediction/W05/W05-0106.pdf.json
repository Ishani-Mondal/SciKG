{"title": [{"text": "Making Hidden Markov Models More Transparent", "labels": [], "entities": []}], "abstractContent": [{"text": "Understanding the decoding algorithm for hidden Markov models is a difficult task for many students.", "labels": [], "entities": []}, {"text": "A comprehensive understanding is difficult to gain from static state transition diagrams and tables of observation production probabilities.", "labels": [], "entities": []}, {"text": "We have built a number of visualizations depicting a hidden Markov model for part-of-speech tagging and the operation of the Viterbi algorithm.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.7059048116207123}]}, {"text": "The visualizations are designed to help students grasp the operation of the HMM.", "labels": [], "entities": []}, {"text": "In addition, we have found that the displays are useful as de-bugging tools for experienced researchers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hidden Markov Models (HMMs) are an important part of the natural language processing toolkit and are often one of the first stochastic generation models that students 1 encounter.", "labels": [], "entities": []}, {"text": "The corresponding Viterbi algorithm is also often the first example of dynamic programming that students encounter.", "labels": [], "entities": []}, {"text": "Thus, HMMs provide an opportunity to start students on the correct path of understanding stochastic models, not simply treating them as black boxes.", "labels": [], "entities": []}, {"text": "Unfortunately, static state transition diagrams, tables of probability values, and lattice diagrams are not enough for many students.", "labels": [], "entities": []}, {"text": "They have a general idea of how a HMM works but often have common misconceptions.", "labels": [], "entities": []}, {"text": "For example, we have found that students often believe that as the Viterbi algorithm calculates joint state sequence observation sequence probabilities, the best state sequence so far is always a prefix of global best path.", "labels": [], "entities": []}, {"text": "This is of course false.", "labels": [], "entities": []}, {"text": "Working along example to show this is very tedious and thus text books seldom provide such examples.", "labels": [], "entities": []}, {"text": "Even for practitioners, HMMs are often opaque in that the cause of a mis-tagging error is often left uncharacterized.", "labels": [], "entities": []}, {"text": "A display would be helpful to pinpoint why an HMM chose an incorrect state sequence instead of the correct one.", "labels": [], "entities": []}, {"text": "Below we describe two displays that attempt to remedy the above mentioned problems and we discuss a Java implementation of these displays in the context of a part-of-speech tagging HMM.", "labels": [], "entities": [{"text": "part-of-speech tagging HMM", "start_pos": 158, "end_pos": 184, "type": "TASK", "confidence": 0.7175228794415792}]}, {"text": "The system is freely available and has an XML model specification that allows models calculated by other methods to be viewed.", "labels": [], "entities": []}, {"text": "(A standard maximum likelihood estimation was implemented and can be used to create models from tagged data. A model is also provided.) shows a snapshot of our first display.", "labels": [], "entities": []}, {"text": "It contains three kinds of information: most likely path for input, transition probabilities, and history of most likely prefixes for each observation index in the Viterbi lattice.", "labels": [], "entities": []}, {"text": "The user can input text at the bottom of the display, e.g., Pelham pointed out that Georgia voters rejected the bill.", "labels": [], "entities": []}, {"text": "The system then runs Viterbi and animates the search through all possible state sequences and displays the best state sequence prefix as it works its way through the observation 32  The user enters a sequence on the top text field and presses enter, the sequence is tagged and displayed in both the top and bottom text fields.", "labels": [], "entities": []}, {"text": "Finally, the user changes any incorrect tags in the top text field and presses enter and the probability ratio bars are then displayed.", "labels": [], "entities": []}, {"text": "sequence from left to right (these are lines connecting the states in).", "labels": [], "entities": []}, {"text": "At any point, the student can mouse-over a state to see probabilities for transitions out of that state (this is the bar graph in).", "labels": [], "entities": []}, {"text": "Finally, the history of most likely prefixes is displayed (this history appears below the bar graph in).", "labels": [], "entities": []}, {"text": "We mentioned that students often falsely believe that the most likely prefix is extended monotonically.", "labels": [], "entities": []}, {"text": "By seeing the path through the states reconfigure itself in the middle of the observation sequence and by looking at the prefix history, a student has a good chance of dispelling the false belief of monotonicity.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}