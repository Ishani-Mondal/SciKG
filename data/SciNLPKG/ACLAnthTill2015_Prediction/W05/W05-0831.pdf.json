{"title": [{"text": "Novel Reordering Approaches in Phrase-Based Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrase-Based Statistical Machine Translation", "start_pos": 31, "end_pos": 75, "type": "TASK", "confidence": 0.744507797062397}]}], "abstractContent": [{"text": "This paper presents novel approaches to reordering in phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 54, "end_pos": 98, "type": "TASK", "confidence": 0.5938325747847557}]}, {"text": "We perform consistent reordering of source sentences in training and estimate a statistical translation model.", "labels": [], "entities": []}, {"text": "Using this model, we follow a phrase-based monotonic machine translation approach, for which we develop an efficient and flexible reordering framework that allows to easily introduce different reordering constraints.", "labels": [], "entities": [{"text": "phrase-based monotonic machine translation", "start_pos": 30, "end_pos": 72, "type": "TASK", "confidence": 0.6212162971496582}]}, {"text": "In translation, we apply source sentence reordering on word level and use a reordering automaton as input.", "labels": [], "entities": []}, {"text": "We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints.", "labels": [], "entities": []}, {"text": "We further add weights to the reordering automata.", "labels": [], "entities": []}, {"text": "We present detailed experimental results and show that reordering significantly improves translation quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reordering is of crucial importance for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8200913369655609}]}, {"text": "Already () use full unweighted permutations on the level of source words in their early weighted finite-state transducer approach which implemented single-word based translation using conditional probabilities.", "labels": [], "entities": []}, {"text": "Ina refinement with additional phrase-based models, () define a probability distribution overall possible permutations of source sentence phrases and prune the resulting automaton to reduce complexity.", "labels": [], "entities": []}, {"text": "A second category of finite-state translation approaches uses joint instead of conditional probabilities.", "labels": [], "entities": [{"text": "finite-state translation", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.7127363234758377}]}, {"text": "Many joint probability approaches originate in speech-to-speech translation as they are the natural choice in combination with speech recognition models.", "labels": [], "entities": [{"text": "speech-to-speech translation", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.7319545447826385}]}, {"text": "The automated transducer inference techniques OMEGA) and GIATI) work on phrase level, but ignore the reordering problem from the view of the model.", "labels": [], "entities": [{"text": "GIATI", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.8760843873023987}]}, {"text": "Without reordering both in training and during search, sentences can only be translated properly into a language with similar word order.", "labels": [], "entities": []}, {"text": "In () weighted reordering has been applied to target sentences since defining a permutation model on the source side is impractical in combination with speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.7161469012498856}]}, {"text": "In order to reduce the computational complexity, this approach considers only a set of plausible reorderings seen on training data.", "labels": [], "entities": []}, {"text": "Most other phrase-based statistical approaches like the Alignment Template system of rely on (local) reorderings which are implicitly memorized with each pair of source and target phrases in training.", "labels": [], "entities": [{"text": "Alignment Template", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.820708304643631}]}, {"text": "Additional reorderings on phrase level are fully integrated into the decoding process, which increases the complexity of the system and makes it hard to modify.", "labels": [], "entities": []}, {"text": "reviewed two types of reordering constraints for this type of translation systems.", "labels": [], "entities": []}, {"text": "In our work we follow a phrase-based translation approach, applying source sentence reordering on word level.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.7287493348121643}]}, {"text": "We compute a reordering graph ondemand and take it as input for monotonic translation.", "labels": [], "entities": []}, {"text": "This approach is modular and allows easy introduction of different reordering constraints and probabilistic dependencies.", "labels": [], "entities": []}, {"text": "We will show that it performs at least as well as the best statistical machine translation system at the IWSLT Evaluation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.6218274533748627}, {"text": "IWSLT Evaluation", "start_pos": 105, "end_pos": 121, "type": "DATASET", "confidence": 0.8803496956825256}]}, {"text": "In the next section we briefly review the basic theory of our translation system based on weighted finite-state transducers (WFST).", "labels": [], "entities": []}, {"text": "3 we introduce new methods for reordering and alignment monotonization in training.", "labels": [], "entities": [{"text": "alignment monotonization", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.9757420420646667}]}, {"text": "To compare different reordering constraints used in the translation search process we develop an on-demand computable framework for permutation models in Sec.", "labels": [], "entities": [{"text": "translation search", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.9730798304080963}]}, {"text": "4. In the same section we also define and analyze unrestricted and restricted permutations with some of them being first published in this paper.", "labels": [], "entities": []}, {"text": "We conclude the paper by presenting and discussing a rich set of experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the automatic evaluation, we used the criteria from the IWSLT evaluation campaign (), namely word error rate (WER), positionindependent word error rate (PER), and the BLEU and NIST scores ().", "labels": [], "entities": [{"text": "IWSLT evaluation campaign", "start_pos": 60, "end_pos": 85, "type": "DATASET", "confidence": 0.90054718653361}, {"text": "word error rate (WER)", "start_pos": 97, "end_pos": 118, "type": "METRIC", "confidence": 0.9206958611806234}, {"text": "positionindependent word error rate (PER)", "start_pos": 120, "end_pos": 161, "type": "METRIC", "confidence": 0.892939703805106}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9964607357978821}, {"text": "NIST", "start_pos": 180, "end_pos": 184, "type": "DATASET", "confidence": 0.8322982788085938}]}, {"text": "The two scores measure accuracy, i. e. larger scores are better.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.999610960483551}]}, {"text": "The error rates and scores were computed with respect to multiple reference transla- tions, when they were available.", "labels": [], "entities": []}, {"text": "To indicate this, we will label the error rate acronyms with an m.", "labels": [], "entities": []}, {"text": "Both training and evaluation were performed using corpora and references in lowercase and without punctuation marks.", "labels": [], "entities": []}, {"text": "We used reordering and alignment monotonization in training as described in Sec.", "labels": [], "entities": [{"text": "alignment monotonization", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.9506678581237793}]}, {"text": "3. To estimate the matrices of local alignment costs for the sentence pairs in the training corpus we used the state occupation probabilities of GIZA++ IBM-4 model training and interpolated the probabilities of source-to-target and target-to-source training directions.", "labels": [], "entities": [{"text": "GIZA++ IBM-4 model training", "start_pos": 145, "end_pos": 172, "type": "DATASET", "confidence": 0.8915324211120605}]}, {"text": "After that we estimated a smoothed 4-gram language model on the level of bilingual tuples f j , \u02dc e j and represented it as a finite-state transducer.", "labels": [], "entities": []}, {"text": "When translating, we applied moderate beam pruning to the search automaton only when using reordering constraints with window sizes larger than 3.", "labels": [], "entities": [{"text": "translating", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.970427930355072}]}, {"text": "For very large window sizes we also varied the pruning thresholds depending on the length of the input sentence.", "labels": [], "entities": []}, {"text": "Pruning allowed for fast translations and reasonable memory consumption without a significant negative impact on performance.", "labels": [], "entities": []}, {"text": "In our first experiments, we tested the four reordering constraints with various window sizes.", "labels": [], "entities": []}, {"text": "We aimed at improving the translation results on the development corpora and compared the results with two baselines: reordering only the source training sentences and translation of the unreordered test sentences; and the GIATI technique for creating bilingual tuples (f j , \u02dc e j ) without reordering of the source sentences, neither in training nor during translation.", "labels": [], "entities": [{"text": "GIATI", "start_pos": 223, "end_pos": 228, "type": "METRIC", "confidence": 0.91229647397995}]}], "tableCaptions": [{"text": " Table 1: Statistics of the Basic Travel Expression (BTEC) corpora.", "labels": [], "entities": [{"text": "Basic Travel Expression (BTEC) corpora", "start_pos": 28, "end_pos": 66, "type": "DATASET", "confidence": 0.5387269854545593}]}, {"text": " Table 2: Translation results with optimal reorder- ing constraints and window sizes for the BTEC  Japanese-to-English and Chinese-to-English devel- opment corpora. * Optimized for the NIST score.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.97714763879776}, {"text": "BTEC", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.959866464138031}, {"text": "NIST score", "start_pos": 185, "end_pos": 195, "type": "DATASET", "confidence": 0.8629877865314484}]}, {"text": " Table 3: Comparison of the IWSLT-2004 automatic  evaluation results for the described system (WFST)  with those of the best submitted system (AT).", "labels": [], "entities": []}, {"text": " Table 4: Translation results with optimal reordering  constraints and window sizes for the test corpus of  the BTEC IE task. * Optimized for WER.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9742270112037659}, {"text": "BTEC IE task", "start_pos": 112, "end_pos": 124, "type": "TASK", "confidence": 0.6814449230829874}, {"text": "WER", "start_pos": 142, "end_pos": 145, "type": "METRIC", "confidence": 0.628337025642395}]}]}