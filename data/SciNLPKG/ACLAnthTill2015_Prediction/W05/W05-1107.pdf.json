{"title": [{"text": "Scaling High-Order Character Language Models to Gigabytes", "labels": [], "entities": [{"text": "Scaling High-Order Character Language Models", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8273637533187866}]}], "abstractContent": [{"text": "We describe the implementation steps required to scale high-order character language models to gigabytes of training data without pruning.", "labels": [], "entities": []}, {"text": "Our online models build character-level PAT trie structures on the fly using heavily data-unfolded implementations of an mutable daughter maps with along integer count interface.", "labels": [], "entities": [{"text": "PAT trie structures", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8921180764834086}]}, {"text": "Character 8-gram training runs at 200,000 characters per second and allows online tuning of hy-perparameters.", "labels": [], "entities": []}, {"text": "Our compiled models pre-compute all probability estimates for observed n-grams and all interpolation parameters , along with suffix pointers to speedup context computations from proportional to n-gram length to a constant.", "labels": [], "entities": []}, {"text": "The result is compiled models that are larger than the training models, but execute at 2 million characters per second on a desktop PC.", "labels": [], "entities": []}, {"text": "Cross-entropy on held-out data shows these models to be state of the art in terms of performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Character n-gram language models have been applied to just about every problem amenable to statistical language modeling.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 91, "end_pos": 120, "type": "TASK", "confidence": 0.7340597709019979}]}, {"text": "The implementation we describe here has been integrated as the source model in a general noisy-channel decoder (with applications to spelling correction, tokenization and case normalization) and the class models for statistical classification (with applications including spam filtering, topic categorization, sentiment analysis and word-sense disambiguation).", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7922109067440033}, {"text": "tokenization", "start_pos": 154, "end_pos": 166, "type": "TASK", "confidence": 0.9620060324668884}, {"text": "case normalization", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.6337132453918457}, {"text": "statistical classification", "start_pos": 216, "end_pos": 242, "type": "TASK", "confidence": 0.854481041431427}, {"text": "spam filtering", "start_pos": 272, "end_pos": 286, "type": "TASK", "confidence": 0.8352777063846588}, {"text": "sentiment analysis", "start_pos": 310, "end_pos": 328, "type": "TASK", "confidence": 0.9346095621585846}, {"text": "word-sense disambiguation", "start_pos": 333, "end_pos": 358, "type": "TASK", "confidence": 0.7226938158273697}]}, {"text": "In addition to these human language tasks, n-grams are also popular as estimators for entropy-based compression and source models for cryptography. and contain excellent overviews of character-level models and their application from a compression and HMM perspective, respectively.", "labels": [], "entities": []}, {"text": "Our hypothesis was that language-model smoothing would behave very much like the classifiers explored in (, in that more data trumps better estimation technique.", "labels": [], "entities": []}, {"text": "We managed to show that the better of the interpolation models used in, namely Dirichlet smoothing with or without update exclusion, Witten-Bell smoothing with or without update exclusion, and absolute discounting with update exclusion converged for 8-grams after 1 billion characters to cross entropies of 1.43+/-0.01.", "labels": [], "entities": []}, {"text": "The absolute discounting with update exclusion is what Chen and Goodman refer to as the Kneser-Ney method, and it was the clear winner in their evaluation.", "labels": [], "entities": []}, {"text": "They only tested non-parametric Witten-Bell with a suboptimal hyperparameter setting (1.0, just as in Witten and Bell's original implementation).", "labels": [], "entities": []}, {"text": "After a billion characters, roughly 95 percent of the characters were being estimated from their highest-order (7) context.", "labels": [], "entities": []}, {"text": "The two best models, parametric WittenBell and absolute discounting with update exclusion (aka Kneser-Ney), were even closer in crossentropy, and depending on the precise sample (we kept rolling samples as described below), and after a million or so characters, the differences even at the higher variance 12-grams were typically in the +/-0.01 range.", "labels": [], "entities": [{"text": "update exclusion", "start_pos": 73, "end_pos": 89, "type": "METRIC", "confidence": 0.9217463135719299}]}, {"text": "With a roughly 2.0 bit/character deviation, a 10,000 character sample, which is the size we used, leads to a 2\u03c3 (95.45%) confidence interval of +/-0.02, and the conclusion that the differences between these systems was insignificant.", "labels": [], "entities": []}, {"text": "Unlike in the token-based setting, we are not optimistic about the possibility of improving these results dramatically by clustering character contexts.", "labels": [], "entities": []}, {"text": "The lower-order models are very well trained with existing quantities of data and do a good job of this kind of smoothing.", "labels": [], "entities": []}, {"text": "We do believe that training hyperparameters for different model orders independently might improve cross-entropy fractionally; we found that training them hierarchically, as in, actually increased crossentropy.", "labels": [], "entities": []}, {"text": "We believe this is a direct correlate of the effectiveness of update exclusion; the lower-order models do not need to be the best possible models of those orders, but need to provide good estimates when heavily weighted, as in smoothing.", "labels": [], "entities": []}, {"text": "The global optimization allows a single setting to balance these attributes, but optimizing each dimension individually should do even better.", "labels": [], "entities": []}, {"text": "But with the number of estimates taking place at the highest possible orders, we do not believe the amount of smoothing will have that large an impact overall.", "labels": [], "entities": []}, {"text": "These experiments had a practical goal -we needed to choose a language modeling implementation for LingPipe and we didn't want to take the standard Swiss Army Knife approach because most of our users are not interested in running experiments on language modeling, but rather using language models in applications such as information retrieval, classification, or clustering.", "labels": [], "entities": [{"text": "LingPipe", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.9580614566802979}, {"text": "Swiss Army Knife", "start_pos": 148, "end_pos": 164, "type": "DATASET", "confidence": 0.924307624499003}, {"text": "information retrieval, classification", "start_pos": 321, "end_pos": 358, "type": "TASK", "confidence": 0.6895827800035477}]}, {"text": "These applications have actually been shown to perform better on the basis of character language models than token models ().", "labels": [], "entities": []}, {"text": "In addition, characterlevel models require no decisions about tokenization, token normalization and subtoken modeling (as in ().", "labels": [], "entities": [{"text": "token normalization", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8664375245571136}]}, {"text": "We chose to include the Witten-Bell method in our language modeling API because it is derived from full corpus counts, which we also use for collocation and relative frequency statistics within and across corpora, and thus the overall implementation effort was simpler.", "labels": [], "entities": []}, {"text": "For just language modeling, an update exclusion implementation of Kneser-Ney is no more complicated than In this paper, we describe the implementation details behind storing the model counts, how we sample the training character stream to provide low-cost, online leave-one-out style hyperparameter estimation, and how we compile the models and evaluate them over text inputs to achieve linear performance that is nearly independent of n-gram length.", "labels": [], "entities": []}, {"text": "We also describe some of the design patterns used at the interface level for training and execution.", "labels": [], "entities": []}, {"text": "As far as we know, the online leave-one-out analysis is novel, though there are epoch-based precursors in the compression literature.", "labels": [], "entities": []}, {"text": "As far as we know, no one has built a character language model implementation that will come close to the one presented herein terms of scalability.", "labels": [], "entities": []}, {"text": "This is largely because they have not been designed for the task rather than any fundamental limitation.", "labels": [], "entities": []}, {"text": "In fact, we take the main contribution of this paper to be a presentation of simple data sharing and data unfolding techniques that would also apply to token-level language models.", "labels": [], "entities": []}, {"text": "Before starting our presentation, we'll review some of the limitations of existing systems.", "labels": [], "entities": []}, {"text": "For a start, none of the systems of which we are aware can scale to 64-bit values for counts, which is necessary for the size models we are considering without pruning or count scaling.", "labels": [], "entities": []}, {"text": "It's simply easier to find 4 billion instances of a character than of a token.", "labels": [], "entities": []}, {"text": "In fact, the compression models typically use 16 bits for storing counts and then just scale downward when necessary, thus not even trying to store a full set of counts for even modest corpora.", "labels": [], "entities": []}, {"text": "The standard implementations of character models in the compression literature represent ordinary trie nodes as arrays, which is hugely wasteful for large sparse implementations; they represent PAT-trie nodes as pointers into the original text plus counts, which works well for long n-gram lengths (32) over small data sets (1 MB) but does not scale well for reasonable n-gram lengths (8-12) over larger data sets (100MB-1GB).", "labels": [], "entities": []}, {"text": "The standard token-level language models used to restrict attention to 64K tokens and thus require 16-bit token representatives per node just as our characterbased approach; with the advent of large vocabulary speech recognition, they now typically use 32-bits per node just to represent the token.", "labels": [], "entities": [{"text": "large vocabulary speech recognition", "start_pos": 193, "end_pos": 228, "type": "TASK", "confidence": 0.6781193539500237}]}, {"text": "Arrays of daughter nodes and lack of sharing of low-count terminal nodes were the biggest space hogs in our experiments, and as far as we know, none of the standard approaches take the immutable data unfolding approach we adopt to eliminate this overhead.", "labels": [], "entities": []}, {"text": "Thus we would like to stress again that existing characterlevel compression and token-level language modeling systems were simply not designed for handling large character-level models.", "labels": [], "entities": [{"text": "characterlevel compression", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.8009517192840576}, {"text": "token-level language modeling", "start_pos": 80, "end_pos": 109, "type": "TASK", "confidence": 0.6514802773793539}]}, {"text": "We would also like to point out that the standard finite state machine implementations of language models do not save any space over the triebased implementations, typically only approximate smoothing using backoff rather than interpolation, and further suffer from a huge space explosion when determinized.", "labels": [], "entities": []}, {"text": "The main advantage of finite state approaches is at the interface level in that they work well with hand-written constraints and can interface on either side of a given modeling problem.", "labels": [], "entities": []}, {"text": "For instance, typical language models implemented as trivial finite state transducers interface neatly with triphone acoustic models on the one side and with syntactic grammars on the other.", "labels": [], "entities": []}, {"text": "When placed in that context, the constraints from the grammar can often create an overall win in space after composition.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}