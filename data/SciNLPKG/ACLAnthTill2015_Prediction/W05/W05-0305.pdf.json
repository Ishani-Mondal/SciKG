{"title": [{"text": "Attribution and the (Non-)Alignment of Syntactic and Discourse Arguments of Connectives", "labels": [], "entities": [{"text": "Non-)Alignment of Syntactic and Discourse Arguments of Connectives", "start_pos": 21, "end_pos": 87, "type": "TASK", "confidence": 0.7237877726554871}]}], "abstractContent": [{"text": "The annotations of the Penn Discourse Treebank (PDTB) include (1) discourse connectives and their arguments, and (2) attribution of each argument of each connective and of the relation it denotes.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 23, "end_pos": 53, "type": "DATASET", "confidence": 0.9639968276023865}]}, {"text": "Because the PDTB covers the same text as the Penn TreeBank WSJ corpus, syntactic and discourse annotation can be compared.", "labels": [], "entities": [{"text": "Penn TreeBank WSJ corpus", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.9819364547729492}]}, {"text": "This has revealed significant differences between syntactic structure and discourse structure, in terms of the arguments of connectives, due in large part to attribution.", "labels": [], "entities": []}, {"text": "We describe these differences, an algorithm for detecting them, and finally some experimental results.", "labels": [], "entities": []}, {"text": "These results have implications for automating discourse annotation based on syntactic annotation .", "labels": [], "entities": [{"text": "automating discourse annotation", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.5876850585142771}]}], "introductionContent": [{"text": "The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank () with a layer of discourse annotations.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 24, "end_pos": 54, "type": "DATASET", "confidence": 0.9461359282334646}, {"text": "Penn TreeBank", "start_pos": 105, "end_pos": 118, "type": "DATASET", "confidence": 0.8691308498382568}]}, {"text": "A preliminary report on this project was presented at the 2004 workshop on Frontiers in Corpus Annotation (), where we described our annotation of discourse connectives (both explicit and implicit) along with their (clausal) arguments.", "labels": [], "entities": []}, {"text": "Further work done since then includes the annotation of attribution: that is, who has expressed each argument to a discourse connective (the writer or some other speaker or author) and who has expressed the discourse relation itself.", "labels": [], "entities": []}, {"text": "These ascriptions need not be the same.", "labels": [], "entities": []}, {"text": "Of particular interest is the fact that attribution mayor may not play a role in the relation established by a connective.", "labels": [], "entities": []}, {"text": "This may lead to alack of congruence between arguments at the syntactic and the discourse levels.", "labels": [], "entities": []}, {"text": "The issue of congruence is of interest both from the perspective of annotation (where it means that, even within a single sentence, one cannot merely transfer the annotation of syntactic arguments of a subordinate or coordinate conjunction to its discourse arguments), and from the perspective of inferences that these annotations will support in future applications of the PDTB.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We give a brief overview of the annotation of connectives and their arguments in the PDTB in Section 2.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.8537824153900146}]}, {"text": "In Section 3, we describe the annotation of the attribution of the arguments of a connective and the relation it conveys.", "labels": [], "entities": []}, {"text": "In Sections 4 and 5, we describe mismatches that arise between the discourse arguments of a connective and the syntactic annotation as provided by the Penn TreeBank (PTB), in the cases where all the arguments of the connective are in the same sentence.", "labels": [], "entities": [{"text": "Penn TreeBank (PTB)", "start_pos": 151, "end_pos": 170, "type": "DATASET", "confidence": 0.9678365349769592}]}, {"text": "In Section 6, we will discuss some implications of these issues for the theory and practice of discourse annotation and their relevance even at the level of sentence-bound annotation.", "labels": [], "entities": [{"text": "discourse annotation", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.7114076316356659}]}], "datasetContent": [{"text": "Describing the mismatches between the syntactic and discourse levels of annotation requires a detailed analysis of the cases where the tree subtraction algorithm does not detect the same arguments as annotated by the PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 217, "end_pos": 221, "type": "DATASET", "confidence": 0.9312961101531982}]}, {"text": "Hence this first set of experiments was carried out only on Sections 00-01 of the WSJ corpus (about 3500 sentences), which is accepted by the community to be development data.", "labels": [], "entities": [{"text": "Sections 00-01 of the WSJ corpus", "start_pos": 60, "end_pos": 92, "type": "DATASET", "confidence": 0.661702886223793}]}, {"text": "First, the tree subtraction algorithm was run on the PTB annotations in these two sections.", "labels": [], "entities": [{"text": "PTB", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.9125584959983826}]}, {"text": "The arguments detected by the algorithm were classified as: (a) Exact, if the argument detected by the algorithm exactly matches the annotation; (b) Extra Material, if the argument detected contains some additional material in comparison with the annotation; and (c) Omitted Material, if some annotated material was not included in the argument detected.", "labels": [], "entities": [{"text": "Exact", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9520509839057922}]}, {"text": "The results are summarized in 5.", "labels": [], "entities": []}, {"text": "We also evaluated the performance of the tree subtraction procedure on the PTB annotations on Sections 02-24 of the WSJ corpus, and the results are summarized in  Finally we evaluated the algorithm on the output of a statistical parser.", "labels": [], "entities": [{"text": "PTB annotations on Sections 02-24 of the WSJ corpus", "start_pos": 75, "end_pos": 126, "type": "DATASET", "confidence": 0.8143877387046814}]}, {"text": "The parser implementation in) was used in this experiment and it was run in a mode which emulated the parser.", "labels": [], "entities": []}, {"text": "The parser was trained on Sections 02-21 and Sections 22-24 were used as test data, where the parser was run and the tree subtraction algorithm was run on its output.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tree subtraction on the PTB annotations for SUB-", "labels": [], "entities": [{"text": "PTB", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.7902961373329163}]}, {"text": " Table 2. The second  row of Table 2 illustrates the same information for  Arg2. Most of these are instances where irrelevant  clauses were included in the argument detected from  the PTB.", "labels": [], "entities": [{"text": "Arg2", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.7998190522193909}, {"text": "PTB", "start_pos": 184, "end_pos": 187, "type": "DATASET", "confidence": 0.9391810297966003}]}, {"text": " Table 2: Cases which result in extra material being included", "labels": [], "entities": []}, {"text": " Table 3. At this time, we lack a precise under- standing of the remaining mismatches in Arg1, and  the ones resulting in material being omitted from  Arg2.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.8061624765396118}, {"text": "Arg2", "start_pos": 151, "end_pos": 155, "type": "DATASET", "confidence": 0.9205518364906311}]}, {"text": " Table 3: Cases which result in material being omitted from", "labels": [], "entities": []}, {"text": " Table 4: Tree subtraction on PTB annotations for the SUB-", "labels": [], "entities": []}, {"text": " Table 5: Tree subtraction on the output of a statistical parser", "labels": [], "entities": [{"text": "Tree subtraction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6408737003803253}]}]}