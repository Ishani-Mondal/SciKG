{"title": [{"text": "Preprocessing and Normalization for Automatic Evaluation of Machine Translation", "labels": [], "entities": [{"text": "Automatic Evaluation of Machine Translation", "start_pos": 36, "end_pos": 79, "type": "TASK", "confidence": 0.5565343618392944}]}], "abstractContent": [{"text": "Evaluation measures for machine translation depend on several common methods , such as preprocessing, tokenization, handling of sentence boundaries, and the choice of a reference length.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7780235707759857}, {"text": "tokenization", "start_pos": 102, "end_pos": 114, "type": "TASK", "confidence": 0.9722558856010437}, {"text": "handling of sentence boundaries", "start_pos": 116, "end_pos": 147, "type": "TASK", "confidence": 0.7758896052837372}]}, {"text": "In this paper, we describe and review some new approaches to them and compare these to state-of-the-art methods.", "labels": [], "entities": []}, {"text": "We experimentally look into their impact on four established evaluation measures.", "labels": [], "entities": []}, {"text": "For this purpose, we study the correlation between automatic and human evaluation scores on three MT evaluation corpora.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.9039717614650726}]}, {"text": "These experiments confirm that the to-kenization method, the reference length selection scheme, and the use of sentence boundaries we introduce will increase the correlation between automatic and human evaluation scores.", "labels": [], "entities": []}, {"text": "We find that ignoring case information and normalizing evalu-ator scores has a positive effect on the sentence level correlation as well.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation (MT), as any other natural language processing (NLP) research subject, depends on the evaluation of its results.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9241714477539062}]}, {"text": "Unfortunately, human evaluation of MT system output is a time consuming and expensive task.", "labels": [], "entities": [{"text": "MT system output", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.8658283750216166}]}, {"text": "This is why automatic evaluation is preferred to human evaluation in the research community.", "labels": [], "entities": []}, {"text": "Over the last years, a manifold of automatic evaluation measures has been proposed and studied.", "labels": [], "entities": []}, {"text": "This underlines the importance, but also the complexity of finding a suitable evaluation measure for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.9928135275840759}]}, {"text": "We will give a short overview of some measures in section 2 of this paper.", "labels": [], "entities": []}, {"text": "Although most of these measures share similar ideas and foundation, we observe that researchers tend to approach problems common to several measures differently from each other.", "labels": [], "entities": []}, {"text": "A noteworthy example here is the determination of a translation reference length.", "labels": [], "entities": []}, {"text": "In section 3, we will have a look onto structural similarities and differences among several measures, focussing on common steps.", "labels": [], "entities": []}, {"text": "We will show that decisions taken about them can be as important to the outcome of an evaluation, as the choice of the evaluation measure itself.", "labels": [], "entities": []}, {"text": "To this end, we will study the performance of each error measure and setting by comparison with human evaluation on three different evaluation tasks in section 4.", "labels": [], "entities": []}, {"text": "These experiments will show that sophisticated tokenization as well as adding sentence boundaries and a good choice for the reference lengths will improve correlation between automatic and human evaluation significantly.", "labels": [], "entities": []}, {"text": "Case normalization and evaluator normalization are helpful only when evaluating on sentence level; system level evaluation is not affected by these methods.", "labels": [], "entities": [{"text": "Case normalization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7676971554756165}, {"text": "evaluator normalization", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6969527453184128}]}, {"text": "After a discussion of these results in section 5, we will conclude this paper in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The majority of MT evaluation approaches are based on the distance or similarity of MT candidate output to a set of reference translations, i.e. to sentences which are known to be correct.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9654393494129181}, {"text": "MT candidate output", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.8636534214019775}]}, {"text": "The lower this distance is, or the higher the similarity, the better the candidate translations are considered to be, and thus the better the MT system.", "labels": [], "entities": [{"text": "similarity", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9824037551879883}, {"text": "MT", "start_pos": 142, "end_pos": 144, "type": "TASK", "confidence": 0.909511387348175}]}, {"text": "Out of the vast amount of measures, we will focus on the following measures that are widely used in research and in evaluation campaigns: WER, PER, BLEU, and NIST.", "labels": [], "entities": [{"text": "WER", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.879172146320343}, {"text": "PER", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.9897268414497375}, {"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9973533153533936}, {"text": "NIST", "start_pos": 158, "end_pos": 162, "type": "DATASET", "confidence": 0.9320100545883179}]}, {"text": "Leta test set consist of k = 1, . .", "labels": [], "entities": []}, {"text": ", K candidate sentences E k generated by an MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9230905175209045}]}, {"text": "For each candidate sentence E k , we have a set of r = 1, . .", "labels": [], "entities": []}, {"text": ", R k reference sentences E r,k . Let I k denote the length, and I * k the reference length for each sentence E k . We will explain in section 3.3 how the reference length is calculated.", "labels": [], "entities": []}, {"text": "These included no sentence boundaries, but tokenization with treatment of abbreviations, see table 1.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.9679327011108398}]}, {"text": "For sentence evaluation, conditions included evaluator normalization.", "labels": [], "entities": [{"text": "sentence evaluation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.772966206073761}, {"text": "evaluator normalization", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.7004098445177078}]}, {"text": "We used these settings in the other experiments, too, if not stated otherwise.", "labels": [], "entities": []}, {"text": "shows the correlation between automatic and human scores.", "labels": [], "entities": []}, {"text": "On the TIDES corpora the system level correlation is particularly high, at a moderate sentence level correlation.", "labels": [], "entities": [{"text": "TIDES corpora", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.8324088454246521}, {"text": "system level correlation", "start_pos": 25, "end_pos": 49, "type": "METRIC", "confidence": 0.5399640599886576}]}, {"text": "We assume the latter is due to the poor sentence inter-annotator agreement on these corpora, which is then smoothed out on system level.", "labels": [], "entities": []}, {"text": "On the BTEC corpus a high sentence level correlation accompanies a significantly lower system level correlation.", "labels": [], "entities": [{"text": "BTEC corpus", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9678673446178436}]}, {"text": "Note that due to the much lower number of samples on the system level (e.g. 5 vs. 5500), small changes in the sentence level correlation are more likely to be significant than such changes on system level.", "labels": [], "entities": []}, {"text": "We have verified these effects by inspecting the rank correlation on both levels, as well as by experiments on other corpora.", "labels": [], "entities": [{"text": "rank correlation", "start_pos": 49, "end_pos": 65, "type": "METRIC", "confidence": 0.9024642407894135}]}, {"text": "Although these experiments support our findings, we have omitted results here for the sake of clarity.", "labels": [], "entities": []}], "tableCaptions": []}