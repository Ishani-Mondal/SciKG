{"title": [], "abstractContent": [{"text": "Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use.", "labels": [], "entities": [{"text": "Chunk parsing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.677341029047966}]}, {"text": "In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple sliding-window method and maximum entropy classifiers for phrase recognition in each level of chunking.", "labels": [], "entities": [{"text": "chunk parsing", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9357629418373108}, {"text": "phrase recognition", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.7524083852767944}]}, {"text": "Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/sentence).", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.9957084258397421}, {"text": "chunk parser", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.7021738737821579}]}, {"text": "We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers, and show that the search method can further improve the parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 194, "end_pos": 201, "type": "TASK", "confidence": 0.9603445529937744}, {"text": "accuracy", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.6659407615661621}]}], "introductionContent": [{"text": "Chunk parsing) is a simple parsing strategy both in implementation and concept.", "labels": [], "entities": [{"text": "Chunk parsing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.5607656240463257}]}, {"text": "The parser first performs chunking by identifying base phrases, and convert the identified phrases to non-terminal symbols.", "labels": [], "entities": []}, {"text": "The parser again performs chunking on the updated sequence and convert the newly recognized phrases into non-terminal symbols.", "labels": [], "entities": []}, {"text": "The parser repeats this procedure until there are no phrases to be chunked.", "labels": [], "entities": []}, {"text": "After finishing these chunking processes, we can reconstruct the complete parse tree of the sentence from the chunking results.", "labels": [], "entities": []}, {"text": "Although the conceptual simplicity of chunk parsing is appealing, satisfactory performance for practical use has not yet been achieved with this parsing strategy.", "labels": [], "entities": [{"text": "chunk parsing", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.83766108751297}]}, {"text": "Sang achieved an f-score of 80.49 on the Penn Treebank by using the IOB tagging method for each level of chunking).", "labels": [], "entities": [{"text": "f-score", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9963287711143494}, {"text": "Penn Treebank", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9834330379962921}]}, {"text": "However, there is a very large gap between their performance and that of widely-used practical parsers).", "labels": [], "entities": []}, {"text": "The performance of chunk parsing is heavily dependent on the performance of phrase recognition in each level of chunking.", "labels": [], "entities": [{"text": "chunk parsing", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.8648765683174133}, {"text": "phrase recognition", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7291410863399506}]}, {"text": "We show in this paper that the chunk parsing strategy is indeed appealing in that it can give considerably better performance than previously reported by using a different approach for phrase recognition and that it enables us to build a very fast parser that gives high-precision outputs.", "labels": [], "entities": [{"text": "chunk parsing", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.8554521203041077}, {"text": "phrase recognition", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.8403022885322571}]}, {"text": "This advantage could open up the possibility of using full parsers for large-scale information extraction from the Web corpus and real-time information extraction where the system needs to analyze the documents provided by the users on run-time.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.7621495425701141}, {"text": "real-time information extraction", "start_pos": 130, "end_pos": 162, "type": "TASK", "confidence": 0.6631996631622314}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the overall chunk parsing strategy employed in this work.", "labels": [], "entities": [{"text": "chunk parsing", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.833983451128006}]}, {"text": "Section 3 describes the slidingwindow based method for identifying chunks.", "labels": [], "entities": []}, {"text": "Two filtering methods to reduce the computational cost are presented in sections 4 and 5.", "labels": [], "entities": []}, {"text": "Section 6 explains the maximum entropy classifier and the feature set.", "labels": [], "entities": []}, {"text": "Section 7 describes methods for searching the best parse.", "labels": [], "entities": []}, {"text": "Experimental results on the Penn Treebank corpus are given in Section 8.", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.9952976902325948}]}, {"text": "Section 10 offers some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran parsing experiments using the Penn Treebank corpus, which is widely used for evaluating parsing algorithms.", "labels": [], "entities": [{"text": "parsing", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.9644419550895691}, {"text": "Penn Treebank corpus", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.9962502916653951}]}, {"text": "The training set consists of sections 02-21.", "labels": [], "entities": []}, {"text": "We used section 22 as the development data, with which we tuned the feature set and parameters for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 99, "end_pos": 106, "type": "TASK", "confidence": 0.9752251505851746}]}, {"text": "The test set consists of section 23 and we report the performance of the parser on the set.", "labels": [], "entities": []}, {"text": "We used the evalb script provided by Sekine and Collins for evaluating the labeled recall/precision (LR/LP) of the parser outputs 1 . All the experiments were carried out on a server having a 2.6 GHz AMD Opteron CPU and 16GB memory.", "labels": [], "entities": [{"text": "recall/precision (LR/LP)", "start_pos": 83, "end_pos": 107, "type": "METRIC", "confidence": 0.9241695776581764}]}], "tableCaptions": [{"text": " Table 1. This con- version reduces the sparseness of the rules.", "labels": [], "entities": []}, {"text": " Table 2: Effectiveness of the naive Bayes filtering on some representative nonterminals.", "labels": [], "entities": []}, {"text": " Table 4: Parsing performance on section 23 (all sen- tences, gold-standard POS tags) with the determin- istic algorithm.", "labels": [], "entities": []}, {"text": " Table 5: Parsing performance on section 23 (all sen- tences, gold-standard POS tags) with the search al- gorithm.", "labels": [], "entities": []}, {"text": " Table 6: Comparison with other work. Parsing per- formance on section 23 (all sentences).", "labels": [], "entities": []}]}