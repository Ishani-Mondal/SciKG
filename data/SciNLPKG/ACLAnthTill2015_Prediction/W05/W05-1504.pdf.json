{"title": [{"text": "Parsing with Soft and Hard Constraints on Dependency Length *", "labels": [], "entities": []}], "abstractContent": [{"text": "In lexicalized phrase-structure or dependency parses, a word's modifiers tend to fall near it in the string.", "labels": [], "entities": [{"text": "dependency parses", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.720202624797821}]}, {"text": "We show that a crude way to use dependency length as a parsing feature can substantially improve parsing speed and accuracy in English and Chinese, with more mixed results on German.", "labels": [], "entities": [{"text": "parsing", "start_pos": 97, "end_pos": 104, "type": "TASK", "confidence": 0.9746748208999634}, {"text": "speed", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.5515475273132324}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9969732761383057}]}, {"text": "We then show similar improvements by imposing hard bounds on dependency length and (additionally) modeling the resulting sequence of parse fragments.", "labels": [], "entities": []}, {"text": "This simple \"vine grammar\" formalism has only finite-state power, but a context-free parameterization with some extra parameters for stringing fragments together.", "labels": [], "entities": []}, {"text": "We exhibit a linear-time chart parsing algorithm with a low grammar constant.", "labels": [], "entities": [{"text": "linear-time chart parsing", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.6878124674161276}]}], "introductionContent": [{"text": "Many modern parsers identify the headword of each constituent they find.", "labels": [], "entities": []}, {"text": "This makes it possible to identify the word-to-word dependencies implicit in a parse.", "labels": [], "entities": []}, {"text": "1 (Some parsers, known as dependency parsers, even return these dependencies as their primary output.)", "labels": [], "entities": []}, {"text": "Why bother to identify these dependencies?", "labels": [], "entities": []}, {"text": "The typical reason is to model the fact that some word pairs are more likely than others to engage in a dependency relationship.", "labels": [], "entities": []}, {"text": "In this paper, we propose a different reason to identify dependencies in candidate parses: to evaluate not the dependency's word pair but its length (i.e., the string distance between the two words).", "labels": [], "entities": []}, {"text": "Dependency lengths differ from * This work was supported by NSF ITR grant IIS-0313193 to the first author and a fellowship from the Fannie and John Hertz Foundation to the second author.", "labels": [], "entities": [{"text": "NSF ITR grant IIS-0313193", "start_pos": 60, "end_pos": 85, "type": "DATASET", "confidence": 0.7320758551359177}]}, {"text": "The views expressed are not necessarily endorsed by the sponsors.", "labels": [], "entities": []}, {"text": "The authors thank Mark Johnson, Eugene Charniak, Charles Schafer, Keith Hall, and John Hale for helpful discussion and Elliott Dr\u00e1bek and Markus Dreyer for insights on (respectively) Chinese and German parsing.", "labels": [], "entities": []}, {"text": "They also thank an anonymous reviewer for suggesting the German experiments.", "labels": [], "entities": []}, {"text": "Ina phrase-structure parse, if phrase X headed byword token x is a subconstituent of phrase Y headed byword token y = x, then x is said to depend on y.", "labels": [], "entities": []}, {"text": "Ina more powerful compositional formalism like LTAG or CCG, dependencies can be extracted from the derivation tree.", "labels": [], "entities": []}, {"text": "It has recently been questioned whether these \"bilexical\" features actually contribute much to parsing performance (, at least when one has only a million words of training.", "labels": [], "entities": [{"text": "parsing", "start_pos": 95, "end_pos": 102, "type": "TASK", "confidence": 0.9746431112289429}]}, {"text": "typical parsing features in that they cannot be determined from tree-local information.", "labels": [], "entities": []}, {"text": "Though lengths are not usually considered, we will see that bilexical dynamic-programming parsing algorithms can easily consider them as they build the parse.", "labels": [], "entities": [{"text": "dynamic-programming parsing", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.659347414970398}]}, {"text": "Like any other feature of trees, dependency lengths can be explicitly used as features in a probability model that chooses among trees.", "labels": [], "entities": []}, {"text": "Such a model will tend to disfavor long dependencies (at least of some kinds), as these are empirically rare.", "labels": [], "entities": []}, {"text": "In the first part of the paper, we show that such features improve a simple baseline dependency parser.", "labels": [], "entities": []}, {"text": "If the bias against long dependencies is strengthened into a hard constraint that absolutely prohibits long dependencies, then the parser turns into a partial parser with only finite-state power.", "labels": [], "entities": []}, {"text": "In the second part of the paper, we show how to perform chart parsing in asymptotic linear time with a low grammar constant.", "labels": [], "entities": [{"text": "chart parsing", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.7055074870586395}]}, {"text": "Such a partial parser does less work than a full parser in practice, and in many cases recovers a more precise set of dependencies (with little loss in recall).", "labels": [], "entities": [{"text": "recall", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.998021125793457}]}], "datasetContent": [{"text": "We trained models A-C, using unsmoothed maximum likelihood estimation, on three treebanks: the Penn (English) Treebank (split in the standard way, \u00a72-21 train/ \u00a723 test, or 950K/57K words), the Penn Chinese Treebank (80% train/10% test or 508K/55K words), and the German TIGER corpus (80%/10% or 539K/68K words).", "labels": [], "entities": [{"text": "Penn (English) Treebank", "start_pos": 95, "end_pos": 118, "type": "DATASET", "confidence": 0.9264082312583923}, {"text": "Penn Chinese Treebank", "start_pos": 194, "end_pos": 215, "type": "DATASET", "confidence": 0.9613466064135233}, {"text": "German TIGER corpus", "start_pos": 264, "end_pos": 283, "type": "DATASET", "confidence": 0.8586861491203308}]}, {"text": "Estimation was a simple matter of counting automaton events and normalizing counts into probabilities.", "labels": [], "entities": [{"text": "Estimation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9378676414489746}]}, {"text": "For each model, we also trained the three length-sensitive versions described in \u00a73.3.", "labels": [], "entities": []}, {"text": "The German corpus contains non-projective trees.", "labels": [], "entities": [{"text": "German corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8724563419818878}]}, {"text": "None of our parsers can recover non-projective dependencies (nor can our models produce them).", "labels": [], "entities": []}, {"text": "This fact was ignored when counting events for maximum likelihood estimation: in particular, we always trained L wand R won the sequence of w's immediate children, even in non-projective trees.", "labels": [], "entities": []}, {"text": "Our results (Tab. 1) show that sharpening the probabilities with the most sophisticated distance factors p(\u2206 | d, h, c), consistently improved the speed of all parsers.", "labels": [], "entities": [{"text": "speed", "start_pos": 147, "end_pos": 152, "type": "METRIC", "confidence": 0.9669885039329529}]}, {"text": "The change to the code is trivial.", "labels": [], "entities": []}, {"text": "The only overhead is the cost of looking up and multiplying in the extra distance factors.", "labels": [], "entities": []}, {"text": "Accuracy also improved over the baseline models of English and Chinese, as well as the simpler baseline models of German.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9944730401039124}]}, {"text": "Again, the most sophisticated distance factors helped most, but even the simplest distance factor usually obtained most of the accuracy benefit.", "labels": [], "entities": [{"text": "distance", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9215938448905945}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9994869232177734}]}, {"text": "German model C fell slightly inaccuracy.", "labels": [], "entities": []}, {"text": "The speedup here suggests that the probabilities were sharpened, but often in favor of the wrong parses.", "labels": [], "entities": []}, {"text": "We did not analyze the errors on German; it may Heads were extracted for English using Michael Collins' rules and Chinese using Fei Xia's rules (defaulting in both cases to right-most heads where the rules fail).", "labels": [], "entities": []}, {"text": "German heads were extracted using the TIGER Java API; we discarded all resulting dependency structures that were cyclic or unconnected (6%).", "labels": [], "entities": []}, {"text": "We measure speed abstractly by the number of items built and pushed on the agenda.", "labels": [], "entities": [{"text": "speed", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9536334276199341}]}, {"text": "be relevant that 25% of the German sentences contained a non-projective dependency between nonpunctuation tokens.", "labels": [], "entities": []}, {"text": "Studying the parser output for English, we found that the length-sensitive models preferred closer attachments, with 19.7% of tags having a nearer parent in the best parse under model C with p(\u2206 | d, h, c) than in the original model C, 77.7% having a parent at the same distance, and only 2.5% having a farther parent.", "labels": [], "entities": []}, {"text": "The surviving long dependencies (at any length > 1) tended to be much more accurate, while the (now more numerous) length-1 dependencies were slightly less accurate than before.", "labels": [], "entities": [{"text": "accurate", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9893584251403809}]}, {"text": "We caution that length sensitivity's most dramatic improvements to accuracy were on the worse baseline models, which had more room to improve.", "labels": [], "entities": [{"text": "length sensitivity", "start_pos": 16, "end_pos": 34, "type": "METRIC", "confidence": 0.874582052230835}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9993789196014404}]}, {"text": "The better baseline models (B and C) were already able to indirectly capture some preference for short dependencies, by learning that some parts of speech were unlikely to have multiple left or multiple right dependents.", "labels": [], "entities": []}, {"text": "Enhancing B and C therefore contributed less, and indeed may have had some harmful effect by over-penalizing some structures that were already appropriately penalized.", "labels": [], "entities": []}, {"text": "15 It remains to be seen, therefore, whether distance features would help state-of-the art parsers that are already much better than model C.", "labels": [], "entities": []}, {"text": "Such parsers may already incorporate features that indirectly impose a good model of distance, though perhaps not as cheaply.", "labels": [], "entities": []}, {"text": "Our experiments used the asymptotically fast hybrid parsing algorithm above.", "labels": [], "entities": []}, {"text": "We used the same left and right automata as in model C, the best-performing model from \u00a73.2.", "labels": [], "entities": []}, {"text": "However, we now define R $ to be a first-order (bigram) Markov model ( \u00a74.1).", "labels": [], "entities": []}, {"text": "We trained and tested on the same headed treebanks as before ( \u00a73.7), except that we modified the training trees to make them feasible ( \u00a74.2).", "labels": [], "entities": []}, {"text": "Results are shown in Figures 3 (precision/recall tradeoff) and 4 (accuracy/speed tradeoff), fork \u2208 {1, 2, ..., 10, 15, 20}.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9984253644943237}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.7683706283569336}, {"text": "accuracy/speed tradeoff", "start_pos": 66, "end_pos": 89, "type": "METRIC", "confidence": 0.8549989759922028}]}, {"text": "Dots correspond to different values of k.", "labels": [], "entities": []}, {"text": "On English and Chinese, some values of k actually achieve better F -measure accuracy than the unbounded parser, by eliminating errors.", "labels": [], "entities": [{"text": "F -measure accuracy", "start_pos": 65, "end_pos": 84, "type": "METRIC", "confidence": 0.8738570213317871}]}, {"text": "We observed that changing R $ from a bigram to a unigram model significantly hurt performance, showing that it is in fact useful to empirically model likely sequences of parse fragments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dependency parsing of POS tag sequences with simple probabilistic split bilexical grammars. The models differ only  in how they weight the same candidate parse trees. Length-sensitive models are larger but can improve dependency accuracy  and speed. (Recall is measured as the fraction of non-punctuation tags whose correct parent (if not the $ symbol) was correctly  recovered by the parser; it equals precision, unless the parser left some sentences unparsed (or incompletely parsed, as in  \u00a74), in", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8041012585163116}, {"text": "accuracy", "start_pos": 239, "end_pos": 247, "type": "METRIC", "confidence": 0.9656168222427368}, {"text": "Recall", "start_pos": 261, "end_pos": 267, "type": "METRIC", "confidence": 0.992997407913208}, {"text": "precision", "start_pos": 413, "end_pos": 422, "type": "METRIC", "confidence": 0.9992928504943848}]}]}