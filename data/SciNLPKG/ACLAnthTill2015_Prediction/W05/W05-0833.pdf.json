{"title": [{"text": "Hybrid Example-Based SMT: the Best of Both Worlds?", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9249904751777649}]}], "abstractContent": [{"text": "(Way and Gough, 2005) provide an in-depth comparison of their Example-Based Machine Translation (EBMT) system with a Statistical Machine Translation (SMT) system constructed from freely available tools.", "labels": [], "entities": [{"text": "Example-Based Machine Translation (EBMT)", "start_pos": 62, "end_pos": 102, "type": "TASK", "confidence": 0.7771426339944204}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 117, "end_pos": 154, "type": "TASK", "confidence": 0.7762209971745809}]}, {"text": "According to a wide variety of automatic evaluation metrics, they demonstrated that their EBMT system outper-formed the SMT system by a factor of two to one.", "labels": [], "entities": [{"text": "SMT", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9875917434692383}]}, {"text": "Nevertheless, they did not test their EBMT system against a phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.8074803948402405}]}, {"text": "Obtaining their training and test data for English-French, we carryout a number of experiments using the Pharaoh SMT Decoder.", "labels": [], "entities": [{"text": "Pharaoh SMT Decoder", "start_pos": 105, "end_pos": 124, "type": "DATASET", "confidence": 0.6247353255748749}]}, {"text": "While better results are seen when Pharaoh is seeded with Giza++ word-and phrase-based data compared to EBMT sub-sentential alignments, in general better results are obtained when combinations of this 'hybrid' data is used to construct the translation and probability models.", "labels": [], "entities": []}, {"text": "While for the most part the EBMT system of (Gough & Way, 2004b) outperforms any flavour of the phrase-based SMT systems constructed in our experiments, combining the data sets automatically induced by both Giza++ and their EBMT system leads to a hybrid system which improves on the EBMT system per se for French-English.", "labels": [], "entities": [{"text": "SMT", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.7825227379798889}]}], "introductionContent": [{"text": ") provide what are to our knowledge the first published results comparing Example-Based and Statistical models of Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.8554425597190857}]}, {"text": "Given that most MT research carried out today is corpus-based, it is somewhat surprising that until quite recently no qualitative research existed on the relative performance of the two approaches.", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9927518367767334}]}, {"text": "This maybe due to a number of factors: the relative unavailability of EBMT systems, the lack of participation of EBMT researchers in competitive evaluations or the dominance in the MT research community of the SMT approach-whenever one paradigm finds favour with the clear majority of MT practitioners, the assumption made by most of the community is that this way of doing things is clearly better than the alternatives.", "labels": [], "entities": [{"text": "MT research community", "start_pos": 181, "end_pos": 202, "type": "TASK", "confidence": 0.8722539146741232}, {"text": "SMT", "start_pos": 210, "end_pos": 213, "type": "TASK", "confidence": 0.9889637231826782}]}, {"text": "Like (), we find this regrettable: the only basis on which such views should be allowed to permeate our field is following extensive testing and evaluation.", "labels": [], "entities": []}, {"text": "Nonetheless, given that no EBMT systems are freely available, very few research groups are in the position of being able to carryout such work.", "labels": [], "entities": []}, {"text": "This paper extends the work of () by testing EBMT against phrase-based models of SMT, rather than the word-based models used in this previous work.", "labels": [], "entities": [{"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9739471673965454}]}, {"text": "In so doing, it provides a more complete evaluation of the main question at hand, namely whether an SMT system outperforms an EBMT system on reasonably large training and test sets.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9933432340621948}]}, {"text": "We obtained the same training and test data used in (, and evaluated a number of SMT systems which use the Pharaoh decoder 1 against the Marker-Based EBMT system of, for French-English and EnglishFrench.", "labels": [], "entities": [{"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.98731529712677}, {"text": "EnglishFrench", "start_pos": 189, "end_pos": 202, "type": "DATASET", "confidence": 0.8498770594596863}]}, {"text": "We provide results using a range of automatic evaluation metrics: BLEU (), Precision and Recall (, and Word-and Sentence Error Rates.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9992051720619202}, {"text": "Precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9933604598045349}, {"text": "Recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.5266140103340149}]}, {"text": "() observe that EBMT tends to outperform a word-based SMT model, and our experiments show that a number of different phrase-based SMT systems still tend to fall short of the quality obtained via EBMT for these evaluation metrics.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.8103947043418884}, {"text": "SMT", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.8073849678039551}]}, {"text": "However, when Pharaoh is seeded with the data sets automatically induced by both Giza++ and their EBMT system, better results are seen for French-English than for the EBMT system per se.", "labels": [], "entities": []}, {"text": "The remainder of the paper is constructed as follows.", "labels": [], "entities": []}, {"text": "In section 2, we summarize the main ideas behind typical models of SMT and EBMT, as well as the EBMT system of) used in our experiments.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9814175963401794}]}, {"text": "In section 3, we revisit the experiments and results carried out by.", "labels": [], "entities": []}, {"text": "In section 4, we describe our extensions to their work, and compare their findings to ours, and in section 5, present a number of hybrid SMT models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.9907817244529724}]}, {"text": "Finally, we conclude and offer some thoughts for future work in section 6, and in section 7 present some further comments on the narrowing gap between EBMT and phrase-based SMT.", "labels": [], "entities": [{"text": "phrase-based SMT", "start_pos": 160, "end_pos": 176, "type": "TASK", "confidence": 0.4944443255662918}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparing the EBMT system of (Gough &", "labels": [], "entities": []}, {"text": " Table 1.  Essentially, all the automatic evaluation metrics bar  one (Precision) suggest that EBMT can outperform  SMT from English-French. Surprisingly, however,  apart from SER, all evaluation scores are higher us- ing 100K sentence pairs as training data rather than  the full 203K sentences. It is generally assumed that  increasing the size of the training data for corpus- based MT systems will improve the quality of the  output translations. (", "labels": [], "entities": [{"text": "Precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9428231716156006}, {"text": "SMT", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9868175983428955}, {"text": "SER", "start_pos": 176, "end_pos": 179, "type": "METRIC", "confidence": 0.9927474856376648}, {"text": "MT", "start_pos": 386, "end_pos": 388, "type": "TASK", "confidence": 0.9218688011169434}]}, {"text": " Table 2: Comparing the EBMT system of (Gough & Way,", "labels": [], "entities": [{"text": "Gough & Way", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.8722179532051086}]}, {"text": " Table 3: Seeding Pharaoh with Giza++ and EBMT sub-", "labels": [], "entities": [{"text": "Seeding Pharaoh", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9177231788635254}]}, {"text": " Table 4: Seeding Pharaoh with Giza++ and EBMT sub-", "labels": [], "entities": [{"text": "Seeding Pharaoh", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9191648066043854}]}, {"text": " Table 5: Seeding Pharaoh with Giza++ word and EBMT", "labels": [], "entities": [{"text": "Seeding Pharaoh", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9200341105461121}, {"text": "EBMT", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.6386818885803223}]}, {"text": " Table 5. Comparing these  figures to those in", "labels": [], "entities": []}, {"text": " Table 6: Seeding Pharaoh with Giza++ word and EBMT", "labels": [], "entities": [{"text": "Seeding Pharaoh", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9201302826404572}, {"text": "EBMT", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.6420360803604126}]}, {"text": " Table 6.  While recall drops slightly, all the other metrics  show a slight increase compared to the performance  obtained when Pharaoh is seeded with Giza++ word- and phrase-alignments (cf.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9995501637458801}]}, {"text": " Table 7: Seeding Pharaoh with all Giza++ and EBMT sub-", "labels": [], "entities": [{"text": "Seeding Pharaoh", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9105874598026276}]}, {"text": " Table 8: Seeding Pharaoh with all Giza++ and EBMT sub-", "labels": [], "entities": [{"text": "Seeding Pharaoh", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9132187068462372}]}, {"text": " Table 8. This  time this hybrid SMT system does outperform the  EBMT system of", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9938596487045288}]}, {"text": " Table 9: Comparing the hybrid phrase-based SMT system us-", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9280104041099548}]}]}