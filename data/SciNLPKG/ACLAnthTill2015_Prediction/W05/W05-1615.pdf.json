{"title": [{"text": "Evaluation of an NLG System using Post-Edit Data: Lessons Learnt", "labels": [], "entities": []}], "abstractContent": [], "introductionContent": [{"text": "Natural Language Generation (NLG) systems must of course be evaluated, like all NLP systems.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7501001954078674}]}, {"text": "Previous work on NLG evaluation has focused on either experiments conducted with users who read the generated texts, or on comparisons of generated texts to corpora of human-written texts.", "labels": [], "entities": [{"text": "NLG evaluation", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.9282892644405365}]}, {"text": "In this paper we describe an evaluation technique, which looks at how much humans need to post-edit generated texts before they are released to users.", "labels": [], "entities": []}, {"text": "Post-edit evaluations are common in machine translation, but we believe that ours is the first large-scale post-edit evaluation of an NLG system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7279558479785919}]}, {"text": "The system being evaluated is SUMTIME-MOUSAM], an NLG system, which generates marine weather forecasts from Numerical Weather Prediction (NWP) data.", "labels": [], "entities": [{"text": "marine weather forecasts from Numerical Weather Prediction (NWP)", "start_pos": 78, "end_pos": 142, "type": "TASK", "confidence": 0.7018608033657074}]}, {"text": "SUMTIME-MOUSAM is operational and is used by Weathernews (UK) Ltd to generate 150 draft forecasts per day, which are post-edited by Weathernews forecasters before being released to clients.", "labels": [], "entities": [{"text": "SUMTIME-MOUSAM", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.5125067234039307}, {"text": "Weathernews (UK) Ltd", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.9140969634056091}, {"text": "Weathernews", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.9272276163101196}]}], "datasetContent": [{"text": "As stated in Section 2.1, we were attracted to post-edit evaluation because we believed that (A) people would only edit things that were clearly wrong; and (B) post-editing was an important usefulness metric from the perspective of our users (forecasters).", "labels": [], "entities": []}, {"text": "Looking back, (B) was certainly true.", "labels": [], "entities": []}, {"text": "The amount of post-editing that generated texts require is a crucial component of the cost of using SUMTIME-MOUSAM, and hence of the attractiveness of the system to users (forecasters).", "labels": [], "entities": []}, {"text": "Although we have not measured the time required for performing post-edits, we have used edit-distance measures used in MT evaluations as an approximate cost metric.", "labels": [], "entities": [{"text": "MT evaluations", "start_pos": 119, "end_pos": 133, "type": "TASK", "confidence": 0.9089649021625519}]}, {"text": "We have computed our cost metric by setting different cost (weight) values to different edit operations.", "labels": [], "entities": []}, {"text": "Cost of add and replace operations is set to 5 and cost of delete is set to 1 as used in Su et al.", "labels": [], "entities": [{"text": "Cost", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9399885535240173}]}, {"text": "The ratio of the cost of edits and the cost of writing the entire forecast manually (adding all the words) is computed to be 0.15.", "labels": [], "entities": []}, {"text": "(A) however was perhaps less true than we had hoped.", "labels": [], "entities": []}, {"text": "also described postedited texts in MT as at times noisy.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.7572129368782043}]}, {"text": "Our analysis of manually written forecasts had highlighted a number of \"noise\" elements that made it more difficult to extract information from such corpora.", "labels": [], "entities": []}, {"text": "Basically there are many ways of communicating information in text, and the fact that a generated text doesn't match a corpus text does not mean that the generated text is wrong.", "labels": [], "entities": []}, {"text": "We assumed that people would only post-edit mistakes, where the generated text was wrong or sub-optimal, and hence postedit data would be better for evaluation purposes than corpus comparisons.", "labels": [], "entities": []}, {"text": "In fact, however, there were many justifications for postedits: 1.", "labels": [], "entities": []}, {"text": "Fixing problems in the generated texts (such as overuse of then); 2.", "labels": [], "entities": []}, {"text": "Refining/optimizing the texts (such as using fora time); 3.", "labels": [], "entities": []}, {"text": "Individual preferences (such as easing vs decreasing); and 4.", "labels": [], "entities": []}, {"text": "Downstream consequences of earlier changes (such as introducing SSW in B2, in the example of Section 3.2).", "labels": [], "entities": [{"text": "SSW", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.6121227145195007}]}, {"text": "We wanted to use our post-edit data to improve the system, not just to quantify its performance, and we discovered that we could not do this without attempting to analyze why post-edits were made.", "labels": [], "entities": []}, {"text": "Probably the best way of doing this was to discuss post-edits with the forecasters.", "labels": [], "entities": []}, {"text": "Alternatively, we could have asked forecasters to fill in problem sheets to capture their explanation of post-edits.", "labels": [], "entities": []}, {"text": "Such feedback from the forecasters would have allowed us to reason with postedit data to improve our system.", "labels": [], "entities": []}, {"text": "In] we explained that we found that analysis of human-written corpora was more useful if it was combined with directly working with domain experts; and essentially this (perhaps not surprisingly) is our conclusion about post-edit data as well.", "labels": [], "entities": []}, {"text": "One of the lessons we learnt from this exercise has been that post-edit evaluations are useful to compute a cost metric to quantify the usefulness of a system.", "labels": [], "entities": []}, {"text": "For example, as described earlier, we have computed a cost metric, 0.15 signifying the post-editing effort.", "labels": [], "entities": []}, {"text": "Post-edit evaluations are also useful in revealing general problem areas in a system.", "labels": [], "entities": []}, {"text": "For example, as described in section 3.3, our evaluation showed that ellipsis related problems are more serious in our system than others.", "labels": [], "entities": []}, {"text": "However, post-edit evaluations are not affective in discovering specific problems in a system.", "labels": [], "entities": []}, {"text": "The main reason for this is that many post-edits, as stated earlier, do not actually fix problems in the generated text at all.", "labels": [], "entities": []}, {"text": "The real post-edits that fixed problems in the generated text were buried among the other noisy post-edits.", "labels": [], "entities": []}, {"text": "This lesson of course is the result of our method of postedit evaluation.", "labels": [], "entities": [{"text": "postedit evaluation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7170099318027496}]}, {"text": "Post-editing was not supported by SUMTIME-MOUSAM and forecasters used Marfors (see section 2.3) to perform post-editing.", "labels": [], "entities": []}, {"text": "Therefore, we had to accept the post-edit data with all the noise.", "labels": [], "entities": []}, {"text": "In MT, post-editors often work under predefined guidelines on post-editing and also use post-editing tools.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9767252802848816}]}, {"text": "For example, post-editing tools automatically revise texts to fix 'down-stream' consequences of human edits.", "labels": [], "entities": []}, {"text": "If post-edit tools are similarly integrated into NLG systems, there is going to be a significant reduction in the number of noisy post-edits allowing us to focus on real post-edits.", "labels": [], "entities": []}, {"text": "Because post-editing is subjective varying from individual to individual, we need to understand the post-editing behaviour of individuals to analyze the noisy post-edit data.", "labels": [], "entities": []}, {"text": "Although we have data on forecaster variations in our postedit corpus, these variations have not been observed from different forecasters post-editing the same text.", "labels": [], "entities": [{"text": "postedit corpus", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9243734180927277}]}, {"text": "This we could have achieved by performing a pilot before the actual evaluation.", "labels": [], "entities": []}, {"text": "For the pilot all the forecasters post-edit the same set of forecasts, thus revealing their individual preferences.", "labels": [], "entities": []}, {"text": "Post-edit data from the pilot would have enabled us to factor out the effects of forecaster variation from the real evaluation data.", "labels": [], "entities": []}, {"text": "As described above noise in the post-edit data can be reduced by using post-edit tools and by performing a pilot before the real evaluation.", "labels": [], "entities": []}, {"text": "This means that postedit evaluations need preparation in the form of developing post-edit tools and carrying out pilot studies.", "labels": [], "entities": [{"text": "postedit evaluations", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.9245171844959259}]}, {"text": "This is another lesson we learnt from our current evaluation.", "labels": [], "entities": []}, {"text": "Although analyzing the post-edit data was a major endeavour for us, the overall cost of post-edit evaluation was not much compared to the effort that would have been required to conduct end user experiments on 2728 texts.", "labels": [], "entities": []}, {"text": "Of course, this was only true because SUMTIME-MOUSAM texts were being post-edited in any case by Weathernews.", "labels": [], "entities": [{"text": "Weathernews", "start_pos": 97, "end_pos": 108, "type": "DATASET", "confidence": 0.9838670492172241}]}, {"text": "The cost-effectiveness of post-edit evaluation is less clear if the evaluators must organize and pay for the post-editing, as Mitkov and An Ha did.", "labels": [], "entities": []}, {"text": "In this context we should speculate that when more and more NLG systems are deployed in the real world, post-editing will be accepted as a component in the process of automatic text generation much in the same way post-editing is now apart of MT.", "labels": [], "entities": [{"text": "text generation", "start_pos": 177, "end_pos": 192, "type": "TASK", "confidence": 0.6874443292617798}]}], "tableCaptions": [{"text": " Table 1. Weather Data produced by an NWP model for 12- Jun 2002", "labels": [], "entities": [{"text": "Weather Data", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.8518268764019012}]}, {"text": " Table 3 Match Score for A3 and B1", "labels": [], "entities": [{"text": "Match Score", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.9734867811203003}]}, {"text": " Table 4. Based on the match scores computed above A3 is  aligned with B2. Similarly A1 is aligned with B1. A2 is  unaligned, and treated as a deleted phrase.", "labels": [], "entities": [{"text": "A1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9922358393669128}]}, {"text": " Table 4. Match Score for A3 and B2", "labels": [], "entities": [{"text": "Match Score", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9776684641838074}]}, {"text": " Table 5. Detailed Edit Analysis", "labels": [], "entities": [{"text": "Detailed Edit Analysis", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7193998098373413}]}, {"text": " Table 6. Wind 10m data for 14 Jul 2003", "labels": [], "entities": [{"text": "Wind 10m data for 14 Jul 2003", "start_pos": 10, "end_pos": 39, "type": "DATASET", "confidence": 0.8048360007149833}]}, {"text": " Table  6. Our content determination algorithm first segments the  data in table 6 (see Sripada et al", "labels": [], "entities": [{"text": "content determination", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.8100256621837616}]}, {"text": " Table 7. Results of the Evaluation showing summary cate- gories and their frequencies  Going back to the successfully aligned phrases, 43914  (60%) are perfect matches, and the remaining 21519 (30%)  are mismatches. Table 7 summarises the mismatches.  Here, each mismatch is classified as  \u2022 Ellipses: additions and deletions. For example, delet-", "labels": [], "entities": []}]}