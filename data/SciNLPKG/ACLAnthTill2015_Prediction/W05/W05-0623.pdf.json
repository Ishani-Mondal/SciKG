{"title": [{"text": "A Joint Model for Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7545218666394552}]}], "abstractContent": [{"text": "We present a semantic role labeling system submitted to the closed track of the CoNLL-2005 shared task.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.634270171324412}, {"text": "CoNLL-2005 shared task", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.7911139329274496}]}, {"text": "The system, introduced in (Toutanova et al., 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discrimi-native re-ranking framework.", "labels": [], "entities": []}, {"text": "We also describe experiments aimed at increasing the robustness of the system in the presence of syntactic parse errors.", "labels": [], "entities": []}, {"text": "Our final system achieves F1-Measures of 76.68 and 78.45 on the development and the WSJ portion of the test set, respectively.", "labels": [], "entities": [{"text": "F1-Measures", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9995453953742981}, {"text": "WSJ", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8248413801193237}]}], "introductionContent": [{"text": "It is evident that there are strong statistical patterns in the syntactic realization and ordering of the arguments of verbs; for instance, if an active predicate has an A0 argument it is very likely to come before an A1 argument.", "labels": [], "entities": []}, {"text": "Our model aims to capture such dependencies among the labels of nodes in a syntactic parse tree.", "labels": [], "entities": []}, {"text": "However, building such a model is computationally expensive.", "labels": [], "entities": []}, {"text": "Since the space of possible joint labelings is exponential in the number of parse tree nodes, a model cannot exhaustively consider these labelings unless it makes strong independence assumptions.", "labels": [], "entities": []}, {"text": "To overcome this problem, we adopt a discriminative re-ranking approach reminiscent of).", "labels": [], "entities": []}, {"text": "We use a local model, which labels arguments independently, to generate a smaller number of likely joint labelings.", "labels": [], "entities": []}, {"text": "These candidate labelings are in turn input to a joint model which can use global features and re-score the candidates.", "labels": [], "entities": []}, {"text": "Both the local and global re-ranking models are log-linear (maximum entropy) models.", "labels": [], "entities": []}, {"text": "In the following sections, we briefly describe our local and joint models and the system architecture for combining them.", "labels": [], "entities": []}, {"text": "We list the features used by our models, with an emphasis on new features, and compare the performance of a local and a joint model on the CoNLL shared task.", "labels": [], "entities": [{"text": "CoNLL shared task", "start_pos": 139, "end_pos": 156, "type": "DATASET", "confidence": 0.7569554249445597}]}, {"text": "We also study an approach to increasing the robustness of the semantic role labeling system to syntactic parser errors, by considering multiple parse trees generated by a statistical parser.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our final results we used a joint model with \u03b1 = 1.5 (local model weight), \u03b2 = 1 (parse tree logprobability weight) , N = 15 (candidate labelings from the local model to consider) , and k = 5 (number of alternative parses).", "labels": [], "entities": []}, {"text": "The whole training set for the task was used to train the models.", "labels": [], "entities": []}, {"text": "It takes about 2 hours to train a local identification model, 40 minutes to train a local classification model, and 7 hours to train a joint re-ranking model.", "labels": [], "entities": []}, {"text": "In, we present our final development and test results using this model.", "labels": [], "entities": []}, {"text": "The percentage of perfectly labeled propositions for the three sets is 55.11% (development), 56.52% (test), and 37.06% (Brown test).", "labels": [], "entities": []}, {"text": "The improvement achieved by the joint model relative to the local model is about 2 points absolute in F-Measure, similar to the improvement when gold-standard syntactic parses are used ().", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9818083643913269}]}, {"text": "The relative error reduction is much lower for automatic parses, possibly due to a lower upper bound on performance.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.9540585577487946}]}, {"text": "It is clear from the drop in performance from the WSJ to Brown test set that our learned model's features do not generalize very well to related domains.", "labels": [], "entities": [{"text": "WSJ to Brown test set", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.8953875184059144}]}], "tableCaptions": [{"text": " Table 1: Overall results (top) and detailed results  on the WSJ test (bottom) on the closed track of the  CoNLL shared task.", "labels": [], "entities": [{"text": "WSJ test", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.7377146482467651}, {"text": "CoNLL shared task", "start_pos": 107, "end_pos": 124, "type": "DATASET", "confidence": 0.7157541116078695}]}]}