{"title": [{"text": "A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate?", "labels": [], "entities": [{"text": "Extrinsic Evaluation of Text Summarization", "start_pos": 18, "end_pos": 60, "type": "TASK", "confidence": 0.7675167560577393}]}], "abstractContent": [{"text": "This paper demonstrates the usefulness of summaries in an extrinsic task of relevance judgment based on anew method for measuring agreement , Relevance-Prediction, which compares sub-jects' judgments on summaries with their own judgments on full text documents.", "labels": [], "entities": [{"text": "summaries", "start_pos": 42, "end_pos": 51, "type": "TASK", "confidence": 0.959258496761322}]}, {"text": "We demonstrate that, because this measure is more reliable than previous gold-standard measures, we are able to make stronger statistical statements about the benefits of summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 171, "end_pos": 184, "type": "TASK", "confidence": 0.9784252047538757}]}, {"text": "We found positive correlations between ROUGE scores and two different summary types, where only weak or negative correlations were found using other agreement measures.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9841098189353943}]}, {"text": "However , we show that ROUGE maybe sensitive to the choice of summarization style.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.8600047826766968}, {"text": "summarization", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9695398211479187}]}, {"text": "We discuss the importance of these results and the implications for future summarization evaluations.", "labels": [], "entities": [{"text": "summarization evaluations", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.8971606492996216}]}], "introductionContent": [{"text": "People often prefer to read a summary of a text document, e.g., news headlines, scientific abstracts, movie previews and reviews, and meeting minutes.", "labels": [], "entities": []}, {"text": "Correspondingly, the explosion of online textual material has prompted advanced research in document summarization.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.621488094329834}]}, {"text": "Although researchers have demonstrated that users can read summaries faster than full text () with some loss of accuracy, researchers have found it difficult to draw strong conclusions about the usefulness of summarization due to the low level of interannotator agreement in the gold standards that they have used.", "labels": [], "entities": [{"text": "summaries", "start_pos": 59, "end_pos": 68, "type": "TASK", "confidence": 0.9443551898002625}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9971305727958679}, {"text": "summarization", "start_pos": 209, "end_pos": 222, "type": "TASK", "confidence": 0.9676112532615662}]}, {"text": "Definitive conclusions about the usefulness of summaries would provide justification for continued research and development of new summarization methods.", "labels": [], "entities": [{"text": "summaries", "start_pos": 47, "end_pos": 56, "type": "TASK", "confidence": 0.9739794135093689}, {"text": "summarization", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.9774541258811951}]}, {"text": "To investigate the question of whether text summarization is useful in an extrinsic task, we examined human performance in a relevance assessment task using a human text surrogate (i.e. text intended to stand in the place of a document).", "labels": [], "entities": [{"text": "text summarization", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.6489413678646088}]}, {"text": "We use single-document English summaries as these are sufficient for investigating task-based usefulness, although more elaborate surrogates are possible, e.g., those that span more than one document.", "labels": [], "entities": []}, {"text": "The next section motivates the need for developing anew framework for measuring task-based usefulness.", "labels": [], "entities": []}, {"text": "Section 3 presents a novel extrinsic measure called Relevance-Prediction.", "labels": [], "entities": [{"text": "Relevance-Prediction", "start_pos": 52, "end_pos": 72, "type": "METRIC", "confidence": 0.9309073090553284}]}, {"text": "Section 4 demonstrates that this is a more reliable measure than that of previous gold standard methods, e.g., the LDC-Agreement method used for SUMMAC-style evaluations, and that this reliability allows us to make stronger statistical statements about the benefits of summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 269, "end_pos": 282, "type": "TASK", "confidence": 0.9759735465049744}]}, {"text": "We expect these findings to be important for future summarization evaluations.", "labels": [], "entities": [{"text": "summarization", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9893468022346497}]}, {"text": "Section 5 presents the results of correlation between task usefulness and the Recall Oriented Understudy for Gisting Evaluation (ROUGE) metric.", "labels": [], "entities": []}, {"text": "While we show that ROUGE correlates with task usefulness (using our Relevance-Prediction measure), we detect a slight difference between informative, extractive headlines (containing words from the full document) and less informative, non-extractive \"eye-catchers\" (containing words that might not appear in the full document, and intended to entice a reader to read the entire document).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9902772903442383}, {"text": "Relevance-Prediction", "start_pos": 68, "end_pos": 88, "type": "METRIC", "confidence": 0.9509807825088501}]}, {"text": "Section 6 further highlights the importance of this point and discusses the implications for automatic evaluation of non-extractive summaries.", "labels": [], "entities": []}, {"text": "To evaluate nonextractive summaries reliably, an automatic measure may require knowledge of sophisticated meaning units.", "labels": [], "entities": []}, {"text": "It is our hope that the conclusions drawn herein will prompt investigation into more sophisticated automatic metrics as researchers shift their focus to non-extractive summaries.", "labels": [], "entities": []}], "datasetContent": [{"text": "We define anew extrinsic measure of task-based usefulness called Relevance-Prediction, where we compare a summary-based decision to the subject's own full-text decision rather than to a different subject's decision.", "labels": [], "entities": [{"text": "Relevance-Prediction", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.9491955637931824}]}, {"text": "Our findings differ from that of the SUMMAC results () in that using Relevance-Prediction as an alternative to comparision to a gold standard is a more realistic agreement measure for assessing usefulness in a relevance assessment task.", "labels": [], "entities": [{"text": "Relevance-Prediction", "start_pos": 69, "end_pos": 89, "type": "METRIC", "confidence": 0.9370233416557312}]}, {"text": "For example, users performing browsing tasks must examine document surrogates, but open the full-text only if they expect the document to be interesting to them.", "labels": [], "entities": []}, {"text": "They are not trying to decide if the document will be interesting to someone else.", "labels": [], "entities": []}, {"text": "To determine the usefulness of summarization, we focus on two questions: \u2022 Can users make judgments on summaries that are consistent with their full-text judgments?", "labels": [], "entities": [{"text": "summarization", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9894381761550903}]}, {"text": "\u2022 Can users make judgments on summaries more quickly than on full document text?", "labels": [], "entities": [{"text": "summaries", "start_pos": 30, "end_pos": 39, "type": "TASK", "confidence": 0.8537760376930237}]}, {"text": "First we describe the Relevance-Prediction measure for determining whether users can make accurate judgments with a summary.", "labels": [], "entities": [{"text": "Relevance-Prediction", "start_pos": 22, "end_pos": 42, "type": "METRIC", "confidence": 0.97855544090271}]}, {"text": "Following this, we describe our experiments and results using this measure, including the timing results of summaries compared to full documents.", "labels": [], "entities": [{"text": "timing", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9737227559089661}]}, {"text": "Ten human subjects were recruited to evaluate full-text documents and two summary types.", "labels": [], "entities": []}, {"text": "The original text documents were taken from the Topic Detection and Tracking 3 (TDT-3) corpus (Allan et al., 1999) which contains news stories and headlines, topic and event descriptions, and a mapping between news stories and their related topic and/or events.", "labels": [], "entities": [{"text": "Topic Detection and Tracking 3 (TDT-3) corpus (Allan et al., 1999)", "start_pos": 48, "end_pos": 114, "type": "DATASET", "confidence": 0.8094748631119728}]}, {"text": "Although the TDT-3 collection contains transcribed speech documents, our investigation was restricted to documents that were originally text, i.e., newspaper or newswire, not broadcast news.", "labels": [], "entities": [{"text": "TDT-3 collection", "start_pos": 13, "end_pos": 29, "type": "DATASET", "confidence": 0.9775828719139099}]}, {"text": "For our experiment we selected three distinct events and related document sets 5 from TDT-3.", "labels": [], "entities": [{"text": "TDT-3", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.950191855430603}]}, {"text": "For each event, the subjects were given a description of the event (written by LDC) and then asked to judge relevance of a set of 20 documents associated with that event (using three different presentation types to be discussed below).", "labels": [], "entities": []}, {"text": "The events used from the TDT data set were events from world news occurring in 1998.", "labels": [], "entities": [{"text": "TDT data set", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9544516404469808}]}, {"text": "It is possible that the subjects had some prior knowledge about the events, yet we believe that this would not affect their ability to complete the task.", "labels": [], "entities": []}, {"text": "Subjects' background knowledge of an event can also make this task more similar to real-world browsing tasks, in which subjects are often familiar with the event or topic they are searching for.", "labels": [], "entities": []}, {"text": "The 20 documents were retrieved by a search engine.", "labels": [], "entities": []}, {"text": "We used a constrained subset where exactly half (10) were judged relevant by the LDC annotators.", "labels": [], "entities": []}, {"text": "Because all 20 documents were somewhat similar to the event, this approach ensured that our task would be more difficult than it would be if we had chosen documents from completely unrelated events (where the choice of relevance would be obvious even from a poorly written summary).", "labels": [], "entities": []}, {"text": "Each document was pre-annotated with the headline associated with the original newswire source.", "labels": [], "entities": []}, {"text": "These headlines were used as the first summary type.", "labels": [], "entities": []}, {"text": "We refer to them as HEAD (Headline Surrogate).", "labels": [], "entities": [{"text": "HEAD", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9760801792144775}]}, {"text": "The average length of the HEAD surrogates was 53 characters.", "labels": [], "entities": []}, {"text": "In addition, we commissioned human-generated summaries of each document as the second summary type; we refer to this as HUM (Human Surrogate).", "labels": [], "entities": [{"text": "HUM", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.8781771659851074}]}, {"text": "The average length of the HUM surrogates was 72 characters.", "labels": [], "entities": []}, {"text": "Although neither of these summaries was produced automatically, our experiment allowed us to focus on the question of summary usefulness and to learn about the differences in presentation style as a first step toward experimentation with the output of automatic summarization systems.", "labels": [], "entities": []}, {"text": "Two main factors were measured: (1) differences in judgments for the three presentation types (HEAD, HUM, and the full-text document) and (2) judgment time.", "labels": [], "entities": []}, {"text": "Each subject made a total of 60 judgments for each presentation type since there were 3 distinct events and 20 documents per event.", "labels": [], "entities": []}, {"text": "To facilitate the analysis of the data, the subjects' judgments were constrained to two possibilities, relevant or not relevant.", "labels": [], "entities": []}, {"text": "Although the HEAD and HUM surrogates were both produced by humans, they differed in style.", "labels": [], "entities": []}, {"text": "The HEAD surrogates were shorter than the HUM surrogates by 26%.", "labels": [], "entities": [{"text": "HEAD", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.6397173404693604}]}, {"text": "Many of these were \"eye-catchers\" designed to entice the reader to examine the entire document (i.e., purchase the newspaper); that is, the HEAD surrogates were not intended to stand in the place of the full document.", "labels": [], "entities": []}, {"text": "By contrast, the writers of the HUM surrogates were instructed to write text that conveyed what happened in the full document.", "labels": [], "entities": []}, {"text": "We observed that the HUM surrogates used more words and phrases extracted from the full documents than the HEAD surrogates.", "labels": [], "entities": []}, {"text": "Experiments were conducted using a web browser (Internet Explorer) on a PC in the presence of the experimenter.", "labels": [], "entities": []}, {"text": "Subjects were given written and verbal instructions for completing their task and were asked to make relevance judgments on a practice event set.", "labels": [], "entities": []}, {"text": "The judgments from the practice event set were not included in our experimental results or used in our analyses.", "labels": [], "entities": []}, {"text": "The written instructions were given to aid subjects in determining requirements for relevance.", "labels": [], "entities": []}, {"text": "For example, in an Election event documents describing new people in office, new public officials, change in governments or parliaments were suggested as evidence for relevance.", "labels": [], "entities": []}, {"text": "Each of the ten subjects made judgments on 20 documents for each of three different events.", "labels": [], "entities": []}, {"text": "After reading each document or summary, the subjects clicked on a radio button corresponding to their judgment and clicked a submit button to move to the next document description.", "labels": [], "entities": []}, {"text": "Subjects were not allowed to move to the next summary/document until a valid selection was made.", "labels": [], "entities": []}, {"text": "No backing up was allowed.", "labels": [], "entities": []}, {"text": "Judgment time was computed as the number of seconds it took the subject to read the full text document or surrogate, comprehend it, compare it to the event description, and make a judgment (timed up until the subject clicked the submit button).", "labels": [], "entities": [{"text": "Judgment time", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8282729685306549}]}, {"text": "We hypothesized that the summaries would allow subjects to achieve a Relevance-Prediction rate of 70-90%.", "labels": [], "entities": [{"text": "Relevance-Prediction rate", "start_pos": 69, "end_pos": 94, "type": "METRIC", "confidence": 0.9902426898479462}]}, {"text": "Since these summaries were significantly shorter than the original document text, we expected that the rate would not be 100% compared to the judgments made on the full document text.", "labels": [], "entities": []}, {"text": "However, we expected higher than a 50% ratio, i.e., higher than that of random judgments on all of the surrogates.", "labels": [], "entities": []}, {"text": "We also expected high performance because the meaning of the original document text is best preserved when written by a human.", "labels": [], "entities": []}, {"text": "A second hypothesis is that the HEAD surrogates would yield a significantly lower agreement rate than that of the HUM surrogates.", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 82, "end_pos": 96, "type": "METRIC", "confidence": 0.9705647230148315}]}, {"text": "Our commissioned HUM surrogates were written to stand in place of the full document, whereas the HEAD surrogates were written to catch a reader's interest.", "labels": [], "entities": []}, {"text": "This suggests that the HEAD surrogates might not provide as informative a description of the original documents as the HUM surrogates.", "labels": [], "entities": []}, {"text": "We also tested a third hypothesis: that our RelevancePrediction measure would be more reliable than that of the LDC-Agreement method used for SUMMAC-style evaluations (thus providing a more stable framework for evaluating summarization techniques).", "labels": [], "entities": [{"text": "RelevancePrediction measure", "start_pos": 44, "end_pos": 71, "type": "METRIC", "confidence": 0.9637639820575714}, {"text": "summarization", "start_pos": 222, "end_pos": 235, "type": "TASK", "confidence": 0.9756313562393188}]}, {"text": "LDC-Agreement compares a subject's judgment on a surrogate or full text against the \"correct\" judgments as assigned by the TDT corpus annotators).", "labels": [], "entities": [{"text": "TDT corpus annotators", "start_pos": 123, "end_pos": 144, "type": "DATASET", "confidence": 0.9108483195304871}]}, {"text": "Finally, we tested the hypothesis that using a text summary for judging relevance would take considerably less time than using the corresponding full-text document.", "labels": [], "entities": []}, {"text": "shows the subjects' judgments using both Relevance-Prediction and LDC-Agreement for each of three events.", "labels": [], "entities": [{"text": "Relevance-Prediction", "start_pos": 41, "end_pos": 61, "type": "METRIC", "confidence": 0.9967286586761475}, {"text": "LDC-Agreement", "start_pos": 66, "end_pos": 79, "type": "METRIC", "confidence": 0.9083511233329773}]}, {"text": "Using our Relevance-Prediction measure, the HUM surrogates yield averages between 79% and 86%, with an overall average of 81%, thus confirming our first hypothesis.", "labels": [], "entities": [{"text": "Relevance-Prediction", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9741212129592896}]}, {"text": "However, we failed to confirm our second hypothesis.", "labels": [], "entities": []}, {"text": "The HEAD Relevance-Prediction rates were between 71% and 82%, with an overall average of 76%, which was lower than the rates for HUM, but the difference was not statistically significant.", "labels": [], "entities": [{"text": "HEAD", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.6456066966056824}, {"text": "Relevance-Prediction", "start_pos": 9, "end_pos": 29, "type": "METRIC", "confidence": 0.654578447341919}, {"text": "HUM", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.5424543023109436}]}, {"text": "It appeared that subjects were able to make consistent relevance decisions from the non-extractive HEAD surrogates, even though these were shorter and less informative than the HUM surrogates.", "labels": [], "entities": []}, {"text": "A closer look reveals that the HEAD summaries sometimes contained enough information to judge relevance, yielding almost the same number of true positives: Relevance-Prediction (RP) and LDC-Agreement (LDC) Rates for HEAD and HUM Surrogates for each Event uses words that do not appear in the original document (hope and mistakes), the subject may infer the relevance of this surrogate by relating hope to the notion of forming a coalition government and mistakes to violence.", "labels": [], "entities": [{"text": "Relevance-Prediction (RP)", "start_pos": 156, "end_pos": 181, "type": "METRIC", "confidence": 0.9341571480035782}, {"text": "LDC-Agreement (LDC) Rates", "start_pos": 186, "end_pos": 211, "type": "METRIC", "confidence": 0.8361426830291748}]}, {"text": "On the other hand, we found that the lower degree of informativeness of HEAD surrogates gave rise to over 50% more false negatives than the HUM summaries.", "labels": [], "entities": [{"text": "HUM summaries", "start_pos": 140, "end_pos": 153, "type": "DATASET", "confidence": 0.838433712720871}]}, {"text": "This statistically significant difference will be discussed further in Section 6.", "labels": [], "entities": []}, {"text": "As for our third hypothesis, illustrates a substantial difference between the two agreement measures.", "labels": [], "entities": []}, {"text": "For each of the three events, the RelevancePrediction rate is at least five percent higher than that of the LDC-Agreement approach, with an average of 8.8% increase for the HEAD summary and a 13.3% average increase for the HUM summary.", "labels": [], "entities": [{"text": "RelevancePrediction rate", "start_pos": 34, "end_pos": 58, "type": "METRIC", "confidence": 0.9595455229282379}, {"text": "HUM summary", "start_pos": 223, "end_pos": 234, "type": "DATASET", "confidence": 0.7052982449531555}]}, {"text": "The average rates across events show a statistically significant difference between LDC-Agreement and Relevance-Prediction for both HUM summaries with p<0.01 and HEAD summaries with p<0.05.", "labels": [], "entities": [{"text": "LDC-Agreement", "start_pos": 84, "end_pos": 97, "type": "METRIC", "confidence": 0.8698561191558838}, {"text": "Relevance-Prediction", "start_pos": 102, "end_pos": 122, "type": "METRIC", "confidence": 0.9873009920120239}, {"text": "HUM summaries", "start_pos": 132, "end_pos": 145, "type": "TASK", "confidence": 0.6156770586967468}]}, {"text": "This significance was determined through use of a single factor ANOVA statistical analysis.", "labels": [], "entities": [{"text": "significance", "start_pos": 5, "end_pos": 17, "type": "METRIC", "confidence": 0.9548674821853638}, {"text": "ANOVA", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.903805673122406}]}, {"text": "The higher Relevance-Prediction rate supports our statement that this approach provides a more stable framework for evaluating different summarization techniques.", "labels": [], "entities": [{"text": "Relevance-Prediction rate", "start_pos": 11, "end_pos": 36, "type": "METRIC", "confidence": 0.9819978177547455}, {"text": "summarization", "start_pos": 137, "end_pos": 150, "type": "TASK", "confidence": 0.9883618950843811}]}, {"text": "Finally, the average timing results shown in confirm our fourth hypothesis.", "labels": [], "entities": [{"text": "timing", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.7863667607307434}]}, {"text": "The subjects took 4-5 seconds (on average) to make judgments on both the HEAD and HUM summaries, as compared to about 13.4 seconds to make judgments on full text documents.", "labels": [], "entities": [{"text": "HEAD and HUM summaries", "start_pos": 73, "end_pos": 95, "type": "DATASET", "confidence": 0.6050833612680435}]}, {"text": "This shows that it takes subjects almost 3 times longer to make judgments on full text documents as it took to make judgments on the summaries (HEAD and HUM).", "labels": [], "entities": [{"text": "HEAD", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.722501277923584}]}, {"text": "This finding is not surprising since text summaries are an order of magnitude shorter than full-text documents.", "labels": [], "entities": [{"text": "text summaries", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.5846519023180008}]}, {"text": "We now turn to the task of correlating our extrinsic task performance with scores produced by an intrinsic evaluation measure.", "labels": [], "entities": []}, {"text": "We used the Recall Oriented Understudy for Gisting Evaluation (ROUGE) metric version 1.2.1.", "labels": [], "entities": []}, {"text": "In previous studies ) ROUGE was shown to have a very low correlation with the LDC-Agreement measurement results of the extrinsic task.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9884946942329407}]}, {"text": "This was attributed to low interannotator agreement in the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9354550242424011}]}, {"text": "Our goal was to test whether our new RelevancePrediction technique would allow us to induce higher correlations with ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.9673857092857361}]}], "tableCaptions": [{"text": " Table 1: Relevance-Prediction (RP) and LDC-Agreement (LDC) Rates for HEAD and HUM Surrogates for each Event", "labels": [], "entities": [{"text": "Relevance-Prediction (RP)", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.9414094984531403}, {"text": "LDC-Agreement (LDC) Rates", "start_pos": 40, "end_pos": 65, "type": "METRIC", "confidence": 0.8857087850570678}]}, {"text": " Table 2: Relevance-Prediction Rates for HEAD and HUM Surrogates (Representative Partition of Size 4)", "labels": [], "entities": [{"text": "Relevance-Prediction Rates", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.9729358851909637}, {"text": "HUM Surrogates", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.6708657890558243}]}, {"text": " Table 3: LDC-Agreement Rates for HEAD and HUM Surrogates (Representative Partition of Size 4)", "labels": [], "entities": [{"text": "HUM Surrogates", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.6957789659500122}]}, {"text": " Table 4: Average Rouge-1 Scores for HEAD and HUM Surrogates (Representative Partition of Size 4)", "labels": [], "entities": [{"text": "Average Rouge-1 Scores", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.9402349392573038}, {"text": "HUM Surrogates", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.6130750328302383}]}, {"text": " Table 5: Pearson Correlations with ROUGE-1 for  Relevance-Prediction (RP) and LDC-Agreement (LDC),  where Partition size (P) = 1, 2, and 4", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9302754402160645}, {"text": "Partition size (P)", "start_pos": 107, "end_pos": 125, "type": "METRIC", "confidence": 0.7420204222202301}]}, {"text": " Table 6: Subjects' Judgments and Corresponding Average ROUGE 1 Scores", "labels": [], "entities": [{"text": "ROUGE 1 Scores", "start_pos": 56, "end_pos": 70, "type": "METRIC", "confidence": 0.9407823085784912}]}]}