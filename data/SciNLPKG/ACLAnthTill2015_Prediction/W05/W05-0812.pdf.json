{"title": [{"text": "Improved HMM Alignment Models for Languages with Scarce Resources", "labels": [], "entities": [{"text": "Improved HMM Alignment", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8005944490432739}]}], "abstractContent": [{"text": "We introduce improvements to statistical word alignment based on the Hidden Markov Model.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.6763794223467509}]}, {"text": "One improvement incorporates syntactic knowledge.", "labels": [], "entities": []}, {"text": "Results on the workshop data show that alignment performance exceeds that of a state-of-the art system based on more complex models, resulting in over a 5.5% absolute reduction in error on Romanian-English.", "labels": [], "entities": [{"text": "absolute reduction in error", "start_pos": 158, "end_pos": 185, "type": "METRIC", "confidence": 0.7021389305591583}]}], "introductionContent": [{"text": "The most widely used alignment model is IBM Model 4 (.", "labels": [], "entities": [{"text": "alignment", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.9594796895980835}, {"text": "IBM Model 4", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.8730396628379822}]}, {"text": "In empirical evaluations it has outperformed the other IBM Models and a Hidden Markov Model (HMM).", "labels": [], "entities": []}, {"text": "It was the basis fora system that performed very well in a comparison of several alignment systems.", "labels": [], "entities": []}, {"text": "Implementations are also freely available.", "labels": [], "entities": []}, {"text": "The IBM Model 4 search space cannot be efficiently enumerated; therefore it cannot be trained directly using Expectation Maximization (EM).", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 109, "end_pos": 138, "type": "TASK", "confidence": 0.7256815791130066}]}, {"text": "In practice, a sequence of simpler models such as IBM Model 1 and an HMM Model are used to generate initial parameter estimates and to enumerate a partial search space which can be expanded using hill-climbing heuristics.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.88994167248408}]}, {"text": "IBM Model 4 parameters are then estimated over this partial search space as an approximation to EM.", "labels": [], "entities": [{"text": "EM", "start_pos": 96, "end_pos": 98, "type": "DATASET", "confidence": 0.587256908416748}]}, {"text": "This approach yields good results, but it has been observed that the IBM Model 4 performance is only slightly better than that of the underlying HMM Model used in this bootstrapping process.", "labels": [], "entities": []}, {"text": "Based on this observation, we hypothesize that implementations of IBM Model 4 derive most of their performance benefits from the underlying HMM Model.", "labels": [], "entities": [{"text": "IBM Model 4", "start_pos": 66, "end_pos": 77, "type": "DATASET", "confidence": 0.8562773664792379}]}, {"text": "Furthermore, owing to the simplicity of HMM Models, we believe that they are more conducive to study and improvement than more complex models such as IBM: The improvement in Alignment Error Rate (AER) is shown for both P(f|e) and P(e|f) alignments on the Romanian-English development set over several iterations of the IBM Model 1 \u2192 HMM \u2192 IBM Model 4 training sequence.", "labels": [], "entities": [{"text": "Alignment Error Rate (AER)", "start_pos": 174, "end_pos": 200, "type": "METRIC", "confidence": 0.9628220200538635}, {"text": "Romanian-English development set", "start_pos": 255, "end_pos": 287, "type": "DATASET", "confidence": 0.6676718393961588}, {"text": "IBM Model 1 \u2192 HMM \u2192 IBM Model 4 training sequence", "start_pos": 319, "end_pos": 368, "type": "DATASET", "confidence": 0.6962457786906849}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Training parameters for the workshop data (see Section 2.2). Parameters n, \u03c6 1 , \u03c6 2 , and \u03b1 were used in the  initialization of P(f|e) model, while n \u22121 , \u03c6 \u22121  1 , \u03c6 \u22121  2 , and \u03b1 \u22121 were used in the initialization of the P(e|f) model.", "labels": [], "entities": []}, {"text": " Table 2: Results on the workshop data. The systems highlighted in bold are the ones that were used in the shared task.  For each corpus, the last row shown represents the results that were actually submitted. Note that for English-Hindi,  our self-reported results in the unlimited task are slightly lower than the original results submitted for the workshop,  which contained an error.", "labels": [], "entities": []}]}