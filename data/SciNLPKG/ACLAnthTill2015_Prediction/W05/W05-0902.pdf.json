{"title": [{"text": "On the Subjectivity of Human Authored Short Summaries", "labels": [], "entities": [{"text": "Subjectivity of Human Authored Short Summaries", "start_pos": 7, "end_pos": 53, "type": "TASK", "confidence": 0.6586194187402725}]}], "abstractContent": [{"text": "We address the issue of human subjec-tivity when authoring summaries, aiming at a simple, robust evaluation of machine generated summaries.", "labels": [], "entities": []}, {"text": "Applying across comprehension test on human authored short summaries from broadcast news, the level of subjectivity is gauged among four authors.", "labels": [], "entities": []}, {"text": "The instruction set is simple, thus there is enough room for subjectiv-ity.", "labels": [], "entities": []}, {"text": "However the approach is robust because the test does not use the absolute score, relying instead on relative comparison , effectively alleviating the subjectiv-ity.", "labels": [], "entities": []}, {"text": "Finally we illustrate the application of the above scheme when evaluating the in-formativeness of machine generated summaries .", "labels": [], "entities": []}], "introductionContent": [{"text": "Subjectivity plays an important role when removing the unwanted or redundant information for summarising a document.", "labels": [], "entities": [{"text": "summarising a document", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.9074125289916992}]}, {"text": "Human beings tend to disagree on what should be a 'one good summary'.", "labels": [], "entities": []}, {"text": "This is probably because every individual, whilst arriving at a summary, looks at things from a different perspective.", "labels": [], "entities": []}, {"text": "Guided by various factors such as educational background, profession, personal interests and experience, an individual decides whether a certain aspect is worth being included in a summary.", "labels": [], "entities": []}, {"text": "What might seem relevant to one person could be deemed redundant by another when reading the same story, thus accounting for more than one 'correct' summary.", "labels": [], "entities": []}, {"text": "The issue of subjectivity gains prominence as the compression ratio increases, i.e., the shorter the summary, the larger the number of 'correct' summaries ().", "labels": [], "entities": [{"text": "compression ratio", "start_pos": 50, "end_pos": 67, "type": "METRIC", "confidence": 0.9553031325340271}]}, {"text": "This is due to the fact that assimilation of seemingly important contents takes priority while discarding the redundant information.", "labels": [], "entities": []}, {"text": "This is a highly subjective aspect.", "labels": [], "entities": []}, {"text": "Although the subjectivity reflects individual's thoughts, there will also be some information commonly observed in different summaries of the same story.", "labels": [], "entities": []}, {"text": "Stated otherwise, words in a summary may vary, phrases may vary, and often the grammatical structure may not be the same, but a certain degree of information maybe common across summaries.", "labels": [], "entities": []}, {"text": "To what degree is information uniform across different summaries?", "labels": [], "entities": [{"text": "summaries", "start_pos": 55, "end_pos": 64, "type": "TASK", "confidence": 0.9456355571746826}]}, {"text": "How much subjectivity is there?", "labels": [], "entities": []}, {"text": "How do we account for similar information stated using different words, expressions, or grammatical structure when comparing summaries?", "labels": [], "entities": [{"text": "summaries", "start_pos": 125, "end_pos": 134, "type": "TASK", "confidence": 0.8833609223365784}]}, {"text": "How does this help when gauging the informativeness?", "labels": [], "entities": []}, {"text": "Does the subjectivity cause any adverse effects when evaluating summaries?", "labels": [], "entities": [{"text": "summaries", "start_pos": 64, "end_pos": 73, "type": "TASK", "confidence": 0.9261715412139893}]}, {"text": "It is these questions that we aim to address in this paper.", "labels": [], "entities": []}, {"text": "Let us assume that the atomic facts of a summary account for its relevance.", "labels": [], "entities": []}, {"text": "Then, a simple question that elicits anyone of these atomic facts represents a benchmark for assessing its informativeness.", "labels": [], "entities": []}, {"text": "We wish to evaluate the quality of a summary in terms of atomic facts commonly observed in-, or subjectively discarded from, assorted human authored short summaries.", "labels": [], "entities": []}, {"text": "In our quest to quantify the subjectivity, we devise across comprehension test along the lines of () for extracting atomic contents.", "labels": [], "entities": []}, {"text": "The comprehension testis modelled on a question-answer style framework.", "labels": [], "entities": []}, {"text": "'Crossing' the model turns out to bean effective scheme for measuring the divergence among multiple summaries.", "labels": [], "entities": []}, {"text": "Questions are prepared by the subject who wrote the original summary (Section 3).", "labels": [], "entities": []}, {"text": "Their answers should be derived by reading the summary alone.", "labels": [], "entities": []}, {"text": "Summary-questionnaire pairs are then swapped in such away that any summary is paired with questions written by other subjects (Section 4).", "labels": [], "entities": []}, {"text": "The number of questions that cannot be answered by reading the summary accounts for the subjectiveness of the author (Section 5).", "labels": [], "entities": []}, {"text": "Finally, we address how the cross comprehension test can be used for evaluating machine generated summaries (Section 6).", "labels": [], "entities": [{"text": "evaluating machine generated summaries", "start_pos": 69, "end_pos": 107, "type": "TASK", "confidence": 0.5186571106314659}]}], "datasetContent": [{"text": "Each of the four 'one line' summaries from the 51 broadcast news stories were evaluated using three sets of 'crossed' questions.", "labels": [], "entities": []}, {"text": "shows, when paired with questions by other subjects, how many answers could be found in a candidate summary.", "labels": [], "entities": []}, {"text": "The figure indicates that summaries authored by the different subjects contained 'relevant' information for less than half (47% overall average for four subjects) of questions.", "labels": [], "entities": []}, {"text": "The number goes up slightly (61%) if 'partially relevant' answers are included.", "labels": [], "entities": []}, {"text": "The number of answers that were 'not found ' indicates the level of subjectivity for this 'summary writing' exercise; more than one third (35%) of information that one subject thought Figure 2: Summary relevance was measured when evaluated against questions by other subjects, while questionnaire relevance was calculated when evaluated against summaries by other subjects.", "labels": [], "entities": []}, {"text": "The objective of this evaluation is to measure the information content of machine generated summaries using a human authored summary as a yardstick.", "labels": [], "entities": []}, {"text": "Although very subjective for many cases, a human summary can still be a reference if we do not treat them as a 'gold standard'.", "labels": [], "entities": []}, {"text": "The cross comprehension test of machine generated and human authored summaries is illustrated in Machine generated summary: senate to vote to approve the expansion of north atlantic treaty organisation to bigger nato means us obligations Summary by subject B: US Senate to decide on NATO expansion; US assesses bigger NATO more arms deal but poor ties with Russia.", "labels": [], "entities": [{"text": "NATO expansion", "start_pos": 283, "end_pos": 297, "type": "TASK", "confidence": 0.7613275349140167}]}, {"text": "Questions by subject D: 1.", "labels": [], "entities": []}, {"text": "What is happening to the NATO?", "labels": [], "entities": [{"text": "NATO", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.917723536491394}]}, {"text": "2. Who sees this move as a threat?", "labels": [], "entities": []}, {"text": "3. Who is bearing the main cost?: Evaluation of machine and human authored summaries using questions by the different subject..", "labels": [], "entities": []}, {"text": "Questions are set by the different author from the one who wrote the summary.", "labels": [], "entities": []}, {"text": "A human authored summary may still be the best summary in many respects, but it will no longer be considered perfect.", "labels": [], "entities": []}, {"text": "One may target the relevance level of the human summary (e.g., 61% for the 'one line' summary task from the broadcast news stories) for automatic summarisation research.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 146, "end_pos": 159, "type": "TASK", "confidence": 0.8817034959793091}]}, {"text": "shows one example from those with which we are currently experimenting.", "labels": [], "entities": []}, {"text": "Answers sought by Subject D were 'expansion', 'Russian', and 'American taxpayers', respectively.", "labels": [], "entities": []}, {"text": "Given this question set, answers are 'relevant', 'relevant', and 'not found ' for the summary by Subject B, and answers found in the machine generated summary are 'relevant', 'not found ', and 'not found ', respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: This table shows the average number of  words and characters for each summary, and the av- erage number of questions per summary.", "labels": [], "entities": [{"text": "av- erage number", "start_pos": 97, "end_pos": 113, "type": "METRIC", "confidence": 0.7953746616840363}]}]}