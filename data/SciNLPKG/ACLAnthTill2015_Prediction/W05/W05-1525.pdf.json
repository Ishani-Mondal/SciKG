{"title": [{"text": "Generic parsing for multi-domain semantic interpretation", "labels": [], "entities": [{"text": "Generic parsing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7790444195270538}, {"text": "multi-domain semantic interpretation", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.7206052243709564}]}], "abstractContent": [], "introductionContent": [{"text": "Producing detailed syntactic and semantic representations of natural language is essential for practical dialog systems such as plan-based assistants and tutorial systems.", "labels": [], "entities": []}, {"text": "Development of such systems is time-consuming and costly as they are typically hand-crafted for each application, and dialog corpus data is more difficult to obtain than text.", "labels": [], "entities": []}, {"text": "The TRIPS parser and grammar addresses these issues by providing broad coverage of common constructions in practical dialog and producing semantic representations suitable for dialog processing across domains.", "labels": [], "entities": [{"text": "TRIPS parser", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.5863160490989685}]}, {"text": "Our system bootstraps dialog system development in new domains and helps build parsed corpora.", "labels": [], "entities": []}, {"text": "Evaluating deep parsers is a challenge (e.g.,).", "labels": [], "entities": []}, {"text": "Although common bracketing accuracy metrics may provide a baseline, they are insufficient for applications such as ours that require complete and correct semantic representations produced by the parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.49601638317108154}]}, {"text": "We evaluate our parser on bracketing accuracy against a statistical parser as a baseline, then on a word sense disambiguation task, and finally on full sentence syntactic and semantic accuracy in multiple domains as a realistic measure of system performance and portability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.6713470220565796}, {"text": "word sense disambiguation task", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.6961631700396538}]}], "datasetContent": [{"text": "As a rough baseline, we compared the bracketing accuracy of our parser to that of a statistical parser), Bikel-M, trained on 4294 TRIPS parse trees from the Monroe corpus, task-oriented human dialogs in an emergency rescue domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9347628951072693}, {"text": "Monroe corpus", "start_pos": 157, "end_pos": 170, "type": "DATASET", "confidence": 0.8932850956916809}, {"text": "emergency rescue domain", "start_pos": 206, "end_pos": 229, "type": "TASK", "confidence": 0.7996879617373148}]}, {"text": "100 randomly selected utterances were held out for testing.", "labels": [], "entities": []}, {"text": "The gold standard for evaluation is created with the help of the parser ).", "labels": [], "entities": []}, {"text": "Corpus utterances are parsed, and the parsed output is checked by trained annotators for full-sentence syntactic and semantic accuracy, reliable with a kappa score 0.79.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9721633791923523}]}, {"text": "For test utterances for which TRIPS failed to produce a correct parse, gold standard trees were manually constructed independently by two linguists and reconciled.", "labels": [], "entities": []}, {"text": "shows results for the 100 test utterances and for the subset for which TRIPS finds a spanning parse (74).", "labels": [], "entities": []}, {"text": "Bikel-M performs somewhat better on the bracketing task for the entire test set, which includes utterances for which TRIPS failed to find a parse, but it is lower on complete matches, which are crucial for semantic interpretation.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 206, "end_pos": 229, "type": "TASK", "confidence": 0.7780801057815552}]}, {"text": "All test utts  Word senses are an important part of the LF representation, so we also evaluated TRIPS on word sense tagging against a baseline of the most common word senses in Monroe.", "labels": [], "entities": [{"text": "TRIPS", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9972360730171204}, {"text": "word sense tagging", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.6486301720142365}]}, {"text": "There were 546 instances of ambiguous words in the 100 test utterances.", "labels": [], "entities": []}, {"text": "TRIPS tagged 90.3% (493) of these correctly, compared to the baseline model of 75.3% (411) correct.", "labels": [], "entities": []}, {"text": "To evaluate portability to new domains, we compared TRIPS full sentence accuracy on a subset of Monroe that underwent a fair amount of development ( ) to corpora of keyboard tutorial session transcripts from new domains in basic electronics (BEETLE) and differentiation (LAM) ().", "labels": [], "entities": [{"text": "TRIPS full sentence accuracy", "start_pos": 52, "end_pos": 80, "type": "METRIC", "confidence": 0.5154204294085503}, {"text": "BEETLE", "start_pos": 242, "end_pos": 248, "type": "METRIC", "confidence": 0.9737921953201294}]}, {"text": "The only development for these domains was addition of missing lexical items and two grammar rules.", "labels": [], "entities": []}, {"text": "TRIPS full accuracy requires correct speech act, word sense and thematic role assignment as well as complete constituent match.", "labels": [], "entities": [{"text": "TRIPS", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.7352255582809448}, {"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9973840117454529}]}, {"text": "Error analysis shows that certain senses and subcategorization frames for existing words are still  needed in the new domains, which can be rectified fairly quickly.", "labels": [], "entities": []}, {"text": "Finding and addressing such gaps is part of bootstrapping a system in anew domain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Bracketing results for Monroe test sets (R:  recall, P: precision, CM: complete match).", "labels": [], "entities": [{"text": "Bracketing", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9947891235351562}, {"text": "Monroe test sets", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.795798639456431}, {"text": "R:", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9023589491844177}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.6509796380996704}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.8287757039070129}, {"text": "complete match)", "start_pos": 81, "end_pos": 96, "type": "METRIC", "confidence": 0.8766159017880758}]}, {"text": " Table 2: TRIPS full sentence syntactic and semantic  accuracy in 3 domains (Acc: full accuracy; Cov.: #  spanning parses; Prec: full acc. on spanning parses).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.8170416951179504}, {"text": "Acc", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9931501746177673}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.5325883626937866}]}]}