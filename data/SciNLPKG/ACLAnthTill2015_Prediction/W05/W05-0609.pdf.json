{"title": [{"text": "Discriminative Training of Clustering Functions: Theory and Experiments with Entity Identification", "labels": [], "entities": [{"text": "Entity Identification", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.7327869832515717}]}], "abstractContent": [{"text": "Clustering is an optimization procedure that partitions a set of elements to optimize some criteria, based on a fixed distance metric defined between the elements.", "labels": [], "entities": []}, {"text": "Clustering approaches have been widely applied in natural language processing and it has been shown repeatedly that their success depends on defining a good distance metric, one that is appropriate for the task and the clustering algorithm used.", "labels": [], "entities": []}, {"text": "This paper develops a framework in which clustering is viewed as a learning task, and proposes away to train a distance metric that is appropriate for the chosen clustering algorithm in the context of the given task.", "labels": [], "entities": []}, {"text": "Experiments in the context of the entity identification problem exhibit significant performance improvements over state-of-the-art clustering approaches developed for this problem.", "labels": [], "entities": [{"text": "entity identification problem", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.8691717584927877}]}], "introductionContent": [{"text": "Clustering approaches have been widely applied to natural language processing (NLP) problems.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.7662864327430725}]}, {"text": "Typically, natural language elements (words, phrases, sentences, etc.) are partitioned into non-overlapping classes, based on some distance (or similarity) metric defined between them, in order to provide some level of syntactic or semantic abstraction.", "labels": [], "entities": []}, {"text": "A key example is that of class-based language models) where clustering approaches are used in order to partition words, determined to be similar, into sets.", "labels": [], "entities": []}, {"text": "This enables estimating more robust statistics since these are computed over collections of \"similar\" words.", "labels": [], "entities": []}, {"text": "A large number of different metrics and algorithms have been experimented with these problems ().", "labels": [], "entities": []}, {"text": "Similarity between words was also used as a metric in a distributional clustering algorithm in), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their senses.", "labels": [], "entities": []}, {"text": "At a higher level, () disambiguated personal names by clustering people's home pages using a TFIDF similarity, and several other researchers have applied clustering at the same level in the context of the entity identification problem ().", "labels": [], "entities": [{"text": "entity identification", "start_pos": 205, "end_pos": 226, "type": "TASK", "confidence": 0.7313780188560486}]}, {"text": "Similarly, approaches to coreference resolution) use clustering to identify groups of references to the same entity.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.9532053470611572}]}, {"text": "Clustering is an optimization procedure that takes as input (1) a collection of domain elements along with (2) a distance metric between them and (3) an algorithm selected to partition the data elements, with the goal of optimizing some form of clustering quality with respect to the given distance metric.", "labels": [], "entities": []}, {"text": "For example, the K-Means clustering approach) seeks to maximize a measure of tightness of the resulting clusters based on the Euclidean distance.", "labels": [], "entities": []}, {"text": "Clustering is typically called an unsupervised method, since data elements are used without labels during the clustering process and labels are not used to provide feedback to the optimization process.", "labels": [], "entities": []}, {"text": "E.g., labels are not taken into account when measuring the quality of the partition.", "labels": [], "entities": []}, {"text": "However, in many cases, supervision is used at the application level when determining an appropriate distance metric (e.g.,) and more).", "labels": [], "entities": []}, {"text": "This scenario, however, has several setbacks.", "labels": [], "entities": []}, {"text": "First, the process of clustering, simply a function that partitions a set of elements into different classes, involves no learning and thus lacks flexibility.", "labels": [], "entities": []}, {"text": "Second, clustering quality is typically defined with respect to a fixed distance metric, without utilizing any direct supervision, so the practical clustering outcome could be disparate from one's intention.", "labels": [], "entities": [{"text": "clustering", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.9604758620262146}]}, {"text": "Third, when clustering with a given algorithm and a fixed metric, one in fact makes some implicit assumptions on the data and the task (e.g.,); more on that below).", "labels": [], "entities": []}, {"text": "For example, the optimal conditions under which for K-means works are that the data is generated from a uniform mixture of Gaussian models; this may not hold in reality.", "labels": [], "entities": []}, {"text": "This paper proposes anew clustering framework that addresses all the problems discussed above.", "labels": [], "entities": []}, {"text": "Specifically, we define clustering as a learning task: in the training stage, a partition function, parameterized by a distance metric, is trained with respect to a specific clustering algorithm, with supervision.", "labels": [], "entities": []}, {"text": "Some of the distinct properties of this framework are that: (1) The training stage is formalized as an optimization problem in which a partition function is learned in away that minimizes a clustering error.", "labels": [], "entities": []}, {"text": "The clustering error is well-defined and driven by feedback from labeled data.", "labels": [], "entities": []}, {"text": "(3) Training a distance metric with respect to any given clustering algorithm seeks to minimize the clustering error on training data that, under standard learning theory assumptions, can be shown to imply small error also in the application stage.", "labels": [], "entities": []}, {"text": "(4) We develop a general learning algorithm that can be used to learn an expressive distance metric over the feature space (e.g., it can make use of kernels).", "labels": [], "entities": []}, {"text": "While our approach makes explicit use of labeled data, we argue that, in fact, many clustering applications in natural language also exploit this information off-line, when exploring which metrics are appropriate for the task.", "labels": [], "entities": []}, {"text": "Our framework makes better use of this resource by incorporating it directly into the metric training process; training is driven by true clustering error, computed via the specific algorithm chosen to partition the data.", "labels": [], "entities": []}, {"text": "We study this new framework empirically on the entity identification problem -identifying whether different mentions of real world entities, such as \"JFK\" and \"John Kennedy\", within and across text documents, actually represent the same concept).", "labels": [], "entities": [{"text": "entity identification", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7572823166847229}, {"text": "JFK\" and \"John Kennedy\"", "start_pos": 150, "end_pos": 173, "type": "DATASET", "confidence": 0.6370572618075779}]}, {"text": "Our experimental results exhibit a significant performance improvement over existing approaches (20% \u2212 30% F 1 error reduction) on all three types of entities we study, and indicate its promising prospective in other natural language tasks.", "labels": [], "entities": [{"text": "F 1 error reduction", "start_pos": 107, "end_pos": 126, "type": "METRIC", "confidence": 0.9491171389818192}]}, {"text": "The rest of this paper discusses existing clustering approaches (Sec.", "labels": [], "entities": []}, {"text": "2) and then introduces our Supervised Discriminative Clustering framework (SDC) (Sec. 3) and a general learner for training in it (Sec. 4).", "labels": [], "entities": []}, {"text": "5 describes the entity identification problem and Sec.", "labels": [], "entities": [{"text": "entity identification", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.8203850984573364}]}, {"text": "6 compares different clustering approaches on this task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experimental study focuses on (1) evaluating the supervised discriminative clustering approach on entity identification; (2) comparing it with existing pairwise classification and clustering approaches widely used in similar tasks; and (3) further analyzing the characteristics of this new framework.", "labels": [], "entities": [{"text": "entity identification", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.7829481959342957}]}, {"text": "We use the TREC corpus to evaluate different approaches in identifying three types of entities: People, Locations and Organization.", "labels": [], "entities": [{"text": "TREC corpus", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.8189176321029663}]}, {"text": "For each type, we generate three pairs of training and test sets, each containing about 300 names.", "labels": [], "entities": []}, {"text": "We note that the three entity types yield very different data sets, exhibited by some statistical properties . Results on each entity type will be averaged over the three sets and ten runs of two-fold cross-validation for each of them.", "labels": [], "entities": []}, {"text": "For SDC, given a training set with annotated name pairs, a distance function is first trained using the algorithm in (in 20 iterations) with respect to a clustering algorithm and then be used to partition the corresponding test set with the same algorithm.", "labels": [], "entities": [{"text": "SDC", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9473422765731812}]}, {"text": "For a comparative evaluation, the outcomes of each approach on a test set of names are converted to a classification overall possible pairs of names (including nonmatching pairs).", "labels": [], "entities": []}, {"text": "Only examples in the set M p , those that are predicated to belong to the same entity (positive predictions) are used in the evaluation, and are compared with the set M a of examples annotated as positive.", "labels": [], "entities": []}, {"text": "The performance of an approach is then evaluated by F 1 value, defined as: Ma| |Mp|+|Ma| . The average SoftTFIDF similarity between names of the same entity is 0.81, 0.89 and 0.95 for people, locations and organizations respectively.", "labels": [], "entities": [{"text": "F 1", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9529365599155426}, {"text": "SoftTFIDF similarity", "start_pos": 103, "end_pos": 123, "type": "METRIC", "confidence": 0.5087722837924957}]}, {"text": "presents the performance of different approaches (described in Sec. 5) on identifying the three entity types.", "labels": [], "entities": []}, {"text": "We experimented with different clustering algorithms but only the results by Single-Linkage are reported for Cluster over LMR (P|W) and SDC, since they are the best.", "labels": [], "entities": []}], "tableCaptions": []}