{"title": [{"text": "A Bayesian mixture model for term re-occurrence and burstiness", "labels": [], "entities": [{"text": "burstiness", "start_pos": 52, "end_pos": 62, "type": "TASK", "confidence": 0.7054322361946106}]}], "abstractContent": [{"text": "This paper proposes a model for term re-occurrence in a text collection based on the gaps between successive occurrences of a term.", "labels": [], "entities": []}, {"text": "These gaps are modeled using a mixture of exponential distributions.", "labels": [], "entities": []}, {"text": "Parameter estimation is based on a Bayesian framework that allows us to fit a flexible model.", "labels": [], "entities": [{"text": "Parameter estimation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.813931405544281}]}, {"text": "The model provides measures of a term's re-occurrence rate and within-document burstiness.", "labels": [], "entities": []}, {"text": "The model works for all kinds of terms, be it rare content word, medium frequency term or frequent function word.", "labels": [], "entities": []}, {"text": "A measure is proposed to account for the term's importance based on its distribution pattern in the corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditionally, Information Retrieval (IR) and Statistical Natural Language Processing (NLP) applications have been based on the \"bag of words\" model.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.8550536215305329}]}, {"text": "This model assumes term independence and homogeneity of the text and document under consideration, i.e. the terms in a document are all assumed to be distributed homogeneously.", "labels": [], "entities": []}, {"text": "This immediately leads to the Vector Space representation of text.", "labels": [], "entities": []}, {"text": "The immense popularity of this model is due to the ease with which mathematical and statistical techniques can be applied to it.", "labels": [], "entities": []}, {"text": "The model assumes that once a term occurs in a document, its overall frequency in the entire document is the only useful measure that associates a term with a document.", "labels": [], "entities": []}, {"text": "It does not take into consideration whether the term occurred in the beginning, middle or end of the document.", "labels": [], "entities": []}, {"text": "Neither does it consider whether the term occurs many times in close succession or whether it occurs uniformly throughout the document.", "labels": [], "entities": []}, {"text": "It also assumes that additional positional information does not provide any extra leverage to the performance of the NLP and IR applications based on it.", "labels": [], "entities": []}, {"text": "This assumption has been shown to be wrong in certain applications.", "labels": [], "entities": []}, {"text": "Existing models for term distribution are based on the above assumption, so they can merely estimate the term's frequency in a document or a term's topical behavior fora content term.", "labels": [], "entities": [{"text": "term distribution", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7137510031461716}]}, {"text": "The occurrence of a content word is classified as topical or non-topical based on whether it occurs once or many times in the document.", "labels": [], "entities": []}, {"text": "We are not aware of any existing model that makes less stringent assumptions and models the distribution of occurrences of a term.", "labels": [], "entities": []}, {"text": "In this paper we describe a model for term reoccurrence in text based on the gaps between successive occurrences of the term and the position of its first occurrence in a document.", "labels": [], "entities": [{"text": "term reoccurrence", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.6874129176139832}]}, {"text": "The gaps are modeled by a mixture of exponential distributions.", "labels": [], "entities": []}, {"text": "Nonoccurrence of a term in a document is modeled by the statistical concept of censoring, which states that the event of observing a certain term is censored at the end of the document, i.e. the document length.", "labels": [], "entities": []}, {"text": "The modeling is done in a Bayesian framework.", "labels": [], "entities": []}, {"text": "The organization of the paper is as follows.", "labels": [], "entities": []}, {"text": "In section 2 we discuss existing term distribution models, the issue of burstiness and some other work that demonstrates the failure of the \"bag of words\" as-sumption.", "labels": [], "entities": []}, {"text": "In section 3 we describe our mixture model, the issue of censoring and the Bayesian formulation of the model.", "labels": [], "entities": []}, {"text": "Section 4 describes the Bayesian estimation theory and methodology.", "labels": [], "entities": []}, {"text": "In section 5 we talk about ways of drawing inferences from our model, present parameter estimates on some chosen terms and present case studies fora few selected terms.", "labels": [], "entities": []}, {"text": "We discuss our conclusions and suggest directions for future work in section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Parameter estimates of the model for some  selected terms, sorted by the \ud97b\udf59  \u03bb 1 / \ud97b\udf59  \u03bb 2 value", "labels": [], "entities": []}]}