{"title": [{"text": "Searching for Grammaticality: Propagating Dependencies in the Viterbi Algorithm", "labels": [], "entities": []}], "abstractContent": [{"text": "In many text-to-text generation scenarios (for instance , summarisation), we encounter human-authored sentences that could be composed by recycling portions of related sentences to form new sentences.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.7361047267913818}, {"text": "summarisation)", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.9373676478862762}]}, {"text": "In this paper, we couch the generation of such sentences as a search problem.", "labels": [], "entities": []}, {"text": "We investigate a statistical sentence generation method which recombines words to form new sentences.", "labels": [], "entities": [{"text": "statistical sentence generation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.6371531983216604}]}, {"text": "We propose an extension to the Viterbi algorithm designed to improve the grammaticality of generated sentences.", "labels": [], "entities": []}, {"text": "Within a statistical framework, the extension favours those partially generated strings with a probable dependency tree structure.", "labels": [], "entities": []}, {"text": "Our preliminary evaluations show that our approach generates less fragmented text than a bigram base-line.", "labels": [], "entities": []}], "introductionContent": [{"text": "In abstract-like automatic summary generation, we often require the generation of new and previously unseen summary sentences given some closely related sentences from a source text.", "labels": [], "entities": [{"text": "abstract-like automatic summary generation", "start_pos": 3, "end_pos": 45, "type": "TASK", "confidence": 0.6539093032479286}]}, {"text": "We refer to these as Non-Verbatim Sentences.", "labels": [], "entities": []}, {"text": "These sentences are used instead of extracted sentences fora variety of reasons including improved conciseness and coherence.", "labels": [], "entities": []}, {"text": "The need fora mechanism to generate such sentences is supported by recent work showing that sentence extraction is not sufficient to account for the scope of written human summaries.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7419638186693192}]}, {"text": "Jing and McKeown found that only 42% of sentences from summaries of news text were extracted sentences.", "labels": [], "entities": []}, {"text": "This is also supported by the work of Knight and Marcu (cited by), which finds that only 2.7% of human summary sentences are extracts.", "labels": [], "entities": []}, {"text": "In our own work on United Nations Humanitarian Aid Proposals, we noticed that only 30% of sentences are extracted from the source document, either verbatim or with trivial string replacements.", "labels": [], "entities": [{"text": "United Nations Humanitarian Aid Proposals", "start_pos": 19, "end_pos": 60, "type": "TASK", "confidence": 0.5882112145423889}]}, {"text": "While the figures do vary, it shows that additional mechanisms beyond sentence extraction are needed.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7825050055980682}]}, {"text": "In response to this, our general research problem is one in which given a set of related sentences, a single summary sentence is produced by recycling words from the input sentence set.", "labels": [], "entities": []}, {"text": "In this paper, we adopt a statistical technique to allow easy portability across domains.", "labels": [], "entities": []}, {"text": "The Viterbi algorithm is used to search for the best traversal of a network of words, effectively searching through the sea of possible word sequences.", "labels": [], "entities": []}, {"text": "We modify the algorithm so that it narrows the search space not only to those sequences with a semblance of grammaticality (via n-grams), but further still to those that are grammatical sentences preserving the dependency structure found in the source material.", "labels": [], "entities": []}, {"text": "Consider the following ungrammatical word sequence typical of that produced by an n-gram generator, \"The quick brown fox jumped over the lazy dog slept by the log \".", "labels": [], "entities": []}, {"text": "One diagnosis of the problem is that the word dog is also used as the subject of the second verb slept.", "labels": [], "entities": []}, {"text": "Ideally, we want to avoid such sequences since they introduce text fragments, in this case \"slept by the log\".", "labels": [], "entities": []}, {"text": "We could, for example, record the fact that dog is already governed by the verb jumped, and thus avoid appending a second governing word slept.", "labels": [], "entities": []}, {"text": "To do so, our extension propagates dependency features during the search and uses these features to influence the choice of words suitable for appending to a partially generated sentence.", "labels": [], "entities": []}, {"text": "Dependency relations are derived from shallow syntactic dependency structure.", "labels": [], "entities": []}, {"text": "Specifically, we use representations of relations between ahead and modifier, ignoring relationship labels for the present.", "labels": [], "entities": []}, {"text": "Within the search process, we constrain the choice of future words by preferring words that are likely to attach to the dependency structure of the partially generated sentence.", "labels": [], "entities": []}, {"text": "Thus, sequences with plausible structures are ranked higher.", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the problem in detail and our approach.", "labels": [], "entities": []}, {"text": "We outline our use of the Viterbi algorithm in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we describe how this is extended to cater for dependency features.", "labels": [], "entities": []}, {"text": "We compare related research in Section 5.", "labels": [], "entities": []}, {"text": "A preliminary evaluation is presented and discussed in Section 6.", "labels": [], "entities": []}, {"text": "Finally, we conclude with future work in Section 7.", "labels": [], "entities": []}, {"text": "However, this constitutes an enormous space which requires efficient searching.", "labels": [], "entities": []}, {"text": "Whilst reducing a vocabulary to a suitable subset narrows this space somewhat, we can use statistical models, representing properties of language, to prune the search space of word sequences further to those strings that reflect real language usage.", "labels": [], "entities": []}, {"text": "For example, n-gram models limit the word sequences examined to those that seem grammatically correct, at least for small windows of text.", "labels": [], "entities": []}, {"text": "However, n-grams alone often result in sentences that, whilst near-grammatical, are often just gibberish.", "labels": [], "entities": []}, {"text": "When combined with a (word) content selection model, we narrow the search space even further to those sentences that appear to make sense.", "labels": [], "entities": []}, {"text": "Accordingly, approaches such as Witbrock and Mittal and have investigated models that improve the choice of words in the sentence.", "labels": [], "entities": []}, {"text": "Witbrock and Mittal's content model chooses words that make good headlines, whilst that of Wan et al. attempts to ensure that, given a short document like a news article, only words from sentences of the same subtopic are combined to form anew sentences.", "labels": [], "entities": []}, {"text": "In this paper, we narrow the search space to those sequences that conserve dependency structures from within the input text.", "labels": [], "entities": []}, {"text": "Our algorithm extension essentially passes along the longdistance context of dependency head information of the preceding word sequence, in order to influence the choice of the next word appended to the sentence.", "labels": [], "entities": []}, {"text": "This dependency structure is constructed statistically by an O(n) algorithm, which is folded into the Viterbi algorithm.", "labels": [], "entities": []}, {"text": "Thus, the extension is in an O(n 4 ) algorithm.", "labels": [], "entities": []}, {"text": "The use of dependency relations further constrains the search space.", "labels": [], "entities": []}, {"text": "Competing paths through the search space are ranked taking into account the proposed dependency structures of the partially generated word sequences.", "labels": [], "entities": []}, {"text": "Sentences with probable dependency structures are ranked higher.", "labels": [], "entities": []}, {"text": "To model the probability of a dependency relation, we use the statistical dependency models inspired by those described in Collins.", "labels": [], "entities": [{"text": "Collins", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.9776740074157715}]}], "datasetContent": [{"text": "In this section, we outline our preliminary evaluation of grammaticality in which we compare our dependency based generation method against a baseline.", "labels": [], "entities": []}, {"text": "To study any improvements in grammaticality, we compare our dependency based generation method against a baseline consisting of sentences generated using bigram model.", "labels": [], "entities": []}, {"text": "In the evaluation, we do not use any smoothing algorithms for dependency counts.", "labels": [], "entities": []}, {"text": "For both our approach and the baseline, Katz's Back-off smoothing algorithm is used for bigram probabilities.", "labels": [], "entities": []}, {"text": "For our evaluation cases, we use the Information Fusion data collected by.", "labels": [], "entities": [{"text": "Information Fusion data collected", "start_pos": 37, "end_pos": 70, "type": "DATASET", "confidence": 0.7143465429544449}]}, {"text": "This data is made up of news articles that have been first grouped by topic, and then component sentences further clustered by similarity of events.", "labels": [], "entities": []}, {"text": "There are 100 sentence clusters and on average there are 4 sentences per cluster.", "labels": [], "entities": []}, {"text": "Each sentence in the cluster is initially passed through the Connexor dependency parser (www.connexor.com) to obtain dependency relations.", "labels": [], "entities": []}, {"text": "Each sentence cluster forms an evaluation casein which we generate a single sentence.", "labels": [], "entities": []}, {"text": "Example output and the original text of the cluster is presented in.", "labels": [], "entities": []}, {"text": "To give both our approach and the baseline the greatest chance of generating a sentence, we obtain our bigrams from our evaluation cases.", "labels": [], "entities": []}, {"text": "5 Aside from this preprocessing to collect input sentence bigrams and dependencies, there is no training as such.", "labels": [], "entities": []}, {"text": "For each evaluation case, both our system and the baseline method generates a set of answer strings, from 3 to 40 words in length.", "labels": [], "entities": []}, {"text": "For each generated output of a given sentence length, we count the number of times the Connexor parser resorts to returning partial parses.", "labels": [], "entities": []}, {"text": "This count, albeit a noisy one, is used as our measure of ungrammaticality.", "labels": [], "entities": []}, {"text": "We calculate the average ungrammaticality score across evaluation cases for each sentence length.", "labels": [], "entities": []}, {"text": "Note that this is permissible in this case because we are not making any claims about the coverage of our model.", "labels": [], "entities": []}], "tableCaptions": []}