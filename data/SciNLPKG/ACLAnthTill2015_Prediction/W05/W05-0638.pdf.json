{"title": [{"text": "Exploiting Full Parsing Information to Label Semantic Roles Using an Ensemble of ME and SVM via Integer Linear Programming", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose a method that exploits full parsing information by representing it as features of argument classification models and as constraints in integer linear learning programs.", "labels": [], "entities": [{"text": "argument classification", "start_pos": 108, "end_pos": 131, "type": "TASK", "confidence": 0.7322129011154175}]}, {"text": "In addition, to take advantage of SVM-based and Maximum Entropy-based argument classification models, we incorporate their scoring matrices, and use the combined matrix in the above-mentioned integer linear programs.", "labels": [], "entities": [{"text": "Maximum Entropy-based argument classification", "start_pos": 48, "end_pos": 93, "type": "TASK", "confidence": 0.6057260185480118}]}, {"text": "The experimental results show that full parsing information not only increases the F-score of argument classification models by 0.7%, but also effectively removes all labeling inconsistencies , which increases the F-score by 0.64%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9968991279602051}, {"text": "argument classification", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.6842421889305115}, {"text": "F-score", "start_pos": 214, "end_pos": 221, "type": "METRIC", "confidence": 0.9979758858680725}]}, {"text": "The ensemble of SVM and ME also boosts the F-score by 0.77%.", "labels": [], "entities": [{"text": "SVM", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.9525271058082581}, {"text": "ME", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9775810241699219}, {"text": "F-score", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9984090924263}]}, {"text": "Our system achieves an F-score of 76.53% in the development set and 76.38% in Test WSJ.", "labels": [], "entities": [{"text": "F-score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9997701048851013}, {"text": "Test WSJ", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.8427258431911469}]}], "introductionContent": [{"text": "The Semantic Role Labeling problem can be formulated as a sentence tagging problem.", "labels": [], "entities": [{"text": "Semantic Role Labeling problem", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.8459390103816986}, {"text": "sentence tagging", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.7233654260635376}]}, {"text": "A sentence can be represented as a sequence of words, as phrases (chunks), or as a parsing tree.", "labels": [], "entities": []}, {"text": "The basic units of a sentence are words, phrases, and constituents in these representations, respectively..", "labels": [], "entities": []}, {"text": "established that Constituentby-Constituent (C-by-C) is better than Phrase-byPhrase (P-by-P), which is better than Word-byWord (W-by-W).", "labels": [], "entities": []}, {"text": "This is probably because the boundaries of the constituents coincide with the arguments; therefore, C-by-C has the highest argument identification F-score among the three approaches.", "labels": [], "entities": [{"text": "F-score", "start_pos": 147, "end_pos": 154, "type": "METRIC", "confidence": 0.5447179079055786}]}, {"text": "In addition, a full parsing tree also provides richer syntactic information than a sequence of chunks or words.", "labels": [], "entities": []}, {"text": "compared the seven most common features as well as several features related to the target constituent's parent and sibling constituents.", "labels": [], "entities": []}, {"text": "Their experimental results show that using other constituents' information increases the F-score by 6%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9992226362228394}]}, {"text": "represent full parsing information as constraints in integer linear programs.", "labels": [], "entities": []}, {"text": "Their experimental results show that using such information increases the argument classification accuracy by 1%.", "labels": [], "entities": [{"text": "argument classification", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.735208123922348}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9773688316345215}]}, {"text": "In this paper, we not only add more full parsing features to argument classification models, but also represent full parsing information as constraints in integer linear programs (ILP) to resolve label inconsistencies.", "labels": [], "entities": [{"text": "argument classification", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.7226020991802216}]}, {"text": "We also build an ensemble of two argument classification models: Maximum Entropy and SVM by combining their argument classification results and applying them to the abovementioned ILPs.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data, which is part of the PropBank corpus, consists of sections from the Wall Street Journal part of the Penn Treebank.", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.9609449207782745}, {"text": "Wall Street Journal part of the Penn Treebank", "start_pos": 78, "end_pos": 123, "type": "DATASET", "confidence": 0.9459664300084114}]}, {"text": "All experiments were carried out using Section 2 to Section 21 for training, Section 24 for development, and Section 23 for testing.", "labels": [], "entities": []}, {"text": "Unlike CoNLL-2004, part of the Brown corpus is also included in the test set.", "labels": [], "entities": [{"text": "CoNLL-2004", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.8841036558151245}, {"text": "Brown corpus", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.9686674177646637}]}, {"text": "shows that our system makes little difference to the development set and Test WSJ.", "labels": [], "entities": [{"text": "Test WSJ", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.8679114282131195}]}, {"text": "However, due to the intrinsic difference between the WSJ and Brown corpora, our system performs better on Test WSJ than on Test Brown..", "labels": [], "entities": [{"text": "WSJ", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.754966139793396}, {"text": "Test WSJ", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.8260593116283417}]}, {"text": "Results of all configurations on the development set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Overall results (top) and detailed results  on the WSJ test (bottom).", "labels": [], "entities": [{"text": "WSJ test", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.8195145428180695}]}, {"text": " Table 2. Results of all configurations on the devel- opment set.", "labels": [], "entities": []}]}