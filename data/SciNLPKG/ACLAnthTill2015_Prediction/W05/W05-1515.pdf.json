{"title": [], "abstractContent": [{"text": "Ordinary classification techniques can drive a conceptually simple constituent parser that achieves near state-of-the-art accuracy on standard test sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9981327652931213}]}, {"text": "Here we present such a parser, which avoids some of the limitations of other discriminative parsers.", "labels": [], "entities": []}, {"text": "In particular, it does not place any restrictions upon which types of features are allowed.", "labels": [], "entities": []}, {"text": "We also present several innovations for faster training of dis-criminative parsers: we show how training can be parallelized, how examples can be generated prior to training without a working parser, and how independently trained sub-classifiers that have never done any parsing can be effectively combined into a working parser.", "labels": [], "entities": []}, {"text": "Finally, we propose anew figure-of-merit for best-first parsing with confidence-rated inferences.", "labels": [], "entities": []}, {"text": "Our implementation is freely available at: http://cs.nyu.edu/\u02dcturian/ software/parser/", "labels": [], "entities": []}], "introductionContent": [{"text": "Discriminative machine learning methods have improved accuracy on many NLP tasks, such as POStagging (, machine translation, and relation extraction).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9985238909721375}, {"text": "machine translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.8131844103336334}, {"text": "relation extraction", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.8425402939319611}]}, {"text": "There are strong reasons to believe the same would be true of parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.9784170389175415}]}, {"text": "However, only limited advances have been made thus far, perhaps due to various limitations of extant discriminative parsers.", "labels": [], "entities": []}, {"text": "In this paper, we present some innovations aimed at reducing or eliminating some of these limitations, specifically for the task of constituent parsing: \u2022 We show how constituent parsing can be performed using standard classification techniques.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.6838420182466507}, {"text": "constituent parsing", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.6335067898035049}]}, {"text": "\u2022 Classifiers for different non-terminal labels can be induced independently and hence training can be parallelized.", "labels": [], "entities": []}, {"text": "\u2022 The parser can use arbitrary information to evaluate candidate constituency inferences.", "labels": [], "entities": []}, {"text": "\u2022 Arbitrary confidence scores can be aggregated in a principled manner, which allows beam search.", "labels": [], "entities": [{"text": "Arbitrary confidence scores", "start_pos": 2, "end_pos": 29, "type": "METRIC", "confidence": 0.9316109220186869}, {"text": "beam search", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.8921183943748474}]}, {"text": "In Section 2 we describe our approach to parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9773858785629272}]}, {"text": "In Section 3 we present experimental results.", "labels": [], "entities": []}, {"text": "The following terms will help to explain our work.", "labels": [], "entities": []}, {"text": "A span is a range over contiguous words in the input sentence.", "labels": [], "entities": []}, {"text": "Spans cross if they overlap but neither contains the other.", "labels": [], "entities": []}, {"text": "An item (or constituent) is a (span, label) pair.", "labels": [], "entities": []}, {"text": "A state is a set of parse items, none of which may cross.", "labels": [], "entities": []}, {"text": "A parse inference is a pair (S , i), given by the current state Sand an item i to be added to it.", "labels": [], "entities": []}, {"text": "A parse path (or history) is a sequence of parse inferences over some input sentence).", "labels": [], "entities": []}, {"text": "An item ordering (ordering, for short) constrains the order in which items maybe inferred.", "labels": [], "entities": []}, {"text": "In particular, if we prescribe a complete item ordering, the parser is deterministic and each state corresponds to a unique parse path.", "labels": [], "entities": []}, {"text": "For some input sentence and gold-standard parse, a state is correct if the parser can infer zero or more additional items to obtain the gold-standard parse.", "labels": [], "entities": []}, {"text": "A parse path is correct if it leads to a correct state.", "labels": [], "entities": []}, {"text": "An inference is correct if adding its item to its state is correct.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following Taskar, Klein, Collins, Koller, and Manning (2004), we trained and tested on \u2264 15 word sentences in the English Penn Treebank (), 10% of the entire treebank byword count.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 114, "end_pos": 135, "type": "DATASET", "confidence": 0.7593302130699158}]}, {"text": "We used sections 02-21 (9753 sentences) for training, section 24 (321 sentences) for development, and section 23 (603 sentences) for testing, preprocessed as per.", "labels": [], "entities": []}, {"text": "We evaluated our parser using the standard PARSEVAL measures: labelled precision, recall, and F-measure (LPRC, LRCL, and LFMS, respectively), which are computed based on the number of constituents in the parser's output that match those in the gold-standard parse.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.796808123588562}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9974774718284607}, {"text": "F-measure", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9981133937835693}]}, {"text": "We tested whether the observed differences in PARSEVAL measures are significant at p = 0.05 using a stratified shuffling test (Cohen, 1995, Section 5.3.2) with one million trials.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.5207118988037109}]}, {"text": "As mentioned in Section 1, the parser cannot infer any item that crosses an item already in the state.", "labels": [], "entities": []}, {"text": "We placed three additional candidacy restrictions on inferences: (a) Items must be inferred under the bottom-up item ordering; (b) To ensure the parser does not enter an infinite loop, no two items in a state can have both the same span and the same label; (c) An item can have no more than K = 5 children.", "labels": [], "entities": []}, {"text": "(Only 0.24% of non-terminals in the preprocessed development set have more than five children.)", "labels": [], "entities": []}, {"text": "The number of candidate inferences at each state, as well as the number of training examples generated by the algorithm in Section 2.1.1, is proportional to K.", "labels": [], "entities": []}, {"text": "In our experiment, there were roughly |E \u03bb | \u2248 1.7 million training examples for each classifier.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4 PARSEVAL results on the \u2264 15 words  development set of the baseline, varying the beam  width. Also, the MRL that achieved this LFMS and  the total number of decision tree splits at this MRL.  Dev Dev Dev  MRL #splits  LFMS LRCL LPRC\u02c6\u0398 LPRC\u02c6 LPRC\u02c6\u0398 total  Beam=1 86.36 86.20 86.53  2.03 7068  Baseline 87.16 86.32 88.02  1.42 9297", "labels": [], "entities": []}, {"text": " Table 5 PARSEVAL results on the \u2264 15 words de- velopment set, given the amount of context avail- able. is statistically significant. The score differences  between \"context 0\" and \"context 1\" are significant,  whereas the differences between \"context 1\" and the  baseline are not.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9246342182159424}, {"text": "words de- velopment set", "start_pos": 38, "end_pos": 61, "type": "METRIC", "confidence": 0.5858020842075348}]}, {"text": " Table 6 PARSEVAL results of decision stumps on  the \u2264 15 words development set, through 8200  splits. The differences between the stumps run and  the baseline are statistically significant.  Dev Dev Dev", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.957891047000885}, {"text": "Dev Dev Dev", "start_pos": 192, "end_pos": 203, "type": "DATASET", "confidence": 0.7987313469250997}]}, {"text": " Table 7 PARSEVAL results of deterministic parsers  on the \u2264 15 words development set through 8700  splits. A shaded cell means that the difference be- tween this value and that of the baseline is statisti- cally significant. All differences between l2r and r2l  are significant.  Dev Dev Dev", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9203104972839355}, {"text": "Dev Dev Dev", "start_pos": 281, "end_pos": 292, "type": "DATASET", "confidence": 0.8495550354321798}]}, {"text": " Table 8 PARSEVAL results of the full vocabulary  parser on the \u2264 15 words development set. The dif- ferences between the full vocabulary run and the  baseline are not statistically significant.  Dev Dev Dev", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9894351363182068}]}]}