{"title": [{"text": "Maximum Entropy based Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7300395568211874}]}], "abstractContent": [], "introductionContent": [{"text": "The semantic role labeling (SRL) refers to finding the semantic relation (e.g. Agent, Patient, etc.) between a predicate and syntactic constituents in the sentences.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.8157517910003662}]}, {"text": "Especially, with the argument information of the predicate, we can derive the predicateargument structures, which are useful for the applications such as automatic information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 164, "end_pos": 186, "type": "TASK", "confidence": 0.6821647584438324}]}, {"text": "As previous work on the SRL, there have been many machine learning approaches.", "labels": [], "entities": [{"text": "SRL", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.890950083732605}]}, {"text": "(. In this paper, we present a two-phase SRL method based on a maximum entropy (ME) model.", "labels": [], "entities": [{"text": "SRL", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9740787148475647}, {"text": "maximum entropy (ME)", "start_pos": 63, "end_pos": 83, "type": "METRIC", "confidence": 0.6992773413658142}]}, {"text": "We first identify parse constituents that represent valid semantic arguments of a given predicate, and then assign appropriate semantic roles to the the identified parse constituents.", "labels": [], "entities": []}, {"text": "In the two-phase SRL method, the performance of the argument identification phase is very important, because the argument classification is performed on the region identified at the identification phase.", "labels": [], "entities": [{"text": "SRL", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9693830013275146}, {"text": "argument identification phase", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.7592671612898508}, {"text": "argument classification", "start_pos": 113, "end_pos": 136, "type": "TASK", "confidence": 0.7236942052841187}]}, {"text": "In this study, in order to improve the performance of identification, we try to incorporate clause boundary restriction and tree distance restriction into pre-processing of the identification phase.", "labels": [], "entities": [{"text": "identification", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.9723460078239441}]}, {"text": "Since features for identifying arguments are different from features for classifying a role, we need to determine different feature sets appropriate for the tasks.", "labels": [], "entities": []}, {"text": "We determine final feature sets for each phase with experiments.", "labels": [], "entities": []}, {"text": "We participate in the closed challenge of the CoNLL-2005 shared task and report results on both development and test sets.", "labels": [], "entities": []}, {"text": "A detailed description of the task, data and related work can be found in Carreras and M` arquez (2005).", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the proposed method, we have experimented with CoNLL-2005 datasets (Wall Street sections 02-21 as training set, Charniak' trees).", "labels": [], "entities": [{"text": "CoNLL-2005 datasets", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.9401294589042664}, {"text": "Wall Street sections 02-21", "start_pos": 76, "end_pos": 102, "type": "DATASET", "confidence": 0.9488684684038162}]}, {"text": "The results have been evaluated by using the srl-eval.pl script provided by the shared task organizers.", "labels": [], "entities": []}, {"text": "For building classifiers, we utilized the Zhang le's MaxEnt toolkit 2 , and the L-BFGS parameter estimation algorithm with Gaussian Prior smoothing.", "labels": [], "entities": [{"text": "Zhang le's MaxEnt toolkit", "start_pos": 42, "end_pos": 67, "type": "DATASET", "confidence": 0.7568691730499267}]}, {"text": "shows the different ways of reducing the number of argument candidates.", "labels": [], "entities": []}, {"text": "The 2nd and 3rd columns (#can., %can.) indicate the number of argument candidates and the percentage of argument candidates that satisfy each restriction on the training set.", "labels": [], "entities": []}, {"text": "The 4th and 5th columns (#arg., %arg.) indicate the number of correct arguments and the percentage of correct arguments that satisfy each restriction on the training set.", "labels": [], "entities": []}, {"text": "The last column (F \u03b2=1 ) indicates the performance of the identification task on the development set by applying each restriction.", "labels": [], "entities": [{"text": "F \u03b2", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9848463237285614}]}, {"text": "In no restriction, All 1 extracts candidates from all the nonterminals's child nodes of a tree.", "labels": [], "entities": []}, {"text": "All 2 filter the nonterminals which include at least one non-: Performance of various feature combinations (top) and performance of each phase (bottom).", "labels": [], "entities": []}, {"text": "terminal child 3 . All 3 filter the nonterminals which include at least one nonterminal child and have distance from P p . We use All 3 as a baseline.", "labels": [], "entities": []}, {"text": "In restriction on clause boundary, for example, 2/0 means that the left search boundary for identifying the argument is set to the left boundary of the second upper clause, and the right search boundary is set to the right boundary of the immediate clause.", "labels": [], "entities": []}, {"text": "According to the experimental results, we use 7/1 tree distance restriction for all following experiments.", "labels": [], "entities": []}, {"text": "By applying the restriction, we can remove about 47.3% (%can.=52.70%) of total argument candidates as compared with All 3 . 93.90% (%arg.) corresponds to the upper bound on recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 173, "end_pos": 179, "type": "METRIC", "confidence": 0.9959688186645508}]}, {"text": "In order to estimate the relative contribution of each feature, we measure performance of each phase on the development set by leaving out one feature at a time, as shown in the top of.", "labels": [], "entities": []}, {"text": "Precision, Recall, and F \u03b2=1 represent the performance of the identification task, and Accuracy represent the performance of the classification task only with 100% correct argument identification respectively.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.994878888130188}, {"text": "Recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9943483471870422}, {"text": "F \u03b2", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9892917573451996}, {"text": "Accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9995560050010681}]}, {"text": "All represents the performance of the experiment when all 26 features introduced by section 2.2 are considered.", "labels": [], "entities": []}, {"text": "Finally, for identification, we use 24 features except gov and pos+clau, and obtain an F \u03b2=1 of 80.59%, as shown in the bottom of.", "labels": [], "entities": [{"text": "identification", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.978872537612915}, {"text": "F \u03b2=1", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9643996208906174}]}, {"text": "Also, for classification, we use 23 features except pred type, cont POS, and pos+clau, and obtain an Accuracy of 87.16%.", "labels": [], "entities": [{"text": "classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9630849957466125}, {"text": "Accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9998005032539368}]}, {"text": "presents our best system performance on the development set, and the performance of the same system on the test set.", "labels": [], "entities": []}, {"text": "shows the performance on the development set using the onephase method and the two-phase method respectively.", "labels": [], "entities": []}, {"text": "The one-phase method is implemented by incorporating the identification into the classification.", "labels": [], "entities": []}, {"text": "one-phase shows the performance of the experiment when 25 features except pos+clau are used.", "labels": [], "entities": []}, {"text": "Experimental results show that the two-phase method is better than the one-phase method in our evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Different ways of reducing candidates.", "labels": [], "entities": []}, {"text": " Table 3: Performance of various feature combina- tions (top) and performance of each phase (bottom).", "labels": [], "entities": []}, {"text": " Table 4: Overall results (top) and detailed results on  the WSJ test (bottom).", "labels": [], "entities": [{"text": "WSJ test", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.8141472339630127}]}, {"text": " Table 5: Performance of one-phase vs. two-phase.", "labels": [], "entities": []}]}