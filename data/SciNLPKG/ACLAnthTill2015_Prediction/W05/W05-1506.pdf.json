{"title": [], "abstractContent": [{"text": "We discuss the relevance of k-best parsing to recent applications in natural language processing , and develop efficient algorithms for k-best trees in the framework of hypergraph parsing.", "labels": [], "entities": [{"text": "k-best parsing", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.5287259221076965}, {"text": "hypergraph parsing", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.7330171167850494}]}, {"text": "To demonstrate the efficiency, scal-ability and accuracy of these algorithms, we present experiments on Bikel's implementation of Collins' lexicalized PCFG model, and on Chiang's CFG-based decoder for hierarchical phrase-based translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9987006187438965}, {"text": "hierarchical phrase-based translation", "start_pos": 201, "end_pos": 238, "type": "TASK", "confidence": 0.587605208158493}]}, {"text": "We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications.", "labels": [], "entities": [{"text": "parse reranking", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.9461875259876251}]}], "introductionContent": [{"text": "Many problems in natural language processing (NLP) involve optimizing some objective function over a set of possible analyses of an input string.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.8080118596553802}]}, {"text": "This set is often exponential-sized but can be compactly represented by merging equivalent subanalyses.", "labels": [], "entities": []}, {"text": "If the objective function is compatible with a packed representation, then it can be optimized efficiently by dynamic programming.", "labels": [], "entities": []}, {"text": "For example, the distribution of parse trees fora given sentence under a PCFG can be represented as a packed forest from which the highest-probability tree can be easily extracted.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9209529757499695}]}, {"text": "However, when the objective function f has no compatible packed representation, exact inference would be intractable.", "labels": [], "entities": []}, {"text": "To alleviate this problem, one common approach from machine learning is loopy belief propagation.", "labels": [], "entities": [{"text": "loopy belief propagation", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.5937236944834391}]}, {"text": "Another solution (which is popular in NLP) is to split the computation into two phases: in the first phase, use some compatible objective function f \ud97b\udf59 to produce a k-best list (the top k candidates under f \ud97b\udf59 ), which serves as an approximation to the full set.", "labels": [], "entities": []}, {"text": "Then, in the second phase, optimize f overall the analyses in the k-best list.", "labels": [], "entities": []}, {"text": "A typical example is discriminative reranking on k-best lists from a generative module, such as) for parsing and for translation, where the reranking model has nonlocal features that cannot be computed during parsing proper.", "labels": [], "entities": [{"text": "parsing", "start_pos": 101, "end_pos": 108, "type": "TASK", "confidence": 0.9682419300079346}, {"text": "translation", "start_pos": 117, "end_pos": 128, "type": "TASK", "confidence": 0.9816849827766418}]}, {"text": "Again, the equivalence relation will in general not be compatible with the parsing algorithm, so the k-best lists can be used to approximate f \ud97b\udf59 , as in Data Oriented Parsing) and in speech recognition ().", "labels": [], "entities": [{"text": "Data Oriented Parsing", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.6587938268979391}, {"text": "speech recognition", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.7818852066993713}]}, {"text": "Another instance of this k-best approach is cascaded optimization.", "labels": [], "entities": [{"text": "cascaded optimization", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.8515303134918213}]}, {"text": "NLP systems are often cascades of modules, where we want to optimize the modules' objective functions jointly.", "labels": [], "entities": []}, {"text": "However, often a module is incompatible with the packed representation of the previous module due to factors like non-local dependencies.", "labels": [], "entities": []}, {"text": "So we might want to postpone some disambiguation by propagating k-best lists to subsequent phases, as in joint parsing and semantic role labeling (), information extraction and coreference resolution), and formal semantics of TAG ().", "labels": [], "entities": [{"text": "joint parsing and semantic role labeling", "start_pos": 105, "end_pos": 145, "type": "TASK", "confidence": 0.5593020270268122}, {"text": "information extraction and coreference resolution", "start_pos": 150, "end_pos": 199, "type": "TASK", "confidence": 0.7534151256084443}]}, {"text": "Moreover, much recent work on discriminative training uses k-best lists; they are sometimes used to approximate the normalization constant or partition function (which would otherwise be intractable), or to train a model by optimizing some metric incompatible with the packed representation.", "labels": [], "entities": []}, {"text": "For example, shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score (among other metrics) of the model on the data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 133, "end_pos": 143, "type": "METRIC", "confidence": 0.9879754781723022}]}, {"text": "Similarly, uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.997718334197998}]}, {"text": "For algorithms whose packed representations are graphs, such as Hidden Markov Models and other finitestate methods, Ratnaparkhi's MXPARSE parser), and many stack-based machine translation decoders), the k-best paths problem is well-studied in both pure algorithmic context (see and for surveys) and NLP/Speech community).", "labels": [], "entities": []}, {"text": "This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs () (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders.", "labels": [], "entities": [{"text": "parsing-based MT decoders", "start_pos": 185, "end_pos": 210, "type": "TASK", "confidence": 0.740336020787557}]}, {"text": "Any algorithm expressible as a weighted deductive system falls into this class.", "labels": [], "entities": []}, {"text": "In our experiments, we apply the algorithms to the lexicalized PCFG parser of, which is very similar to Collins' Model 2, and to asynchronous CFG based machine translation system).", "labels": [], "entities": [{"text": "Collins' Model 2", "start_pos": 104, "end_pos": 120, "type": "DATASET", "confidence": 0.9319005608558655}, {"text": "machine translation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.6557949483394623}]}], "datasetContent": [{"text": "We report results from two sets of experiments.", "labels": [], "entities": []}, {"text": "For probabilistic parsing, we implemented Algorithms 0, 1, and 3 on top of a widely-used parser) and conducted experiments on parsing efficiency and the quality of the k-best-lists.", "labels": [], "entities": []}, {"text": "We also implemented Algorithms 2 and 3 in a parsing-based MT decoder ( and report results on decoding speed.", "labels": [], "entities": [{"text": "parsing-based MT decoder", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.6812637646993002}]}, {"text": "Bikel's parser is a state-of-the-art multilingual parser based on lexicalized context-free models.", "labels": [], "entities": []}, {"text": "It does support k-best parsing, but, following Collins' parse-reranking work) (see also Section 5.1.2), it accomplishes this by simply abandoning dynamic programming, i.e., no items are considered equivalent).", "labels": [], "entities": []}, {"text": "Theoretically, the time complexity is exponential inn (the input sentence length) and constant ink, since, without merging of equivalent items, there is no limit on the number of items in the chart.", "labels": [], "entities": [{"text": "ink", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9623761177062988}]}, {"text": "In practice, beam search is used to reduce the observed time.", "labels": [], "entities": [{"text": "beam search", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9313694536685944}]}, {"text": "But with the standard beam width of 10 \u22124 , this method becomes prohibitively expensive for n \u2265 25 on Bikel's parser.", "labels": [], "entities": []}, {"text": "used a narrower 10 \u22123 beam and further applied a cell limit of 100, 6 but, as we will show below, this has a detrimental effect on the quality of the output.", "labels": [], "entities": []}, {"text": "We therefore omit this method from our speed comparisons, and use our implementation of Algorithm 0 (na\u00a8\u0131vena\u00a8\u0131ve) as the baseline.", "labels": [], "entities": []}, {"text": "We implemented our k-best Algorithms 0, 1, and 3 on top of Bikel's parser and conducted experiments on a 2.4 GHz 64-bit AMD Opteron with 32 GB memory.", "labels": [], "entities": []}, {"text": "The program is written in Java 1.5 running on the Sun JVM in server mode with a maximum heap size of 5 GB.", "labels": [], "entities": []}, {"text": "For this experiment, we used sections 02-21 of the Penn Treebank (PTB) as the training data and section 23 (2416 sentences) for evaluation, as is now standard.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 51, "end_pos": 70, "type": "DATASET", "confidence": 0.9769542217254639}]}, {"text": "We ran Bikel's parser using its settings to emulate Model 2 of.", "labels": [], "entities": []}, {"text": "Our second experiment was on a CKY-based decoder fora machine translation system, implemented in Python 2.4 accelerated with Psyco 1.3).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7123732268810272}]}, {"text": "We implemented Algorithms 2 and 3 to compute k-best English translations of Mandarin sentences.", "labels": [], "entities": []}, {"text": "Because the CFG used in this system is large to begin with (millions of rules), and then effectively intersected with a finite-state machine on the English side (the language model), the grammar constant for this system is quite large.", "labels": [], "entities": [{"text": "CFG", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.9023116230964661}]}, {"text": "The decoder uses a relatively narrow beam search for efficiency.", "labels": [], "entities": []}, {"text": "We ran the decoder on a 2.8 GHz Xeon with 4 GB of memory, on 331 sentences from the 2002 NIST MTEval test set.", "labels": [], "entities": [{"text": "NIST MTEval test set", "start_pos": 89, "end_pos": 109, "type": "DATASET", "confidence": 0.8531711995601654}]}, {"text": "We tested Algorithm 2 fork = 2 i , 3 \u2264 i \u2264 10, and Algorithm 3 (offline algorithm) fork = 2 i , 3 \u2264 i \u2264 20.", "labels": [], "entities": []}, {"text": "For each sentence, we measured the time to calculate the k-best list, not including the initial 1-best parsing phase.", "labels": [], "entities": []}, {"text": "We then averaged the times over our test set to produce the graph of, which shows that Algorithm 3 runs an average of about 300 times faster than Algorithm 2.", "labels": [], "entities": []}, {"text": "Furthermore, we were able to test Algorithm 3 up to k = 10 6 in a reasonable amount of time.", "labels": [], "entities": []}], "tableCaptions": []}