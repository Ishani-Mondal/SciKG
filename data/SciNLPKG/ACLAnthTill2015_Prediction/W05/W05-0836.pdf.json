{"title": [{"text": "Training and Evaluating Error Minimization Rules for Statistical Machine Translation", "labels": [], "entities": [{"text": "Evaluating Error Minimization", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.8631404240926107}, {"text": "Statistical Machine Translation", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.8744065562884012}]}], "abstractContent": [{"text": "Decision rules that explicitly account for non-probabilistic evaluation metrics in machine translation typically require special training, often to estimate parameters in exponential models that govern the search space and the selection of candidate translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7688392400741577}]}, {"text": "While the traditional Maximum A Posteriori (MAP) decision rule can be optimized as a piecewise linear function in a greedy search of the parameter space, the Minimum Bayes Risk (MBR) decision rule is not well suited to this technique, a condition that makes past results difficult to compare.", "labels": [], "entities": [{"text": "Maximum A Posteriori", "start_pos": 22, "end_pos": 42, "type": "METRIC", "confidence": 0.7272821664810181}]}, {"text": "We present a novel training approach for non-tractable decision rules, allowing us to compare and evaluate these and other decision rules on a large scale translation task, taking advantage of the high dimensional parameter space available to the phrase based Pharaoh decoder.", "labels": [], "entities": []}, {"text": "This comparison is timely, and important, as decoders evolve to represent more complex search space decisions and are evaluated against innovative evaluation metrics of translation quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "State of the art statistical machine translation takes advantage of exponential models to incorporate a large set of potentially overlapping features to select translations from a set of potential candidates.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.6365170081456503}]}, {"text": "As discussed in, the direct translation model represents the probability of target sentence 'English' e = e 1 . .", "labels": [], "entities": [{"text": "direct translation", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.6498647481203079}]}, {"text": "e I being the translation fora source sentence 'French' f = f 1 . .", "labels": [], "entities": []}, {"text": "f J through an exponential, or log-linear model where e is a single candidate translation for f from the set of all English translations E, \u03bb is the parameter vector for the model, and each h k is a feature function of e and f . In practice, we restrict E to the set Gen(f ) which is a set of highly likely translations discovered by a decoder (.", "labels": [], "entities": []}, {"text": "Selecting a translation from this model under the Maximum A Posteriori (MAP) criteria yields transl \u03bb (f ) = arg max e p \u03bb (e|f ) . This decision rule is optimal under the zeroone loss function, minimizing the Sentence Error Rate ().", "labels": [], "entities": [{"text": "Maximum A Posteriori (MAP)", "start_pos": 50, "end_pos": 76, "type": "METRIC", "confidence": 0.7852915227413177}, {"text": "Sentence Error Rate", "start_pos": 210, "end_pos": 229, "type": "METRIC", "confidence": 0.7028450767199198}]}, {"text": "Using the log-linear form to model p \u03bb (e|f ) gives us the flexibility to introduce overlapping features that can represent global context while decoding (searching the space of candidate translations) and rescoring (ranking a set of candidate translations before performing the arg max operation), albeit at the cost of the traditional source-channel generative model of translation proposed in (.", "labels": [], "entities": []}, {"text": "A significant impact of this paradigm shift, however, has been the movement to leverage the flexibility of the exponential model to maximize performance with respect to automatic evaluation met-rics.", "labels": [], "entities": []}, {"text": "Each evaluation metric considers different aspects of translation quality, both at the sentence and corpus level, often achieving high correlation to human evaluation.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8100587427616119}]}, {"text": "It is clear that the decision rule stated in (1) does not reflect the choice of evaluation metric, and substantial work has been done to correct this mismatch in criteria.", "labels": [], "entities": []}, {"text": "Approaches include integrating the metric into the decision rule, and learning \u03bb to optimize the performance of the decision rule.", "labels": [], "entities": []}, {"text": "In this paper we will compare and evaluate several aspects of these techniques, focusing on Minimum Error Rate (MER) training and Minimum Bayes Risk (MBR) decision rules, within a novel training environment that isolates the impact of each component of these methods.", "labels": [], "entities": [{"text": "Minimum Error Rate (MER", "start_pos": 92, "end_pos": 115, "type": "METRIC", "confidence": 0.7706602275371551}, {"text": "Minimum Bayes Risk (MBR)", "start_pos": 130, "end_pos": 154, "type": "METRIC", "confidence": 0.7503324896097183}]}], "datasetContent": [{"text": "We now describe competing strategies to address the problem of modeling the evaluation metric within the decoding and rescoring process, and introduce our contribution towards training non-tractable error surfaces.", "labels": [], "entities": []}, {"text": "The methods discussed below make use of Gen(f ), the approximation to the complete candidate translation space E, referred to as an n-best list.", "labels": [], "entities": []}, {"text": "Details regarding n-best list generation from decoder output can be found in ().", "labels": [], "entities": []}, {"text": "Our goal is to evaluate the impact of the three decision rules discussed above on a large scale translation task that takes advantage of multidimensional features in the exponential model.", "labels": [], "entities": []}, {"text": "In this section we describe the experimental framework used in this evaluation.", "labels": [], "entities": []}, {"text": "This paper focuses on the BLEU metric as presented in ().", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9626757800579071}]}, {"text": "The BLEU metric is defined on a corpus level as follows.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.961890459060669}]}, {"text": "where p n represent the precision of n-grams suggested in e and BP is a brevity penalty measuring the relative shortness of e over the whole corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9985321760177612}, {"text": "BP", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9982293248176575}]}, {"text": "To use the BLEU metric in the candidate pairwise loss calculation in (4), we need to make a decision regarding cases where higher order n-grams matches are not found between two candidates.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9986996650695801}]}, {"text": "suggest that if any n-grams are not matched then the pairwise BLEU score is set to zero.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.966657966375351}]}, {"text": "As an alternative we first estimate corpuswide n-gram counts on the development set.", "labels": [], "entities": []}, {"text": "When the pairwise counts are collected between sentences pairs, they are added onto the baseline corpus counts to and scored by BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9926066398620605}]}, {"text": "This scoring simulates the process of scoring additional sentences after seeing a whole corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Comparing BLEU scores generated by alternative training methods and decision rules", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9781840443611145}]}]}