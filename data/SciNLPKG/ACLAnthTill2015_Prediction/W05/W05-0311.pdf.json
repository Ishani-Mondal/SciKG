{"title": [{"text": "The Reliability of Anaphoric Annotation, Reconsidered: Taking Ambiguity into Account", "labels": [], "entities": []}], "abstractContent": [{"text": "We report the results of a study of the reliability of anaphoric annotation which (i) involved a substantial number of naive subjects, (ii) used Krippendorff's \u03b1 instead of K to measure agreement, as recently proposed by Passonneau, and (iii) allowed annotators to mark anaphoric expressions as ambiguous.", "labels": [], "entities": []}], "introductionContent": [{"text": "We tackle three limitations with the current state of the art in the annotation of anaphoric relations.", "labels": [], "entities": []}, {"text": "The first problem is the lack of a truly systematic study of agreement on anaphoric annotation in the literature: none of the studies we are aware of) is completely satisfactory, either because only a small number of coders was involved, or because agreement beyond chance couldn't be assessed for lack of an appropriate statistic, a situation recently corrected by.", "labels": [], "entities": []}, {"text": "The second limitation, which is particularly serious when working on dialogue, is our still limited understanding of the degree of agreement on references to abstract objects, as in discourse deixis).", "labels": [], "entities": []}, {"text": "The third shortcoming is a problem that affects all types of semantic annotation.", "labels": [], "entities": []}, {"text": "In all annotation studies we are aware of, 1 the fact that an expression may not have a unique interpretation in the context of its The one exception is occurrence is viewed as a problem with the annotation scheme, to be fixed by, e.g., developing suitably underspecified representations, as done particularly in work on wordsense annotation), but also on dialogue act tagging.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 356, "end_pos": 376, "type": "TASK", "confidence": 0.6421588162581126}]}, {"text": "Unfortunately, the underspecification solution only genuinely applies to cases of polysemy, not homonymy, and anaphoric ambiguity is not a case of polysemy.", "labels": [], "entities": []}, {"text": "Consider the dialogue excerpt in (1): 2 it's not clear to us (nor was to our annotators, as we'll see below) whether the demonstrative that in utterance unit 18.1 refers to the 'bad wheel' or 'the boxcar'; as a result, annotators' judgments may disagree -but this doesn't mean that the annotation scheme is faulty; only that what is being said is genuinely ambiguous.", "labels": [], "entities": []}, {"text": "(1) 18.1 S: ....", "labels": [], "entities": []}, {"text": "18.6 it turns out that the boxcar at Elmira 18.7 has a bad wheel 18.8 and they're ..", "labels": [], "entities": [{"text": "Elmira 18.7", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9549737274646759}]}, {"text": "gonna start fixing that at midnight 18.9 but it won't be ready until 8 19.1 M: oh what a pain in the butt This problem is encountered with all types of annotation; the view that all types of disagreement indicate a problem with the annotation scheme-i.e., that somehow the problem would disappear if only we could find the right annotation scheme, or concentrate on the 'right' types of linguistic judgmentsis, in our opinion, misguided.", "labels": [], "entities": [{"text": "M", "start_pos": 76, "end_pos": 77, "type": "METRIC", "confidence": 0.9914365410804749}]}, {"text": "A better approach is to find when annotators disagree because of intrinsic problems with the text, or, even better, to develop methods to identify genuinely ambiguous expressions-the ultimate goal of this work.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first briefly review previous work on anaphoric annotation and on reliability indices.", "labels": [], "entities": [{"text": "reliability", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9509375095367432}]}, {"text": "We then discuss our experiment with anaphoric annotation, and its results.", "labels": [], "entities": []}, {"text": "Finally, we discuss the implications of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The text annotated in the experiment was dialogue 3.2 from the TRAINS 91 corpus.", "labels": [], "entities": [{"text": "TRAINS 91 corpus", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.9442996581395467}]}, {"text": "Subjects were trained on dialogue 3.1.", "labels": [], "entities": []}, {"text": "The subjects performed their annotations on Viglen Genie workstations with LG Flatron monitors running Windows XP, using the MMAX 2 annotation tool).", "labels": [], "entities": [{"text": "MMAX", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.91370689868927}]}, {"text": "Eighteen paid subjects participated in the experiment, all students at the University of Essex, mostly undergraduates from the Departments of Psychology and Language and Linguistics.", "labels": [], "entities": []}, {"text": "The subjects performed the experiment together in one lab, each working on a separate computer.", "labels": [], "entities": []}, {"text": "The experiment was run in two sessions, each consisting of two hour-long parts separated by a 30 minute break.", "labels": [], "entities": []}, {"text": "The first part of the first session was devoted to training: subjects were given the annotation manual and taught how to use the software, and then annotated the training text together.", "labels": [], "entities": []}, {"text": "After the break, the subjects annotated the first half of the dialogue (up to utterance 19.6).", "labels": [], "entities": []}, {"text": "The second session took place five days later.", "labels": [], "entities": []}, {"text": "In the first part we quickly pointed out some problems in the first session (for instance reminding the subjects to be careful during the annotation), and then immediately the subjects annotated the second half of the dialogue, and wrote up a summary.", "labels": [], "entities": []}, {"text": "The second part of the second session was used fora separate experiment with a different dialogue and a slightly different annotation scheme.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Cases of good agreement on categories", "labels": [], "entities": []}, {"text": " Table 3: Comparing K and \u03b1", "labels": [], "entities": []}, {"text": " Table 4: Values of \u03b1 for the first half of dialogue 3.2", "labels": [], "entities": []}]}