{"title": [{"text": "An Experiment Setup for Collecting Data for Adaptive Output Planning in a Multimodal Dialogue System", "labels": [], "entities": [{"text": "Adaptive Output Planning", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.7802003423372904}]}], "abstractContent": [{"text": "We describe a Wizard-of-Oz experiment setup for the collection of multimodal interaction data fora Music Player application.", "labels": [], "entities": []}, {"text": "This setup was developed and used to collect experimental data as part of a project aimed at building a flexible multimodal dialogue system which provides an interface to an MP3 player, combining speech and screen input and output.", "labels": [], "entities": []}, {"text": "Besides the usual goal of WOZ data collection to get realistic examples of the behavior and expectations of the users, an equally important goal for us was to observe natural behavior of multiple wizards in order to guide our system development.", "labels": [], "entities": [{"text": "WOZ data collection", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7015696962674459}]}, {"text": "The wizards' responses were therefore not constrained by a script.", "labels": [], "entities": []}, {"text": "One of the challenges we had to address was to allow the wizards to produce varied screen output a in real time.", "labels": [], "entities": []}, {"text": "Our setup includes a preliminary screen output planning module, which prepares several versions of possible screen output.", "labels": [], "entities": []}, {"text": "The wizards were free to speak, and/or to select a screen output.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the larger context of the TALK project 1 we are developing a multimodal dialogue system fora Music Player application for in-car and in-home use, which should support natural, flexible interaction and collaborative behavior.", "labels": [], "entities": [{"text": "TALK project 1", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.8200571735699972}]}, {"text": "The system functionalities include playback control, manipulation of playlists, and searching a large MP3 database.", "labels": [], "entities": []}, {"text": "We believe that in order to achieve this goal, the system needs to provide advanced adaptive multimodal output.", "labels": [], "entities": []}, {"text": "We are conducting Wizard-of-Oz experiments] in order to guide the development of our system.", "labels": [], "entities": []}, {"text": "On the one hand, the experiments should give us data on how the potential users interact with such an application.", "labels": [], "entities": []}, {"text": "But we also need data on the multimodal interaction strategies that the system should employ to achieve the desired naturalness, flexibility and collaboration.", "labels": [], "entities": []}, {"text": "We therefore need a setup where the wizard has freedom of choice w.r.t. their response and its realization through single or multiple modalities.", "labels": [], "entities": []}, {"text": "This makes it different from previous multimodal experiments, e.g., in the SmartKom project, where the wizard(s) followed a strict script.", "labels": [], "entities": []}, {"text": "But what we need is also different in several aspects from taking recordings of straight human-human interactions: the wizard does not hear the user's input directly, but only gets a transcription, parts of which are sometimes randomly deleted (in order to approximate imperfect speech recognition); the user does not hear the wizard's spoken output directly either, as the latter is transcribed and re-synthesized (to produce system-like sounding output).", "labels": [], "entities": []}, {"text": "The interactions should thus more realistically approximate an interaction with a system, and thereby contain similar phenomena (cf.).", "labels": [], "entities": []}, {"text": "The wizard should be able to present different screen outputs in different context, depending on the search results and other aspects.", "labels": [], "entities": []}, {"text": "However, the wizard cannot design screens on the fly, because that would take too long.", "labels": [], "entities": []}, {"text": "Therefore, we developed a setup which includes modules that support the wizard by providing automatically calculated screen output options the wizard can select from if s/he want to present some screen output.", "labels": [], "entities": []}, {"text": "Outline In this paper we describe our experiment setup and the first experiences with it.", "labels": [], "entities": []}, {"text": "In Section 2 we overview the research goals that our setup was designed to address.", "labels": [], "entities": []}, {"text": "The actual setup is presented in detail in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4 we describe the collected data, and we summarize the lessons we learnt on the basis of interviewing the experiment participants.", "labels": [], "entities": []}, {"text": "We briefly discuss possible improvements of the setup and our future plans with the data in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our aim was to gather interactions where the wizard can combine spoken and visual feedback, namely, displaying (complete or partial) results of a database search, and the user can speak or select on the screen.", "labels": [], "entities": []}, {"text": "We describe here some of the details of the experiment.", "labels": [], "entities": []}, {"text": "The experimental setup is shown schematically in.", "labels": [], "entities": []}, {"text": "There are five people involved in each session of the experiment: an experiment leader, two transcribers, a user and a wizard.", "labels": [], "entities": []}, {"text": "The wizards play the role of an MP3 player application and are given access to a database of information (but not actual music) of more than 150,000 music albums (almost 1 shows an example screenshot of the music database as it is presented to the wizard.", "labels": [], "entities": []}, {"text": "Subjects are given a set of predefined tasks and are told to accomplish them by using an MP3 player with a multimodal interface.", "labels": [], "entities": []}, {"text": "Tasks include playing songs/albums and building playlists, where the subject is given varying amounts of information to help them find/decide on which song to play or add to the playlist.", "labels": [], "entities": []}, {"text": "Ina part of the session the users also get a primary driving task, using a Lane Change driving simulator.", "labels": [], "entities": [{"text": "Lane Change driving", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.6136357585589091}]}, {"text": "This enabled us to test the viability of combining primary and secondary task in our experiment setup.", "labels": [], "entities": []}, {"text": "We also aimed to gain initial insight regarding the difference in interaction flow under such conditions, particularly with regard to multimodality.", "labels": [], "entities": []}, {"text": "The wizards can speak freely and display the search result or the playlist on the screen.", "labels": [], "entities": []}, {"text": "The users can also speak as well as make selections on the screen.", "labels": [], "entities": []}, {"text": "The user's utterances are immediately transcribed by a typist and also recorded.", "labels": [], "entities": []}, {"text": "The transcription is then presented to the wizard.", "labels": [], "entities": []}, {"text": "We did this for two reasons: (1) To deprive the wizards of information encoded in the intonation of utterances, because our system will not have access to it either.", "labels": [], "entities": []}, {"text": "(2) To be able to corrupt the user input in a controlled way, simulating understanding problems at the acoustic level.", "labels": [], "entities": []}, {"text": "Unlike, who simulate automatic speech recognition errors using phone-confusion models, we used a tool that \"deletes\" parts of the transcribed utterances, replacing them by three dots.", "labels": [], "entities": [{"text": "speech recognition errors", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.7837072511514028}]}, {"text": "Word deletion was triggered by the experiment leader.", "labels": [], "entities": [{"text": "Word deletion", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6393879801034927}]}, {"text": "The word deletion rate varied: 20% of the utterances got weakly and 20% strongly corrupted.", "labels": [], "entities": [{"text": "word deletion", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.6831567883491516}]}, {"text": "In 60% of the cases the wizard saw the transcribed speech uncorrupted.", "labels": [], "entities": []}, {"text": "The wizard's utterances are also transcribed (and recorded): Screenshot from the display presentation tool offering options for screen output to the wizard for second-level of choice what to display an how. and presented to the user via a speech synthesizer.", "labels": [], "entities": []}, {"text": "There are two reasons for doing this: One is to maintain the illusion for the subjects that they are actually interacting with a system, since it is known that there are differences between humanhuman and human-computer dialogue, and we want to elicit behavior in the latter condition; the other has to do with the fact that synthesized speech is imperfect and sometimes difficult to understand, and we wanted to reproduce this condition.", "labels": [], "entities": []}, {"text": "The transcription is also supported by a typing and spelling correction module to minimize speech synthesis errors and thus help maintain the illusion of a working system.", "labels": [], "entities": []}, {"text": "Since it would be impossible for the wizard to construct layouts for screen output on the fly, he gets support for his task from the WOZ system: When the wizard performs a database query, a graphical interface presents him a first level of output alternatives, as shown in.", "labels": [], "entities": [{"text": "WOZ", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.8118095993995667}]}, {"text": "The choices are found (i) albums, (ii) songs, or (iii) artists.", "labels": [], "entities": []}, {"text": "For a second level of choice, the system automatically computes four possible screens, as shown in.", "labels": [], "entities": []}, {"text": "The wizard can chose one of the offered options to display to the user, or decide to clear the user's screen.", "labels": [], "entities": []}, {"text": "Otherwise, the user's screen remains unchanged.", "labels": [], "entities": []}, {"text": "It is therefore up to the wizard to decide whether to use speech only, display only, or to combine speech and display.", "labels": [], "entities": []}, {"text": "The types of screen output are (i) a simple text-message conveying how many results were found, (ii) output of a list of just the names (of albums, songs or artists) with the corresponding number of matches (for songs) or length (for albums), (iii) a table of the complete search results, and (iv) a table of the complete search results, but only displaying a subset of columns.", "labels": [], "entities": []}, {"text": "For each screen output type, the system uses heuristics based on the search to decide, e.g., which columns should be displayed.", "labels": [], "entities": []}, {"text": "These four screens are presented to the wizard in different quadrants on a monitor (cf., allowing for selection with a simple mouse click.", "labels": [], "entities": []}, {"text": "The heuristics for the decision what to display implement preliminary strategies we designed for our system.", "labels": [], "entities": []}, {"text": "We are aware that due to the use of these heuristics, the wizard's output realization may not be always ideal.", "labels": [], "entities": []}, {"text": "We have collected feedback from both the wizards and the users in order to evaluate whether the output options were satisfactory (cf. Section 4 for more details).", "labels": [], "entities": []}, {"text": "Technical Setup To keep our experimental system modular and flexible we implemented it on the basis of the Open Agent Architecture (OAA), which is a framework for integrating a community of software agents in a distributed environment.", "labels": [], "entities": []}, {"text": "Each system module is encapsulated by an OAA wrapper to form an OAA agent, which is able to communicate with the OAA community.", "labels": [], "entities": []}, {"text": "The experimental system consists of 12 agents, all of them written in Java.", "labels": [], "entities": []}, {"text": "We made use of an OAA monitor agent which comes with the current OAA distribution to trace all communication events within the system for logging purposes.", "labels": [], "entities": [{"text": "OAA distribution", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.8894060254096985}]}, {"text": "The setup ran distributed over six PCs running different versions of Windows and Linux.", "labels": [], "entities": []}], "tableCaptions": []}