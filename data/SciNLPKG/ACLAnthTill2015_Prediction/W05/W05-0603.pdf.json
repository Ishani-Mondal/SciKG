{"title": [{"text": "Search Engine Statistics Beyond the n-gram: Application to Noun Compound Bracketing", "labels": [], "entities": []}], "abstractContent": [{"text": "In order to achieve the long-range goal of semantic interpretation of noun compounds , it is often necessary to first determine their syntactic structure.", "labels": [], "entities": [{"text": "semantic interpretation of noun compounds", "start_pos": 43, "end_pos": 84, "type": "TASK", "confidence": 0.8596243143081665}]}, {"text": "This paper describes an unsupervised method for noun compound bracketing which extracts statistics from Web search engines using a \u03c7 2 measure, anew set of surface features, and paraphrases.", "labels": [], "entities": [{"text": "noun compound bracketing", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.7873387336730957}]}, {"text": "On a gold standard, the system achieves results of 89.34% (base-line 66.80%), which is a sizable improvement over the state of the art (80.70%).", "labels": [], "entities": []}], "introductionContent": [{"text": "An important but understudied language analysis problem is that of noun compound bracketing, which is generally viewed as a necessary step towards noun compound interpretation.", "labels": [], "entities": [{"text": "language analysis", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7483312487602234}, {"text": "noun compound bracketing", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.7487401763598124}, {"text": "noun compound interpretation", "start_pos": 147, "end_pos": 175, "type": "TASK", "confidence": 0.7864214976628622}]}, {"text": "Consider the following contrastive pair of noun compounds: (1) liver cell antibody (2) liver cell line In example (1) an antibody targets a liver cell, while (2) refers to a cell line which is derived from the liver.", "labels": [], "entities": []}, {"text": "In order to make these semantic distinctions accurately, it can be useful to begin with the correct grouping of terms, since choosing a particular syntactic structure limits the options left for semantics.", "labels": [], "entities": []}, {"text": "Although equivalent at the part of speech (POS) level, these two noun compounds have different syntactic trees.", "labels": [], "entities": []}, {"text": "The distinction can be represented as a binary tree or, equivalently, as a binary bracketing: In this paper, we describe a highly accurate unsupervised method for making bracketing decisions for noun compounds (NCs).", "labels": [], "entities": [{"text": "bracketing decisions for noun compounds (NCs)", "start_pos": 170, "end_pos": 215, "type": "TASK", "confidence": 0.7398478239774704}]}, {"text": "We improve on the current standard approach of using bigram estimates to compute adjacency and dependency scores by introducing the use of the \u03c7 2 measure for this problem.", "labels": [], "entities": []}, {"text": "We also introduce anew set of surface features for querying Web search engines which prove highly effective.", "labels": [], "entities": [{"text": "querying Web search engines", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.7996932119131088}]}, {"text": "Finally, we experiment with paraphrases for improving prediction statistics.", "labels": [], "entities": []}, {"text": "We have evaluated the application of combinations of these features to predict NC bracketing on two distinct collections, one consisting of terms drawn from encyclopedia text, and another drawn from bioscience text.", "labels": [], "entities": [{"text": "NC bracketing", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.9325726926326752}]}, {"text": "The remainder of this paper describes related work, the word association models, the surface features, the paraphrase features and the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with the dataset from, in order to produce results comparable to those of Lauer and Keller & Lapata.", "labels": [], "entities": []}, {"text": "The set consists of 244 unambiguous 3-noun NCs extracted from Grolier's encyclopedia; however, only 216 of these NCs are unique.", "labels": [], "entities": []}, {"text": "Lauer (1995) derived n-gram frequencies from the Grolier's corpus and tested the dependency and the adjacency models using this text.", "labels": [], "entities": []}, {"text": "To help combat data sparseness issues he also incorporated a taxonomy and some additional information (see Related Work section above).", "labels": [], "entities": []}, {"text": "derived their statistics from the Web and achieved results close to Lauer's using simple lexical models.", "labels": [], "entities": []}, {"text": "We constructed anew set of noun compounds from the biomedical literature.", "labels": [], "entities": []}, {"text": "Using the Open NLP tools, we sentence splitted, tokenized, POS tagged and shallow parsed a set of 1.4 million MEDLINE abstracts (citations between 1994 and 2003).", "labels": [], "entities": []}, {"text": "Then we extracted all 3-noun sequences falling in the last three positions of noun phrases (NPs) found in the shallow parse.", "labels": [], "entities": []}, {"text": "If the NP contained other nouns, the sequence was discarded.", "labels": [], "entities": []}, {"text": "This allows for NCs which are modified by adjectives, determiners, and soon, but prevents extracting 3-noun NCs that are part of longer NCs.", "labels": [], "entities": []}, {"text": "For details, see).", "labels": [], "entities": []}, {"text": "This procedure resulted in 418,678 different NC types.", "labels": [], "entities": []}, {"text": "We manually investigated the most frequent ones, removing those that had errors in tokenization (e.g., containing words like transplan or tation), POS tagging (e.g., acute lung injury, where acute was wrongly tagged as a noun) or shallow parsing (e.g., situ hybridization, that misses in).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 147, "end_pos": 158, "type": "TASK", "confidence": 0.8289584219455719}]}, {"text": "We had to consider the first 843 examples in order to obtain 500 good ones, which suggests an extraction accuracy of 59%.", "labels": [], "entities": [{"text": "extraction", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.7946443557739258}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.8535968065261841}]}, {"text": "This number is low mainly because the tokenizer handles dash-connected words as a single token (e.g. factor-alpha) and many tokens contained other special characters (e.g. cd4+), which cannot be used in a query against a search engine and had to be discarded.", "labels": [], "entities": []}, {"text": "The 500 NCs were annotated independently by two judges, one of which has a biomedical background; the other one was one of the authors.", "labels": [], "entities": []}, {"text": "The problematic cases were reconsidered by the two judges and after agreement was reached, the set contained: 361 left bracketed, 69 right bracketed and 70 ambiguous NCs.", "labels": [], "entities": []}, {"text": "The latter group was excluded from the experiments.", "labels": [], "entities": []}, {"text": "We calculated the inter-annotator agreement on the 430 cases that were marked as unambiguous after agreement.", "labels": [], "entities": []}, {"text": "Using the original annotator's choices, we obtained an agreement of 88% or 82%, depending on whether we consider the annotations, that were initially marked as ambiguous by one of the judges to be correct.", "labels": [], "entities": []}, {"text": "The corresponding values for the kappa statistics were .606 (substantial agreement) and .442 (moderate agreement).", "labels": [], "entities": []}, {"text": "The n-grams, surface features, and paraphrase counts were collected by issuing exact phrase queries, limiting the pages to English and requesting filtering of similar results.", "labels": [], "entities": []}, {"text": "For each NC, we generated all possible word inflections (e.g., tumor and tumors) and alternative word variants (e.g., tumor and tumour).", "labels": [], "entities": []}, {"text": "For the biomedical dataset they were automatically obtained from the UMLS Specialist lexicon.", "labels": [], "entities": [{"text": "UMLS Specialist lexicon", "start_pos": 69, "end_pos": 92, "type": "DATASET", "confidence": 0.9650197426478068}]}, {"text": "10 For Lauer's set we used Carroll's morphological tools.", "labels": [], "entities": []}, {"text": "11 For bigrams, we inflect only the second word.", "labels": [], "entities": []}, {"text": "Similarly, fora prepositional paraphrase we generate all possible inflected forms for the two parts, before and after the preposition.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Lauer Set. Shown are the numbers for cor- rect (  \u221a  ), incorrect (\u00d7), and no prediction (\u2205), fol- lowed by precision (P, calculated over  \u221a  and \u00d7 only)  and coverage (C, % examples with prediction). We  use \"\u2192\" for back-off to another model in case of \u2205.", "labels": [], "entities": [{"text": "cor- rect (  \u221a  )", "start_pos": 47, "end_pos": 64, "type": "METRIC", "confidence": 0.7877968847751617}, {"text": "incorrect", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9365953803062439}, {"text": "fol- lowed", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9627624750137329}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.964419424533844}, {"text": "coverage", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9935888051986694}]}, {"text": " Table 3: Comparison to previous unsupervised  results on Lauer's set. The results of Keller & La- pata are on half of Lauer's set and thus are only in- directly comparable (note the different baseline).", "labels": [], "entities": []}]}