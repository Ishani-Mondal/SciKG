{"title": [{"text": "Evaluating Automatic Summaries of Meeting Recordings", "labels": [], "entities": [{"text": "Evaluating Automatic Summaries of Meeting", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8783366441726684}]}], "abstractContent": [{"text": "The research below explores schemes for evaluating automatic summaries of business meetings, using the ICSI Meeting Corpus (Janin et al., 2003).", "labels": [], "entities": [{"text": "summaries of business meetings", "start_pos": 61, "end_pos": 91, "type": "TASK", "confidence": 0.8315972983837128}, {"text": "ICSI Meeting Corpus", "start_pos": 103, "end_pos": 122, "type": "DATASET", "confidence": 0.9482306241989136}]}, {"text": "Both automatic and subjective evaluations were carried out, with a central interest being whether or not the two types of evaluations correlate with each other.", "labels": [], "entities": []}, {"text": "The evaluation metrics were used to compare and contrast differing approaches to automatic summarization, the deterioration of summary quality on ASR output versus manual transcripts, and to determine whether manual extracts are rated significantly higher than automatic extracts.", "labels": [], "entities": [{"text": "summarization", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.850236713886261}, {"text": "ASR output", "start_pos": 146, "end_pos": 156, "type": "TASK", "confidence": 0.8787935376167297}]}], "introductionContent": [{"text": "In the field of automatic summarization, it is widely agreed upon that more attention needs to be paid to the development of standardized approaches to summarization evaluation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.8203439712524414}, {"text": "summarization evaluation", "start_pos": 152, "end_pos": 176, "type": "TASK", "confidence": 0.960915595293045}]}, {"text": "For example, the current incarnation of the Document Understanding Conference is putting its main focus on the development of evaluation schemes, including semiautomatic approaches to evaluation.", "labels": [], "entities": [{"text": "Document Understanding Conference", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.723356286684672}]}, {"text": "One semiautomatic approach to evaluation is ROUGE (, which is primarily based on ngram co-occurrence between automatic and human summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9837902188301086}]}, {"text": "A key question of the research contained herein is how well ROUGE correlates with human judgments of summaries within the domain of meeting speech.", "labels": [], "entities": []}, {"text": "If it is determined that the two types of evaluations correlate strongly, then ROUGE will likely be a valuable and robust evaluation tool in the development stage of a summarization system, when the cost of frequent human evaluations would be prohibitive.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9884684681892395}, {"text": "summarization", "start_pos": 168, "end_pos": 181, "type": "TASK", "confidence": 0.9786261320114136}]}, {"text": "Three basic approaches to summarization are evaluated and compared below: Maximal Marginal Relevance, Latent Semantic Analysis, and featurebased classification.", "labels": [], "entities": [{"text": "summarization", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.9644309878349304}, {"text": "Latent Semantic Analysis", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.5535615781943003}, {"text": "featurebased classification", "start_pos": 132, "end_pos": 159, "type": "TASK", "confidence": 0.6822839826345444}]}, {"text": "The other major comparisons in this paper are between summaries on ASR versus manual transcripts, and between manual and automatic extracts.", "labels": [], "entities": [{"text": "ASR", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9447717070579529}]}, {"text": "For example, regarding the former, it might be expected that summaries on ASR transcripts would be rated lower than summaries on manual transcripts, due to speech recognition errors.", "labels": [], "entities": []}, {"text": "Regarding the comparison of manual and automatic extracts, the manual extracts can bethought of as a gold standard for the extraction task, representing the performance ceiling that the automatic approaches are aiming for.", "labels": [], "entities": []}, {"text": "More detailed descriptions of the summarization approaches and experimental setup can be found in ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9727146029472351}]}, {"text": "That work relied solely on ROUGE as an evaluation metric, and this paper proceeds to investigate whether ROUGE alone is a reliable metric for our summarization domain, by comparing the automatic scores with recently-gathered human evaluations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9223533272743225}, {"text": "summarization domain", "start_pos": 146, "end_pos": 166, "type": "TASK", "confidence": 0.9221445620059967}]}, {"text": "Also, it should be noted that while we are at the moment only utilizing intrinsic evaluation methods, our ultimate plan is to evaluate these meeting summaries extrinsically within the context of a meeting browser ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We used human summaries of the ICSI Meeting corpus for evaluation and for training the feature-based approaches.", "labels": [], "entities": [{"text": "ICSI Meeting corpus", "start_pos": 31, "end_pos": 50, "type": "DATASET", "confidence": 0.9596331715583801}]}, {"text": "An evaluation set of six meetings was defined and multiple human summaries were created for these meetings, with each test meeting having either three or four manual summaries.", "labels": [], "entities": []}, {"text": "The remaining meetings were regarded as training data and a single human summary was created for these.", "labels": [], "entities": []}, {"text": "Our summaries were created as follows.", "labels": [], "entities": []}, {"text": "Annotators were given access to a graphical user interface (GUI) for browsing an individual meeting that included earlier human annotations: an orthographic transcription time-synchronized with the audio, and a topic segmentation based on a shallow hierarchical decomposition with keyword-based text labels describing each topic segment.", "labels": [], "entities": []}, {"text": "Immediately after authoring a textual summary, annotators were asked to create an extractive summary, using a different GUI.", "labels": [], "entities": []}, {"text": "This GUI showed both their textual summary and the orthographic transcription, without topic segmentation but with one line per dialogue act based on the pre-existing MRDA coding () (The dialogue act categories themselves were not displayed, just the segmentation).", "labels": [], "entities": []}, {"text": "Annotators were told to extract dialogue acts that together would convey the information in the textual summary, and could be used to support the correctness of that summary.", "labels": [], "entities": []}, {"text": "They were given no specific instructions about the number or percentage of acts to extractor about redundant dialogue act.", "labels": [], "entities": []}, {"text": "For each dialogue act extracted, they were then required in a second pass to choose the sentences from the textual summary supported by the dialogue act, creating a many-to-many mapping between the recording and the textual summary.", "labels": [], "entities": []}, {"text": "The MMR and LSA approaches are both unsupervised and do not require labelled training data.", "labels": [], "entities": [{"text": "MMR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7292511463165283}]}, {"text": "For both feature-based approaches, the GMM classifiers were trained on a subset of the training data representing approximately 20 hours of meetings.", "labels": [], "entities": [{"text": "GMM classifiers", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7391887009143829}]}, {"text": "We performed summarization using both the human transcripts and speech recognizer output.", "labels": [], "entities": [{"text": "summarization", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9838432669639587}]}, {"text": "The speech recognizer output was created using baseline acoustic models created using a training set consisting of 300 hours of conversational telephone speech from the Switchboard and Callhome corpora.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.6545715034008026}, {"text": "Callhome corpora", "start_pos": 185, "end_pos": 201, "type": "DATASET", "confidence": 0.8682329654693604}]}, {"text": "The resultant models (cross-word triphones trained on conversational side based cepstral mean normalised PLP features) were then MAP adapted to the meeting domain using the ICSI corpus (.", "labels": [], "entities": [{"text": "ICSI corpus", "start_pos": 173, "end_pos": 184, "type": "DATASET", "confidence": 0.9746787846088409}]}, {"text": "A trigram language model was employed.", "labels": [], "entities": []}, {"text": "Fair recognition output for the whole corpus was obtained by dividing the corpus into four parts, and employing a leave one out procedure (training the acoustic and language models on three parts of the corpus and testing on the fourth, rotating to obtain recognition results for the full corpus).", "labels": [], "entities": []}, {"text": "This resulted in an average word error rate (WER) of 29.5%.", "labels": [], "entities": [{"text": "average word error rate (WER)", "start_pos": 20, "end_pos": 49, "type": "METRIC", "confidence": 0.8528152193341937}]}, {"text": "Automatic segmentation into dialogue acts or sentence boundaries was not performed: the dialogue act boundaries for the manual transcripts were mapped onto the speech recognition output.", "labels": [], "entities": []}, {"text": "A particular interest in our research is how automatic measures of informativeness correlate with human judgments on the same criteria.", "labels": [], "entities": []}, {"text": "During the development stage of a summarization system it is not feasible to employ many hours of manual evaluations, and so a critical issue is whether or not software packages such as ROUGE are able to measure informativeness in away that correlates with subjective summarization evaluations.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9896113872528076}]}], "tableCaptions": [{"text": " Table 1: Human Scores for 4 Approaches on Manual  Transcripts", "labels": [], "entities": []}, {"text": " Table 2: Human Scores for 4 Approaches on ASR  Transcripts", "labels": [], "entities": [{"text": "ASR  Transcripts", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.9028229713439941}]}]}