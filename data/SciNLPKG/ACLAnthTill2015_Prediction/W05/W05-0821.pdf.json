{"title": [{"text": "Improved Language Modeling for Statistical Machine Translation", "labels": [], "entities": [{"text": "Improved Language Modeling", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9145342707633972}, {"text": "Statistical Machine Translation", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.8332685033480326}]}], "abstractContent": [{"text": "Statistical machine translation systems use a combination of one or more translation models and a language model.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.6938058435916901}]}, {"text": "While there is a significant body of research addressing the improvement of translation models, the problem of optimizing language models fora specific translation task has not received much attention.", "labels": [], "entities": []}, {"text": "Typically , standard word trigram models are used as an out-of-the-box component in a statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.6292865872383118}]}, {"text": "In this paper we apply language model-ing techniques that have proved beneficial in automatic speech recognition to the ACL05 machine translation shared data task and demonstrate improvements over a baseline system with a standard language model.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.6352552274862925}, {"text": "machine translation shared data task", "start_pos": 126, "end_pos": 162, "type": "TASK", "confidence": 0.7698374450206756}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) makes use of a noisy channel model where a sentence \u00af e in the desired language can be conceived of as originating as a sentence \u00af fin a source language.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7872301985820135}]}, {"text": "The goal is to find, for every input utterance \u00af f, the best hypothesis \u00af e * such that \u00af e * = argmax \u00af e P (\u00af e| \u00af f ) = argmax \u00af e P ( \u00af f |\u00af e)P (\u00af e) (1) P ( \u00af f |\u00af e) is the translation model expressing probabilistic constraints on the association of source and target strings.", "labels": [], "entities": []}, {"text": "P (\u00af e) is a language model specifying the probability of target language strings.", "labels": [], "entities": []}, {"text": "Usually, a standard word trigram model of the form is used, where \u00af e = e 1 , ..., e l . Each word is predicted based on a history of two preceding words.", "labels": [], "entities": []}, {"text": "Most work in SMT has concentrated on developing better translation models, decoding algorithms, or minimum error rate training for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9952241778373718}, {"text": "SMT", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.99210125207901}]}, {"text": "Comparatively little effort has been spent on language modeling for machine translation.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.6948458850383759}, {"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.786788135766983}]}, {"text": "In other fields, particularly in automatic speech recognition (ASR), there exists a large body of work on statistical language modeling, addressing e.g. the use of word classes, language model adaptation, or alternative probability estimation techniques.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 33, "end_pos": 67, "type": "TASK", "confidence": 0.8167878488699595}, {"text": "statistical language modeling", "start_pos": 106, "end_pos": 135, "type": "TASK", "confidence": 0.773348073164622}, {"text": "language model adaptation", "start_pos": 178, "end_pos": 203, "type": "TASK", "confidence": 0.640228678782781}]}, {"text": "The goal of this study was to use some of the language modeling techniques that have proved beneficial for ASR in the past and to investigate whether they transfer to statistical machine translation.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7000337541103363}, {"text": "ASR", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9945888519287109}, {"text": "statistical machine translation", "start_pos": 167, "end_pos": 198, "type": "TASK", "confidence": 0.6568023761113485}]}, {"text": "In particular, this includes language models that make use of morphological and part-of-speech information, so-called factored language models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: First-pass (left column) and oracle results  (right column) on the dev set (% BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9982245564460754}]}, {"text": " Table 2: Second-pass rescoring results (% BLEU)  on the dev set for 4-gram LM, 3-gram FLM, and  their combination.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9991582632064819}, {"text": "FLM", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.8665077090263367}]}]}