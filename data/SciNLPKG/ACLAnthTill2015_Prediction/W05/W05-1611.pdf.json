{"title": [{"text": "Discrete Optimization as an Alternative to Sequential Processing in NLG", "labels": [], "entities": [{"text": "Discrete Optimization", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8244790434837341}]}], "abstractContent": [{"text": "We present an NLG system that uses Integer Linear Programming to integrate different decisions involved in the generation process.", "labels": [], "entities": []}, {"text": "Our approach provides an alternative to pipeline-based sequential processing which has become prevalent in today's NLG applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "From an engineering perspective, one of the major considerations in building a Natural Language Generation (NLG) system is the choice of the architecture.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 79, "end_pos": 112, "type": "TASK", "confidence": 0.7560731569925944}]}, {"text": "Two important issues that need to be considered at this stage are firstly, the modularization of the linguistic decisions involved in the generation process and secondly, the processing flow (cf.).", "labels": [], "entities": []}, {"text": "On one side of the spectrum lie integrated systems, with all linguistic decisions being handled within a single process (e.g.).", "labels": [], "entities": []}, {"text": "Such architectures are theoretically attractive, as they assume a close coordination of different types of linguistic decisions, which are known to be dependent on one another (cf. e.g.).", "labels": [], "entities": []}, {"text": "A major disadvantage of integrated models is the complexity that they necessarily involve, which results in poor portability and scalability.", "labels": [], "entities": []}, {"text": "On the other side of the spectrum there are highly modularized pipeline architectures.", "labels": [], "entities": []}, {"text": "A prominent example of this second case is the consensus pipeline architecture recognized by and further elaborated in.", "labels": [], "entities": []}, {"text": "The modularization of Reiter's model occurs at two levels.", "labels": [], "entities": []}, {"text": "First, individual linguistic decisions of the same type (e.g. involving lexical or syntactic choice) are grouped together within single low level tasks, such as lexicalization, aggregation or ordering.", "labels": [], "entities": []}, {"text": "Second, tasks are allocated to three highlevel generation stages, i.e. Document Planning, Microplanning and Surface Realization.", "labels": [], "entities": [{"text": "Document Planning", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.9268705248832703}, {"text": "Surface Realization", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7007024437189102}]}, {"text": "The processing flow in the pipeline architecture is sequential, with individual tasks being executed in a predetermined order.", "labels": [], "entities": []}, {"text": "A study of applied NLG systems] reveals, however, that while most applied NLG systems rely on sequential processing, they do not follow the strict modularization that the consensus model assumes.", "labels": [], "entities": []}, {"text": "Low-level tasks are spread over various generation stages and may in fact be executed more than once at diverse positions in the pipeline.", "labels": [], "entities": []}, {"text": "An attempt to account for commonalities that many NLG systems share, without imposing too many restrictions, as is the case with Reiter's \"consensus\" model, is the Reference Architecture for Generation Systems (RAGS).", "labels": [], "entities": []}, {"text": "RAGS is an abstract specification of an NLG architecture that focuses on two issues: data types that the generation process manipulates and a generic model of the interactions between modules, based on a common central server.", "labels": [], "entities": []}, {"text": "An important feature of RAGS is that it leaves the question of processing flow to the actual implementation.", "labels": [], "entities": [{"text": "RAGS", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.9325307011604309}]}, {"text": "Hence it is theoretically possible to build both fully integrated as well as pipeline-based systems that would observe the RAGS principles.", "labels": [], "entities": [{"text": "RAGS", "start_pos": 123, "end_pos": 127, "type": "TASK", "confidence": 0.5911821722984314}]}, {"text": "Two implementations of RAGS presented in demonstrate an intermediate way.", "labels": [], "entities": [{"text": "RAGS", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.7979110479354858}]}, {"text": "In this paper we present a novel approach to building an integrated NLG system, in which the generation process is modeled as a discrete optimization problem.", "labels": [], "entities": []}, {"text": "It provides an extension to the classification-based generation framework, presented in.", "labels": [], "entities": []}, {"text": "We first assume modularization of the generation process at the lowest possible level: individual tasks correspond to realizations of single form elements (FEs) that buildup a linguistic expression.", "labels": [], "entities": []}, {"text": "The decisions that these tasks involve are then represented as classification tasks and integrated via an Integer Linear Programming (ILP) formulation (see e.g..", "labels": [], "entities": []}, {"text": "This way we avoid the well known ordering problem that is present in all pipeline-based systems.", "labels": [], "entities": []}, {"text": "Observing, at least partially, the methodological principles of RAGS, we specify the architecture of our system at two independent levels.", "labels": [], "entities": [{"text": "RAGS", "start_pos": 64, "end_pos": 68, "type": "TASK", "confidence": 0.7654077410697937}]}, {"text": "At the abstract level, the low-level generation tasks are defined, all based on the same input/output interface.", "labels": [], "entities": []}, {"text": "At the implementation level, the processing flow and integration method are determined.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: in Section 2 we present briefly the classification-based generation framework and remark on the shortcomings of pipeline-based processing.", "labels": [], "entities": []}, {"text": "In Section 3 we introduce the ILP formulation of the generation task, and in Section 4 we report on the experiments and evaluation of the system.", "labels": [], "entities": [{"text": "ILP formulation", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.6844408214092255}]}], "datasetContent": [{"text": "In order to evaluate our approach we conducted a series of experiments with two implementations of the ILP model and two different pipelines.", "labels": [], "entities": []}, {"text": "Each system takes as input the treebased representation of the semantic content of route directions described in Section 2.", "labels": [], "entities": []}, {"text": "The generation process traverses the temporal tree in a depth-first fashion, and for each node a single discourse unit is realized.: Joint distribution matrix for selected labels of tasks Connective (horizontal) and Verb Form (vertical), computed for all discourse units in a corpus.", "labels": [], "entities": []}, {"text": "We evaluated our system using leave-one-out crossvalidation, i.e. for all texts in the corpus, each text was used once for testing, and the remaining texts provided the training data.", "labels": [], "entities": []}, {"text": "To solve individual classification tasks we used the decision tree learner C4.5 in the pipeline systems and the Naive Bayes algorithm: Results reached by the implemented ILP systems and two baselines.", "labels": [], "entities": []}, {"text": "For both pipeline systems, Pos.", "labels": [], "entities": []}, {"text": "stands for the position of the tasks in the pipeline.", "labels": [], "entities": []}, {"text": "yielded highest results in the respective configurations . To solve the ILP models we used lp solve, a highly efficient GNU-licence Mixed Integer Programming (MIP) solver 11 , that implements the Branch-and-Bound algorithm.", "labels": [], "entities": [{"text": "GNU-licence Mixed Integer Programming (MIP) solver", "start_pos": 120, "end_pos": 170, "type": "TASK", "confidence": 0.5541248843073845}]}, {"text": "For each task we applied a feature selection procedure (cf.) to determine which semantic features should betaken as the input by the basic classifiers.", "labels": [], "entities": []}, {"text": "To evaluate individual tasks we applied two metrics: accuracy, calculated as the proportion of correct classifications to the total number of instances, and the \u03ba statistic, which corrects for the proportion of classifications that might occur by chance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9995152950286865}]}, {"text": "For end-to-end evaluation, we applied the Phi coefficient to measure the degree of similarity between the vector representations of the generated form (i.e. built from the outcomes of individual tasks) and the reference form obtained from the test data.", "labels": [], "entities": [{"text": "Phi coefficient", "start_pos": 42, "end_pos": 57, "type": "METRIC", "confidence": 0.9682500064373016}]}, {"text": "The Phi-based similarity metric is similar to \u03ba as it compensates for the fact that a match between two multi-label features is more difficult to obtain than in the case of binary features.", "labels": [], "entities": []}, {"text": "This measure tells us how well all the tasks have been solved together, which in our case amounts to generating the whole text.", "labels": [], "entities": []}, {"text": "The results presented in show that the ILP systems achieved highest accuracy and \u03ba for most tasks and reached the highest overall Phi score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9995469450950623}, {"text": "\u03ba", "start_pos": 81, "end_pos": 82, "type": "METRIC", "confidence": 0.9595544338226318}, {"text": "Phi score", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9697173833847046}]}, {"text": "Notice that ILP2 improved the accuracy of both pipeline systems for the three correlated tasks that we discussed before, i.e. Connective, S Exp. and Verb Form.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9995476603507996}]}, {"text": "Another group of correlated tasks for which the results appear interesting are i.e. Verb Lex., Phrase Type and Phrase Rank (cf..", "labels": [], "entities": [{"text": "Verb", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9053424596786499}]}, {"text": "got higher scores in Pipeline2, with outputs from both Phrase Type and Phrase Rank (see the respective pipeline positions), but the reverse effect did not occur: scores for both phrase tasks were lower in Pipeline1 when they had access to the output from Verb Lex., contrary to what we might expect.", "labels": [], "entities": [{"text": "Verb Lex.", "start_pos": 255, "end_pos": 264, "type": "DATASET", "confidence": 0.880003958940506}]}, {"text": "Apparently, this was due to the low accuracy for Verb Lex.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.999567449092865}, {"text": "Verb Lex", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9166146516799927}]}, {"text": "which caused the We have found that indirect comparison C4.5 performs better than Naive Bayes but the probability distribution that it outputs is strongly biased towards the winning label.", "labels": [], "entities": [{"text": "We", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9616486430168152}]}, {"text": "In this case it is practically impossible for the ILP system to change the classifier's decision, as the costs of other labels get extremely high.", "labels": [], "entities": []}, {"text": "Hence the more balanced probability distribution given by Naive Bayes can be easier corrected in the optimization process.", "labels": [], "entities": []}, {"text": "11 http://www.geocities.com/lpsolve/ already mentioned error propagation.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.6757474839687347}]}, {"text": "This example shows well the advantage that optimization processing brings: both ILP systems reached much higher scores for all three tasks.", "labels": [], "entities": [{"text": "optimization processing", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.9211700558662415}]}, {"text": "Finally, it appears as no coincidence that the three tasks involving lexical choice, i.e. Connective, Verb Lex. and Preposition Lex.", "labels": [], "entities": []}, {"text": "scored lower than the syntactic tasks in all systems.", "labels": [], "entities": []}, {"text": "This can be attributed partially to the limitations of retrieval measures which do not allow for the fact, that in a given semantic content more than one lexical form can be appropriate.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Joint distribution matrix for selected labels of tasks  Connective (horizontal) and Verb Form (vertical), computed  for all discourse units in a corpus.", "labels": [], "entities": []}, {"text": " Table 4: Joint distribution matrix for tasks Connective and  Verb Form, considering only disc. units similar to (c): until  you see the river side in front of you, at Phi-threshold \u2265 0.8.", "labels": [], "entities": [{"text": "Phi-threshold", "start_pos": 168, "end_pos": 181, "type": "METRIC", "confidence": 0.9394221901893616}]}, {"text": " Table 5: Results reached by the implemented ILP systems and two baselines. For both pipeline systems, Pos. stands for the  position of the tasks in the pipeline.", "labels": [], "entities": []}]}