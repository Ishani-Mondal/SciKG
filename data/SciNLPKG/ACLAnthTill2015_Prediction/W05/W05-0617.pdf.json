{"title": [], "abstractContent": [{"text": "We address the problem of learning a morphological automaton directly from a monolingual text corpus without recourse to additional resources.", "labels": [], "entities": []}, {"text": "Like previous work in this area, our approach exploits orthographic regularities in a search for possible morphological segmentation points.", "labels": [], "entities": []}, {"text": "Instead of affixes, however, we search for affix transformation rules that express correspondences between term clusters induced from the data.", "labels": [], "entities": []}, {"text": "This focuses the system on substrings having syntactic function, and yields cluster-to-cluster transformation rules which enable the system to process unknown morphological forms of known words accurately.", "labels": [], "entities": []}, {"text": "A stem-weighting algorithm based on Hubs and Authorities is used to clarify ambiguous segmentation points.", "labels": [], "entities": [{"text": "clarify ambiguous segmentation points", "start_pos": 68, "end_pos": 105, "type": "TASK", "confidence": 0.6433817520737648}]}, {"text": "We evaluate our approach using the CELEX database.", "labels": [], "entities": [{"text": "CELEX database", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.9824246764183044}]}], "introductionContent": [{"text": "This paper presents a completely unsupervised method for inducing morphological knowledge directly from a large monolingual text corpus.", "labels": [], "entities": []}, {"text": "This method works by searching for transformation rules that express correspondences between term clusters which are induced from the corpus in an initial step.", "labels": [], "entities": []}, {"text": "It covers both inflectional and derivational morphology, and is able to process previously unseen morphs of a word, as long as one of its morphs has been assigned to a cluster.", "labels": [], "entities": []}, {"text": "Aside from its academic appeal, acquisition of this morphological knowledge is a step toward the goal of rapidly retargetable natural language processing.", "labels": [], "entities": []}, {"text": "Toward this end, we envisage two uses for it: 1.", "labels": [], "entities": []}, {"text": "It can be used to perform morphological normalization (i.e., stemming).", "labels": [], "entities": [{"text": "morphological normalization", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.7353597283363342}]}, {"text": "2. In the form of transformation rules, it can help us classify unknown words, thereby enhancing the utility of cluster-based features for applications such as information extraction).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.8321430087089539}]}, {"text": "There is a considerable literature on the problem of morphology induction in general, and unsupervised (or lightly supervised) induction in particular.", "labels": [], "entities": [{"text": "morphology induction", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.8827658891677856}]}, {"text": "Much of the work attempts to exploit orthographic regularities alone, seeking affixation patterns (or signatures) that permit a compressive representation of the corpus.", "labels": [], "entities": []}, {"text": "Several researchers propose algorithms based on the minimum description length (MDL) principle, achieving reasonable success in discovering regular morphological patterns).", "labels": [], "entities": [{"text": "minimum description length (MDL)", "start_pos": 52, "end_pos": 84, "type": "METRIC", "confidence": 0.7474703788757324}]}, {"text": "MDL has information theoretic underpinnings, and an information theoretic objective function achieves similar success).", "labels": [], "entities": [{"text": "MDL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8858988285064697}]}, {"text": "Note that none of these approaches attempts to account for the syntactic dimension of affixation.", "labels": [], "entities": []}, {"text": "And all must adopt strategies to cope with a very large search space (the power set of the vocab-ulary, in the limit).", "labels": [], "entities": []}, {"text": "Such strategies form a common theme in these papers.", "labels": [], "entities": []}, {"text": "Our approach implicitly employs term cooccurrence statistics in the form of statistically derived term clusters.", "labels": [], "entities": []}, {"text": "A number of researchers use such statistics directly.", "labels": [], "entities": []}, {"text": "A common technique is to cast a word as a distribution over other words that occur within some limited window across the corpus.", "labels": [], "entities": []}, {"text": "This definition of co-occurrence yields a semantic distance measure which tends to draw together inflectional variants of a word.", "labels": [], "entities": []}, {"text": "Combined with heuristics such as string edit distance, it can be used to find reliable conflation sets (.", "labels": [], "entities": [{"text": "string edit distance", "start_pos": 33, "end_pos": 53, "type": "METRIC", "confidence": 0.5031295319398245}]}, {"text": "A somewhat tighter definition of co-occurrence, which nevertheless yields a semantic distance measure, serves as the basis of a method that captures irregular inflectional transformations in.", "labels": [], "entities": []}, {"text": "Schone and Jurafsky (2001) employ distributions over adjacent words (yielding a syntactic distance metric) to improve the precision of their conflation sets.", "labels": [], "entities": [{"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9977172613143921}]}, {"text": "In contrast with these approaches, ours is predicated on a strictly local notion of co-occurrence.", "labels": [], "entities": []}, {"text": "It is well known that clustering terms from a corpus in English or a related language, using a distance measure based on local co-occurrence, yields clusters that correspond roughly to part of speech categories).", "labels": [], "entities": []}, {"text": "The heart of our idea is to search for affix transformation rules mapping terms in one cluster to those in another.", "labels": [], "entities": []}, {"text": "The search for such rules has previously been conducted in the context of supervised part-of-speech tagging, but not to our knowledge using word clusters.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.6953004598617554}]}, {"text": "Basing the search for affix patterns on a syntactic partition of the vocabulary, albeit a noisy one, greatly reduces the size of the space of possible conflation sets.", "labels": [], "entities": []}, {"text": "Furthermore, the resulting rules can be assigned a syntactic interpretation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate by taking the highest-ranked trace, using the ordering heuristics described in the previous section, as the system's analysis of a given word.", "labels": [], "entities": []}, {"text": "This analysis takes the form of a sequence of hypothetical wordforms, from a putative stem to the target wordform (e.g., decide, decision, decisions).", "labels": [], "entities": []}, {"text": "The CELEX morphological database () is used to produce a reference analysis, by tracing back from the target wordform through any inflectional affixation, then through successive derivational affixations until a stem is reached.", "labels": [], "entities": [{"text": "CELEX morphological database", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.8689162135124207}]}, {"text": "Occasionally, this yields more than one analysis.", "labels": [], "entities": []}, {"text": "In such cases, all analyses are retained, and the system's analysis is given the most optimistic score.", "labels": [], "entities": []}, {"text": "In other words, if a CELEX analysis is found which matches the system's analysis, it is judged to be correct.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Results of experiments using the Wall  Street Journal corpus.", "labels": [], "entities": [{"text": "Wall  Street Journal corpus", "start_pos": 43, "end_pos": 70, "type": "DATASET", "confidence": 0.9773607552051544}]}]}