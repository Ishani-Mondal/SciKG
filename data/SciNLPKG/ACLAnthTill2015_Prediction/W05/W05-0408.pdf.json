{"title": [{"text": "Automatic identification of sentiment vocabulary: exploiting low associa- tion with known sentiment terms", "labels": [], "entities": [{"text": "identification of sentiment vocabulary", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.8507755100727081}]}], "abstractContent": [{"text": "We describe an extension to the technique for the automatic identification and labeling of sentiment terms described in Tur-ney (2002) and Turney and Littman (2002).", "labels": [], "entities": [{"text": "automatic identification and labeling of sentiment terms", "start_pos": 50, "end_pos": 106, "type": "TASK", "confidence": 0.8406297564506531}]}, {"text": "Their basic assumption is that sentiment terms of similar orientation tend to co-occur at the document level.", "labels": [], "entities": []}, {"text": "We add a second assumption, namely that sentiment terms of opposite orientation tend not to co-occur at the sentence level.", "labels": [], "entities": []}, {"text": "This additional assumption allows us to identify sentiment-bearing terms very reliably.", "labels": [], "entities": []}, {"text": "We then use these newly identified terms in various scenarios for the sentiment classification of sentences.", "labels": [], "entities": [{"text": "sentiment classification of sentences", "start_pos": 70, "end_pos": 107, "type": "TASK", "confidence": 0.9321716725826263}]}, {"text": "We show that our approach outperforms Turney's original approach.", "labels": [], "entities": []}, {"text": "Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance.", "labels": [], "entities": []}, {"text": "We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.999505877494812}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9989099502563477}]}], "introductionContent": [{"text": "The field of sentiment classification has received considerable attention from researchers in recent years (, Yu and Hatzivassiloglou 2003 and many others).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.941340833902359}]}, {"text": "The identification and classification of sentiment constitutes a problem that is orthogonal to the usual task of text classification.", "labels": [], "entities": [{"text": "identification and classification of sentiment", "start_pos": 4, "end_pos": 50, "type": "TASK", "confidence": 0.8390081822872162}, {"text": "text classification", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7288158982992172}]}, {"text": "Whereas in traditional text classification the focus is on topic identification, in sentiment classification the focus is on the assessment of the writer's sentiment toward the topic.", "labels": [], "entities": [{"text": "text classification", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7999147772789001}, {"text": "topic identification", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.744262084364891}, {"text": "sentiment classification", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.9386044144630432}]}, {"text": "Movie and product reviews have been the main focus of many of the recent studies in this area.", "labels": [], "entities": [{"text": "Movie and product reviews", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.573020689189434}]}, {"text": "Typically, these reviews are classified at the document level, and the class labels are \"positive\" and \"negative\".", "labels": [], "entities": []}, {"text": "In this work, in contrast, we narrow the scope of investigation to the sentence level and expand the set of labels, making a threefold distinction between \"positive\", \"neutral\", and \"negative\".", "labels": [], "entities": []}, {"text": "The narrowing of scope is motivated by the fact that for realistic text mining on customer feedback, the document level is too coarse, as described in.", "labels": [], "entities": [{"text": "text mining", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.7235462665557861}]}, {"text": "The expansion of the label set is also motivated by real-world concerns; while it is a given that review text expresses positive or negative sentiment, in many cases it is necessary to also identify the cases that don't carry strong expressions of sentiment at all.", "labels": [], "entities": []}, {"text": "Traditional approaches to text classification require large amounts of labeled training data.", "labels": [], "entities": [{"text": "text classification", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8499812185764313}]}, {"text": "Acquisition of such data can be costly and timeconsuming.", "labels": [], "entities": []}, {"text": "Due to the highly domain-specific nature of the sentiment classification task, moving from one domain to another typically requires the acquisition of anew set of training data.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.9415578246116638}]}, {"text": "For this reason, unsupervised or very weakly supervised methods for sentiment classification are especially desirable.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.9718853831291199}]}, {"text": "Our focus, therefore, is on methods that require very little data annotation.", "labels": [], "entities": []}, {"text": "We describe a method to automatically identify the sentiment vocabulary in a domain.", "labels": [], "entities": []}, {"text": "This method rests on three special properties of the sentiment domain: 1.", "labels": [], "entities": []}, {"text": "the presence of certain words can serve as a proxy for the class label 2.", "labels": [], "entities": []}, {"text": "sentiment terms of similar orientation tend to co-occur 3.", "labels": [], "entities": []}, {"text": "sentiment terms of opposite orientation tend to not co-occur at the sentence level.  and exploit the first two generalizations for unsupervised sentiment classification of movie reviews.", "labels": [], "entities": [{"text": "sentiment classification of movie reviews", "start_pos": 144, "end_pos": 185, "type": "TASK", "confidence": 0.8039851367473603}]}, {"text": "They use the two terms excellent and poor as seed terms to determine the semantic orientation of other terms.", "labels": [], "entities": []}, {"text": "These seed terms can be viewed as proxies for the class labels \"positive\" and \"negative\", allowing for the exploitation of otherwise unlabeled data: Terms that tend to co-occur with excellent in documents tend to be of positive orientation, and vice versa for poor.", "labels": [], "entities": []}, {"text": "starts from a small (2 word) set of terms with known orientation (excellent and poor).", "labels": [], "entities": []}, {"text": "Given a set of terms with unknown sentiment orientation, Turney (2002) then uses the PMI-IR algorithm to issue queries to the web and determine, for each of these terms, its pointwise mutual information (PMI) with the two seed words across a large set of documents.", "labels": [], "entities": []}, {"text": "Term candidates are constrained to be adjectives, which tend to be the strongest bearers of sentiment.", "labels": [], "entities": []}, {"text": "The sentiment orientation (SO) of a term is then determined by the difference between its association (PMI) with the positive seed term excellent and its association with the negative seed term poor.", "labels": [], "entities": [{"text": "sentiment orientation (SO)", "start_pos": 4, "end_pos": 30, "type": "METRIC", "confidence": 0.6649860978126526}]}, {"text": "The resulting list of terms and associated sentiment orientations can then be used to implement a classifier: semantic orientation of the terms in a document of unknown sentiment is added up, and if the overall score is positive, the document is classified as being of positive sentiment, otherwise it is classified as negative.", "labels": [], "entities": []}, {"text": "extend this approach by (1) applying it at the sentence level (instead of the document-level), (2) taking into account non-adjectival parts-of-speech, and (3) using larger sets of seed words.", "labels": [], "entities": []}, {"text": "Their classification goal also differs from Turney's: it is to distinguish opinion sentences from factual statements.", "labels": [], "entities": []}, {"text": "Turney et al.'s approach is based on the assumption that sentiment terms of similar orientation tend to co-occur in documents.", "labels": [], "entities": []}, {"text": "Our approach takes advantage of a second assumption: At the sentence level, sentiment terms of opposite orientation tend not to co-occur.", "labels": [], "entities": []}, {"text": "This is, of course, an assumption that will only hold in general, with exceptions.", "labels": [], "entities": []}, {"text": "Basically, the assumption is that sentences of the following form: I dislike X.", "labels": [], "entities": []}, {"text": "I really like X. are more frequent than \"mixed sentiment\" sentences such as I dislike X but I really like Y.", "labels": [], "entities": []}, {"text": "It has been our experience that this generalization does hold often enough to be useful.", "labels": [], "entities": [{"text": "generalization", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.9698262214660645}]}, {"text": "We propose to utilize this assumption to identify a set of sentiment terms in a domain.", "labels": [], "entities": []}, {"text": "We select the terms that have the lowest PMI scores on the sentence level with respect to a set of manually selected seed words.", "labels": [], "entities": [{"text": "PMI scores", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9660137593746185}]}, {"text": "If our assumption about low association at the sentence level is correct, this set of low-scoring terms will be particularly rich in sentiment terms.", "labels": [], "entities": []}, {"text": "We can then use this newly identified set to: (1) use Turney's method to find the orientation for the terms and employ the terms and their scores in a classifier, and (2) use Turney's method to find the orientation for the terms and add the new terms as additional seed terms fora second iteration As opposed to Turney (2002), we do not use the web as a resource to find associations, rather we apply the method directly to in-domain data.", "labels": [], "entities": []}, {"text": "This has the disadvantage of not being able to apply the classification to any arbitrary domain.", "labels": [], "entities": []}, {"text": "It is worth noting, however, that even in Turney (2002) the choice of seed words is explicitly motivated by domain properties of movie reviews.", "labels": [], "entities": []}, {"text": "In the remainder of the paper we will describe results from various experiments based on this assumption.", "labels": [], "entities": []}, {"text": "We also show how we can combine this method with a Naive Bayes bootstrapping approach that takes further advantage of the unlabeled data).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were performed as follows: We started with a small set of manually-selected and annotated seed terms.", "labels": [], "entities": []}, {"text": "We used 4 positive and 6 negative seed terms.", "labels": [], "entities": []}, {"text": "We decided to use a few more negative seed words because of the inherent positive skew in the data that makes the identification of negative sentences particularly hard.", "labels": [], "entities": [{"text": "identification of negative sentences", "start_pos": 114, "end_pos": 150, "type": "TASK", "confidence": 0.8522381484508514}]}, {"text": "The terms we used are: negative hate suck unreliable There was no tuning of the set of initial seed terms; the 10 words were originally chosen intuitively, as words that we observed frequently when manually inspecting the data.", "labels": [], "entities": []}, {"text": "We then used these seed terms in two basic ways: (1) We used them as seeds fora Turneystyle determination of the semantic orientation of words in the corpus (semantic orientation, or SO method).", "labels": [], "entities": []}, {"text": "As mentioned above, this process is based on the assumption that terms of similar orientation tend to co-occur.", "labels": [], "entities": []}, {"text": "(2) We used them to mine sentiment vocabulary from the unlabeled data using the additional assumption that sentiment terms of opposite orientation tend not to co-occur at the sentence level (sentiment mining, or SM method).", "labels": [], "entities": [{"text": "sentiment mining", "start_pos": 191, "end_pos": 207, "type": "TASK", "confidence": 0.8010586798191071}]}, {"text": "This method yields a set of sentiment terms, but no orientation for that set of terms.", "labels": [], "entities": []}, {"text": "We continue by using the SO method to find the semantic orientation for this set of sentiment terms, effectively using SM as a feature selection method for sentiment terminology.", "labels": [], "entities": [{"text": "sentiment terminology", "start_pos": 156, "end_pos": 177, "type": "TASK", "confidence": 0.8563766777515411}]}, {"text": "Pseudo-code for the SO and SM approaches is provided in and.", "labels": [], "entities": [{"text": "SO", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9393450021743774}, {"text": "SM", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.6515799164772034}]}, {"text": "As a first step for both SO and SM methods (not shown in the pseudocode), PMI needs to be calculated for each pair (f, s) of feature f and seed word s over the collection of feature vectors.", "labels": [], "entities": []}, {"text": "In the first scenario (using straightforward SO), features F range overall observed features in the data (modulo the aforementioned count cutoff of 10).", "labels": [], "entities": []}, {"text": "In the second scenario (SM + SO), features F range over the n% of features with the lowest PMI scores with respect to any of the seed words that were identified using the sentiment mining technique in.", "labels": [], "entities": [{"text": "PMI", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9708894491195679}, {"text": "sentiment mining", "start_pos": 171, "end_pos": 187, "type": "TASK", "confidence": 0.8593932092189789}]}, {"text": "The result of both SO and SM+SO is a list of unigram features which have an associated semantic orientation score, indicating their sentiment orientation: the higher the score, the more \"positive\" a term, and vice versa.", "labels": [], "entities": []}, {"text": "This list of features and associated scores can be used to construct a simple classifier: for each sentence with unknown sentiment, we take the sum of the semantic orientation scores for all of the unigrams in that sentence.", "labels": [], "entities": []}, {"text": "This overall score determines the classification of the sentence as \"positive\", \"neutral\" or \"negative\" as shown in.", "labels": [], "entities": []}, {"text": "The two thresholds used in classification need to be determined empirically by taking the distribution of class values in the corpus into account.", "labels": [], "entities": []}, {"text": "For our experiments we simply took the distribution of class labels in the 400 sentence development test set as an approximation of the overall class label distribution: we determined that distribution to be 15.5% for negative sentences, 21.5% for neutral sentences, and 63.0% for positive sentences.", "labels": [], "entities": [{"text": "400 sentence development test set", "start_pos": 75, "end_pos": 108, "type": "DATASET", "confidence": 0.6663636565208435}]}, {"text": "Scores for all sentence vectors in the corpus are then collected using the scoring part of the algorithm in.", "labels": [], "entities": []}, {"text": "The scores are sorted and the thresholds are determined as the cutoffs for the top 63% and bottom 15.5% of scores respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: combining SM and SO.", "labels": [], "entities": [{"text": "SM", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.964056670665741}]}, {"text": " Table 4. Increasing the number  of seed features through the SM feature selection  method increases precision and recall by several  percentage points. In particular, precision and re- call for negative sentences are boosted.", "labels": [], "entities": [{"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9995077848434448}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9995274543762207}, {"text": "precision", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.9994860887527466}, {"text": "re- call", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9183661341667175}]}, {"text": " Table 4: Using 2 iterations to increase the seed feature  set", "labels": [], "entities": []}, {"text": " Table 6: Average precision and recall for SVMs for  small numbers of labeled examples", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9337899088859558}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.896325945854187}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9993088245391846}, {"text": "SVMs", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.9402581453323364}]}]}