{"title": [{"text": "A Probabilistic Setting and Lexical Cooccurrence Model for Textual Entailment", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.760926216840744}]}], "abstractContent": [{"text": "This paper proposes a general probabilis-tic setting that formalizes a probabilistic notion of textual entailment.", "labels": [], "entities": []}, {"text": "We further describe a particular preliminary model for lexical-level entailment, based on document cooccurrence probabilities, which follows the general setting.", "labels": [], "entities": []}, {"text": "The model was evaluated on two application independent datasets, suggesting the relevance of such probabilistic approaches for entailment modeling.", "labels": [], "entities": [{"text": "entailment modeling", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.8494566977024078}]}], "introductionContent": [{"text": "Many Natural Language Processing (NLP) applications need to recognize when the meaning of one text can be expressed by, or inferred from, another text.", "labels": [], "entities": []}, {"text": "Information Retrieval (IR), Question Answering (QA), Information Extraction (IE), text summarization and Machine Translation (MT) evaluation are examples of applications that need to assess this semantic relationship between text segments.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8442882418632507}, {"text": "Question Answering (QA)", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.8524876356124877}, {"text": "Information Extraction (IE)", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.8443669021129608}, {"text": "text summarization", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7627913355827332}, {"text": "Machine Translation (MT) evaluation", "start_pos": 105, "end_pos": 140, "type": "TASK", "confidence": 0.8733291327953339}]}, {"text": "The Textual Entailment Recognition task () has recently been proposed as an application independent framework for modeling such inferences.", "labels": [], "entities": [{"text": "Textual Entailment Recognition task", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.8739305734634399}]}, {"text": "Within the textual entailment framework, a text t is said to entail a textual hypothesis h if the truth of h can be inferred from t.", "labels": [], "entities": []}, {"text": "Textual entailment captures generically abroad range of inferences that are relevant for multiple applications.", "labels": [], "entities": [{"text": "Textual entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7457501292228699}]}, {"text": "For example, a QA system has to identify texts that entail a hypothesized answer.", "labels": [], "entities": []}, {"text": "Given the question \"Does John Speak French?\", a text that includes the sentence \"John is a fluent French speaker\" entails the suggested answer \"John speaks French.\"", "labels": [], "entities": []}, {"text": "In many cases, though, entailment inference is uncertain and has a probabilistic nature.", "labels": [], "entities": [{"text": "entailment inference", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.9623444378376007}]}, {"text": "For example, a text that includes the sentence \"John was born in France.\" does not strictly entail the above answer.", "labels": [], "entities": [{"text": "John was born in France.", "start_pos": 48, "end_pos": 72, "type": "DATASET", "confidence": 0.8880846619606018}]}, {"text": "Yet, it is clear that it does increase substantially the likelihood that the hypothesized answer is true.", "labels": [], "entities": []}, {"text": "The uncertain nature of textual entailment calls for its explicit modeling in probabilistic terms.", "labels": [], "entities": []}, {"text": "We therefore propose a general generative probabilistic setting for textual entailment, which allows a clear formulation of concrete probabilistic models for this task.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7481396496295929}]}, {"text": "We suggest that the proposed setting may provide a unifying framework for modeling uncertain semantic inferences from texts.", "labels": [], "entities": []}, {"text": "An important sub task of textual entailment, which we term lexical entailment, is recognizing if the lexical concepts in a hypothesis hare entailed from a given text t, even if the relations which hold between these concepts may not be entailed from t.", "labels": [], "entities": []}, {"text": "This is typically a necessary, but not sufficient, condition for textual entailment.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.8339651226997375}]}, {"text": "For example, in order to infer from a text the hypothesis \"Chrysler stock rose,\" it is a necessary that the concepts of Chrysler, stock and rise must be inferred from the text.", "labels": [], "entities": [{"text": "Chrysler stock rose", "start_pos": 59, "end_pos": 78, "type": "DATASET", "confidence": 0.9251981973648071}]}, {"text": "However, for proper entailment it is further needed that the right relations hold between these concepts.", "labels": [], "entities": []}, {"text": "In this paper we demonstrate the relevance of the general probabilistic setting for modeling lexical entailment, by devising a preliminary model that is based on document co-occurrence probabilities in a bag of words representation.", "labels": [], "entities": []}, {"text": "Although our proposed lexical system is relatively simple, as it doesn't rely on syntactic or other deeper analysis, it nevertheless was among the top ranking systems in the first Recognising Textual Entailment (RTE) Challenge ().", "labels": [], "entities": [{"text": "Recognising Textual Entailment (RTE) Challenge", "start_pos": 180, "end_pos": 226, "type": "TASK", "confidence": 0.7054968391145978}]}, {"text": "The model was evaluated also on an additional dataset, where it compares favorably with a state-of-the-art heuristic score.", "labels": [], "entities": []}, {"text": "These results suggest that the proposed probabilistic framework is a promising basis for devising improved models that incorporate richer information.", "labels": [], "entities": []}], "datasetContent": [{"text": "The RTE dataset () consists of sentence pairs annotated for entailment.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8608031868934631}]}, {"text": "Fo this dataset we used word cooccurrence frequencies obtained from a web search engine.", "labels": [], "entities": []}, {"text": "The details of this experiment are described in.", "labels": [], "entities": []}, {"text": "The resulting accuracy on the test set was 59% and the resulting confidence weighted score was 0.57.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9997237324714661}, {"text": "confidence weighted score", "start_pos": 65, "end_pos": 90, "type": "METRIC", "confidence": 0.9501418670018514}]}, {"text": "Both are statistically significantly better than chance at the 0.01 level.", "labels": [], "entities": []}, {"text": "The baseline model (6) from Section 3.2, which takes into account only terms appearing in both the text and hypothesis, achieved an accuracy of only 56%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9996606111526489}]}, {"text": "Although our proposed lexical system is relatively simple, as it doesn't rely on syntactic or other deeper analysis, it nevertheless was among the top ranking systems in the RTE Challenge.", "labels": [], "entities": [{"text": "RTE Challenge", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.5589336156845093}]}, {"text": "In addition to the RTE dataset we were interested in evaluating the model on a more representative set of texts and hypotheses that better corresponds to applicative settings.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.8414068222045898}]}, {"text": "We focused on the information seeking setting, common in applications such as QA and IR, in which a hypothesis is given and it is necessary to identify texts that entail it.", "labels": [], "entities": [{"text": "information seeking", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7341211140155792}]}, {"text": "An annotator was asked to choose 60 hypotheses based on sentences from the first few documents in the Reuters Corpus Volume 1 ().", "labels": [], "entities": [{"text": "Reuters Corpus Volume 1", "start_pos": 102, "end_pos": 125, "type": "DATASET", "confidence": 0.9769421070814133}]}, {"text": "The annotator was instructed to choose sentential hypotheses such that their truth could easily be evaluated.", "labels": [], "entities": []}, {"text": "We further required that the hypotheses convey a reasonable information need in such away that they might correspond to potential questions, semantic queries or IE relations.", "labels": [], "entities": [{"text": "IE relations", "start_pos": 161, "end_pos": 173, "type": "TASK", "confidence": 0.797105610370636}]}, {"text": "shows a few of the hypotheses.", "labels": [], "entities": []}, {"text": "In order to create a set of candidate entailing texts for the given set of test hypotheses, we followed the common practice of WordNet based ex-3 () actually proposed the above score with no normalizing denominator.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.9618604183197021}]}, {"text": "However fora given hypothesis it results with the same ranking of candidate entailing texts.", "labels": [], "entities": []}, {"text": "Using WordNet, we expanded the hypotheses' terms with morphological alternations and semantically related words 4 . For each hypothesis stop words were removed and all content words were expanded as described above.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 6, "end_pos": 13, "type": "DATASET", "confidence": 0.9719584584236145}]}, {"text": "Boolean Search included a conjunction of the disjunction of the term's expansions and was performed at the paragraph level over the full Reuters corpus, as common in IR for QA.", "labels": [], "entities": [{"text": "Reuters corpus", "start_pos": 137, "end_pos": 151, "type": "DATASET", "confidence": 0.935030460357666}]}, {"text": "Since we wanted to focus our research on semantic variability we excluded from the result set paragraphs that contain all original words of the hypothesis or their morphological derivations.", "labels": [], "entities": []}, {"text": "The resulting dataset consists of 50 hypotheses and over a million retrieved paragraphs (10 hypotheses had only exact matches).", "labels": [], "entities": []}, {"text": "The number of paragraphs retrieved per hypothesis range from 1 to 400,000.", "labels": [], "entities": []}, {"text": "5  The model's entailment probability, tep, was compared to the following two baseline models.", "labels": [], "entities": []}, {"text": "The first, denoted as base, is the na\u00efve baseline in which all retrieved texts are presumed to entail the hypothesis with equal confidence.", "labels": [], "entities": []}, {"text": "This baseline corresponds to systems which perform blind expansion with no weighting.", "labels": [], "entities": []}, {"text": "The second baseline, entscore, is the entailment score (6) from 3.2.", "labels": [], "entities": [{"text": "entailment score (6)", "start_pos": 38, "end_pos": 58, "type": "METRIC", "confidence": 0.9334354877471924}]}, {"text": "The top 20 best results for all methods were given to judges to be annotated for entailment.", "labels": [], "entities": []}, {"text": "Judges were asked to annotate an example as true if given the text they can infer with high confidence that the hypothesis is true (similar to the guidelines published for the RTE Challenge dataset).", "labels": [], "entities": [{"text": "RTE Challenge dataset", "start_pos": 176, "end_pos": 197, "type": "DATASET", "confidence": 0.8112969597180685}]}, {"text": "Accordingly, they were instructed to annotate the example as false if either they believe the hypothesis is false given the text or if the text is unrelated to the hypothesis.", "labels": [], "entities": []}, {"text": "In total there were 1683 text-hypothesis pairs, which were randomly divided between two judges.", "labels": [], "entities": []}, {"text": "In order to measure agreement, we had 200 of the pairs annotated by both judges, yielding a moderate agreement (a Kappa of 0.6).", "labels": [], "entities": [{"text": "agreement", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.8970137238502502}, {"text": "Kappa", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.9306915998458862}]}], "tableCaptions": [{"text": " Table 1: example sentence pairs", "labels": [], "entities": []}]}