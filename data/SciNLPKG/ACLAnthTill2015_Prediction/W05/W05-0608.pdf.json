{"title": [], "abstractContent": [{"text": "In this paper we propose and evaluate a technique to perform semi-supervised learning for Text Categorization.", "labels": [], "entities": [{"text": "Text Categorization", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7719326913356781}]}, {"text": "In particular we defined a kernel function, namely the Domain Kernel, that allowed us to plug \"external knowledge\" into the supervised learning process.", "labels": [], "entities": []}, {"text": "External knowledge is acquired from unlabeled data in a totally unsupervised way, and it is represented by means of Domain Models.", "labels": [], "entities": []}, {"text": "We evaluated the Domain Kernel in two standard benchmarks for Text Categoriza-tion with good results, and we compared its performance with a kernel function that exploits a standard bag-of-words feature representation.", "labels": [], "entities": []}, {"text": "The learning curves show that the Domain Kernel allows us to reduce drastically the amount of training data required for learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text Categorization (TC) deals with the problem of assigning a set of category labels to documents.", "labels": [], "entities": [{"text": "Text Categorization (TC)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8409304678440094}]}, {"text": "Categories are usually defined according to a variety of topics (e.g. SPORT vs. POLITICS) and a set of hand tagged examples is provided for training.", "labels": [], "entities": []}, {"text": "In the state-of-the-art TC settings supervised classifiers are used for learning and texts are represented by means of bag-of-words.", "labels": [], "entities": []}, {"text": "Even if, in principle, supervised approaches reach the best performance in many Natural Language Processing (NLP) tasks, in practice it is not always easy to apply them to concrete applicative settings.", "labels": [], "entities": []}, {"text": "In fact, supervised systems for TC require to be trained a large amount of hand tagged texts.", "labels": [], "entities": [{"text": "TC", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9916566610336304}]}, {"text": "This situation is usually feasible only when there is someone (e.g. a big company) that can easily provide already classified documents to train the system.", "labels": [], "entities": []}, {"text": "In most of the cases this scenario is quite unpractical, if not infeasible.", "labels": [], "entities": []}, {"text": "An example is the task of categorizing personal documents, in which the categories can be modified according to the user's interests: new categories are often introduced and, possibly, the available labeled training for them is very limited.", "labels": [], "entities": []}, {"text": "In the NLP literature the problem of providing large amounts of manually annotated data is known as the Knowledge Acquisition Bottleneck.", "labels": [], "entities": []}, {"text": "Current research in supervised approaches to NLP often deals with defining methodologies and algorithms to reduce the amount of human effort required for collecting labeled examples.", "labels": [], "entities": []}, {"text": "A promising direction to solve this problem is to provide unlabeled data together with labeled texts to help supervision.", "labels": [], "entities": []}, {"text": "In the Machine Learning literature this learning schema has been called semisupervised learning.", "labels": [], "entities": []}, {"text": "It has been applied to the TC problem using different techniques: co-training), EM-algorithm (), Transduptive SVM and Latent Semantic Indexing (.", "labels": [], "entities": [{"text": "TC problem", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.9425441026687622}]}, {"text": "In this paper we propose a novel technique to perform semi-supervised learning for TC.", "labels": [], "entities": [{"text": "TC", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.955597460269928}]}, {"text": "The underlying idea behind our approach is that lexical co-herence (i.e. co-occurence in texts of semantically related terms) () is an inherent property of corpora, and it can be exploited to help a supervised classifier to build a better categorization hypothesis, even if the amount of labeled training data provided for learning is very low.", "labels": [], "entities": []}, {"text": "Our proposal consists of defining a Domain Kernel and exploiting it inside a Support Vector Machine (SVM) classification framework for TC).", "labels": [], "entities": []}, {"text": "The Domain Kernel relies on the notion of Domain Model, which is a shallow representation for lexical ambiguity and variability.", "labels": [], "entities": []}, {"text": "Domain Models can be acquired in an unsupervised way from unlabeled data, and then exploited to define a Domain Kernel (i.e. a generalized similarity function among documents) 1 . We evaluated the Domain Kernel in two standard benchmarks for TC (i.e. Reuters and 20News-groups), and we compared its performance with a kernel function that exploits a more standard Bagof-Words (BoW) feature representation.", "labels": [], "entities": []}, {"text": "The use of the Domain Kernel got a significant improvement in the learning curves of both tasks.", "labels": [], "entities": []}, {"text": "In particular, there is a notable increment of the recall, especially with few learning examples.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9988131523132324}]}, {"text": "In addition, F1 measure increases by 2.8 points in the Reuters task at full learning, achieving the state-of-the-art results.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9810980558395386}, {"text": "Reuters task", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.8106387853622437}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the notion of Domain Model and describes an automatic acquisition technique based on Latent Semantic Analysis (LSA).", "labels": [], "entities": []}, {"text": "In Section 3 we illustrate the SVM approach to TC, and we define a Domain Kernel that exploits Domain Models to estimate similarity among documents.", "labels": [], "entities": [{"text": "TC", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9745596647262573}]}, {"text": "In Section 4 the performance of the Domain Kernel are compared with a standard bag-of-words feature representation, showing the improvements in the learning curves.", "labels": [], "entities": []}, {"text": "Section 5 describes the previous attempts to exploit semisupervised learning for TC, while section 6 concludes the paper and proposes some directions for future research.", "labels": [], "entities": [{"text": "TC", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9729841351509094}]}], "datasetContent": [{"text": "We compared the performance of both K D and K BoW on two standard TC benchmarks.", "labels": [], "entities": []}, {"text": "In subsection 4.1 we describe the evaluation tasks and the preprocessing steps, in 4.2 we describe some algorithmic details of the TC system adopted.", "labels": [], "entities": [{"text": "TC", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.9058805704116821}]}, {"text": "Finally in subsection 4.3 we compare the learning curves of K D and K BoW .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Micro-F1 with full learning", "labels": [], "entities": []}, {"text": " Table 3: Number of training examples needed by  K D and K BoW to reach the same micro-F1 on the  Reuters task", "labels": [], "entities": [{"text": "Reuters task", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.9712521731853485}]}, {"text": " Table 4: Number of training examples needed by  K D and K BoW to reach the same micro-F1 on the  20newsgroups task", "labels": [], "entities": []}]}