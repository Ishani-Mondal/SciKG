{"title": [{"text": "Semantic Role Labeling as Sequential Tagging", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7965159813563029}, {"text": "Sequential Tagging", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8409820199012756}]}], "abstractContent": [{"text": "In this paper we present a semantic role labeling system submitted to the CoNLL-2005 shared task.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.6031794250011444}, {"text": "CoNLL-2005 shared task", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.7876667380332947}]}, {"text": "The system makes use of partial and full syntactic information and converts the task into a sequential BIO-tagging.", "labels": [], "entities": []}, {"text": "As a result, the labeling architecture is very simple.", "labels": [], "entities": [{"text": "labeling", "start_pos": 17, "end_pos": 25, "type": "TASK", "confidence": 0.9692429900169373}]}, {"text": "Building on a state-of-the-art set of features, a binary classifier for each label is trained using AdaBoost with fixed depth decision trees.", "labels": [], "entities": []}, {"text": "The final system, which combines the outputs of two base systems performed F 1 =76.59 on the official test set.", "labels": [], "entities": [{"text": "F 1", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9919201731681824}, {"text": "official test set", "start_pos": 93, "end_pos": 110, "type": "DATASET", "confidence": 0.793427308400472}]}, {"text": "Additionally , we provide results comparing the system when using partial vs. full parsing input information.", "labels": [], "entities": []}, {"text": "1 Goals and System Architecture The goal of our work is twofold.", "labels": [], "entities": []}, {"text": "On the one hand, we want to test whether it is possible to implement a competitive SRL system by reducing the task to a sequential tagging.", "labels": [], "entities": [{"text": "SRL", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9752468466758728}]}, {"text": "On the other hand, we want to investigate the effect of replacing partial parsing information by full parsing.", "labels": [], "entities": []}, {"text": "For that, we built two different individual systems with a shared sequential strategy but using UPC chunks-clauses, and Char-niak's parses, respectively.", "labels": [], "entities": []}, {"text": "We will refer to those systems as PP UPC and FP CHA , hereinafter.", "labels": [], "entities": [{"text": "PP UPC", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.7829554378986359}, {"text": "FP CHA", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.6977931261062622}]}, {"text": "Both partial and full parsing annotations provided as input information are of hierarchical nature.", "labels": [], "entities": []}, {"text": "Our system navigates through these syntactic structures in order to select a subset of constituents organized sequentially (i.e., non embedding).", "labels": [], "entities": []}, {"text": "Propositions are treated independently, that is, each target verb generates a sequence of tokens to be annotated.", "labels": [], "entities": []}, {"text": "We call this pre-processing step sequentialization.", "labels": [], "entities": []}, {"text": "The sequential tokens are selected by exploring the sentence spans or regions defined by the clause boundaries 1.", "labels": [], "entities": []}, {"text": "The topmost syntactic constituents falling inside these regions are selected as tokens.", "labels": [], "entities": []}, {"text": "Note that this strategy is independent of the input syntactic annotation explored, provided it contains clause boundaries.", "labels": [], "entities": []}, {"text": "It happens that, in the case of full parses, this node selection strategy is equivalent to the pruning process defined by Xue and Palmer (2004), which selects sibling nodes along the path of ancestors from the verb predicate to the root of the tree 2.", "labels": [], "entities": []}, {"text": "Due to this pruning stage, the upper-bound recall figures are 95.67% for PP UPC and 90.32% for FP CHA.", "labels": [], "entities": [{"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9933826923370361}, {"text": "PP UPC", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.8909119069576263}, {"text": "FP CHA", "start_pos": 95, "end_pos": 101, "type": "DATASET", "confidence": 0.9362978935241699}]}, {"text": "These values give F 1 performance upper bounds of 97.79 and 94.91, respectively, assuming perfect predictors (100% precision).", "labels": [], "entities": [{"text": "F 1 performance upper", "start_pos": 18, "end_pos": 39, "type": "METRIC", "confidence": 0.9685105681419373}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9896892309188843}]}, {"text": "The nodes selected are labeled with B-I-O tags depending if they are at the beginning, inside, or outside of a verb argument.", "labels": [], "entities": []}, {"text": "There is a total of 37 argument types, which amount to 37*2+1=75 labels.", "labels": [], "entities": []}, {"text": "Regarding the learning algorithm, we used generalized AdaBoost with real-valued weak classifiers, which constructs an ensemble of decision trees of fixed depth (Schapire and Singer, 1999).", "labels": [], "entities": []}, {"text": "We considered a one-vs-all decomposition into binary prob-1 Regions to the right of the target verb corresponding to ancestor clauses are omitted in the case of partial parsing.", "labels": [], "entities": []}, {"text": "2 With the unique exception of the exploration inside sibling PP constituents proposed by (Xue and Palmer, 2004).", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We trained the classification models using the complete training set (sections from 02 to 21).", "labels": [], "entities": []}, {"text": "Once converted into one sequence per target predicate, the resulting set amounts 1,049,049 training examples in the PP UPC model and 828,811 training examples in the FP CHA model.", "labels": [], "entities": [{"text": "PP UPC model", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.885558545589447}, {"text": "FP CHA model", "start_pos": 166, "end_pos": 178, "type": "DATASET", "confidence": 0.8611021240552267}]}, {"text": "The average number of labels per argument is 2.071 and 1.068, respectively.", "labels": [], "entities": []}, {"text": "This fact makes \"I\" labels very rare in the FP CHA model.", "labels": [], "entities": [{"text": "FP CHA", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.8246977925300598}]}, {"text": "When running AdaBoost, we selected as weak rules decision trees of fixed depth 4 (i.e., each branch may represent a conjunction of at most 4 basic features) and trained a classification model per label for up to 2,000 rounds.", "labels": [], "entities": []}, {"text": "We applied some simplifications to keep training times and memory requirements inside admissible bounds.", "labels": [], "entities": []}, {"text": "First, we discarded all the argument labels that occur very infrequently and trained only the 41 most frequent labels in the case of PP UPC and the 35 most frequent in the case of FP CHA . The remaining labels where joined in anew label \"other\" in training and converted into \"O\" whenever the SRL system assigns a \"other\" label during testing.", "labels": [], "entities": [{"text": "PP UPC", "start_pos": 133, "end_pos": 139, "type": "DATASET", "confidence": 0.7103201448917389}, {"text": "FP CHA", "start_pos": 180, "end_pos": 186, "type": "DATASET", "confidence": 0.834607869386673}]}, {"text": "Second, we performed a simple frequency filtering by discarding those features occurring less than 15 times in the training set.", "labels": [], "entities": []}, {"text": "As an exception, the frequency threshold for the features referring to the verb predicate was set to 3.", "labels": [], "entities": []}, {"text": "The final number of features we worked with is 105,175 in the case of PP UPC and 80,742 in the case of FP CHA . Training with these very large data and feature sets becomes an issue.", "labels": [], "entities": [{"text": "PP UPC", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.849747359752655}, {"text": "FP CHA", "start_pos": 103, "end_pos": 109, "type": "DATASET", "confidence": 0.9381376504898071}]}, {"text": "Fortunately, we could split the computation among six machines in a Linux cluster.", "labels": [], "entities": []}, {"text": "Using our current implementation combining Perl and C++ we could train the complete models in about 2 days using memory requirements between 1.5GB and 2GB.", "labels": [], "entities": []}, {"text": "Testing with the ensembles of 2,000 decision trees per label is also not very efficient, though the resulting speed is admissible, e.g., the development set is tagged in about 30 minutes using a standard PC.", "labels": [], "entities": []}, {"text": "The overall results obtained by our individual PP UPC and FP CHA SRL systems are presented in table 1, with the best results in boldface.", "labels": [], "entities": [{"text": "PP UPC", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.8235431909561157}, {"text": "FP CHA SRL", "start_pos": 58, "end_pos": 68, "type": "DATASET", "confidence": 0.8474399646123251}]}, {"text": "As expected, the FP CHA system significantly outperformed the PP UPC system, though the results of the later can be considered competitive.", "labels": [], "entities": [{"text": "FP CHA", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.7306154072284698}, {"text": "PP UPC system", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.8231923778851827}]}, {"text": "This fact is against the belief, expressed as one of the conclusions of the CoNLL-2004 shared task, that full-parsing systems are about 10 F 1 points over partial-parsing systems.", "labels": [], "entities": []}, {"text": "In this case, we obtain a performance difference of 2.18 points in favor of FP CHA . Apart from resulting performance, there are additional advantages when using the FP CHA approach.", "labels": [], "entities": [{"text": "FP", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.8249413967132568}, {"text": "FP CHA", "start_pos": 166, "end_pos": 172, "type": "DATASET", "confidence": 0.7847473621368408}]}, {"text": "Due to the coarser granularity of sequence tokens, FP CHA sequences are shorter.", "labels": [], "entities": []}, {"text": "There are 21% less training examples and a much lower quantity of \"I\" tags to predict (the mapping between syntactic constituents and arguments is mostly one-to-one).", "labels": [], "entities": []}, {"text": "As a consequence, FP CHA classifiers train faster with less memory requirements, and achieve competitive results (near the optimal) with much less rounds of boosting.", "labels": [], "entities": [{"text": "FP CHA classifiers", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.5634264349937439}]}, {"text": "Also related to the token granularity, the number of completely correct outputs is 4.13 points higher in FP CHA , showing that the resulting labelings are structurally better than those of PP UPC . Interestingly, the PP UPC and FP CHA systems make quite different argument predictions.", "labels": [], "entities": [{"text": "FP CHA", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.8091025650501251}, {"text": "FP CHA", "start_pos": 228, "end_pos": 234, "type": "DATASET", "confidence": 0.9213645458221436}]}, {"text": "For instance, FP CHA is better at recognizing A0 and A1 arguments since parse constituents corresponding to these arguments tend to be mostly correct.", "labels": [], "entities": [{"text": "FP CHA", "start_pos": 14, "end_pos": 20, "type": "TASK", "confidence": 0.46626873314380646}]}, {"text": "Comparatively, PP UPC is better at recognizing A2-A4 arguments since they are further from the verb predicate: Overall results of the individual systems on the development set. and tend to accumulate more parsing errors, while the fine granularity of the PP UPC sequences still allow to capture them 4 . Another interesting observation is that the precision of both systems is much higher than the recall.", "labels": [], "entities": [{"text": "PP UPC", "start_pos": 15, "end_pos": 21, "type": "TASK", "confidence": 0.5308133512735367}, {"text": "precision", "start_pos": 348, "end_pos": 357, "type": "METRIC", "confidence": 0.9993851184844971}, {"text": "recall", "start_pos": 398, "end_pos": 404, "type": "METRIC", "confidence": 0.9990018010139465}]}, {"text": "The previous two facts suggest that combining the outputs of the two systems may lead to a significant improvement.", "labels": [], "entities": []}, {"text": "We experimented with a greedy combination scheme for joining the maximum number of arguments from both solutions in order to increase coverage and, hopefully, recall.", "labels": [], "entities": [{"text": "coverage", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9924803972244263}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9968420267105103}]}, {"text": "It proceeds departing from an empty solution by: First, adding all the arguments from FP CHA in which this method performs best; Second, adding all the arguments from PP UPC in which this method performs best; and Third, making another loop through the two methods adding the arguments not considered in the first loop.", "labels": [], "entities": [{"text": "FP CHA", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.5653156638145447}]}, {"text": "At each step, we require that the added arguments do not overlap/embed with arguments in the current solution and also that they do not introduce repetitions of A0-A5 arguments.", "labels": [], "entities": []}, {"text": "The results on the   development set (presented in table 1) confirm our expectations, since a performance increase of 1.18 points over the best individual system was observed, mainly caused by recall improvement.", "labels": [], "entities": [{"text": "recall", "start_pos": 193, "end_pos": 199, "type": "METRIC", "confidence": 0.9984838366508484}]}, {"text": "The final system we presented at the shared task performs exactly this solution merging procedure.", "labels": [], "entities": [{"text": "solution merging", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.7123306095600128}]}, {"text": "When applied on the WSJ test set, the combination scheme seems to generalize well, since an improvement is observed with respect to the development set.", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.9550458590189616}]}, {"text": "See the official results of our system, which are presented in table 2.", "labels": [], "entities": []}, {"text": "Also from that table, it is worth noting that the F 1 performance drops by more than 9 points when tested on the Brown test set, indicating that the results obtained on the WSJ corpora do not generalize well to corpora with other genres.", "labels": [], "entities": [{"text": "F 1 performance", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.9114632407824198}, {"text": "Brown test set", "start_pos": 113, "end_pos": 127, "type": "DATASET", "confidence": 0.939441998799642}, {"text": "WSJ corpora", "start_pos": 173, "end_pos": 184, "type": "DATASET", "confidence": 0.9319309294223785}]}, {"text": "The study of the sources of this lower performance deserves further investigation, though we do not believe that it is attributable to the greedy combination scheme.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overall results of the individual systems on  the development set.", "labels": [], "entities": []}, {"text": " Table 2: Overall results (top) and detailed results on  the WSJ test (bottom).", "labels": [], "entities": [{"text": "WSJ test", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.8157025575637817}]}]}