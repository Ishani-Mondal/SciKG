{"title": [{"text": "Lexical and Structural Biases for Function Parsing", "labels": [], "entities": [{"text": "Function Parsing", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.6830677688121796}]}], "abstractContent": [{"text": "In this paper, we explore two extensions to an existing statistical parsing model to produce richer parse trees, annotated with function labels.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.5963314175605774}]}, {"text": "We achieve significant improvements in parsing by modelling directly the specific nature of function labels , as both expressions of the lexical semantics properties of a constituent and as syntactic elements whose distribution is subject to structural locality constraints.", "labels": [], "entities": [{"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9770268797874451}]}, {"text": "We also reach state-of-the-art accuracy on function labelling.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9991719722747803}, {"text": "function labelling", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7153644263744354}]}, {"text": "Our results suggest that current statistical parsing methods are sufficiently robust to produce accurate shallow functional or semantic annotation , if appropriately biased.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.684855729341507}]}], "introductionContent": [{"text": "Natural language processing methods producing shallow semantic output are starting to emerge as the next step towards successful developments in natural language understanding.", "labels": [], "entities": [{"text": "Natural language processing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6406500538190206}, {"text": "natural language understanding", "start_pos": 145, "end_pos": 175, "type": "TASK", "confidence": 0.663127621014913}]}, {"text": "Incremental, robust parsing systems will be the core enabling technology for interactive, speech-based question answering and dialogue systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7581168413162231}]}, {"text": "In recent years, corpora annotated with semantic and function labels have seen the light () and semantic role labelling has taken centre-stage as a challenging new task.", "labels": [], "entities": []}, {"text": "State-of-the-art statistical parsers have not yet responded to this challenge.", "labels": [], "entities": []}, {"text": "State-of-the-art statistical parsers trained on the Penn Treebank (PTB)  duce trees annotated with bare phrase structure labels).", "labels": [], "entities": [{"text": "Penn Treebank (PTB)  duce trees", "start_pos": 52, "end_pos": 83, "type": "DATASET", "confidence": 0.9785275374140058}]}, {"text": "The trees of the Penn Treebank, however, are also decorated with function labels, labels that indicate the grammatical and semantic relationship of phrases to each other in the sentence.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.9944302439689636}]}, {"text": "shows the simplified tree representation with function labels fora sample sentence from the PTB corpus (section 00) The Government's borrowing authority dropped at midnight Tuesday to 2.80 trillion from 2.87 trillion.", "labels": [], "entities": [{"text": "PTB corpus", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.9932614862918854}]}, {"text": "Unlike phrase structure labels, function labels are contextdependent and encode a shallow level of phrasal and lexical semantics, as observed first in).", "labels": [], "entities": []}, {"text": "For example, while the authority in will always be a Noun Phrase, it could be a subject, as in the example, or an object, as in the sentence They questioned his authority, depending on its position in the sentence.", "labels": [], "entities": []}, {"text": "To some extent, function labels overlap with semantic role labels as defined in illustrates the complete list of function labels in the Penn Treebank, partitioned into four classes.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 136, "end_pos": 149, "type": "DATASET", "confidence": 0.9947628974914551}]}, {"text": "Current statistical parsers do not use or output this richer information because performance of the parser usually decreases considerably, since a more complex task is being solved.", "labels": [], "entities": []}, {"text": "(, for instance report a reduction in parsing accuracy of an unlexicalised PCFG from 77.8% to 72.9% if using function labels in training.) also reports a decrease in performance when attempting to integrate his function labelling system with a full parser.", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.9687707424163818}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.8747294545173645}]}, {"text": "Conversely, researchers interested in producing richer semantic outputs have concentrated on two-stage systems, where the semantic labelling task is performed on the output of a parser, in a pipeline architecture divided in several stages (.", "labels": [], "entities": []}, {"text": "See also the common task of), where parsing has sometimes not been used and has been replaced by chunking.", "labels": [], "entities": [{"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9659343957901001}]}, {"text": "In this paper, we present a parser that produces richer output using information available in a corpus incrementally.", "labels": [], "entities": []}, {"text": "Specifically, the parser outputs additional labels indicating the function of a constituent in the tree, such as NP-SBJ or PP-TMP in the tree shown in.", "labels": [], "entities": []}, {"text": "Following), we concentrate on syntactic and semantic function labels.", "labels": [], "entities": []}, {"text": "We will ignore the other two classes, for they do not form natural classes.", "labels": [], "entities": []}, {"text": "Like previous work, constituents that do not bear any function label will receive a NULL label.", "labels": [], "entities": []}, {"text": "Strictly speaking, this label corresponds to two NULL labels: the SYN-NULL and the SEM-NULL.", "labels": [], "entities": []}, {"text": "A node bearing the SYN-NULL label is anode that does not bear any other syntactic label.", "labels": [], "entities": []}, {"text": "Analogously, the SEM-NULL label completes the set of semantic labels.", "labels": [], "entities": []}, {"text": "Note that both the SYN-NULL label and the SEM-NULL are necessary, since both a syntactic and a semantic label can label a given constituent.", "labels": [], "entities": []}, {"text": "We present work to test the hypothesis that a current statistical parser can output richer information robustly, that is without any degradation of the parser's accuracy on the original parsing task, by explicitly modelling function labels as the locus where the lexical semantics of the elements in the sentence and syntactic locality domains interact.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9984353184700012}]}, {"text": "Briefly, our method consists in augmenting the parser with features and biases that capture both lexical semantics projections and structural regularities underlying the distribution of sequences of function labels in a sentence.", "labels": [], "entities": []}, {"text": "We achieve state-of-the-art results both in parsing and function labelling.", "labels": [], "entities": [{"text": "parsing", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.9682859778404236}]}, {"text": "This result has several consequences.", "labels": [], "entities": []}, {"text": "On the one hand, we show that it is possible to build a single integrated robust system successfully.", "labels": [], "entities": []}, {"text": "This is an interesting achievement, as a task combining function labelling and parsing is more complex than simple parsing.", "labels": [], "entities": []}, {"text": "While the function of a constituent and its structural position are often correlated, they sometimes diverge.", "labels": [], "entities": []}, {"text": "For example, some nominal temporal modifiers occupy an object position without being objects, like Tuesday in the tree above.", "labels": [], "entities": []}, {"text": "Moreover, given current limited availability of annotated tree banks, this more complex task will have to be solved with the same overall amount of data, aggravating the difficulty of estimating the model's parameters due to sparse data.", "labels": [], "entities": []}, {"text": "Solving this more complex problem successfully, then, indicates that the models used are robust.", "labels": [], "entities": []}, {"text": "Our results also provide some new insights into the discussion about the necessity of parsing for function or semantic role labelling (, showing that parsing is beneficial.", "labels": [], "entities": []}, {"text": "On the other hand, function labelling while parsing opens the way to interactive applications that are not possible in a two-stage architecture.", "labels": [], "entities": [{"text": "function labelling", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.8130830824375153}]}, {"text": "Because the parser produces richer output incrementally at the same time as parsing, it can be integrated in speechbased applications, as well as be used for language models.", "labels": [], "entities": []}, {"text": "Conversely, output annotated with more informative labels, such as function or semantic labels, underlies all domain-independent question answering () or shallow semantic interpretation systems).", "labels": [], "entities": [{"text": "question answering", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.7014114111661911}]}], "datasetContent": [{"text": "To assess the relevance of our fine-grained tags and history representations for functional labelling, we compare two augmented models to two baseline models without these augmentations indicated in Table 2 as no-biases and H03.", "labels": [], "entities": []}, {"text": "The baseline called H03 refers to our runs of the parser described in), which is not trained on input annotated with function labels.", "labels": [], "entities": []}, {"text": "Comparison to this model gives us an external reference to whether function labelling improves parsing.", "labels": [], "entities": []}, {"text": "The baseline called nobiases refers to a model without any structural or lexical biases, but trained on input annotated with function labels.", "labels": [], "entities": []}, {"text": "This comparison will tell us if the biases are useful or if the reported improvements could have been obtained without explicit manipulation of the parsing biases.", "labels": [], "entities": []}, {"text": "All SSN function parsers were trained on sections 2-21 from the PTB and validated on section 24.", "labels": [], "entities": [{"text": "SSN function parsers", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.9086544911066691}, {"text": "PTB", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.984855592250824}]}, {"text": "They are trained on parse trees whose labels include syntactic and semantic function labels.", "labels": [], "entities": []}, {"text": "The models, as well as the parser described in, are run only once.", "labels": [], "entities": []}, {"text": "This explains the little difference in performance between our results for H03 in our table of results and those cited in, where the best of three runs on the validation set is chosen.", "labels": [], "entities": [{"text": "H03", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.8425204753875732}]}, {"text": "To evaluate the performance of our function parsing experiments, we extend standard Parseval measures of labelled recall and precision to include function labels.", "labels": [], "entities": [{"text": "function parsing", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.735811710357666}, {"text": "Parseval", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.8735567331314087}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.8977838158607483}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9962706565856934}]}, {"text": "The augmented models have a total of 188 nonterminals to represents labels of constituents: Percentage F-measure (F), recall (R), and precision (P) of the SSN baseline and augmented parsers. of lowering the five function labels, 83 new part-ofspeech tags were introduced to partition the original tag set.", "labels": [], "entities": [{"text": "Percentage F-measure (F)", "start_pos": 92, "end_pos": 116, "type": "METRIC", "confidence": 0.9174901247024536}, {"text": "recall (R)", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9536723792552948}, {"text": "precision (P)", "start_pos": 134, "end_pos": 147, "type": "METRIC", "confidence": 0.9663063883781433}]}, {"text": "SSN parsers do not tag their input sentences.", "labels": [], "entities": [{"text": "SSN parsers", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7937372624874115}]}, {"text": "To provide the augmented models with tagged input sentences, we trained an SVM tagger whose features and parameters are described in detail in ().", "labels": [], "entities": []}, {"text": "Trained on section 2-21, the tagger reaches a performance of 95.8% on the test set (section 23) of the PTB using our new tag set.", "labels": [], "entities": [{"text": "tagger", "start_pos": 29, "end_pos": 35, "type": "TASK", "confidence": 0.961480438709259}, {"text": "PTB", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.9050861597061157}]}, {"text": "Both parsing results taking function labels into account in the evaluation (FLABEL) and results not taking them into account in the evaluation (FLABEL-less) are reported in, which shows results on the test set, section 23 of the PTB.", "labels": [], "entities": [{"text": "parsing", "start_pos": 5, "end_pos": 12, "type": "TASK", "confidence": 0.9600827693939209}, {"text": "FLABEL", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9837461709976196}, {"text": "FLABEL-less", "start_pos": 144, "end_pos": 155, "type": "METRIC", "confidence": 0.9551399350166321}, {"text": "PTB", "start_pos": 229, "end_pos": 232, "type": "DATASET", "confidence": 0.9788913130760193}]}, {"text": "Both the model augmented only with lexical information (through tag splitting) and the one augmented both with finer-grained tags and representations of syntactic locality perform better than our comparison baseline H03, but only the latter is significantly better (p < .01, using)'s randomised test).", "labels": [], "entities": [{"text": "tag splitting", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.7287813127040863}]}, {"text": "This indicates that while information projected from the lexical items is very important, only a combination of lexical semantics information and careful modelling of syntactic domains provides a significant improvement.", "labels": [], "entities": []}, {"text": "Parsing results outputting function labels (FLA-BEL columns) reported in indicate that parsing function labels is more difficult than parsing bare phrase-structure labels (compare the FLABEL column to the FLABEL-less column).", "labels": [], "entities": [{"text": "parsing function labels", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.8930325508117676}, {"text": "FLABEL column", "start_pos": 184, "end_pos": 197, "type": "DATASET", "confidence": 0.7981731593608856}, {"text": "FLABEL-less", "start_pos": 205, "end_pos": 216, "type": "DATASET", "confidence": 0.8478156924247742}]}, {"text": "They also show that our model including finer-grained tags and locality biases performs better than the one including only finer-grained tags when outputting function labels.", "labels": [], "entities": []}, {"text": "This suggests that our model with both lexical and structural biases performs better than our no-biases comparison baseline precisely because it is able to learn to parse function labels more accurately.", "labels": [], "entities": []}, {"text": "Comparisons to the baseline without biases indicates clearly that the observed improvements, both on function parsing and on parsing without taking function labels into consideration would not have been obtained without explicit biases.", "labels": [], "entities": [{"text": "function parsing", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.6589402854442596}]}, {"text": "Individual performance on syntactic and semantic function labelling compare favourably to previous attempts).", "labels": [], "entities": [{"text": "syntactic and semantic function labelling", "start_pos": 26, "end_pos": 67, "type": "TASK", "confidence": 0.6032327771186828}]}, {"text": "Note that the maximal precision or recall score of function labelling is strictly smaller than one-hundred percent if the precision or the recall of the parser is less than one-hundred percent.", "labels": [], "entities": [{"text": "maximal", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.923054039478302}, {"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.7942508459091187}, {"text": "recall score", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.9808604717254639}, {"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9986112117767334}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9941167831420898}]}, {"text": "Following), incorrectly parsed constituents will be ignored (roughly 11% of the total) in the evaluation of the precision and recall of the function labels, but not in the evaluation of the parser.", "labels": [], "entities": [{"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9989380240440369}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9907922744750977}]}, {"text": "Of the correctly parsed constituents, some bear function labels, but the overwhelming majority do not bear any label, or rather, in our notation, they bear a NULL label.", "labels": [], "entities": []}, {"text": "To avoid calculating excessively optimistic scores, constituents bearing the NULL label are not taken into consideration for computing overall recall and precision figures.", "labels": [], "entities": [{"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9889171123504639}, {"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.975897490978241}]}, {"text": "NULLlabelled constituents are only needed to calculate the precision and recall of other function labels.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.999180018901825}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9957321286201477}]}, {"text": "For example, consider the confusion matrix Min Table 3 below, which reports scores for the semantic labels recovered by the no-biases model.", "labels": [], "entities": []}, {"text": "Precision is computed as . Recall is computed analogously.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9893617033958435}, {"text": "Recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9979416728019714}]}, {"text": "Notice that M [n, n], that is the [SEM-NULL,SEM-NULL] cell in the matrix, is never taken into account.", "labels": [], "entities": []}, {"text": "Syntactic labels are recovered with very high accuracy (F 96.5%, R 95.5% and P 97.5%) by the model with both lexical and structural biases, and so are semantic labels, which are considerably more difficult (F 85.6%, R 81.5% and P 90.2%).) uses specialised models for the two types  of function labels, reaching an F-measure of 98.7% for syntactic labels and 83.4% for semantic labels as best accuracy measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9976765513420105}, {"text": "F", "start_pos": 56, "end_pos": 57, "type": "METRIC", "confidence": 0.9032132029533386}, {"text": "F-measure", "start_pos": 314, "end_pos": 323, "type": "METRIC", "confidence": 0.9984525442123413}, {"text": "accuracy", "start_pos": 392, "end_pos": 400, "type": "METRIC", "confidence": 0.9968441724777222}]}, {"text": "Previous work that uses, like us, a single model for both types of labels reaches an F measure of 95.7% for syntactic labels and 79.0% for semantic labels).", "labels": [], "entities": [{"text": "F measure", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.99134761095047}]}, {"text": "Although functional information is explicitly annotated in the PTB, it has not yet been exploited by any state-of-the-art statistical parser with the notable exception of the second parsing model of).", "labels": [], "entities": [{"text": "PTB", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.8964690566062927}]}, {"text": "Collins's second model uses a few function labels to discriminate between arguments and adjuncts, and includes parameters to generate subcategorisation frames.", "labels": [], "entities": []}, {"text": "Subcategorisation frames are modelled as multisets of arguments that are sisters of a lexicalised head child.", "labels": [], "entities": []}, {"text": "Some major differences distinguish Collins's subcategorisation parameters from our structural biases.", "labels": [], "entities": []}, {"text": "First, lexicalised head children are not explicitly represented in our model.", "labels": [], "entities": []}, {"text": "Second, we do not discriminate between arguments and adjuncts: we only encode the distinctions between syntactic function labels and semantic ones.", "labels": [], "entities": []}, {"text": "As shown in) this difference does not correspond to the difference between arguments and adjuncts.", "labels": [], "entities": []}, {"text": "Finally, our model does not implement any distinction between right and left subcategorisation frames.", "labels": [], "entities": []}, {"text": "In Collins's model, the left and right subcategorisation frames are conditionally independent and arguments occupying a complement position (to the right of the head) are independent of arguments occurring in a specifier position (to the left of the head).", "labels": [], "entities": []}, {"text": "In our model, no such independence assumptions are stated, because the model is biased towards phrases related to each other by the c-command relation.", "labels": [], "entities": []}, {"text": "Such relation could involve both elements at the left and at the right of the head.", "labels": [], "entities": []}, {"text": "Relations of functional assignments between subjects and objects, for example, could be captured.", "labels": [], "entities": []}, {"text": "The most important observation, however, is that modelling function labels as the interface between syntax and semantics yields a significant improvement on parsing performance, as can be verified in the FLABEL-less column of.", "labels": [], "entities": [{"text": "FLABEL-less", "start_pos": 204, "end_pos": 215, "type": "METRIC", "confidence": 0.7101532816886902}]}, {"text": "This is a crucial observation in the light of the current approaches to function or semantic role labelling and its relation to parsing.", "labels": [], "entities": [{"text": "function or semantic role labelling", "start_pos": 72, "end_pos": 107, "type": "TASK", "confidence": 0.5951925098896027}]}, {"text": "An improvement in parsing performance by better modelling of function labels indicates that this complex problem is better solved as a single integrated task and that current two-step architectures might be missing on successful ways to improve both the parsing and the labelling task.", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9814026951789856}]}, {"text": "In particular, recent models of semantic role labelling separate input indicators of the correlation between the structural position in the tree and the semantic label, such as path, from those indicators that encode constraints on the sequence, such as the previously assigned role ().", "labels": [], "entities": []}, {"text": "In this way, they can never encode directly the constraining power of a certain role in a given structural position onto a following node in its structural position.", "labels": [], "entities": []}, {"text": "In our augmented model, we attempt to capture these constraints by directly modelling syntactic domains.", "labels": [], "entities": []}, {"text": "Our results confirm the findings in ( ).", "labels": [], "entities": []}, {"text": "They take a critical look at some commonly used features in the semantic role labelling task, such as the path feature.", "labels": [], "entities": [{"text": "semantic role labelling task", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.7190741151571274}]}, {"text": "They suggest that the path feature is not very effective because it is sparse.", "labels": [], "entities": []}, {"text": "Its sparseness is due to the occurrence of intermediate nodes that are not relevant for the syntactic relations between an argument and its predicate.", "labels": [], "entities": []}, {"text": "Our model of domains is less noisy, because it can focus only on c-commanding nodes bearing function labels, thus abstracting away from those nodes that smear the pertinent relations.", "labels": [], "entities": []}, {"text": "( share the motivation of our work, although they apply it to a different task.", "labels": [], "entities": []}, {"text": "Like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information.", "labels": [], "entities": []}, {"text": "Our results also confirm the importance of lexical information, the lesson drawn from), who find that correctly modelling sequence information is not sufficient.", "labels": [], "entities": []}, {"text": "Lexical information is very important, as it reflects the lexical semantics of the constituents.", "labels": [], "entities": []}, {"text": "Both factors, syntactic domains and lexical information, are needed to significantly improve parsing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Percentage F-measure (F), recall (R), and precision (P) of the SSN baseline and augmented parsers.", "labels": [], "entities": [{"text": "Percentage F-measure (F)", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.8863134264945984}, {"text": "recall (R)", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9633595645427704}, {"text": "precision (P)", "start_pos": 52, "end_pos": 65, "type": "METRIC", "confidence": 0.9761587828397751}]}, {"text": " Table 3: Confusion matrix for the no-biases baseline model, tested on the validation set (section 24 of PTB).", "labels": [], "entities": [{"text": "PTB", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.8921056985855103}]}]}