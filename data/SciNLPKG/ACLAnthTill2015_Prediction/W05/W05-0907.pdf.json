{"title": [{"text": "Evaluating DUC 2004 Tasks with the QARLA Framework", "labels": [], "entities": [{"text": "Evaluating DUC 2004 Tasks", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7508638948202133}, {"text": "QARLA Framework", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.9628994464874268}]}], "abstractContent": [{"text": "This papers reports the application of the QARLA evaluation framework to the DUC 2004 testbed (tasks 2 and 5).", "labels": [], "entities": [{"text": "DUC 2004 testbed", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.977531909942627}]}, {"text": "Our experiment addresses two issues: how well QARLA evaluation measures correlate with human judgements, and what additional insights can be provided by the QARLA framework to the DUC evaluation exercises.", "labels": [], "entities": [{"text": "DUC evaluation", "start_pos": 180, "end_pos": 194, "type": "TASK", "confidence": 0.5592360198497772}]}], "introductionContent": [{"text": "QARLA () is a framework that uses similarity to models as a building block for the evaluation of automatic summarisation systems.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 107, "end_pos": 120, "type": "TASK", "confidence": 0.8552648425102234}]}, {"text": "The input of QARLA is a summarisation task, a set of test cases, a set of similarity metrics, and sets of models and automatic summaries (peers) for each test case.", "labels": [], "entities": [{"text": "summarisation task", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8936165571212769}]}, {"text": "With such a testbed, QARLA provides: \u2022 A measure, QUEEN, which combines assorted similarity metrics to estimate the quality of automatic summarisers.", "labels": [], "entities": [{"text": "QUEEN", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.925471305847168}]}, {"text": "\u2022 A measure, KING, to select the best combination of similarity metrics.", "labels": [], "entities": [{"text": "KING", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9699383974075317}]}, {"text": "\u2022 An estimation, JACK, of the reliability of the testbed for evaluation purposes.", "labels": [], "entities": [{"text": "estimation", "start_pos": 5, "end_pos": 15, "type": "METRIC", "confidence": 0.9314466118812561}, {"text": "JACK", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9817858338356018}, {"text": "reliability", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.961821973323822}]}, {"text": "The QARLA framework does not rely on human judges.", "labels": [], "entities": [{"text": "QARLA framework", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7915602028369904}]}, {"text": "It is interesting, however, to find out how well an evaluation using QARLA correlates with human judges, and whether QARLA can provide additional insights into an evaluation based on human assessments.", "labels": [], "entities": []}, {"text": "In this paper, we apply the QARLA framework to the output of two different evaluation exercises: DUC 2004 tasks 2 and 5 ().", "labels": [], "entities": [{"text": "DUC 2004 tasks 2", "start_pos": 97, "end_pos": 113, "type": "DATASET", "confidence": 0.9138912707567215}]}, {"text": "Task 2 requires short (one-hundred word) summaries for assorted document sets; Task 5 consists of generating a short summary in response to a \"Who is\" question.", "labels": [], "entities": []}, {"text": "In Section 2, we summarise the QARLA evaluation framework; in Section 3, we describe the similarity metrics used in the experiments.", "labels": [], "entities": []}, {"text": "Section 4 discusses the results of the QARLA framework using such metrics on the DUC testbeds.", "labels": [], "entities": [{"text": "QARLA framework", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.7663934528827667}, {"text": "DUC testbeds", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.9845190048217773}]}, {"text": "Finally, Section 5 draws some conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "QARLA uses similarity to models for the evaluation of automatic summarisation systems.", "labels": [], "entities": [{"text": "QARLA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9080782532691956}, {"text": "summarisation", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.8479912877082825}]}, {"text": "Here we summarise its main features; the reader may refer to) for details.", "labels": [], "entities": []}, {"text": "The input of the framework is: \u2022 A summarisation task (e.g. topic oriented, informative multi-document summarisation on a given domain/corpus).", "labels": [], "entities": []}, {"text": "\u2022 A set T of test cases (e.g. topic/document set pairs for the example above) \u2022 A set of summaries M produced by humans (models), and a set of automatic summaries A (peers), for every test case.", "labels": [], "entities": []}, {"text": "\u2022 A set X of similarity metrics to compare summaries.", "labels": [], "entities": []}, {"text": "With this input, QARLA provides three main measures that we describe below.", "labels": [], "entities": [{"text": "QARLA", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.47537651658058167}]}, {"text": "\u2022 The last column shows the best metric set, considering all possible metric combinations.", "labels": [], "entities": []}, {"text": "In both DUC tasks, the best combination is {Rpre-W-1.2.b, TVM.512.", "labels": [], "entities": [{"text": "Rpre-W-1.2.b", "start_pos": 44, "end_pos": 56, "type": "METRIC", "confidence": 0.8405714631080627}, {"text": "TVM.512", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9518907070159912}]}, {"text": "This metric set gets better KING values than any individual metric in isolation (17% better than the second best for task 2, and 23% better for task 5).", "labels": [], "entities": [{"text": "KING", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9945355653762817}]}, {"text": "This is an interesting result confirming that we can improve our ability to characterise human summaries just by combining standard similarity metrics in the QARLA framework.", "labels": [], "entities": [{"text": "characterise human summaries", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.798080583413442}, {"text": "QARLA framework", "start_pos": 158, "end_pos": 173, "type": "DATASET", "confidence": 0.8878837823867798}]}, {"text": "Note also that both metrics in the best set are content-oriented.", "labels": [], "entities": []}, {"text": "\u2022 Rpre-W.1.2.b (inverted ROUGE measure, using non-contiguous word sequences, removing stopwords, without stemming) obtains the highest individual KING for task 2, and is one of the best in task 5, confirming that ROUGEbased metrics area robust way of evaluating summaries, and indicating that non-contiguous word sequences can be more useful for evaluation purposes than n-grams.", "labels": [], "entities": [{"text": "Rpre-W.1.2.b", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.9563918709754944}, {"text": "KING", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9546679258346558}]}, {"text": "\u2022 TVM metrics get higher values when considering more terms (TVM.512), confirming that comparing with just a few terms (e.g. TVM.4) is not informative enough.", "labels": [], "entities": [{"text": "TVM.512", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.8638956546783447}]}, {"text": "\u2022 Overall, KING values are higher for task 5, suggesting that there is more agreement between human summaries in topic-oriented tasks.", "labels": [], "entities": [{"text": "KING", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.998712420463562}]}, {"text": "The QUEEN measure provides two kinds of information to compare automatic summarisation systems: which are the best systems -according to the best metric set-, and which are the individual features of every automatic summariser -according to individual similarity metrics-.", "labels": [], "entities": [{"text": "QUEEN measure", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7427446246147156}, {"text": "summarisation", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.8813260793685913}]}], "tableCaptions": []}