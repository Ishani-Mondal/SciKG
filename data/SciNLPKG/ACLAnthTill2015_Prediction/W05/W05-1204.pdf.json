{"title": [{"text": "Training Data Modification for SMT Considering Groups of Synonymous Sentences", "labels": [], "entities": [{"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.996525228023529}]}], "abstractContent": [{"text": "Generally speaking, statistical machine translation systems would be able to attain better performance with more training sets.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.6412248313426971}]}, {"text": "Unfortunately, well-organized training sets are rarely available in the real world.", "labels": [], "entities": []}, {"text": "Consequently , it is necessary to focus on modifying the training set to obtain high accuracy for an SMT system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9986145496368408}, {"text": "SMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.9933069348335266}]}, {"text": "If the SMT system trained the translation model, the translation pair would have a low probability when there are many variations for target sentences from a single source sentence.", "labels": [], "entities": [{"text": "SMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9809157252311707}]}, {"text": "If we decreased the number of variations for the translation pair, we could construct a superior translation model.", "labels": [], "entities": []}, {"text": "This paper describes the effects of modification on the training corpus when consideration is given to synonymous sentence groups.", "labels": [], "entities": []}, {"text": "We attempt three types of modification: compression of the training set, replacement of source and target sentences with a selected sentence from the synonymous sentence group, and replacement of the sentence on only one side with the selected sentence from the synonymous sentence group.", "labels": [], "entities": []}, {"text": "As a result, we achieve improved performance with the replacement of source-side sentences .", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, many researchers have focused their interest on statistical machine translation (SMT) systems, with particular attention given to models and decoding algorithms.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 58, "end_pos": 95, "type": "TASK", "confidence": 0.8089642971754074}]}, {"text": "The quantity of the training corpus has received less attention, although of course the earlier reports do address the quantity issue.", "labels": [], "entities": [{"text": "quantity", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9558717608451843}, {"text": "quantity", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9754299521446228}]}, {"text": "In most cases, the larger the training corpus becomes, the higher accuracy is achieved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9995204210281372}]}, {"text": "Usually, the quantity problem of the training corpus is discussed in relation to the size of the training corpus and system performance; therefore, researchers study line graphs that indicate the relationship between accuracy and training corpus size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.9983134269714355}]}, {"text": "On the other hand, needless to say, a single sentence in the source language can be used to translate several sentences in the target language.", "labels": [], "entities": []}, {"text": "Such various possibilities for translation make MT system development and evaluation very difficult.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9898977279663086}, {"text": "MT system development", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.8948206504185995}]}, {"text": "Consequently, here we employ multiple references to evaluate MT systems like BLEU () and NIST).", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9842970967292786}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9959520101547241}, {"text": "NIST", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.8821619153022766}]}, {"text": "Moreover, such variations in translation have a negative effect on training in SMT because when several sentences of input-side language are translated into the exactly equivalent output-side sentences, the probability of correct translation decreases due to the large number of possible pairs of expressions.", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9960135221481323}]}, {"text": "Therefore, if we can restrain or modify the training corpus, the SMT system might achieve high accuracy.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9922681450843811}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9979720711708069}]}, {"text": "As an example of modification, different output-side sentences paired with the exactly equivalent input-side sentences are replaced with one target sentence.", "labels": [], "entities": []}, {"text": "These sentence replacements are required for synonymous sentence sets.", "labels": [], "entities": [{"text": "sentence replacements", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.7391013503074646}]}, {"text": "discussed synonymous sets of sentences.", "labels": [], "entities": []}, {"text": "Here, we employ a method to group them as away of modifying the training corpus for use with SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9849410653114319}]}, {"text": "This paper focuses on how to control the corpus while giving consideration to synonymous sentence groups.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the SMT systems used in these experiments.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9954891800880432}]}, {"text": "The SMT systems' decoder is a graph-based decoder ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9739710092544556}]}, {"text": "The first pass of the decoder generates a word-graph, a compact representation of alternative translation candidates, using abeam search based on the scores of the lexicon and language models.", "labels": [], "entities": []}, {"text": "In the second pass, an A* search traverses the graph.", "labels": [], "entities": []}, {"text": "The edges of the word-graph, or the phrase translation candidates, are generated by the list of word translations obtained from the inverted lexicon model.", "labels": [], "entities": []}, {"text": "The phrase translations extracted from the Viterbi alignments of the training corpus also constitute the edges.", "labels": [], "entities": []}, {"text": "Similarly, the edges are also created from dynamically extracted phrase translations from the bilingual sentences ( BLEU: A weighted geometric mean of the ngram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9977420568466187}]}, {"text": "NIST: An arithmetic mean of the n-gram matches between test and reference sentences multiplied by a length factor, which again penalizes short translation sentences.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9374043941497803}]}, {"text": "mWER (): Multiple reference word-error rate, which computes the edit distance (minimum number of insertions, deletions, and substitutions) between test and reference sentences.", "labels": [], "entities": []}, {"text": "mPER: Multiple reference position-independent word-error rate, which computes the edit distance without considering the word order.", "labels": [], "entities": [{"text": "mPER", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7820284366607666}]}, {"text": "In this section, we show the experimental results for the JE/EJ and JC/CJ systems..", "labels": [], "entities": [{"text": "JE/EJ", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.6656052470207214}]}, {"text": "Evaluation results for JE system Modification of the training data is based on the synonymous sentence group with the JE pair.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 Statistics used in BTEC data", "labels": [], "entities": [{"text": "BTEC data", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.9363933503627777}]}, {"text": " Table 2. Evaluation results for EJ System", "labels": [], "entities": [{"text": "EJ System", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.8917892277240753}]}, {"text": " Table 3. Evaluation results for JE system", "labels": [], "entities": [{"text": "JE", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.6482833623886108}]}, {"text": " Table 4. Evaluation results for CJ based on the JC  language pair", "labels": [], "entities": [{"text": "JC  language", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.8716590106487274}]}, {"text": " Table 5. Evaluation results for JC based on the JC  language pair", "labels": [], "entities": [{"text": "JC", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.6672722697257996}, {"text": "JC  language", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.8747501075267792}]}]}