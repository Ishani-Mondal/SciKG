{"title": [], "abstractContent": [{"text": "We present a strictly lexical parsing model where all the parameters are based on the words.", "labels": [], "entities": []}, {"text": "This model does not rely on part-of-speech tags or grammatical categories.", "labels": [], "entities": []}, {"text": "It maximizes the conditional probability of the parse tree given the sentence.", "labels": [], "entities": []}, {"text": "This is in contrast with most previous models that compute the joint probability of the parse tree and the sentence.", "labels": [], "entities": []}, {"text": "Although the maximization of joint and conditional probabilities are theoretically equivalent, the conditional model allows us to use distributional word similarity to generalize the observed frequency counts in the training corpus.", "labels": [], "entities": []}, {"text": "Our experiments with the Chi-nese Treebank show that the accuracy of the conditional model is 13.6% higher than the joint model and that the strictly lexicalized conditional model outper-forms the corresponding unlexicalized model based on part-of-speech tags.", "labels": [], "entities": [{"text": "Chi-nese Treebank", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.7163500636816025}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9996708631515503}]}], "introductionContent": [{"text": "There has been a great deal of progress in statistical parsing in the past decade.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8980062901973724}]}, {"text": "A common characteristic of these parsers is their use of lexicalized statistics.", "labels": [], "entities": []}, {"text": "However, it was discovered recently that bi-lexical statistics (parameters that involve two words) actually played much smaller role than previously believed.", "labels": [], "entities": []}, {"text": "It was found in) that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output.", "labels": [], "entities": []}, {"text": "observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser.", "labels": [], "entities": []}, {"text": "When considering only bigram statistics involved in the highest probability parse, this percentage becomes 28.8%.", "labels": [], "entities": []}, {"text": "However, even when the bi-lexical statistics do get used, they are remarkably similar to their back-off values using part-of-speech tags.", "labels": [], "entities": []}, {"text": "Therefore, the utility of bi-lexical statistics becomes rather questionable.", "labels": [], "entities": []}, {"text": "presented an unlexicalized parser that eliminated all lexicalized parameters.", "labels": [], "entities": []}, {"text": "Its performance was close to the state-of-the-art lexicalized parsers.", "labels": [], "entities": []}, {"text": "We present a statistical dependency parser that represents the other end of spectrum where all statistical parameters are lexical and the parser does not require part-of-speech tags or grammatical categories.", "labels": [], "entities": [{"text": "statistical dependency parser", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.6384889582792918}]}, {"text": "We call this strictly lexicalized parsing.", "labels": [], "entities": []}, {"text": "A part-of-speech lexicon has always been considered to be a necessary component in any natural language parser.", "labels": [], "entities": []}, {"text": "This is true in early rule-based as well as modern statistical parsers and in dependency parsers as well as constituency parsers.", "labels": [], "entities": [{"text": "statistical parsers", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.6759020686149597}, {"text": "dependency parsers", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.6817067116498947}, {"text": "constituency parsers", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.7027392983436584}]}, {"text": "The need for part-of-speech tags arises from the sparseness of natural language data.", "labels": [], "entities": []}, {"text": "They provide generalizations of words that are critical for parsers to deal with the sparseness.", "labels": [], "entities": []}, {"text": "Words belonging to the same part-of-speech are expected to have the same syntactic behavior.", "labels": [], "entities": []}, {"text": "Instead of part-of-speech tags, we rely on distributional word similarities computed automatically from a large unannotated text corpus.", "labels": [], "entities": []}, {"text": "One of the benefits of strictly lexicalized parsing is that funds investors continue to pour cash into money Many the parser can be trained with a treebank that only contains the dependency relationships between words.", "labels": [], "entities": []}, {"text": "The annotators do not need to annotate parts-of-speech or non-terminal symbols (they don't even have to know about them), making the construction of the treebank easier.", "labels": [], "entities": []}, {"text": "Strictly lexicalized parsing is especially beneficial for languages such as Chinese, where partsof-speech are not as clearly defined as English.", "labels": [], "entities": []}, {"text": "In Chinese, clear indicators of a word's part-ofspeech such as suffixes -ment, -ous or function words such as the, are largely absent.", "labels": [], "entities": []}, {"text": "In fact, monolingual Chinese dictionaries that are mainly intended for native speakers almost never contain part-of-speech information.", "labels": [], "entities": []}, {"text": "In the next section, we present a method for modeling the probabilities of dependency trees.", "labels": [], "entities": []}, {"text": "Section 3 applies similarity-based smoothing to the probability model to deal with data sparseness.", "labels": [], "entities": []}, {"text": "We then present experimental results with the Chinese Treebank in Section 4 and discuss related work in Section 5.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9866357445716858}]}], "datasetContent": [{"text": "We experimented with our parser on the Chinese Treebank (CTB) 3.0.", "labels": [], "entities": [{"text": "Chinese Treebank (CTB) 3.0", "start_pos": 39, "end_pos": 65, "type": "DATASET", "confidence": 0.9799876709779104}]}, {"text": "We used the same data split as): Sections 1-270 and 400-931 as the training set, Sections 271-300 as testing and Sections 301-325 as the development set.", "labels": [], "entities": []}, {"text": "The CTB contains constituency trees.", "labels": [], "entities": []}, {"text": "We converted them to dependency trees using the same method and the head table as).", "labels": [], "entities": []}, {"text": "Parsing Chinese generally involve segmentation as a preprocessing step.", "labels": [], "entities": [{"text": "Parsing Chinese", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9184431731700897}, {"text": "segmentation", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9754408597946167}]}, {"text": "We used the gold standard segmentation in the CTB.", "labels": [], "entities": [{"text": "CTB", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9585592746734619}]}, {"text": "The distributional similarities between the Chinese words are computed using the Chinese Gigaword corpus.", "labels": [], "entities": [{"text": "Chinese Gigaword corpus", "start_pos": 81, "end_pos": 104, "type": "DATASET", "confidence": 0.9220878879229227}]}, {"text": "We did not segment the Chinese corpus when computing the word similarity.", "labels": [], "entities": []}, {"text": "We measure the quality of the parser by the undirected accuracy, which is defined as the number of correct undirected dependency links divided by the total number of dependency links in the corpus (the treebank parse and the parser output always have the same number of links).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9530878067016602}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "It can be seen that the performance of the parser is highly correlated with the length of the sentences.", "labels": [], "entities": []}], "tableCaptions": []}