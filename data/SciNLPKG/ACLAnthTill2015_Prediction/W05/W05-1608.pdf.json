{"title": [{"text": "Incremental Generation of Multimodal Deixis Referring to Objects", "labels": [], "entities": [{"text": "Incremental Generation of Multimodal Deixis Referring to Objects", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.8721199035644531}]}], "abstractContent": [{"text": "This paper describes an approach for the generation of multimodal deixis to be uttered by an anthro-pomorphic agent in virtual reality.", "labels": [], "entities": []}, {"text": "The proposed algorithm integrates pointing and definite description.", "labels": [], "entities": []}, {"text": "Doing so, the context-dependent discriminatory power of the gesture determines the content-selection for the verbal constituent.", "labels": [], "entities": []}, {"text": "The concept of a pointing cone is used to model the region singled out by a pointing gesture and to distinguish two referential functions called object-pointing and region-pointing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deixis anchors utterances in their spatio-temporal context and can therefore be seen as a central part of the aboutness of language.", "labels": [], "entities": []}, {"text": "In face-to-face interaction deixis is typically expressed using several modalities.", "labels": [], "entities": []}, {"text": "In this paper we describe an approach for the generation of multimodal deixis referring to objects.", "labels": [], "entities": []}, {"text": "These expressions integrate two different kinds of referring to objects, indicating the location of an object by pointing or describing its properties by a definite description.", "labels": [], "entities": []}, {"text": "Following McNeill, we distinguish between abstract pointings and pointings into concrete domains.", "labels": [], "entities": []}, {"text": "Here, we focus on pointings into concrete domains co-occurring with verbal expressions, typically definite noun phrases.", "labels": [], "entities": []}, {"text": "As we will see further on, the interrelation between gesture and verbal expression is of a complex nature.", "labels": [], "entities": []}, {"text": "Both are often under-specified; only together they identify the referent unambiguously.", "labels": [], "entities": []}, {"text": "In the growing number of applications which are characterised by an anthropomorphic human-computer interface there is an increasing need for robust mechanisms when referring to objects by speech and gesture.", "labels": [], "entities": []}, {"text": "Emphasising the importance of deixis in the interaction with humanoid agents, introduced the expression deictic believability.", "labels": [], "entities": []}, {"text": "In contrast, the generation of multimodal reference is an open issue until now, while the generation of referring expressions, which identify objects by description, is well investigated (several computational models have been proposed over the last years).", "labels": [], "entities": []}, {"text": "The approach proposed for the generation of multimodal deictic expressions is based on the incremental algorithm by.", "labels": [], "entities": []}, {"text": "This algorithm for the generation of verbal referring expressions was adapted in that the spatial property location, which can be expressed either absolutely by pointing or relationally by verbal expressions (e.g. \"the left object\"), is evaluated besides other object properties in content-selection.", "labels": [], "entities": [{"text": "generation of verbal referring expressions", "start_pos": 23, "end_pos": 65, "type": "TASK", "confidence": 0.7418721079826355}]}, {"text": "Taking account of the inherent impreciseness of pointing gestures, two referential functions of pointing are distinguished, object-pointing and region-pointing.", "labels": [], "entities": []}, {"text": "While object-pointing refers on its own, region-pointing is used to narrow down the set of objects from which the referent has to be distinguished by a definite description.", "labels": [], "entities": []}, {"text": "The described research is undertaken in the course of the development of human computer interfaces for natural interaction in Virtual Reality (VR).", "labels": [], "entities": [{"text": "Virtual Reality (VR)", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.6837163627147674}]}, {"text": "Conducting empirical investigations and developing computational models we focus on dialogues in a construction task domain, where a kit consisting of generic parts is used to construct models of mechanical objects such as a toy airplane.", "labels": [], "entities": []}, {"text": "A typical setting consists of a human instructor and an anthropomorphic virtual agent interacting in face-to-face manner in VR realised in a three-side Cave-like installation.", "labels": [], "entities": []}, {"text": "Our human-sized virtual agent called Max is able to interpret simple multimodal (speech and gesture) input from the human instructor on the one hand and to produce synchronised output involving synthetic speech, facial display and gesture on the other hand.", "labels": [], "entities": []}, {"text": "As illustrated in, Max and the human dialogue partner are located at a virtual table with toy parts and communicate about how to assemble them.", "labels": [], "entities": []}, {"text": "Speech and gesture are used by both interlocutors to specify tasks and select relevant objects.", "labels": [], "entities": []}, {"text": "On the way towards dialogue generation a setting we call demonstration games has been established to get at the understanding and generation of complex deictic expressions.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.8419195711612701}]}, {"text": "These demonstration games which reduce interaction to two turns are based on the minimal dialogue games proposed by.", "labels": [], "entities": []}, {"text": "The setting consists of two interlocutors located at a table with some objects lying on it.", "labels": [], "entities": []}, {"text": "One interlocutor has to indicate an object by speech and gesture, and the other interlocutor has to give feedback on which object was referred to.", "labels": [], "entities": []}, {"text": "Ina human-human realisation this setting is used to conduct empirical studies to investigate the referring behaviour of subjects].", "labels": [], "entities": []}, {"text": "An annotated corpus was acquired which comprises 65 multimodal demonstrations uttered by several subjects.", "labels": [], "entities": []}, {"text": "Ina human-machine realisation the setting is used as a testbed for the developed communicative abilities of our agent concerning deictic reference.", "labels": [], "entities": [{"text": "deictic reference", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.7611445486545563}]}, {"text": "This enables us to directly link and compare the results of speech-gesture processing with empirically recorded data in a comparable setting.", "labels": [], "entities": []}, {"text": "In the section to follow, the role of pointing in multimodal referring expressions is analysed in more detail.", "labels": [], "entities": []}, {"text": "The concept of a pointing cone and two referential functions of pointing, object-pointing and region-pointing, are introduced.", "labels": [], "entities": []}, {"text": "3 a short overview on related work concerning the generation of referring expression is given.", "labels": [], "entities": []}, {"text": "The incremental algorithm proposed by underlying our approach is outlined in Sec.", "labels": [], "entities": []}, {"text": "4 the content-selection algorithm proposed for multimodal deictic expressions is described in detail.", "labels": [], "entities": []}, {"text": "5 illustrates its functionality giving an example.", "labels": [], "entities": []}, {"text": "6 describes the embedding of the algorithm in a generation framework and the current potentials and limitations of the approach.", "labels": [], "entities": []}, {"text": "The paper concludes with a short discussion of the proposed approach.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}