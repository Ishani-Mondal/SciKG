{"title": [{"text": "Statistical Generation: Three Methods Compared and Evaluated", "labels": [], "entities": [{"text": "Statistical Generation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8640022277832031}]}], "abstractContent": [{"text": "Statistical NLG has largely meant n-gram modelling which has the considerable advantages of lending robustness to NLG systems, and of making automatic adaptation to new domains from raw corpora possible.", "labels": [], "entities": []}, {"text": "On the downside, n-gram models are expensive to use as selection mechanisms and have a built-in bias towards shorter realisations.", "labels": [], "entities": []}, {"text": "This paper looks at treebank-training of generators, an alternative method for building statistical models for NLG from raw corpora, and two different ways of using treebank-trained models during generation.", "labels": [], "entities": []}, {"text": "Results show that the treebank-trained generators achieve improvements similar to a 2-gram generator over a baseline of random selection.", "labels": [], "entities": []}, {"text": "However , the treebank-trained generators achieve this at a much lower cost than the 2-gram generator, and without its strong preference for shorter real-isations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditionally, NLG systems have been built as deterministic decision-makers, that is to say, they generate one string of words for any given input, in a sequence of decisions that increasingly specify the word string.", "labels": [], "entities": []}, {"text": "In practice, this has meant carefully handcrafting generators to make decisions locally, at each step in the generation process.", "labels": [], "entities": []}, {"text": "Such generators tend to be specialised and domain-specific, are hard to adapt to new domains, or even subdomains, and have noway of dealing with incomplete or incorrect input.", "labels": [], "entities": []}, {"text": "Wide-coverage tools only exist for surface realisation, and tend to require highly specific and often idiosyncratic inputs.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7179752141237259}]}, {"text": "The rest of NLP has reached a stage where state-of-the-art tools are expected to be generic: wide-coverage, reusable and robust.", "labels": [], "entities": []}, {"text": "NLG is lagging behind the field on all three counts.", "labels": [], "entities": [{"text": "NLG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9702125191688538}]}, {"text": "The last decade has seen anew generation of NLG methodologies that are characterised by a separation between the definition of the generation space (all possible generation processes from inputs to outputs) on the one hand, and control over the decisions that lead to a (set of) output realisation(s), on the other.", "labels": [], "entities": []}, {"text": "In making this separation, generate-and-select NLG takes several crucial steps towards genericity: reusing systems becomes easier if the selection mechanism can be adjusted or replaced separately, without changing the definition of the generation space; coverage can be increased more easily if every expansion of the generation space does not have to be accompanied by handcrafted rules controlling the resulting increase in nondeterminism; and certain types of selection methods can provide robustness, for example through probabilistic choice.", "labels": [], "entities": []}, {"text": "Statistical generation has aroused by far the most interest among these methods, and it has mostly meant n-gram selection: a packed representation of all alternative (partial) realisations is produced, and an n-gram language model is applied to select the most likely realisation.", "labels": [], "entities": [{"text": "Statistical generation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8365959525108337}]}, {"text": "N -gram methods have several desirable properties: they offer a fully automatic method for building and adapting control mechanisms for generate-and-select NLG from raw corpora (reusability); they base selections on statistical models (robustness); and they can potentially be used for deep as well as surface generation.", "labels": [], "entities": []}, {"text": "However, n-gram models are expensive to apply: in order to select the most likely realisation according to an n-gram model, all alternative realisations have to be generated and the probability of each realisation according to the model has to be calculated.", "labels": [], "entities": []}, {"text": "This can get very expensive (even if packed representations of the set of alternatives are used), especially when the system accepts incompletely specified input, because the number of alternatives can be vast.", "labels": [], "entities": []}, {"text": "In Halogen, Langkilde deals with trillions of alternatives, and the generator used in the experiments reported in this paper has up to 10 40 alternative realisations (see Section 4.3 for empirical evidence of the relative inefficiency of n-gram generation).", "labels": [], "entities": []}, {"text": "Furthermore, n-gram models have a built-in bias towards shorter strings.", "labels": [], "entities": []}, {"text": "This is because they calculate the likelihood of a string of words as the joint probability of the words, or, more precisely, as the product of the probabilities of each word given then \u2212 1 preceding words.", "labels": [], "entities": []}, {"text": "The likelihood of any string will therefore generally be lower than that of any of its substrings (see Section 4.3 for empirical evidence of this bias).", "labels": [], "entities": []}, {"text": "This is wholly inappropriate for NLG where equally good realisations can vary greatly in length (see Section 5 for discussion of normalisation for length in statistical modelling).", "labels": [], "entities": []}, {"text": "The research reported in this paper is part of an ongoing research project 1 the purpose of which is to investigate issues in generic NLG.", "labels": [], "entities": [{"text": "generic NLG", "start_pos": 126, "end_pos": 137, "type": "TASK", "confidence": 0.6757096648216248}]}, {"text": "The experiments (Section 4.3) were carried out to evaluate and compare different methods for exploiting the frequencies of word sequences and word sequence cooccurrences in raw text corpora to build models for NL generation, and different ways of using such models during generation.", "labels": [], "entities": [{"text": "NL generation", "start_pos": 210, "end_pos": 223, "type": "TASK", "confidence": 0.8648065030574799}]}, {"text": "One of the methods uses a standard 2-gram model for selection among all realisations (with anew selection algorithm, see Section 4.3).", "labels": [], "entities": []}, {"text": "The other two use a treebanktrained model of the generation space (Section 3).", "labels": [], "entities": []}, {"text": "The basic idea behind treebank-training of generators is simple: determine for the strings and substrings in the corpus the different ways in which the generator could have generated them, i.e. the different sequences of decisions that lead to them, then collect frequency counts for individual decisions, and determine a probability distribution over decisions on this basis.", "labels": [], "entities": []}, {"text": "In the experiments, treebank-trained generation models are combined with two different ways of using them during generation, one locally and one globally optimal (Section 3.1).", "labels": [], "entities": []}, {"text": "All three methods are evaluated on a corpus of weather forecasts (Section 4.1).", "labels": [], "entities": []}], "datasetContent": [{"text": "The converted corpus (as described in Section 4.1 above) consisted of 2,123 instances, corresponding to a total of 22,985 words.", "labels": [], "entities": []}, {"text": "This may not sound like much, but considering that the entire corpus only has a vocabulary of about 90 words (not counting wind directions), and uses only a handful of different syntactic structures, the corpus provides extremely good coverage (an initial impression confirmed by the small differences between training and testing data results below).", "labels": [], "entities": []}, {"text": "The corpus was divided at random into training and testing data at a ratio of 9:1.", "labels": [], "entities": []}, {"text": "The training set was used to treebanktrain the weather forecast generation grammar (as described in Section 3.1) and a back-off 2-gram model (using the SRILM toolkit,).", "labels": [], "entities": [{"text": "weather forecast generation", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.737096389134725}, {"text": "SRILM toolkit", "start_pos": 152, "end_pos": 165, "type": "DATASET", "confidence": 0.8944444954395294}]}, {"text": "The treebank-trained generation grammar was used in conjunction with a greedy and a Viterbi generation method.", "labels": [], "entities": []}, {"text": "The 2-gram model was used in more or less exactly the way reported in the Halogen papers.", "labels": [], "entities": [{"text": "Halogen papers", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.7855798900127411}]}, {"text": "That is to say, the packed AND/OR-tree representation of all alternatives is generated in full, then the 2-gram model is applied to select the single best one.", "labels": [], "entities": []}, {"text": "One small difference to Halogen is that a Viterbi algorithm was used to identify the single most likely string.", "labels": [], "entities": []}, {"text": "This is achieved as follows.", "labels": [], "entities": []}, {"text": "The AND/OR-tree is interpreted directly as a finite-state automaton where the states correspond to words as well as to the nodes in the AND/OR-tree.", "labels": [], "entities": []}, {"text": "The transitions are assigned probabilities according to the 2-gram model, and then a straightforward single-best Viterbi algorithm is applied to find the best path through the automaton.", "labels": [], "entities": []}, {"text": "Random selection among all alternatives was used as a baseline.", "labels": [], "entities": []}, {"text": "All results were evaluated against the gold standard (the human written forecasts) of the test set.", "labels": [], "entities": []}, {"text": "Results were validated with 5-fold cross-validation.", "labels": [], "entities": []}, {"text": "In the following overview of the results 4 , the similarity between automatically generated forecasts and gold standard was measured by conventional string-edit (SE) distance with substitution at cost 2, and deletion and insertion at cost 1.", "labels": [], "entities": [{"text": "string-edit (SE) distance", "start_pos": 149, "end_pos": 174, "type": "METRIC", "confidence": 0.7064353227615356}, {"text": "insertion", "start_pos": 221, "end_pos": 230, "type": "METRIC", "confidence": 0.9747553467750549}]}, {"text": "Baseline results are given as absolute SE scores, results for the non-random generators in terms of improvement over the baseline (reduction in string-edit distance, with SE score in brackets) .", "labels": [], "entities": [{"text": "SE", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.87675541639328}, {"text": "SE score", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9794173240661621}]}], "tableCaptions": [{"text": " Table 1: BLEU n scores for training and test sets, 1 \u2264 n \u2264 4.", "labels": [], "entities": [{"text": "BLEU n scores", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9584527413050333}]}]}