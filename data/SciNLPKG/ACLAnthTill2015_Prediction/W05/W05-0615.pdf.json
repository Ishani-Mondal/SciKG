{"title": [{"text": "Representational Bias in Unsupervised Learning of Syllable Structure", "labels": [], "entities": [{"text": "Representational Bias", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9406006932258606}]}], "abstractContent": [{"text": "Unsupervised learning algorithms based on Expectation Maximization (EM) are often straightforward to implement and provably converge on a local likelihood maximum.", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.8226511955261231}]}, {"text": "However, these algorithms often do not perform well in practice.", "labels": [], "entities": []}, {"text": "Common wisdom holds that they yield poor results because they are overly sensitive to initial parameter values and easily get stuck in local (but not global) maxima.", "labels": [], "entities": []}, {"text": "We present a series of experiments indicating that for the task of learning syllable structure, the initial parameter weights are not crucial.", "labels": [], "entities": [{"text": "learning syllable structure", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.8021205464998881}]}, {"text": "Rather, it is the choice of model class itself that makes the difference between successful and unsuccessful learning.", "labels": [], "entities": []}, {"text": "We use a language-universal rule-based algorithm to find a good set of parameters, and then train the parameter weights using EM.", "labels": [], "entities": []}, {"text": "We achieve word accuracy of 95.9% on German and 97.1% on English, as compared to 97.4% and 98.1% respectively for supervised training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.946785569190979}]}], "introductionContent": [{"text": "The use of statistical methods in computational linguistics has produced advances in tasks such as parsing, information retrieval, and machine translation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 99, "end_pos": 106, "type": "TASK", "confidence": 0.9731091856956482}, {"text": "information retrieval", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.780817449092865}, {"text": "machine translation", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.8204761445522308}]}, {"text": "However, most of the successful work to date has used supervised learning techniques.", "labels": [], "entities": []}, {"text": "Unsupervised algorithms that can learn from raw linguistic data, as humans can, remain a challenge.", "labels": [], "entities": []}, {"text": "Ina statistical framework, one method that can be used for unsupervised learning is to devise a probabilistic model of the data, and then choose the values for the model parameters that maximize the likelihood of the data under the model.", "labels": [], "entities": []}, {"text": "If the model contains hidden variables, there is often no closed-form expression for the maximum likelihood parameter values, and some iterative approximation method must be used.", "labels": [], "entities": []}, {"text": "Expectation Maximization (EM)) is one way to find parameter values that at least locally maximize the likelihood for models with hidden variables.", "labels": [], "entities": [{"text": "Expectation Maximization (EM))", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8306034088134766}]}, {"text": "EM is attractive because at each iteration, the likelihood of the data is guaranteed not to decrease.", "labels": [], "entities": []}, {"text": "In addition, there are efficient dynamic-programming versions of the EM algorithm for several classes of models that are important in computational linguistics, such as the forwardbackward algorithm for training Hidden Markov Models (HMMs) and the inside-outside algorithm for training Probabilistic Context-Free Grammars (PCFGs).", "labels": [], "entities": []}, {"text": "Despite the advantages of maximum likelihood estimation and its implementation via various instantiations of the EM algorithm, it is widely regarded as ineffective for unsupervised language learning.", "labels": [], "entities": [{"text": "maximum likelihood estimation", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.5397404432296753}]}, {"text": "showed that with only a tiny amount of tagged training data, supervised training of an HMM part-of-speech tagger outperformed unsupervised EM training.", "labels": [], "entities": [{"text": "HMM part-of-speech tagger", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.7086713512738546}]}, {"text": "Later results (e.g.) seemed to indicate that other methods of unsupervised learning could be more effective (although the work of suggests that the difference maybe far less than previ-ously assumed).", "labels": [], "entities": []}, {"text": "recently achieved more encouraging results using an EM-like algorithm to induce syntactic constituent grammars, based on a deficient probability model.", "labels": [], "entities": []}, {"text": "It has been suggested that EM often yield poor results because it is overly sensitive to initial parameter values and tends to converge on likelihood maxima that are local, but not global.", "labels": [], "entities": [{"text": "EM", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9709776639938354}]}, {"text": "In this paper, we present a series of experiments indicating that for the task of learning a syllable structure grammar, the initial parameter weights are not crucial.", "labels": [], "entities": [{"text": "learning a syllable structure grammar", "start_pos": 82, "end_pos": 119, "type": "TASK", "confidence": 0.6791249513626099}]}, {"text": "Rather, it is the choice of the model class, i.e., the representational bias, that makes the difference between successful and unsuccessful learning.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we first describe the task itself and the structure of the two different classes of models we experimented with.", "labels": [], "entities": []}, {"text": "We then present a deterministic algorithm for choosing a good set of parameters for this task.", "labels": [], "entities": []}, {"text": "The algorithm is based on language-universal principles of syllabification, but produces different parameters for each language.", "labels": [], "entities": []}, {"text": "We apply this algorithm to English and German data, and describe the results of experiments using EM to learn the parameter weights for the resulting models.", "labels": [], "entities": []}, {"text": "We conclude with a discussion of the implications of our experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present a series of experiments using EM to learn a model of syllable structure.", "labels": [], "entities": []}, {"text": "All of our experiments use the same German and English 20,000-word training corpora and 10,000-word testing corpora as described in Section 2.", "labels": [], "entities": []}, {"text": "For our first experiment, we ran the categorical parser on the training corpora and estimated a model from the parse trees it produced, as described in the previous section.", "labels": [], "entities": []}, {"text": "This is essentially a single step of Viterbi EM training.", "labels": [], "entities": []}, {"text": "We then continued to train the model by running (standard) EM to convergence.", "labels": [], "entities": []}, {"text": "Results of this experiment with Categorical Parsing + EM (CP + EM) are shown in.", "labels": [], "entities": []}, {"text": "For both German and English, using this learning method with the bigram model yields performance that is much better than the categorical parser alone, though not quite as good as the fully supervised regime.", "labels": [], "entities": []}, {"text": "On the other hand, training a positional model from the categorical parser's output and then running EM causes performance to degrade.", "labels": [], "entities": []}, {"text": "To determine whether the good performance of Of course, for unsupervised learning, it is not necessary to use a distinct testing corpus.", "labels": [], "entities": []}, {"text": "We did so in order to use the same testing corpus for both supervised and unsupervised learning experiments, to ensure fair comparison of results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for German: % of all words (or  multisyllabic words) correctly syllabified.", "labels": [], "entities": []}, {"text": " Table 2: Results for English.", "labels": [], "entities": [{"text": "English", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.9003483653068542}]}]}