{"title": [{"text": "Choosing an Optimal Architecture for Segmentation and POS-Tagging of Modern Hebrew", "labels": [], "entities": [{"text": "POS-Tagging of Modern Hebrew", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.516530267894268}]}], "abstractContent": [{"text": "A major architectural decision in designing a disambiguation model for seg-mentation and Part-of-Speech (POS) tagging in Semitic languages concerns the choice of the input-output terminal symbols over which the probability distributions are defined.", "labels": [], "entities": [{"text": "Part-of-Speech (POS) tagging", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.6699851274490356}]}, {"text": "In this paper we develop a segmenter and a tagger for He-brew based on Hidden Markov Models (HMMs).", "labels": [], "entities": []}, {"text": "We start out from a morphological analyzer and a very small morphologically annotated corpus.", "labels": [], "entities": []}, {"text": "We show that a model whose terminal symbols are word segments (=morphemes), is advantageous over a word-level model for the task of POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 132, "end_pos": 143, "type": "TASK", "confidence": 0.8873628973960876}]}, {"text": "However, for segmentation alone, the morpheme-level model has no significant advantage over the word-level model.", "labels": [], "entities": []}, {"text": "Error analysis shows that both models are not adequate for resolving a common type of segmentation ambiguity in Hebrew-whether or not a word in a written text is prefixed by a definiteness marker.", "labels": [], "entities": []}, {"text": "Hence, we propose a morpheme-level model where the definiteness morpheme is treated as a possible feature of morpheme terminals.", "labels": [], "entities": []}, {"text": "This model exhibits the best overall performance, both in POS tagging and in segmentation.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.8962396085262299}, {"text": "segmentation", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.961198627948761}]}, {"text": "Despite the small size of the annotated corpus available for Hebrew, the results achieved using our best model are on par with recent results on Modern Standard Arabic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Texts in Semitic languages like Modern Hebrew (henceforth Hebrew) and Modern Standard Arabic (henceforth Arabic), are based on writing systems that allow the concatenation of different lexical units, called morphemes.", "labels": [], "entities": []}, {"text": "Morphemes may belong to various Part-of-Speech (POS) classes, and their concatenation forms textual units delimited by white space, which are commonly referred to as words.", "labels": [], "entities": []}, {"text": "Hence, the task of POS tagging for Semitic languages consists of a segmentation subtask and a classification subtask.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9581999182701111}]}, {"text": "Crucially, words can be segmented into different alternative morpheme sequences, wherein each segmentation morphemes maybe ambiguous in terms of their POS tag.", "labels": [], "entities": []}, {"text": "This results in a high level of overall ambiguity, aggravated by the lack of vocalization in modern Semitic texts.", "labels": [], "entities": []}, {"text": "One crucial problem concerning POS tagging of Semitic languages is how to adapt existing methods in the best way, and which architectural choices have to be made in light of the limited availability of annotated corpora (especially for Hebrew).", "labels": [], "entities": [{"text": "POS tagging of Semitic languages", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.9155614018440247}]}, {"text": "This paper outlines some alternative architectures for POS tagging of Hebrew text, and studies them empirically.", "labels": [], "entities": [{"text": "POS tagging of Hebrew text", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.9089439988136292}]}, {"text": "This leads to some general conclusions about the optimal architecture for disambiguating Hebrew, and (reasonably) other Semitic languages as well.", "labels": [], "entities": []}, {"text": "The choice of tokenization level has major consequences for the implementation using HMMs, the sparseness of the statistics, the balance of the Markov condi-tioning, and the possible loss of information.", "labels": [], "entities": []}, {"text": "The paper reports on extensive experiments for comparing different architectures and studying the effects of this choice on the overall result.", "labels": [], "entities": []}, {"text": "Our best result is on par with the best reported POS tagging results for Arabic, despite the much smaller size of our annotated corpus.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.7371383607387543}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 defines the task of POS tagging in Hebrew, describes the existing corpora and discusses existing related work.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9196535050868988}]}, {"text": "Section 3 concentrates on defining the different levels of tokenization, specifies the details of the probabilistic framework that the tagger employs, and describes the techniques used for smoothing the probability estimates.", "labels": [], "entities": []}, {"text": "Section 4 compares the different levels of tokenization empirically, discusses their limitations, and proposes an improved model, which outperforms both of the initial models.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.9694077372550964}]}, {"text": "Finally, section 5 discusses the conclusions of our study for segmentation and POS tagging of Hebrew in particular, and Semitic languages in general.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.9715666174888611}, {"text": "POS tagging", "start_pos": 79, "end_pos": 90, "type": "TASK", "confidence": 0.8533145189285278}]}], "datasetContent": [{"text": "In this section we report on an empirical comparison between the two levels of tokenization presented in the previous section.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.9632450342178345}]}, {"text": "Analysis of the results leads to an improved morpheme-level model, which outperforms both of the initial models.", "labels": [], "entities": []}, {"text": "Each architectural configuration was evaluated in 5-fold cross-validated experiments.", "labels": [], "entities": []}, {"text": "Ina train/test split of the corpus, the training set includes 1,598 sentences on average, which on average amount to 28,738 words and 39,282 morphemes.", "labels": [], "entities": []}, {"text": "The test set includes 250 sentences.", "labels": [], "entities": []}, {"text": "We estimate segmentation accuracy -the percentage of words correctly segmented into morphemes, as well as tagging accuracy -the percentage of words that were correctly segmented for which each morpheme was assigned the correct POS tag.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8494508266448975}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.7379745244979858}]}, {"text": "For each parameter, the average over the five folds is reported, with the standard deviation in parentheses.", "labels": [], "entities": []}, {"text": "We used two-tailed paired t-test for testing the significance of the difference between the average results of different systems.", "labels": [], "entities": []}, {"text": "The significance level (p-value) is reported.", "labels": [], "entities": [{"text": "significance level", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.9789538383483887}]}, {"text": "The first two lines in: Level of tokenization -experimental results morpheme tagger is considerably better than what is achieved by the word tagger (difference of 0.79% with significance level p = 0.01).", "labels": [], "entities": [{"text": "morpheme tagger", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.6832506656646729}]}, {"text": "This is in spite of the fact that the segmentation achieved by the word tagger is a little better (and a segmentation error implies incorrect tagging).", "labels": [], "entities": []}, {"text": "Our hypothesis is that: Morpheme-level taggers outperform word-level taggers in their tagging accuracy, since they suffer less from data sparseness.", "labels": [], "entities": [{"text": "Morpheme-level taggers", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.6141122728586197}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9859116077423096}]}, {"text": "However, they lack some word-level knowledge that is required for segmentation.", "labels": [], "entities": []}, {"text": "This hypothesis is supported by the number of once-occurring terminals in each level: 8,582 in the word level, versus 5,129 in the morpheme level.", "labels": [], "entities": []}, {"text": "Motivated by this hypothesis, we next consider what kind of word-level information is required for the morpheme-level tagger in order to do better in segmentation.", "labels": [], "entities": []}, {"text": "One natural enhancement for the morpheme-level model involves adding information about word boundaries to the tag set.", "labels": [], "entities": []}, {"text": "In the enhanced tag set, nonterminal symbols include additional features that indicate whether the tagged morpheme starts/ends a word.", "labels": [], "entities": []}, {"text": "Unfortunately, we found that adding word boundary information in this way did not improve segmentation accuracy.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 90, "end_pos": 102, "type": "TASK", "confidence": 0.9648960828781128}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9066985249519348}]}, {"text": "However, error analysis revealed a very common type of segmentation errors, which was found to be considerably more frequent in morpheme tagging than in word tagging.", "labels": [], "entities": [{"text": "morpheme tagging", "start_pos": 128, "end_pos": 144, "type": "TASK", "confidence": 0.692048117518425}, {"text": "word tagging", "start_pos": 153, "end_pos": 165, "type": "TASK", "confidence": 0.7532608211040497}]}, {"text": "This kind of errors involves a missing or an extra covert definiteness marker 'h'.", "labels": [], "entities": []}, {"text": "For example, the word bbit can be segmented either as b-bit (\"in a house\") or as b-h-bit (\"in the house\"), pronounced bebayit and babayit, respectively.", "labels": [], "entities": []}, {"text": "Unlike other cases of segmentation ambiguity, which often just manifest lexical facts about spelling of Hebrew stems, this kind of ambiguity is productive: it occurs whenever the stem's POS allows definiteness, and is preceded by one of the prepositions b/k/l.", "labels": [], "entities": [{"text": "segmentation ambiguity", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.922215074300766}]}, {"text": "In morpheme tagging, this type of error was found on average in 1.71% of the words (46% of the segmentation errors).", "labels": [], "entities": [{"text": "morpheme tagging", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.7474078238010406}]}, {"text": "In word tagging, it was found only in 1.36% of the words (38% of the segmentation errors).", "labels": [], "entities": [{"text": "word tagging", "start_pos": 3, "end_pos": 15, "type": "TASK", "confidence": 0.7654553055763245}]}, {"text": "Since in Hebrew there should be agreement between the definiteness status of a noun and its related adjective, this kind of ambiguity can sometimes be resolved syntactically.", "labels": [], "entities": []}, {"text": "For instance: \"bbit hgdwl\" implies b-h-bit (\"in the big house\") \"bbit gdwl\" implies b-bit (\"in a big house\") By contrast, in many other cases both analyses are syntactically valid, and the choice between them requires consideration of a wider context, or some world knowledge.", "labels": [], "entities": []}, {"text": "For example, in the sentence hlknw lmsibh (\"we went to a/the party\"), lmsibh can be analyzed either as l-msibh (indefinite,\"to a party\") or as l-h-mbsibh (definite,\"to the party\").", "labels": [], "entities": []}, {"text": "Whether we prefer \"the party\" or \"a party\" depends on contextual information that is not available for the POS tagger.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 107, "end_pos": 117, "type": "DATASET", "confidence": 0.7912373244762421}]}, {"text": "Lexical statistics can provide valuable information in such situations, since some nouns are more common in their definite form, while other nouns are more common as indefinite.", "labels": [], "entities": []}, {"text": "For example, consider the word lmmflh (\"to a/the government\"), which can be segmented either as l-mmflh or l-h-mmflh.", "labels": [], "entities": []}, {"text": "The: Representation of l-h-mmflh in each level of tokenization stem mmflh (\"government\") was found 25 times in the corpus, out of which only two occurrences were indefinite.", "labels": [], "entities": [{"text": "tokenization stem mmflh", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.6655759215354919}]}, {"text": "This strong lexical evidence in favor of l-h-mmflh is completely missed by the morphemelevel tagger, in which morphemes are assumed to be independent.", "labels": [], "entities": []}, {"text": "The lexical model of the wordlevel tagger better models this difference, since it does take into account the frequencies of l-mmflh and l-h-mmlh, in measuring P(lmmflh|IN-NN) and P(lmmflh|IN-H-NN).", "labels": [], "entities": []}, {"text": "However, since the word tagger considers lmmflh, hmmflh (\"the government\"), and mmflh (\"a government\") as independent words, it still exploits only part of the potential lexical evidence about definiteness.", "labels": [], "entities": [{"text": "mmflh", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.8042812347412109}]}, {"text": "In order to better model such situations, we changed the morpheme-level model as follows.", "labels": [], "entities": []}, {"text": "In definite words the definiteness article h is treated as a manifestation of a morphological feature of the stem.", "labels": [], "entities": []}, {"text": "Hence the definiteness marker's POS tag (H) is prefixed to the POS tag of the stem.", "labels": [], "entities": [{"text": "POS tag (H)", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9223747253417969}]}, {"text": "We refer by M+h to the resulting model that uses this assumption, which is rather standard in theoretical linguistic studies of Hebrew.", "labels": [], "entities": []}, {"text": "The M+h model can be viewed as an intermediate level of tokenization, between morpheme and word tokenization.", "labels": [], "entities": []}, {"text": "The different analyses obtained by the three models of tokenization are demonstrated in.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 55, "end_pos": 67, "type": "TASK", "confidence": 0.9701941609382629}]}, {"text": "As shown in, the M+h model shows remarkable improvement in segmentation (0.49%, p < 0.001) compared with the initial morphemelevel model (M).", "labels": [], "entities": []}, {"text": "As expected, the frequency of segmentation errors that involve covert definiteness (h) dropped from 1.71% to 1.25%.", "labels": [], "entities": [{"text": "frequency", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9732747077941895}, {"text": "covert definiteness (h)", "start_pos": 63, "end_pos": 86, "type": "METRIC", "confidence": 0.6738974273204803}]}, {"text": "The adjusted morpheme tagger also outperforms the word level tagger in segmentation (0.31%, p = 0.069).", "labels": [], "entities": []}, {"text": "Tagging was improved as well (0.3%, p = 0.068).", "labels": [], "entities": [{"text": "Tagging", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.8779144883155823}]}, {"text": "According to these results, tokenization as in the M+h model is preferable to both plain-morpheme and plain-word tokenization.", "labels": [], "entities": []}], "tableCaptions": []}