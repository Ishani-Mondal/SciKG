{"title": [{"text": "Improving sequence segmentation learning by predicting trigrams", "labels": [], "entities": [{"text": "Improving sequence segmentation learning", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.9097640812397003}]}], "abstractContent": [{"text": "Symbolic machine-learning classifiers are known to suffer from near-sightedness when performing sequence segmentation (chunking) tasks in natural language processing: without special architectural additions they are oblivious of the decisions they made earlier when making new ones.", "labels": [], "entities": [{"text": "sequence segmentation (chunking) tasks", "start_pos": 96, "end_pos": 134, "type": "TASK", "confidence": 0.868291050195694}]}, {"text": "We introduce anew pointwise-prediction single-classifier method that predicts tri-grams of class labels on the basis of win-dowed input sequences, and uses a simple voting mechanism to decide on the labels in the final output sequence.", "labels": [], "entities": []}, {"text": "We apply the method to maximum-entropy, sparse-winnow, and memory-based classifiers using three different sentence-level chunk-ing tasks, and show that the method is able to boost generalization performance inmost experiments, attaining error reductions of up to 51%.", "labels": [], "entities": []}, {"text": "We compare and combine the method with two known alternative methods to combat near-sightedness, viz.", "labels": [], "entities": []}, {"text": "a feedback-loop method and a stacking method, using the memory-based clas-sifier.", "labels": [], "entities": [{"text": "stacking", "start_pos": 29, "end_pos": 37, "type": "TASK", "confidence": 0.9608753323554993}]}, {"text": "The combination with a feedback loop suffers from the label bias problem, while the combination with a stacking method produces the best overall results.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of generalization performances of three machine-learning algorithms in terms of F- score on the three test sets without and with class trigrams. Each third column displays the error reduction  in F-score by the class trigrams method over the other method. The best performances per task are printed  in bold.", "labels": [], "entities": [{"text": "F- score", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9909836848576864}, {"text": "F-score", "start_pos": 217, "end_pos": 224, "type": "METRIC", "confidence": 0.9358804225921631}]}, {"text": " Table 2: Comparison of generalization perfor- mances in terms of F-score of MBL on the three test  sets, with and without a feedback loop, and the error  reduction attained by the feedback-loop method, the  F-score of the trigram-class method, and the F-score  of the combination of the two methods.", "labels": [], "entities": [{"text": "F-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9933968782424927}, {"text": "error  reduction", "start_pos": 148, "end_pos": 164, "type": "METRIC", "confidence": 0.9409738481044769}, {"text": "F-score", "start_pos": 208, "end_pos": 215, "type": "METRIC", "confidence": 0.9831551313400269}, {"text": "F-score", "start_pos": 253, "end_pos": 260, "type": "METRIC", "confidence": 0.9946995973587036}]}, {"text": " Table 3: Comparison of generalization perfor- mances in terms of F-score of MBL on the three test  sets, without stacking, and with perfect and adaptive  stacking.", "labels": [], "entities": [{"text": "F-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9939881563186646}]}, {"text": " Table 4: Comparison of generalization perfor- mances in terms of F-score by MBL on the three test  sets, with adaptive stacking, trigram classes, and the  combination of the two.", "labels": [], "entities": [{"text": "F-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9829457998275757}, {"text": "MBL", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.7300174236297607}]}]}