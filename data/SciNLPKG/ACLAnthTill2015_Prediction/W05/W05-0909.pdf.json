{"title": [{"text": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7700740098953247}, {"text": "MT Evaluation", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9725468754768372}]}], "abstractContent": [{"text": "We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machine-produced translation and human-produced reference translations.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.7088779807090759}, {"text": "machine translation evaluation", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.8567684690157572}]}, {"text": "Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore , METEOR can be easily extended to include more advanced matching strategies.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.5707513093948364}]}, {"text": "Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference.", "labels": [], "entities": []}, {"text": "We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality.", "labels": [], "entities": []}, {"text": "We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets.", "labels": [], "entities": [{"text": "Pearson R correlation", "start_pos": 15, "end_pos": 36, "type": "METRIC", "confidence": 0.8711234529813131}, {"text": "LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets", "start_pos": 99, "end_pos": 163, "type": "DATASET", "confidence": 0.9279017703873771}]}, {"text": "We perform segment-by-segment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.7922340035438538}, {"text": "R correlation", "start_pos": 72, "end_pos": 85, "type": "METRIC", "confidence": 0.9673390090465546}, {"text": "Arabic data", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.7625433206558228}, {"text": "Chinese data", "start_pos": 137, "end_pos": 149, "type": "DATASET", "confidence": 0.8411677777767181}]}, {"text": "This is shown to bean improvement on using simply unigram-precision, unigram-recall and their harmonic F1 combination.", "labels": [], "entities": [{"text": "F1", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.821711003780365}]}, {"text": "We also perform experiments to show the relative contributions of the various mapping modules.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic Metrics for machine translation (MT) evaluation have been receiving significant attention in the past two years, since IBM's BLEU metric was proposed and made available.", "labels": [], "entities": [{"text": "machine translation (MT) evaluation", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.8822025159994761}, {"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9965985417366028}]}, {"text": "BLEU and the closely related NIST metric) have been extensively used for comparative evaluation of the various MT systems developed under the DARPA TIDES research program, as well as by other MT researchers.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9833674430847168}, {"text": "MT", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.9764190912246704}, {"text": "DARPA TIDES research program", "start_pos": 142, "end_pos": 170, "type": "DATASET", "confidence": 0.831995502114296}]}, {"text": "The utility and attractiveness of automatic metrics for MT evaluation has consequently been widely recognized by the MT community.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9714961349964142}, {"text": "MT", "start_pos": 117, "end_pos": 119, "type": "TASK", "confidence": 0.9857776761054993}]}, {"text": "Evaluating an MT system using such automatic metrics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators.", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9845166802406311}]}, {"text": "In addition to their utility for comparing the performance of different systems on a common translation task, automatic metrics can be applied on a frequent and ongoing basis during system development, in order to guide the development of the system based on concrete performance improvements.", "labels": [], "entities": []}, {"text": "Evaluation of Machine Translation has traditionally been performed by humans.", "labels": [], "entities": [{"text": "Evaluation of Machine Translation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6585216149687767}]}, {"text": "While the main criteria that should betaken into account in assessing the quality of MT output are fairly intuitive and well established, the overall task of MT evaluation is both complex and task dependent.", "labels": [], "entities": [{"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9911608099937439}, {"text": "MT evaluation", "start_pos": 158, "end_pos": 171, "type": "TASK", "confidence": 0.979823648929596}]}, {"text": "MT evaluation has consequently been an area of significant research in itself over the years.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9901707172393799}]}, {"text": "A wide range of assessment measures have been proposed, not all of which are easily quantifiable.", "labels": [], "entities": []}, {"text": "Recently developed frameworks, such as FEMTI (, are attempting to devise effective platforms for combining multi-faceted measures for MT evaluation in effective and user-adjustable ways.", "labels": [], "entities": [{"text": "FEMTI", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.5019559860229492}, {"text": "MT evaluation", "start_pos": 134, "end_pos": 147, "type": "TASK", "confidence": 0.9697326421737671}]}, {"text": "While a single one-dimensional numeric metric cannot hope to fully capture all aspects of MT evaluation, such metrics are still of great value and utility.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.974573403596878}]}, {"text": "In order to be both effective and useful, an automatic metric for MT evaluation has to satisfy several basic criteria.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.9657633006572723}]}, {"text": "The primary and most intuitive requirement is that the metric have very high correlation with quantified human notions of MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 122, "end_pos": 124, "type": "TASK", "confidence": 0.9845372438430786}]}, {"text": "Furthermore, a good metric should be as sensitive as possible to differences in MT quality between different systems, and between different versions of the same system.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9832921624183655}]}, {"text": "The metric should be consistent (same MT system on similar texts should produce similar scores), reliable (MT systems that score similarly can be trusted to perform similarly) and general (applicable to different MT tasks in a wide range of domains and scenarios).", "labels": [], "entities": [{"text": "MT tasks", "start_pos": 213, "end_pos": 221, "type": "TASK", "confidence": 0.9139735996723175}]}, {"text": "Needless to say, satisfying all of the above criteria is extremely difficult, and all of the metrics that have been proposed so far fall short of adequately addressing most if not all of these requirements.", "labels": [], "entities": []}, {"text": "Nevertheless, when appropriately quantified and converted into concrete test measures, such requirements can set an overall standard by which different MT evaluation metrics can be compared and evaluated.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 152, "end_pos": 165, "type": "TASK", "confidence": 0.8921690583229065}]}, {"text": "In this paper, we describe METEOR 1 , an automatic metric for MT evaluation which we have been developing.", "labels": [], "entities": [{"text": "METEOR 1", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9826489388942719}, {"text": "MT evaluation", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9690248668193817}]}, {"text": "METEOR was designed to explicitly address several observed weaknesses in IBM's BLEU metric.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.42532384395599365}, {"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9834745526313782}]}, {"text": "It is based on an explicit word-to-word matching between the MT output being evaluated and one or more reference translations.", "labels": [], "entities": []}, {"text": "Our current matching supports not only matching between words that are identical in the two strings being compared, but can also match words that are simple morphological variants of each other (i.e. they have an identical stem), and words that are synonyms of each other.", "labels": [], "entities": []}, {"text": "We envision ways in which this strict matching can be further expanded in the future, and describe these at the end of the paper.", "labels": [], "entities": []}, {"text": "Each possible matching is scored based on a combination of several features.", "labels": [], "entities": []}, {"text": "These currently include unigram-precision, unigram-recall, and a direct measure of how out-oforder the words of the MT output are with respect to the reference.", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9161385297775269}]}, {"text": "The score assigned to each individual sentence of MT output is derived from the best scoring match among all matches overall reference translations.", "labels": [], "entities": [{"text": "MT output", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.8889537155628204}]}, {"text": "The maximal-scoring match-1 METEOR: Metric for Evaluation of Translation with Explicit ORdering ing is then also used in order to calculate an aggregate score for the MT system over the entire test set.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.8752748370170593}, {"text": "Evaluation of Translation", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.5381933351357778}, {"text": "MT", "start_pos": 167, "end_pos": 169, "type": "TASK", "confidence": 0.9709512591362}]}, {"text": "Section 2 describes the metric in detail, and provides a full example of the matching and scoring.", "labels": [], "entities": []}, {"text": "In previous work), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.6950717568397522}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9912419319152832}]}, {"text": "We demonstrated that METEOR has significantly improved correlation with human judgments.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.7192173600196838}]}, {"text": "Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments.", "labels": [], "entities": [{"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9990299940109253}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9992825388908386}]}, {"text": "The previous analysis focused on correlation with human judgments at the system level.", "labels": [], "entities": []}, {"text": "In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.", "labels": [], "entities": [{"text": "METEOR score", "start_pos": 71, "end_pos": 83, "type": "METRIC", "confidence": 0.9599749445915222}]}, {"text": "High-levels of correlation at the segment level are important because they are likely to yield a metric that is sensitive to minor differences between systems and to minor differences between different versions of the same system.", "labels": [], "entities": []}, {"text": "Furthermore, current levels of correlation at the sentence level are still rather low, offering a very significant space for improvement.", "labels": [], "entities": [{"text": "correlation", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9612948894500732}]}, {"text": "The results reported in this paper demonstrate that all of the individual components included within METEOR contribute to improved correlation with human judgments.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.48256590962409973}]}, {"text": "In particular, METEOR is shown to have statistically significant better correlation compared to unigram-precision, unigramrecall and the harmonic F1 combination of the two.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9208524823188782}, {"text": "F1", "start_pos": 146, "end_pos": 148, "type": "METRIC", "confidence": 0.9013992547988892}]}, {"text": "We are currently in the process of exploring several further enhancements to the current METEOR metric, which we believe have the potential to significantly further improve the sensitivity of the metric and its level of correlation with human judgments.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.8381396532058716}]}, {"text": "Our work on these directions is described in further detail in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper, we are interested in evaluating METEOR as a metric that can evaluate translations on a sentence-by-sentence basis, rather than on a coarse grained system-by-system basis.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.542819082736969}]}, {"text": "The standard metrics -BLEU and NIST -were however designed for system level scoring, hence computing sentence level scores using BLEU or the NIST evaluation mechanism is unfair to those algorithms.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9861528873443604}, {"text": "NIST", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.9234982132911682}, {"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9174978137016296}]}, {"text": "To provide a point of comparison however, table 1 shows the system level correlation between human judgments and various MT evaluation algorithms and sub components of METEOR over the Chinese portion of the Tides 2003 dataset.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 121, "end_pos": 134, "type": "TASK", "confidence": 0.9019083082675934}, {"text": "METEOR", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.49535229802131653}, {"text": "Chinese portion of the Tides 2003 dataset", "start_pos": 184, "end_pos": 225, "type": "DATASET", "confidence": 0.699349684374673}]}, {"text": "Specifically, these correlation figures were obtained as follows: Using each algorithm we computed one score per Chinese system by calculating the aggregate scores produced by that algorithm for that system.", "labels": [], "entities": []}, {"text": "We also obtained the overall human judgment for each system by averaging all the human scores for that system's translations.", "labels": [], "entities": []}, {"text": "We then computed the Pearson correlation between these system level human judgments and the system level scores for each algorithm; these numbers are presented in Observe that simply using Recall as the MT evaluation metric results in a significant improvement in correlation with human judgment over both the BLEU and the NIST algorithms.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 21, "end_pos": 40, "type": "METRIC", "confidence": 0.9783699214458466}, {"text": "MT evaluation", "start_pos": 203, "end_pos": 216, "type": "TASK", "confidence": 0.8199452459812164}, {"text": "BLEU", "start_pos": 310, "end_pos": 314, "type": "METRIC", "confidence": 0.9659788608551025}, {"text": "NIST", "start_pos": 323, "end_pos": 327, "type": "DATASET", "confidence": 0.9225587248802185}]}, {"text": "These correlations further improve slightly when precision is taken into account (in the F1 measure), when the recall is weighed more heavily than precision (in the Fmean measure) and when a penalty is levied for fragmented matches (in the main METEOR measure).", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.999330997467041}, {"text": "F1 measure", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9684858024120331}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.996307373046875}, {"text": "precision", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.9988631010055542}, {"text": "Fmean measure", "start_pos": 165, "end_pos": 178, "type": "DATASET", "confidence": 0.9155888855457306}, {"text": "METEOR", "start_pos": 245, "end_pos": 251, "type": "METRIC", "confidence": 0.9162443280220032}]}, {"text": "As mentioned in the previous section, our main goal in this paper is to evaluate METEOR and its components on their translation-by-translation level correlation with human judgment.", "labels": [], "entities": []}, {"text": "Towards this end, in the rest of this paper, our evaluation methodology is as follows: For each system, we compute the METEOR Score for every translation produced by the system, and then compute the correlation between these individual scores and the human assessments (average of the adequacy and fluency scores) for the same translations.", "labels": [], "entities": [{"text": "METEOR Score", "start_pos": 119, "end_pos": 131, "type": "METRIC", "confidence": 0.9808615744113922}]}, {"text": "Thus we get a single Pearson R value for each system for which we have human assessments.", "labels": [], "entities": [{"text": "Pearson R value", "start_pos": 21, "end_pos": 36, "type": "METRIC", "confidence": 0.9325588941574097}]}, {"text": "Finally we average the R values of all the systems for each of the two language data sets to arrive at the overall average correlation for the Chinese dataset and the Arabic dataset.", "labels": [], "entities": [{"text": "Chinese dataset", "start_pos": 143, "end_pos": 158, "type": "DATASET", "confidence": 0.8258772194385529}, {"text": "Arabic dataset", "start_pos": 167, "end_pos": 181, "type": "DATASET", "confidence": 0.6973641216754913}]}, {"text": "This number ranges between -1.0 (completely negatively correlated) to +1.0 (completely positively correlated).", "labels": [], "entities": []}, {"text": "We compare the correlation between human assessments and METEOR Scores produced above with that between human assessments and precision, recall and Fmean scores to show the advantage of the various components in the METEOR scoring function.", "labels": [], "entities": [{"text": "METEOR Scores", "start_pos": 57, "end_pos": 70, "type": "METRIC", "confidence": 0.7911953032016754}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9995232820510864}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9979597330093384}, {"text": "Fmean", "start_pos": 148, "end_pos": 153, "type": "METRIC", "confidence": 0.9951403141021729}]}, {"text": "Finally we run METEOR using different mapping modules, and compute the correlation as described above for each configuration to show the effect of each unigram mapping mechanism.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.6275290846824646}]}, {"text": "We computed sentence by sentence correlation between METEOR Scores and human assessments (average of adequacy and fluency scores) for each translation for every system.", "labels": [], "entities": [{"text": "METEOR Scores", "start_pos": 53, "end_pos": 66, "type": "METRIC", "confidence": 0.9485227465629578}]}, {"text": "show the Pearson R correlation values for each system, as well as the average correlation value per language dataset.", "labels": [], "entities": [{"text": "Pearson R correlation", "start_pos": 9, "end_pos": 30, "type": "METRIC", "confidence": 0.7228827675183614}, {"text": "correlation", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.8684691786766052}]}], "tableCaptions": [{"text": " Table 1: Comparison of human/METEOR correlation  with BLEU and NIST/human correlations", "labels": [], "entities": [{"text": "METEOR correlation", "start_pos": 30, "end_pos": 48, "type": "METRIC", "confidence": 0.9210967719554901}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9965923428535461}]}, {"text": " Table 2: Correlation between METEOR Scores and  Human Assessments for the Arabic Dataset", "labels": [], "entities": [{"text": "METEOR Scores", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.9607326686382294}, {"text": "Arabic Dataset", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.9094204902648926}]}, {"text": " Table 4: Correlations between human assessments and  precision, recall, Fmean and METEOR Scores, aver- aged over systems in the Arabic dataset", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9994862079620361}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9991481304168701}, {"text": "Fmean", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.868894636631012}, {"text": "METEOR Scores", "start_pos": 83, "end_pos": 96, "type": "METRIC", "confidence": 0.968539834022522}]}, {"text": " Table 5: Correlations between human assessments and  precision, recall, Fmean and METEOR Scores, aver- aged over systems in the Chinese dataset", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9994066953659058}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9989975094795227}, {"text": "Fmean", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.8597514629364014}, {"text": "METEOR Scores", "start_pos": 83, "end_pos": 96, "type": "METRIC", "confidence": 0.9710753858089447}, {"text": "Chinese dataset", "start_pos": 129, "end_pos": 144, "type": "DATASET", "confidence": 0.8546434044837952}]}, {"text": " Table 6: Comparing correlations produced by different  module stages on the Arabic dataset.", "labels": [], "entities": [{"text": "Arabic dataset", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.7708487808704376}]}, {"text": " Table 7: Comparing correlations produced by different  module stages, on the Chinese dataset", "labels": [], "entities": [{"text": "Chinese dataset", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.9254628717899323}]}, {"text": " Table 8: Comparing correlations between METEOR  Scores and both raw and normalized human assessments", "labels": [], "entities": [{"text": "METEOR  Scores", "start_pos": 41, "end_pos": 55, "type": "METRIC", "confidence": 0.917888730764389}]}]}