{"title": [{"text": "TextTree Construction for Parser and Treebank Development", "labels": [], "entities": []}], "abstractContent": [{"text": "TextTrees, introduced in (Newman, 2005), are skeletal representations formed by systematically converting parser output trees into unlabeled indented strings with minimal bracketing.", "labels": [], "entities": []}, {"text": "Files of TextTrees can be read rapidly to evaluate the results of parsing long documents, and are easily edited to allow limited-cost treebank development.", "labels": [], "entities": [{"text": "parsing long documents", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.8941139181454977}]}, {"text": "This paper reviews the TextTree concept, and then describes the implementation of the almost parser-and grammar-independent TextTree generator, as well as auxiliary methods for producing parser review files and inputs to bracket scoring tools.", "labels": [], "entities": []}, {"text": "The results of some limited experiments in TextTree usage are also provided.", "labels": [], "entities": []}], "introductionContent": [{"text": "The TextTree representation was developed to support a limited-resource effort to build anew hybrid English parser . When the parser reached significant apparent coverage, in terms of numbers of sentences receiving some parse, the need arose to quickly assess the quality of the parses produced, for purposes of detecting coverage gaps, refining analyses, and measurement.", "labels": [], "entities": []}, {"text": "But this was hampered by the use of a detailed parser output representation.", "labels": [], "entities": []}, {"text": "The two most common parser-output displays of constituent structure are: (a) multi-line labeled and bracketed strings, with indentation indicating dominance, and (b) 2-dimensional trees.", "labels": [], "entities": []}, {"text": "While these displays are indispensable in grammar development, they cannot be scanned quickly.", "labels": [], "entities": [{"text": "grammar development", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8848724067211151}]}, {"text": "Labels and brackets interfere with reading.", "labels": [], "entities": []}, {"text": "And, The hybrid combines the chunker part of the fast, robust XIP parser) with an ATN-style parser operating primarily on the chunks.", "labels": [], "entities": []}, {"text": "although relatively flat 2D node + edge trees for short sentences can be grasped at a glance, for long sentences this property must be compromised.", "labels": [], "entities": []}, {"text": "In contrast, for languages with a relatively fixed word order, and a tendency to postmodification, TextTrees capture the dependencies found by a parser in a natural, almost self-explanatory way.", "labels": [], "entities": []}, {"text": "For example: Indented elements are usually right-hand postmodifiers or subordinates of the lexical head of the nearest preceding less-indented line.", "labels": [], "entities": []}, {"text": "Brackets are generally used only to delimit coordinations (by), nested clauses (by {\u2026}), and identified multi-words (by |\u2026|).", "labels": [], "entities": [{"text": "Brackets", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9737391471862793}]}, {"text": "Reading a TextTree fora correct parse is similar to reading ordinary text, but reading a TextTree for an incorrect parse is jarring.", "labels": [], "entities": []}, {"text": "For example, the following TextTree fora 33-word sentence exposes several errors made by the hybrid parser: TextTrees can be embedded in bulk parser output files with arbitrary surrounding information.", "labels": [], "entities": []}, {"text": "shows an excerpt from such a file, containing the TextTree-form results of parsing the roughly 500-sentence \"Executive Summary\" of the 9/11 Commission Report by the hybrid parser, with more detailed results for each sentence accessible via links.", "labels": [], "entities": [{"text": "parsing the roughly 500-sentence \"Executive Summary\" of the 9/11 Commission Report", "start_pos": 75, "end_pos": 157, "type": "TASK", "confidence": 0.7163431147734324}]}, {"text": "(Note: the links in are greyed to indicate that they are not operational in the illustration.)", "labels": [], "entities": []}, {"text": "Such files can also be edited efficiently to produce limited-function treebanks, because the needed modifications are easy to identify, labels are unnecessary, and little attention to bracketing is required.", "labels": [], "entities": []}, {"text": "Edited and unedited TextTree files can then be mapped into files containing fully bracketed strings (although bracketed differently than the original parse trees), and compared by bracket scoring methods derived from Black et al.", "labels": [], "entities": []}, {"text": "Section 2 below examines the problems presented by detailed parse trees for late-stage parser development activities in more detail.", "labels": [], "entities": []}, {"text": "Section 3 describes the inputs to and outputs from the TextTree generator, and Section 4 the generator implementation.", "labels": [], "entities": [{"text": "TextTree generator", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.9010293781757355}]}, {"text": "Section 5 discusses the use of the TextTree generator in producing TextTree files for parser output review and TextTreebank construction, and the use of TextTreebanks in parser measurement.", "labels": [], "entities": [{"text": "parser output review", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.8531351288159689}, {"text": "TextTreebank construction", "start_pos": 111, "end_pos": 136, "type": "TASK", "confidence": 0.7551156878471375}, {"text": "parser measurement", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.9060407876968384}]}, {"text": "The results of some limited experiments in TextTree file use are provided in Section 6.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.9135984778404236}]}, {"text": "Section 7 discusses related work and Section 8 explores some directions for further exploitation of TextTrees.", "labels": [], "entities": []}, {"text": "6 (3) We have come together with a unity of purpose because our nation demands it. best more chunks We have come together with a unity of purpose because {our nation demands it}.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes two limited experiments to assess the efficiency of reviewing parser outputs for accuracy using TextTrees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9942875504493713}]}, {"text": "One of the experiments also measures the efficiency of TextTreebank creation  The document used in the first experiment was the roughly 500-sentence \"Executive Summary\" of the 9/11 Commission Report.", "labels": [], "entities": [{"text": "TextTreebank creation", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.6499446928501129}, {"text": "Executive Summary\" of the 9/11 Commission Report", "start_pos": 150, "end_pos": 198, "type": "TASK", "confidence": 0.7861102402210236}]}, {"text": "After parsing by the hybrid parser, the expected TextTree file, excerpted in, was created, reproducing each sentence and, for parsed sentences, the TextTree string for the best parse obtained, and a links to the detailed two-dimensional tree representation.", "labels": [], "entities": [{"text": "TextTree file", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8861297070980072}]}, {"text": "Of the 503 sentences, averaging 20 words in length, 93% received a parse.", "labels": [], "entities": []}, {"text": "However, reviewing the TextTree file revealed that at least 191 of the 470 parsed sentences were not parsed correctly, indicating an actual parser accuracy for the document of at most 55%.", "labels": [], "entities": [{"text": "TextTree file", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.9500418901443481}, {"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.958591103553772}]}, {"text": "Reviewing the TextTrees required 92 minutes, giving a review rate of 6170 words per hour, including checking detailed parses for sentences where errors might have lain in the TextTree formatting.", "labels": [], "entities": [{"text": "TextTrees", "start_pos": 14, "end_pos": 23, "type": "DATASET", "confidence": 0.9237409830093384}]}, {"text": "That review rate can be compared to the results of for post-annotation proofreading of relatively flat, indented, but fully labelled and bracketed trees.", "labels": [], "entities": []}, {"text": "Those results indicated that: \"...", "labels": [], "entities": []}, {"text": "experienced annotators can proofread previously corrected material at very high speeds.", "labels": [], "entities": []}, {"text": "A parsed subcorpus \u2026was recently proofread at an average speed of approximately 4,000 words per annotator per hour.", "labels": [], "entities": []}, {"text": "At this rate\u2026, annotators are able to find and correct gross errors in parsing, but do not have time to check, for example, whether they agree with all prepositional phrase attachments.\"", "labels": [], "entities": []}, {"text": "While the two tasks are not exactly comparable, if we assume that little or no editing was required in proofreading, the ballpark improvement of 50% is encouraging.", "labels": [], "entities": []}, {"text": "For the second experiment, we used the CB01 file 3 of the Brown Corpus (, and reviewed both the TextTree file and then, separately, the detailed 2D parse trees also produced.", "labels": [], "entities": [{"text": "CB01 file 3 of the Brown Corpus", "start_pos": 39, "end_pos": 70, "type": "DATASET", "confidence": 0.853962915284293}, {"text": "TextTree file", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.950459361076355}]}, {"text": "While the parser reported that 91 of the 103 sentences, or 88%, received a parse, the review of the TextTree file determined that at most only 50 sentences, or 48.5%, received a fully correct parse.", "labels": [], "entities": [{"text": "TextTree file", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.9497613310813904}]}, {"text": "The review of the detailed parse trees revealed three additional errors.", "labels": [], "entities": []}, {"text": "The comparison of review times was less decisive in this experiment, with the rate for the TextTree review being 5733 words per hour, and that for the detailed 2D representation 4410 words per hour.", "labels": [], "entities": [{"text": "TextTree review", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.9130108952522278}]}, {"text": "However, there were non-quantifiable differences in the reviews.", "labels": [], "entities": []}, {"text": "One difference was that the TextTree review was a fairly relaxed exercise, while the review of the 2D representations was done with a conscious attempt at speed, and was quite stressful-not something one would like to repeat.", "labels": [], "entities": [{"text": "TextTree review", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.8725342452526093}, {"text": "speed", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.9699265956878662}]}, {"text": "Another difference was that scanning the TextTree file provided afar better cumulative sense of the kinds of problems presented by the document/genre, which might be further exploited by a more interactive format (e.g., using HTML forms) allowing users to classify erroneous parses by error type.", "labels": [], "entities": [{"text": "TextTree file", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.8858385980129242}]}, {"text": "The experiment was then extended to check the extent to which TextTree files could efficiently edited for purposes of limited-function treebank creation.", "labels": [], "entities": []}, {"text": "For this purpose, to minimize typing when a sentence had no complete parse, the TextTree file included the list of chunks identified by the XIP parser.", "labels": [], "entities": [{"text": "TextTree file", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.8763531744480133}]}, {"text": "A similar strategy could be used with parsers that, when no complete parse is found, return an unrelated sequence of adjacent constituent parses.", "labels": [], "entities": []}, {"text": "This is done by some statistical and finite-state-based parsers, as well as by parsers employing the \"fitted parse\" method of or the \"fragment parse\" method described by.", "labels": [], "entities": []}, {"text": "With part-of-speech tags removed Editing the TextTrees for the 104 sentences, with an average sentence length of 21 words, required 83 minutes, giving a rate of 1518 words per hour.", "labels": [], "entities": []}, {"text": "This might be compared with the average of 750 words per hour for the initial annotation of parses in the Penn Treebank experiment) mentioned earlier.", "labels": [], "entities": [{"text": "Penn Treebank experiment", "start_pos": 106, "end_pos": 130, "type": "DATASET", "confidence": 0.9857624173164368}]}, {"text": "After the TextTreebank was created, the bracketing script described in section 5.1 was applied both to the original TextTree file and to the TextTreebank, and the results were submitted to the EVALB program, which reported a bracketing recall of 71%, a bracketing precision of 84%, and an average crossing bracket count of 1.15.", "labels": [], "entities": [{"text": "TextTree file", "start_pos": 116, "end_pos": 129, "type": "DATASET", "confidence": 0.9306113719940186}, {"text": "TextTreebank", "start_pos": 141, "end_pos": 153, "type": "DATASET", "confidence": 0.9506995677947998}, {"text": "bracketing", "start_pos": 225, "end_pos": 235, "type": "TASK", "confidence": 0.949597954750061}, {"text": "recall", "start_pos": 236, "end_pos": 242, "type": "METRIC", "confidence": 0.6268495917320251}, {"text": "precision", "start_pos": 264, "end_pos": 273, "type": "METRIC", "confidence": 0.9373114109039307}, {"text": "crossing bracket count", "start_pos": 297, "end_pos": 319, "type": "METRIC", "confidence": 0.7075843612353007}]}, {"text": "Two sentences were not processed because of token mismatches.", "labels": [], "entities": []}, {"text": "As expected, these scores were much higher than the percentage of sentences correctly parsed.", "labels": [], "entities": []}], "tableCaptions": []}