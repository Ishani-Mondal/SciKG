{"title": [{"text": "Studying Feature Generation from Various Data Representations for Answer Extraction", "labels": [], "entities": [{"text": "Feature Generation", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7290509343147278}, {"text": "Answer Extraction", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.9205573201179504}]}], "abstractContent": [{"text": "In this paper, we study how to generate features from various data representations, such as surface texts and parse trees, for answer extraction.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.8978753685951233}]}, {"text": "Besides the features generated from the surface texts, we mainly discuss the feature generation in the parse trees.", "labels": [], "entities": []}, {"text": "We propose and compare three methods, including feature vector, string kernel and tree kernel, to represent the syntactic features in Support Vector Machines.", "labels": [], "entities": []}, {"text": "The experiment on the TREC question answering task shows that the features generated from the more struc-tured data representations significantly improve the performance based on the features generated from the surface texts.", "labels": [], "entities": [{"text": "TREC question answering task", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.8116127103567123}]}, {"text": "Furthermore, the contribution of the individual feature will be discussed in detail.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open domain question answering (QA), as defined by the TREC competitions, represents an advanced application of natural language processing (NLP).", "labels": [], "entities": [{"text": "Open domain question answering (QA)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8122076903070722}]}, {"text": "It aims to find exact answers to open-domain natural language questions in a large document collection.", "labels": [], "entities": []}, {"text": "For example: Q2131: Who is the mayor of San Francisco?", "labels": [], "entities": []}, {"text": "Answer: Willie Brown A typical QA system usually consists of three basic modules: 1.", "labels": [], "entities": [{"text": "Willie Brown", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.7603956758975983}, {"text": "QA", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9523380994796753}]}, {"text": "Question Processing (QP) Module, which finds some useful information from the questions, such as expected answer type and key words.", "labels": [], "entities": [{"text": "Question Processing (QP) Module", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.710816686352094}]}, {"text": "2. Information Retrieval (IR) Module, which searches a document collection to retrieve a set of relevant sentences using the question key words.", "labels": [], "entities": [{"text": "Information Retrieval (IR) Module", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.8415469427903494}]}, {"text": "3. Answer Extraction (AE) Module, which analyzes the relevant sentences using the information provided by the QP module and identify the answer phrase.", "labels": [], "entities": [{"text": "Answer Extraction (AE) Module", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.7437870055437088}]}, {"text": "In recent years, QA systems trend to be more and more complex, since many other NLP techniques, such as named entity recognition, parsing, semantic analysis, reasoning, and external resources, such as WordNet, web, databases, are incorporated.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 104, "end_pos": 128, "type": "TASK", "confidence": 0.632013738155365}]}, {"text": "The various techniques and resources may provide the indicative evidences to find the correct answers.", "labels": [], "entities": []}, {"text": "These evidences are further combined by using a pipeline structure, a scoring function or a machine learning method.", "labels": [], "entities": []}, {"text": "In the machine learning framework, it is critical but not trivial to generate the features from the various resources which maybe represented as surface texts, syntactic structures and logic forms, etc.", "labels": [], "entities": []}, {"text": "The complexity of feature generation strongly depends on the complexity of data representation.", "labels": [], "entities": [{"text": "feature generation", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.741890162229538}]}, {"text": "Many previous QA systems () have well studied the features in the surface texts.", "labels": [], "entities": []}, {"text": "In this paper, we will use the answer extraction module of QA as a case study to further explore how to generate the features for the more complex sentence representations, such as parse tree.", "labels": [], "entities": [{"text": "answer extraction module", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.7742660542329153}]}, {"text": "Since parsing gives the deeper understanding of the sentence, the features generated from the parse tree are expected to improve the performance based on the features generated from the surface text.", "labels": [], "entities": []}, {"text": "The answer ex-traction module is built using Support Vector Machines (SVM).", "labels": [], "entities": []}, {"text": "We propose three methods to represent the features in the parse tree: 1.", "labels": [], "entities": []}, {"text": "features are designed by domain experts, extracted from the parse tree and represented as a feature vector; 2.", "labels": [], "entities": []}, {"text": "the parse tree is transformed to anode sequence and a string kernel is employed; 3.", "labels": [], "entities": []}, {"text": "the parse tree is retained as the original representation and a tree kernel is employed.", "labels": [], "entities": []}, {"text": "Although many textual features have been used in the others' AE modules, it is not clear that how much contribution the individual feature makes.", "labels": [], "entities": []}, {"text": "In this paper, we will discuss the effectiveness of each individual textual feature in detail.", "labels": [], "entities": []}, {"text": "We further evaluate the effectiveness of the syntactic features we proposed.", "labels": [], "entities": []}, {"text": "Our experiments using TREC questions show that the syntactic features improve the performance by 7.57 MRR based on the textual features.", "labels": [], "entities": [{"text": "MRR", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9988307356834412}]}, {"text": "It indicates that the new features based on a deeper language understanding are necessary to push further the machine learning-based QA technology.", "labels": [], "entities": []}, {"text": "Furthermore, the three representations of the syntactic features are compared.", "labels": [], "entities": []}, {"text": "We find that keeping the original data representation by using the data-specific kernel function in SVM may capture the more comprehensive evidences than the predefined features.", "labels": [], "entities": []}, {"text": "Although the features we generated are specific to the answer extraction task, the comparison between the different feature representations maybe helpful to explore the syntactic features for the other NLP applications.", "labels": [], "entities": [{"text": "answer extraction task", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.9067140618960062}]}], "datasetContent": [{"text": "We apply the AE module to the TREC QA task.", "labels": [], "entities": [{"text": "AE", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9863856434822083}, {"text": "TREC QA task", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.5866051912307739}]}, {"text": "To evaluate the features in the AE module independently, we suppose that the IR module has got 100% precision and only passes those sentences containing the proper answers to the AE module.", "labels": [], "entities": [{"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9993075132369995}]}, {"text": "The AE module is to identify the proper answers from the given sentence collection.", "labels": [], "entities": [{"text": "AE", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9742257595062256}]}, {"text": "We use the questions of TREC8, 9, 2001 and 2002 for training and the questions of TREC2003 for testing.", "labels": [], "entities": [{"text": "TREC8", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.882953941822052}, {"text": "TREC2003", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.9179949760437012}]}, {"text": "The following steps are used to generate the data: 1.", "labels": [], "entities": []}, {"text": "Retrieve the relevant documents for each question based on the TREC judgments.", "labels": [], "entities": [{"text": "TREC", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.5879105925559998}]}, {"text": "2. Extract the sentences, which match both the proper answer and at least one question keyword, from these documents.", "labels": [], "entities": []}, {"text": "3. Tag the proper answer in the sentences based on the TREC answer patterns: Two objects representing the relations between answer candidates and target words.", "labels": [], "entities": []}, {"text": "In TREC 2003, there are 413 factoid questions in which 51 questions (NIL questions) are not returned with the proper answers by TREC.", "labels": [], "entities": [{"text": "TREC 2003", "start_pos": 3, "end_pos": 12, "type": "DATASET", "confidence": 0.8609732985496521}, {"text": "TREC", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.925368070602417}]}, {"text": "According to our data generation process, we cannot provide data for those NIL questions because we cannot get the sentence collections.", "labels": [], "entities": []}, {"text": "Therefore, the AE module will fail on all of the NIL questions and the number of the valid questions should be 362.", "labels": [], "entities": [{"text": "AE", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9394568800926208}, {"text": "NIL questions", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.7790459394454956}]}, {"text": "In the experiment, we still test the module on the whole question set (413 questions) to keep consistent with the other's work.", "labels": [], "entities": []}, {"text": "The training set contains 1252 questions.", "labels": [], "entities": []}, {"text": "The performance of our system is evaluated using the mean reciprocal rank (MRR).", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 53, "end_pos": 79, "type": "METRIC", "confidence": 0.8267184595266978}]}, {"text": "Furthermore, we also list the percentages of the correct answers respectively in terms of the top 5 answers and the top 1 answer returned.", "labels": [], "entities": []}, {"text": "We employ the SVM Light) to incorporate the features and classify the answer candidates.", "labels": [], "entities": []}, {"text": "No post-processes are used to adjust the answers in the experiments.", "labels": [], "entities": []}, {"text": "Firstly, we evaluate the effectiveness of the textual features, described in Section 5.", "labels": [], "entities": []}, {"text": "We incorporate them into SVM using the three kernel functions: linear kernel, polynomial kernel and RBF kernel, which are introduced in Section 4.: Performance for kernels In order to evaluate the contribution of the individual feature, we test out module using different feature combinations, as shown in.", "labels": [], "entities": [{"text": "RBF kernel", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9602631628513336}]}, {"text": "Several findings are concluded: 1.", "labels": [], "entities": []}, {"text": "With only the syntactic tag features F syn.", "labels": [], "entities": []}, {"text": ", the module achieves a basic level MRR of 31.38.", "labels": [], "entities": [{"text": "MRR", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.8469448089599609}]}, {"text": "The questions \"Q1903: How many time zones are therein the world?\" is correctly answered from the sentence \"The world is divided into 24 time zones.\".", "labels": [], "entities": []}, {"text": "2. The orthographic features F orth.", "labels": [], "entities": []}, {"text": "show the positive effect with 7.12 MRR improvement based on F syn.", "labels": [], "entities": [{"text": "MRR", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9946545362472534}, {"text": "F", "start_pos": 60, "end_pos": 61, "type": "METRIC", "confidence": 0.9897648096084595}]}, {"text": "They help to find the proper answer \"Grover Cleveland\" for the question \"Q2049: What president served 2 nonconsecutive terms?\" from the sentence \"Grover Cleveland is the forgotten twoterm American president.\", while F syn.", "labels": [], "entities": []}, {"text": "wrongly identify \"president\" as the answer.", "labels": [], "entities": []}, {"text": "3. The named entity features F ne are also beneficial as they make the 4.46 MRR increase based on F syn.", "labels": [], "entities": [{"text": "MRR increase", "start_pos": 76, "end_pos": 88, "type": "METRIC", "confidence": 0.9681394696235657}]}, {"text": "For the question \"Q2076: What company owns the soft drink brand \"Gatorade\"?\", F ne find the proper answer \"Quaker Oats\" in the sentence \"Marineau , 53 , had distinguished himself by turning the sports drink Gatorade into amass consumer brand while an executive at Quaker Oats During his 18-month\u2026\", while F syn.", "labels": [], "entities": [{"text": "Quaker Oats", "start_pos": 107, "end_pos": 118, "type": "DATASET", "confidence": 0.947431743144989}]}, {"text": "return the wrong answer \"Marineau\".: Performance for syntactic feature representations shows the performances of FeatureVector, StringKernel and TreeKernel.", "labels": [], "entities": []}, {"text": "All of them improve the performance based on the textual features (F syn.", "labels": [], "entities": []}, {"text": "+F ne +F trg ) by 3.04 MRR, 6.05 MRR and 7.57 MRR respectively.", "labels": [], "entities": [{"text": "MRR", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9897242784500122}, {"text": "MRR", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9647759199142456}, {"text": "MRR", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9447234869003296}]}, {"text": "The probable reason maybe that the features generated from the structured data representation may capture the more linguistic-motivated evidences for the proper answers.", "labels": [], "entities": []}, {"text": "For example, the syntactic features help to find the answer \"nitrogen\" for the question \"Q2139: What gas is 78 percent of the earth 's atmosphere?\" in the sentence \"One thing they haven't found in the moon's atmosphere so far is nitrogen, the gas that makes up more than threequarters of the Earth's atmosphere.\", while the textual features fail on it.", "labels": [], "entities": []}, {"text": "Furthermore, the StringKernel (+3.01MRR) and TreeKernel (+4.53MRR) achieve the higher performance than FeatureVector, which maybe explained that keeping the original data representations by incorporating the data-specific kernels in SVM may capture the more comprehensive evidences than the predefined features.", "labels": [], "entities": []}, {"text": "Moreover, TreeKernel slightly outperforms StringKernel by 1.52 MRR.", "labels": [], "entities": [{"text": "MRR", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9903505444526672}]}, {"text": "The reason maybe that when we transform the representation of the syntactic relation from the tree to the node sequence, some information maybe lost, such as the sibling node of the answer candidates.", "labels": [], "entities": []}, {"text": "Sometimes the information is useful to find the proper answers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. The core idea of the tree kernel ( , )", "labels": [], "entities": []}, {"text": " Table 3: Performance for kernels", "labels": [], "entities": [{"text": "kernels", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.7558827996253967}]}, {"text": " Table 4: Performance for feature combinations", "labels": [], "entities": []}, {"text": " Table 5: Performance for syntactic feature repre- sentations", "labels": [], "entities": [{"text": "syntactic feature repre- sentations", "start_pos": 26, "end_pos": 61, "type": "TASK", "confidence": 0.7085363745689393}]}]}