{"title": [], "abstractContent": [{"text": "This work explores computing distribu-tional similarity between sub-parses, i.e., fragments of a parse tree, as an extension to general lexical distributional similarity techniques.", "labels": [], "entities": []}, {"text": "In the same way that lexical distributional similarity is used to estimate lexical semantic similarity, we propose using distributional similarity between sub-parses to estimate the semantic similarity of phrases.", "labels": [], "entities": []}, {"text": "Such a technique will allow us to identify paraphrases where the component words are not semantically similar.", "labels": [], "entities": []}, {"text": "We demonstrate the potential of the method by applying it to a small number of examples and showing that the paraphrases are more similar than the non-paraphrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "An expression is said to textually entail another expression if the meaning of the second expression can be inferred from the meaning of the first.", "labels": [], "entities": []}, {"text": "For example, the sentence \"London is an English city,\" textually entails the sentence \"London is in England.\"", "labels": [], "entities": []}, {"text": "As discussed by in their introduction to the first Recognising Textual Entailment Challenge, identifying textual entailment can be seen as a subtask of a variety of other natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "Recognising Textual Entailment Challenge", "start_pos": 51, "end_pos": 91, "type": "TASK", "confidence": 0.8080759793519974}, {"text": "identifying textual entailment", "start_pos": 93, "end_pos": 123, "type": "TASK", "confidence": 0.759201725323995}]}, {"text": "For example, Question Answering (QA) can be cast as finding an answer which is entailed by the proposition in the question.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.8998726606369019}]}, {"text": "Other identified tasks include summarization, paraphrasing, Information Extraction (IE), Information Retrieval (IR) and Machine Translation (MT).", "labels": [], "entities": [{"text": "summarization", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9883716702461243}, {"text": "Information Extraction (IE)", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.7877291798591614}, {"text": "Information Retrieval (IR)", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.8187974095344543}, {"text": "Machine Translation (MT)", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.8388495564460754}]}, {"text": "The Natural Habitats (NatHab) project) provides an interesting setting in which to study paraphrase and tex-1 http://www.informatics.susx.ac.uk/projects/nathab/ tual entailment recognition as a tool for natural language understanding.", "labels": [], "entities": [{"text": "Natural Habitats (NatHab) project", "start_pos": 4, "end_pos": 37, "type": "DATASET", "confidence": 0.7204181551933289}, {"text": "tual entailment recognition", "start_pos": 161, "end_pos": 188, "type": "TASK", "confidence": 0.7319548726081848}, {"text": "natural language understanding", "start_pos": 203, "end_pos": 233, "type": "TASK", "confidence": 0.6571754018465678}]}, {"text": "The aim of the project is to enable non-technical users to configure their pervasive computing environments.", "labels": [], "entities": []}, {"text": "They do this by stating policies in natural language which describe how they wish their environment to behave.", "labels": [], "entities": []}, {"text": "For example, a user, who wishes to restrict the use of their colour printer to the printing of colour documents, might have as a policy, \"Never print black-and-white documents on my colour printer.\"", "labels": [], "entities": []}, {"text": "Similarly, a user, who wishes to be alerted by email when their mobile phone battery is low, might have as a policy, \"If my mobile phone battery is low then send mean email.\"", "labels": [], "entities": []}, {"text": "The natural language understanding task is to interpret the user's utterance with reference to a set of policy templates and an ontology of services (e.g. print) and concepts (e.g. document).", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.6705730855464935}]}, {"text": "The use of policy templates and an ontology restricts the number of possible meanings that a user can express.", "labels": [], "entities": []}, {"text": "However, there is still considerable variability in the way these policies can be expressed.", "labels": [], "entities": []}, {"text": "Simple variations on the theme of the second policy above include, \"Send mean email whenever my mobile phone battery is low,\" and \"If the charge on my mobile phone is low then email me.\"", "labels": [], "entities": []}, {"text": "Our approach is to tackle the interpretation problem by identifying parts of expressions that are paraphrases of those expressions whose interpretation with respect to the ontology is more directly encoded.", "labels": [], "entities": []}, {"text": "Here, we investigate extending distributional similarity methods from words to sub-parses.", "labels": [], "entities": []}, {"text": "The rest of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we discuss the background to our work.", "labels": [], "entities": []}, {"text": "We consider the limitations of an approach based on lexical similarity and syntactic templates, which motivates us to look directly at the similarity of larger units.", "labels": [], "entities": []}, {"text": "In Section 3, we introduce our proposed approach, which is to measure the distributional similarity of sub-parses.", "labels": [], "entities": []}, {"text": "In Section 4, we consider examples from the Pascal Textual Entailment Challenge Datasets 2 () and demonstrate empirically how similarity can be found between corresponding phrases when parts of the phrases cannot be said to be similar.", "labels": [], "entities": [{"text": "Pascal Textual Entailment Challenge Datasets 2", "start_pos": 44, "end_pos": 90, "type": "DATASET", "confidence": 0.6235504349072775}]}, {"text": "In Section 5, we present our conclusions and directions for further work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of feature types and tokens ex- tracted for each Phrase", "labels": [], "entities": [{"text": "Phrase", "start_pos": 66, "end_pos": 72, "type": "TASK", "confidence": 0.47624674439430237}]}]}