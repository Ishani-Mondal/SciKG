{"title": [{"text": "Recognizing Paraphrases and Textual Entailment using Inversion Transduction Grammars", "labels": [], "entities": [{"text": "Recognizing Paraphrases and Textual Entailment", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.753643149137497}]}], "abstractContent": [{"text": "We present first results using paraphrase as well as textual entailment data to test the language universal constraint posited by Wu's (1995, 1997) Inversion Transduction Grammar (ITG) hypothesis.", "labels": [], "entities": [{"text": "Inversion Transduction Grammar (ITG)", "start_pos": 148, "end_pos": 184, "type": "TASK", "confidence": 0.7360204507907232}]}, {"text": "In machine translation and alignment, the ITG Hypothesis provides a strong inductive bias, and has been shown empirically across numerous language pairs and corpora to yield both efficiency and accuracy gains for various language acquisition tasks.", "labels": [], "entities": [{"text": "machine translation and alignment", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.7214867323637009}, {"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.9966679215431213}]}, {"text": "Mono-lingual paraphrase and textual entailment recognition datasets, however, potentially facilitate closer tests of certain aspects of the hypothesis than bilingual parallel corpora, which simultaneously exhibit many irrelevant dimensions of cross-lingual variation.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.6555165847142538}]}, {"text": "We investigate this using simple generic Bracketing ITGs containing no language-specific linguistic knowledge.", "labels": [], "entities": []}, {"text": "Experimental results on the MSR Paraphrase Corpus show that, even in the absence of any thesaurus to accommodate lexical variation between the paraphrases, an uninterpolated average precision of at least 76% is obtainable from the Bracketing ITG's structure matching bias alone.", "labels": [], "entities": [{"text": "MSR Paraphrase Corpus", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.919435719648997}, {"text": "precision", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.7633854150772095}, {"text": "Bracketing ITG", "start_pos": 231, "end_pos": 245, "type": "DATASET", "confidence": 0.8658418655395508}]}, {"text": "This is consistent with experimental results on the Pascal Recognising Textual Entailment Challenge Corpus, which show surpisingly strong results fora number of the task subsets.", "labels": [], "entities": [{"text": "Pascal Recognising Textual Entailment Challenge Corpus", "start_pos": 52, "end_pos": 106, "type": "DATASET", "confidence": 0.5649982293446859}]}], "introductionContent": [{"text": "The Inversion Transduction Grammar or ITG formalism, which historically was developed in the context of translation and alignment, hypothesizes strong expressiveness restrictions that constrain paraphrases to vary word order only in certain allowable nested permutations of arguments (.", "labels": [], "entities": [{"text": "Inversion Transduction Grammar or ITG formalism", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.7667389661073685}, {"text": "translation and alignment", "start_pos": 104, "end_pos": 129, "type": "TASK", "confidence": 0.8146634499231974}]}, {"text": "The ITG Hypothesis has been more extensively studied across different languages, but newly available paraphrase datasets provide intriguing opportu- The author would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in part through grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09, and Marine Carpuat and Yihai Shen for invaluable assistance in preparing the datasets and stoplist.", "labels": [], "entities": [{"text": "Hong Kong Research Grants Council (RGC)", "start_pos": 184, "end_pos": 223, "type": "DATASET", "confidence": 0.8481201082468033}, {"text": "Marine Carpuat", "start_pos": 325, "end_pos": 339, "type": "DATASET", "confidence": 0.9517267644405365}]}, {"text": "nities for meaningful analysis of the ITG Hypothesis in a monolingual setting.", "labels": [], "entities": []}, {"text": "The strong inductive bias imposed by the ITG Hypothesis has been repeatedly shown empirically to yield both efficiency and accuracy gains for numerous language acquisition tasks, across a variety of language pairs and tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9953840374946594}]}, {"text": "For example, show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 118, "end_pos": 149, "type": "TASK", "confidence": 0.5974090099334717}, {"text": "Verbmobil corpus) and French-English (Canadian Hansards corpus", "start_pos": 181, "end_pos": 243, "type": "DATASET", "confidence": 0.8066291544172499}]}, {"text": "find that unsupervised alignment using Bracketing ITGs produces significantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model).", "labels": [], "entities": []}, {"text": "With regard to translation rather than alignment accuracy, show that decoding under ITG constraints yields significantly lower word error rates and BLEU scores than the IBM constraints.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.973907470703125}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.8370860815048218}, {"text": "word error rates", "start_pos": 127, "end_pos": 143, "type": "METRIC", "confidence": 0.6864823798338572}, {"text": "BLEU scores", "start_pos": 148, "end_pos": 159, "type": "METRIC", "confidence": 0.9793050587177277}]}, {"text": "We are conducting a series of investigations motivated by the following observation: the empirically demonstrated suitability of ITG paraphrasing constraints across languages should hold, if anything, even more strongly in the monolingual case.", "labels": [], "entities": []}, {"text": "The monolingual case allows in some sense closer testing of various implications of the ITG hypothesis, without irrelevant dimensions of variation arising from other cross-lingual phenomena.", "labels": [], "entities": []}, {"text": "Asymmetric textual entailment recognition (RTE) datasets, in particular the Pascal Recognising Textual Entailment Challenge Corpus (), provide testbeds that abstract over many tasks, including information retrieval, comparable documents, reading comprehension, question answering, information extraction, machine translation, and paraphrase acquisition.", "labels": [], "entities": [{"text": "Asymmetric textual entailment recognition (RTE)", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8213686176708767}, {"text": "Pascal Recognising Textual Entailment Challenge Corpus", "start_pos": 76, "end_pos": 130, "type": "DATASET", "confidence": 0.4847470124562581}, {"text": "information retrieval", "start_pos": 193, "end_pos": 214, "type": "TASK", "confidence": 0.763883501291275}, {"text": "question answering", "start_pos": 261, "end_pos": 279, "type": "TASK", "confidence": 0.8603824973106384}, {"text": "information extraction", "start_pos": 281, "end_pos": 303, "type": "TASK", "confidence": 0.8280770480632782}, {"text": "machine translation", "start_pos": 305, "end_pos": 324, "type": "TASK", "confidence": 0.8023505508899689}, {"text": "paraphrase acquisition", "start_pos": 330, "end_pos": 352, "type": "TASK", "confidence": 0.9010872542858124}]}, {"text": "At the same time, the emergence of paraphrasing datasets presents an opportunity for complementary experiments on the task of recognizing symmetric bidirectional entailment rather than asymmetric directional entailment.", "labels": [], "entities": []}, {"text": "In particular, for this study we employ the MSR Paraphrase Corpus).", "labels": [], "entities": [{"text": "MSR Paraphrase Corpus", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.8896463314692179}]}], "datasetContent": [{"text": "Our objective here was to isolate the effect of the ITG constraint bias.", "labels": [], "entities": []}, {"text": "No training was performed with the available development sets.", "labels": [], "entities": []}, {"text": "Rather, the aim was to establish foundational baseline results, to see in this first round of paraphrase recognition experiments what results could be obtained with the simplest versions of the ITG models.", "labels": [], "entities": [{"text": "paraphrase recognition", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.8359761238098145}]}, {"text": "The MSR Paraphrase Corpus test set consists of 1725 candidate paraphrase string pairs, each annotated for semantic equivalence by two or three human collectors.", "labels": [], "entities": [{"text": "MSR Paraphrase Corpus test set", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.8754764080047608}]}, {"text": "Within the test set, 66.5% of the examples were annotated as being semantically equivalent.", "labels": [], "entities": []}, {"text": "The corpus was originally generated via a combination of automatic filtering methods, making it difficult to make specific claims about distributional neutrality, due to the arbitrary nature of the example selection process.", "labels": [], "entities": []}, {"text": "The ITG scoring model produced an uninterpolated average precision (also known as confidence weighted score) of 76.1%.", "labels": [], "entities": [{"text": "ITG scoring", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7629869878292084}, {"text": "uninterpolated average precision", "start_pos": 34, "end_pos": 66, "type": "METRIC", "confidence": 0.6077651182810465}, {"text": "confidence weighted score)", "start_pos": 82, "end_pos": 108, "type": "METRIC", "confidence": 0.9556413888931274}]}, {"text": "This represents an improvement of roughly 10% over the random baseline.", "labels": [], "entities": []}, {"text": "Note that this improvement can be achieved with no thesaurus or lexical similarity model, and no parameter training.", "labels": [], "entities": []}, {"text": "The experimental procedure for the monolingual textual entailment recognition task is the same as for paraphrase recognition, except that one string serves as the Text and the other serves as the Hypothesis.", "labels": [], "entities": [{"text": "monolingual textual entailment recognition task", "start_pos": 35, "end_pos": 82, "type": "TASK", "confidence": 0.7064339101314545}, {"text": "paraphrase recognition", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.8999573886394501}]}, {"text": "Results on the textual entailment recognition task are consistent with the above paraphrase recognition results.", "labels": [], "entities": [{"text": "textual entailment recognition task", "start_pos": 15, "end_pos": 50, "type": "TASK", "confidence": 0.8493517637252808}, {"text": "paraphrase recognition", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7609624266624451}]}, {"text": "For the PASCAL RTE challenge datasets, across all subsets overall, the model produced a confidence-weighted score of 54.97% (better than chance at the 0.05 level).", "labels": [], "entities": [{"text": "PASCAL RTE challenge datasets", "start_pos": 8, "end_pos": 37, "type": "DATASET", "confidence": 0.6199286058545113}]}, {"text": "All examples were labeled, so precision, recall, and f-score are equivalent; the accuracy was 51.25%.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9997865557670593}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9995392560958862}, {"text": "f-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9930406808853149}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9998071789741516}]}, {"text": "For the RTE task we also investigated a second variant of the model, in which a list of 172 words from a stoplist was excluded from the lexical transductions.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 8, "end_pos": 16, "type": "TASK", "confidence": 0.9168932735919952}]}, {"text": "The motivation for this model was to discount the effect of words such as \"the\" or \"of\" since, more often than not, they could be irrelevant to the RTE task.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 148, "end_pos": 156, "type": "TASK", "confidence": 0.8979845643043518}]}, {"text": "Surprisingly, the stoplisted model produced worse results.", "labels": [], "entities": []}, {"text": "The overall confidence-weighted score was 53.61%, and the accuracy was 50.50%.", "labels": [], "entities": [{"text": "confidence-weighted score", "start_pos": 12, "end_pos": 37, "type": "METRIC", "confidence": 0.948490709066391}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9998766183853149}]}, {"text": "We discuss the reasons below in the context of specific subsets.", "labels": [], "entities": []}, {"text": "As one might expect, the Bracketing ITG models performed better on the subsets more closely approximating the tasks for which Bracketing ITGs were designed: comparable documents (CD), paraphrasing (PP), and information extraction (IE).", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 207, "end_pos": 234, "type": "TASK", "confidence": 0.8244713604450226}]}, {"text": "We will discuss some important caveats on the machine translation (MT) and reading comprehension (RC) subsets.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.8261582434177399}]}, {"text": "The subsets least close to the Bracketing ITG models are information retrieval (IR) and question answering (QA).", "labels": [], "entities": [{"text": "Bracketing ITG", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.7746976912021637}, {"text": "information retrieval (IR)", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.8208809494972229}, {"text": "question answering (QA)", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.8468536257743835}]}], "tableCaptions": []}