{"title": [{"text": "Genetic Algorithms for Feature Relevance Assignment in Memory-Based Language Processing", "labels": [], "entities": [{"text": "Feature Relevance Assignment", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.80997767051061}]}], "abstractContent": [{"text": "We investigate the usefulness of evolutionary algorithms in three incarnations of the problem of feature relevance assignment in memory-based language processing (MBLP): feature weight-ing, feature ordering and feature selection.", "labels": [], "entities": [{"text": "feature relevance assignment in memory-based language processing (MBLP)", "start_pos": 97, "end_pos": 168, "type": "TASK", "confidence": 0.772119453549385}, {"text": "feature ordering", "start_pos": 190, "end_pos": 206, "type": "TASK", "confidence": 0.7762582004070282}, {"text": "feature selection", "start_pos": 211, "end_pos": 228, "type": "TASK", "confidence": 0.7028397470712662}]}, {"text": "We use a simple genetic algorithm (GA) for this problem on two typical tasks in natural language processing: morphological synthesis and unknown word tagging.", "labels": [], "entities": [{"text": "morphological synthesis", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.7664956748485565}, {"text": "unknown word tagging", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.6692194938659668}]}, {"text": "We find that GA feature selection always significantly outperforms the MBLP variant without selection and that feature ordering and weighting with CA significantly outperforms a situation where no weight-ing is used.", "labels": [], "entities": []}, {"text": "However, GA selection does not significantly do better than simple iterative feature selection methods, and GA weighting and ordering reach only similar performance as current information-theoretic feature weighting methods.", "labels": [], "entities": [{"text": "GA selection", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.9699819087982178}, {"text": "GA weighting", "start_pos": 108, "end_pos": 120, "type": "TASK", "confidence": 0.7963396310806274}]}, {"text": "1 Memory-Based Language Processing Memory-Based Language Processing (Daele-mans, van den Bosch, and Zavrel, 1999) is based on the idea that language acquisition should be seen as the incremental storage of exemplars of specific tasks, and language processing as analogical reasoning on the basis of these stored exemplars.", "labels": [], "entities": [{"text": "Memory-Based Language Processing Memory-Based Language Processing", "start_pos": 2, "end_pos": 67, "type": "TASK", "confidence": 0.7115007638931274}]}, {"text": "These exemplars take the form of a vector of, typically, nominal features , describing a linguistic problem and its context, and an associated class symbol representing the solution to the problem.", "labels": [], "entities": []}, {"text": "A new instance is categorized on the basis of its similarity with a memory instance and its associated * Research funded by CELE, S.AI.L Trust V.Z.W., Ieper, Belgium. class.", "labels": [], "entities": [{"text": "CELE", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.8612483739852905}]}, {"text": "The basic algorithm we use to calculate the distance between two items is a variant of IB1 (Aha, Kibler, and Albert, 1991).", "labels": [], "entities": [{"text": "IB1", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.8089604377746582}]}, {"text": "IB1 does not solve the problem of modeling the difference in relevance between the various sources of information.", "labels": [], "entities": [{"text": "IB1", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6166580319404602}]}, {"text": "In an MBLP approach, this can be overcome by means of feature weighting.", "labels": [], "entities": []}, {"text": "The IBi-IG algorithm uses information gain to weight the cost of a feature value mismatch during comparison.", "labels": [], "entities": []}, {"text": "IGTREE is a variant in which an oblivious decision tree is created with features as tests, and in which tests are ordered according to information gain of the associated features.", "labels": [], "entities": [{"text": "IGTREE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6929181218147278}]}, {"text": "In this case, the accuracy of the trained system is very much dependent on a good feature ordering.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9991937279701233}]}, {"text": "For all variants of MBLP discussed here, feature selection can also improve both accuracy and efficiency by discarding some features altogether because of their irrelevance or even counter-productivity in learning to solve the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9986293315887451}]}, {"text": "In our experiments we will use a relevance assignment method that radically differs from information-theoretic measures: genetic algorithms.", "labels": [], "entities": [{"text": "relevance assignment", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.743134081363678}]}, {"text": "2 Genetic Algorithms for Assigning Relevance In the experiments, we linked our memory-based learner TIMBL 1 to PGAPACK 2.", "labels": [], "entities": [{"text": "PGAPACK", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9174237251281738}]}, {"text": "During the weighting experiments a gene corresponds to a specific real-valued feature-weight (we will indicate this by including GA in the algorithm name, i.e. IB1-GA and GATREE, cf. IBi-IG and IGTREE).", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}