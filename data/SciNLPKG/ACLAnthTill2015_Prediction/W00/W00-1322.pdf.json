{"title": [{"text": "An Empirical Study of the Domain Dependence of Supervised Word Sense Disambiguation Systems*", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.6433146198590597}]}], "abstractContent": [{"text": "This paper describes a set of experiments carried out to explore the domain dependence of alternative supervised Word Sense Disam-biguation algorithms.", "labels": [], "entities": []}, {"text": "The aim of the work is threefold: studying the performance of these algorithms when tested on a different corpus from that they were trained on; exploring their ability to tune to new domains, and demonstrating empirically that the Lazy-Boosting algorithm outperforms state-of-the-art supervised WSD algorithms in both previous situations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is the problem of assigning the appropriate meaning (sense) to a given word in a text or discourse.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7954162210226059}, {"text": "assigning the appropriate meaning (sense) to a given word in a text or discourse", "start_pos": 50, "end_pos": 130, "type": "TASK", "confidence": 0.7559277098625898}]}, {"text": "Resolving the ambiguity of words is a central problem for large scale language understanding applications and their associate tasks), e.g., machine translation, information retrieval, reference resolution, parsing, etc.", "labels": [], "entities": [{"text": "Resolving the ambiguity of words", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8540632486343384}, {"text": "machine translation", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.8434810042381287}, {"text": "information retrieval", "start_pos": 161, "end_pos": 182, "type": "TASK", "confidence": 0.8007231950759888}, {"text": "reference resolution", "start_pos": 184, "end_pos": 204, "type": "TASK", "confidence": 0.8388979434967041}, {"text": "parsing", "start_pos": 206, "end_pos": 213, "type": "TASK", "confidence": 0.8026019930839539}]}, {"text": "WSD is one of the most important open problems in NLP.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6601046323776245}]}, {"text": "Despite the wide range of approaches investigated and the large effort devoted to tackle this problem, to date, no large-scale broad-coverage and highly accurate WSD system has been built --see the main conclusions of the first edition of SensEval).", "labels": [], "entities": [{"text": "WSD", "start_pos": 162, "end_pos": 165, "type": "TASK", "confidence": 0.9296717643737793}]}, {"text": "One of the most successful current lines of research is the corpus-based approach in \" This research has been partially funded by the Spanish Research Department (CICYT's project TIC98-0423-C06). by the EU Commission (, and by the Catalan Research Department (CIRIT's consolidated research group 1999SGR-150 and CIRIT's grant 1999FI 00773).", "labels": [], "entities": [{"text": "Spanish Research Department", "start_pos": 134, "end_pos": 161, "type": "DATASET", "confidence": 0.8622799118359884}]}, {"text": "which statistical or Machine Learning (ML) algorithms are applied to learn statistical models or classifiers from corpora in order to perform WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.9637936949729919}]}, {"text": "Generally, supervised approaches 1 have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudo-words.", "labels": [], "entities": []}, {"text": "Many standard M L algorithms for supervised learning have been applied, such as: Decision), Neural Networks (, Bayesian learning (), Exemplar-Based learning),), etc.", "labels": [], "entities": []}, {"text": "Unfortunately, there have been very few direct comparisons between alternative methods for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.987395167350769}]}, {"text": "In general, supervised learning presumes that the training examples are somehow reflective of the task that will be performed by the trainee on other data.", "labels": [], "entities": []}, {"text": "Consequently, the performance of such systems is commonly estimated by testing the algorithm on a separate part of the set of training examples (say 10-20% of them), or by N-fold cross-validation, in which the set of examples is partitioned into N disjoint sets (or folds), and the trainingtest procedure is repeated N times using all combinations of N-1 folds for training and 1 fold for testing.", "labels": [], "entities": []}, {"text": "In both cases, test examples are different from those used for training, but they belong to the same corpus, and, therefore, they are expected to be quite similar.", "labels": [], "entities": []}, {"text": "Although this methodology could be valid for certain NLP problems, such as English Part-of-Speech tagging, we think that there exists reasonable evidence to say that, in WSD, accuracy results cannot be simply extrapolated to other domains (contrary to the opinion of other authors): On the aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus.", "labels": [], "entities": [{"text": "English Part-of-Speech tagging", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.5338809986909231}, {"text": "WSD", "start_pos": 170, "end_pos": 173, "type": "TASK", "confidence": 0.8591091632843018}, {"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9963334798812866}]}, {"text": "one hand, WSD is very dependant to the domain of application () --see also, in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora.", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9402902126312256}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9979514479637146}]}, {"text": "Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover \"all\" potential types of examples.", "labels": [], "entities": []}, {"text": "To date, a thorough study of the domain dependence of WSD --in the style of other studies devoted to parsing)--has not been carried out.", "labels": [], "entities": [{"text": "WSD", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.8397175073623657}, {"text": "parsing", "start_pos": 101, "end_pos": 108, "type": "TASK", "confidence": 0.9643940329551697}]}, {"text": "We think that such an study is needed to assess the validity of the supervised approach, and to determine to which extent a tuning process is necessary to make real WSD systems portable.", "labels": [], "entities": [{"text": "WSD", "start_pos": 165, "end_pos": 168, "type": "TASK", "confidence": 0.9261259436607361}]}, {"text": "In order to corroborate the previous hypotheses, this paper explores the portability and tuning of four different ML algorithms (previously applied to WSD) by training and testing them on different corpora.", "labels": [], "entities": []}, {"text": "Additionally, supervised methods suffer from the \"knowledge acquisition bottleneck\" (.", "labels": [], "entities": []}, {"text": "(Ng, 1997b) estimates that the manual annotation effort necessary to build abroad coverage semantically annotated English corpus is about 16 personyears.", "labels": [], "entities": []}, {"text": "This overhead for supervision could be much greater if a costly tuning procedure is required before applying any existing system to each new domain.", "labels": [], "entities": []}, {"text": "Due to this fact, recent works have focused on reducing the acquisition cost as well as the need for supervision in corpus-based methods.", "labels": [], "entities": []}, {"text": "It is our belief that the research by) 2 provide enough evidence towards the \"opening\" of the bottleneck in the near future.", "labels": [], "entities": []}, {"text": "For that reason, it is worth further investigating the robustness and portability of existing supervised ML methods to better resolve the WSD problem.", "labels": [], "entities": [{"text": "WSD problem", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.9174142777919769}]}, {"text": "It is important to note that the focus of this work will be on the empirical crosscorpus evaluation of several M L supervised algorithms.", "labels": [], "entities": []}, {"text": "Other important issues, such as: selecting the best attribute set, discussing an appropriate definition of senses for the task, etc., are not addressed in this paper.", "labels": [], "entities": []}, {"text": "eIn the line of using lexical resources and search engunes to automatically collect training examples from large text collections or Internet.", "labels": [], "entities": [{"text": "eIn", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9302359223365784}]}, {"text": "This paper is organized as follows: Section 2 presents the four ML algorithms compared.", "labels": [], "entities": [{"text": "ML", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9737648367881775}]}, {"text": "In section 3 the setting is presented in detail, including the corpora and the experimental methodology used.", "labels": [], "entities": []}, {"text": "Section 4 reports the experiments carried out and the results obtained.", "labels": [], "entities": []}, {"text": "Finally, section 5 concludes and outlines some lines for further research.", "labels": [], "entities": []}, {"text": "2 Learning Algorithms Tested 2.1 Naive-Bayes (NB) Naive Bayes is intended as a simple representative of statistical learning methods.", "labels": [], "entities": []}, {"text": "It has been used in its most classical setting (.", "labels": [], "entities": []}, {"text": "That is, assuming independence of features, it classifies anew example by assigning the class that maximizes the conditional probability of the class given the observed sequence of features of that example.", "labels": [], "entities": []}, {"text": "Model probabilities are estimated during training process using relative frequencies.", "labels": [], "entities": []}, {"text": "To avoid the effect of zero counts when estimating probabilities, a very simple smoothing technique has been used, which was proposed in.", "labels": [], "entities": []}, {"text": "Despite its simplicity, Naive Bayes is claimed to obtain state-of-theart accuracy on supervised WSD in many papers).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9914012551307678}, {"text": "WSD", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9321292638778687}]}], "datasetContent": [{"text": "The comparison of algorithms has been performed in series of controlled experiments using exactly the same training and test sets.", "labels": [], "entities": []}, {"text": "There are 7 combinations of training-test sets called: A+B-A+B, A+B-A, A+B-B, A-A, B-B, A-B, and B-A, respectively.", "labels": [], "entities": []}, {"text": "In this notation, the training set is placed at the left hand side of symbol \"-\", while the test set is at the right hand side.", "labels": [], "entities": []}, {"text": "For instance, A-B means that the training set is corpus A and the test set is corpus B.", "labels": [], "entities": []}, {"text": "The symbol \"+\" stands for set union, therefore A+B-B means that the training set is A union B and the test set is B.", "labels": [], "entities": []}, {"text": "When comparing the performance of two algorithms, two different statistical tests of significance have been apphed depending on the case.", "labels": [], "entities": []}, {"text": "A-B and B-A combinations represent a single training-test experiment.", "labels": [], "entities": []}, {"text": "In this cases, the McNemar's test of significance is used (with a confidence value of: X1,0.952 = 3.842), which is proven to be more robust than a simple test for the difference of tw0_proportions.", "labels": [], "entities": [{"text": "McNemar's test of significance", "start_pos": 19, "end_pos": 49, "type": "METRIC", "confidence": 0.8171241044998169}]}, {"text": "In the other combinations, a 10-fold crossvalidation was performed in order to prevent nouns verbs).", "labels": [], "entities": []}, {"text": "shows the accuracy figures of the four methods in all combinations of training and test sets s.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999698281288147}]}, {"text": "Standard deviation numbers are supplied in all cases involving cross validation.", "labels": [], "entities": [{"text": "cross validation", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.6533389985561371}]}, {"text": "M FC stands fora Most-Frequent-sense Classifier, that is, a naive classifier that learns the most frequent sense of the training set and uses it to classify all examples of the test set.", "labels": [], "entities": []}, {"text": "Averaged results are presented for nouns.", "labels": [], "entities": []}, {"text": "verbs, and overall, and the best results for each case are printed in boldface.", "labels": [], "entities": []}, {"text": "The following conclusions can be drawn: \u2022 LB outperforms all other methods in all cases.", "labels": [], "entities": []}, {"text": "Additionally, this superiority is statistically significant, except when comparing LB to the PEB approach in the cases marked with an asterisk.", "labels": [], "entities": []}, {"text": "\u2022 Surprisingly, LB in A+B-A (or A+B-B) does not achieve substantial improvement to the results of A-A (or B-S) win fact, the first variation is not statistically significant and the second is only slightly significant.", "labels": [], "entities": [{"text": "LB", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9719634056091309}]}, {"text": "That is, the addition of extra examples from another domain does not necessarily contribute to improve the results on the original corpus.", "labels": [], "entities": []}, {"text": "This effect is also observed in the other methods, specially in some cases (e.g. Snow in A+B-A vs. A-A) in which the joining of both training corpora is even counterproductive.", "labels": [], "entities": []}, {"text": "SThe second and third column correspond to the train and test sets used by \u2022 Regarding the portability of the systems, very disappointing results are obtained.", "labels": [], "entities": []}, {"text": "Restricting to [B results, we observe that the accuracy obtained in A-B is 47.1% while the accuracy in B-B (which can be considered an upper bound for LB in B corpus) is 59.0%, that is, a drop of 12 points.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9994829893112183}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.999060332775116}]}, {"text": "Furthermore, 47.1% is only slightly better than the most frequent sense in corpus B, 45.5%.", "labels": [], "entities": []}, {"text": "The comparison in the reverse direction is even worse: a drop from 71.3% (A-A) to 52.0% (B-A), which is lower than the most frequent sense of corpus A, 55.9%.", "labels": [], "entities": []}, {"text": "The previous experiment shows that classitiers trained on the A corpus do notwork well on the B corpus, and vice-versa.", "labels": [], "entities": []}, {"text": "Therefore, it seems that some kind of tuning process is necessary to adapt supervised systems to each new domain.", "labels": [], "entities": []}, {"text": "This experiment explores the effect of a simple tuning process consisting of adding to the original training set a relatively small sarnple of manually sense tagged examples of the new domain.", "labels": [], "entities": []}, {"text": "The size of this supervised portion varies from 10% to 50% of the available corpus in steps of 10% (the remaining 50% is kept for testing).", "labels": [], "entities": []}, {"text": "This set of experiments will be referred to as A+%B-B, or conversely, to In order to determine to which extent the original training set contributes to accurately disambiguate in the new domain, we also calculate the results for %A-A (and %B-B), that is, using only the tuning corpus for training.", "labels": [], "entities": []}, {"text": "As expected, the accuracy of all methods grows (towards the upper bound) as more tuning corpus is added to the training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9995366334915161}]}, {"text": "However, the relation between X+%Y-Y and %Y-Y reveals some interesting facts.", "labels": [], "entities": []}, {"text": "In plots 2a,  However, this is not the situation of LazyBoosting (plots 4a and 4b), for which a moderate (but consistent) improvement of accuracy is observed when retaining the original training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9992173910140991}]}, {"text": "Therefore, Lazy[3oosting shows again a better behaviour than their competitors when moving from one domain to another.", "labels": [], "entities": []}, {"text": "The bad results about portability could be explained by, at least, two reasons: 1) Corpus A and [3 have a very different distribution of senses, and, therefore, different a-priori biases; 2) Examples of corpus A and [3 contain different information, and, therefore, the learning algorithms acquire different (and non interchangeable) classification cues from both corpora,.", "labels": [], "entities": [{"text": "portability", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9775356650352478}]}, {"text": "The first hypothesis is confirmed by observing the bar plots of, which contain the distribution of the four most frequent senses of some sample words in the corpora A and B. respectively.", "labels": [], "entities": []}, {"text": "In order to check the second hypothesis, two new sense-balanced corpora have been generated from the DSO corpus, by equilibrating the number of examples of each sense between A and B parts.", "labels": [], "entities": [{"text": "DSO corpus", "start_pos": 101, "end_pos": 111, "type": "DATASET", "confidence": 0.9654915928840637}]}, {"text": "In this way, the first difficulty is artificially overrided and the algorithms should be portable if examples of both parts are quite similar.", "labels": [], "entities": []}, {"text": "shows the results obtained by LazyBoosting on these new corpora.", "labels": [], "entities": []}, {"text": "Regarding portability, we observe a significant accuracy decrease of 7 and 5 points from A-A to B-A, and from B-B to A-B, respectively 9.", "labels": [], "entities": [{"text": "portability", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9787220358848572}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9995360374450684}]}, {"text": "That is, even when the sazne distribution of senses is conserved between training and test examples, the portability of the supervised WSD systems is not guaranteed.", "labels": [], "entities": []}, {"text": "These results imply that examples have to be largely different from one corpus to another.", "labels": [], "entities": []}, {"text": "By studying the weak rules generated by kazyBoosting in both cases, we could corroborate this fact.", "labels": [], "entities": []}, {"text": "On the one hand, the type of features used in the rules were significantly different between corpora, and, additionally, there were very few rules that apply to both sets; On the other hand, the sign of the prediction of many of these common rules was somewhat contradictory between corpora.", "labels": [], "entities": []}, {"text": "9This loss inaccuracy is not as important as m the first experiment, due to the simplification provided by the balancing of sense distributions.: Accuracy results (5= standard deviation) of LazyBoosting on the sense-balanced corpora Furthermore, these results are in contradiction with the idea of \"robust broad-coverage WSD\" introduced by, in which a supervised system trained on a large enough corpora (say a thousand examples per word) ~hould provide accurate disambiguation on any corpora (or, at least significantly better than MFS).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9871642589569092}]}, {"text": "Consequently, it is our belief that a number of issues regarding portability, tuning, knowledge acquisition, etc., should be thoroughly studied before stating that the supervised ML paradigm is able to resolve a realistic WSD problem.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.7619993388652802}, {"text": "WSD problem", "start_pos": 222, "end_pos": 233, "type": "TASK", "confidence": 0.9299071133136749}]}, {"text": "Regarding the M L algorithms tested, the contribution of this work consist of empirically demonstrating that the LazyBoosting algorithm outperforms other three state-of-theart supervised ML methods for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 202, "end_pos": 205, "type": "TASK", "confidence": 0.8774703741073608}]}, {"text": "this algorithm is proven to have better properties when is applied to new domains.", "labels": [], "entities": []}, {"text": "Further work is planned to be done in the following directions: \u2022 Extensively evaluate LazyBoosting on the WSD task.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 107, "end_pos": 115, "type": "TASK", "confidence": 0.8884913623332977}]}, {"text": "This would include taking into account additional/alternative attributes and testing the algorithm in other corpora --specially on sense-tagged corpora automatically obtained from Internet or large text collections using nonsupervised methods ().", "labels": [], "entities": []}, {"text": "\u2022 Since most of the knowledge learned from a domain is not useful when changing to anew domain, further investigation is needed on tuning strategies, specially on those using non-supervised algorithms.", "labels": [], "entities": []}, {"text": "\u2022 It is known that mislabelled examples resulting from annotation errors tend to be hard examples to classify correctly, and, therefore, tend to have large weights in the final distribution.", "labels": [], "entities": []}, {"text": "This observation allows both to identify the noisy examples and use LazyBoosting as away to improve data quality.", "labels": [], "entities": []}, {"text": "Preliminary experiments have been already carried out in this direction on the DSO corpus.", "labels": [], "entities": [{"text": "DSO corpus", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.9782877266407013}]}, {"text": "\u2022 Moreover, the inspection of the rules learned by kazyBoosting could provide evidence about similar behaviours of apriori different senses.", "labels": [], "entities": []}, {"text": "This type of knowledge could be useful to perform clustering of too fine-grained or artificial senses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Information about the set of 21 words of reference.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy results (:i: standard deviation) of the methods on all training-test combina- tions", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990972280502319}]}, {"text": " Table 3: Accuracy results (5= standard deviation) of LazyBoosting on the sense-balanced corpora", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993857145309448}]}]}