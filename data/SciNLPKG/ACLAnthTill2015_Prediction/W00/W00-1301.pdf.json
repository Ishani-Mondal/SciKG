{"title": [], "abstractContent": [{"text": "A wide range of natural language problems can be viewed as disambiguating between a small set of alternatives based upon the string context surrounding the ambiguity site.", "labels": [], "entities": []}, {"text": "In this paper we demonstrate that classification accuracy can be improved by invoking a more descriptive feature set than what is typically used.", "labels": [], "entities": [{"text": "classification", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.9787527322769165}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.930167019367218}]}, {"text": "We present a technique that disambiguates by learning regular expressions describing the stnng contexts in which the ambiguity sites appear.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many natural language tasks are essentially nway classification problems, where classification decisions are made from a small set of choices, based upon the linguistic context in which the ambiguity site occurs.", "labels": [], "entities": [{"text": "nway classification", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7051497995853424}]}, {"text": "Examples of such tasks include: confusable word set disambiguation; word sense disambiguation; determining such lexical features as pronoun case and determiner number for machine translation; part of speech tagging; named entity labeling; spelling correction; and some formulations of skeletal parsing.", "labels": [], "entities": [{"text": "confusable word set disambiguation", "start_pos": 32, "end_pos": 66, "type": "TASK", "confidence": 0.577594555914402}, {"text": "word sense disambiguation", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.7160970667997996}, {"text": "machine translation", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.7491314113140106}, {"text": "part of speech tagging", "start_pos": 192, "end_pos": 214, "type": "TASK", "confidence": 0.6117851361632347}, {"text": "named entity labeling", "start_pos": 216, "end_pos": 237, "type": "TASK", "confidence": 0.6600075761477152}, {"text": "spelling correction", "start_pos": 239, "end_pos": 258, "type": "TASK", "confidence": 0.9258130788803101}, {"text": "skeletal parsing", "start_pos": 285, "end_pos": 301, "type": "TASK", "confidence": 0.736009269952774}]}, {"text": "Very similar feature sets have been used across machine learning algorithms and across classification problems.", "labels": [], "entities": []}, {"text": "For example, in confusable word set disambiguation, systems typically use as features the occurrence of a particular word within a window of +/-n words of the target, and collocations based on the words and part of speech tags of up to two words to the left and two words to the fight of the target.", "labels": [], "entities": [{"text": "word set disambiguation", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.7240554889043173}]}, {"text": "Below we present a machine learning algorithm that learns from a much richer feature set than that typically used for classification in natural language.", "labels": [], "entities": []}, {"text": "Our algorithm learns rule sequences for n-way classification, where the condition of a rule can be a restricted regular expression on the string context in which the ambiguity site appears.", "labels": [], "entities": []}, {"text": "We demonstrate that using this more powerful feature space leads to an improvement in disambiguation performance on confusable words.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test whether learning RREs can improve disambiguafion accuracy, we explored the task of confusion set disambiguation (Golding and Roth 1999).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9724995493888855}, {"text": "confusion set disambiguation", "start_pos": 91, "end_pos": 119, "type": "TASK", "confidence": 0.6102333962917328}]}, {"text": "We trained and applied two different rule sequence learners, one which used the standard feature set for this problem (e.g. the identical feature set to that used in (Golding and Roth 1999) and (Mangu and Brill 1997) and described in the introduction, and one which learned RR.Es.", "labels": [], "entities": [{"text": "RR.Es", "start_pos": 274, "end_pos": 279, "type": "TASK", "confidence": 0.8326812982559204}]}, {"text": "4 Because we wanted to deterinine what could be gained by using RREs, we ran an ablation study where we kept everything else constant across the two runs, and did not use performance enhancing techniques such as parameter tuning on held out data or classifier combination.", "labels": [], "entities": []}, {"text": "Both learners were given a window of +/-5 words surrounding the ambiguity site.", "labels": [], "entities": []}, {"text": "Context was not allowed to cross sentence boundaries.", "labels": [], "entities": []}, {"text": "The training and test set were derived by finding all instances of the confusable words in the Brown Corpus, using the Penn Treebank parts of speech and tokenization, and then dividing this set into 80% for training and 20% for testing.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.9824705123901367}, {"text": "Penn Treebank parts", "start_pos": 119, "end_pos": 138, "type": "DATASET", "confidence": 0.9794818162918091}]}, {"text": "For the RRE-based system, we mapped the +/-5 word window of context into a string as follows (where wi is a word and ti is apart of speech tag): Wi.", "labels": [], "entities": []}, {"text": "1 MIDDLE Wi+l ti+l wi+2 ti+2 wi+3 ti+3 wi+4 ti+4 wi+5 ti+5 where MIDDLE is the ambiguity site.", "labels": [], "entities": [{"text": "MIDDLE", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9499509334564209}, {"text": "MIDDLE", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.8388285040855408}]}, {"text": "Both for execution time and space considerations for the learner and for fear of overtraining, we put abound on the length of the RRE that could be learned, s We define anatomic RRE as any RRE derived without any concatenation operations.", "labels": [], "entities": []}, {"text": "Then the length of an RRE is defined as the number of atomic RREs which that RRE is made up of.", "labels": [], "entities": []}, {"text": "The atom \"MIDDLE\" is not counted in length.", "labels": [], "entities": [{"text": "MIDDLE", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.7552369832992554}]}, {"text": "Below we give two examples of rules that were learned for one confusion set: 6 The first rule says to change the disambiguation guess to << passed >> if the word before is not a determiner and the word after is a preposition.", "labels": [], "entities": []}, {"text": "This matches contexts such as : << ...", "labels": [], "entities": []}, {"text": ">> while not matching contexts such as : << ...", "labels": [], "entities": []}, {"text": "made in the past by ...", "labels": [], "entities": []}, {"text": ">> The second rule captures contexts such as : << ...", "labels": [], "entities": []}, {"text": "the hike passed the campground ...", "labels": [], "entities": []}, {"text": ">~ while not matching contexts such as : << ...", "labels": [], "entities": []}, {"text": "want to take a hike past the campground...", "labels": [], "entities": []}, {"text": ">> In, we show test set results from running the rule sequence learner with both the standard set of features and with RRE-based features.", "labels": [], "entities": []}, {"text": "7 The results are sorted by training corpus size, with the raise/rise training corpus being the smallest and the then/than training corpus being the largest.", "labels": [], "entities": []}, {"text": "Baseline accuracy is 5 Note that this does not imply abound on the length of a string to which an RRE can apply.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9763337969779968}, {"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.8847668170928955}]}, {"text": "6 DT= determiner, IN = preposition, biN = singular noun.", "labels": [], "entities": [{"text": "IN", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9676286578178406}]}, {"text": "7 While these results look worse than those achieved by other systems, as reported in, we used different data splits and tokenization.", "labels": [], "entities": []}, {"text": "Our baseline accuracies are significantly lower than the baselines for their test sets.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.8046987056732178}]}, {"text": "If we account for this by instead measuring percent error reduction compared to baseline accuracy, then our average reduction is better than that reported for the BaySpell system, but worse than that of WinSpell.", "labels": [], "entities": [{"text": "percent error reduction", "start_pos": 44, "end_pos": 67, "type": "METRIC", "confidence": 0.7361332575480143}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9761300683021545}, {"text": "BaySpell system", "start_pos": 163, "end_pos": 178, "type": "DATASET", "confidence": 0.917144387960434}, {"text": "WinSpell", "start_pos": 203, "end_pos": 211, "type": "DATASET", "confidence": 0.9005196690559387}]}, {"text": "If we add voting to our system (WinSpell employs voting), then we attain results on par with WinSpell.", "labels": [], "entities": [{"text": "WinSpell", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.9103235602378845}]}, {"text": "the accuracy attained on the test set by always picking the word that appears more frequently in the training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994778037071228}]}, {"text": "In we see that the RRE-based system outperforms the standard system on 9 of the confusion sets, the standard system outperforms the RRE-based system on 2 and the two systems attain identical results on 3.", "labels": [], "entities": [{"text": "RRE-based", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.5874825119972229}]}, {"text": "We see that the relative performance of the RRE-based learner is better overall on the larger training sets than on the smaller sets.", "labels": [], "entities": [{"text": "RRE-based learner", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7831825613975525}]}, {"text": "This is to be expected, as more data is needed to support learning the more expressive RRE-based rules.", "labels": [], "entities": []}, {"text": "Pooling all of the test sets into one big set, the RRE-based system achieves an overall accuracy of 89.9%, compared to 88.5% for the standard learner.", "labels": [], "entities": [{"text": "RRE-based", "start_pos": 51, "end_pos": 60, "type": "TASK", "confidence": 0.5675596594810486}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9996078610420227}]}, {"text": "Weighting each confusion pair equally, the RRE-based system achieves an overall accuracy of 88.4%, compared to 86.9% for the standard learner.", "labels": [], "entities": [{"text": "RRE-based", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.5808411836624146}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9996448755264282}]}], "tableCaptions": [{"text": " Table 1 Test Set Results: Standard vs RRE- Based Features", "labels": [], "entities": [{"text": "RRE- Based", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9215418299039205}]}, {"text": " Table 2 Performance Analysis Across Different  Sets", "labels": [], "entities": [{"text": "Performance Analysis", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.6804299205541611}]}]}