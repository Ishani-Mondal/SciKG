{"title": [{"text": "Mining Discourse Markers for Chinese Textual Summarization", "labels": [], "entities": [{"text": "Chinese Textual Summarization", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.5669131477673849}]}], "abstractContent": [{"text": "Discourse markers foreshadow the message thrust of texts and saliently guide their rhetorical structure which are important for content filtering and text abstraction.", "labels": [], "entities": [{"text": "content filtering", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.7431141138076782}, {"text": "text abstraction", "start_pos": 150, "end_pos": 166, "type": "TASK", "confidence": 0.7352445721626282}]}, {"text": "This paper reports on efforts to automatically identify and classify discourse markers in Chinese texts using heuristic-based and corpus-based data-mining methods, as an integral part of automatic text summarization via rhetorical structure and Discourse Markers.", "labels": [], "entities": [{"text": "classify discourse markers in Chinese texts", "start_pos": 60, "end_pos": 103, "type": "TASK", "confidence": 0.7600114643573761}, {"text": "automatic text summarization", "start_pos": 187, "end_pos": 215, "type": "TASK", "confidence": 0.5973845620950063}]}], "introductionContent": [{"text": "Discourse is understood to refer to any form of language-based communication involving multiple sentences or utterances.", "labels": [], "entities": []}, {"text": "The most important forms of discourse of interest to computerized natural .language processing are text and dialogue.", "labels": [], "entities": [{"text": "computerized natural .language processing", "start_pos": 53, "end_pos": 94, "type": "TASK", "confidence": 0.6393404841423035}]}, {"text": "While discourse such as written text normally appears to \u2022 be a linear sequence of clauses and sentences, it has \"long been recognized by linguists that these clauses and sentences tend to cluster together into units, called discourse segments, that are related pragmatically to form a hierarchical structure.", "labels": [], "entities": []}, {"text": "Discourse analysis goes beyond the levels of syntactic and semantic analysis, which typically treats each sentence as an isolated, independent unit.", "labels": [], "entities": [{"text": "Discourse analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9005557298660278}, {"text": "syntactic and semantic analysis", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6753949373960495}]}, {"text": "The function of discourse analysis is to divide a text into discourse segments, and to recognize and re-construct the discourse structure of the text as intended by its author.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7394076287746429}]}, {"text": "Results of discourse analysis can be used to solve many important NLP problems such as anaphoric reference, tense and aspect analysis, intention recognition (, or'can be directly applied to computational NLP applications such as text abstraction () and text generation.", "labels": [], "entities": [{"text": "tense and aspect analysis", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.6523421630263329}, {"text": "intention recognition", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.6878601163625717}, {"text": "text generation", "start_pos": 253, "end_pos": 268, "type": "TASK", "confidence": 0.8192764222621918}]}, {"text": "Automatic text abstraction has received considerable attention (see fora comprehensive review).", "labels": [], "entities": [{"text": "Automatic text abstraction", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7044917047023773}]}, {"text": "While some statistical approaches have had some success in extracting one or more sentences which can serve as a summary (), summarization in general has remained an elusive task.", "labels": [], "entities": [{"text": "summarization", "start_pos": 125, "end_pos": 138, "type": "TASK", "confidence": 0.9886983036994934}]}, {"text": "develop a system SUMMONS to summarize full text input using templates produced by the message understanding systems, developed under ARPA human language technology.", "labels": [], "entities": [{"text": "summarize full text input", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.8292406648397446}]}, {"text": "Unlike previous approaches, their system summarizes a series of news articles on the same event, producing a paragraph consisting of one or more sentences.", "labels": [], "entities": [{"text": "summarizes a series of news articles", "start_pos": 41, "end_pos": 77, "type": "TASK", "confidence": 0.8197196225325266}]}, {"text": "uses a blackboard system architecture with co-operating object-oriented agents and a dynamic text representation which borrows its conceptual relations from Rhetorical Structure Theory (RST) (.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 157, "end_pos": 190, "type": "TASK", "confidence": 0.7038589964310328}]}, {"text": "Furthermore, connectionist models of discourse summarization have also attracted a lot of attention ().", "labels": [], "entities": [{"text": "discourse summarization", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.6540476828813553}]}, {"text": "The main underlying principles are the distributed encoding of concepts and the simulation of human association with a large amount of processing nodes.", "labels": [], "entities": []}, {"text": "What is crucial in this approach is to provide a subconceptual layer in the linguistic reasoning.", "labels": [], "entities": []}, {"text": "As in, summarization techniques in text analysis are severely impaired by the absence of a generally accepted discourse model and the use of superstructural schemes is promising for abstracting text.", "labels": [], "entities": [{"text": "summarization", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.9822868704795837}, {"text": "text analysis", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.7867863774299622}]}, {"text": "describes a text processing system that can identify anaphors so that they maybe utilized to enhance sentence selection.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.6967558562755585}]}, {"text": "It is based on the assumption that sentences which contain nonanaphoric noun phrases and introduce key concepts into the text are worthy of inclusion in an abstract., T'sou et al.", "labels": [], "entities": []}, {"text": "(1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST).", "labels": [], "entities": [{"text": "summarization", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9648023247718811}, {"text": "Rhetorical Structure Theory (RST)", "start_pos": 80, "end_pos": 113, "type": "TASK", "confidence": 0.6974259912967682}]}, {"text": "The theory has been exploited in a. number of computational systems (e.g. Hovy 1993).", "labels": [], "entities": []}, {"text": "The main idea is to build a discourse tree where each node of the tree represents a RST relation.", "labels": [], "entities": [{"text": "RST relation", "start_pos": 84, "end_pos": 96, "type": "TASK", "confidence": 0.8744832873344421}]}, {"text": "Summarization is achieved by trimming unimportant sentences on the basis of the relative saliency or rhetorical relations.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9816895723342896}, {"text": "trimming unimportant sentences", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.8819393316904703}]}, {"text": "On the other hand, cohesion can also provide context to aid in the resolution of ambiguity as well as in text summarization.", "labels": [], "entities": [{"text": "resolution of ambiguity", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.8771764039993286}, {"text": "text summarization", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7087779641151428}]}, {"text": "describes a method based on text coherence which models text in terms of macro-level relations between clauses or sentences to help determine the overall argumentative structure of the text.", "labels": [], "entities": []}, {"text": "They examine the extent to which cohesion and coherence can each be used to establish saliency of textual units.", "labels": [], "entities": []}, {"text": "The SIFAS (S,yntactic Marker based EullText Abstration System) system has been designed and implemented to use discourse markers in the automatic summarization of Chinese.", "labels": [], "entities": [{"text": "summarization of Chinese", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.8780670960744222}]}, {"text": "Section 2 provides an introduction to discourse markers in Chinese.", "labels": [], "entities": []}, {"text": "An overview of SIFAS is presented in Section 3.", "labels": [], "entities": [{"text": "SIFAS", "start_pos": 15, "end_pos": 20, "type": "TASK", "confidence": 0.9010886549949646}]}, {"text": "In Section 4, we describe a coding scheme for tagging every discourse marker appearing in the SIFAS corpus.", "labels": [], "entities": [{"text": "SIFAS corpus", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.8149307072162628}]}, {"text": "In Section 5, we introduce a heuristic-based algorithm for automatic tagging of discourse markers.", "labels": [], "entities": [{"text": "automatic tagging of discourse markers", "start_pos": 59, "end_pos": 97, "type": "TASK", "confidence": 0.7668131351470947}]}, {"text": "In Section 6, we describe the application of the C4.5 algorithm to the same task.", "labels": [], "entities": []}, {"text": "In Section 7, we present the evaluation results of applying the two algorithms to corpus tagging, followed by a conclusion.", "labels": [], "entities": [{"text": "corpus tagging", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.7591548562049866}]}], "datasetContent": [{"text": "In order to evaluate the effectiveness of the heuristic-based algorithm, we randomly selected 40 editorials from Ming Pao, a Chinese newspaper of Hong Kong, to form our test data.", "labels": [], "entities": [{"text": "Ming Pao, a Chinese newspaper of Hong Kong", "start_pos": 113, "end_pos": 155, "type": "DATASET", "confidence": 0.850419263044993}]}, {"text": "Only editorials are chosen because they are mainly argumentative texts and their lengths are relatively uniform.", "labels": [], "entities": []}, {"text": "The steps of evaluation consist of: 1) tagging all of the test data using the heuristic-based algorithm, and 2) proofreading, correcting and recording all the tagging errors by a human encoder.", "labels": [], "entities": []}, {"text": "The resulting statistics include, for each editorial in the test data, the number of lexical items (#Lltms), the number of sentences (#Sens), the number of discourse markers (#Mrkrs), and the number of sentences containing at least one discourse marker (#CSens).", "labels": [], "entities": []}, {"text": "Our evaluation is based on counting the number of discourse markers that are correctly tagged.", "labels": [], "entities": []}, {"text": "For incorrectly tagged discourse markers, we classify them according to the types of errors that we have introduced in T'.", "labels": [], "entities": []}, {"text": "We define two evaluation metrics as follows: Gross Accuracy (GA) is defined to be the percentage of correctly tagged discourse markers to the total number of discourse markers while RelationMatching Accuracy (RMA) is defined to be the percentage of correctly tagged discourse markers to the total number of discourse markers minus those errors caused by non-markers and unrecorded markers.", "labels": [], "entities": [{"text": "Gross Accuracy (GA)", "start_pos": 45, "end_pos": 64, "type": "METRIC", "confidence": 0.8977355837821961}, {"text": "RelationMatching Accuracy (RMA)", "start_pos": 182, "end_pos": 213, "type": "METRIC", "confidence": 0.9460032939910888}]}, {"text": "The results for our testing.", "labels": [], "entities": []}, {"text": "data have GA = 68.89% and RMA = 95.07%.", "labels": [], "entities": [{"text": "GA", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9996495246887207}, {"text": "RMA", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9987603425979614}]}, {"text": "Since the heuristic-based algorithm does not assume any knowledge of the statistics and behavioral patterns of discourse markers, our GA demonstrates the usefulness of the algorithm in alleviating the burden of human encoders in developing a sufficiently large-corpus for the purpose of studying the usage of discourse markers.", "labels": [], "entities": []}, {"text": "In our experiment, most errors come from tagging non-discourse markers as discourse markers).", "labels": [], "entities": []}, {"text": "This is due to the fact that, similar to the question of cue phrase polysemy (Hirschberg and Litman 1993), many Chinese discourse markers have both discourse senses and alternate sentential senses in different ;utterances.", "labels": [], "entities": []}, {"text": "Zhe ('this') buguo shi ('only is') yi ('one') ge ('classifier') wanxiao ('joke') ...('This is only a joke'.)", "labels": [], "entities": []}, {"text": "(sentential sense) \u2022 ...Buguo ('however'), wo (T) bu ('neg') zheyang ('thus') renwei ('consider') \u2022 ..('But I don't think so.')", "labels": [], "entities": []}, {"text": "(discourse sense)  Algorithm (with C4.5) In Section 6, we discuss how machine learning techniques have been applied to the problem of discourse marker disambiguation in Chinese.", "labels": [], "entities": [{"text": "discourse marker disambiguation", "start_pos": 134, "end_pos": 165, "type": "TASK", "confidence": 0.7244966228802999}]}, {"text": "In our experiment, there area total of 2627 cases.", "labels": [], "entities": []}, {"text": "In our decision tree construction, we use 75 percent of the total cases as a training set, and the remaining 25 percent of cases as a test set.", "labels": [], "entities": [{"text": "decision tree construction", "start_pos": 7, "end_pos": 33, "type": "TASK", "confidence": 0.7091100017229716}]}, {"text": "Many decision trees can be generated by adjusting the parameters in the learning algorithm.", "labels": [], "entities": []}, {"text": "Many decision trees generated in our experiment have an accuracy around 80% for both the training set and the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9994564652442932}]}, {"text": "shows one of the possible decision trees in our experiment.", "labels": [], "entities": []}, {"text": "The last branch of the decision tree", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2 shows the  minimum, maximum and average values of these  characteristics. The ratio of the average number of  discourse markers to the average number of lexical  items is 4.37%, and the ratio of the average  number of sentences  discourse marker to  sentences is 62.66%.  #Lltms", "labels": [], "entities": []}]}