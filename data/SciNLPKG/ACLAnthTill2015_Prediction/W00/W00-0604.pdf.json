{"title": [{"text": "Answer Extraction Towards better Evaluations of NLP Systems", "labels": [], "entities": [{"text": "Answer Extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8317273557186127}]}], "abstractContent": [{"text": "We argue that reading comprehension tests are not particularly suited for the evaluation of NLP systems.", "labels": [], "entities": []}, {"text": "Reading comprehension tests are specifically designed to evaluate human reading skills, and these require vast amounts of world knowledge and common-sense reasoning capabilities.", "labels": [], "entities": []}, {"text": "Experience has shown that this kind of full-fledged question answering (QA) over texts from a wide range of domains is so difficult for machines as to be far beyond the present state of the art of NLP.", "labels": [], "entities": [{"text": "question answering (QA) over texts", "start_pos": 52, "end_pos": 86, "type": "TASK", "confidence": 0.8467290784631457}]}, {"text": "To advance the field we propose a much more modest evaluation set:up, viz.", "labels": [], "entities": []}, {"text": "Answer Extraction (AE) over texts from highly restricted domains.", "labels": [], "entities": [{"text": "Answer Extraction (AE)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8138533353805542}]}, {"text": "AE aims at retrieving those sentences from documents that contain the explicit answer to a user query.", "labels": [], "entities": []}, {"text": "AE is less ambitious than full-fledged QA but has a number of important advantages over QA.", "labels": [], "entities": [{"text": "AE", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9362490773200989}]}, {"text": "It relies mainly on linguistic knowledge and needs only a very limited amount of world knowledge and few inference rules.", "labels": [], "entities": []}, {"text": "However, it requires the solution of a number of key linguistic problems.", "labels": [], "entities": []}, {"text": "This makes AE a suitable task to advance NLP techniques in a measurable way.", "labels": [], "entities": [{"text": "AE", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.6191483736038208}]}, {"text": "Finally, there is areal demand for working AE systems in techni: cal domains.", "labels": [], "entities": []}, {"text": "We outline how evaluation procedures for AE systems over real world domains might look like and discuss their feasibility.", "labels": [], "entities": [{"text": "AE", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.8174399733543396}]}, {"text": "1 On the Design of Evaluation Methods for NLP Systems The idea that the systematic and principled evaluation of document processing systems is crucial for the development of the field as a whole has gained wide acceptance in the community during the last decade.", "labels": [], "entities": []}, {"text": "Ina number of large-scale projects (among them TREC (Voorhees and Harman, 1998) and MUC (MUC-7, 1998)), evaluation procedures for specific types of systems have been used extensively, and refined over the years.", "labels": [], "entities": [{"text": "TREC (Voorhees and Harman, 1998)", "start_pos": 47, "end_pos": 79, "type": "DATASET", "confidence": 0.776438444852829}, {"text": "MUC (MUC-7, 1998))", "start_pos": 84, "end_pos": 102, "type": "DATASET", "confidence": 0.8043300608793894}]}, {"text": "Three things were common to these evaluations: First, the systems to be evaluated were each very closely tied to a particular task (document retrieval and information extraction, respectively).", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7170878350734711}, {"text": "information extraction", "start_pos": 155, "end_pos": 177, "type": "TASK", "confidence": 0.7454837411642075}]}, {"text": "Second, the evaluation was of the black box type, i.e. it considered only system input-output relations without regard to the specific mechanisms by which the outputs were obtained.", "labels": [], "entities": []}, {"text": "Third, the amount of data to be processed was enormous (several gi-gabytes for TREC).", "labels": [], "entities": []}, {"text": "There is general agreement that these competitive evaluations had a striking and beneficial effect on the performance of the various systems tested over the years.", "labels": [], "entities": []}, {"text": "However, it is also recognized (albeit less generally) that these evaluation experiments also had the, less beneficial , effect that the participating systems focussed increasingly more narrowly on those few parameters that were measured in the evaluation , to the detriment of more general properties.", "labels": [], "entities": []}, {"text": "In some cases this meant that powerful and linguistically interesting but slow systems were dropped in favour of shallow but fast systems with precious little linguistic content.", "labels": [], "entities": []}, {"text": "Thus the system with which SRI participated in the MUC-3 evaluation in 1991, TACITUS (Hobbs et al., 1991), a true text-understanding system, was later replaced by FASTUS (Appelt et al., 1995; Hobbs et al., 1996), a much simpler , and vastly faster, information extraction system.", "labels": [], "entities": [{"text": "MUC-3 evaluation in 1991", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.54152861982584}, {"text": "TACITUS", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.9881369471549988}, {"text": "FASTUS", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.8008670210838318}, {"text": "information extraction", "start_pos": 249, "end_pos": 271, "type": "TASK", "confidence": 0.7513554096221924}]}, {"text": "The reason was that TACITUS was spending so much of its time attempting to make sense of portions of the text that were irrelevant to the task that recall was mediocre.", "labels": [], "entities": [{"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9962736368179321}]}, {"text": "We argue that the setup of these competitive evaluations , and in particular the three parameters mentioned above, drove the development of the participating systems towards becoming impres-20", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Methods for NLP Systems The idea that the systematic and principled evaluation of document processing systems is crucial for the development of the field as a whole has gained wide acceptance in the community during the last decade.", "labels": [], "entities": []}, {"text": "Ina number of large-scale projects (among them TREC) and MUC), evaluation procedures for specific types of systems have been used extensively, and refined over the years.", "labels": [], "entities": [{"text": "TREC", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.6540877223014832}, {"text": "MUC", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.5059809684753418}]}, {"text": "Three things were common to these evaluations: First, the systems to be evaluated were each very closely tied to a particular task (document retrieval and information extraction, respectively).", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7170878350734711}, {"text": "information extraction", "start_pos": 155, "end_pos": 177, "type": "TASK", "confidence": 0.7454837411642075}]}, {"text": "Second, the evaluation was of the black box type, i.e. it considered only system input-output relations without regard to the specific mechanisms by which the outputs were obtained.", "labels": [], "entities": []}, {"text": "Third, the amount of data to be processed was enormous (several gigabytes for TREC).", "labels": [], "entities": []}, {"text": "There is general agreement that these competitive evaluations had a striking and beneficial effect on the performance of the various systems tested over the years.", "labels": [], "entities": []}, {"text": "However, it is also recognized (albeit less generally) that these evaluation experiments also had the, less beneficial, effect that the participating systems focussed increasingly more narrowly on those few parameters that were measured in the evaluation, to the detriment of more general properties.", "labels": [], "entities": []}, {"text": "In some cases this meant that powerful and linguistically interesting but slow systems were dropped in favour of shallow but fast systems with precious little linguistic content.", "labels": [], "entities": []}, {"text": "Thus the system with which SRI participated in the MUC-3 evaluation in 1991, TACITUS, a true text-understanding system, was later replaced by FASTUS (), a much simpler, and vastly faster, information extraction system.", "labels": [], "entities": [{"text": "MUC-3 evaluation in 1991", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.5556913018226624}, {"text": "TACITUS", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.9729094505310059}, {"text": "FASTUS", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.8779428601264954}, {"text": "information extraction", "start_pos": 188, "end_pos": 210, "type": "TASK", "confidence": 0.7484026849269867}]}, {"text": "The reason was that TACITUS was spending so much of its time attempting to make sense of portions of the text that were irrelevant to the task that recall was mediocre.", "labels": [], "entities": [{"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9962736368179321}]}, {"text": "We argue that the set-up of these competitive evaluations, and in particular the three parameters mentioned above, drove the development of the participating systems towards becoming impres-sive feats of engineering, fine-tuned to one very specific task, but with limited relevance outside this task and with little linguistically relevant content.", "labels": [], "entities": []}, {"text": "We argue that these evaluations therefore did not drive progress in Computational Linguistics very much.", "labels": [], "entities": [{"text": "Computational Linguistics", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.8267257809638977}]}, {"text": "We therefore think it a timely idea to conceive of evaluation methodologies which measure the linguistically relevant functions of NLP systems and thus advance Computational Linguistics as a science rather than as an engineering discipline.", "labels": [], "entities": []}, {"text": "The suggestion made by the organizers of this workshop on how this could be achieved has-four comPonents.", "labels": [], "entities": []}, {"text": "First, they suggest to use full-fledged text-based question answering (QA) as task.", "labels": [], "entities": [{"text": "full-fledged text-based question answering (QA)", "start_pos": 27, "end_pos": 74, "type": "TASK", "confidence": 0.7078059273106712}]}, {"text": "Second, they suggest a relatively small amount off text (compared with the volumes of text used in TREC) as test data.", "labels": [], "entities": []}, {"text": "Third they (seem to) suggest to .use texts from a wide range off domains.", "labels": [], "entities": []}, {"text": "Finally they suggest to use pre-existing question/answer pairs, developed for and tested on humans, as evaluation benchmark).", "labels": [], "entities": []}, {"text": "However, our experience in the field leads us to believe that this evaluation set-up will not help Computational Linguistics as much as it would be needed, mainly because it is way too ambitious.", "labels": [], "entities": [{"text": "Computational Linguistics", "start_pos": 99, "end_pos": 124, "type": "TASK", "confidence": 0.8297356367111206}]}, {"text": "We fear that this fact will force developers, again, to design all kinds of ad-hoc solutions and efficiency hacks which will severely limit the scientific relevance of the resulting systems.", "labels": [], "entities": []}, {"text": "We argue that three of the four components of the suggested set-up must be reduced considerably in scope to make the test-bed helpful.", "labels": [], "entities": []}, {"text": "First, we think the task is too difficult.", "labels": [], "entities": []}, {"text": "Fullfledged QA on the basis of natural language texts is far beyond the present state of the art.", "labels": [], "entities": [{"text": "Fullfledged QA", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7089837789535522}]}, {"text": "The example of the text-based QA system LILOG has shown that the analysis of texts to the depth required for real QA over their contents is so resource intensive as to be unaffordable in any real world context.", "labels": [], "entities": []}, {"text": "After an investment of around 65 person-years of work the LILOG system could answer questions over a few (reputedly merely three) texts of around one page length each from an extremely narrow domain (city guides and the like).", "labels": [], "entities": []}, {"text": "We think it is fair to say that the situation in our field has not changed enough in the meantime to invalidate this finding.", "labels": [], "entities": []}, {"text": "Second, we agree that the volume off data to be used should be relatively small.", "labels": [], "entities": []}, {"text": "We must avoid that the sheer pressure of the volumes of texts to be processed forces system developers to use shallow methods.", "labels": [], "entities": []}, {"text": "Third, we think it is very important to restrict the domain of the task.", "labels": [], "entities": []}, {"text": "We certainly do not argue in favour of some toy domain but we get the impression that the reading comprehension texts under consideration cover afar too wide range of topics.", "labels": [], "entities": []}, {"text": "We think that technical manuals area better choice.", "labels": [], "entities": []}, {"text": "They cover a narrow domain (such as computer operating systems, or airplanes), and they also use a relatively restricted type of language with a reasonably clear semantic foundation.", "labels": [], "entities": []}, {"text": "Fourth, we think that tests that are specifically designed to evaluate to what extent a human being understands a text are intrinsically unsuitable for our present purposes.", "labels": [], "entities": []}, {"text": "Although it would admittedly be very convenient to have \"well written\" texts, \"good\" questions about them and the \"correct\" answers all in one package, the texts are not \"real world\" language (in that they were written specifically for these tests), and the questions are:just far too difficult, primarily because they rely on exactly those components of language understanding where humans excel and computers are abominably poor (inferences over world knowledge).", "labels": [], "entities": []}, {"text": "In Section 2 we outline what kinds of problems would have to be solved by a QA system if it were to answer the test questions given in).", "labels": [], "entities": []}, {"text": "Most of the problems would require enormous amounts of world knowledge and vast numbers of lexical inference rules fora solution, on top of all the \"classical\" linguistic problems our field has been struggling with (ambiguities, anaphoric references, synonymy/hyponymy).", "labels": [], "entities": []}, {"text": "We will then argue in Section 3 that a more restricted kind of task, Answer Extraction, is better suited as experimental set-up as it would focus our forces on these unsolved but reasonably well-understood problems, rather than divert them to the illunderstood and fathomless black' hole of world knowledge.", "labels": [], "entities": [{"text": "Answer Extraction", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.9811791777610779}]}, {"text": "In Section 4, we will finally outline how evaluation procedures in this context might look like.", "labels": [], "entities": []}, {"text": "Reading comprehension tests are designed to measure how well human readers understand what they read.", "labels": [], "entities": []}, {"text": "Each story comes with a set of questions about information that is stated or implied in the text.", "labels": [], "entities": []}, {"text": "The readers demonstrate their understanding of the story by answering the questions about it.", "labels": [], "entities": []}, {"text": "Thus, reading comprehension tests assume a cognitive process of human beings.", "labels": [], "entities": []}, {"text": "This process involves expanding the mental model of a text by using its implications and presuppositions, retrieving the stored information, performing inferences to make implicit information explicit, and generating the surface strings that express this information.", "labels": [], "entities": []}, {"text": "Many different forms of knowledge take part in this process: linguistic, procedural and world knowledge.", "labels": [], "entities": []}, {"text": "All these forms coalesce in the memory of the reader and it is very difficult to clearly distinguish and reconstruct them in a QA system.", "labels": [], "entities": []}, {"text": "At first sight the story published in) is easy to understand because the sentences are short and cohesive.", "labels": [], "entities": []}, {"text": "But it turns out that a classic QA system would need vast amounts of knowledge and inference rules in order to understand the text and to give sensible answers.", "labels": [], "entities": []}, {"text": "Let us investigate what kind of information a full-fledged QA system needs in order to answer the questions that come with the reading comprehension test) and discuss how difficult it is to provide this information.", "labels": [], "entities": []}, {"text": "To answer the first question (1) Who collects maple sap?", "labels": [], "entities": []}, {"text": "the system needs to know that the mass noun sap in the text sentence is indeed the maple sap mentioned in the question.", "labels": [], "entities": []}, {"text": "The compound noun maple sap is a semantically narrower term than the noun sap and encodes an implicit relation between the first element maple and the head noun sap.", "labels": [], "entities": []}, {"text": "This relation names the origin of the material.", "labels": [], "entities": []}, {"text": "Since no explicit information about the relation between the two objects is available in the text an ideal QA system would have to assume such a relation by a form of abductive reasoning.", "labels": [], "entities": []}], "tableCaptions": []}