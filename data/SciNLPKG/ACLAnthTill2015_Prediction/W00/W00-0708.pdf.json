{"title": [], "abstractContent": [{"text": "Article choice can pose difficult problems in applications such as machine translation and automated summarization.", "labels": [], "entities": [{"text": "Article choice", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8434033393859863}, {"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.8309803009033203}, {"text": "summarization", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.7578280568122864}]}, {"text": "In this paper, we investigate the use of corpus data to collect statistical generalizations about article use in English in order to be able to generate articles automatically to supplement a symbolic generator.", "labels": [], "entities": []}, {"text": "We use data from the Penn Treebank as input to a memory-based learner (TiMBL 3.0; Daelemans et al., 2000) which predicts whether to generate an article with respect to an English base noun phrase.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9943715631961823}]}, {"text": "We discuss competitive results obtained using a variety of lexical, syntactic and semantic features that play an important role in automated article generation.", "labels": [], "entities": [{"text": "automated article generation", "start_pos": 131, "end_pos": 159, "type": "TASK", "confidence": 0.5984803736209869}]}], "introductionContent": [{"text": "Article choice can pose difficult problems in natural language applications.", "labels": [], "entities": [{"text": "Article choice", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.851813793182373}]}, {"text": "Machine translation (MT) is an example of such an application.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8986644506454468}]}, {"text": "When translating from a source language that lacks articles, such as Japanese or Russian, to one that requires them, such as English or German, the system must somehow generate the source language articles.", "labels": [], "entities": []}, {"text": "Similarly in automated summarization: when sentences or fragments are combined or reduced, it is possible that the form of a noun phrase (NP) is changed such that a change of the article associated with the NP's head becomes necessary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.7009155750274658}]}, {"text": "For example, consider the sentences A talk will be given on Friday about NLP; The talk will last .for one hour which might get summarized as Friday's NLP talk will last one * Visiting CSLI, Stanford.", "labels": [], "entities": []}, {"text": "t Visiting CSLI, Stanford University hour.", "labels": [], "entities": [{"text": "Stanford University hour", "start_pos": 17, "end_pos": 41, "type": "DATASET", "confidence": 0.6717242300510406}]}, {"text": "However, given the input sentences, it is not clear how to decide not to generate an article for the subject NP in the output sentence.", "labels": [], "entities": []}, {"text": "Another important application is in the field known as augmentative and alternative communication (AAC).", "labels": [], "entities": [{"text": "augmentative and alternative communication (AAC)", "start_pos": 55, "end_pos": 103, "type": "TASK", "confidence": 0.7509800834315163}]}, {"text": "In particular, people who have lost the ability to speak sometimes use a text-to-speech generator as a prosthetic device.", "labels": [], "entities": []}, {"text": "But most disabilities which affect speech, such as stroke or amyotrophic lateral sclerosis (ALS or Lou Gehrig's disease), also cause some more general motor impairment, which means that prosthesis users cannot achieve a text input rate comparable to normal typing speeds even if they are able to use a keyboard.", "labels": [], "entities": []}, {"text": "Many have to rely on a slower physical interface (headstick, head-pointer, eye-tracker etc).", "labels": [], "entities": []}, {"text": "We are attempting to use a range of NLP technology to improve text input speed for such users.", "labels": [], "entities": []}, {"text": "Article choice is particularly important for this application: many AAC users drop articles and resort to a sort of telegraphese, but this causes degradation in comprehension of synthetic speech and contributes to its perception as unnatural and robot-like.", "labels": [], "entities": [{"text": "Article choice", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7714877128601074}]}, {"text": "Our particular goal is to be able to use an article generator in conjunction with a symbolic generator for AAC.", "labels": [], "entities": []}, {"text": "In this paper we investigate the use of corpus data to collect statistical generalizations about article use in English so as to be able to generate them automatically.", "labels": [], "entities": []}, {"text": "We use data from the Penn Treebank as input to a memory-based learner (TiMBL 3.0;) that is used to predict whether to generate the or alan or no article.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9954594671726227}]}, {"text": "1 We discuss a variety of lexical, syntactic and semantic features that 1We assume a postprocessor to determine whether to generate a or an as described in play an important role in automated article generation, and compare our results with other researchers'.", "labels": [], "entities": [{"text": "automated article generation", "start_pos": 182, "end_pos": 210, "type": "TASK", "confidence": 0.6066734691460928}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 relates our work to that of others.", "labels": [], "entities": []}, {"text": "Section 3 introduces the features we use.", "labels": [], "entities": []}, {"text": "Section 4 introduces the learning method we use.", "labels": [], "entities": []}, {"text": "We discuss our results in Section 5 and suggest some directions for future research, then conclude with some final remarks in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the features discussed in section 3 with respect to a number of different memorybased learning methods as implemented in the TiMBL system (.", "labels": [], "entities": [{"text": "TiMBL system", "start_pos": 135, "end_pos": 147, "type": "DATASET", "confidence": 0.8973194062709808}]}, {"text": "We considered two different learning algorithms.", "labels": [], "entities": []}, {"text": "The first, IB1 is a k-nearest neighbour algorithm.", "labels": [], "entities": [{"text": "IB1", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.5516510009765625}]}, {"text": "3. This can be used with two different metrics to judge the distance between the examples: overlap and modified value difference metric (MVDM).", "labels": [], "entities": [{"text": "overlap", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9866777658462524}, {"text": "modified value difference metric (MVDM)", "start_pos": 103, "end_pos": 142, "type": "METRIC", "confidence": 0.8394245037010738}]}, {"text": "TiMBL automatically learns weights for the features, using one of five different weighting methods: no weighting, gain ratio, information gain, chi-squared and shared variance.", "labels": [], "entities": [{"text": "TiMBL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8190555572509766}]}, {"text": "The second algorithm, IGTREE, stores examples in a tree which is pruned according to the weightings.", "labels": [], "entities": [{"text": "IGTREE", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.6136817336082458}]}, {"text": "This makes it much faster and of comparable accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9983606934547424}]}, {"text": "The results for these different methods, fork = 1, 4, 16 are displayed in.", "labels": [], "entities": []}, {"text": "IB1 is tested with leave-oneout cross-validation, IGTREE with ten-fold cross validation.", "labels": [], "entities": [{"text": "IB1", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7101898193359375}, {"text": "IGTREE", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9783128499984741}]}, {"text": "The best results were (82.6%) for IB1 with the MVDM metric, and either no weighting or weighting by gain ratio.", "labels": [], "entities": [{"text": "IB1", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.7844825983047485}]}, {"text": "IGTREE did not perform as well.", "labels": [], "entities": [{"text": "IGTREE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.64084392786026}]}, {"text": "We investigated more values of k, from 1 to 200, and found they had little influence on the accuracy results with k = 4 or 5 performing slightly better.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9995155334472656}]}, {"text": "We also tested each of the features described in Section 3 in isolation and then all together.", "labels": [], "entities": []}, {"text": "We used the best performing algorithm from our earlier experiment: IB1 with MVDM, gain ratio and k = 4.", "labels": [], "entities": []}, {"text": "The results of this are given in.", "labels": [], "entities": []}, {"text": "When interpreting these results it is important to recall the figures provided in.", "labels": [], "entities": []}, {"text": "The most common article, for any PoS, was no and for many PoS, including pronouns, generating no article is always correct.", "labels": [], "entities": []}, {"text": "There is more variation in NPs headed by common nouns and adjectives, and a little in NPs headed by proper nouns.", "labels": [], "entities": []}, {"text": "Our baseline therefore consists of never 3Strictly speaking, it is a k nearest distance algorithm, which looks at all examples in the nearest k distances, the number of which maybe greater thank.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of NP instances in Wall  Street Journal data (300,744 NPs in all)", "labels": [], "entities": [{"text": "Wall  Street Journal data", "start_pos": 42, "end_pos": 67, "type": "DATASET", "confidence": 0.9566527754068375}]}, {"text": " Table 5. Interestingly,  features which were not useful on their own,  proved useful in combination with the head  noun. The most useful features appear to be the  category of the embedding constituent (81.1%)  and the presence or absence of a determiner  (80.9%). Combining all the features gave an  accuracy of 82.9%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 302, "end_pos": 310, "type": "METRIC", "confidence": 0.9994888305664062}]}, {"text": " Table 5: Accuracy with combined features", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991021156311035}]}, {"text": " Table 3: Accuracy results broken down with respect to memory-based learning methods used", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999273955821991}]}, {"text": " Table 6: Accuracy versus Size of Training Data", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9979555606842041}, {"text": "Size of Training", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.835783064365387}]}]}