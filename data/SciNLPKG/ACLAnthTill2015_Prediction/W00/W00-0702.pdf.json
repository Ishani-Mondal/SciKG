{"title": [], "abstractContent": [{"text": "Broad-coverage grammars tend to be highly ambiguous.", "labels": [], "entities": []}, {"text": "When such grammars are used in a restricted domain, it maybe desirable to specialize them, in effect trading some coverage fora reduction in ambiguity.", "labels": [], "entities": []}, {"text": "Grammar specialization is here given a novel formulation as an optimization problem, in which the search is guided by a global measure combining coverage, ambiguity and grammar size.", "labels": [], "entities": [{"text": "Grammar specialization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7892928421497345}]}, {"text": "The method, applicable to any unification grammar with a phrase-structure backbone, is shown to be effective in specializing a broad-coverage LFG for French.", "labels": [], "entities": []}], "introductionContent": [{"text": "Expressive grammar formalisms allow grammar developers to capture complex linguistic generalizations concisely and elegantly, thus greatly facilitating grammar development and maintenance.", "labels": [], "entities": []}, {"text": "Broad-coverage grammars, however, tend to overgenerate considerably, thus allowing large amounts of spurious ambiguity.", "labels": [], "entities": []}, {"text": "If the benefits resulting from more concise grammatical descriptions are to outweigh the costs of spurious ambiguity, the latter must be brought down.", "labels": [], "entities": []}, {"text": "We here investigate a corpus-based compilation technique that reduces overgeneration and spurious ambiguity without jeopardizing coverage or burdening the grammar developer.", "labels": [], "entities": []}, {"text": "The current work extends previous work on corpus-based grammar specialization, which applies variants of explanation-based learning (EBL) to grammars of natural languages.", "labels": [], "entities": [{"text": "corpus-based grammar specialization", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.6912870605786642}]}, {"text": "The earliest work) builds a specialized grammar by chunking together grammar rule combinations while parsing training examples.", "labels": [], "entities": []}, {"text": "What rules to combine is specified by hand-coded criteria.", "labels": [], "entities": []}, {"text": "Subsequent work views the problem as that of cutting up each tree in a treebank of correct parse trees into subtrees, after which the rule combinations corresponding to the subtrees determine the rules of the specialized grammar.", "labels": [], "entities": []}, {"text": "This approach reports experimental results, using the SRI Core Language Engine,, in the ATIS domain, of more than a 3-fold speedup at a cost of 5% in grammatical coverage, the latter which is compensated by an increase in parsing accuracy.", "labels": [], "entities": [{"text": "SRI Core Language Engine", "start_pos": 54, "end_pos": 78, "type": "DATASET", "confidence": 0.88812655210495}, {"text": "ATIS domain", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.8935351967811584}, {"text": "parsing", "start_pos": 222, "end_pos": 229, "type": "TASK", "confidence": 0.9481056332588196}, {"text": "accuracy", "start_pos": 230, "end_pos": 238, "type": "METRIC", "confidence": 0.8258330225944519}]}, {"text": "Later work) attempts to automatically determine appropriate tree-cutting criteria, the former using local measures, the latter using global ones.", "labels": [], "entities": []}, {"text": "The current work reverts to the view of EBL as chunking grammar rules.", "labels": [], "entities": []}, {"text": "It extends the latter work by formulating grammar specialization as a global optimization problem over the space of all possible specialized grammars with an objective function based on the coverage, ambiguity and size of the resulting grammar.", "labels": [], "entities": [{"text": "formulating grammar specialization", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.8845963478088379}]}, {"text": "The method was evaluated on the LFG grammar for French developed within the PAR-GRAM project), but it is applicable to any unification grammar with a phrase-structure backbone where the reference treebank contains all possible analyses for each training example, along with an indication of which one is the correct one.", "labels": [], "entities": [{"text": "LFG grammar", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.8904694318771362}]}, {"text": "To explore the space of possible grammars, a special treebank representation was developed, called a ]folded treebank, which allows the objective function to be computed very efficiently for each candidate grammar.", "labels": [], "entities": []}, {"text": "This representation relies on the fact that all possible parses returned by the original grammar for each training sentence axe available and the fact that the grammar specialization never introduces new parses; it only removes existing ones.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 describes the initial candidate grammar and the operators used to generate new candidate grammars from any given one.", "labels": [], "entities": []}, {"text": "The function to be maximized is introduced and motivated in Section 3.", "labels": [], "entities": []}, {"text": "The folded treebank representation is described in Section 4, while Section 5 presents the experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We specialized a broad-coverage LFG grammar for French on a corpus of technical documentation using the method described above.", "labels": [], "entities": []}, {"text": "The treebank consisted of 960 sentences which were all known to be covered by the original grammar.", "labels": [], "entities": []}, {"text": "For each sentence, all the trees returned by the original grammar were available, together with a manually assigned indication of which was the correct one.", "labels": [], "entities": []}, {"text": "The environment used, the Xerox Linguistic Environment () implements aversion of optimality theory where parses are assigned \"optimality marks\" based on a number of criteria, and are ranked according to these marks.", "labels": [], "entities": [{"text": "Xerox Linguistic Environment", "start_pos": 26, "end_pos": 54, "type": "DATASET", "confidence": 0.8240259885787964}]}, {"text": "The set of parses with the best marks are called the optimal parses fora sentence.", "labels": [], "entities": []}, {"text": "The correct parse was also an optimal parse for 913 out of 960 sentences.", "labels": [], "entities": []}, {"text": "Given this, the specialization was aimed at reducing the number of optimal parses per sentence.", "labels": [], "entities": []}, {"text": "We ran a series of ten-fold cross-validation experiments; the results are summarized in the table in.", "labels": [], "entities": []}, {"text": "The first line contains values for the original grammar.", "labels": [], "entities": []}, {"text": "The second line contains measures for the first-order pruning grammar, i.e., the grammar with all and only those rules actually used incorrect parses in the training set, with no combination inhibited.", "labels": [], "entities": []}, {"text": "Lines 3 and 4 list results for fully specialized grammars.", "labels": [], "entities": []}, {"text": "Results in the third line were obtained with a value for ~corr equal to 15 times the value of Ainc in the objective function: in other words, during training we were willing to lose a correct parse only if at least 15 incorrect parses were canceled as well.", "labels": [], "entities": []}, {"text": "Results in the fourth line were obtained when this ratio was reduced to 10.", "labels": [], "entities": []}, {"text": "The average number of parses per sentence is reported in the first column, whereas the second lists the average number of optimal parses.", "labels": [], "entities": []}, {"text": "Coverage was measured as the fraction of sentences which still receive the correct parse with the specialized grammar.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.893887460231781}]}, {"text": "To assess the trade off between coverage and ambiguity reduction, we computed the F-score 4 considering only optimal parses when computing precision.", "labels": [], "entities": [{"text": "coverage", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9633175730705261}, {"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9982155561447144}, {"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9964497089385986}]}, {"text": "This measure should not be confused with the F-score on labelled bracketing reported for many stochastic parsers; here precision and recall concern perfect matching of whole trees.", "labels": [], "entities": [{"text": "F-score", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9955606460571289}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9994163513183594}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.999091625213623}]}, {"text": "Recall is the same as coverage: the ratio between the number of correct parses produced by the specialized grammar and the total number of correct parses (equalling the total number of sentences in the test set).", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9886853694915771}, {"text": "coverage", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9951400756835938}]}, {"text": "Precision is the ratio between the number of correct parses produced by the specialized grammar and the total number of parses produced by the same grammar.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9903302192687988}]}, {"text": "The fourth column lists values for the F-score when equal weight is given to precision and recall.", "labels": [], "entities": [{"text": "F-score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9989019632339478}, {"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9995943903923035}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9982522130012512}]}, {"text": "Intuitively, however, in many cases missing the correct parse is more of a problem than returning spurious parses, so we also computed the F-score with a much larger emphasis on recall, i.e., with a = 0.1.", "labels": [], "entities": [{"text": "F-score", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.9943861961364746}, {"text": "recall", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.998934805393219}]}, {"text": "The corresponding values are listed in the last column.", "labels": [], "entities": []}, {"text": "The average number of parses per sentence, both optimal and non-optimal, decreases significantly as more and more aggressive specialization.is carried out, and consequently, more coverage is lost.", "labels": [], "entities": []}, {"text": "The most aggressive form of spe-: Results of the cialization gives the highest F-score for c~ = 0.5, whereas somewhat more conservative parameter settings lead to a better F-score when recall is valued more.", "labels": [], "entities": [{"text": "F-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9988177418708801}, {"text": "F-score", "start_pos": 172, "end_pos": 179, "type": "METRIC", "confidence": 0.9978930354118347}, {"text": "recall", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.9973145127296448}]}, {"text": "A speedup of a factor 4 is achieved already by first-order pruning and remains approximately the same after further specialization.", "labels": [], "entities": []}], "tableCaptions": []}