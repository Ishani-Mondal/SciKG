{"title": [{"text": "Stochastic Finite-State models for Spoken Language Machine ': anslation", "labels": [], "entities": [{"text": "anslation", "start_pos": 62, "end_pos": 71, "type": "TASK", "confidence": 0.7713561654090881}]}], "abstractContent": [{"text": "Stochastic finite-state models are efficiently learn-able from data, effective for decoding and are associated with a calculus for composing models which allows for tight integration of constraints frora various levels of language processing.", "labels": [], "entities": []}, {"text": "In this paper, we present a method for stochastic finite-state machine translation that is trained automatically from pairs of source and target utterances.", "labels": [], "entities": [{"text": "stochastic finite-state machine translation", "start_pos": 39, "end_pos": 82, "type": "TASK", "confidence": 0.6177147179841995}]}, {"text": "We use this method to develop models for English-Japanese and Japanese-English translation.", "labels": [], "entities": [{"text": "Japanese-English translation", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.6843869537115097}]}, {"text": "We have embedded the Japanese-English translation system in a call routing task of unconstrained speech utterances.", "labels": [], "entities": [{"text": "Japanese-English translation", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.60306017100811}, {"text": "call routing", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.8088635206222534}]}, {"text": "We evaluate the efficacy of the translation system :in the context of this application.", "labels": [], "entities": []}], "introductionContent": [{"text": "Finite state models have been extensively applied to many aspects of language processing including, speech recognition, phonology, morphology, chunking and parsing (Roche, 1.999).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7559743821620941}]}, {"text": "Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.", "labels": [], "entities": []}, {"text": "I In this paper, we develop stochastic finite-state models (SFSM) for statistical machine translation (SMT) and explore the performance limits of such models in the context of translation in limited domains.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 70, "end_pos": 107, "type": "TASK", "confidence": 0.8205651293198267}]}, {"text": "We are also interested in these models since they allow fora tight integration with a speech recognizer for speech-to-speech translation.", "labels": [], "entities": [{"text": "speech-to-speech translation", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.7063146978616714}]}, {"text": "In particular we are interested in one-pass decoding and translation of speech as opposed to the more prevalent approach of translation of speech lattices.", "labels": [], "entities": [{"text": "translation of speech", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.8900975584983826}, {"text": "translation of speech lattices", "start_pos": 124, "end_pos": 154, "type": "TASK", "confidence": 0.8219833970069885}]}, {"text": "The problem of machine translation can be viewed as consisting of two phases: (a) lexical choice phase 1 Furthermore, software implementing the finite-stal;e calculus is available for research purposes.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8111832737922668}]}, {"text": "where appropriate target language lexical items are chosen for each source language lexical item and (b) reordering phase where the chosen target language lexical items are reordered to produce a meaningful target language string.", "labels": [], "entities": []}, {"text": "In our approach, we will represent these two phases using stochastic finitestate models which can be composed together to result in a single stochastic finite-state model for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 175, "end_pos": 178, "type": "TASK", "confidence": 0.9932215213775635}]}, {"text": "Thus our method can be viewed as a direct translation approach of transducing strings of the source language to strings of the target language.", "labels": [], "entities": []}, {"text": "There are other approaches to statistical machine translation where translation is achieved through transduction of source language structure to target language structure (.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.6484570602575938}]}, {"text": "There are also large international multi-site projects such as VERBMOBIL) and CSTAR () that are involved in speech-to-speech translation in limited domains.", "labels": [], "entities": [{"text": "VERBMOBIL", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.6535632610321045}, {"text": "speech-to-speech translation", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.7041844427585602}]}, {"text": "The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean.", "labels": [], "entities": []}, {"text": "Finite-state models for SMT have been previously suggested in the literature ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9965349435806274}]}, {"text": "In (), a deterministic transducer is used to implement an English-Spanish speech translation system.", "labels": [], "entities": [{"text": "English-Spanish speech translation", "start_pos": 58, "end_pos": 92, "type": "TASK", "confidence": 0.6776287953058878}]}, {"text": "In), finite-state machine translation is based on ( and is used for decoding the target language string.", "labels": [], "entities": [{"text": "finite-state machine translation", "start_pos": 5, "end_pos": 37, "type": "TASK", "confidence": 0.6617195804913839}]}, {"text": "However, no experimental results are reported using this approach.", "labels": [], "entities": []}, {"text": "\u2022 Our approach differs from the previous approaches in both the lexical choice and the reordering phases.", "labels": [], "entities": []}, {"text": "Unlike the previous approaches, the lexical choice phase in our approach is decomposed into phraselevel and sentence-level translation models.", "labels": [], "entities": [{"text": "sentence-level translation", "start_pos": 108, "end_pos": 134, "type": "TASK", "confidence": 0.7059315741062164}]}, {"text": "The phrase-level translation is learned based on joint entropy reduction of the source and target languages and a variable length n-gram model (VNSA) () is learrmd for the sentence-level translation.", "labels": [], "entities": [{"text": "phrase-level translation", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7518211007118225}, {"text": "sentence-level translation", "start_pos": 172, "end_pos": 198, "type": "TASK", "confidence": 0.6974346935749054}]}, {"text": "For the construc-tion of the bilingual lexicon needed for lexical choice, we use the alignment algorithm presented in) which takes advantage of hierarchical decomposition of strings and thus performs a structure-based alignment.", "labels": [], "entities": []}, {"text": "In the previous approaches, a bilingual lexicon is constructed using a string-based alignment.", "labels": [], "entities": []}, {"text": "Another difference between our approach and the previous approaches is in the reordering of the target language lexical items.", "labels": [], "entities": []}, {"text": "In), an FSM that represents all strings resulting from the permutations of the lexical items produced by lexical choice is constructed and the most likely translation is retrieved using a target language model.", "labels": [], "entities": []}, {"text": "In), the lexical items are associated with markers that allow for reconstruction of the target language string.", "labels": [], "entities": []}, {"text": "Our reordering step is similar to that proposed in (Knight and A1-Onalzan, 1998) but does not incur the expense of creating a permutation lattice.", "labels": [], "entities": []}, {"text": "We use a phrase-based VNSA target language model to retrieve the most likely translation from the lattice.", "labels": [], "entities": []}, {"text": "In addition, we have used the resulting finitestate translation method to implement an EnglishJapanese speech and text translation system and a Japanese-English text translation system.", "labels": [], "entities": [{"text": "EnglishJapanese speech and text translation", "start_pos": 87, "end_pos": 130, "type": "TASK", "confidence": 0.5719104290008545}, {"text": "Japanese-English text translation", "start_pos": 144, "end_pos": 177, "type": "TASK", "confidence": 0.6288946270942688}]}, {"text": "We present evaluation results for these systems and discuss their limitations.", "labels": [], "entities": []}, {"text": "We also evaluate the efficacy of this translation model in the context of a telecom application such as call routing.", "labels": [], "entities": [{"text": "call routing", "start_pos": 104, "end_pos": 116, "type": "TASK", "confidence": 0.8196814954280853}]}, {"text": "The layout of the paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we discuss the architecture of the finite-state translation system.", "labels": [], "entities": [{"text": "finite-state translation", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.6766599118709564}]}, {"text": "We discuss the algorithm for learning lexical and phrasal translation in Section 3.", "labels": [], "entities": [{"text": "learning lexical and phrasal translation", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.6912770628929138}]}, {"text": "The details of the translation model are presented in Section 4 and our method for reordering the output is presented in Section 5.", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9641820192337036}]}, {"text": "In Section 6 we discuss the call classification application and present motivations for embedding translation in such an application.", "labels": [], "entities": [{"text": "call classification", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8911421597003937}]}, {"text": "In Section 6.1 we present the experiments and evaluation results for the various translation systems on text input.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we discuss issues concerning evaluation of the translation system.", "labels": [], "entities": [{"text": "translation", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9689732789993286}]}, {"text": "The data for the experiments reported in this section were obtained from the customer side of operator-customer conversations, with the customer-caxe application described above and detailed in ().", "labels": [], "entities": []}, {"text": "Each of the customer's utterance transcriptions were then manually translated into Japanese.", "labels": [], "entities": []}, {"text": "A total of 15,457 EnglishJapanese sentence pairs was split into 12,204 training sentence pairs and 3,253 test sentence pairs.", "labels": [], "entities": []}, {"text": "The objective of this experiment is to measure the performance of a translation system in the context of an application.", "labels": [], "entities": []}, {"text": "In an automated call router there axe two important performance measures.", "labels": [], "entities": []}, {"text": "The first is the probability of false rejection, where a call is falsely rejected.", "labels": [], "entities": []}, {"text": "Since such calls would be transferred to a human agent, this corresponds to a missed opportunity for automation.", "labels": [], "entities": []}, {"text": "The second measure is the probability of correct classification.", "labels": [], "entities": [{"text": "correct classification", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.4308314025402069}]}, {"text": "Errors in this dimension lead to misinterpretations that must be resolved by a dialog manager).", "labels": [], "entities": []}, {"text": "Using our approach described in the previous sections, we have trained a unigram, bigram and trigram VNSA based translation models with and without phrases.", "labels": [], "entities": []}, {"text": "shows lexical choice (bagof-tokens) accuracy for these different translation models measured in terms of recall, precision and F-measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9979189038276672}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9992473125457764}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9992197751998901}, {"text": "F-measure", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9961406588554382}]}, {"text": "In order to measure the effectiveness of our translation models for this task we classify Japanese utterances based on their English translations.", "labels": [], "entities": []}, {"text": "plots the false rejection rate against the correct classification rate of the classifier on the English generated by three different Japanese to English translation models for the set of Japanese test sentences.", "labels": [], "entities": [{"text": "false rejection rate", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8258221546808878}]}, {"text": "The figure also shows the performance of the classifier using the correct English text as input.", "labels": [], "entities": []}, {"text": "There area few interesting observations to be made from the.", "labels": [], "entities": []}, {"text": "Firstly, the task performance on the text data is asymptotically similar to the task performance on the translation output.", "labels": [], "entities": []}, {"text": "In other words, the system performance is not significantly affected by the translation process; a Japanese transcription would most often be associated with the same call type after translation as if the original were English.", "labels": [], "entities": []}, {"text": "This result is particularly interesting inspite of the impoverished reordering phase of the target language words.", "labels": [], "entities": []}, {"text": "We believe that this result is due to the nature of the application where the classifier is mostly relying on the existence of certain key words and phrases, not necessarily in any particular order.", "labels": [], "entities": []}, {"text": "The task performance improved from the unigram-based translation model to phrase unigrambased translation model corresponding to the improvement in the lexical choice accuracy in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.831256628036499}]}, {"text": "Also, at higher false rejection rates, the task performance is better for trigram-based translation model than the phrase trigram-based translation model since the precision of lexical choice is better than that of the phrase trigram-based model as shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.9987533092498779}]}, {"text": "This difference narrows at lower false rejection rate.", "labels": [], "entities": [{"text": "false rejection rate", "start_pos": 33, "end_pos": 53, "type": "METRIC", "confidence": 0.7666763265927633}]}, {"text": "We are currently working on evaluating the translation system in an application independent method and developing improved models of reordering needed for better translation system.", "labels": [], "entities": [{"text": "translation", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.9702038764953613}]}], "tableCaptions": [{"text": " Table 1: Example bitexts and with alignment information", "labels": [], "entities": []}, {"text": " Table 2: Bilanguage strings resulting from alignments shown in Table 1.  (%EPS% represents the null symbol c).", "labels": [], "entities": []}, {"text": " Table 3: Lexical choice accuracy of the Japanese to English Translation System with and without phrases", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9640084505081177}, {"text": "Japanese to English Translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.5308566763997078}]}]}