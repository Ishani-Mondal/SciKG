{"title": [{"text": "One Sense per Collocation and Genre/Topic Variations", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper revisits the one sense per collocation hypothesis using fine-grained sense distinctions and two different corpora.", "labels": [], "entities": []}, {"text": "We show that the hypothesis is weaker for fine-grained sense distinctions (70% vs. 99% reported earlier on 2-way ambiguities).", "labels": [], "entities": []}, {"text": "We also show that one sense per collocation does hold across corpora, but that collocations vary from one corpus to the other, following genre and topic variations.", "labels": [], "entities": []}, {"text": "This explains the low results when performing word sense disambiguation across corpora.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.7058068613211314}]}, {"text": "In fact, we demonstrate that when two independent corpora share a related genre/topic, the word sense disambiguation results would be better.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.5580805341402689}]}, {"text": "Future work on word sense disambiguation will have to take into account genre and topic as important parameters on their models.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.7589871088663737}]}], "introductionContent": [{"text": "In the early nineties two famous papers claimed that the behavior of word senses in texts adhered to two principles: one sense per discourse () and one sense per collocation.", "labels": [], "entities": []}, {"text": "These hypotheses were shown to hold for some particular corpora (totaling 380 Mwords) on words with 2-way ambiguity.", "labels": [], "entities": []}, {"text": "The word sense distinctions came from different sources (translations into French, homophones, homographs, pseudo-words, etc.), but no dictionary or lexical resource was linked to them.", "labels": [], "entities": [{"text": "word sense distinctions", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.6513782640298208}]}, {"text": "In the case of the one sense per collocation paper, several corpora were used, but nothing is said on whether the collocations hold across corpora.", "labels": [], "entities": []}, {"text": "Since the papers were published, word sense disambiguation has moved to deal with finegrained sense distinctions from widely recognized semantic lexical resources; ontologies like Sensus, Cyc, EDR, WordNet, EuroWordNet, etc. or machine-readable dictionaries like OALDC, Webster's, LDOCE, etc.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.7892091274261475}, {"text": "EuroWordNet", "start_pos": 207, "end_pos": 218, "type": "DATASET", "confidence": 0.8005620837211609}]}, {"text": "This is due, in part, to the availability of public hand-tagged material, e.g. SemCor () and the DSO collection.", "labels": [], "entities": [{"text": "DSO collection", "start_pos": 97, "end_pos": 111, "type": "DATASET", "confidence": 0.9860589802265167}]}, {"text": "We think that the old hypotheses should be tested under the conditions of this newly available data.", "labels": [], "entities": []}, {"text": "This paper focuses on the DSO collection, which was tagged with WordNet senses) and comprises sentences extracted from two different corpora: the balanced Brown Corpus and the Wall Street Journal corpus. has shown that the one sense per discourse hypothesis does not hold for finegrained senses in SemCor and DSO.", "labels": [], "entities": [{"text": "DSO collection", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.8589602708816528}, {"text": "Brown Corpus", "start_pos": 155, "end_pos": 167, "type": "DATASET", "confidence": 0.789601594209671}, {"text": "Wall Street Journal corpus.", "start_pos": 176, "end_pos": 203, "type": "DATASET", "confidence": 0.9351030141115189}]}, {"text": "His results have been confirmed in our own experiments.", "labels": [], "entities": []}, {"text": "We will therefore concentrate on the one sense per collocation hypothesis, considering these two questions: \u2022 Does the collocation hypothesis hold across corpora, that is, across genre and topic variations (compared to a single corpus, probably with little genre and topic variations)?", "labels": [], "entities": []}, {"text": "\u2022 Does the collocation hypothesis hold for freegrained sense distinctions (compared to homograph level granularity)?", "labels": [], "entities": []}, {"text": "The experimental tools to test the hypothesis will be decision lists based on various kinds of collocational information.", "labels": [], "entities": []}, {"text": "We will compare the performance across several corpora (the Brown Corpus and Wall Street Journal parts of the DSO collection), and also across different sections of the Brown Corpus, selected according to the genre and topics covered.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.9958365261554718}, {"text": "Wall Street Journal parts of the DSO collection", "start_pos": 77, "end_pos": 124, "type": "DATASET", "confidence": 0.8092592507600784}, {"text": "Brown Corpus", "start_pos": 169, "end_pos": 181, "type": "DATASET", "confidence": 0.9946237206459045}]}, {"text": "We will also perform a direct comparison, using agreement statistics, of the collocations used and of the results obtained.", "labels": [], "entities": []}, {"text": "This study has special significance at this point of word sense disambiguation research.", "labels": [], "entities": [{"text": "word sense disambiguation research", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.8424703627824783}]}, {"text": "A recent study concludes that, for currently available handtagged data, the precision is limited to around 70% when tagging all words in a running text.", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9996439218521118}]}, {"text": "In the course of extending available data, the efforts to use corpora tagged by independent teams of researchers have been shown to fail), as have failed some tuning experiments (, and an attempt to use examples automatically acquired from the Internet).", "labels": [], "entities": []}, {"text": "All these studies obviated the fact that the examples come from different genre and topics.", "labels": [], "entities": []}, {"text": "Future work that takes into account the conclusions drawn in this paper will perhaps be able to automatically extend the number of examples available and tackle the acquisition problem.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "The resources used and the experimental settings are presented first.", "labels": [], "entities": []}, {"text": "Section 3 presents the collocations considered and Section 4 explains how decision lists have been adapted to n-way ambiguities.", "labels": [], "entities": []}, {"text": "Sections 5 and 6 show the incorpus and cross-corpora experiments, respectively.", "labels": [], "entities": []}, {"text": "Section 7 discusses the effect of drawing training and testing data from the same documents.", "labels": [], "entities": []}, {"text": "Section 8 evaluates the impact of genre and topic variations, which is fiarther discussed in Section 9.", "labels": [], "entities": []}, {"text": "Finally, Section 10 presents some conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to analyze and compare the behavior of several kinds of collocations (cf. Section 3),", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data for selected words. Part of", "labels": [], "entities": []}, {"text": " Table 4: Train on WSJ, tag WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.9711499810218811}, {"text": "WSJ", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.696760892868042}]}, {"text": " Table 5: Train on BC, tag BC.", "labels": [], "entities": [{"text": "BC", "start_pos": 19, "end_pos": 21, "type": "DATASET", "confidence": 0.7181715369224548}, {"text": "tag", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.8092507719993591}, {"text": "BC", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.4862896800041199}]}, {"text": " Table 6: Local content-word collocations for", "labels": [], "entities": []}, {"text": " Table 7: Train on BC, tag WSJ", "labels": [], "entities": [{"text": "BC", "start_pos": 19, "end_pos": 21, "type": "DATASET", "confidence": 0.9034499526023865}, {"text": "tag", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.8411959409713745}, {"text": "WSJ", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.6868326663970947}]}, {"text": " Table 8: Train on WSJ, tag BC", "labels": [], "entities": [{"text": "WSJ", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.9641127586364746}, {"text": "tag BC", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.8859560191631317}]}, {"text": " Table 9: Collocations shared and m  contradiction between BC and WSJ.", "labels": [], "entities": [{"text": "BC", "start_pos": 59, "end_pos": 61, "type": "DATASET", "confidence": 0.8191361427307129}, {"text": "WSJ", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.634299635887146}]}, {"text": " Table 10: Contradictory senses of point  way, the testing examples and training examples  are guaranteed to come from different  documents. We also think that this experiment  would show more realistic performance figures,  as a real application can not expect to find  examples from the documents used for training.", "labels": [], "entities": []}, {"text": " Table 11: Train on WSJ, tag WSJ,", "labels": [], "entities": [{"text": "WSJ", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.9766111373901367}, {"text": "WSJ", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.6109347939491272}]}, {"text": " Table 12: Train on BC, tag BC,", "labels": [], "entities": [{"text": "BC", "start_pos": 20, "end_pos": 22, "type": "DATASET", "confidence": 0.6966152191162109}, {"text": "tag", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.8872261643409729}, {"text": "BC", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.8076032996177673}]}, {"text": " Table 13: Overall results in different", "labels": [], "entities": []}, {"text": " Table 14: Tagging different categories in BC.", "labels": [], "entities": [{"text": "Tagging different categories in BC", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.652190750837326}]}]}