{"title": [{"text": "Dialogue Management in the Mercury Flight Reservation System", "labels": [], "entities": [{"text": "Dialogue Management", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8407799303531647}, {"text": "Mercury Flight Reservation", "start_pos": 27, "end_pos": 53, "type": "DATASET", "confidence": 0.8822826544443766}]}], "abstractContent": [{"text": "This paper describes the dialogue module of the Mercury systemewhich has been underdevelopment over the past year or two.", "labels": [], "entities": [{"text": "Mercury systemewhich", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.9412340223789215}]}, {"text": "Mercury provides telephone access to an on-line flight database, and allows users to plan and price itineraries between major airports worldwide.", "labels": [], "entities": [{"text": "Mercury", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8739973902702332}]}, {"text": "The main focus of this paper is the dialogue control strategy, which is based on a set of ordered rules as a mechanism to manage complex dialogue interactions.", "labels": [], "entities": [{"text": "dialogue control strategy", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.8444676597913107}]}, {"text": "The paper also describes the interactions between the dialogue component and the other servers of the system, mediated via a central hub.", "labels": [], "entities": []}, {"text": "We evaluated the system on 49 dialogues from users booking real flights, and report on a number of quantitative measures of the dialogue interaction.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dialogue modeling is a critical and challenging aspect of conversational systems, particularly when users are permitted flexibility with regard to defining the constraints of the task.", "labels": [], "entities": [{"text": "Dialogue modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8137356638908386}]}, {"text": "For systems that adopt a strict system-initiated approach, it is feasible to define a set of states and state transitions depending on the usually small number of possible user actions at each state.", "labels": [], "entities": []}, {"text": "However, if the user is permitted to say anything within the scope of the recognizer at anytime, such a finite-state solution becomes unwieldy.", "labels": [], "entities": []}, {"text": "We are interested in the development of mixed-initiative systems, where the system may make specific requests or suggestions, but the user is not required to be compliant.", "labels": [], "entities": []}, {"text": "Instead of a finite state dialogue model, we choose to decompose dialogue state into a set of state variables.", "labels": [], "entities": []}, {"text": "The activities fora given turn typically involve the sequential execution of a number of specialized routines, each of which performs a specific part of the dialogue requirements and alters the state variables in particular ways.", "labels": [], "entities": []}, {"text": "To determine which of the operations should be performed, the system consults a dialogue control table, which is specified in a simple scripting language.", "labels": [], "entities": []}, {"text": "This paper describes experiments with using this approach to dialogue modeling in the context of our Mercury flight reservation system.", "labels": [], "entities": [{"text": "dialogue modeling", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8672361373901367}, {"text": "Mercury flight reservation", "start_pos": 101, "end_pos": 127, "type": "TASK", "confidence": 0.5721778770287832}]}, {"text": "Mercury allows users to plan air travel between 226 cities worldwide.", "labels": [], "entities": [{"text": "Mercury", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9107557535171509}]}, {"text": "Following log-on, the user interacts with the system to select the flights of their trip.", "labels": [], "entities": []}, {"text": "When the flight plan is completed, the system takes the initiative to offer to price and email the itinerary.", "labels": [], "entities": []}, {"text": "Finally, the system asks the user a few questions to help determine user satisfaction.", "labels": [], "entities": []}, {"text": "The overall system makes use of the GALAXY architecture [Seneffet al], which consists of a number of specialized servers that communicate with one another via a central programmable hub.", "labels": [], "entities": [{"text": "GALAXY architecture [Seneffet al]", "start_pos": 36, "end_pos": 69, "type": "DATASET", "confidence": 0.7553834468126297}]}, {"text": "An audio server captures the user's speech via a Dialogic board, and transmits the waveform to the speech recognizer [Glass et al].", "labels": [], "entities": []}, {"text": "The language understanding component] parses a word graph produced by the recognizer and delivers a semantic frame, encoding the meaning of the utterance, to the discourse component.", "labels": [], "entities": []}, {"text": "The output of the discourse component] is the framein-context, which is transformed into a flattened Eform (electronic form) by the generation server.", "labels": [], "entities": []}, {"text": "This E-form is delivered to the turn manager, and provides the initial settings of the dialogue state.", "labels": [], "entities": []}, {"text": "The turn manager consults the dialogue control table to decide which operations to perform, and typically engages in a module-to-module subdialogue to retrieve tables from the database.", "labels": [], "entities": []}, {"text": "It prepares a response frame, which mayor may not include tabular entries.", "labels": [], "entities": []}, {"text": "The response frame is sent to the generation component] which transforms it in parallel into both a text string and an annotated string that specifies the input controls for the speech synthesizer.", "labels": [], "entities": []}, {"text": "Finally, the speech synthesizer transmits a waveform to the audio server which then relays the spoken response to the user over the telephone.", "labels": [], "entities": []}, {"text": "The entire dialogue is recorded in detail in a log file for later examination.", "labels": [], "entities": []}], "datasetContent": [{"text": "Mercury first became available for data collection in October '99.", "labels": [], "entities": [{"text": "Mercury", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9208389520645142}, {"text": "data collection in October '99", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.8681723475456238}]}, {"text": "Prospective users must first enroll by filling in a simple form on a Web page, where they enter, minimally, their name, email address, and password (a date).", "labels": [], "entities": []}, {"text": "Once the user's name has been added to the recognizer and language understanding components, they receive an email message informing them of the telephone number.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.6846486926078796}]}, {"text": "Users are encouraged to attempt to book real trips.", "labels": [], "entities": []}, {"text": "From late October to early December, we collected 49 dialogues involving real flight bookings, and these form the basis for our evaluation studies.", "labels": [], "entities": []}, {"text": "Overall, 73% of the bookings were successful (36/49).", "labels": [], "entities": []}, {"text": "We used a very strict requirement for success.", "labels": [], "entities": []}, {"text": "For example, in one dialogue considered unsuccessful the system did not know the airline that the user requested, and so the user compromised and booked the trip on a different airline.", "labels": [], "entities": []}, {"text": "Three of the failures are due to the user simply hanging up in frustration, and three others are due to the system hanging up due to a misrecognized \"good-bye.\"", "labels": [], "entities": []}, {"text": "Two failures were due to user inattentiveness.", "labels": [], "entities": []}, {"text": "The user believed that the trip was correctly booked, but a misrecognition produced a different itinerary than the one they were specifying.", "labels": [], "entities": []}, {"text": "Finally, four of the failures involved completely correct bookings, but the system was unable to follow through with the pricing and/or emailing of the itinerary.", "labels": [], "entities": []}, {"text": "Some of these involved inadequacies in the dialogue module, once the user did not provide the expected response to a system request.", "labels": [], "entities": []}, {"text": "There was a striking difference in recognition error between the successful and the incomplete bookings (11.5% vs 26% WER).", "labels": [], "entities": [{"text": "recognition error", "start_pos": 35, "end_pos": 52, "type": "METRIC", "confidence": 0.806403785943985}, {"text": "WER", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9976106882095337}]}, {"text": "A heavy foreign accent accounted for some of the recognition problems.", "labels": [], "entities": []}, {"text": "Some easily measurable statistics for the successes are given in.", "labels": [], "entities": []}, {"text": "These numbers were computed on the \"core dialogue,\" defined as the interval subsequent to logging on and up until the itinerary is fully specified, but has not yet been priced.", "labels": [], "entities": []}, {"text": "On average users required less than four minutes to complete the core dialogue, although three outliers took more than seven minutes.", "labels": [], "entities": []}, {"text": "We have long been interested in seeking evaluation metrics that are automatic and that can apply on a per-utterance basis but evaluate a significant portion of the system beyond the recognizer.", "labels": [], "entities": []}, {"text": "In we proposed an E-form evaluation metric, which compares an E-form obtained by parsing the original orthography against that obtained by parsing the selected recognizer hypothesis.", "labels": [], "entities": []}, {"text": "We believe this is a good metric for evaluating how well the recognizer and parser are doing, but it says nothing about the discourse and dialogue components.", "labels": [], "entities": []}, {"text": "We recently devised two new evaluation metrics, which we believe are useful measures for assessing the performance of the recognizer, parser, discourse, and dialogue components, collectively.", "labels": [], "entities": []}, {"text": "To compute the measures, we must reprocess the log file after the orthographic transcription has been provided for the user queries.", "labels": [], "entities": []}, {"text": "Basically, both the recognizer hypothesis and the original orthography are run through the system utterance by utterance, with the discourse and dialogue states being maintained exclusively by the recognizer branch.", "labels": [], "entities": []}, {"text": "For both branches, the Eform that is produced after the turn manager has finished processing the query is sent to a special evaluation server.", "labels": [], "entities": []}, {"text": "This server maintains a running record of all the attributes that appear in the orthography path, comparing them against their counterparts in the recognizer path.", "labels": [], "entities": []}, {"text": "The two parameters that emerge from comparing these E-forms we refer to as information bit rate (IBR) and user frustration (UF).", "labels": [], "entities": [{"text": "information bit rate (IBR)", "start_pos": 75, "end_pos": 101, "type": "METRIC", "confidence": 0.8291728595892588}, {"text": "user frustration (UF)", "start_pos": 106, "end_pos": 127, "type": "METRIC", "confidence": 0.738800299167633}]}, {"text": "IBR measures the average number of new attributes introduced per user query.", "labels": [], "entities": [{"text": "IBR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7614554762840271}]}, {"text": "A subsequent query that reiterates the same attribute is excluded since it did not introduce any new information.", "labels": [], "entities": []}, {"text": "Thus if the user said, \"I want to go from Seattle to Chicago on December 27,\" and the system misrecognized the date as \"December 22,\" then a subsequent query, \"I said December 27\" would be registered as contributing a 0 count to the IBR parameter.", "labels": [], "entities": [{"text": "IBR parameter", "start_pos": 233, "end_pos": 246, "type": "DATASET", "confidence": 0.6706047356128693}]}, {"text": "parameter tabulates how many turns it took, on average, for an intended attribute to be transmitted successfully to the system.", "labels": [], "entities": []}, {"text": "Thus, in the example above, the source and destination each took one turn, but the date took two.", "labels": [], "entities": []}, {"text": "There are some difficulties with rerunning the dialogue at a later time.", "labels": [], "entities": []}, {"text": "Both the system and the database are in a state of flux, and so the dialogue can become incoherent.", "labels": [], "entities": []}, {"text": "For example, in one case the user said, \"Book it,\" in response to a single flight being proposed, but due to changes in the flight schedule, the system proposed three flights in the rerun and the dialogue became incoherent from that point on.", "labels": [], "entities": []}, {"text": "To help alleviate incoherence, we provide a mechanism to artificially offset the date, at least to assure that the dates they have selected haven't already pasaed.", "labels": [], "entities": [{"text": "date", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9383443593978882}]}, {"text": "!_n spite of the above problems, we feel that these'evaluation metrics show considerable promise.", "labels": [], "entities": []}, {"text": "Ina pilot study, we processed a subset of our data through this evaluation configuration.", "labels": [], "entities": []}, {"text": "We identified a set of 17 attributes that could be monitored.", "labels": [], "entities": []}, {"text": "Five percent of the utterances had orthographies that failed to parse.", "labels": [], "entities": []}, {"text": "These are unevaluable without human reannotation, and are hence eliminated from the pool in the discussion below, although they clearly are likely to be very problematic.", "labels": [], "entities": []}, {"text": "summarizes the results for information bit rate for the remainder of the utterances.", "labels": [], "entities": [{"text": "information bit rate", "start_pos": 27, "end_pos": 47, "type": "METRIC", "confidence": 0.7887896299362183}]}, {"text": "A surprisingly large percentage of the utterances introduce no new concepts.", "labels": [], "entities": []}, {"text": "Some, but not all, of these are similar to the date misrecognition example given above.", "labels": [], "entities": []}, {"text": "Others are cases where the user was confused about the state of the system's knowledge, and decided to simply repeat all the preceding constraints just to make sure.", "labels": [], "entities": []}, {"text": "Some are also misfirings of the endpoint detector producing content-free utterances such as \"okay.\"", "labels": [], "entities": []}, {"text": "In other cases the user intended an action, but the system's understanding mechanism was not sophisticated enough.", "labels": [], "entities": []}, {"text": "For example \"That's good\" meaning \"book it.\"", "labels": [], "entities": []}, {"text": "We were pleased with the percentage of sentences that contained more than one attribute.", "labels": [], "entities": []}, {"text": "We believe that atypical directed dialogue would have far fewer utterances with more than one attribute.", "labels": [], "entities": []}, {"text": "Excluding the 5% of utterances whose orthography failed to parse, our system achieved a 1.05% user frustration rate.", "labels": [], "entities": []}, {"text": "This means that, on average, one out of every 20 attributes had to be entered twice.", "labels": [], "entities": []}, {"text": "We were very pleased with this number.", "labels": [], "entities": []}], "tableCaptions": []}