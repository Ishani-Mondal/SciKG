{"title": [{"text": "Introduction to the CoNLL-2000 Shared Task: Chunking", "labels": [], "entities": [{"text": "Chunking", "start_pos": 44, "end_pos": 52, "type": "TASK", "confidence": 0.7448716163635254}]}], "abstractContent": [{"text": "We describe the CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words, so-called text chunking.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.6872893869876862}]}, {"text": "We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text chunking is a useful preprocessing step for parsing.", "labels": [], "entities": [{"text": "Text chunking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6992095708847046}, {"text": "parsing", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.9732673764228821}]}, {"text": "There has been a large interest in recognizing non-overlapping noun phrases and follow-up papers) but relatively little has been written about identifying phrases of other syntactic categories.", "labels": [], "entities": []}, {"text": "The CoNLL-2000 shared task attempts to fill this gap.", "labels": [], "entities": [{"text": "CoNLL-2000 shared task", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8027265071868896}]}], "datasetContent": [{"text": "For the CoNLL shared task, we have chosen to work with the same sections of the Penn Treebank as the widely used data set for base noun phrase recognition: WSJ sections 15-18 of the Penn Treebank as training material and section 20 as test material 3.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9965584874153137}, {"text": "base noun phrase recognition", "start_pos": 126, "end_pos": 154, "type": "TASK", "confidence": 0.6046385169029236}, {"text": "WSJ", "start_pos": 156, "end_pos": 159, "type": "DATASET", "confidence": 0.8030133247375488}, {"text": "Penn Treebank", "start_pos": 182, "end_pos": 195, "type": "DATASET", "confidence": 0.8159162402153015}]}, {"text": "The chunks in the data were selected to match the descriptions in the previous section.", "labels": [], "entities": []}, {"text": "An overview of the chunk types in the training data can be found in table 1.", "labels": [], "entities": []}, {"text": "De data sets contain tokens (words and punctuation marks), information about the location of sentence boundaries and information about chunk boundaries.", "labels": [], "entities": []}, {"text": "Additionally, a partof-speech (POS) tag was assigned to each token by a standard POS tagger ( trained on the Penn Treebank).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.9947355687618256}]}, {"text": "We used these POS tags rather than the Treebank ones in order to make sure that the performance rates obtained for this data are realistic estimates for data for which no treebank POS tags are available.", "labels": [], "entities": [{"text": "Treebank", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9657072424888611}]}, {"text": "In our example sentence in section 2, we have used brackets for encoding text chunks.", "labels": [], "entities": []}, {"text": "In the data sets  B-X I-X 0 first word of a chunk of type X non-initial word in an X chunk word outside of any chunk This representation type is based on a representation proposed by for noun phrase chunks.", "labels": [], "entities": []}, {"text": "The three tag groups are sufficient for encoding the chunks in the data since these are non-overlapping.", "labels": [], "entities": []}, {"text": "Using these chunk tags makes it possible to approach the chunking task as a word classification task.", "labels": [], "entities": [{"text": "word classification task", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.7996035814285278}]}, {"text": "We can use chunk tags for representing our example sentence in the following way: First, the percentage of detected phrases that are correct (precision).", "labels": [], "entities": [{"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9990420937538147}]}, {"text": "Second, the percentage of phrases in the data that were found by the chunker (recall).", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9967357516288757}]}, {"text": "And third, the FZ=i rate which is equal to (f12 + 1)*precision*recall / (~2,precision+recall) with ~=1).", "labels": [], "entities": [{"text": "FZ=i rate", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9519853591918945}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.5936093926429749}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.5607855916023254}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.973702073097229}]}, {"text": "The latter rate has been used as the target for optimization 4.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of chunks per phrase type  in the training data (211727 tokens, 106978  chunks).", "labels": [], "entities": []}, {"text": " Table 2: Performance of the eleven systems on the test data. The baseline results have been  obtained by selecting the most frequent chunk tag for each part-of-speech tag.", "labels": [], "entities": []}]}