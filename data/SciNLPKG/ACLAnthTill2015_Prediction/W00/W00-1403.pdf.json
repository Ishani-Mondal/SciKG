{"title": [{"text": "An Empirical Study in Multilingual Natural Language Generation: What Should A Text Planner Do?", "labels": [], "entities": [{"text": "Multilingual Natural Language Generation", "start_pos": 22, "end_pos": 62, "type": "TASK", "confidence": 0.6522494852542877}]}], "abstractContent": [{"text": "We present discourse annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees fora collection of Japanese texts and their corresponding English translations.", "labels": [], "entities": []}, {"text": "We discuss implications of our empirical findings for the task of text planning in the context of implementing multilingual natural language generation systems.", "labels": [], "entities": [{"text": "text planning", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.8065590560436249}, {"text": "multilingual natural language generation", "start_pos": 111, "end_pos": 151, "type": "TASK", "confidence": 0.6569039821624756}]}], "introductionContent": [{"text": "The natural language generation community has emphasized fora number of years the strengths of multilingual generation (MGEN) systems).", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.6460920373598734}]}, {"text": "These strengths concern the reuse of knowledge, the support for early drafts in several languages, the support for maintaining consistency when making changes, the support for producing alternative formulations, and the potential for producing higher quality outputs than machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 272, "end_pos": 291, "type": "TASK", "confidence": 0.7851316034793854}]}, {"text": "(The weaknesses concern the high-cost of building large, language-independent knowledge bases, and the dilficulty of producing high-quality. broad-coverage generation algorithms.)", "labels": [], "entities": []}, {"text": "From an economic perspective, the more a system can rely on language independent modules for the purpose of multilingual generatiom the better.", "labels": [], "entities": []}, {"text": "If an MGEN system needs to develop language dependent knowledge bases, and language dependent algorithms for content selection, text planning, and sentence planning, it-is difficult to justify its economic viability.", "labels": [], "entities": [{"text": "content selection", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7261826992034912}, {"text": "text planning", "start_pos": 128, "end_pos": 141, "type": "TASK", "confidence": 0.7724900841712952}, {"text": "sentence planning", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.755994975566864}]}, {"text": "However, if most of these components are language independent and/or much of the code can be re-used, an MGEN system becomes a viable option.", "labels": [], "entities": [{"text": "MGEN", "start_pos": 105, "end_pos": 109, "type": "TASK", "confidence": 0.5656141638755798}]}, {"text": ".Many of the earl3 implementations of MGEN systems have adopted the perspective that text planners can be implemented as language-independent modules (lordanskaja el, ;11., 1992: Goldberg et el., 1994), possibly followed by a hm:aricatwn stage, in which discourse l.rees are re-written to refleet~ language-specific constraints (.", "labels": [], "entities": []}, {"text": "Although such an approach maybe adequate for highly restricted text genres, such as weather forecasts, it usually poses problems for less restricted genres.", "labels": [], "entities": []}, {"text": "Studies of instruction manuals suggest that there are variations with respect to the way high-level communicative goals are realized across languages.", "labels": [], "entities": []}, {"text": "For example, noticed that sentences (1), (2), and (3), which were taken from a trilingual instruction manual fora step-aerobics machine, yield nonisomorphic Rhetorical Structure ( analyses in English, French, and German respectively (see.", "labels": [], "entities": []}, {"text": "Hmvever, previous.discourse ,studies do .not estimate how ubiquitous such non-isomorphic analyses are.", "labels": [], "entities": []}, {"text": "Are the examples above an exception or the norm?", "labels": [], "entities": []}, {"text": "Are non-isomorphic analyses specific to discourse structures built, across elementary discourse units of single sentences, or do they also occur across sentences and paragraphs?", "labels": [], "entities": []}, {"text": "If nonisomorphism is ubiquitous, how should an MGEN system be designed in order to effectively deal with non-isomorphic discourse structures when mapping knowledge bases into multiple languages?", "labels": [], "entities": []}, {"text": "In this paper, we describe an experiment that was designed to answer these questions.: Contrasting multilingual discourse structure representations how discourse structures differ across languages, we manually built a parallel corpus of discourse trees of newspaper Japanese texts and their corresponding English translations.", "labels": [], "entities": []}, {"text": "In section 2, we present some of the problems specific to the construction of such a corpus.", "labels": [], "entities": []}, {"text": "In section 3, we present our experiment and discuss our empirical findings.", "labels": [], "entities": []}, {"text": "In Section 4, we discuss the implications of our work for the task of text planning, in the context of multilingual natural language generation.", "labels": [], "entities": [{"text": "text planning", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.8331864476203918}, {"text": "multilingual natural language generation", "start_pos": 103, "end_pos": 143, "type": "TASK", "confidence": 0.6510168388485909}]}], "datasetContent": [{"text": "In order to assess how similar discourse structures are across languages, we built manually a corpus of discourse trees for 40 Japanese texts and their corresponding translations.", "labels": [], "entities": []}, {"text": "The texts, selected randomly from the ARPA corpus, contained on average about 460 words.", "labels": [], "entities": [{"text": "ARPA corpus", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.960722953081131}]}, {"text": "We developed a discourse annotation protocol for ,Japanese and English along the lines followed by.", "labels": [], "entities": []}, {"text": "We used Marcu's discourse annotation tool (1999) in order to manually construct the discourse structure of all Japanese and English texts it, the corpus.", "labels": [], "entities": []}, {"text": "10~. of the Japanese and English texts were rhetorically labeled by two of us.", "labels": [], "entities": []}, {"text": "The agreement was statistically significant (Kappa = 0.65.0 > 0.01 for Japanese and Kappa = 0.748,0 > 0.01 for English).", "labels": [], "entities": [{"text": "Kappa = 0.65.0", "start_pos": 45, "end_pos": 59, "type": "METRIC", "confidence": 0.9494601885477701}]}, {"text": "The tool and the annotation protocol are available at. http://www, isi.edt,/~r, zarcu/softwa,-e/.", "labels": [], "entities": []}, {"text": "For each pair of Japanese-English discourse, structures, we also built, manually an alignment file, which specified the correspondence between the edus of the Japanese and English texts.", "labels": [], "entities": []}, {"text": "For example, the English tree in is characterized by 10 subsentential spans, which span across positions,,,,,,,,, and.", "labels": [], "entities": []}, {"text": "(Span [1,6] subsumes 2 sentences, so it is not sub-sentential.)", "labels": [], "entities": []}, {"text": "The Japanese discourse tree has only 4 spans that could be matched in the same positions with English spans, namely spans.,, and.", "labels": [], "entities": []}, {"text": "Hence the similarity between the Japanese tree and the English tree with respect to their discourse structure below the sentence level has a recall of 4/10 and a precision of 4/ll (in, there are 11 sub-sentential Japanese spans).", "labels": [], "entities": [{"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9993439316749573}, {"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9977052807807922}]}, {"text": "In computing Position-Independent (P-I) recall and precision figures, even when a Japanese span \"floated\" during the translation to a position in the English tree that was different from the position in the initial tree, the P-I recall and precision figures are affected less than when computing PositionDependent figures.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9754956364631653}, {"text": "recall", "start_pos": 229, "end_pos": 235, "type": "METRIC", "confidence": 0.5271154642105103}, {"text": "precision", "start_pos": 240, "end_pos": 249, "type": "METRIC", "confidence": 0.9359992742538452}]}, {"text": "The position-independent figures reflect the intuition that if two trees tl and t2 both have a subtree t, tl and 12 are more similar than if they were if they didn't share ally subtree.", "labels": [], "entities": []}, {"text": "For instance, for the spans at the sub-sentential level in the trees in the position-independent recall is 6/10 and the position-independent precision is 6/11 because in addition to spans,,, and, one can also match Japanese spat, to English spa,, and Japanese spa,, to Japanese span.", "labels": [], "entities": [{"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9403115510940552}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.7613214254379272}]}, {"text": "The Position-Independent figures offer a more optimistic metric for comparing discourse trees.", "labels": [], "entities": []}, {"text": "They span a wider range of values than the Position-Dependent figures, which enables a finer grained comparison, which in turn enables a better characterization of the differ.ences between Japanese and English discourse structures.", "labels": [], "entities": []}, {"text": "In order to provide a better estimate of how close two discourse trees were, we computed PositionDependent and -Independent recall and precision figures for the sentential level (where units are given by edus and spans are given by sets of edus or single sentences); paragraph level (where units are given by sentences and spans are given by sets of sentences or single paragraphs): and text level (where units are given by paragraphs and spans are given by sets of paragraphs).", "labels": [], "entities": [{"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9049123525619507}, {"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9969998002052307}]}, {"text": "These figures offer a detailed picture of how discourse structures and relations are mapped -from one languageto the other.", "labels": [], "entities": []}, {"text": "Some of the differences at the sentence level can be explained by differences between the syntactic structures of Japanese: Similarity of the Japanese and English discourse structures and English.", "labels": [], "entities": []}, {"text": "The differences at the paragraph and text levels have a purely rhetorical explanation.", "labels": [], "entities": []}, {"text": "As expected, when one computes the recall and precision figures with respect to the nuclearity and relation assignments, one also factors in the nuclearity status and the rhetorical relation that is associated with each span.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9990309476852417}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9933910369873047}]}, {"text": "summarizes the results (P-D and P-I (R)ecall and (P)recision figures) for each level (Sentence, Paragraph, and Text).", "labels": [], "entities": []}, {"text": "It presents Recall and Precision figures with respect to span assignment, nuclearity status, and rhetorical relation labeling of discourse spans.", "labels": [], "entities": [{"text": "span assignment", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.6822648048400879}, {"text": "rhetorical relation labeling of discourse spans", "start_pos": 97, "end_pos": 144, "type": "TASK", "confidence": 0.7735765278339386}]}, {"text": "The numbers in the \"Weighted Average\" line report averages of the Sentence-, Paragraph-, and Text-specific figures, weighted according to the number of units at each level.", "labels": [], "entities": []}, {"text": "The numbers in the \"All\" line reflect recall and precision figures computed across the entire trees, with no attention paid t.o sentence and paragraph boundaries.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9989810585975647}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9954519867897034}]}, {"text": "Given the significantly different syntactic structures of Japanese and English.", "labels": [], "entities": []}, {"text": "we were not surprised by tile low recall and precision results that reflect the similarity between discourse trees built below the sentence level.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9987586736679077}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9985571503639221}]}, {"text": "However, as shows, there are astonishing differences between discourse trees at the paragraph and text.", "labels": [], "entities": []}, {"text": "For exampie, the Position-Independent figures show that only about 62% of the sentences: and only :about 53% of the hierarchical spans built across sentences could be matched between the two corpora.", "labels": [], "entities": [{"text": "exampie", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9359984397888184}]}, {"text": "When one looks at the nuclearity status and rhetorical relations associated with the spans built across sentences, the P-I recall and precision figures drop to about 43c2~ and :/5~ respectively.", "labels": [], "entities": [{"text": "P-I", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9331433773040771}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.6994015574455261}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9978525638580322}]}, {"text": "The differences in recall and precision are exl)lained both by differen,-es in the way information is packaged rote paragraphs in the-two languages arid the way it is structured rhetorically both within and above the paragraph level.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9990512728691101}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9987977743148804}, {"text": "differen", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9680808186531067}]}], "tableCaptions": [{"text": " Table 1: Similarity of the Japanese and English discourse structures", "labels": [], "entities": []}]}