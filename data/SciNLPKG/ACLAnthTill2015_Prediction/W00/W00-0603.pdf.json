{"title": [{"text": "A Rule-based Question Answering System for Reading Comprehension Tests", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7186755686998367}]}], "abstractContent": [{"text": "We have developed a rule-based system, Quarc, that can reada short story and find the sentence in the story that best answers a given question.", "labels": [], "entities": []}, {"text": "Quarc uses heuristic rules that look for lexical and semantic clues in the question and the story.", "labels": [], "entities": []}, {"text": "We have tested Quarc on reading comprehension tests typically given to children in grades 3-6.", "labels": [], "entities": []}, {"text": "Overall, Quarc found the correct sentence 40% of the time, which is encouraging given the simplicity of its rules.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9703119993209839}]}], "introductionContent": [{"text": "In the United States, we evaluate the reading ability of children by giving them reading comprehension tests.", "labels": [], "entities": []}, {"text": "These test typically consist of a short story followed by questions.", "labels": [], "entities": []}, {"text": "Presumably, the tests are designed so that the reader must understand important aspects of the story to answer the questions correctly.", "labels": [], "entities": []}, {"text": "For this reason, we believe that reading comprehension tests can be a valuable tool to assess the state of the art in natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 118, "end_pos": 148, "type": "TASK", "confidence": 0.6591348250706991}]}, {"text": "These tests are especially challenging because they can discuss virtually any topic.", "labels": [], "entities": []}, {"text": "Consequently, broad-coverage natural language processing (NLP) techniques must be used.", "labels": [], "entities": [{"text": "broad-coverage natural language processing (NLP)", "start_pos": 14, "end_pos": 62, "type": "TASK", "confidence": 0.7187652119568416}]}, {"text": "But the reading comprehension tests also require semantic understanding, which is difficult to achieve with broad-coverage techniques.", "labels": [], "entities": []}, {"text": "We have developed a system called Quarc that \"takes\" reading comprehension tests.", "labels": [], "entities": []}, {"text": "Given a story and a question, Quarc finds the sentence in the story that best answers the question.", "labels": [], "entities": []}, {"text": "Quarc does not use deep language understanding or sophisticated techniques, yet it achieved 40% accuracy in our experiments.", "labels": [], "entities": [{"text": "Quarc", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9301614761352539}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.999506950378418}]}, {"text": "Quarc uses hand-crafted heuristic rules that look for lexical and semantic clues in the question and the story.", "labels": [], "entities": []}, {"text": "In the next section, we describe the reading comprehension tests.", "labels": [], "entities": []}, {"text": "In the following sections, we describe the rules used by Quarc and present experimental results.", "labels": [], "entities": []}, {"text": "shows an example of a reading comprehension test from Remedia Publications.", "labels": [], "entities": [{"text": "Remedia Publications", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.9340426325798035}]}, {"text": "Each testis followed by five \"WH\" questions: WHO, WHAT, WHEN, WHERE, and WHY.", "labels": [], "entities": [{"text": "WH", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9376713633537292}, {"text": "WHO", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9859001040458679}, {"text": "WHEN", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.7414389252662659}, {"text": "WHERE", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9109393358230591}, {"text": "WHY", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9376887083053589}]}, {"text": "1 The answers to the questions typically refer to a string in the text, such as a name or description, which can range in length from a single noun phrase to an entire clause or sentence.", "labels": [], "entities": []}, {"text": "The answers to WHEN and WHERE questions are also sometimes inferred from the dateline of the story.", "labels": [], "entities": [{"text": "WHEN", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.5964483022689819}]}, {"text": "For example, contains the answer to the WHEN question in.", "labels": [], "entities": [{"text": "WHEN question", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.7619737684726715}]}, {"text": "Ideally, a natural language processing system would produce the exact answer to a question.", "labels": [], "entities": []}, {"text": "Identifying the precise boundaries of the answer can be tricky, however.", "labels": [], "entities": []}, {"text": "We will focus on the somewhat easier task of identifying the sentence that contains the answer to a question.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluates Quarc on the same data set that was used to evaluate the DeepRead reading comprehension system).", "labels": [], "entities": []}, {"text": "This data set contains 115 reading comprehension tests, 55 of which were used for development and 60 of which were reserved for testing purposes.", "labels": [], "entities": []}, {"text": "We also used the answer keys created by the DeepRead developers ().", "labels": [], "entities": [{"text": "DeepRead developers", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.9087721109390259}]}, {"text": "The HumSent answers are sentences that a human judged to be the best answer for each question.", "labels": [], "entities": []}, {"text": "The AutSent answers are generated automatically by determining which sentence contains the highest percentage of words in the published answer key, excluding stopwords.", "labels": [], "entities": []}, {"text": "We focused on obtaining the best possible HumSent score because we believed that humans were more reliable than the automatic word-counting routine.", "labels": [], "entities": [{"text": "HumSent score", "start_pos": 42, "end_pos": 55, "type": "METRIC", "confidence": 0.45957884192466736}]}, {"text": "shows Quarc's results for each type of question as well as its overall results.", "labels": [], "entities": [{"text": "Quarc", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.7615621089935303}]}, {"text": "Quarc achieved 40% HumSent accuracy overall, but the accuracy varied substantially across question types.", "labels": [], "entities": [{"text": "HumSent", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.7881677150726318}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8573939800262451}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9991355538368225}]}, {"text": "Quarc performed the best on WHEN questions, achieving 55% accuracy, and performed the worst on WHAT and WHY questions, reaching only 28% accuracy.", "labels": [], "entities": [{"text": "WHEN questions", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9234187304973602}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9993495345115662}, {"text": "WHAT", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.9096139669418335}, {"text": "WHY", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.789359986782074}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9970069527626038}]}, {"text": "Quarc's rules use a variety of knowledge sources, so we ran a set of experiments to evaluate the contribution of each type of knowledge..", "labels": [], "entities": []}, {"text": "We then added the dateline rules for the WHEN and WHERE questions, and added the WHY rules that reward the sentences immediately preceding and following the best WordMatch sentence (rules #1-3 in).", "labels": [], "entities": [{"text": "dateline", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9832903146743774}, {"text": "WHY", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.888752818107605}]}, {"text": "shows that these additions (+Why/Dateline) also improved results for all three question types.", "labels": [], "entities": []}, {"text": "Finally, we added the remaining rules that look for specific words and phrases.", "labels": [], "entities": []}, {"text": "The final version of Quarc achieved 40% HumSent accuracy, which compares favorably with DeepRead's results (36% HumSent accuracy).", "labels": [], "entities": [{"text": "HumSent", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.8797005414962769}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.7776275277137756}, {"text": "HumSent accuracy", "start_pos": 112, "end_pos": 128, "type": "METRIC", "confidence": 0.6518025398254395}]}, {"text": "Furthermore, DeepRead's best results used handtagged named entity recognition and handtagged coreference resolution.", "labels": [], "entities": [{"text": "handtagged named entity recognition", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.5707076489925385}, {"text": "coreference resolution", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.8461703062057495}]}, {"text": "Quarc did not rely on any hand-tagging and did not perform any coreference reslution.", "labels": [], "entities": [{"text": "coreference reslution", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.8958950042724609}]}, {"text": "We also ran an experiment to evaluate the quality of Quarc's tie-breaking procedure, which was described at the end of Section 3.", "labels": [], "entities": [{"text": "Quarc's tie-breaking", "start_pos": 53, "end_pos": 73, "type": "DATASET", "confidence": 0.7827038168907166}]}, {"text": "When more than one sentence is tied with the best score, Quarc selects the sentence that appears earliest in the story, except for WHY questions when Quarc chooses the sentence appearing latest in the story.", "labels": [], "entities": []}, {"text": "shows the results of removing this tie-breaking procedure, so that Quarc is allowed to output all sentences that received the top score.", "labels": [], "entities": []}, {"text": "These results represent an upper bound on performance if Quarc had a perfect tie-breaking mechanism.", "labels": [], "entities": [{"text": "Quarc", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.9071311354637146}]}, {"text": "shows that Quarc's performance on WHAT, WHEN, and WHY questions improved by several percentage points, but performance on WHO and WHERE questions was basically the same.", "labels": [], "entities": [{"text": "Quarc", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.8708587884902954}, {"text": "WHAT", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.8704487085342407}, {"text": "WHEN", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.7340102791786194}, {"text": "WHO", "start_pos": 122, "end_pos": 125, "type": "DATASET", "confidence": 0.8115598559379578}]}, {"text": "Overall, Quarc was able to identify 46% of the correct sentences by generating 1.75 hypotheses per question on average.", "labels": [], "entities": []}, {"text": "These results suggest that a better tie-breaking procedure could substantially improve Quarc's performance by choosing between the top two or three candidates more intelligently.", "labels": [], "entities": []}], "tableCaptions": []}