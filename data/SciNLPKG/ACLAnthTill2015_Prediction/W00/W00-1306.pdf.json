{"title": [{"text": "Sample Selection for Statistical Grammar Induction", "labels": [], "entities": [{"text": "Statistical Grammar Induction", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.8853151599566141}]}], "abstractContent": [{"text": "Corpus-based grz.mmar induction relies on using many hand-parsed sentences as training examples.", "labels": [], "entities": [{"text": "Corpus-based grz.mmar induction", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.4593498210112254}]}, {"text": "However, the construction of a training corpus with detailed syntactic analysis for every sentence is a labor-intensive task.", "labels": [], "entities": []}, {"text": "We propose to use sample selection methods to minimize the amount of annotation needed in the training data, thereby reducing the workload of the human annotators.", "labels": [], "entities": []}, {"text": "This paper shows that the amount of annotated training data can be reduced by 36% without degrading the quality of the induced grammars.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In this paper, we propose two uncertaintybased evaluation functions for estimating the training utilities of the candidate sentences.", "labels": [], "entities": []}, {"text": "The first is a simple heuristic that uses the length of a sentence to estimate uncertainties.", "labels": [], "entities": []}, {"text": "The second function computes uncertainty in terms of the entropy of the parse trees that the hypothesis-grammar generated for the sentence.", "labels": [], "entities": []}, {"text": "flen(S, G) = length(s).", "labels": [], "entities": [{"text": "flen", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9762023091316223}, {"text": "length(s)", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9158956557512283}]}, {"text": "To determine the effectiveness of selecting training examples with the two proposed evaluation functions, we compare them against a baseline of random selection (frand(S, G) = rand()).", "labels": [], "entities": []}, {"text": "The task is to induce grammars from selected sentences in the Wall Street Journal (WSJ) corpus, and to parse unseen test sentences with the trained gr~.mmars.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus", "start_pos": 62, "end_pos": 94, "type": "DATASET", "confidence": 0.9516419172286987}]}, {"text": "Because the vocabulary size (and the grammar size by extension) is very large, we have substituted the words with their part-of-speech tags to avoid additional computational complexity in training the grammar.", "labels": [], "entities": []}, {"text": "After replacing the words with part-of-speech tags, the vocabulary size of the corpus is reduced to 47 tags.", "labels": [], "entities": []}, {"text": "We repeat the study for two different candidate-pool sizes.", "labels": [], "entities": []}, {"text": "For the first experiment, we assume that there exists an abundant sup--ply of unlabeled data.", "labels": [], "entities": []}, {"text": "Based on empirical observations (as will be shown in Section 6), for the task we are considering, the induction algorithm typically reaches its asymptotic limit after training with 2600 sentences; therefore, it is sufficient to allow fora candidate-pool size of U = 3500 unlabeled WSJ sentences.", "labels": [], "entities": []}, {"text": "In the second experiment, we restrict the size of the candidate-pool such that U contains only 900 unlabeled sentences.", "labels": [], "entities": []}, {"text": "This experiment studies how the paucity of training data affects the evaluation functions.", "labels": [], "entities": []}, {"text": "For both experiments, each of the three evaluation functions: frand, ften, and fte, is applied to the sample selection learning algorithm shown in, where concept C is the current hypothesis-grammar G, and L, the set of labeled training data; initially consists of 100 sentences.", "labels": [], "entities": []}, {"text": "In every iteration, n = 100 new sentences are picked from U to be added to L, and anew C is induced from the updated L.", "labels": [], "entities": []}, {"text": "After the hypothesis-grammar is updated, it is tested.", "labels": [], "entities": []}, {"text": "The quality of the induced grammax is judged by its ability to generate correct parses for unseen test sentences.", "labels": [], "entities": []}, {"text": "We use the consistent bracketing metric (i.e., the percentage of brackets in the proposed parse not crossing brackets of the true parse) to measure parsing accuracy 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.79905104637146}]}, {"text": "To ensure the staffstical significance of the results, we report the average often trials for each experiment 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of pair-wise t-test with 95% confidence comparing the best set of grammars  induced with the baseline (after 26 selection iterations) to the sets of grammars induced under", "labels": [], "entities": []}, {"text": " Table 2: Summary of pair-wise t-test with 95% confidence comparing the best set of grammars  induced with the baseline (after 9 selection iterations) to the sets of grammars induced under", "labels": [], "entities": []}]}