{"title": [{"text": "Two Statistical Parsing Models Applied to the Chinese Treebank", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.941215842962265}]}], "abstractContent": [{"text": "This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.5871826708316803}, {"text": "Chinese Treebank", "start_pos": 105, "end_pos": 121, "type": "DATASET", "confidence": 0.9810595214366913}]}, {"text": "We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al., 1998) and a TAG-based parsing model, adapted from (Chiang, 2000).", "labels": [], "entities": [{"text": "BBN's SIFT System", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.9105347990989685}]}, {"text": "On sentences with <40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9993504881858826}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.999345600605011}, {"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9970531463623047}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9966680407524109}]}], "introductionContent": [{"text": "Ever since the success of HMMs' application to part-of-speech tagging in, machine learning approaches to natural language processing have steadily become more widespread.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.7073172628879547}]}, {"text": "This increase has of course been due to their proven efficacy in many tasks, but also to their engineering effiCacy.", "labels": [], "entities": []}, {"text": "Many machine learning approaches let the data speak for itself (data ipsa loquuntur), as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems.", "labels": [], "entities": []}, {"text": "The success of statistical methods in particular has been quite evident in the area of syntactic parsing, most recently with the outstanding results of) and on the now-standard English test set of the Penn Treebank (.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.8430346548557281}, {"text": "English test set of the Penn Treebank", "start_pos": 177, "end_pos": 214, "type": "DATASET", "confidence": 0.8280033724648612}]}, {"text": "A significant trend in parsing models has been the incorporation of linguistically-motivated features; however, it is important to note that \"linguistically-motivated\" does not necessarily mean \"language-dependent\"---often, it means just the opposite.", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9713682532310486}]}, {"text": "For example, almost all statistical parsers make use of lexicalized nonterminals in someway, which allows lexical items' indiosyncratic parsing preferences to be modeled, but the paring between head words and their parent nonterminals is determined almost entirely by the training data, thereby making this feature--which models preferences of particular words of a particular language---almost entirely languageindependent.", "labels": [], "entities": []}, {"text": "In this paper, we will explore the use of two parsing models, which were originally designed for English parsing, on parsing Chinese, using the newly-available Chinese Treebank.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 160, "end_pos": 176, "type": "DATASET", "confidence": 0.9197443723678589}]}, {"text": "We will show that the languagedependent components of these parsers are quite compact, and that with little effort they can be adapted to produce promising results for Chinese parsing.", "labels": [], "entities": [{"text": "Chinese parsing", "start_pos": 168, "end_pos": 183, "type": "TASK", "confidence": 0.7078467011451721}]}, {"text": "We also discuss directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to put the new Chinese Treebank results into context with the unmodified (English) parsing models, we present results on two test sets from the Wall Street Journal: WSJ-all, which is the complete Section 23 (the de facto standard test set for English parsing), and WSJ-small, which is the first 400 sentences of Section 23 and which is roughly comparable in size to the Chinese test set.", "labels": [], "entities": [{"text": "Chinese Treebank results", "start_pos": 24, "end_pos": 48, "type": "DATASET", "confidence": 0.8843384981155396}, {"text": "Wall Street Journal", "start_pos": 153, "end_pos": 172, "type": "DATASET", "confidence": 0.9363067547480265}, {"text": "WSJ-all", "start_pos": 174, "end_pos": 181, "type": "DATASET", "confidence": 0.6089768409729004}, {"text": "English parsing", "start_pos": 252, "end_pos": 267, "type": "TASK", "confidence": 0.6102655529975891}, {"text": "WSJ-small", "start_pos": 274, "end_pos": 283, "type": "DATASET", "confidence": 0.7955700159072876}, {"text": "Chinese test set", "start_pos": 379, "end_pos": 395, "type": "DATASET", "confidence": 0.831204891204834}]}, {"text": "Furthermore, when testing on WSJ-small, we trained on a subset of our English training data roughly equivalent in size to our Chinese training set (Sections 02 and 03 of the Penn Treebank); we have indicated models trained on all English training with \"-all\", and models trained with the reduced English training set with \"-small\".", "labels": [], "entities": [{"text": "WSJ-small", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.9326428174972534}, {"text": "Chinese training set (Sections 02 and 03 of the Penn Treebank)", "start_pos": 126, "end_pos": 188, "type": "DATASET", "confidence": 0.7148850514338567}]}, {"text": "Therefore, by comparing the WSJ-small results with the Chinese results, one can reasonably gauge the performance gap between English parsing on the Penn Treebank and Chinese parsing on the Chinese Treebank.", "labels": [], "entities": [{"text": "WSJ-small", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.5846143364906311}, {"text": "English parsing", "start_pos": 125, "end_pos": 140, "type": "TASK", "confidence": 0.5379632413387299}, {"text": "Penn Treebank", "start_pos": 148, "end_pos": 161, "type": "DATASET", "confidence": 0.9918889105319977}, {"text": "Chinese Treebank", "start_pos": 189, "end_pos": 205, "type": "DATASET", "confidence": 0.9677992463111877}]}, {"text": "The reader will note that the modified BBN model does significantly poorer than) on Chinese.", "labels": [], "entities": [{"text": "BBN", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.6938163638114929}]}, {"text": "While more investigation is required, we suspect part of the difference maybe due to the fact that currently, the BBN model uses language-specific rules to guess part of speech tags for unknown words.", "labels": [], "entities": []}], "tableCaptions": []}