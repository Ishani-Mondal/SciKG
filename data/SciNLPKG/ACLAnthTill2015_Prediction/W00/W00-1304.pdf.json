{"title": [{"text": "Coaxing Confidences from an Old Friend: Probabilistic Classifications from Transformation Rule Lists", "labels": [], "entities": []}], "abstractContent": [{"text": "Transformation-based learning has been successfully employed to solve many natural language processing problems.", "labels": [], "entities": []}, {"text": "It has many positive features , but one drawback is that it does not provide estimates of class membership probabilities.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel method for obtaining class membership probabilities from a transformation-based rule list classifier.", "labels": [], "entities": []}, {"text": "Three experiments are presented which measure the model-ing accuracy and cross-entropy of the probabilistic classifier on unseen data and the degree to which the output probabilities from the classifier can be used to estimate confidences in its classification decisions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9939447045326233}]}, {"text": "The results of these experiments show that, for the task of text chunking 1, the estimates produced by this technique are more informative than those generated by a state-of-the-art decision tree.", "labels": [], "entities": [{"text": "text chunking 1", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.8283178806304932}]}], "introductionContent": [{"text": "In natural language processing, a great amount of work has gone into the development of machine learning algorithms which extract useful linguistic information from resources such as dictionaries, newswire feeds, manually annotated corpora and web pages.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6625630855560303}]}, {"text": "Most of the effective methods can be roughly divided into rule-based and probabilistic algorithms.", "labels": [], "entities": []}, {"text": "In general, the rule-based methods have the advantage of capturing the necessary information in a small and concise set of rules.", "labels": [], "entities": []}, {"text": "In part-of-speech tagging, for example, rule-based and probabilistic methods achieve comparable accuracies, but rule-based methods capture the knowledge in a hundred or so simple rules, while the probabilistic methods have a very high--dimensional parameter space (millions of parameters).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.7241760492324829}]}, {"text": "One of the main advantages of probabilistic methods, on the other hand, is that they include a measure of uncertainty in their output.", "labels": [], "entities": []}, {"text": "This can take the form of a probability distribution over potential outputs, or it maybe a ranked list of IA11 the experiments are performed on text chnnklng.", "labels": [], "entities": [{"text": "IA11", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.8214907050132751}]}, {"text": "The technique presented is general-purpose, however, and can be applied to many tasks for which transformationbased learning performs well, without changing the interrials of the learner.", "labels": [], "entities": []}, {"text": "These uncertainty measures are useful in situations where both the classification of an sample and the system's confidence in that classification are needed.", "labels": [], "entities": []}, {"text": "An example of this is a situation in an ensemble system where ensemble members disagree and a decision must be made about how to resolve the disagreement.", "labels": [], "entities": []}, {"text": "A similar situation arises in pipeline systems, such as a system which performs parsing on the output of a probabilistic part-of-speech tagging.", "labels": [], "entities": [{"text": "parsing on the output of a probabilistic part-of-speech tagging", "start_pos": 80, "end_pos": 143, "type": "TASK", "confidence": 0.5656786925262876}]}, {"text": "Transformation-based learning (TBL)) is a successful rule-based machine learning algorithm in natural language processing.", "labels": [], "entities": [{"text": "Transformation-based learning (TBL))", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8585922360420227}]}, {"text": "It has been applied to a wide variety of tasks, including part of speech tagging, noun phrase chvnklng), parsing, spelling correction (, prepositional phrase attachment, dialog act tagging (), segmentation and message understanding (), often achieving stateof-the-art performance with a small and easilyunderstandable list of rules.", "labels": [], "entities": [{"text": "part of speech tagging", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.6204749271273613}, {"text": "parsing", "start_pos": 105, "end_pos": 112, "type": "TASK", "confidence": 0.9726058840751648}, {"text": "spelling correction", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.8327769041061401}, {"text": "prepositional phrase attachment", "start_pos": 137, "end_pos": 168, "type": "TASK", "confidence": 0.6856180826822916}, {"text": "dialog act tagging", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.6443074146906534}, {"text": "message understanding", "start_pos": 210, "end_pos": 231, "type": "TASK", "confidence": 0.7454925179481506}]}, {"text": "In this paper, we describe a novel method which enables a transformation-based classifier to generate a probability distribution on the class labels.", "labels": [], "entities": []}, {"text": "Application of the method allows the transformation rule list to retain the robustness of the transformation-based algorithms, while benefitting from the advantages of a probabilistic classifter.", "labels": [], "entities": []}, {"text": "The usefulness of the resulting probabilities is demonstrated by comparison with another stateof-the-art classifier, the C4.5 decision tree).", "labels": [], "entities": []}, {"text": "The performance of our algorithm compares favorably across many dimensions: it obtains better perplexity and cross-entropy; an active learning algorithm using our system outperforms a similar algorithm using decision trees; and finally, our algorithm has better rejection curves than a similar decision tree.", "labels": [], "entities": []}, {"text": "Section 2 presents the transformation based learning paradigm; Section 3 describes the algorithm for construction of the decision tree associated with the transformation based list; Section 4 describes the experiments in detail and Section 5 concludes the paper and outlines the future work.", "labels": [], "entities": []}, {"text": "The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set.", "labels": [], "entities": []}, {"text": "An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made.", "labels": [], "entities": []}, {"text": "These definitions and notation will be used throughout the paper: \u2022 X denotes the sample space; \u2022 C denotes the set of possible classifications of the samples; \u2022 The state space is defined as 8 = X x C.", "labels": [], "entities": []}, {"text": "\u2022 7r will usually denote a predicate defined on X; \u2022 A ruler is defined as a predicate -class label -time tuple, (~r,c,t), c E C,t E N, where t is the learning iteration in which when the rule was learned, its position in the list.", "labels": [], "entities": []}, {"text": "\u2022 A ruler = (~r, c, t) applies to a state (z, y) if 7r(z) = true and c # y.", "labels": [], "entities": []}, {"text": "Using a TBL framework to solve a problem assumes the existence of: \u2022 An initial class assignment (mapping from X to ,.9).", "labels": [], "entities": []}, {"text": "This can be as simple as the most common class label in the training set, or it can be the output from another classifier.", "labels": [], "entities": []}, {"text": "\u2022 A set of allowable templates for rules.", "labels": [], "entities": []}, {"text": "These templates determine the predicates the rules will test, and they have the biggest influence over the behavior of the system.", "labels": [], "entities": []}, {"text": "\u2022 An objective function for learning.", "labels": [], "entities": []}, {"text": "Unlike in many other learning algorithms, the objective function for TBL will typically optimize the evaluation function.", "labels": [], "entities": []}, {"text": "An often-used method is the difference in performance resulting from applying the rule.", "labels": [], "entities": []}, {"text": "At the beginning of the learning phase, the training set is first given an initial class assignment.", "labels": [], "entities": []}, {"text": "The system then iteratively executes the following steps: 1.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three experiments that demonstrate the effectiveness and appropriateness of our probability estimates are presented in this section.", "labels": [], "entities": []}, {"text": "The experiments are performed on text chunking, a subproblem of syntactic parsing.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.7944261729717255}, {"text": "syntactic parsing", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7795042097568512}]}, {"text": "Unlike full parsing, the sentences are divided into non-overlapping phrases, where each word belongs to the lowest parse constituent that dominates it.", "labels": [], "entities": [{"text": "full parsing", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.7572035193443298}]}, {"text": "The data used in all of these experiments is the CoNLL-2000 phrase chunking corpus).", "labels": [], "entities": [{"text": "CoNLL-2000 phrase chunking corpus", "start_pos": 49, "end_pos": 82, "type": "DATASET", "confidence": 0.8458211272954941}]}, {"text": "The corpus consists of sections 15-18 and section 20 of the Penn Treebank (, and is pre-divided into a 8936-sentence (211727 tokens) training set and a 2012-sentence (47377 tokens) test set.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.9955535531044006}]}, {"text": "The chunk tags are derived from the parse tree constituents, and the part-of-speech tags were generated by the Brill tagger.", "labels": [], "entities": [{"text": "Brill tagger", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.8821060359477997}]}, {"text": "As was noted by, text chunking can be mapped to a tagging task, where each word is tagged with a chunk tag representing the phrase that it belongs to.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.7387337386608124}]}, {"text": "An example sentence from the corpus is shown in.", "labels": [], "entities": []}, {"text": "As a contrasting system, our results are compared with those produced by a C4.5 decision tree system (henceforth C4.5).", "labels": [], "entities": []}, {"text": "The reason for using C4.5 is twofold: firstly, it is a widely-used algorithm which achieves state-.of-theart performance on abroad variety of tasks; and  To perform a fair evaluation, extra care was taken to ensure that both C4.5 and TBLDT explore as similar a sample space as possible.", "labels": [], "entities": []}, {"text": "The systems were allowed to consult the word, the part-of-speech, and the chunk tag of all examples within a window of 5 positions (2 words on either side) of each target example.", "labels": [], "entities": []}, {"text": "2 Since multiple features covering the entire vocabulary of the training set would be too large a space for C4.5 to deal with, in all of experiments where TBLDT is directly compared with C4.5, the word types that both systems can include in their predicates are restricted to the most \"ambiguous\" 100 words in the training set, as measured by the number of chunk tag types that are assigned to them.", "labels": [], "entities": []}, {"text": "The initial prediction was made for both systems using a class assignment based solely on the part-ofspeech tag of the word.", "labels": [], "entities": []}, {"text": "Considering chunk tags within a contextual window of the target word raises a problem with C4.5.", "labels": [], "entities": []}, {"text": "A decision tree generally trains on independent samples and does not take into account changes of any features in the context.", "labels": [], "entities": []}, {"text": "In our case, the samples are dependent; the classification of sample i is a feature for sample i + 1, which means that changing the classification for sample i affects the context of sample i + 1.", "labels": [], "entities": []}, {"text": "To address this problem, the C4.5 systems are trained with the correct chlmk~ in the left context.", "labels": [], "entities": []}, {"text": "When the system is used for classification, input is processed in a left-to-right manner;and the output of the system is fed forward to be used as features in the left context of following samples.", "labels": [], "entities": []}, {"text": "Since C4.5 generates probabilities for each classification decision, they can be redirected into the input for the next position.", "labels": [], "entities": []}, {"text": "Providing the decision treewith this confidence information effectively allows it to perform a limited search over the entire sentence.", "labels": [], "entities": []}, {"text": "C4.5 does have one advantage over TBLDT, however.", "labels": [], "entities": [{"text": "TBLDT", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.47906428575515747}]}, {"text": "A decision tree can be trained using the subsetting feature, where questions asked are of the form: \"does feature f belong to the set FT'.", "labels": [], "entities": []}, {"text": "This is not something that a TBL can do readily, 2The TBL templates are similar to those used in l~am.~haw and Marcus (1999).", "labels": [], "entities": []}, {"text": "but since the objective is in comparing TBLDT to another state-of-the-art system, this feature was enabled.", "labels": [], "entities": [{"text": "TBLDT", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.62083899974823}]}, {"text": "In all our experiments, ~ is set to 1, giving equal weight to precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9996463060379028}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9982119798660278}]}, {"text": "The reported performances are all measured with the evaluation tool provided with the CoNLL corpus).", "labels": [], "entities": [{"text": "CoNLL corpus", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.9652277827262878}]}], "tableCaptions": [{"text": " Table 3: Cross entropy and perplexities for two  C4.5 systems and the TBLDT system", "labels": [], "entities": [{"text": "TBLDT", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.9023082256317139}]}, {"text": " Table 4: Performance of TBLDT on the CoNLL  Test Set", "labels": [], "entities": [{"text": "TBLDT", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.7258511185646057}, {"text": "CoNLL  Test Set", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9836092193921407}]}, {"text": " Table 5: Performance of C4.5 on the CoNLL Test  Set", "labels": [], "entities": [{"text": "CoNLL Test  Set", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9831488927205404}]}]}