{"title": [{"text": "What's yours and what's mine: Determining Intellectual Attribution in Scientific Text", "labels": [], "entities": [{"text": "Determining Intellectual Attribution in Scientific Text", "start_pos": 30, "end_pos": 85, "type": "TASK", "confidence": 0.7648413528998693}]}], "abstractContent": [{"text": "We believe that identifying the structure of scientific argumentation in articles can help in tasks such as automatic summarization or the automated construction of citation indexes.", "labels": [], "entities": [{"text": "identifying the structure of scientific argumentation in articles", "start_pos": 16, "end_pos": 81, "type": "TASK", "confidence": 0.7065790630877018}, {"text": "summarization", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.7494862675666809}]}, {"text": "One particularly important aspect of this structure is the question of whoa given scientific statement is attributed to: other researchers, the field in general, or the authors themselves.", "labels": [], "entities": []}, {"text": "We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text.", "labels": [], "entities": []}, {"text": "In this paper we concentrate on two particular features, namely the occurrences of prototypical agents and their actions in scientific text.", "labels": [], "entities": []}], "introductionContent": [{"text": "When writing an article, one does not normally go straight to presenting the innovative scientific claim.", "labels": [], "entities": []}, {"text": "Insteacl, one establishes other, wellknown scientific facts first, which are contributed by other researchers.", "labels": [], "entities": []}, {"text": "Attribution of ownership often happens explicitly, by phrases such as \"Chomsky claims that\".", "labels": [], "entities": [{"text": "Attribution of ownership", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8971171776453654}]}, {"text": "The question of intellectual attribution is important for researchers: not understanding the argumentative status of part of the text is a common problem for nonexperts reading highly specific texts aimed at experts.", "labels": [], "entities": []}, {"text": "In particular, after reading an article, researchers need to know who holds the \"knowledge claim\" fora certain fact that interests them.", "labels": [], "entities": []}, {"text": "We propose that segmentation according to intellectual ownership can be done automatically, and that such a segmentation has advantages for various shallow text understanding tasks.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.6926978677511215}]}, {"text": "At the heart of our classification scheme is the following trisection: * BACKGROUND (generally known work) * OWN, new work and . specific OTHER work.", "labels": [], "entities": [{"text": "BACKGROUND", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9862478375434875}, {"text": "OWN", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9845249056816101}, {"text": "OTHER", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9387205839157104}]}, {"text": "The advantages of a segmentation at a rhetorical level is that rhetorics is conveniently constant tThis work was done while the first author was at the HCRC Language Technology Group, Edinburgh.", "labels": [], "entities": [{"text": "HCRC Language Technology Group, Edinburgh", "start_pos": 152, "end_pos": 193, "type": "DATASET", "confidence": 0.8949996133645376}]}], "datasetContent": [{"text": "We carried out two evaluations.", "labels": [], "entities": []}, {"text": "Evaluation A tests whether all patterns were recognized as intended by the algorithm, and whether patterns were found that should not have been recognized.", "labels": [], "entities": []}, {"text": "Evaluation B tests how well agent and action recognition helps us perform argumentative zoning automatically.", "labels": [], "entities": [{"text": "agent and action recognition", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6919506341218948}]}, {"text": "We first manually evaluated the error level of the POS-Tagging of finite verbs, as our algorithm crucially relies on finite verbs.", "labels": [], "entities": [{"text": "error level", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9657445251941681}]}, {"text": "Ina random sample of 100 sentences from our corpus (containing a total of 184 finite verbs), the tagger showed a recall of 1.", "labels": [], "entities": [{"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9996191263198853}]}, {"text": "Start from the first finite verb in the sentence.", "labels": [], "entities": []}, {"text": "2. Check right context of the finite verb for verbal forms of interest which might makeup more complex tenses.", "labels": [], "entities": []}, {"text": "Remain within the assumed clause boundaries; do not cross commas or other finite verbs.", "labels": [], "entities": []}, {"text": "Once the main verb of that construction (the \"semantic\" verb) has been found, a simple morphological analysis determines its lemma; the tense and voice of the construction follow from the succession of auxiliary verbs encountered.", "labels": [], "entities": []}, {"text": "3. Look up the lemma of semantic verb in Action Lexicon; return the associated Action Class if successful.", "labels": [], "entities": []}, {"text": "Else return Action 0. 4. Determine if one of the 32 fixed negation words contained in the lexicon (e.g. \"not, don't, neither\") is present within a fixed window of 6 to the right of the finite verb.", "labels": [], "entities": []}, {"text": "5. Search for the agent either as a by-PP to the right, or as a subject-NP to the left, depending on the voice of the construction as determined in step 2.", "labels": [], "entities": []}, {"text": "Remain within assumed clause boundaries.", "labels": [], "entities": []}, {"text": "6. If one of the Agent Patterns matches within that area in the sentence, return the Agent Type.", "labels": [], "entities": []}, {"text": "Else return Agent 0. 7. Repeat Steps 1-6 until there are no more finite verbs left.", "labels": [], "entities": []}, {"text": "We found that for the 174 correctly determined finite verbs (out of the total 184), the heuristics for negation worked without any errors (100% accuracy).", "labels": [], "entities": [{"text": "negation", "start_pos": 103, "end_pos": 111, "type": "TASK", "confidence": 0.9779834151268005}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.998908281326294}]}, {"text": "The correct semantic verb was determined in 96% percent of all cases; errors are mostly due to misrecognition of clause boundaries.", "labels": [], "entities": []}, {"text": "Action Type lookup was fully correct, even in the case of phrasal verbs and longer idiomatic expressions (\"have to\" is a NEED..ACTION; \"be inspired by\" is a, CONTINUE_ACTION).", "labels": [], "entities": []}, {"text": "There were 7 voice errors, 2 of which were due to POS-tagging errors (past participle misrecognized).", "labels": [], "entities": []}, {"text": "The remaining 5 voice errors correspond to a 98% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9958829879760742}]}, {"text": "gives an example fora voice error (underlined) in the output of the action/agent determination.", "labels": [], "entities": []}, {"text": "Correctness of Agent Type determination was tested on a random sample of 100 sentences containing at least one agent, resulting in 111 agents.", "labels": [], "entities": [{"text": "Correctness of Agent Type determination", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6084876179695129}]}, {"text": "No agent pattern that should have been identified was missed (100% recall).", "labels": [], "entities": [{"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9785657525062561}]}, {"text": "Of the 111 agents, 105 cases were completely correct: the agent pattern covered the complete grammatical subject or by-PP intended (precision of 95%).", "labels": [], "entities": [{"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9993731379508972}]}, {"text": "There was one complete error, caused by a POS-tagging error.", "labels": [], "entities": []}, {"text": "In 5 of the 111 agents, the pattern covered only part of a subject NP (typically the NP in a postmodifying PP), as in the phrase \"the problem with these approaches\" which was classified as REF_AGENT.", "labels": [], "entities": [{"text": "REF_AGENT", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.8424756328264872}]}, {"text": "These cases (counted as errors) indeed constitute no grave errors, as they still give an indication which type of agents the nominal phrase is associated with.", "labels": [], "entities": []}, {"text": "Argumentative Zoning We evaluated the usefulness of the Agent and Action features by measuring if they improve the classification results of our stochastic classifier for argumentative zones.", "labels": [], "entities": [{"text": "Argumentative Zoning", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6688505113124847}]}, {"text": "We use 14 features given in, some of which are adapted from sentence extraction techniques  Two different statistical models were used: a Naive Bayesian model as in experiment, cf., and an ngram model over sentences, cf..", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7173614501953125}]}, {"text": "Learning is supervised and training examples are provided by our previous human annotation.", "labels": [], "entities": []}, {"text": "Classification preceeds sentence by sentence.", "labels": [], "entities": [{"text": "Classification preceeds sentence", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9271801710128784}]}, {"text": "The ngram model combines evidence from the context (Cm-1, Cm-2) and from I sententiai features (F,~,o...Fmj-t), assuming that those two factors are independent of each other.", "labels": [], "entities": []}, {"text": "It uses the same likelihood estimation as the Naive Bayes, but maximises a context-sensitive prior using the Viterbi algorithm.", "labels": [], "entities": []}, {"text": "We received best results for n=2, i.e. a bigram model.", "labels": [], "entities": []}, {"text": "The results of stochastic classification (presented in were compiled with a 10-fold cross-validation on our 80-paper corpus, containing a total of 12422 sentences (classified items).", "labels": [], "entities": [{"text": "stochastic classification", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.8453953862190247}]}, {"text": "As the first baseline, we use a standard text categorization method for classification (where each sentence is considered as a document*) Baseline 1 has an accuracy of 69%, which is low considering that the most frequent category (OWN) also coyerrs 69% of all sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9991531372070312}]}, {"text": "Worse still, the classifier classifies almost all sentences as OWN and OTHER segments (the most frequent categories).", "labels": [], "entities": []}, {"text": "Recall on the rare categories but important categories AIM, TEXTUAL, CONTRAST and BASIS is zero or very low.", "labels": [], "entities": [{"text": "AIM", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.8926985859870911}, {"text": "TEXTUAL", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.991558313369751}, {"text": "CONTRAST", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9671844244003296}, {"text": "BASIS", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9956293106079102}]}, {"text": "Text classification is therefore not a solution.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7851782739162445}]}], "tableCaptions": []}