{"title": [{"text": "Reading Comprehension Programs in a Statistical-Language-Processing Class*", "labels": [], "entities": []}], "abstractContent": [{"text": "We present-some new results for the reading comprehension task described in [3] that improve on the best published results-from 36% in [3] to 41% (the best of the systems described herein).", "labels": [], "entities": []}, {"text": "We discuss a variety of techniques that tend to give small improvements , ranging from the fairly simple (give verbs more weight in answer selection) to the fairly complex (use specific techniques for answering specific kinds of questions).", "labels": [], "entities": []}], "introductionContent": [{"text": "CS241, the graduate course in statistical language processing at Brown University, had as its class project the creation of programs to answer reading-comprehension tests.", "labels": [], "entities": [{"text": "CS241", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8532216548919678}, {"text": "statistical language processing", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.8114538391431173}]}, {"text": "In particular, we used the Remedia TM reading comprehension test data as annotated by a group at MITRE Corporation, henceforth called the Deep Read group.", "labels": [], "entities": [{"text": "Remedia TM reading comprehension test data", "start_pos": 27, "end_pos": 69, "type": "DATASET", "confidence": 0.6487031330664953}]}, {"text": "The class divided itself into four groups with sizes ranging from two to four students.", "labels": [], "entities": []}, {"text": "In the first half of the semester the goal was to reproduce the results of Deep Read and of one aother.", "labels": [], "entities": []}, {"text": "After this learning and debugging period the groups were encouraged to think of and implement new ideas.", "labels": [], "entities": []}, {"text": "The Deep Read group provided us with an on-line version of the Remedia material along with several marked up versions of * This research was supported in part by NSF grant LIS SBR 9720368.", "labels": [], "entities": [{"text": "Remedia material", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.7391389012336731}, {"text": "NSF grant LIS SBR 9720368", "start_pos": 162, "end_pos": 187, "type": "DATASET", "confidence": 0.8180546283721923}]}, {"text": "Thanks to Marc Light and the group at MITRE Corporation for providing the online versions of the reading comprehension material and the Brown Laboratory for Linguistic Information Processing (BLLIP) for providing the parsed and pronoun referenced versions. same.", "labels": [], "entities": []}, {"text": "The material encompasses four grade levels --third through sixth.", "labels": [], "entities": []}, {"text": "Each grade levels consists of thirty stories plus five questions for each story.", "labels": [], "entities": []}, {"text": "Each story has the form of a newspaper article, including a title and dateline.", "labels": [], "entities": []}, {"text": "Following, we used grades three and six as our development corpus and four and five for testing.", "labels": [], "entities": []}, {"text": "The questions on each story are typically one each of the \"who, what, where, why, and when\" varieties.", "labels": [], "entities": []}, {"text": "The Deep Read group answered these questions by finding the sentence in the story that best answers the question.", "labels": [], "entities": []}, {"text": "One of the marked up versions they provide indicates those sentences Titles and datelines are also considered possible answers to the questions.", "labels": [], "entities": []}, {"text": "In about 10% of the cases Deep Read judged no sentence standing on its own to be a good answer.", "labels": [], "entities": [{"text": "Deep Read", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.8040469884872437}]}, {"text": "In these cases no answer to the question is considered correct.", "labels": [], "entities": []}, {"text": "Ina few cases more than one answer is acceptable and all of them are so marked.", "labels": [], "entities": []}, {"text": "Deep Read also provided aversion with person/place/time markings inserted automatically by the Alembic named-entity system.", "labels": [], "entities": [{"text": "Deep Read", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9173617660999298}]}, {"text": "Henceforth we refer to this as NE (named entity) material.", "labels": [], "entities": []}, {"text": "As discussed below, these markings are quite useful.", "labels": [], "entities": []}, {"text": "In addition to the mark-ups provided by Deep Read, the groups were also g~ven a machine annotated version with full parse trees and pronoun coreference.", "labels": [], "entities": []}, {"text": "The Deep Read group suggests several different metrics for judging the performance of reading-comprehension-questionanswering programs.", "labels": [], "entities": []}, {"text": "However, their data show that the performance of theii: programs goes up and down on all of the metrics in Extra credit for main verb match named entity Specific techniques for all question types Prefer sentences with same subject term frequency times inverse document frequency Specific good words for \"why\" questions tandem.", "labels": [], "entities": []}, {"text": "We implemented several of those metrics ourselves, but to keep things simple we only report results on one of themhow often (in percent) the program answers a question by choosing a correct sentence (as judged in the answer mark-ups).", "labels": [], "entities": []}, {"text": "Following we refer to this as the \"humsent\" (human annotated sentence) metric.", "labels": [], "entities": [{"text": "humsent\" (human annotated sentence)", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.6526157941137042}]}, {"text": "Note that if more than one sentence is marked as acceptable, a program response of any of those sentences is considered correct.", "labels": [], "entities": []}, {"text": "If no sentence is marked, the program cannot get the answer correct, so there is an upper bound of approximately 90% accuracy for this metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.999302864074707}]}], "datasetContent": [], "tableCaptions": []}