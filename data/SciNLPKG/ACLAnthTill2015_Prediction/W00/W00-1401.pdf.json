{"title": [], "abstractContent": [{"text": "Certain generation applications may profit from the use of stochastic methods.", "labels": [], "entities": []}, {"text": "In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models.", "labels": [], "entities": []}, {"text": "In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment.", "labels": [], "entities": []}, {"text": "This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects.", "labels": [], "entities": []}, {"text": "To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment.", "labels": [], "entities": []}, {"text": "The experiment confirms that intrinsic metrics cannot replace human evaluation , but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development.", "labels": [], "entities": []}], "introductionContent": [{"text": "For many applications in natural language generation (NLG), the range of linguistic expressions that must be generated is quite restricted, and a grammar fora surface realization component can be fully specified by hand.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.829077790180842}]}, {"text": "Moreover, iLL inany cases it is very important not to deviate from very specific output in generation (e.g., maritime weather reports), in which case hand-crafted grammars give excellent control.", "labels": [], "entities": []}, {"text": "In these cases, evaluations of the generator that rely on human judgments (Lester and Porter, I997) or on human annotation of the test corpora are quite sufficient ....", "labels": [], "entities": [{"text": "I997", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.7665865421295166}]}, {"text": "in other NLG applications the variety of the output is much larger, and the demands on the quality of the output are solnewhat less stringent.", "labels": [], "entities": []}, {"text": "A typical example is NLG in the context of (interlingua-or transfer-based) inachine translation.", "labels": [], "entities": [{"text": "interlingua-or transfer-based) inachine translation", "start_pos": 44, "end_pos": 95, "type": "TASK", "confidence": 0.6293379783630371}]}, {"text": "Another reason for relaxing the quality of the output maybe that not enough time is available to develop a full gramnlar fora new target, language in NLG.", "labels": [], "entities": []}, {"text": "ILL all these cases, stochastic methods provide an alternative to hand-crafted approaches to NLG.", "labels": [], "entities": []}], "datasetContent": [{"text": "The simple accuracy, generation accuracy, simple tree accuracy and generation tree accuracy for the two experiments are tabulated in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9418690800666809}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8436204791069031}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.55506831407547}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8834613561630249}]}, {"text": "The test corpus is a randomly chosen subset of 100 sentences from the Section 20 of WSJ.", "labels": [], "entities": [{"text": "Section 20 of WSJ", "start_pos": 70, "end_pos": 87, "type": "DATASET", "confidence": 0.8609182387590408}]}, {"text": "The dependency structures for the test sentences were obtained automatically from converting the Penn TreeBank phrase structure trees, in the same way as was done to Create the training corpus.", "labels": [], "entities": [{"text": "Penn TreeBank phrase structure trees", "start_pos": 97, "end_pos": 133, "type": "DATASET", "confidence": 0.974952244758606}]}, {"text": "The average length of the test sentences is 16.7 words with a longest sentence being 24 words in length.", "labels": [], "entities": []}, {"text": "As can be seen, the supertag-based model improves over the baseline LR model on all four baseline quantitative metrics.", "labels": [], "entities": []}, {"text": "We have presented four metrics which we can compute automatically.", "labels": [], "entities": []}, {"text": "In order to determine whether the metrics correlate with independent notions understandability or quality, we have performed evaluation experiments with human subjects.", "labels": [], "entities": []}, {"text": "In the web-based experiment, we ask human subjects to read a short paragraph from the WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.887999415397644}]}, {"text": "We present three or five variants of the last sentence of this paragraph on the same page, and ask the subject to judge them along two dimensions: Here we summarize two experiments that we have performed that use different tree nmdels.", "labels": [], "entities": []}, {"text": "(For a more detailed comparisons of different tree models, see).) o For the baseline experiment, we impose a random tree structure for each sentence of the corpus and build a Tree Model whose parameters consist of whether a lexeme ld precedes or follows her mother lexeme [ ....", "labels": [], "entities": []}, {"text": "We call this the Baseline Left-Right (LR) Model.", "labels": [], "entities": []}, {"text": "This model generates There was estimate for phase the second no cost.", "labels": [], "entities": []}, {"text": "o In the second experiment we use the-system as described in Section 2.", "labels": [], "entities": []}, {"text": "We employ the supertag-based tree model whose parameters consist of whether a lexeme ld with supertag sd is a dependent of lexeme 1,,, with supertag s,,,.", "labels": [], "entities": []}, {"text": "Furthermore we use the information provided by the XTAG grammar to order the dependents.", "labels": [], "entities": []}, {"text": "This model generates There was no cost estimate for\" the second phase . for our example input, .which is indeed.the sentence found in the WS.I. o Understandability: How easy is this sentence to understand?", "labels": [], "entities": [{"text": "WS.I. o", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.9632267653942108}]}, {"text": "Options range from \"Extremely easy\" (= 7) to \"Just barely possible\" (=4) to \"Impossible\" (=1).", "labels": [], "entities": [{"text": "Impossible", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9787327647209167}]}, {"text": "(Intermediate numeric values can also be chosen but have no description associated with them.) o Quality: How well-written is this sentence?", "labels": [], "entities": [{"text": "Quality", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9797491431236267}]}, {"text": "Options range from \"Extremely well-written'\" (= 7) to \"Pretty bad\" (=4) to \"Horrible (=1  Given the variance between subjects we first normalized the data.", "labels": [], "entities": []}, {"text": "We subtracted the mean score for each subject from each observed score and then divided this by standard deviation of the scores for that subject.", "labels": [], "entities": []}, {"text": "As expected our data showed strong correlations between normalized understanding and quality judgments for each sentence variant (r(22) = 0.94, p < 0.0001).", "labels": [], "entities": []}, {"text": "Our main hypothesis is that the two tree-based metrics correlate better with both understandability and quality than the string-based metrics.", "labels": [], "entities": []}, {"text": "Correlations of the two string metrics with normalized understanding for each sentence variant were not significant (r  A second aim of ()Lit\" qualitative evaluation was to lest various models of the relationship between intrinsic variables and qualitative user judgments.", "labels": [], "entities": []}, {"text": "\\Ve proposed a mmlber-of'models:in which various conLfrom the two tree models binations of intrinsic metrics were used to predict user judgments of understanding and quality.", "labels": [], "entities": []}, {"text": ".We conducted a series of linear regressions with normalized judgments of understanding and quality as the dependent measures and as independent measures different combinations of one of our four metrics with sentence length, and with the \"problem\" variables that we used to define the string metrics (S, I, D, M, I', D' -see Section 3 for definitions).", "labels": [], "entities": []}, {"text": "One sentence variant was excluded from the data set, on the grounds that the severely \"mangled\" sentence happened to turnout well-formed and with nearly the same nleaning as the target sentence.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We first tested models using one of our metrics as a single intrinsic factor to explain the dependent variable.", "labels": [], "entities": []}, {"text": "We then added the \"problem\" variables.", "labels": [], "entities": []}, {"text": "6 and could boost tile explanatory power while maintaining significance.", "labels": [], "entities": [{"text": "tile explanatory", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.749566912651062}]}, {"text": "In, we show only some con> binations, which show that tile best results were obtained by combining the simple tree accuracy with the number of Substitutions (S) and the sentence length.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.7833072543144226}]}, {"text": "As we can see, the number of substitutions ..... has an.important effecVon explanatory.power,, while that of sentence length is much more modest (but more important for quality than for understanding).", "labels": [], "entities": []}, {"text": "Furthermore, the number of substitutions has more explanatory power than the number of moves (and in fact. than any of the other \"problem\" variables).", "labels": [], "entities": []}, {"text": "The two regressions for understanding and writing show very sinlilar results.", "labels": [], "entities": [{"text": "understanding", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9650061130523682}]}, {"text": "Normalized understand6None of tile \"problem\" variables have much explanatory power on their own (nor (lid they achieve significance).: Testing different models of user judgments (S is number of substitutions, M number of moved elements) ing was best modeled as:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Scores for the sample sentence according to the four metrics", "labels": [], "entities": []}, {"text": " Table 3: Testing different models of user judgments (S is number of substitutions, M number of moved  elements)", "labels": [], "entities": []}]}