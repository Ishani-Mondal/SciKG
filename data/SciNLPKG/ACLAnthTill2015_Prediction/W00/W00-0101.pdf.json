{"title": [{"text": "Sentences vs. Phrases: Syntactic Complexity in Multimedia Information Retrieval", "labels": [], "entities": [{"text": "Multimedia Information Retrieval", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.631985197464625}]}], "abstractContent": [{"text": "In experiments on a natural language information retrieval system that retrieves images based on textual captions, we show that syntactic complexity actually aids retrieval.", "labels": [], "entities": [{"text": "natural language information retrieval", "start_pos": 20, "end_pos": 58, "type": "TASK", "confidence": 0.6990811675786972}]}, {"text": "We compare two types of captioned images, those characterized with full sentences in English, and those characterized by lists of words and phrases.", "labels": [], "entities": []}, {"text": "The full-sentence captions show a 15% increase in retrieval accuracy over the word-list captions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9145617485046387}]}, {"text": "We conclude that the syntactic complexity maybe of use in fact because it decreases semantic ambiguity: the word-list captions maybe syntactically simple, but they are semantically confusingly complex.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we describe experiments conducted on an image retrieval system, PictureQuest, which uses text captions to characterize images.", "labels": [], "entities": []}, {"text": "The text captions are of two types.", "labels": [], "entities": []}, {"text": "Optimally, they consist of a prose description of the image, generally two to three sentences, with perhaps three or four additional words or phrases that describe emotional or non-literal image content, e.g.", "labels": [], "entities": []}], "datasetContent": [{"text": "While the sentence captions are syntactically more complex, by almost any measure, they contain more information than the legacy word list captions.", "labels": [], "entities": [{"text": "sentence captions", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.693671926856041}]}, {"text": "Specifically, the part-ofspeech tagger and the noun phrase pattern matcher are essentially useless with the word lists, since they rely on syntactic patterns that are not present.", "labels": [], "entities": [{"text": "noun phrase pattern matcher", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6153659895062447}]}, {"text": "We therefore hypothesized that our retrieval accuracy would be lower with the legacy word list captions than with the sentence captions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.978999137878418}]}, {"text": "We performed two sets of experiments, one with legacy word list captions and the other with sentence captions.", "labels": [], "entities": []}, {"text": "Fortunately, the corpus can be easily divided, since it is possible to select image providers with either full sentence or word list captions, and limit the search to those providers.", "labels": [], "entities": []}, {"text": "In order to ensure that we did not introduce a bias because of the quality of captioning fora particular provider, we aggregated scores from at least three providers in each test.", "labels": [], "entities": []}, {"text": "Because the collection is large and live, and includes ranked results, we selected a modified version of precision at 20 rather than a manual gold standard precision/recall test.", "labels": [], "entities": [{"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9996792078018188}, {"text": "precision", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.9952914714813232}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.7795785069465637}]}, {"text": "We chose this evaluation path for the following reasons: \u2022 Ranking image relevance was difficult for humans \u2022 The collection was large and live, i.e. changing daily \u2022 The modified measure more accurately reflected user evaluations We performed experiments initially with manual ranking, and found that it was impossible to get reliable cross-coder judgements for ranked results.", "labels": [], "entities": []}, {"text": "That is, we could get humans to assess whether an image should or should not have been included, but the rankings did not yield agreement.", "labels": [], "entities": []}, {"text": "Complicating the problem was the fact that we had a large collection (400,000+ images), and creating a test subset meant that most queries would generate almost no relevant results.", "labels": [], "entities": []}, {"text": "Finally, we wanted to focus more on precision than on recall, because our work with users had made it clear that precision was far more important in this application.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9988333582878113}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9777919054031372}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9972781538963318}]}, {"text": "To evaluate precision at 20 for this collection, we used the crossing measure introduced in Flank 1998.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9993067979812622}, {"text": "Flank 1998", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.9287534654140472}]}, {"text": "The crossing measure (in which any image ranked above another, better-matching image counts as an error) is both finer-grained and better suited to a ranking application in which user evaluations are not binary.", "labels": [], "entities": [{"text": "crossing measure", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.7023050487041473}]}, {"text": "We calibrated the crossing measure (on a subset of the queries) as follows: That is, we calculated the precision \"for all terms\" as a binary measure with respect to a query, and scored an error if any terms in the query were not matched.", "labels": [], "entities": [{"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9985038042068481}]}, {"text": "For the \"any term\" precision measure, we scored an error only if the image failed to match any term in the query in such away that a user would consider it a partial match.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9948953986167908}]}, {"text": "Thus, for example, for an \"all terms\" match, tall glass of beer succeeded only when the images showed (and captions mentioned) all three terms tall, glass, and beer, or their synonyms.", "labels": [], "entities": []}, {"text": "For an \"any-term\" match, tall or glass or beer or a direct synonym would need to be present (but not, say, glasses).", "labels": [], "entities": []}, {"text": "(For two of the test queries, fewer than 20 images were retrieved, so the measure is, more precisely, R-precision: precision at the number of documents retrieved or at 20 or 5, whichever is less.", "labels": [], "entities": [{"text": "R-precision", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9911365509033203}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.998773992061615}]}], "tableCaptions": []}