{"title": [{"text": "Flexible Speech Act Based Dialogue Management", "labels": [], "entities": [{"text": "Flexible Speech Act Based Dialogue Management", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8092609445254008}]}], "abstractContent": [{"text": "We present an application independent dialogue engine that reasons on application dependent knowledge sources to calculate predictions about how a dialogue might continue.", "labels": [], "entities": []}, {"text": "Predictions are language independent and are translated into language dependent structures for recognition and synthesis.", "labels": [], "entities": []}, {"text": "Further, we discuss how the predictions account for different kinds of dialogue, e.g., question-answer or mixed initiative.", "labels": [], "entities": []}], "introductionContent": [{"text": "The computerized spoken information systems (or Spoken Dialogue System--SDS) that we will consider in this paper are systems where a computer acts as the operator of some service and interacts with a user in natural language, e.g., switchboard, directory assistance, or ticket service.", "labels": [], "entities": [{"text": "Spoken Dialogue System--SDS)", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.6498385320107142}]}, {"text": "Before an SDS can provide its information, it needs to acquire data from the user, e.g., customer name and number, birth date, service location, or service date.", "labels": [], "entities": []}, {"text": "We call these parameter values.", "labels": [], "entities": []}, {"text": "In an SDS they are acquired orally and speech recognition is used to decode the speech signal into words.", "labels": [], "entities": [{"text": "SDS", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9709904789924622}, {"text": "speech recognition", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7005121111869812}]}, {"text": "A dialogue manager facilitates the negotiation of parameter values between a user and an SDS.", "labels": [], "entities": []}, {"text": "We emphasize keeping our dialogue manager application and language independent, thus we factored out the independent information into two components.", "labels": [], "entities": []}, {"text": "A dialogue engine calculates predictions for how to continue a dialogue from dependent knowledge sources (e.g., dialogue grammar and history, application description).", "labels": [], "entities": []}, {"text": "A pragmatic interpreter maps syntactic/semantic interpretation results onto predictions.", "labels": [], "entities": []}, {"text": "Our predictions are called dialogue primitives; GEN-primitives predict system utterances and REC-primitives predict user utterances.", "labels": [], "entities": []}, {"text": "They are language independent and on both the recognition and the generation side, other modules translate them into language dependent structures.", "labels": [], "entities": []}, {"text": "In this paper, we will discuss the kinds of primitives our dialogue manager calculates and how they account for different kinds of dialogue, e.g., question-answer or mixed initiative.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}