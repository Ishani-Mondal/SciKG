{"title": [{"text": "Comparing linguistic interpretation schemes for English corpora", "labels": [], "entities": [{"text": "Comparing linguistic interpretation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.675277183453242}]}], "abstractContent": [{"text": "Project AMALGAM explored a range of Part-of-Speech tagsets and phrase structure parsing schemes used in modern English corpus-based research.", "labels": [], "entities": [{"text": "phrase structure parsing", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.6998728712399801}]}, {"text": "The PoS-tagging schemes and parsing schemes include some which have been used for hand annotation of corpora or manual post-editing of automatic taggers or parsers; and others which are unedited output of a parsing program.", "labels": [], "entities": []}, {"text": "Project deliverables include: a detailed description of each PoS-tagging scheme, and multi-tagged corpus; a \"Corpus-neutral\" tokenization scheme; a family of PoS-taggers, for 8 PoS-tagsets; a method for \"PoS-tagset conversion\", a sample of texts parsed according to a range of parsing schemes: a MultiTreebank; an Internet service allowing researchers worldwide free access to the above resources, including a simple email-based method for PoS-tagging any English text with any or all PoS-tagset(s).", "labels": [], "entities": [{"text": "PoS-tagset conversion", "start_pos": 204, "end_pos": 225, "type": "TASK", "confidence": 0.736446738243103}]}, {"text": "We conclude that the range of tagging and parsing schemes in use is too varied to allow agreement on a standard; and that parser-evaluation based on 'bracket-matching' is unfair to more sophisticated parsers.", "labels": [], "entities": []}], "introductionContent": [{"text": "The International Computer Archive of Modern and medieval English, ICAME, is an international research network focussing on English Corpus Linguistics, including the collation and linguistic annotation of English language corpora, and applications of these linguistically interpreted corpora.", "labels": [], "entities": [{"text": "International Computer Archive of Modern and medieval English, ICAME", "start_pos": 4, "end_pos": 72, "type": "DATASET", "confidence": 0.7727877795696259}]}, {"text": "ICAME publishes an annual ICAME Journal (now in its 24th volume) and holds an annual ICAME conference (ICAME'2000, the 19th ICAME conference, was held in Sydney, Australia).", "labels": [], "entities": [{"text": "ICAME", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9302226901054382}, {"text": "ICAME Journal", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.6886626482009888}]}, {"text": "Many English Corpus Linguistics projects reported in ICAME Journal and elsewhere involve grammatical analysis or tagging of English texts (eg. Each new project reviewed existing tagging schemes, and chose which to adopt and/or adapt.", "labels": [], "entities": [{"text": "ICAME Journal", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9454937875270844}]}, {"text": "The project AMALGAM (Automatic Mapping Among Lexico-Grammatical Annotation Models) has explored a range of Part-of-Speech tagsets and parsing schemes used in ICAME corpus-based research.", "labels": [], "entities": [{"text": "Automatic Mapping Among Lexico-Grammatical Annotation Models)", "start_pos": 21, "end_pos": 82, "type": "TASK", "confidence": 0.8166052103042603}, {"text": "ICAME corpus-based research", "start_pos": 158, "end_pos": 185, "type": "TASK", "confidence": 0.8091434836387634}]}, {"text": "The PoS-tagging schemes include: Brown (, LOB (, parts (man 1986), SEC (, POW (Souter 1989b), UPenn (Santorini 1990), LLC, ICE, and BNC (Garside 1996).", "labels": [], "entities": [{"text": "SEC", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.770326554775238}, {"text": "UPenn (Santorini 1990)", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.7043177902698516}, {"text": "BNC", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9620109796524048}]}, {"text": "The parsing schemes include some which have been used for hand annotation of corpora or manual post-editing of automatic parsers; and others which are unedited output of a parsing program.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Text sources for the multi-tagged corpus.  Sentences  Words  Average Sentence Length  London teenager speech (COLT)  60  407  6.8  Radio broadcasts (SEC)  60  2016  33.6  Software manuals (IPSM)  60  1016  16.9  Total:  180  3439  19.1", "labels": [], "entities": [{"text": "Average Sentence Length", "start_pos": 71, "end_pos": 94, "type": "METRIC", "confidence": 0.7671959400177002}, {"text": "London teenager speech (COLT)  60  407  6.8  Radio broadcasts (SEC)  60  2016  33.6  Software manuals (IPSM)  60  1016  16.9  Total:  180  3439  19.1", "start_pos": 96, "end_pos": 245, "type": "DATASET", "confidence": 0.8515295247236888}]}, {"text": " Table 3. Examples where the standardised tokenizer clashes with a specific tagging scheme (POW)  Tokeniser/  Correct analysis  Tagger Output  in POW corpus", "labels": [], "entities": []}, {"text": " Table 4. Model size and accuracy of the re-trained Brill multi-tagger", "labels": [], "entities": [{"text": "Model size", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8868362307548523}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9984763264656067}]}, {"text": " Table 5. The most common PoS-tagging errors.", "labels": [], "entities": [{"text": "PoS-tagging errors", "start_pos": 26, "end_pos": 44, "type": "METRIC", "confidence": 0.728777140378952}]}, {"text": " Table 6. Accuracy found after manual proof-reading of multi-tagged corpus", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998740017414093}]}]}