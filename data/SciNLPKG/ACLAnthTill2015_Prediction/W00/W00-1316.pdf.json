{"title": [{"text": "A Machine Learning Approach to Answering Questions for :Reading Comprehension Tests", "labels": [], "entities": [{"text": "Answering Questions", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.9075089991092682}]}], "abstractContent": [{"text": "In this paper, we report results on answering questions for the reading comprehension task, using a machine learning approach.", "labels": [], "entities": []}, {"text": "We evaluated our approach on the Remedia data set, a common data set used in several recent papers on the reading comprehension task.", "labels": [], "entities": [{"text": "Remedia data set", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.8560492595036825}]}, {"text": "Our learning approach achieves accuracy competitive to previous approaches that rely on hand-crafted, deterministic rules and algorithms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9991433620452881}]}, {"text": "To the best of our knowledge, this is the first work that reports that the use of a machine learning approach achieves competitive results on answering questions for reading comprehension tests.", "labels": [], "entities": []}], "introductionContent": [{"text": "The advent of the Internet has resulted in a massive information explosion.", "labels": [], "entities": []}, {"text": "We need to have an effective and efficient means of locating just the desired information.", "labels": [], "entities": []}, {"text": "The field of information retrieval (IR) is the traditional discipline that addresses this problem.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.8712885916233063}]}, {"text": "However, most of the prior work in IR deal more with document retrieval rather than \"information\" retrieval.", "labels": [], "entities": [{"text": "IR", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.984357476234436}, {"text": "information\" retrieval", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.7054001887639364}]}, {"text": "This also applies to search engines on the Internet.", "labels": [], "entities": []}, {"text": "Current search engines take a list of input words and return a ranked list of web pages that contain (or not contain) the given words.", "labels": [], "entities": []}, {"text": "It is then left to the user to search through the returned list of web pages for the information that he needs.", "labels": [], "entities": []}, {"text": "While finding the web pages that contain the desired information is an important first step, what an information seeker needs is often an answer to a question.", "labels": [], "entities": []}, {"text": "That is, given a question, we want a system to return the exact answers to the question, and not just the documents to allow us to further search for the * Leong Hwee Teo's current a~ation: Defence Medical Research Institute, Defence Science and Technology Agency, 1 Depot Road, Defence Technology Tower A, ~19-05, Singapore 109679 l:leonghw~dsl:a.gov, sg answers.", "labels": [], "entities": [{"text": "Defence Medical Research Institute", "start_pos": 190, "end_pos": 224, "type": "DATASET", "confidence": 0.8820571899414062}, {"text": "Defence Science and Technology Agency, 1 Depot Road", "start_pos": 226, "end_pos": 277, "type": "DATASET", "confidence": 0.8521569569905599}, {"text": "Defence Technology Tower A", "start_pos": 279, "end_pos": 305, "type": "DATASET", "confidence": 0.8363863676786423}, {"text": "Singapore 109679 l:leonghw", "start_pos": 315, "end_pos": 341, "type": "DATASET", "confidence": 0.8525087594985962}]}, {"text": "The need for question answering (QA) systems has prompted the initiation of the question answering track in TREC-8) to address this problem.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.8723633170127869}, {"text": "question answering", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.8346152901649475}, {"text": "TREC-8", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.6816788911819458}]}, {"text": "In the QA track, each participant is given a list of 200 questions, and the goal is to locate answers to these questions from a document database consisting of hundreds of thousands of documents (about two gigabytes of text).", "labels": [], "entities": []}, {"text": "Each participant is to return a ranked list of the five best answer strings for each question, where each answer string is a string of 50 bytes (or 250 bytes) that contains an answer to the question.", "labels": [], "entities": []}, {"text": "What, when, where, and who questions that have explicit answers given in some document in the database are emphasized, but not why questions.", "labels": [], "entities": []}, {"text": "Ina related but independent effort, a group at MITRE has investigated question answering in the context of the reading comprehension task).", "labels": [], "entities": [{"text": "question answering", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7627842128276825}]}, {"text": "The documents in this task axe 115 children stories at grade two to five from Remedia Publications, and the task involves answering five questions (who, what, when, where, and why question) per story, as a measure of how well a system has understood the story.", "labels": [], "entities": [{"text": "Remedia Publications", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.905837893486023}]}, {"text": "Each story has an average of 20 sentences, and the question answering task as formulated fora computer program is to select a sentence in the story that answers to a question.", "labels": [], "entities": [{"text": "question answering", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7046415507793427}]}, {"text": "For about 10% of the questions, there is not a single sentence in the story that is judged to answer the question.", "labels": [], "entities": []}, {"text": "Conversely, a question can have multiple correct answers, where each of several individual sentences is a correct answer.", "labels": [], "entities": []}, {"text": "An example story from the Remedia corpus and its five accompanying questions axe given in.", "labels": [], "entities": [{"text": "Remedia corpus", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.8142170310020447}]}, {"text": "Each story has a title (such as \"Storybook Person Found Alive!\") and dateline (such as \"ENGLAND,) in the Remedia corpus.", "labels": [], "entities": [{"text": "dateline", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9646701812744141}, {"text": "ENGLAND", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9756355285644531}, {"text": "Remedia corpus", "start_pos": 105, "end_pos": 119, "type": "DATASET", "confidence": 0.9098333120346069}]}, {"text": "(ENGLAND,) -Christopher Robin is alive and well.", "labels": [], "entities": [{"text": "ENGLAND", "start_pos": 1, "end_pos": 8, "type": "METRIC", "confidence": 0.8629353046417236}, {"text": "Christopher Robin", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.673380047082901}]}, {"text": "He is the same person that you read about in the book, Winnie the Pooh.", "labels": [], "entities": []}, {"text": "As a boy, Chris lived in a pretty home called Cotchfield Farm.", "labels": [], "entities": [{"text": "Cotchfield Farm", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.939277708530426}]}, {"text": "When Chris was three years old, his father wrote a poem about him.", "labels": [], "entities": []}, {"text": "The poem was printed in a magazine for others to read.", "labels": [], "entities": []}, {"text": "Mr. Robin then wrote a book.", "labels": [], "entities": []}, {"text": "He made up a fairy tale land where Chris lived.", "labels": [], "entities": []}, {"text": "There was a bear called W~nnie the Pooh.", "labels": [], "entities": []}, {"text": "There was also an owl and a young pig, called a piglet.", "labels": [], "entities": []}, {"text": "All the animals were stuffed toys that Chris owned.", "labels": [], "entities": []}, {"text": "Mr. Robin made them come to life with his words.", "labels": [], "entities": []}, {"text": "The places in the story were all near Cotchfield Farm.", "labels": [], "entities": [{"text": "Cotchfield Farm", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.8772958815097809}]}, {"text": "Winnie the Pooh was written in 1925.", "labels": [], "entities": []}, {"text": "Children still love to read about Christopher Robin and his animal friends.", "labels": [], "entities": [{"text": "Christopher Robin and his animal friends", "start_pos": 34, "end_pos": 74, "type": "TASK", "confidence": 0.809760590394338}]}, {"text": "Most people don't know he is areal person who is grown now.", "labels": [], "entities": []}, {"text": "He has written two books of his own.", "labels": [], "entities": []}, {"text": "They tell what it is like to be famous.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our learning approach, we trained AQUAREA$ on the same development set of stories and tested it on the same test set of stories as those used in all past work on the reading comprehension task;).", "labels": [], "entities": []}, {"text": "Specifically, the set of stories used are published by Remedia Publicatious.", "labels": [], "entities": [{"text": "Remedia Publicatious", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.8958823978900909}]}, {"text": "We used the same softcopy version created by the MITRE group, and the material includes manual annotations of named entities and coreference cbalns as done by the MITRE group.", "labels": [], "entities": [{"text": "MITRE group", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.924291729927063}, {"text": "MITRE group", "start_pos": 163, "end_pos": 174, "type": "DATASET", "confidence": 0.9132517278194427}]}, {"text": "The training set consists of 28 stories from grade 2 and 27 stories from grade 5.", "labels": [], "entities": []}, {"text": "The test set consists of 30 stories from grade 3 and 30 stories from grade 4.", "labels": [], "entities": []}, {"text": "Within the 60 test stories, there are 59 who questions, 61 what questions, 60 when questions, 60 where questions, and 60 why questions, fora total of 300 test questions.", "labels": [], "entities": []}, {"text": "The scoring metric that we used for evaluation is HumSent, which is the percentage of test questions for which AQUAREAS has chosen a correct sentence as the answer.", "labels": [], "entities": [{"text": "HumSent", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9470962882041931}]}, {"text": "This metric is originally proposed by).", "labels": [], "entities": []}, {"text": "The correct answer sentences are chosen manually by the MITRE group.", "labels": [], "entities": [{"text": "MITRE group", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.8617064952850342}]}, {"text": "Although there were a few other scoring met-rics originally proposed in), all the metrics were found to correlate well with one another.", "labels": [], "entities": []}, {"text": "As such, all subsequent work (;) uses HumSent as the main scoring metric, and it is also the scoring metric that we adopted in this paper.", "labels": [], "entities": [{"text": "HumSent", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.6860202550888062}]}, {"text": "Based on the complete set of 20 features described in the previous section, we trained one classifier per question type.", "labels": [], "entities": []}, {"text": "For each question type, we uniformly use the same, identical set of features.", "labels": [], "entities": []}, {"text": "The following learning parameters were found to give the best HuinSent accuracy and were uniformly used in generating all the decision tree classifiers for all question types reported in this paper: m = 37, t = 7, and nip cost = 1.2.", "labels": [], "entities": [{"text": "HuinSent", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.7510586380958557}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9526360034942627}]}, {"text": "Using a large rn results in simpler decision trees, t = 7 results in the use of boosting with multiple decision trees.", "labels": [], "entities": []}, {"text": "Since there area lot more negative training examples compared to positive training examples (ratio of approximately 4:1), there is a tendency to generate a default tree classifying all training examples as negative (since the accuracy of such a tree is already quite goodabout 80% on our skewed distribution of training examples).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 226, "end_pos": 234, "type": "METRIC", "confidence": 0.9987573623657227}]}, {"text": "Setting nip cost at 1.2 will make it more costly to misclassify a positive training example as negative, and thus more costly to generate the default tree, resulting in better accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9967448711395264}]}, {"text": "We achieved an overall HumSent accuracy of 39.3% on the 300 test questions.", "labels": [], "entities": [{"text": "HumSent", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.7879053354263306}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.7343471646308899}]}, {"text": "The breakdown into the number of questions answered correctly per question type is shown in the first row of Table 1.", "labels": [], "entities": []}, {"text": "Our results indicate that our machine learning approach can achieve accuracy comparable with other approaches that rely on handcrafted, deterministic rules and algorithms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9992923736572266}]}, {"text": "For comparison, the HumSent scores reported in the work of (Hirschm~.n et al., 1999), (),), and () are 36.3%, 41%, 39.7%, and 14%, respectively.", "labels": [], "entities": [{"text": "Hirschm~.n et al., 1999)", "start_pos": 60, "end_pos": 84, "type": "DATASET", "confidence": 0.877500981092453}]}, {"text": "shows the sequence of decision trees generated via boosting for the when question type.", "labels": [], "entities": []}, {"text": "All the trees look reasonable and intuitive.", "labels": [], "entities": []}, {"text": "The first tree states that if the Diff-from-Max-Word-Match is zero (i.e., the sentence has the highest number of word match to the question), then the sentence is an answer.", "labels": [], "entities": []}, {"text": "Otherwise, the classifier tests for whether the sentence contains a date.", "labels": [], "entities": []}, {"text": "If it does, then the sentence is an answer, else it is not an answer.", "labels": [], "entities": []}, {"text": "The second tree is a default tree that just classifies any sentence as not an answer.", "labels": [], "entities": []}, {"text": "The rest of the trees similarly test on features that we intuitively feel are indicative of whether a sentence answers to a when question.", "labels": [], "entities": []}, {"text": "To investigate the relative importance of each type of features, we remove one type of features at a time and observe its impact on HuinSent accuracy.", "labels": [], "entities": [{"text": "HuinSent", "start_pos": 132, "end_pos": 140, "type": "DATASET", "confidence": 0.8817347884178162}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9622267484664917}]}, {"text": "The resulting drop inaccuracy is tabulated in the remaining rows of.", "labels": [], "entities": []}, {"text": "The rows are ordered in decreasing overall HumSent accuracy.", "labels": [], "entities": [{"text": "HumSent", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.6657847166061401}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.943484902381897}]}, {"text": "As expected, removing the word match feature causes the largest drop in overall accuracy, and the accuracy decline affects all question types.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9952404499053955}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9990057349205017}]}, {"text": "Removing the five named entity features also causes a large decline, affecting mainly the who, when, and where questions.", "labels": [], "entities": []}, {"text": "Named entities are useful for answering these question types, since who typically asks fora person (or organization), when asks for date or time, and where asks for location.", "labels": [], "entities": []}, {"text": "What is perhaps a little surprising is that the seven automatically discovered keywords are also found to be very important, and removing these seven features causes the second largest decline in overall HumSent accuracy.", "labels": [], "entities": [{"text": "HumSent", "start_pos": 204, "end_pos": 211, "type": "DATASET", "confidence": 0.6855858564376831}, {"text": "accuracy", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.8646530508995056}]}, {"text": "Coreference is found to affect the who, when, and where questions, as expected.", "labels": [], "entities": []}, {"text": "The previous and next word/verb matches cause the largest decline for why questions, dropping the number of correctly answered why questions to 3.", "labels": [], "entities": [{"text": "why questions", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.9315398335456848}]}, {"text": "Removing verb match also causes a 3% drop in overall accuracy, while dateline and title only affect the when questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9754648208618164}]}, {"text": "In our future work, we plan to investigate other potential knowledge sources that may further improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9965200424194336}]}, {"text": "We also plan to investigate the use of other supervised machine learning algorithms for this problem.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy using different set", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981979727745056}]}]}