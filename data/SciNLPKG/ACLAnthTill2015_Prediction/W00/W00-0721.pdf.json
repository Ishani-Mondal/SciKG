{"title": [{"text": "Shallow Parsing by Inferencing with Classifiers*", "labels": [], "entities": []}], "abstractContent": [{"text": "We study the problem of identifying phrase structure.", "labels": [], "entities": [{"text": "identifying phrase structure", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.8849910100301107}]}, {"text": "We formalize it as the problem of combining the outcomes of several different clas-sifiers in away that provides a coherent inference that satisfies some constraints, and develop two general approaches for it.", "labels": [], "entities": []}, {"text": "The first is a Markovian approach that extends standard HMMs to allow the use of a rich observations structure and of general classifiers to model state-observation dependencies.", "labels": [], "entities": []}, {"text": "The second is an extension of constraint satisfaction formalisms.", "labels": [], "entities": []}, {"text": "We also develop efficient algorithms under both models and study them experimentally in the context of shallow parsing.", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.6180060803890228}]}, {"text": "1 1 Identifying Phrase Structure The problem of identifying phrase structure can be formalized as follows.", "labels": [], "entities": [{"text": "identifying phrase structure", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.8156300981839498}]}, {"text": "Given an input string O =< ol, 02,..., On >, a phrase is a substring of consecutive input symbols oi, oi+l,...,oj.", "labels": [], "entities": []}, {"text": "Some external mechanism is assumed to consistently (or stochastically) annotate substrings as phrases 2.", "labels": [], "entities": []}, {"text": "Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language (Church, 1988; Ramshaw and Marcus, 1995; Mufioz et al., 1999; Cardie and Pierce, 1998).", "labels": [], "entities": []}, {"text": "The identification mechanism works by using classifiers that process the input string and recognize in the input string local signals which * This research is supported by NSF grants IIS-9801638, SBR-9873450 and IIS-9984168.", "labels": [], "entities": [{"text": "SBR-9873450", "start_pos": 196, "end_pos": 207, "type": "DATASET", "confidence": 0.6360337734222412}, {"text": "IIS-9984168", "start_pos": 212, "end_pos": 223, "type": "DATASET", "confidence": 0.9092795848846436}]}, {"text": "1Full version is in (Punyakanok and Roth, 2000).", "labels": [], "entities": [{"text": "1Full", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9047755002975464}]}, {"text": "2We assume here a single type of phrase, and thus each input symbol is either in a phrase or outside it.", "labels": [], "entities": []}, {"text": "All the methods we discuss can be extended to deal with several kinds of phrases in a string, including different kinds of phrases and embedded phrases. are indicative to the existence of a phrase.", "labels": [], "entities": []}, {"text": "Local signals can indicate that an input symbol o is inside or outside a phrase (IO modeling) or they can indicate that an input symbol o opens or closes a phrase (the OC modeling) or some combination of the two.", "labels": [], "entities": []}, {"text": "In any case, the local signals can be combined to determine the phrases in the input string.", "labels": [], "entities": []}, {"text": "This process, however , needs to satisfy some constraints for the resulting set of phrases to be legitimate.", "labels": [], "entities": []}, {"text": "Several types of constraints, such as length and order can be formalized and incorporated into the mechanisms studied here.", "labels": [], "entities": []}, {"text": "For simplicity, we focus only on the most basic and common constraint we assume that phrases do not overlap.", "labels": [], "entities": []}, {"text": "The goal is thus twofold: to learn classifiers that recognize the local signals and to combine these in a ways that respects the constraints.", "labels": [], "entities": []}, {"text": "2 Markov Modeling HMM is a probabilistic finite state automaton used to model the probabilistic generation of sequential processes.", "labels": [], "entities": []}, {"text": "The model consists of a finite set S of states, a set (9 of observations , an initial state distribution P1 (s), a state-transition distribution P(s[s') for s, # E Sand an observation distribution P(o[s) for o E (9 and s 6 S.", "labels": [], "entities": []}, {"text": "3 Ina supervised learning task, an observation sequence O-< ol,o2,...", "labels": [], "entities": []}, {"text": "On > is supervised by a corresponding state sequence S =< sl, s2,.", "labels": [], "entities": []}, {"text": "The supervision can also be supplied , as described in Sec.", "labels": [], "entities": []}, {"text": "1, using the local signals.", "labels": [], "entities": []}, {"text": "Constraints can be incorporated into the HMM by constraining the state transition probability distribution P(s]s').", "labels": [], "entities": []}, {"text": "For example, set P(sV) = 0 for all s, s' such that the transition from s ~ to sis not allowed.", "labels": [], "entities": []}, {"text": "aSee (Rabiner, 1989) fora comprehensive tutorial.", "labels": [], "entities": [{"text": "aSee (Rabiner, 1989)", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.8082934617996216}]}], "introductionContent": [], "datasetContent": [{"text": "We experimented both with base noun phrases (NP) and subject-verb patterns (SV) and show results for two different representations of the observations (that is, different feature sets for the classifiers) -part of speech (POS) tags only and POS with additional lexical information (words).", "labels": [], "entities": []}, {"text": "The data sets used are the standard data sets for this problem) taken from the Wall Street Journal corpus in the Penn Treebank ().", "labels": [], "entities": [{"text": "Wall Street Journal corpus in the Penn Treebank", "start_pos": 79, "end_pos": 126, "type": "DATASET", "confidence": 0.9381101503968239}]}, {"text": "For each model we study three different classifiers.", "labels": [], "entities": []}, {"text": "The simple classifier corresponds to the standard HMM in which P(ols ) is estimated directly from the data.", "labels": [], "entities": []}, {"text": "The NB (naive Bayes) and SNoW classifiers use the same feature set, conjunctions of size 3 of POS tags (+ words) in a window of size 6 around the target word.", "labels": [], "entities": []}, {"text": "The first important observation is that the SV task is significantly more difficult than the NP task.", "labels": [], "entities": []}, {"text": "This is consistent for all models and all features sets.", "labels": [], "entities": []}, {"text": "When comparing between different models and features sets, it is clear that the simple HMM formalism is not competitive with the other two models.", "labels": [], "entities": []}, {"text": "What is interesting here is the very significant sensitivity to the wider notion of observations (features) used by the classifiers, despite the violation of the probabilistic assumptions.", "labels": [], "entities": []}, {"text": "For the easier NP task, the HMM model is competitive with the others when the classifiers used are NB or SNOW.", "labels": [], "entities": [{"text": "NP task", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.9179827272891998}]}, {"text": "In particular, a significant improvement in both probabilistic methods is achieved when their input is given by SNOW.", "labels": [], "entities": [{"text": "SNOW", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9561653733253479}]}, {"text": "Our two main methods, PMM and CSCL, perform very well on predicting NP and SV phrases with CSCL at least as good as any other methods tried on these tasks.", "labels": [], "entities": [{"text": "predicting NP and SV phrases", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.8535561442375184}]}], "tableCaptions": [{"text": " Table 1: Results (F~=l) of different methods", "labels": [], "entities": [{"text": "F~=l)", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.959683746099472}]}]}