{"title": [{"text": "Empirical Term Weighting and Expansion Frequency", "labels": [], "entities": [{"text": "Empirical Term Weighting", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5799945791562399}]}], "abstractContent": [{"text": "We propose an empirical method for estimating term weights directly from relevance judgements, avoiding various standard but potentially troublesome assumptions.", "labels": [], "entities": []}, {"text": "It is common to assume, for example , that weights vary with term frequency (t f) and inverse document frequency (idf) in a particular way, e.g., tf. idf, but the fact that there are so many variants of this formula in the literature suggests that there remains considerable uncertainty about these assumptions.", "labels": [], "entities": [{"text": "term frequency (t f)", "start_pos": 61, "end_pos": 81, "type": "METRIC", "confidence": 0.8852635025978088}, {"text": "inverse document frequency (idf)", "start_pos": 86, "end_pos": 118, "type": "METRIC", "confidence": 0.8086711019277573}]}, {"text": "Our method is similar to the Berkeley regression method where labeled relevance judgements are fit as a linear combination of (transforms of) t f, idf, etc.", "labels": [], "entities": []}, {"text": "Training methods not only improve performance, but also extend naturally to include additional factors such as burstiness and query expansion.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 126, "end_pos": 141, "type": "TASK", "confidence": 0.7450492680072784}]}, {"text": "The proposed histogram-based training method provides a simple way to model complicated interactions among factors such as t f, idf, burstiness and expansion frequency (a generalization of query expansion).", "labels": [], "entities": []}, {"text": "The correct handling of expanded term is realized based on statistical information.", "labels": [], "entities": []}, {"text": "Expansion frequency dramatically improves performance from a level comparable to BKJJBIDS, Berkeley's entry in the Japanese NACSIS NTCIR-1 evaluation for short queries, to the level of JCB1, the top system in the evaluation.", "labels": [], "entities": [{"text": "BKJJBIDS", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.7457964420318604}, {"text": "NACSIS NTCIR-1 evaluation", "start_pos": 124, "end_pos": 149, "type": "DATASET", "confidence": 0.7442300915718079}, {"text": "JCB1", "start_pos": 185, "end_pos": 189, "type": "DATASET", "confidence": 0.7260475158691406}]}, {"text": "JCB1 uses sophisticated (and proprietary) natural language processing techniques developed by Just System, a leader in the Japanese word-processing industry.", "labels": [], "entities": [{"text": "JCB1", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9642327427864075}]}, {"text": "We are encouraged that the proposed method, which is simple to understand and replicate, can reach this level of performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "An empirical method for estimating term weights directly from relevance judgements is proposed.", "labels": [], "entities": []}, {"text": "The method is designed to make as few assumptions as possible.", "labels": [], "entities": []}, {"text": "It is similar to Berkeley's use of regression  The method extends naturally to include additional factors such as query expansion.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 114, "end_pos": 129, "type": "TASK", "confidence": 0.7115469574928284}]}, {"text": "Terms mentioned explicitly in the query receive much larger weights than terms brought in via query expansion.", "labels": [], "entities": []}, {"text": "In addition, whether or not a term t is mentioned explicitly in the query, if t appears in documents brought in by query expansion (el(t) > 1) then twill receive a much larger weight than it would have otherwise (ef(t) = 0).", "labels": [], "entities": []}, {"text": "The interactions among these factors, however, are complicated and collection dependent.", "labels": [], "entities": []}, {"text": "It is safer to use histogram methods than to impose unnecessary and potentially troublesome assumptions such as normality and independence.", "labels": [], "entities": []}, {"text": "Under the vector space model, the score fora document d and a query q is computed by summing a contribution for each term t over an appropriate set of terms, T. T is often limited to terms shared by both the document and the query (minus stop words), though not always (e.g, query expansion).", "labels": [], "entities": [{"text": "query expansion", "start_pos": 275, "end_pos": 290, "type": "TASK", "confidence": 0.7261186987161636}]}, {"text": "Under the probabilistic retrieval model, documents are scored by summing a similar contribution for each term t.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two measures of performance are reported: (1) 11 point average precision and (2) R, precision after retrieving Nrd documents, where Nrd is the number of relevant documents.", "labels": [], "entities": [{"text": "11 point average", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.8217008908589681}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.5283787846565247}, {"text": "R", "start_pos": 81, "end_pos": 82, "type": "METRIC", "confidence": 0.9991317391395569}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9993879795074463}]}, {"text": "We used the \"short query\" condition of the NACSIS NTCIR-1 Test Collection () which consists of about 300,000 documents in Japanese, plus about 30 queries with labeled relevance judgement for training and 53 queries with relevance judgements for testing.", "labels": [], "entities": [{"text": "NACSIS NTCIR-1 Test Collection", "start_pos": 43, "end_pos": 73, "type": "DATASET", "confidence": 0.8848482221364975}]}, {"text": "The result of \"short query\" is shown in page 25 of(), which shows that \"short query\" is hard for statistical methods.", "labels": [], "entities": []}, {"text": "Two previously published systems are included in the tables below: JCB1 and BKJJBIDS.", "labels": [], "entities": [{"text": "JCB1", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.9389742612838745}, {"text": "BKJJBIDS", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9037938714027405}]}, {"text": "JCB1, submitted by Just System, a company with a commercially successful product for Japanese wordprocessing, produced the best results using sophisticated (and proprietary) natural language processing techniques.)", "labels": [], "entities": [{"text": "JCB1", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9538896679878235}]}, {"text": "BKJJBIDS used Berkeley's logistic regression methods (with about half a dozen variables) to fit term weights to the labeled training material.", "labels": [], "entities": [{"text": "BKJJBIDS", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9681735634803772}]}, {"text": "shows that training often helps.", "labels": [], "entities": []}, {"text": "The methods above the line (with the possible exception of JCB1) use training; the methods below the line do not.", "labels": [], "entities": [{"text": "JCB1", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9656639695167542}]}, {"text": "Fit-E has very respectable performance, nearly up to the level of JCB1, not bad fora purely statistical method.", "labels": [], "entities": [{"text": "JCB1", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.8068563342094421}]}, {"text": "The performance of fit-B is close to that of BKJJBIDS.", "labels": [], "entities": [{"text": "fit-B", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9730004072189331}, {"text": "BKJJBIDS", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9036430716514587}]}, {"text": "For comparison sake, fit-B is shown both with and without the K filter.", "labels": [], "entities": [{"text": "fit-B", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9802438020706177}]}, {"text": "The K filter restricts terms to sequences of Katakana and Kanji characters.", "labels": [], "entities": []}, {"text": "BKJJBIDS uses a similar heuristic to eliminate Japanese function words.", "labels": [], "entities": [{"text": "BKJJBIDS", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9635140895843506}]}, {"text": "Although the K filter does not change performance very much, the use of this filter changes the relative order of fit-B and BKJJBIDS.", "labels": [], "entities": [{"text": "fit-B", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.975109338760376}, {"text": "BKJJBIDS", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.7830166220664978}]}, {"text": "These results suggest that R \u2022 B: restrict terms to bursty (B --1) terms \u2022 Ek: require terms to appear in more thank docs brought in by query expansion (el(t) > k).: The best filters (Ek) improve the performance of the best method (fit-E) to nearly the level of JCB1.", "labels": [], "entities": [{"text": "JCB1", "start_pos": 262, "end_pos": 266, "type": "DATASET", "confidence": 0.8768731355667114}]}, {"text": "the K filter is slightly unhelpful.", "labels": [], "entities": []}, {"text": "A number of filters have been considered (table 9).", "labels": [], "entities": []}, {"text": "Results vary somewhat depending on these choices, though not too much, which is fortunate, since since we don't understand stop lists very well.", "labels": [], "entities": []}, {"text": "To the extent that there is a pattern, we suspect that words axe slightly better than bigrams, and that the E filter is slightly better than the B filter which is slightly better than the K filter.", "labels": [], "entities": []}, {"text": "Table 10 shows that the best filters (Ek) improve the performance of the best method (fit-E) to nearly the level of JCB1.: Limits do no harm: two limits are slightly better than one, and oneis slightly better than none.", "labels": [], "entities": [{"text": "JCB1.", "start_pos": 116, "end_pos": 121, "type": "DATASET", "confidence": 0.7961660027503967}]}, {"text": "(UL = upper limit of ~ < idf; LL = lower limit of 0 _< ~) The final experiment shows that restricting ~ to 0 < ~ < id] improves performance slightly.", "labels": [], "entities": [{"text": "UL", "start_pos": 1, "end_pos": 3, "type": "METRIC", "confidence": 0.9226807951927185}]}, {"text": "The combination of both the upper limit and the lower limit is slightly better than just one limit which is better than none.", "labels": [], "entities": []}, {"text": "We view limits as a robustness device.", "labels": [], "entities": []}, {"text": "Hopefully, they won't have to do much but every once in awhile they prevent the system from wandering far astray.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Empirical estimates of A as a function of", "labels": [], "entities": [{"text": "Empirical", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9684324264526367}, {"text": "A", "start_pos": 33, "end_pos": 34, "type": "METRIC", "confidence": 0.8979474306106567}]}, {"text": " Table 3: Training file schema: a record of 25 fields  is computed for each term (ngram) in each query  in training set.", "labels": [], "entities": []}, {"text": " Table 4: Regression coefficients for method fit-G.  This table approximates the data in table 1 with  ~ a(tf) + b(tf), idf. Note that both the inter- cepts, a(tf), and the slopes, b(tf), increase with  tf (with a minor exception for b(4+)).", "labels": [], "entities": [{"text": "Regression", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9875231385231018}]}, {"text": " Table 5: A comparison of the regression coeffi- cients for method fit-G with comparable coeffi- cients from the multiple regression: A = a2 + b2 \u2022  idf + c2 \u2022 log(1 + t f) where a2 -----4.1, b2 = 0.66", "labels": [], "entities": []}, {"text": " Table 6: Regression coefficients for method fit-B.  Note that the slopes and intercepts are larger when  B = 1 than when B = 0 (except when tf = 0).  Even though A usually lies between-0 and idf, we  restrict A to 0 < A < idf, just to make sure.", "labels": [], "entities": [{"text": "Regression", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9933964014053345}]}, {"text": " Table 8: Training helps: methods above the line  use training (with the possible exception of JCB1);  methods below the line do not.", "labels": [], "entities": [{"text": "JCB1", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.9449878334999084}]}, {"text": " Table 9: Filters: results vary somewhat depending  on these choices, though not too much, which is  fortunate, since since we don't understand stop  lists very well.", "labels": [], "entities": []}, {"text": " Table 10: The best filters (Ek) improve the per-", "labels": [], "entities": []}, {"text": " Table 11: Limits do no harm: two limits are  slightly better than one, and oneis slightly bet- ter than none. (UL = upper limit of ~ < idf; LL", "labels": [], "entities": [{"text": "UL", "start_pos": 112, "end_pos": 114, "type": "METRIC", "confidence": 0.8998128771781921}]}]}