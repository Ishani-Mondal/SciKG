{"title": [], "abstractContent": [{"text": "Error-correcting output codes (ECOC) have emerged in machine learning as a successful implementation of the idea of distributed classes.", "labels": [], "entities": []}, {"text": "Monadic class symbols are replaced by bit strings, which are learned by an ensemble of binary-valued classifiers (dichotomizers).", "labels": [], "entities": []}, {"text": "In this study, the idea of ECOC is applied to memory-based language learning with local (k-nearest neighbor) classifiers.", "labels": [], "entities": []}, {"text": "Regression analysis of the experimental results reveals that, in order for ECOC to be successful for language learning, the use of the Modified Value Difference Metric (MVDM) is an important factor, which is explained in terms of population density of the class hyperspace.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised learning methods applied to natural language classification tasks commonly operate on high-level symbolic representations, with linguistic classes that are usually monadic, without internal structure (.", "labels": [], "entities": [{"text": "natural language classification tasks", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.7490868419408798}]}, {"text": "This contrasts with the distributed class encoding commonly found in neural networks.", "labels": [], "entities": []}, {"text": "Error-correcting output codes (ECOC) have been introduced to machine learning as a principled and successful approach to distributed class encoding;).", "labels": [], "entities": [{"text": "distributed class encoding", "start_pos": 121, "end_pos": 147, "type": "TASK", "confidence": 0.6632303595542908}]}, {"text": "With ECOC, monadic classes are replaced by codewords, i.e. binary-valued vectors.", "labels": [], "entities": [{"text": "ECOC", "start_pos": 5, "end_pos": 9, "type": "DATASET", "confidence": 0.8987167477607727}]}, {"text": "An ensemble of separate classifiers (dichotomizers) must be trained to learn the binary subclassifications for every instance in the training set.", "labels": [], "entities": []}, {"text": "During classification, the bit predictions of the various dichotomizers are combined to produce a codeword prediction.", "labels": [], "entities": []}, {"text": "The class codeword which has minimal Hamming distance to the predicted codeword determines the classification of the instance.", "labels": [], "entities": []}, {"text": "Codewords are constructed such that their Hamming distance is maximal.", "labels": [], "entities": []}, {"text": "Extra bits are added to allow for error recovery, allowing the correct class to be determinable even if some bits are wrong.", "labels": [], "entities": [{"text": "error recovery", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7250107526779175}]}, {"text": "An error-correcting output code fora k-class problem constitutes a matrix with k rows and 2 k-1-1 columns.", "labels": [], "entities": []}, {"text": "Rows are the codewords corresponding to classes, and columns are binary subclassifications orbit functions fi such that, for an instance e, and its codeword vector tions.", "labels": [], "entities": []}, {"text": "From an information-theoretic perspective, classification with ECOC is like channel coding: the class of a pattern to be classified is a datum sent over a noisy communication channel.", "labels": [], "entities": [{"text": "channel coding", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7753813862800598}]}, {"text": "The communication channel consists of the trained classifier.", "labels": [], "entities": []}, {"text": "The noise consists of the bias (systematic error) and variance (training set-dependent error) of the classifier, which together makeup for the overall error of the classifier.", "labels": [], "entities": [{"text": "systematic error) and variance (training set-dependent error)", "start_pos": 32, "end_pos": 93, "type": "METRIC", "confidence": 0.7358824044466019}]}, {"text": "The received message must be decoded before it can be interpreted as a classification.", "labels": [], "entities": []}, {"text": "Adding redundancy to a signal before transmission is a well-known technique in digital communication to allow for the recovery of errors due to noise in the channel, and this is the key to the success of ECOC.", "labels": [], "entities": [{"text": "ECOC", "start_pos": 204, "end_pos": 208, "type": "DATASET", "confidence": 0.6712216734886169}]}, {"text": "From a machine learning perspective, an error-correcting output code uniquely partitions the instances in the training set into two disjoint subclasses, 0 or 1.", "labels": [], "entities": []}, {"text": "This can be interpreted as learning a set of class boundaries.", "labels": [], "entities": []}, {"text": "To illustrate this, consider the following binary code fora three-class problem.", "labels": [], "entities": []}, {"text": "(This actually is a one-of-c code with no error-correcting capability (the minimal Hamming distance between the codewords is 1).", "labels": [], "entities": []}, {"text": "As such it is an error-correcting code with lowest error correction, but it serves to illustrate the point.) fl f2 f3 For every combination of classes (C1-C2, C1-C3, C2-C3), the Hamming distance between the codewords is 2.", "labels": [], "entities": [{"text": "error correction", "start_pos": 51, "end_pos": 67, "type": "METRIC", "confidence": 0.9370248019695282}]}, {"text": "These horizontal relations have vertical repercussions as well: for every such pair, two bit functions disagree in the classes they select.", "labels": [], "entities": []}, {"text": "For C1-C2, f2 selects C2 and f3 selects C1.", "labels": [], "entities": []}, {"text": "For C1-C3, fl selects C3 and f3 selects C1.", "labels": [], "entities": []}, {"text": "Finally, for C2-C3, fl selects C3 and f2 selects C2.", "labels": [], "entities": []}, {"text": "So, every class is selected two times, and this implies that every class boundary associated with that class in the feature hyperspace is learned twice.", "labels": [], "entities": []}, {"text": "In general, if the minimal Hamming distance between the codewords of an (error-correcting) code is d, then every class boundary is learned d times.", "labels": [], "entities": []}, {"text": "For the error-correcting code from above this implies an error correction of zero: only two votes support a class boundary, and no vote can be favored in case of a conflict.", "labels": [], "entities": [{"text": "error correction", "start_pos": 57, "end_pos": 73, "type": "METRIC", "confidence": 0.9466211795806885}]}, {"text": "The decoding of the predicted bit string to a class symbol appears to be a form of voting over class boundaries (, and is able to reduce both bias and variance of the classifier.", "labels": [], "entities": [{"text": "variance", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9436813592910767}]}], "datasetContent": [{"text": "The effects of a distributed class representation on generalization accuracy were measured using an experimental matrix based on 5 linguistic datasets, and 8 experimental conditions, addressing feature selection-based ECOC vs. voting-based ECOC, MVDM, values of k larger than 1, and dichotomizer weighting.", "labels": [], "entities": [{"text": "generalization", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.9506415128707886}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.8565093278884888}]}, {"text": "The following linguistic tasks were used.", "labels": [], "entities": []}, {"text": "DIMIN is a Dutch diminutive formation task derived from the Celex lexical database for Dutch ( For feature selection-based ECOC, backward sequential feature elimination was used (Raaijmakers, 1999), repeatedly eliminating features in turn and evaluating each elimination step with 10-fold cross-validation.", "labels": [], "entities": [{"text": "Dutch diminutive formation task", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.6956614404916763}, {"text": "Celex lexical database", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.9289845625559489}]}, {"text": "For dichotomizer weighting, error information of the dichotomizers, determined from separate unweighted 10-fold cross-validation experiments on a separate training set, produced a weighted Hamming distance metric.", "labels": [], "entities": [{"text": "dichotomizer weighting", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8345960676670074}]}, {"text": "Error-based weights were based on raising a small constant ~ in the interval [0, 1) to the power of the number of errors made by the dichotomizer.", "labels": [], "entities": []}, {"text": "Random feature selection drawing features with replacement created feature sets of both different size and composition for every dichotomizer.: Generalization accuracies control groups.", "labels": [], "entities": []}, {"text": "tributed class representations can lead to statistically significant accuracy gains fora variety of linguistic tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9961913824081421}]}, {"text": "The ECOC algorithm based on feature selection and weighted Hamming distance performs best.", "labels": [], "entities": []}, {"text": "Voting-based ECOC performs poorly on DIMIN and STRESS with voting per bit, but significant accuracy gains are achieved by voting per block, putting it on a par with the best performing algorithm.", "labels": [], "entities": [{"text": "STRESS", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.6573523879051208}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9988126754760742}]}, {"text": "Regression analysis was applied to investigate the effect of the Modified Value Difference Metric on ECOC accuracy.", "labels": [], "entities": [{"text": "Regression", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9585145711898804}, {"text": "Modified Value Difference Metric", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.6112525090575218}, {"text": "ECOC", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.6038029193878174}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9395571947097778}]}, {"text": "First, the accuracy gain of MVDM as a function of the information gain ratio of the features was computed.", "labels": [], "entities": [{"text": "accuracy gain", "start_pos": 11, "end_pos": 24, "type": "METRIC", "confidence": 0.9917579591274261}]}, {"text": "The results show a high correlation (0.82, significant at p < 0.05) between these variables, indicating a linear relation.", "labels": [], "entities": []}, {"text": "This is inline with the idea underlying MVDM: whenever two feature values are very predictive of a shared class, they contribute to the similarity between the instances they belong to, which will lead to more accurate classifiers.", "labels": [], "entities": []}, {"text": "Next, regression analysis was applied to determine the effect of MVDM on ECOC, by relating the accuracy gain of MVDM   ).", "labels": [], "entities": [{"text": "ECOC", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9112099409103394}, {"text": "accuracy gain", "start_pos": 95, "end_pos": 108, "type": "METRIC", "confidence": 0.984167754650116}]}, {"text": "This idea is applied to a subset of DIMIN, consisting of all instances classified as j e (one of the five diminutive suffixes for Dutch).", "labels": [], "entities": []}, {"text": "The features for this subset were limited to the last two, consisting of the rhyme and coda of the last syllable of the word, clearly the most informative features for this task.", "labels": [], "entities": [{"text": "rhyme", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9739570021629333}]}, {"text": "displays the two scatter plots.", "labels": [], "entities": []}, {"text": "As can be seen, instances are widely scattered over the feature space for the numerical transform, whereas the MVDMbased transform forms many clusters and produces much higher density.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Generalization accuracies control groups.", "labels": [], "entities": []}, {"text": " Table 4: Generalization accuracies for feature selection-based ECOC (x/ indicates significant improvement over", "labels": [], "entities": []}]}