{"title": [{"text": "Incorporating Position Information into a Maximum Entropy/Minimum Divergence Translation Model", "labels": [], "entities": [{"text": "Minimum Divergence Translation", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.6330718696117401}]}], "abstractContent": [{"text": "I describe two methods for incorporating information about the relative positions of bilingual word pairs into a Maximum Entropy/Minimum Divergence translation model.", "labels": [], "entities": []}, {"text": "The better of the two achieves over 40% lower test corpus perplex-ity than an equivalent combination of a trigram language model and the classical IBM translation model 2.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) systems use a model ofp(tls), the probability that a text sin the source language will translate into a text tin the target language, to determine the best translation fora given source text.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7726223568121592}]}, {"text": "A straightforward way of modeling this distribution is to apply a chain-rule expansion of the form: where ti denotes the ith token int.", "labels": [], "entities": []}, {"text": "1 The objects to be modeled in this case belong to the family of conditional distributions p(wlhi, s), the probability of the ith word int, given the tokens which precede it and the source text.", "labels": [], "entities": []}, {"text": "The main motivation for modeling p(tls ) in terms of p(wlhi , s) is that it simplifies the \"decoding\" problem of finding the most likely target text.", "labels": [], "entities": []}, {"text": "In particular, if hi is known, finding the best word at the current position requires only a straightforward search through the target 1This ignores the issue of normalization over target texts of all possible lengths, which can be easily enforced when desired by using a stop token or a prior distribution over lengths.", "labels": [], "entities": []}, {"text": "vocabulary, and efficient dynamic-programming based heuristics can be used to extend this to sequences of words.", "labels": [], "entities": []}, {"text": "This is very important for applications such as TransType (;, where the task is to make real-time predictions of the text a human translator will type next, based on the source text under translation and some prefix of the target text that has already been typed.", "labels": [], "entities": []}, {"text": "The standard \"noisy channel\" approach used in SMT, where p(tls ) c< p(t)p(slt), is generally too expensive for such applications because it does not permit direct calculation of the probability of a word or sequence of words beginning at the current position.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.994840681552887}]}, {"text": "Complex and expensive search strategies are required to find the best target text in this approach ().", "labels": [], "entities": []}, {"text": "The challenge in modeling p(wlhi,s ) is to combine two disparate sources of conditioning information in an effective way.", "labels": [], "entities": []}, {"text": "One obvious strategy is to use a linear combination of separate language and translation components, of the form: where p(w[hi) is a language model, p(wli , s) is a translation model, and A E [0, 1] is a combining weight.", "labels": [], "entities": []}, {"text": "However, this appears to be a weak technique), even when A is allowed to depend on various features of the context (hi, s).", "labels": [], "entities": []}, {"text": "In previous work, I described a Maximum Entropy/Minimum Divergence (MEMD) model) for p(w[hi, s) which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 (.", "labels": [], "entities": [{"text": "Maximum Entropy/Minimum Divergence (MEMD)", "start_pos": 32, "end_pos": 73, "type": "METRIC", "confidence": 0.7779630795121193}]}, {"text": "This model significantly outperforms an equivalent linear combination of a trigram and model 1 in testcorpus perplexity, despite using several orders of magnitude fewer translation parameters.", "labels": [], "entities": []}, {"text": "Like model 1, its translation component is based only on the occurrences in s of words which are potential translations for w, and does not take into account the positions of these words relative tow.", "labels": [], "entities": []}, {"text": "An obvious enhancement is to incorporate such positional information into the MEMD model, thereby making its translation component analogous to the IBM model 2.", "labels": [], "entities": []}, {"text": "This is the problem I address in this paper.", "labels": [], "entities": []}, {"text": "as each position, including the empty position 0, is considered equally likely to contain a translation for w.", "labels": [], "entities": []}, {"text": "Maximum likelihood estimates for these parameters can be obtained with the EM algorithm over a bilingual training corpus, as described in 2Model 2 was originally formulated for p(tls), but since target words are predicted independently it can also be used for p(wlhi , s).", "labels": [], "entities": []}, {"text": "The only necessary modification in this case is that the position parameters can no longer be conditioned on It[.", "labels": [], "entities": []}, {"text": "where q(w[hi,s) is a reference distribution, f(w, hi, s) maps (w, hi, s) into an n-dimensional feature vector, (~ is a corresponding vector of feature weights (the parameters of the model), and Z(hi, s) = ~w q(w[hi, s) exp((~-f(w, hi)) is a normalizing factor.", "labels": [], "entities": []}, {"text": "For a given choice of q and f, the IIS algorithm ( can be used to find maximum likelihood values for the parameters ~.", "labels": [], "entities": [{"text": "IIS", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.6255331635475159}]}, {"text": "It can be shown) that these are the also the values which minimize the between the model and the reference distribution under the constraint that the expectations of the features (ie, the components of f) with respect to the model must equal their expectations with respect to the empirical distribution derived from the training corpus.", "labels": [], "entities": []}, {"text": "Thus the reference distribution serves as a kind of prior, and should reflect some initial knowledge about the true distribution; and the use of any feature is justified to the extent that its empirical expectation is accurate.", "labels": [], "entities": []}, {"text": "In the present context, the natural choice for the reference distribution q is a trigram language model.", "labels": [], "entities": []}, {"text": "To create a MEMD analog to IBM model 1 (MEMD1), I used boolean features corresponding to bilingual word pairs: where (s, t) is a (source,target) word pair.", "labels": [], "entities": []}, {"text": "Using the notational convention that ast is 0 whenever the corresponding feature fst does not exist in the model, MEMD1 can be written compactly as: p(wlhi,s) = q(wlhi) exp(~ asw)/Z(hi,s).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Corpus segmentation. The train segment was the main training corpus; the held-out 1", "labels": [], "entities": [{"text": "Corpus segmentation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.796055942773819}]}, {"text": " Table 2: Model performances. Linear interpolation is designated with a + sign; and the MEMD2B  position parameters are given as rex, where m and n are the numbers of position partitions and  word-pair partitions respectively.", "labels": [], "entities": []}]}