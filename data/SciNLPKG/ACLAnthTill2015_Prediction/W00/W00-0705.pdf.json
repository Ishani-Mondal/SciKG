{"title": [{"text": "Increasing our Ignorance of Language: Identifying Language Structure in an Unknown 'Signal'", "labels": [], "entities": [{"text": "Increasing our Ignorance of Language", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8536686778068543}, {"text": "Identifying Language Structure in an Unknown 'Signal'", "start_pos": 38, "end_pos": 91, "type": "TASK", "confidence": 0.7913649413320754}]}], "abstractContent": [{"text": "This paper describes algorithms and software developed to characterise and detect generic intelligent language-like features in an input signal, using natural language learning techniques: looking for characteristic statistical \"language-signatures\" in test corpora.", "labels": [], "entities": []}, {"text": "As a first step towards such species-independent language-detection, we present a suite of programs to analyse digital representations of a range of data, and use the results to extrapolate whether or not there are language-like structures which distinguish this data from other sources, such as music, images, and white noise.", "labels": [], "entities": []}, {"text": "Outside our own immediate NLP sphere, generic communication techniques are of particular interest in the astronautical community, where two sessions are dedicated to SETI at their annual International conference with topics ranging from detecting ET technology to the ethics and logistics of message construction (E1-liott and Atwell, 1999; Ollongren, 2000; Vakoch, 2000).", "labels": [], "entities": [{"text": "SETI", "start_pos": 166, "end_pos": 170, "type": "TASK", "confidence": 0.9809166789054871}, {"text": "International", "start_pos": 187, "end_pos": 200, "type": "DATASET", "confidence": 0.8857951760292053}, {"text": "message construction", "start_pos": 292, "end_pos": 312, "type": "TASK", "confidence": 0.6595166176557541}]}], "introductionContent": [{"text": "A useful thought experiment is to imagine eavesdropping on a signal from outer space.", "labels": [], "entities": []}, {"text": "How can you decide that it is a message between intelligent life forms?", "labels": [], "entities": []}, {"text": "We need a 'language detector': or, to put it more accurately, something that separates language from non-language.", "labels": [], "entities": []}, {"text": "But what is special about the language signal that separates it from nonlanguage?", "labels": [], "entities": []}, {"text": "Is it, indeed, separable?", "labels": [], "entities": []}, {"text": "The problem goal is to separate language from non-language without dialogue, and learn something about the structure of language in the passing.", "labels": [], "entities": []}, {"text": "The language may not be human (animals, aliens, computers...), the perceptual space can be unknown, and we cannot assume human language structure but must begin somewhere.", "labels": [], "entities": []}, {"text": "We need to approach the language signal from a naive viewpoint, in effect, increasing our ignorance and assuming as little as possible.", "labels": [], "entities": [{"text": "ignorance", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9713157415390015}]}, {"text": "Given this standpoint, an informal description of 'language' might include that it: \u2022 has structure at several interrelated levels We assume that a language-like signal will be encoded symbolically, i.e. with some kind of character-stream.", "labels": [], "entities": []}, {"text": "Our language-detection algorithm for symbolic input uses a number of statistical clues such as entropy, \"chunking\" to find character bit-length and boundaries, and matching against a Zipfian type-token distribution for \"letters\" and \"words\".", "labels": [], "entities": []}, {"text": "2 Identifying Structure and the 'Character Set' The initial task, given an incoming bit-stream, is to identify if a language-like structure exists and if detected what are the unique patterns/symbols, which constitute its 'character set'.", "labels": [], "entities": []}, {"text": "A visualisation of the alternative possible byte-lengths is gleaned by plotting the entropy calculated fora range of possible byte-lengths (.", "labels": [], "entities": []}, {"text": "In 'real' decoding of unknown scripts it is accepted that identifying the correct set of discrete symbols is no mean feat.", "labels": [], "entities": []}, {"text": "To make life simple for ourselves we assume a digital signal with a fixed number of bits per character.", "labels": [], "entities": []}, {"text": "Very different techniques are required to deal with audio or analogue equivalent waveforms).", "labels": [], "entities": []}, {"text": "We have reason to believe that the following method can be modified to relax this constraint, but this needs to be tested further.", "labels": [], "entities": []}, {"text": "The task then reduces to trying to identify the number of bits per character.", "labels": [], "entities": []}, {"text": "Given the probability of a bit is Pi; the message entropy of a string of length N will be given by the first order measure: If the signal contains merely a set of random digits, the expected value of this function will rise monotonically as N increases.", "labels": [], "entities": []}, {"text": "However, if the string contains a set of symbols of fixed length representing a character set used for communication, it is likely to show some decrease in entropy when analysed in blocks of this length, because the signal is 'less random' when thus blocked.", "labels": [], "entities": []}, {"text": "Of course, we need to analyse blocks that begin and end at character boundaries.", "labels": [], "entities": []}, {"text": "We simply carryout the measurements in sliding windows along the data.", "labels": [], "entities": []}, {"text": "In, we see what happens when we applied this to samples of 8-bit ASCII text.", "labels": [], "entities": []}, {"text": "We notice a clear drop, as predicted, fora bit length of 8.", "labels": [], "entities": []}, {"text": "Modest progress though it maybe, it is not unreasonable to assume that the first piece of evidence for the presence of language-like structure, would be the identification of a low-entropy, character set within the signal.", "labels": [], "entities": []}, {"text": "The next task, still below the stages normally tackled by NLL researchers, is to chunk the incoming character-stream into words.", "labels": [], "entities": []}, {"text": "Looking at a range of (admittedly human language) text, if the text includes a space-like word-separator character, this will be the most frequent character.", "labels": [], "entities": []}, {"text": "So, a plausible hypothesis would be that the most frequent character is a word-separator1; then plot type-token frequency distributions for words, and for word-lengths.", "labels": [], "entities": []}, {"text": "If the distributions are Zipfian, and there are no significant 'outliers' (very large gaps between 'spaces' signifying very long words) then we have evidence corroborating our space hypothesis; this also corroborates our byte-length hypothesis, since the two are interdependent.", "labels": [], "entities": []}, {"text": "fort in a communication act.", "labels": [], "entities": []}, {"text": "Conversely, results obtained similar to the 'flatter' distributions above, when using the most frequent character, is likely to indicate the absence of word separators in the signal.", "labels": [], "entities": []}, {"text": "To ascertain whether the word-length frequency distribution holds for language in general, multiple samples from 20 different languages from Indo-European, Bantu, Semitic, Finno-Ugrian and Malayo-Polynesian groups were analysed (.", "labels": [], "entities": []}, {"text": "Using statistical measures of significance, it was found that most groups fell well within 5-only two individual languages were near exceeding these limits -of the proposed Human language word-length profile (E1-liott et al.,).", "labels": [], "entities": []}, {"text": "Zipf's law is a strong indication of language-like behaviour.", "labels": [], "entities": []}, {"text": "It can be used to segment the signal provided a 'space' character exists.", "labels": [], "entities": []}, {"text": "However, we should not assume Zipf to bean infallible language detector.", "labels": [], "entities": []}, {"text": "Natural phenomena such as molecular distribution in yeast DNA possess characteristics of power laws.", "labels": [], "entities": []}, {"text": "Nevertheless, it is worth noting, that such non-language possessors of power law characteristics generally display distribution ranges far greater than language with long repeats far from each other (; characteristics detectable at this level or at least higher order entropic evaluation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}