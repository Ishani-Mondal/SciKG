{"title": [{"text": "Nonlocal Language Modeling based on Context Co-occurrence Vectors", "labels": [], "entities": [{"text": "Nonlocal Language Modeling", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6041400134563446}]}], "abstractContent": [{"text": "This paper presents a novel nonlocal lmlguage model which utilizes contextual information.", "labels": [], "entities": []}, {"text": "A reduced vector space model calculated from co-occurrences of word pairs provides word co-occurrence vectors.", "labels": [], "entities": []}, {"text": "The sum of word co-occurrence vectors represents tile context of a document, and the cosine similarity between the context vector and the word co-occurrence vectors represents the ]ong-distmlce lexical dependencies.", "labels": [], "entities": []}, {"text": "Experiments on the Mainichi Newspaper corpus show significant improvement in perplexity (5.070 overall and 27.2% on target vocabulary)", "labels": [], "entities": [{"text": "Mainichi Newspaper corpus", "start_pos": 19, "end_pos": 44, "type": "DATASET", "confidence": 0.9885297815004984}, {"text": "perplexity", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9819954633712769}]}], "introductionContent": [{"text": "Human pattern recognition rarely handles isolated or independent objects.", "labels": [], "entities": [{"text": "Human pattern recognition", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.622621476650238}]}, {"text": "We recognize objects in various spatiotemporal circumstances such as an object in a scene, a word in an uttermlce.", "labels": [], "entities": []}, {"text": "These circumstances work as conditions, eliminating ambiguities and enabling robust recognition.", "labels": [], "entities": []}, {"text": "The most challenging topics in machine pattern recognition are in what representation and to what extent those circumstances are utilized.", "labels": [], "entities": [{"text": "machine pattern recognition", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.7986846168835958}]}, {"text": "In laalguage processing, a context--that is; a portion of the utterance or the text before the object--is ml important circumstmlce.", "labels": [], "entities": []}, {"text": "One way of representing a context is statistical language nmdels which provide a word sequence probability, P(w~), where w~ denotes the sequence wi...wj.", "labels": [], "entities": []}, {"text": "In other words, they provide the conditional probability of a word given with the previous word sequence, P( wilw~-l ), which shows the prediction of a word in a given context.", "labels": [], "entities": []}, {"text": "The most conmmn laalguage models used nowadays are N-granl models based on a (N-1)-th order Markov process: event predictions depend on at most (N-1) previous events.", "labels": [], "entities": []}, {"text": "Therefore, they offer the following approximation: P(w.ilw -1) wiJwi_N+l) A common value for N is 2 (bigram language model) or 3 (trigram language model); only a short local context of one or two words is considered.", "labels": [], "entities": []}, {"text": "Even such a local context is effective in some cases.", "labels": [], "entities": []}, {"text": "For example, in Japanese, after the word kokumu 'state affairs', words such as daijin 'minister' mad shou 'department' likely follow; kaijin 'monster' and shou 'priZe' do not.", "labels": [], "entities": []}, {"text": "After dake de 'only at', you cml often find wa (topic-marker), but you hardly find ga (nominative-marker) or wo (accusativemarker).", "labels": [], "entities": []}, {"text": "These examples show behaviors of compound nouns and function word sequences are well handled by bigram mad trigraan models.", "labels": [], "entities": []}, {"text": "These models are exploited in several applications such as speech recognition, optical character recognition and nmrphological analysis.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8582640886306763}, {"text": "optical character recognition", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.7045856912930807}, {"text": "nmrphological analysis", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.7264377176761627}]}, {"text": "Local language models, however, cannot predict nmch in some cases.", "labels": [], "entities": []}, {"text": "For instance, the word probability distribution after de wa 'at (topic-marker)' is very flat.", "labels": [], "entities": []}, {"text": "However, even if the probability distribution is flat in local language models, the probability of daijin 'minister' and kaijin 'monster' must be very different in documents concenfing politics.", "labels": [], "entities": []}, {"text": "Bigram and trigram models are obviously powerless to such kind of nonlocal, long-distmlce lexical dependencies.", "labels": [], "entities": []}, {"text": "This paper presents a nonlocal language model.", "labels": [], "entities": []}, {"text": "The important information concerning long-distance lexical dependencies is the word co-occurrence information.", "labels": [], "entities": []}, {"text": "For example, words such as politics, govermnent, administration, department, tend to co-occur with daijin 'minister'.", "labels": [], "entities": []}, {"text": "It is easy to measure cooccurrences of word pairs from a training corpus, but utilizing them as a representation of context is the problem.", "labels": [], "entities": []}, {"text": "We present a vector 1 2 Figure 1: V~rord-document co-occurrence matrix.", "labels": [], "entities": []}, {"text": "representation of word co-occurrence information; and show that the context can be represented as a sum of word co-occurrence vectors in a docmnent and it is incorporated in a nonlocal language model.", "labels": [], "entities": []}, {"text": "The row-vectors of a word-document cooccurrence matrix represent the co-occurrence information of words.", "labels": [], "entities": []}, {"text": "If two words tend to appear in the same documents, that is: tend to co-occur, their row-vectors are similar, that is, they point in sinfilar directions.", "labels": [], "entities": []}, {"text": "The more document is considered, the more reliable and realistic the co-occurrence information will be.", "labels": [], "entities": []}, {"text": "Then, the row size of a worddocument co-occurrence matrix may become very large.", "labels": [], "entities": []}, {"text": "Since enormous amounts of online text are available these days, row size can become more than a million documents.", "labels": [], "entities": []}, {"text": "Then, it is not practical to use a word-docmnent cooccurrence matrix as it is.", "labels": [], "entities": []}, {"text": "It is necessary to reduce row size and to simulate the tendency in the original matrix by a reduced matrix.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Perplexity results for the stmldard trigrazn model and the context language nmdel.", "labels": [], "entities": []}]}