{"title": [{"text": "Using Perfect Sampling in Parameter Estimation of a Whole Sentence Maximum Entropy Language Model*", "labels": [], "entities": []}], "abstractContent": [{"text": "The Maximum Entropy principle (ME) is an appropriate framework for combining information of a diverse nature from several sources into the same language model.", "labels": [], "entities": []}, {"text": "In order to incorporate long-distance information into the ME framework in a language model, a Whole Sentence Maximum Entropy Language Model (WSME) could be used.", "labels": [], "entities": []}, {"text": "Until now MonteCarlo Markov Chains (MCMC) sampling techniques has been used to estimate the paramenters of the WSME model.", "labels": [], "entities": [{"text": "WSME", "start_pos": 111, "end_pos": 115, "type": "TASK", "confidence": 0.7466835379600525}]}, {"text": "In this paper, we propose the application of another sampling technique: the Perfect Sampling (PS).", "labels": [], "entities": []}, {"text": "The experiment has shown a reduction of 30% in the perplexity of the WSME model over the trigram model and a reduction of 2% over the WSME model trained with MCMC.", "labels": [], "entities": [{"text": "WSME model", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.8382165729999542}, {"text": "MCMC", "start_pos": 158, "end_pos": 162, "type": "DATASET", "confidence": 0.9405567049980164}]}], "introductionContent": [{"text": "The language modeling problem maybe defined as the problem of calculating the probability of a string, p(w) = p(wl,..., Wn).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7604988515377045}]}, {"text": "The probability p(w) is usually calculated via conditional probabilities.", "labels": [], "entities": []}, {"text": "The n-gram model is one of the most widely used language models.", "labels": [], "entities": []}, {"text": "The power of the n-gram model resides in its simple formulation and the ease of training.", "labels": [], "entities": []}, {"text": "On the other hand, ngrams only take into account local information, and important long-distance information contained in the string wl ...", "labels": [], "entities": []}, {"text": "wn cannot be modeled by it.", "labels": [], "entities": []}, {"text": "In an attempt to supplement the local information with long-distance information, hybrid models have been proposed such us (Belle-* This work has been partially supported by the Spanish CYCIT under contract (TIC98/0423-C06).", "labels": [], "entities": []}, {"text": "t Granted by Universidad del Cauca, Popay~n garda, 1998;).", "labels": [], "entities": []}, {"text": "The Maximum Entropy principle is an appropriate framework for combining information of a diverse nature from several sources into the same model: the Maximum Entropy model (ME).", "labels": [], "entities": []}, {"text": "The information is incorporated as features which are submitted to constraints.", "labels": [], "entities": []}, {"text": "The conditional form of the ME model is: where Ai are the parameters to be learned (one for each feature), the fi are usually characteristic functions which are associated to the features and Z(x) = ~y exp{~i~l Aifi(x,y)} is the normalization constant.", "labels": [], "entities": []}, {"text": "The main advantages of ME are its flexibility (local and global information can be included in the model) and its simplicity.", "labels": [], "entities": [{"text": "ME", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.8806218504905701}]}, {"text": "The drawbacks are that the paramenter's estimation is computationally expensive, specially the evaluation of the normalization constant Z(x) andthat the grammatical information contained in the sentence is poorly encoded in the conditional framework.", "labels": [], "entities": [{"text": "paramenter's estimation", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.7520855863889059}]}, {"text": "This is due to the assumption of independence in the conditional events: in the events in the state space, only apart of the information contained in the sentence influences de calculation of the probability).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we have made preliminary experiments using PS in the estimation of the expected value (4) during the learning of the parameters of a WSME model.", "labels": [], "entities": [{"text": "PS", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.9391182661056519}, {"text": "WSME", "start_pos": 147, "end_pos": 151, "type": "TASK", "confidence": 0.677436888217926}]}, {"text": "We have implemented the Cai algorithm) to obtain perfect samples.", "labels": [], "entities": []}, {"text": "The Cai algorithm has the advantage that it doesn't need the definition of the partial order.", "labels": [], "entities": []}, {"text": "The experiments were carried out using a pseudonatural corpus: \"the traveler task \"1.", "labels": [], "entities": []}, {"text": "The traveler task consists in dialogs between travelers and hotel clerks.", "labels": [], "entities": []}, {"text": "The size of the vocabulary is 693 words.", "labels": [], "entities": []}, {"text": "The training set has 490,000 sentences and 4,748,690 words.", "labels": [], "entities": []}, {"text": "The test set has 10,000 sentences and 97,153 words.", "labels": [], "entities": []}, {"text": "Three kinds of features were used in the WSME model: n-grams (1-grams, 2-grams, 3-grams), distance 2 n-grams (d2-2-grams, d2-3-grams) and triggers.", "labels": [], "entities": [{"text": "WSME", "start_pos": 41, "end_pos": 45, "type": "TASK", "confidence": 0.5650953650474548}]}, {"text": "The proposal prior distribution used was a trigram model.", "labels": [], "entities": []}, {"text": "We trained WSME models with different sets of features using the two sampling techniques: MCMC and PS.", "labels": [], "entities": []}, {"text": "We measured the perplexity (PP) of each of the models and obtained the percentage of improvement in the PP with respect to a trigram base-line model (see).", "labels": [], "entities": [{"text": "perplexity (PP)", "start_pos": 16, "end_pos": 31, "type": "METRIC", "confidence": 0.7693313807249069}]}, {"text": "The first model used MCMC techniques (specifically the Independence Metropolis-Hastings algorithm (IMH) 2) and features of n-grams and distance 2 n-grams.", "labels": [], "entities": []}, {"text": "The second model used a 1EuTrans ESPRIT-LTR Project 20268 2IMH has been reported recently as the most useful MCMC algorithm used in the WSME training process.", "labels": [], "entities": [{"text": "1EuTrans ESPRIT-LTR Project 20268", "start_pos": 24, "end_pos": 57, "type": "DATASET", "confidence": 0.8582453280687332}, {"text": "2IMH", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.5527255535125732}, {"text": "WSME training process", "start_pos": 136, "end_pos": 157, "type": "TASK", "confidence": 0.9047436515490214}]}, {"text": "Test set perplexity of the WSME model over the traveler task corpus: IMH with features of n-grams and d-n-grams (IMH), PS with n-grams and d-n-grams (PS) IMH with triggers (IMH-T), PS with triggers (PS-T).", "labels": [], "entities": [{"text": "WSME", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.726182758808136}]}, {"text": "The base-line model is a trigram model PS algorithm and features of n-grams and distance 2 n-grams.", "labels": [], "entities": []}, {"text": "The third model used the IMH algorithm and features of triggers.", "labels": [], "entities": []}, {"text": "The fourth used PS and features of triggers.", "labels": [], "entities": [{"text": "PS", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9793072938919067}]}, {"text": "Finally, in order to compare with the classical methods, we included the trigram base-line model.", "labels": [], "entities": []}], "tableCaptions": []}