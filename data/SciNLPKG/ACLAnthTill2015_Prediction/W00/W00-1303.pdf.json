{"title": [{"text": "Japanese Dependency Structure Analysis Based on Support Vector Machines", "labels": [], "entities": [{"text": "Dependency Structure Analysis", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.6361872653166453}]}], "abstractContent": [{"text": "This paper presents a method of Japanese dependency structure analysis based on Support Vector Machines (SVMs).", "labels": [], "entities": [{"text": "Japanese dependency structure analysis", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.7570536956191063}]}, {"text": "Conventional parsing techniques based on Machine Learning framework, such as Decision Trees and Maximum Entropy Models, have difficulty in selecting useful features as well as finding appropriate combination of selected features.", "labels": [], "entities": []}, {"text": "On the other hand, it is well-known that SVMs achieve high generalization performance even with input data of very high dimensional feature space.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 41, "end_pos": 45, "type": "TASK", "confidence": 0.9565563797950745}]}, {"text": "Furthermore, by introducing the Kernel principle, SVMs can carryout the training in high-dimensional \u2022 spaces with a smaller computational cost independent of their dimensionality.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 50, "end_pos": 54, "type": "TASK", "confidence": 0.9365347027778625}]}, {"text": "We apply SVMs to Japanese dependency structure identification problem.", "labels": [], "entities": [{"text": "Japanese dependency structure identification", "start_pos": 17, "end_pos": 61, "type": "TASK", "confidence": 0.5866608023643494}]}, {"text": "Experimental results on Kyoto University corpus show that our system achieves the accuracy of 89.09% even with small training data (7958 sentences).", "labels": [], "entities": [{"text": "Kyoto University corpus", "start_pos": 24, "end_pos": 47, "type": "DATASET", "confidence": 0.9818436900774637}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9995697140693665}]}], "introductionContent": [{"text": "Dependency structure analysis has been recognized as a basic technique in Japanese sentence analysis, and a number of studies have been proposed for years.", "labels": [], "entities": [{"text": "Dependency structure analysis", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8861493666966757}, {"text": "Japanese sentence analysis", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.6577844023704529}]}, {"text": "Japanese dependency structure is usually defined in terms of the relationship between phrasal units called 'bunsetsu' segments (hereafter \"chunks~).", "labels": [], "entities": []}, {"text": "Generally, dependency structure analysis consists of two steps.", "labels": [], "entities": [{"text": "dependency structure analysis", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.8881371219952902}]}, {"text": "In the first step, dependency matrix is constructed, in which each element corresponds to a pair of chunks and represents the probability of a dependency relation between them.", "labels": [], "entities": []}, {"text": "The second step is to find the optimal combination of dependencies to form the entire sentence.", "labels": [], "entities": []}, {"text": "In previous approaches, these probabilites of dependencies axe given by manually constructed rules.", "labels": [], "entities": []}, {"text": "However, rule-based approaches have problems in coverage and consistency, since there area number of features that affect the accuracy of the final results, and these features usually relate to one another.", "labels": [], "entities": [{"text": "coverage", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9564316868782043}, {"text": "consistency", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9901754260063171}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9969338178634644}]}, {"text": "On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.6242444068193436}]}, {"text": "These approaches have overcome the systems based on the rule-based approaches.", "labels": [], "entities": []}, {"text": "Decision Trees() and Maximum Entropy models) have been applied to dependency or syntactic structure analysis.", "labels": [], "entities": [{"text": "syntactic structure analysis", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.6719385186831156}]}, {"text": "However, these models require an appropriate feature selection in order to achieve a high performance.", "labels": [], "entities": []}, {"text": "In addition, acquisition of an efficient combination of features is difficult in these models.", "labels": [], "entities": []}, {"text": "In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) and are proposed.", "labels": [], "entities": []}, {"text": "These techniques take a strategy that maximize the margin between critical examples and the separating hyperplane.", "labels": [], "entities": []}, {"text": "In particular, compared with other conventional statistical learning algorithms, SVMs achieve high generalization even with training data of a very high dimension.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 81, "end_pos": 85, "type": "TASK", "confidence": 0.9560802578926086}]}, {"text": "Furthermore, by optimizing the Kernel function, SVMs can handle non-linear feature spaces, and carryout the training with considering combinations of more than one feature.", "labels": [], "entities": []}, {"text": "Thanks to such predominant nature, SVMs deliver state-of-the-art performance in realworld applications such as recognition of hand-written letters, or of three dimensional images.", "labels": [], "entities": [{"text": "recognition of hand-written letters", "start_pos": 111, "end_pos": 146, "type": "TASK", "confidence": 0.8561079353094101}]}, {"text": "In the field of natural language processing, SVMs are also applied to text categorization, and are reported to have achieved high accuracy without falling into over-fitting even with a large number of words taken as the features.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6702667276064554}, {"text": "text categorization", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7340376973152161}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.997761607170105}]}, {"text": "In this paper, we propose an application of SVMs to Japanese dependency structure analysis.", "labels": [], "entities": [{"text": "Japanese dependency structure analysis", "start_pos": 52, "end_pos": 90, "type": "TASK", "confidence": 0.6325199827551842}]}, {"text": "We use the features that have been studied in conventional statistical dependency analysis with a little modification on them.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use Kyoto University text corpus (Version 2.0) consisting of articles of Mainichi newspaper annotated with dependency structure().", "labels": [], "entities": [{"text": "Kyoto University text corpus", "start_pos": 7, "end_pos": 35, "type": "DATASET", "confidence": 0.9702707827091217}, {"text": "Mainichi newspaper", "start_pos": 76, "end_pos": 94, "type": "DATASET", "confidence": 0.9773490130901337}]}, {"text": "7,958 sentences from the articles on January 1st to January 7th are used for the training data, and 1,246 sentences from the articles on January 9th are used for the test data.", "labels": [], "entities": []}, {"text": "For the kernel function, we used the polynomial function (9).", "labels": [], "entities": []}, {"text": "We set the soft margin parameter C to be 1.", "labels": [], "entities": [{"text": "soft margin parameter C", "start_pos": 11, "end_pos": 34, "type": "METRIC", "confidence": 0.7936346232891083}]}, {"text": "The feature set used in the experiments are shown in.", "labels": [], "entities": []}, {"text": "The static features are basically taken from Uchimoto's list() with little modification.", "labels": [], "entities": []}, {"text": "In, 'Head' means the rightmost content word in a chunk whose part-of-speech is not a functional category.", "labels": [], "entities": []}, {"text": "'Type' means the rightmost functional word or the inflectional form of the rightmost predicate if there is no functional word in the chunk.", "labels": [], "entities": []}, {"text": "The static features include the information on existence of brackets, question marks and punctuation marks etc.", "labels": [], "entities": []}, {"text": "Besides, there are features that show the relative relation of two chunks, such as distance, and existence of brackets, quotation marks and punctuation marks between them.", "labels": [], "entities": [{"text": "distance", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9793252348899841}]}, {"text": "For dynamic features, we selected functional words or inflection forms of the rightmost predicates in the chunks that appear between two chunks and depend on the modiflee.", "labels": [], "entities": []}, {"text": "shows the result of parsing accuracy under the condition k = 5 (beam width), and d = 3 (dimension of the polynomial functions used for the kernel function).", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9829133152961731}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9850730299949646}]}, {"text": "This table shows two types of dependency accuracy, A and B.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.939265251159668}]}, {"text": "The training data size is measured by the number of sentences.", "labels": [], "entities": []}, {"text": "The accuracy A means the accuracy of the entire dependency relations.", "labels": [], "entities": [{"text": "accuracy A", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9708578288555145}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.998993456363678}]}, {"text": "Since Japanese is a headfinal language, the second chunk from the end of a sentence always modifies the last chunk.", "labels": [], "entities": []}, {"text": "The accuracy B is calculated by excluding this dependency relation.", "labels": [], "entities": [{"text": "accuracy B", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9716665744781494}]}, {"text": "Hereafter, we use the accuracy A, if it is not explicitly specified, since this measure is usually used in other literature.", "labels": [], "entities": [{"text": "accuracy A", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.8301333785057068}]}, {"text": "dynamic feature set is better than the results without them.", "labels": [], "entities": []}, {"text": "The results with dynamic features constantly outperform that with static features only.", "labels": [], "entities": []}, {"text": "In most of cases, the improvements is significant.", "labels": [], "entities": []}, {"text": "In the experiments, we restrict the features only from the chunks that appear between two chunks being in consideration, however, dynamic features could be also taken from the chunks that appear not between the two chunks.", "labels": [], "entities": []}, {"text": "For example, we could also take into consideration the chunk that is modified by the right chunk, or the chunks: Dimension vs. Accuracy (3032 sentences, k = 5) that modify the left chunk.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9954484105110168}]}, {"text": "We leave experiment in such a setting for the future work.", "labels": [], "entities": []}, {"text": "shows the relationship between the size of the training data and the parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9733772277832031}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.945638120174408}]}, {"text": "This figure shows the accuracy of with and without the dynamic features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9996509552001953}]}], "tableCaptions": [{"text": " Table 2: Result (d = 3, k = 5)", "labels": [], "entities": [{"text": "Result", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9945342540740967}]}, {"text": " Table 3: Result without dynamic features  (d = 3, k = 5)", "labels": [], "entities": []}, {"text": " Table 4: Dimension vs. Accuracy (3032 sen- tences, k = 5)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9939736723899841}]}, {"text": " Table 5: Beam width vs. Accuracy (6756 sen- tences, d = 3)", "labels": [], "entities": [{"text": "Beam width", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.821875661611557}, {"text": "Accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.998336911201477}]}]}