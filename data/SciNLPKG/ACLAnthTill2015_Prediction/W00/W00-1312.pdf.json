{"title": [{"text": "Cross-lingual Information Retrieval using Hidden Markov Models", "labels": [], "entities": [{"text": "Cross-lingual Information Retrieval", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6745736598968506}]}], "abstractContent": [{"text": "This paper presents empirical results in cross-lingual information retrieval using English queries to access Chinese documents (TREC-5 and TREC-6) and Spanish documents (TREC-4).", "labels": [], "entities": [{"text": "cross-lingual information retrieval", "start_pos": 41, "end_pos": 76, "type": "TASK", "confidence": 0.7055944403012594}]}, {"text": "Since our interest is in languages where resources maybe minimal, we use an integrated probabilistic model that requires only a bilingual dictionary as a resource.", "labels": [], "entities": []}, {"text": "We explore how a combined probability model of term translation and retrieval can reduce the effect of translation ambiguity.", "labels": [], "entities": [{"text": "term translation", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7489785552024841}]}, {"text": "In addition, we estimate an upper bound on performance, if translation ambiguity were a solved problem.", "labels": [], "entities": [{"text": "translation ambiguity", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.9146609902381897}]}, {"text": "We also measure performance as a function of bilingual dictionary size.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cross-language information retrieval (CLIR) can serve both those users with a smattering of knowledge of other languages and also those fluent in them.", "labels": [], "entities": [{"text": "Cross-language information retrieval (CLIR)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8320549130439758}]}, {"text": "For those with limited knowledge of the other language(s), CLIR offers a wide pool of documents, even though the user does not have the skill to prepare a high quality query in the other language(s).", "labels": [], "entities": []}, {"text": "Once documents are retrieved, machine translation or human translation, if desired, can make the documents usable.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7031449526548386}, {"text": "human translation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7550017237663269}]}, {"text": "For the user who is fluent in two or more languages, even though he/she maybe able to formulate good queries in each of the source languages, CLIR relieves the user from having to do so.", "labels": [], "entities": []}, {"text": "Most CLIR studies have been based on a variant of tf-idf; our experiments instead use a hidden Markov model (HMM) to estimate the probability that a document is relevant given the query.", "labels": [], "entities": []}, {"text": "We integrated two simple estimates of term translation probability into the monolingual HMM model, giving an estimate of the probability that a document is relevant given a query in another language.", "labels": [], "entities": [{"text": "term translation", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.6285193264484406}]}, {"text": "In this paper we address the following questions: \u2022 How can a combined probability model of term translation and retrieval minimize the effect of translation ambiguity?", "labels": [], "entities": [{"text": "term translation", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7187771052122116}]}, {"text": "(Sections 3, 5, 6, 7, and 10) \u2022 What is the upper bound performance using bilingual dictionary lookup for term translation?", "labels": [], "entities": [{"text": "term translation", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.7875823080539703}]}, {"text": "(Section 8) \u2022 How much does performance degrade due to omissions from the bilingual dictionary and how does performance vary with size of such a dictionary?", "labels": [], "entities": []}, {"text": "(Sections 8-9) All experiments were performed using a common baseline, an HMM-based (monolingual) indexing and retrieval engine.", "labels": [], "entities": []}, {"text": "In order to design controlled experiments for the questions above, the IR system was run without sophisticated query expansion techniques.", "labels": [], "entities": [{"text": "IR", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9490669369697571}]}, {"text": "Our experiments are based on the Chinese materials of TREC-5 and TREC-6 and the Spanish materials of TREC-4.", "labels": [], "entities": [{"text": "TREC-6", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.8214460015296936}, {"text": "TREC-4", "start_pos": 101, "end_pos": 107, "type": "DATASET", "confidence": 0.8681908249855042}]}, {"text": "We use x to represent the language (e.g. English) for which retrieval is carried out.", "labels": [], "entities": []}, {"text": "According to that model of monolingual retrieval, it can be shown that", "labels": [], "entities": []}], "datasetContent": [{"text": "For retrieval using English queries to search Chinese documents, we used the TREC5 and TREC6 Chinese data which consists of 164,789 documents from the Xinhua News Agency and People's Daily, averaging 450 Chinese characters/document.", "labels": [], "entities": [{"text": "TREC5", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.5315486192703247}, {"text": "TREC6 Chinese data", "start_pos": 87, "end_pos": 105, "type": "DATASET", "confidence": 0.688698947429657}, {"text": "Xinhua News Agency", "start_pos": 151, "end_pos": 169, "type": "DATASET", "confidence": 0.8806810776392618}, {"text": "People's Daily", "start_pos": 174, "end_pos": 188, "type": "DATASET", "confidence": 0.8505891561508179}]}, {"text": "Each of the TREC topics has three Chinese fields: title, description and narrative, plus manually translated, English versions of each.", "labels": [], "entities": []}, {"text": "We corrected some of the English queries that contained errors, such as \"Dali Lama\" instead of the correct \"Dalai Lama\" and \"Medina\" instead of \"Medellin.\"", "labels": [], "entities": []}, {"text": "Stop words and stop phrases were removed.", "labels": [], "entities": []}, {"text": "We created three versions of Chinese queries and three versions of English queries: short (title only), medium (title and description), and long (all three fields).", "labels": [], "entities": []}, {"text": "For retrieval using English queries to search Spanish documents, we used the TREC4 Spanish data, which has 57,868 documents.", "labels": [], "entities": [{"text": "TREC4 Spanish data", "start_pos": 77, "end_pos": 95, "type": "DATASET", "confidence": 0.8992110888163248}]}, {"text": "It has 25 queries in Spanish with manual translations to English.", "labels": [], "entities": []}, {"text": "We will denote the Chinese data sets as Trec5C and Trec6C and the Spanish data set as Trec4S.", "labels": [], "entities": [{"text": "Chinese data sets", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.8741489847501119}, {"text": "Spanish data set", "start_pos": 66, "end_pos": 82, "type": "DATASET", "confidence": 0.845289925734202}]}, {"text": "We used a Chinese-English lexicon from the Linguistic Data Consortium (LDC).", "labels": [], "entities": [{"text": "Linguistic Data Consortium (LDC)", "start_pos": 43, "end_pos": 75, "type": "DATASET", "confidence": 0.8591048866510391}]}, {"text": "We preprocessed the dictionary as follows: 1.", "labels": [], "entities": []}, {"text": "Stem Chinese words via a simple algorithm to remove common suffixes and prefixes.", "labels": [], "entities": []}, {"text": "2. Use the Porter stemmer on English words.", "labels": [], "entities": []}, {"text": "each of them will be assigned equal probability, i.e., P(eilc)=l/n.", "labels": [], "entities": []}, {"text": "Section 10 supplements this with a corpus-based distribution.)", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparing mono-lingual and cross- lingual retrieval performance. The scores on", "labels": [], "entities": []}, {"text": " Table 3. We can see that the HMM  performs best for every query set. Simple  substitution performs worst. The synonym  approach is significantly better than substitution,  but is consistently worse than the HMM", "labels": [], "entities": []}, {"text": " Table 3: Comparing different methods of  query translation. All numbers are average", "labels": [], "entities": [{"text": "query translation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7934252917766571}]}, {"text": " Table 4: The effect of disambiguation on  retrieval performance. The scores reported", "labels": [], "entities": []}, {"text": " Table 5: The impact of missing the right  translations on retrieval performance. All", "labels": [], "entities": []}, {"text": " Table 6: Performance with different values  of 13. All scores are average precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.7962743043899536}]}]}