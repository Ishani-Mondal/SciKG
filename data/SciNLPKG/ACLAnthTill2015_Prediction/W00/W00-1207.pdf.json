{"title": [{"text": "Statistically-Enhanced New Word Identification in a Rule-Based Chinese System", "labels": [], "entities": [{"text": "Statistically-Enhanced New Word Identification", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.6196824684739113}]}], "abstractContent": [{"text": "This paper presents a mechanism of new word identification in Chinese text where probabilities are used to filter candidate character strings and to assign POS to the selected strings in a ruled-based system.", "labels": [], "entities": [{"text": "word identification", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7214456349611282}]}, {"text": "This mechanism avoids the sparse data problem of pure statistical approaches and the over-generation problem of rule-based approaches.", "labels": [], "entities": []}, {"text": "It improves parser coverage and provides a tool for the lexical acquisition of new words.", "labels": [], "entities": [{"text": "parser coverage", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.9213351607322693}, {"text": "lexical acquisition of new words", "start_pos": 56, "end_pos": 88, "type": "TASK", "confidence": 0.7676671624183655}]}], "introductionContent": [{"text": "In this paper, new words refer to newly coined words, occasional words and other rarely used words that are neither found in the dictionary of a natural language processing system nor recognized by the derivational rules or proper name identification rules of the system.", "labels": [], "entities": [{"text": "proper name identification", "start_pos": 224, "end_pos": 250, "type": "TASK", "confidence": 0.6286045511563619}]}, {"text": "Typical examples of such words are shown in the following sentences, with the new words underlined in bold.", "labels": [], "entities": []}, {"text": "~--~E~ff~,,~R~\" *[]~.2/..~W~m~@~o ~~.~~o The automatic identification of such words by a machine is a trivial task in languages where words are separated by spaces in written texts.", "labels": [], "entities": []}, {"text": "In languages like Chinese, where no word boundary exists in written texts, this is by no means an easy job.", "labels": [], "entities": []}, {"text": "In many cases the machine will not even realize that there is an unfound word in the sentence since most single Chinese characters can be words by themselves.", "labels": [], "entities": []}, {"text": "Purely statistical methods of word segmentation (e.g.,, Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7387015223503113}]}, {"text": "There are also hybrid approaches such as (Nie dt al 1995) where statistical approaches and heuristic rules are combined to identify new words.", "labels": [], "entities": []}, {"text": "They generally perform better than purely statistical segmenters, but the new words they are able to recognize are usually proper names and other relatively frequent words.", "labels": [], "entities": []}, {"text": "They require a reasonably big training corpus and the performance is often domain-specific depending on the training corpus used.", "labels": [], "entities": []}, {"text": "Many word segmenters ignore low-frequency new words and treat their component characters as independent words, since they are often of little significance in applications where the structure of sentences is not taken into consideration.", "labels": [], "entities": [{"text": "word segmenters ignore low-frequency new words", "start_pos": 5, "end_pos": 51, "type": "TASK", "confidence": 0.7974205613136292}]}, {"text": "For in-depth natural language understanding where full parsing is required, however, the identification of those words is critical, because a single unidentified word can cause a whole sentence to fail.", "labels": [], "entities": []}, {"text": "The new word identification mechanism to be presented here is used in a wide coverage Chinese parser that does full sentence analysis.", "labels": [], "entities": [{"text": "word identification", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7692245841026306}, {"text": "full sentence analysis", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.5980330506960551}]}, {"text": "It assumes the word segmentation process described in.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.6698940545320511}]}, {"text": "In this model, word segmentation, including unfound word identification, is not a stand-alone process, but an integral part of sentence analysis.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7539478838443756}, {"text": "word identification", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7336860597133636}, {"text": "sentence analysis", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.7106527537107468}]}, {"text": "The segmentation component provides a word lattice of the sentence that contains all the possible words, and the final disambiguation is achieved in the parsing process.", "labels": [], "entities": []}, {"text": "In what follows, we will discuss two hypotheses and their implementation.", "labels": [], "entities": []}, {"text": "The first one concerns the selection of candidate strings and the second one concerns the assignment of parts of speech (POS) to those strings.", "labels": [], "entities": [{"text": "assignment of parts of speech (POS)", "start_pos": 90, "end_pos": 125, "type": "TASK", "confidence": 0.8292213827371597}]}], "datasetContent": [], "tableCaptions": []}