{"title": [{"text": "Using Co-occurrence Statistics as an Information Source for Partial Parsing of Chinese", "labels": [], "entities": [{"text": "Partial Parsing of Chinese", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.7314837500452995}]}], "abstractContent": [{"text": "Our partial parser for Chinese uses a learned classifier to guide a bottom-up parsing process.", "labels": [], "entities": []}, {"text": "We describe improvements in performance obtained by expanding the information available to the classifier, from POS sequences only, to include measures of word association derived from co-occurrence statistics.", "labels": [], "entities": []}, {"text": "We compare performance using different measures of association, and find that Yule's coefficient of colligation Y gives somewhat better results over other measures.", "labels": [], "entities": [{"text": "Yule's coefficient of colligation Y", "start_pos": 78, "end_pos": 113, "type": "METRIC", "confidence": 0.9043486913045248}]}], "introductionContent": [{"text": "In learning-based approaches to syntactic parsing, the earliest models developed generally ignored the individual identities of words, making decisions based only on their part-of-speech classes.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.824904054403305}]}, {"text": "On the othor hand, many later models see each word as a monolithic entity, with parameters estimated separately for each word type.", "labels": [], "entities": []}, {"text": "In between have been models which auempt to generalize by considering similarity between words, where knowledge about similarity is deduced fi'om hand-written sources (e.g. thesauri), or induced from text.", "labels": [], "entities": []}, {"text": "For example, The SPATTER parser) makes use of the output of a clustering algorithm based on co-occurrence information.", "labels": [], "entities": [{"text": "SPATTER parser", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.5195793062448502}]}, {"text": "Because this co-occurrence information can be derived from inexpensive data with a minimum of pre-processing, it can be very inclusive and informative about even relatively rare words, thus increasing the generalization capability of the parser trained on a much smaller fully annotated corpus.", "labels": [], "entities": []}, {"text": "The cunent work is in this spirit, making complementary use of a relatively small treebank for syntactic information and a relatively large collection of flat text for co-occurrence information.", "labels": [], "entities": []}, {"text": "However, we do not use any kind of clustering, instead using the co-occurrence data directly.", "labels": [], "entities": []}, {"text": "Our parser is a bottom-up parser whose actions are guided by a machine-learning-based decision-making module (we use the SNoW learner developed at the University of Illinois, Urbana..Champaign) for its strength with potentially very large feature sets and for its ease of use).", "labels": [], "entities": []}, {"text": "The learner is able to directly use statistics derived from the co-occu~euce data to guide its decisions.", "labels": [], "entities": []}, {"text": "We collect a variety of statistical measures of association based on bigram co-occurrence data (specifically, mutual information, t-score, X 2, likelihood ratio and Yule's coefficient of colligation Y), and make the statistics available to the decision-making module.", "labels": [], "entities": [{"text": "likelihood ratio", "start_pos": 144, "end_pos": 160, "type": "METRIC", "confidence": 0.9637625217437744}, {"text": "Yule's coefficient of colligation Y", "start_pos": 165, "end_pos": 200, "type": "METRIC", "confidence": 0.9098173677921295}]}, {"text": "We use labelled constituent precision and recall to compare performance of different versions of our parser on unseen test data.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.814359724521637}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9992308616638184}]}, {"text": "We observe a marked improvement in some of the versions using the co-occurrence data, with strongest performance observed in the versions using Yule's coefficient of colligation Y and mutual information, and more modest improvements in those using the other measures.", "labels": [], "entities": [{"text": "Yule's coefficient of colligation Y", "start_pos": 144, "end_pos": 179, "type": "METRIC", "confidence": 0.7842818299929301}]}], "datasetContent": [{"text": "We trained a series of SNoW networks using features sets extended with each of thefive measures, and tested five versions of our parser, One using each of the resulting networks.", "labels": [], "entities": []}, {"text": "This was done on a held-out test set comprising approximately ten percent of our treebank.", "labels": [], "entities": []}, {"text": "The resulting measurements for labeled constituent precision and recall are shown in, arranged according to the geometric mean of the two measurements.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9618008732795715}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9972788691520691}]}, {"text": "It is clear from the table that co-occurrence information can be made useful, and that the measure used to represent this information has a large influence on its usefulness.", "labels": [], "entities": []}, {"text": "There is also a large disparity between the in~rovement in precision, 1.7%, and the improvement in recall, 4.1%.", "labels": [], "entities": [{"text": "in~rovement in", "start_pos": 44, "end_pos": 58, "type": "METRIC", "confidence": 0.854978397488594}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.896136462688446}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9997847676277161}]}, {"text": "We conjecture that this is because the parser odg/nally tended to err in the direction of splitting words into separate chunks, the commoner case, while with the co-occurrence infommtion, it is able to pick out some cases where a strong association suggests that words be joined in the same chunk.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4. Accuracy Measurements of Parsing with Different Measures of Association", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9929682612419128}]}]}