{"title": [{"text": "Adapting a synonym database to specific domains", "labels": [], "entities": [{"text": "Adapting", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9433184862136841}]}], "abstractContent": [{"text": "This paper describes a method for adapting a general purpose synonym database, like WordNet, to a specific domain, where only a subset of the synonymy relations defined in the general database hold.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9494705200195312}]}, {"text": "The method adopts an eliminative approach, based on incrementally pruning the original database.", "labels": [], "entities": []}, {"text": "The method is based on a preliminary manual pruning phase and an algorithm for automatically pruning the database.", "labels": [], "entities": []}, {"text": "This method has been implemented and used for an Information Retrieval system in the aviation domain.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.8452870547771454}]}], "introductionContent": [{"text": "Synonyms can bean important resource for Information Retrieval (IR) applications, and attempts have been made at using them to expand query terms.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.8559847116470337}]}, {"text": "In expanding query terms, overgeneration is as much of a problem as incompleteness or lack of synonym resources.", "labels": [], "entities": []}, {"text": "Precision can dramatically drop because of false hits due to incorrect synonymy relations.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8907558917999268}]}, {"text": "This problem is particularly felt when IR is applied to documents in specific technical domains.", "labels": [], "entities": [{"text": "IR", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9921494722366333}]}, {"text": "In such cases, the synonymy relations that hold in the specific domain are only a restricted portion of the synonymy relations holding fora given language at large.", "labels": [], "entities": []}, {"text": "For instance, a set of synonyms like {cocaine, cocain, coke, snow, C} valid for English, would be detrimental in a specific domain like weather reports, where both snow and C (for Celsius) occur very frequently, but never as synonyms of each other.", "labels": [], "entities": []}, {"text": "We describe a method for creating a domain specific synonym database from a general purpose one.", "labels": [], "entities": []}, {"text": "We use WordNet) as our initial database, and we draw evidence from a domain specific corpus about what synonymy relations hold in the domain.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.932525098323822}]}, {"text": "Our task has obvious relations to word sense disambiguation) (), since both tasks are based on identifying senses of ambiguous words in a text.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.6530668437480927}]}, {"text": "However, the two tasks are quite distinct.", "labels": [], "entities": []}, {"text": "In word sense disambiguation, a set of candidate senses fora given word is checked against each occurrence of the relevant word in a text, and a single candidate sense is selected for each occurrence of the word.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.7314679225285848}]}, {"text": "In our synonym specialization task a set of candidate senses fora given word is checked against an entire corpus, and a subset of candidate senses is selected.", "labels": [], "entities": [{"text": "synonym specialization task", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.8371657729148865}]}, {"text": "Although the latter task could be reduced to the former (by disambiguating all occurrences of a word in a test and taking the union of the selected senses), alternative approaches could also be used.", "labels": [], "entities": []}, {"text": "Ina specific domain, where words can be expected to be monosemous to a large extent, synonym pruning can bean effective alternative (or a complement) to word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 153, "end_pos": 178, "type": "TASK", "confidence": 0.6681609551111857}]}], "datasetContent": [{"text": "All the synsets containing the top ranking terms, according to the hierarchy of criteria described above, are manuMly checked for pruning.", "labels": [], "entities": []}, {"text": "For each term, all the synsets containing the term are clustered together and presented to a human operator, who examines each (term, synset) pair and answers the question: does the term belong to the synset in the specific domain?", "labels": [], "entities": []}, {"text": "Evidence about the answer is drawn from relevant examples automatically extracted from the domain specific corpus.", "labels": [], "entities": []}, {"text": "E.g., following upon our example in the previous section, the operator would be presented with the word snow associated with each of the synsets in (7) and would have to provide a yes/no answer for each of them.", "labels": [], "entities": []}, {"text": "In the specific case, the answer would be likely to be 'no' for (7a) and 'yes' for (75) and (7c).", "labels": [], "entities": []}, {"text": "The evaluator is presented with all the synsets involving a relevant term (even those that did not rank high in terms of scorePoIyCQ) in order to apply a contrastive approach.", "labels": [], "entities": []}, {"text": "It might well be the case that the correct sense fora given term is one for which the term has no synonyms at all (e.g. 7b in the example), therefore all synsets fora given term need to be presented to the evaiuator in order to make an informed choice.", "labels": [], "entities": []}, {"text": "The evaluator provides a yes/no answer for all the (term, synset) he/she is presented with (with some exceptions, as explained in section 3.1).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Frequencies of sample synset terms.", "labels": [], "entities": []}, {"text": " Table 2: Ranking of synsets containing the word C  Frequencies  39.37", "labels": [], "entities": []}, {"text": " Table 3: WordNet optimization results.  DB  Synsets Word-senses  Full WN  99,642  174,008  Reduced WN  9,441  23,368", "labels": [], "entities": [{"text": "DB  Synsets Word-senses  Full WN  99,642  174,008", "start_pos": 41, "end_pos": 90, "type": "DATASET", "confidence": 0.9254845465932574}]}]}