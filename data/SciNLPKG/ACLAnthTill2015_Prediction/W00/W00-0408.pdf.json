{"title": [{"text": "A Comparison of Rankings Produced by Summarization Evaluation Measures mather@us, britannica, corn", "labels": [], "entities": []}], "abstractContent": [{"text": "evaluation measures produce a ranking of all possible extract summaries of a document., Recall-based evaluation measures, which depend on costly human-generated ground truth summaries, produce uncorrelated rankings when ground truth is varied.", "labels": [], "entities": []}, {"text": "This paper proposes using sentence-rank-based and content-based measures for evaluating extract summaries, and compares these with recall-based evaluation measures.", "labels": [], "entities": [{"text": "recall-based", "start_pos": 131, "end_pos": 143, "type": "METRIC", "confidence": 0.9867934584617615}]}, {"text": "Content-based measures increase the correlation of rankings induced by synonymous ground truths, and exhibit other desirable properties.", "labels": [], "entities": []}], "introductionContent": [{"text": "The bulk of active research in the automatic text summarization community centers on developing algorithms to produce extract summaries, e. g. (,,,,), and).", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.6255558530489603}]}, {"text": "Yet understanding how to evaluate their output has received less attention.", "labels": [], "entities": []}, {"text": "\u2022 1997, TIPSTER sponsored a conference (SUM-MAC) where various text summarization algorithms were evaluated for their performance in various tasks ().", "labels": [], "entities": [{"text": "text summarization", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.6834467500448227}]}, {"text": "While extrinsic evaluation measures such as these are often very Concrete, the act of designing the task and scoring the results of the task introduces bias and subject-based variability.", "labels": [], "entities": []}, {"text": "These factors may confound the comparison of summarization algorithms.", "labels": [], "entities": [{"text": "summarization", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9819672107696533}]}, {"text": "Machine-generated summaries also maybe evaluated intrinsically by comparing them with \"ideal\" human-generated summaries.", "labels": [], "entities": []}, {"text": "However, there is often little agreement as to what constitutes the ideal summary of a document.", "labels": [], "entities": []}, {"text": "Both intrinsic and extrinsic methods require time consuming, expert human input in order to evaluate summaries.", "labels": [], "entities": []}, {"text": "While the traditional methods have many advantages, they are costly, and human assessors cannot always agree on summary quality.", "labels": [], "entities": []}, {"text": "If a numerical measure were available which did not depend on human judgement, researchers and developers would be able to immediately assess the effect of modifications to summary generation algorithms\u2022 Also, such a measure might be free of the bias that is introduced by human assessment.", "labels": [], "entities": []}, {"text": "This paper investigates the properties of various numerical measures for evaluating the quality of generic, indicative document summaries.", "labels": [], "entities": []}, {"text": "As explained by, a generic summary is not topic-related, but \"is aimed at abroad readership community\" and an indicative summary tells \"what topics are addressed in the source text, and thus can be used to alert the user as to source content.\"", "labels": [], "entities": []}, {"text": "Section 2 discusses the properties of numerical evaluation measures, points out several drawbacks associated with intrinsic measures and introduces new measures developed by the authors.", "labels": [], "entities": []}, {"text": "An experiment was devised to compare the new evaluation measures with the traditional ones.", "labels": [], "entities": []}, {"text": "The design of this experiment is discussed in Section 3 and its results are presented in Section 4.", "labels": [], "entities": []}, {"text": "The final section includes conclusions and a statement of the future work related to these evaluation measures.", "labels": [], "entities": []}], "datasetContent": [{"text": "An evaluation measure produces a numerical score which can be used to compare different summaries of the same document.", "labels": [], "entities": []}, {"text": "The scores are used to assess summary quality across a collection of test documents in order to produce an average for an algorithm or system.", "labels": [], "entities": []}, {"text": "However, it must be emphasized that the scores are {}9 most significant when considered per document.", "labels": [], "entities": []}, {"text": "For example, two different summaries of a document may have been produced by two different summarization algorithms.", "labels": [], "entities": [{"text": "summaries of a document", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.8408840149641037}]}, {"text": "Presumably, the summary with the higher score indicates that the system which produced it performed better than the other system.", "labels": [], "entities": []}, {"text": "Obviously, if one system consistently produces higher scores than another system, its average score will be higher, and one has reason to believe that it is a better system.", "labels": [], "entities": []}, {"text": "Thus, the important feature of any summary evaluation measure is not the value of its score, but rather the ranking its score imposes on a set of extracts of a document.", "labels": [], "entities": []}, {"text": "To compare two evaluation measures, whose scores may have very different ranges and distributions, one must compare the order in which the measures rank various summaries of a document.", "labels": [], "entities": []}, {"text": "For instance, suppose a summary scoring function Y is completely dependent upon the output of another scoring function X, such as Y --2 X.", "labels": [], "entities": []}, {"text": "Since Y is an increasing function of X, both X and Y will produce the same ranking of any set of summaries.", "labels": [], "entities": []}, {"text": "However, the scores produced by Y will have a very different distribution than those of X and the two sets of scores will not be correlated since the dependence of Y on X is non-linear.", "labels": [], "entities": []}, {"text": "Therefore, in order to compare the scores two different measures assign to a set of summaries, one must compare the ranks . they assign, not the actual scores.", "labels": [], "entities": []}, {"text": "The ranks assigned by an evaluation measure produce equivalence classes of extract summaries; each rank equivalence class contains summaries which received the same score.", "labels": [], "entities": []}, {"text": "When a measure produces the same score for two different summaries of a document, there is a tie, and the equivalence class will contain more than one summary.", "labels": [], "entities": []}, {"text": "All summaries in an equivalence class must share the same rank; let this rank be the midrank of the range of ranks that would have be assigned if each score were distinct.", "labels": [], "entities": []}, {"text": "An evaluation measure should posses the following properties: (i) higher-ranking summaries are more effective or are of higher quality than lower-ranking summaries, and (ii) all of the summaries in a rank equivalence class are moreor-less equally effective.", "labels": [], "entities": []}, {"text": "The following sections contrast the ranking properties of three types of evaluation measures: recall-based measures, a sentence-rank-based measure and content-based measures.", "labels": [], "entities": [{"text": "recall-based", "start_pos": 94, "end_pos": 106, "type": "METRIC", "confidence": 0.9964845180511475}]}, {"text": "These types of measures are defined, their properties are described and their use is explained.", "labels": [], "entities": []}, {"text": "Recall-based evaluation measures are intrinsic.", "labels": [], "entities": []}, {"text": "They compare machine-generated summaries with sentences previously extracted by human assessors or judges.", "labels": [], "entities": []}, {"text": "From each document, the judges extract sentences that they believe makeup the best extract summary of the document.", "labels": [], "entities": []}, {"text": "A summary of a document generated by a summarization algorithm is typically compared to one of these \"ground truth\" summaries by counting the number of sentences the ground truth summary and the algorithm's summary have in common.", "labels": [], "entities": []}, {"text": "Thus, the more sentences a summary has recalled from the ground truth, the higher its score will be.", "labels": [], "entities": [{"text": "score", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.9730557799339294}]}, {"text": "See work by and for examples of the use of this measure.", "labels": [], "entities": []}, {"text": "The recall-based measures introduce a bias since they are based on the Opinions of a small number of assessors.", "labels": [], "entities": [{"text": "recall-based", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9984784722328186}]}, {"text": "It is widely acknowledged) that assessor agreement is typically quite low.", "labels": [], "entities": [{"text": "agreement", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9531171321868896}]}, {"text": "There are at least two sources of this disagreement.", "labels": [], "entities": []}, {"text": "First, it is possible that one human assessor will pick a particular sentence for inclusion in their summary when the content of another sentence or set of sentences is approximately equivalent.", "labels": [], "entities": []}, {"text": "agree: \"...precision and recall are not the best measures for computing document quality.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9995704293251038}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9991154074668884}]}, {"text": "This is due to the fact that a small change in the summary output (e.g., replacing one sentence with an equally good equivalent which happens not to match majority opinion [of the assessors]) can dramatically affect a system's score.\"", "labels": [], "entities": []}, {"text": "We call this source of summary disagreement 'disagreement due to synonymy.'", "labels": [], "entities": []}, {"text": "Here is an example of two human-generated extracts from the same 1991 Wall Street Journal article which contain different sentences, but still seem to be describing an article about violin playing in a film: man.", "labels": [], "entities": [{"text": "Wall Street Journal article", "start_pos": 70, "end_pos": 97, "type": "DATASET", "confidence": 0.9342075884342194}]}, {"text": "The violin program in \"Prince of Tides\" eliminates the critic's usual edge and makes everyone fallback on his basic pair of ears.", "labels": [], "entities": []}, {"text": "EXTRACT 2: Journalistic ethics forbid me from saying if I think \"Prince of Tides\" is as good as \"Citizen Kane,\" but I don't think it's wrong to reveal that the film has some very fine violin playing.", "labels": [], "entities": [{"text": "EXTRACT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7162453532218933}]}, {"text": "But moviegoers will hear Mr. Zuckerman castoff the languor that too often makes him seem like the most bored of great violinists.", "labels": [], "entities": []}, {"text": "With each of these pieces, Mr. Zuckerman takes over the movie and shows what it means to play his instrument with supreme dash.", "labels": [], "entities": []}, {"text": "Another source of disagreement can arise from judges' differing opinions about the true focus of the original document.", "labels": [], "entities": []}, {"text": "In other words, judges disagree on what the document is about.", "labels": [], "entities": []}, {"text": "We call this second source 'disagreement due to focus.'", "labels": [], "entities": []}, {"text": "Here is a human-generated extract of the same article which seems to differ in focus: EXTRACT 3: Columbia Pictures has delayed the New York City and Los Angeles openings of \"Prince Of Tides\" fora week.", "labels": [], "entities": [{"text": "EXTRACT 3", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.8708882927894592}]}, {"text": "So Gothamites and Angelenos, along with the rest of the country, will have to wait until Christmas Day to see this film version of the Pat Conroy novel about a Southern football coach (Nick Nolte) dallying with a Jewish female psychotherapist (Barbra Streisand) in the Big Apple.", "labels": [], "entities": []}, {"text": "Perhaps the postponement is a sign that the studio is looking askance at this expensive product directed and co-produced \".by its female lead.", "labels": [], "entities": []}, {"text": "Whatever the source, disagreements at the sentence level are prevalent.", "labels": [], "entities": []}, {"text": "This has serious implications for measures based on a single opinion, when a slightly different opinion would result in a significantly different score (and rank) for many summaries.", "labels": [], "entities": []}, {"text": "For example, consider the following threesentence ground truth extract of a 37-sentence An extract that replaces sentence 13 with sentence 5: (5) In its most elementary form, it woul~d have setup a one-year examination of im-' prediments to world trade, but it would have also set an agenda for liberalizing trade rules in entirely new areas, such as financial services, telecommunications and investment.", "labels": [], "entities": []}, {"text": "will receive the same recall score as one which replaces sentence 13 with sentence 32: (32) Most nations have yet to go through this process, which they hope to complete by January.", "labels": [], "entities": [{"text": "recall score", "start_pos": 22, "end_pos": 34, "type": "METRIC", "confidence": 0.986402153968811}]}, {"text": "These two alternative summaries both have the same recall rank, but are obviously of very different quality.", "labels": [], "entities": [{"text": "recall rank", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.9829195439815521}]}, {"text": "Considered quantitatively, the only important component of either precision or recall is the 'sentence agreement' J, the number of sentences a summary has in common with the ground truth summary.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9993610978126526}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9960927367210388}, {"text": "sentence agreement' J", "start_pos": 94, "end_pos": 115, "type": "METRIC", "confidence": 0.7230435013771057}]}, {"text": "Following, let M be the number of sentences in aground truth extract summary and let K be the number of sentences in a summary to be evaluated.", "labels": [], "entities": []}, {"text": "With precision P = J/K and recall R = JIM as usual, and F1 = 2PR/(P + R); then elementary algebra shows that F1 = 2J/(M\u00f7K).", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9958167672157288}, {"text": "recall R = JIM", "start_pos": 27, "end_pos": 41, "type": "METRIC", "confidence": 0.88616544008255}, {"text": "F1", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9986214637756348}, {"text": "F1", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9952919483184814}]}, {"text": "Often, a fixed summary length K is used.", "labels": [], "entities": []}, {"text": "(In terms of word count, this represents varying compression rates.)", "labels": [], "entities": []}, {"text": "When a particular ground truth of a given document is chosen, then precision, recall and F1 are all constant multiples of J.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.999774158000946}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9997246861457825}, {"text": "F1", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9995701909065247}]}, {"text": "As such, these measures produce different scores, but the same ranking of all the K-sentence extracts from the document.", "labels": [], "entities": []}, {"text": "Since only this ranking is of interest, it is not necessary to examine more than one of P, Rand F1.", "labels": [], "entities": [{"text": "Rand F1", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.6576054096221924}]}, {"text": "The sentence agreement J can only take on integer values between 0 and M, so J, P, R, and F1 are all discrete variables.", "labels": [], "entities": [{"text": "F1", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9972735047340393}]}, {"text": "Therefore, although there maybe thousands of possible extract summaries of a document, only M + 1 different scores are possible.", "labels": [], "entities": [{"text": "M + 1", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.9374991456667582}]}, {"text": "This will obviously create a large number of ties in rankings produced by the P, R, and F1 scores, and will greatly increase the probability that radically different summaries will be given the same score and rank.", "labels": [], "entities": [{"text": "F1", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9984368681907654}]}, {"text": "On the other hand, two summaries which express the same ideas using different sentences will be given very different scores.", "labels": [], "entities": []}, {"text": "Both of these problems are illustrated by the example above.", "labels": [], "entities": []}, {"text": "Furthermore, if a particular ground truth includes a large proportion of the document's sentences (perhaps it is ~ very concise document), shorter summaries will likely include only sentences which appear in the ground truth.", "labels": [], "entities": []}, {"text": "Consequently, even a randomly selected collection of sentences might obtain the largest possible score.", "labels": [], "entities": []}, {"text": "Thus, recall-based measures are likely to violate both properties (i) and (ii), discussed at the beginning of Section 2.", "labels": [], "entities": [{"text": "recall-based", "start_pos": 6, "end_pos": 18, "type": "METRIC", "confidence": 0.9870014190673828}]}, {"text": "These inherent weaknesses in recall-based measures will be further explored in Section 4.", "labels": [], "entities": [{"text": "recall-based", "start_pos": 29, "end_pos": 41, "type": "METRIC", "confidence": 0.9907379746437073}]}, {"text": "This section describes the experiment that tests how well these summary evaluation metrics perform.", "labels": [], "entities": []}, {"text": "Fifteen documents from the Text Retrieval Conference (TREC) collection were used in the experiment.", "labels": [], "entities": [{"text": "Text Retrieval Conference (TREC)", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.8157819757858912}]}, {"text": "These documents are part of a corpus of 103 newspaper articles.", "labels": [], "entities": []}, {"text": "Each of the documents was tokenized by a language processing algorithm, which performed token aliasing.", "labels": [], "entities": []}, {"text": "In our experiments, the term set was comprised of all the aliases appearing in the full corpus of 103 documents.", "labels": [], "entities": []}, {"text": "This corpus was used for the purposes of term weighting.", "labels": [], "entities": [{"text": "term weighting", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.8066373467445374}]}, {"text": "Four expert judges created extract summaries (ground truths) for each of the documents.", "labels": [], "entities": []}, {"text": "A list of the first 15 documents, along with some of their numerical features is found in.", "labels": [], "entities": []}, {"text": "The judges were instructed to select as many sentences as were necessary to make an \"ideal\" indicative extract summary of the document.", "labels": [], "entities": []}, {"text": "In terms of the count of sentences in the ground truth, the lengths of the summaries varied from document to document.", "labels": [], "entities": []}, {"text": "Ground truth compression rates were generally between 10 and 20 percent.", "labels": [], "entities": [{"text": "Ground truth compression", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6188701391220093}]}, {"text": "The inter-assessor agreement also varied, but was often quite high.", "labels": [], "entities": []}, {"text": "We measured this by calculating the average pairwise recall in the collection of four ground truths.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9522963166236877}]}, {"text": "A suite of summary evaluation measures {Ek } which produce a score fora summary was developed.", "labels": [], "entities": []}, {"text": "These measures may depend on none, one, or all of the collection of ground truth summaries {gj}.", "labels": [], "entities": []}, {"text": "Measures which do not depend on ground truth compute the summarydocument similarity sire(s, d).", "labels": [], "entities": [{"text": "summarydocument similarity sire", "start_pos": 57, "end_pos": 88, "type": "METRIC", "confidence": 0.8543774088223776}]}, {"text": "Content-based measures which depend on a single ground truth gi compute the summary-ground truth similarity sim(s, gi).", "labels": [], "entities": [{"text": "summary-ground truth similarity sim", "start_pos": 76, "end_pos": 111, "type": "METRIC", "confidence": 0.7693162113428116}]}, {"text": "A measure which depends on all of the ground truths gl,.-.,ga, computes a summary's similarity with each ground truth separately and averages these values.", "labels": [], "entities": []}, {"text": "enumerates the 28 different evaluation measures that were compared in this experiment.", "labels": [], "entities": []}, {"text": "Note that the Recall and Kendall measures require aground truth.", "labels": [], "entities": [{"text": "Recall", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.61790931224823}]}, {"text": "In this study, the measures will be used to evaluate extract summaries of a fixed sentence length K.", "labels": [], "entities": []}, {"text": "In all of our tests, K = 3 for reasons of scale which will become clear.", "labels": [], "entities": [{"text": "K", "start_pos": 21, "end_pos": 22, "type": "METRIC", "confidence": 0.9776970148086548}, {"text": "scale", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9741609692573547}]}, {"text": "A summary length of three sentences represents varying proportions of the number of sentences in the full text document, but this length was usually comparable to the lengths of the humangenerated ground truths.", "labels": [], "entities": []}, {"text": "For each document, the collection {Sj} was generated.", "labels": [], "entities": []}, {"text": "This is the set of all possible K-sentence extracts from the document.", "labels": [], "entities": []}, {"text": "If the document has N sentences total, there will be N choose K N) N!", "labels": [], "entities": []}, {"text": "extracts in the exhaustive collection {Sj}.", "labels": [], "entities": []}, {"text": "The focus now is only on the set of all possible summaries and the evaluation measures, and not On any particular summarization algorithm.", "labels": [], "entities": []}, {"text": "For each document, each of the measures in {Ek} was used to rank the sets {Sj}.", "labels": [], "entities": []}, {"text": "(Note that the measures which do notdepend on ground truths could, in fact, be used to generate summaries if it were possible to produce and rank the exhaustive set of fixed-length summaries in real time.", "labels": [], "entities": []}, {"text": "Despite the authors' access to impressive computing power, the process took several hours for each document!)", "labels": [], "entities": []}, {"text": "The next section compares these different rankings of the exhaustive set of extracts for each document.", "labels": [], "entities": []}, {"text": "Calculate their Spearman rank correlation coef-'ficient.", "labels": [], "entities": [{"text": "Spearman rank correlation coef-'ficient", "start_pos": 16, "end_pos": 55, "type": "METRIC", "confidence": 0.7608251174290975}]}, {"text": "When two evaluation measures produce nearly the same ranking of the summary set, the rank correlation will be near 1 and a scatterplot of the two rankings will show points nearly ly-\u2022 ing on a line with slope 1.", "labels": [], "entities": [{"text": "rank correlation", "start_pos": 85, "end_pos": 101, "type": "METRIC", "confidence": 0.8263378441333771}]}, {"text": "When there is little correlation between two rankings, the statistic will be near 0 and the scatterplot will appear to have randomly-distributed points.", "labels": [], "entities": []}, {"text": "A negative correlation indicates that one ranking often reverses the rankings of the other and in this case a rank scatterplot will show points nearly lying on a line with negative slope.", "labels": [], "entities": []}, {"text": "compares the Spearman correlation of the rankings produced by a specific pair of ground truths.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 13, "end_pos": 33, "type": "METRIC", "confidence": 0.4740190654993057}]}, {"text": "The first row contains the correlations of two highly similar ground truth extracts of document 14.", "labels": [], "entities": []}, {"text": "Both of these extracts consisted of three sentences; two of the sentences were common to both extracts.", "labels": [], "entities": []}, {"text": "Not surprisingly, the correlation is high regardless of what measure produced the rankings.", "labels": [], "entities": [{"text": "correlation", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9884853363037109}]}, {"text": "The second row demonstrates an increase (across the row) in correlation between rankings produced by two different ground truth summaries of document 8.", "labels": [], "entities": []}, {"text": "These two ground truths did not disagree in focus, but did disagree due to synonymy --they contain just one sentence in common.", "labels": [], "entities": []}, {"text": "In general, the correlation among the rankings produced by synonymous ground truths was increased most by using the SVD content-based comparison.", "labels": [], "entities": []}, {"text": "illustrates the correlation increase graphically for this pair of ground truths.", "labels": [], "entities": [{"text": "correlation", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.979089617729187}]}, {"text": "By contrast, the third row of displays a decrease (across the row) in correlation between rankings produced by two different ground truths.", "labels": [], "entities": []}, {"text": "In this case, the two ground truths disagreed in .focus: they are Extracts 2 and 3 contrasted in Section 2.1.", "labels": [], "entities": []}, {"text": "Again, the correlation among the rankings produced by the four ground truths was decreased most by using a weighted content-based comparison such as tf-idf or SVD.", "labels": [], "entities": []}, {"text": "These patterns were typical for rankings produced by ground truths which differed in focus, allaying the fear that applying the SVD weighting would produce correlated rankings based on any two ground truths.", "labels": [], "entities": []}, {"text": "Of course, the lack of correlation among recall-based rankings whenever ground truths did not contain exactly the same sentences implies that a different collection of extracts would rank highly if one ground truth were replaced with the other.", "labels": [], "entities": [{"text": "recall-based", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9843065738677979}]}, {"text": "This effect would surely carry through to system averages across a set of documents.", "labels": [], "entities": []}, {"text": "To exemplify the size of this effect, for each document, the summaries which scored highest using one ground truth were scored (using recall) against a second ground truth.", "labels": [], "entities": [{"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9975826740264893}]}, {"text": "With the first ground truths, these high-scoring summaries averaged over 75% recall; using the second ground truths, the same summaries averaged just over 25% recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9992561936378479}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.997826874256134}]}, {"text": "Thus, by simply changing judges, an automatic system which produced these summaries would appear to have a very different success rate.", "labels": [], "entities": []}, {"text": "ity is lessened when content-based measures are used, but the outcomes are still disparate.", "labels": [], "entities": [{"text": "ity", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9786216616630554}]}, {"text": "Evidence suggests that the content-based measures which do not rely on aground truth \u2022 maybe an acceptable substitute to those which do'.-Over the set of 15 documents, the average within-document inter-assessor correlation is 0.61 using term frequency, 0.72 using tf-idf, and 0.67 using SVD.", "labels": [], "entities": []}, {"text": "The average correlation of the ground truth dependent measures with those that perform summary-document comparisons is 0.48 using term frequency, 0.70 using tf-idf, and 0.56 using SVD.", "labels": [], "entities": []}, {"text": "This means that on average, the rankings based on single ground truths are only slightly more correlated to each other than they are to the rankings that do not depend on any ground truth.", "labels": [], "entities": []}, {"text": "As noted in Section 2.1, the recall-based measures exhibit unfavorable scoring properties.", "labels": [], "entities": [{"text": "recall-based", "start_pos": 29, "end_pos": 41, "type": "METRIC", "confidence": 0.9981600642204285}]}, {"text": "shows the histogram of scores assigned to the exhaustive summary set for doc- Each of these measures was based on the same ground truth summary of this document, which contained four sentences.", "labels": [], "entities": []}, {"text": "Clearly, the measures based on a more sophisticated parsing method have a much greater ability to discriminate between summaries.", "labels": [], "entities": []}, {"text": "By contrast, the recail metric can assign one of only four scores to a length 3 summary, based on the value of Ji Elementary combinatorics shows that 4 extracts will receive the highest possible score (and thus will rank first), 126 summaries will rank second, 840 -summaries will rank third, and 1330 summaries will rank last (with a score of 0).", "labels": [], "entities": []}, {"text": "This accounts for all of the 2300 three-sentence extracts that, are possible.", "labels": [], "entities": []}, {"text": "It seems very unlikely that all of the second-ranking summaries are equally effective.", "labels": [], "entities": []}, {"text": "The histogram depicting this distribution is shown at the top of.", "labels": [], "entities": []}, {"text": "This is followed by the histograms for the Kendall metric, and the content-based metrics using term frequency, tf-idf, and SVD weighted vectors, respectively.", "labels": [], "entities": []}, {"text": "The tf-idf and SVD weighted measures produced a very fine distribution of scores, particularly near the top of the range.", "labels": [], "entities": []}, {"text": "That is, these metrics are able to distinguish between different high-scoring summaries.", "labels": [], "entities": []}, {"text": "These patterns in the score histograms were typical across the 15 documents.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test Document & Summary Statistics  Sent.  TREC File Name Count  WSJ911211-0057  34  wsJg00608-0126  34  WSJ900712-0047  \" 18  latwp940604.0027  23  latwp940621.0116  27  latwp940624.0094  17  latwp940707.0400  33  latwp940709.0051  37  latwp940713.0013  34  latwp940713.0014  30  latwp940721.0080  28  latwp940725.0030  36  latwp940725.0128  18  latwp940729.0109  25  ' l'atwp940801.0010  28", "labels": [], "entities": [{"text": "TREC File Name Count  WSJ911211-0057", "start_pos": 53, "end_pos": 89, "type": "DATASET", "confidence": 0.8241843819618225}]}, {"text": " Table 3: Correlation of Ground Truths Depends on Level of Disagreement", "labels": [], "entities": [{"text": "Correlation of Ground Truths Depends", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.7807311058044434}]}]}