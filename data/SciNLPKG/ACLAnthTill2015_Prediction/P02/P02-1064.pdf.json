{"title": [{"text": "An Empirical Study of Active Learning with Support Vector Machines for Japanese Word Segmentation", "labels": [], "entities": [{"text": "Japanese Word Segmentation", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.6198789378007253}]}], "abstractContent": [{"text": "We explore how active learning with Support Vector Machines works well fora non-trivial task in natural language processing.", "labels": [], "entities": []}, {"text": "We use Japanese word segmenta-tion as a test case.", "labels": [], "entities": []}, {"text": "In particular, we discuss how the size of a pool affects the learning curve.", "labels": [], "entities": []}, {"text": "It is found that in the early stage of training with a larger pool, more labeled examples are required to achieve a given level of accuracy than those with a smaller pool.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9980787038803101}]}, {"text": "In addition, we propose a novel technique to use a large number of unlabeled examples effectively by adding them gradually to a pool.", "labels": [], "entities": []}, {"text": "The experimental results show that our technique requires less labeled examples than those with the technique in previous research.", "labels": [], "entities": []}, {"text": "To achieve 97.0 % accuracy, the proposed technique needs 59.3 % of labeled examples that are required when using the previous technique and only 17.4 % of labeled examples with random sampling.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.998989999294281}]}], "introductionContent": [{"text": "Corpus-based supervised learning is now a standard approach to achieve high-performance in natural language processing.", "labels": [], "entities": []}, {"text": "However, the weakness of supervised learning approach is to need an annotated corpus, the size of which is reasonably large.", "labels": [], "entities": []}, {"text": "Even if we have a good supervised-learning method, we cannot get high-performance without an annotated corpus.", "labels": [], "entities": []}, {"text": "The problem is that corpus annotation is labour intensive and very expensive.", "labels": [], "entities": [{"text": "corpus annotation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.790145993232727}]}, {"text": "In order to overcome this, some unsupervised learning methods and minimally-supervised methods, e.g.,), have been proposed.", "labels": [], "entities": []}, {"text": "However, such methods usually depend on tasks or domains and their performance often does not match one with a supervised learning method.", "labels": [], "entities": []}, {"text": "Another promising approach is active learning, in which a classifier selects examples to be labeled, and then requests a teacher to label them.", "labels": [], "entities": []}, {"text": "It is very different from passive learning, in which a classifier gets labeled examples randomly.", "labels": [], "entities": []}, {"text": "Active learning is a general framework and does not depend on tasks or domains.", "labels": [], "entities": [{"text": "Active learning", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7397794425487518}]}, {"text": "It is expected that active learning will reduce considerably manual annotation cost while keeping performance.", "labels": [], "entities": []}, {"text": "However, few papers in the field of computational linguistics have focused on this approach.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.7407226264476776}]}, {"text": "Although there are many active learning methods with various classifiers such as a probabilistic classifier, we focus on active learning with Support Vector Machines (SVMs) because of their performance.", "labels": [], "entities": []}, {"text": "The Support Vector Machine, which is introduced by, is a powerful new statistical learning method.", "labels": [], "entities": []}, {"text": "Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth.", "labels": [], "entities": [{"text": "character recognition", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.8383269608020782}, {"text": "face detection", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.9194187223911285}, {"text": "image classification", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.8111590147018433}]}, {"text": "SVMs have been recently applied to several natural language tasks, including text classification), chunking (, and dependency analysis (.", "labels": [], "entities": [{"text": "text classification", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7866039574146271}, {"text": "chunking", "start_pos": 99, "end_pos": 107, "type": "TASK", "confidence": 0.9651303291320801}, {"text": "dependency analysis", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.8198078870773315}]}, {"text": "SVMs have been greatly successful in such tasks.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.7171841859817505}]}, {"text": "Additionally, SVMs as well as boosting have good theoretical background.", "labels": [], "entities": []}, {"text": "The objective of our research is to develop an effective way to build a corpus and to create highperformance NL systems with minimal cost.", "labels": [], "entities": []}, {"text": "As a first step, we focus on investigating how active learning with SVMs, which have demonstrated excellent performance, works for complex tasks in natural language processing.", "labels": [], "entities": []}, {"text": "For text classification, it is found that this approach is effective).", "labels": [], "entities": [{"text": "text classification", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8795109689235687}]}, {"text": "They used less than 10,000 binary features and less than 10,000 examples.", "labels": [], "entities": []}, {"text": "However, it is not clear that the approach is readily applicable to tasks which have more than 100,000 features and more than 100,000 examples.", "labels": [], "entities": []}, {"text": "We use Japanese word segmentation as a test case.", "labels": [], "entities": [{"text": "Japanese word segmentation", "start_pos": 7, "end_pos": 33, "type": "TASK", "confidence": 0.6407019098599752}]}, {"text": "The task is suitable for our purpose because we have to handle combinations of more than 1,000 characters and a very large corpus exists.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the EDR Japanese Corpus (EDR, 1995) for experiments.", "labels": [], "entities": [{"text": "EDR Japanese Corpus (EDR, 1995)", "start_pos": 12, "end_pos": 43, "type": "DATASET", "confidence": 0.9563209488987923}]}, {"text": "The corpus is assembled from various sources such as newspapers, magazines, and textbooks.", "labels": [], "entities": []}, {"text": "We selected randomly 20,000 sentences for training and 10,000 sentences for testing.", "labels": [], "entities": []}, {"text": "Then, we created examples using the feature encoding method in Section 4.", "labels": [], "entities": []}, {"text": "Through these experiments we used the original SVM tools, the algorithm of which is based on SMO (Sequential Minimal Optimization) by.", "labels": [], "entities": []}, {"text": "We used linear SVMs and set a missclassification cost \ud97b\udf59 to \u00bc\ud97b\udf59\u00be.", "labels": [], "entities": [{"text": "missclassification", "start_pos": 30, "end_pos": 48, "type": "METRIC", "confidence": 0.997628390789032}]}, {"text": "First, we changed the number of labeled examples which were randomly selected.", "labels": [], "entities": []}, {"text": "This is an experiment on passive learning.", "labels": [], "entities": []}, {"text": "shows the accuracy at different sizes of labeled examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996494054794312}]}, {"text": "Second, we changed the number of examples in a pool and ran the active learning algorithm in Section 3.2.", "labels": [], "entities": []}, {"text": "We use the same examples fora pool as those used in the passive learning experiments.", "labels": [], "entities": []}, {"text": "We selected 1,000 examples at each iteration of the active learning.", "labels": [], "entities": []}, {"text": "shows the learning curve of this experiment and is a close-up of.", "labels": [], "entities": []}, {"text": "We see from that active learning works quite well and it significantly reduces labeled examples to be required.", "labels": [], "entities": []}, {"text": "Let us see how many labeled examples are required to achieve 96.0 % accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9848350286483765}]}, {"text": "In active learning with the pool, the size of which is 2,500 sentences (97,349 examples), only 28,813 labeled examples are needed, whereas in passive learning, about 97,000 examples are required.", "labels": [], "entities": []}, {"text": "That means over 70 % reduction is realized by active learning.", "labels": [], "entities": []}, {"text": "In the case of 97 % accuracy, approximately the same percentage of reduction is realized when using the pool, the size of which is 20,000 sentences (776,586 examples).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9966818690299988}]}, {"text": "Now let us see how the accuracy curve varies depending on the size of a pool.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995852112770081}]}, {"text": "Surprisingly, the performance of a larger pool is worse than that of a smaller pool in the early stage of training . One reason for this could be that support vectors in selected examples at each iteration from a larger pool make larger clusters than those selected from a smaller pool do.", "labels": [], "entities": []}, {"text": "In other words, in the case of a larger pool, more examples selected at each iteration would be similar to each other.", "labels": [], "entities": []}, {"text": "We computed variances 6 of each 1,000 selected examples at the learning iteration from 2 to 11).", "labels": [], "entities": []}, {"text": "The variances of se- lected examples using the 20,000 sentence size pool is always lower than those using the 1,250 sentence size pool.", "labels": [], "entities": []}, {"text": "The result is not inconsistent with our hypothesis.", "labels": [], "entities": []}, {"text": "Before we discuss the results of Two Pool Algorithm, we show in how support vectors of a classifier increase and the accuracy changes when using the 2,500 sentence size pool.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9992550015449524}]}, {"text": "It is clear that after the accuracy improvement almost stops, the increment of the number of support vectors is down.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9995614886283875}, {"text": "increment", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9849920272827148}]}, {"text": "We also observed the same phenomenon with different sizes of pools.", "labels": [], "entities": []}, {"text": "We utilize this phenomenon in Algorithm A. Next, we ran Two Pool Algorithm A 7 . The result is shown in.", "labels": [], "entities": []}, {"text": "The accuracy curve of Algorithm A is better than that of the previously proposed method at the number of labeled examples roughly up to 20,000.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996291399002075}]}, {"text": "After that, however, the performance of Algorithm A does not clearly exceed that of the previous method.", "labels": [], "entities": [{"text": "Algorithm A", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.7175958752632141}]}, {"text": "The result of Algorithm B is shown in.", "labels": [], "entities": [{"text": "Algorithm B", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.6365203261375427}]}, {"text": "We have tried three values for AE : 5 %, 10 %, and 20 %.", "labels": [], "entities": [{"text": "AE", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9982006549835205}]}, {"text": "The performance with AE of 10 %, which is best, is plotted in.", "labels": [], "entities": [{"text": "AE", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9998188614845276}]}, {"text": "As noted above, the improvement by Algorithm A is limited, whereas it is remarkable that the accuracy curve of Algorithm B is always the same or better than those of the previous algorithm with different sizes of pools (the detailed information about the performance is shown in Table 3 In order to stabilize the algorithm, we use the following strategy at (d) in: add new unlabeled examples to the primary pool when the current increment of support vectors is less than half of the average increment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.999467670917511}]}, {"text": "uated it by Japanese word segmentation.", "labels": [], "entities": [{"text": "Japanese word segmentation", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.6049673755963644}]}, {"text": "Our technique outperforms the method in previous research and can significantly reduce required labeled examples to achieve a given level of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9953927993774414}]}], "tableCaptions": [{"text": " Table 1: Variances of Selected Examples", "labels": [], "entities": []}, {"text": " Table 2: Accuracy at Different Labeled Data Sizes  with Random Sampling", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9960195422172546}]}, {"text": " Table 3: Accuracy of Different Active Learning Al- gorithms", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.972265899181366}]}]}