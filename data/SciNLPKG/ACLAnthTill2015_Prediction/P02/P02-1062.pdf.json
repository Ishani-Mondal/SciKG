{"title": [{"text": "Ranking Algorithms for Named-Entity Extraction: Boosting and the Voted Perceptron", "labels": [], "entities": [{"text": "Named-Entity Extraction", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6674271374940872}]}], "abstractContent": [{"text": "This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.", "labels": [], "entities": []}, {"text": "The first approach uses a boosting algorithm for ranking problems.", "labels": [], "entities": []}, {"text": "The second approach uses the voted perceptron algorithm.", "labels": [], "entities": []}, {"text": "Both algorithms give comparable , significant improvements over the maximum-entropy baseline.", "labels": [], "entities": []}, {"text": "The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures.", "labels": [], "entities": [{"text": "parsing and tagging", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7697117924690247}]}, {"text": "Examples of such techniques are Markov Random Fields), and boosting algorithms (.", "labels": [], "entities": []}, {"text": "One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included.", "labels": [], "entities": []}, {"text": "A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures.", "labels": [], "entities": []}, {"text": "This discriminative property is shared by the methods of (, and also the Conditional Random Field methods of ().", "labels": [], "entities": []}, {"text": "Ina previous paper), a boosting algorithm was used to rerank the output from an existing statistical parser, giving significant improvements in parsing accuracy on Wall Street Journal data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 144, "end_pos": 151, "type": "TASK", "confidence": 0.9695009589195251}, {"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.934732973575592}, {"text": "Wall Street Journal data", "start_pos": 164, "end_pos": 188, "type": "DATASET", "confidence": 0.974066436290741}]}, {"text": "Similar boosting algorithms have been applied to natural language generation, with good results, in (.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.6609578430652618}]}, {"text": "In this paper we apply reranking methods to named-entity extraction.", "labels": [], "entities": [{"text": "named-entity extraction", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.7187522649765015}]}, {"text": "A state-ofthe-art (maximum-entropy) tagger is used to generate 20 possible segmentations for each input sentence, along with their probabilities.", "labels": [], "entities": []}, {"text": "We describe a number of additional global features of these candidate segmentations.", "labels": [], "entities": []}, {"text": "These additional features are used as evidence in reranking the hypotheses from the max-ent tagger.", "labels": [], "entities": []}, {"text": "We describe two learning algorithms: the boosting method of, and a variant of the voted perceptron algorithm, which was initially described in).", "labels": [], "entities": []}, {"text": "We applied the methods to a corpus of over one million words of tagged web data.", "labels": [], "entities": []}, {"text": "The methods give significant improvements over the maximum-entropy tagger (a 17.7% relative reduction in error-rate for the voted perceptron, and a 15.6% relative improvement for the boosting method).", "labels": [], "entities": [{"text": "error-rate", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9507827758789062}]}, {"text": "One contribution of this paper is to show that existing reranking methods are useful fora new domain, named-entity tagging, and to suggest global features which give improvements on this task.", "labels": [], "entities": [{"text": "named-entity tagging", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.6783322542905807}]}, {"text": "We should stress that another contribution is to show that anew algorithm, the voted perceptron, gives very credible results on a natural language task.", "labels": [], "entities": []}, {"text": "It is an extremely simple algorithm to implement, and is very fast to train (the testing phase is slower, but by no means sluggish).", "labels": [], "entities": []}, {"text": "It should be a viable alternative to methods such as the boosting or Markov Random Field algorithms described in previous work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied the voted perceptron and boosting algorithms to the data described in section 2.3.", "labels": [], "entities": []}, {"text": "Only features occurring on 5 or more distinct training sentences were included in the model.", "labels": [], "entities": []}, {"text": "This resulted Note that, for reasons of explication, the decoding algorithm we present is less efficient than necessary.", "labels": [], "entities": []}, {"text": "For example, when it is preferable to use some book-keeping to avoid recalculation of  in 93,777 distinct features.", "labels": [], "entities": []}, {"text": "The two methods were trained on the training portion (41,992 sentences) of the training set.", "labels": [], "entities": []}, {"text": "We used the development set to pick the best values for tunable parameters in each algorithm.", "labels": [], "entities": []}, {"text": "For boosting, the main parameter to pick is the number of rounds, \u009a . We ran the algorithm fora total of 300,000 rounds, and found that the optimal value for F-measure on the development set occurred after 83,233 rounds.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.9904441237449646}]}, {"text": "For the voted perceptron, the representation was found to give the best results on the development set.", "labels": [], "entities": []}, {"text": "shows the results for the three methods on the test set.", "labels": [], "entities": []}, {"text": "Both of the reranking algorithms show significant improvements over the baseline: a 15.6% relative reduction in error for boosting, and a 17.7% relative error reduction for the voted perceptron.", "labels": [], "entities": [{"text": "error", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.8356181979179382}]}, {"text": "In our experiments we found the voted perceptron algorithm to be considerably more efficient in training, at some cost in computation on test examples.", "labels": [], "entities": []}, {"text": "Another attractive property of the voted perceptron is that it can be used with kernels, for example the kernels over parse trees described in.", "labels": [], "entities": []}, {"text": "(Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper.", "labels": [], "entities": []}, {"text": "See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems.", "labels": [], "entities": []}], "tableCaptions": []}