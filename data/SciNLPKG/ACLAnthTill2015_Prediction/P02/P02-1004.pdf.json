{"title": [{"text": "Machine-learned contexts for linguistic operations in German sentence realization", "labels": [], "entities": [{"text": "German sentence realization", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.5493173797925314}]}], "abstractContent": [{"text": "We show that it is possible to learn the contexts for linguistic operations which map a semantic representation to a surface syntactic tree in sentence realization with high accuracy.", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 143, "end_pos": 163, "type": "TASK", "confidence": 0.7325798124074936}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9926642179489136}]}, {"text": "We cast the problem of learning the contexts for the linguistic operations as classification tasks, and apply straightforward machine learning techniques, such as decision tree learning.", "labels": [], "entities": [{"text": "decision tree learning", "start_pos": 163, "end_pos": 185, "type": "TASK", "confidence": 0.7086876630783081}]}, {"text": "The training data consist of linguistic features extracted from syntactic and semantic representations produced by a linguistic analysis system.", "labels": [], "entities": []}, {"text": "The target features are extracted from links to surface syntax trees.", "labels": [], "entities": []}, {"text": "Our evidence consists of four examples from the German sentence realization system code-named Amalgam: case assignment, assignment of verb position features, extraposition, and syntactic aggregation", "labels": [], "entities": [{"text": "case assignment", "start_pos": 103, "end_pos": 118, "type": "TASK", "confidence": 0.6579889804124832}]}], "introductionContent": [{"text": "The last stage of natural language generation, sentence realization, creates the surface string from an abstract (typically semantic) representation.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.7272112568219503}, {"text": "sentence realization", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7403413504362106}]}, {"text": "This mapping from abstract representation to surface string can be direct, or it can employ intermediate syntactic representations which significantly constrain the output.", "labels": [], "entities": []}, {"text": "Furthermore, the mapping can be performed purely by rules, by application of statistical models, or by a combination of both techniques.", "labels": [], "entities": []}, {"text": "Among the systems that use statistical or machine learned techniques in sentence realization, there are various degrees of intermediate syntactic structure.", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7317764610052109}]}, {"text": "Nitrogen () produces a large set of alternative surface realizations of an input structure (which can vary in abstractness).", "labels": [], "entities": []}, {"text": "This set of candidate surface strings, represented as a word lattice, is then rescored by a wordbigram language model, to produce the bestranked output sentence.", "labels": [], "entities": []}, {"text": "FERGUS), on the other hand, employs a model of syntactic structure during sentence realization.", "labels": [], "entities": [{"text": "FERGUS", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7625488042831421}, {"text": "sentence realization", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.7099828273057938}]}, {"text": "In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system.", "labels": [], "entities": []}, {"text": "This tree-based model chooses a best-ranked XTAG representation fora given dependency structure.", "labels": [], "entities": []}, {"text": "Possible linearizations of the XTAG representation are generated and then evaluated by a language model to pick the best possible linearization, as in Nitrogen.", "labels": [], "entities": []}, {"text": "In contrast, the sentence realization system code-named Amalgam (A Machine Learned Generation Module)) employs a series of linguistic operations which map a semantic representation to a surface syntactic tree via intermediate syntactic representations.", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.7542249262332916}]}, {"text": "The contexts for most of these operations in Amalgam are machine learned.", "labels": [], "entities": []}, {"text": "The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read.", "labels": [], "entities": []}, {"text": "The goal of this paper is to show that it is possible to learn accurately the contexts for linguistically complex operations in sentence realization.", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.7157548666000366}]}, {"text": "We propose that learning the contexts for the application of these linguistic operations can be viewed as per-operation classification problems.", "labels": [], "entities": []}, {"text": "This approach combines advantages of a linguistically informed approach to sentence realization with the advantages of a machine learning approach.", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.7658708393573761}]}, {"text": "The linguistically informed approach allows us to deal with complex linguistic phenomena, while machine learning automates the discovery of contexts that are linguistically relevant and relevant for the domain of the data.", "labels": [], "entities": []}, {"text": "The machine learning approach also facilitates adaptation of the system to anew domain or language.", "labels": [], "entities": []}, {"text": "Furthermore, the quantitative nature of the machine learned models permits finer distinctions and ranking among possible solutions.", "labels": [], "entities": []}, {"text": "To substantiate our claim, we provide four examples from Amalgam: assignment of case, assignment of verb position features, extraposition, and syntactic aggregation.", "labels": [], "entities": [{"text": "Amalgam", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.8424092531204224}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Note that it  is impossible to give precision/recall numbers,  without a hand-disambiguated test set. The  baseline for this task is 0.7049 (accuracy if the  most frequent case (nominative) had been  assigned to all NPs).", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9985142350196838}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9155359268188477}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9994205236434937}]}, {"text": " Table 1. Accuracy of the case assignment model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9964305758476257}, {"text": "case assignment", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7731004059314728}]}, {"text": " Table 2. As a  point of reference for the verb position classifier,  assigning the most frequent value (second) of the  target feature yields a baseline score of 0.4240.", "labels": [], "entities": []}, {"text": " Table 2. Precision, recall, and F-measure for the verb  position model.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9967267513275146}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9967960715293884}, {"text": "F-measure", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9980536699295044}]}, {"text": " Table 3. The model has 116  branching nodes. The baseline for this task is  calculated by applying the most frequent value for  the target feature (\"don't move\") to all nodes. The  baseline for extraposition of infinitival and  complement clauses is very high. The number of  extraposed clauses of both types in the test set", "labels": [], "entities": []}, {"text": " Table 3. Accuracy of the extraposition model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9967854022979736}]}, {"text": " Table 4. As was to be expected  on the basis of linguistic intuition, the value  \"middle\" for the target feature did not play any  role. In the test set there were only 2 observed  instances of that value. The baseline for this task  is 0.8566 (assuming \"first\" as the default value).", "labels": [], "entities": []}, {"text": " Table 4. Precision, recall, and F-measure for the  syntactic aggregation model.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9984993934631348}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9988852143287659}, {"text": "F-measure", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9991680383682251}]}]}