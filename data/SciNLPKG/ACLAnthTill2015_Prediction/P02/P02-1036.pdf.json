{"title": [{"text": "Dynamic programming for parsing and estimation of stochastic unification-based grammars *", "labels": [], "entities": [{"text": "parsing and estimation of stochastic unification-based grammars", "start_pos": 24, "end_pos": 87, "type": "TASK", "confidence": 0.7532451961721692}]}], "abstractContent": [{"text": "Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unification-based grammar (UBG).", "labels": [], "entities": [{"text": "Stochastic unification-based grammars (SUBGs)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7389868895212809}]}, {"text": "Existing algorithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one, or in order to calculate the statistics needed to estimate a grammar from a training corpus.", "labels": [], "entities": [{"text": "parsing and estimation", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.6059186160564423}]}, {"text": "This paper describes a graph-based dynamic programming algorithm for calculating these statistics from the packed UBG parse representations of Maxwell and Kaplan (1995) which does not require enumerating all parses.", "labels": [], "entities": []}, {"text": "Like many graphical algorithms, the dynamic programming algorithm's complexity is worst-case exponential, but is often polynomial.", "labels": [], "entities": []}, {"text": "The key observation is that by using Maxwell and Kaplan packed representations , the required statistics can be rewritten as either the max or the sum of a product of functions.", "labels": [], "entities": []}, {"text": "This is exactly the kind of problem which can be solved by dynamic programming over graphical models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Stochastic Unification-Based Grammars (SUBGs) use log-linear models (also known as exponential or MaxEnt models and Markov Random Fields) to define probability distributions over the parses of a unification grammar.", "labels": [], "entities": [{"text": "Stochastic Unification-Based Grammars (SUBGs", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6616468548774719}]}, {"text": "These grammars can incorporate virtually all kinds of linguistically important constraints (including non-local and non-context-free constraints), and are equipped with a statistically sound framework for estimation and learning.", "labels": [], "entities": []}, {"text": "pointed out that the non-contextfree dependencies of a unification grammar require stochastic models more general than Probabilistic Context-Free Grammars (PCFGs) and Markov Branching Processes, and proposed the use of loglinear models for defining probability distributions over the parses of a unification grammar.", "labels": [], "entities": [{"text": "unification grammar", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.9170843064785004}]}, {"text": "Unfortunately, the maximum likelihood estimator Abney proposed for SUBGs seems computationally intractable since it requires statistics that depend on the set of all parses of all strings generated by the grammar.", "labels": [], "entities": []}, {"text": "This set is infinite (so exhaustive enumeration is impossible) and presumably has a very complex structure (so sampling estimates might take an extremely longtime to converge).", "labels": [], "entities": []}, {"text": "observed that parsing and related tasks only require conditional distributions over parses given strings, and that such conditional distributions are considerably easier to estimate than joint distributions of strings and their parses.", "labels": [], "entities": [{"text": "parsing", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.9839665293693542}]}, {"text": "The conditional maximum likelihood estimator proposed by Johnson et al. requires statistics that depend on the set of all parses of the strings in the training cor-pus.", "labels": [], "entities": []}, {"text": "For most linguistically realistic grammars this set is finite, and for moderate sized grammars and training corpora this estimation procedure is quite feasible.", "labels": [], "entities": []}, {"text": "However, our recent experiments involve training from the Wall Street Journal Penn Tree-bank, and repeatedly enumerating the parses of its 50,000 sentences is quite time-consuming.", "labels": [], "entities": [{"text": "Wall Street Journal Penn Tree-bank", "start_pos": 58, "end_pos": 92, "type": "DATASET", "confidence": 0.9521916389465332}, {"text": "enumerating the parses of its 50,000 sentences", "start_pos": 109, "end_pos": 155, "type": "TASK", "confidence": 0.7322054590497699}]}, {"text": "Matters are only made worse because we have moved some of the constraints in the grammar from the unification component to the stochastic component.", "labels": [], "entities": []}, {"text": "This broadens the coverage of the grammar, but at the expense of massively expanding the number of possible parses of each sentence.", "labels": [], "entities": []}, {"text": "In the mid-1990s unification-based parsers were developed that do not enumerate all parses of a string but instead manipulate and return a \"packed\" representation of the set of parses.", "labels": [], "entities": []}, {"text": "This paper describes how to find the most probable parse and the statistics required for estimating a SUBG from the packed parse set representations proposed by.", "labels": [], "entities": []}, {"text": "This makes it possible to avoid explicitly enumerating the parses of the strings in the training corpus.", "labels": [], "entities": []}, {"text": "The methods proposed here are analogues of the well-known dynamic programming algorithms for Probabilistic Context-Free Grammars (PCFGs); specifically the Viterbi algorithm for finding the most probable parse of a string, and the InsideOutside algorithm for estimating a PCFG from unparsed training data.", "labels": [], "entities": []}, {"text": "1 In fact, because Maxwell and Kaplan packed representations are just Truth Maintenance System (TMS) representations (Forbus and de), the statistical techniques described here should extend to non-linguistic applications of TMSs as well.", "labels": [], "entities": []}, {"text": "Dynamic programming techniques have been applied to log-linear models before.", "labels": [], "entities": []}, {"text": "mention that dynamic programming can be used to compute the statistics required for conditional estimation of log-linear models based on context-free grammars where the properties can include arbitrary functions of the input string.", "labels": [], "entities": []}, {"text": "(which appeared after this paper was accepted) is the closest related work we know of.", "labels": [], "entities": []}, {"text": "They describe a technique for calculating the statistics required to estimate a log-linear parsing model with non-local properties from packed feature forests.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "The next section describes unification grammars and Maxwell and Kaplan packed representation.", "labels": [], "entities": [{"text": "unification grammars", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.9326204359531403}]}, {"text": "The following section reviews stochastic unification grammars (Abney, 1997) and the statistical quantities required for efficiently estimating such grammars from parsed training data).", "labels": [], "entities": [{"text": "stochastic unification grammars", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.736745814482371}]}, {"text": "The final substantive section of this paper shows how these quantities can be defined directly in terms of the Maxwell and Kaplan packed representations.", "labels": [], "entities": []}, {"text": "The notation used in this paper is as follows.", "labels": [], "entities": []}, {"text": "Variables are written in uppercase italic, e.g., X, Y , etc., the sets they range over are written in script, e.g., X , Y, etc., while specific values are written in lowercase italic, e.g., x, y, etc.", "labels": [], "entities": []}, {"text": "In the case of vector-valued entities, subscripts indicate particular components.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}