{"title": [], "abstractContent": [{"text": "We present a constancy rate principle governing language generation.", "labels": [], "entities": [{"text": "language generation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7310876995325089}]}, {"text": "We show that this principle implies that local measures of entropy (ignoring context) should increase with the sentence number.", "labels": [], "entities": []}, {"text": "We demonstrate that this is indeed the case by measuring entropy in three different ways.", "labels": [], "entities": []}, {"text": "We also show that this effect has both lexical (which words are used) and non-lexical (how the words are used) causes.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is well-known from Information Theory that the most efficient way to send information through noisy channels is at a constant rate.", "labels": [], "entities": [{"text": "Information Theory", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8111394643783569}]}, {"text": "If humans try to communicate in the most efficient way, then they must obey this principle.", "labels": [], "entities": []}, {"text": "The communication medium we examine in this paper is text, and we present some evidence that this principle holds here.", "labels": [], "entities": []}, {"text": "Entropy is a measure of information first proposed by.", "labels": [], "entities": [{"text": "Entropy", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6590622067451477}]}, {"text": "Informally, entropy of a random variable is proportional to the difficulty of correctly guessing the value of this variable (when the distribution is known).", "labels": [], "entities": []}, {"text": "Entropy is the highest when all values are equally probable, and is lowest (equal to 0) when one of the choices has probability of 1, i.e. deterministically known in advance.", "labels": [], "entities": [{"text": "Entropy", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9874789714813232}]}, {"text": "In this paper we are concerned with entropy of English as exhibited through written text, though these results can easily be extended to speech as well.", "labels": [], "entities": []}, {"text": "The random variable we deal with is therefore a unit of text (a word, for our purposes 1 ) that a random person who has produced all the previous words in the text stream is likely to produce next.", "labels": [], "entities": []}, {"text": "We have as many random variables as we have words in a text.", "labels": [], "entities": []}, {"text": "The distributions of these variables are obviously different and depend on all previous words produced.", "labels": [], "entities": []}, {"text": "We claim, however, that the entropy of these random variables is on average the same 2 .", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}