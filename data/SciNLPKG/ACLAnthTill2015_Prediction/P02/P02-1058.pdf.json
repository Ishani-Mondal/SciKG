{"title": [{"text": "From Single to Multi-document Summarization: A Prototype System and its Evaluation", "labels": [], "entities": [{"text": "Multi-document Summarization", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.5679843425750732}]}], "abstractContent": [{"text": "NeATS is a multi-document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order.", "labels": [], "entities": [{"text": "NeATS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8895320892333984}]}, {"text": "NeATS is among the best performers in the large scale summarization evaluation DUC 2001.", "labels": [], "entities": [{"text": "NeATS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9156025648117065}, {"text": "DUC 2001", "start_pos": 79, "end_pos": 87, "type": "DATASET", "confidence": 0.6498919427394867}]}], "introductionContent": [{"text": "In recent years, text summarization has been enjoying a period of revival.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.830902099609375}]}, {"text": "Two workshops on Automatic Summarization were held in.", "labels": [], "entities": [{"text": "Automatic Summarization", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.8123511373996735}]}, {"text": "However, the area is still being fleshed out: most past efforts have focused only on single-document summarization, and no standard test sets and large scale evaluations have been reported or made available to the Englishspeaking research community except the TIPSTER SUMMAC Text Summarization evaluation ().", "labels": [], "entities": [{"text": "TIPSTER SUMMAC Text Summarization", "start_pos": 260, "end_pos": 293, "type": "TASK", "confidence": 0.5532819777727127}]}, {"text": "To address these issues, the Document Understanding Conference (DUC) sponsored by the National Institute of Standards and Technology (NIST) started in 2001 in the United States.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)", "start_pos": 29, "end_pos": 68, "type": "TASK", "confidence": 0.8513075113296509}]}, {"text": "The Text Summarization Challenge (TSC) task under the NTCIR (NII-NACSIS Test C ollection for IR Systems) project started in 2000 in Japan.", "labels": [], "entities": [{"text": "Text Summarization Challenge (TSC)", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.8778443038463593}, {"text": "NTCIR (NII-NACSIS Test C ollection", "start_pos": 54, "end_pos": 88, "type": "DATASET", "confidence": 0.8418660859266917}]}, {"text": "DUC and TSC both aim to compile standard training and test collections that can be shared among researchers and to provide common and large scale evaluations in single and multiple document summarization for their participants.", "labels": [], "entities": [{"text": "DUC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9745460748672485}, {"text": "TSC", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.6710668802261353}]}, {"text": "In this paper we describe a multi -document summarization system NeATS.", "labels": [], "entities": [{"text": "NeATS", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.9133095145225525}]}, {"text": "It attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order.", "labels": [], "entities": []}, {"text": "We outline the NeATS system and describe how it performs content selection, filtering, and presentation in Section 2.", "labels": [], "entities": [{"text": "content selection, filtering", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.693144679069519}]}, {"text": "Section 3 gives a brief overview of the evaluation procedure used in).", "labels": [], "entities": []}, {"text": "Section 4 discusses evaluation metrics, and Section 5 the results.", "labels": [], "entities": []}, {"text": "We conclude with future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "NIST assessors who created the 'ideal' written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors' summaries, and baseline summaries.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9349977374076843}]}, {"text": "In addition, two baseline summaries were created automatically as reference points.", "labels": [], "entities": []}, {"text": "The first baseline, lead baseline, took the first 50, 100, 200, and 400 words in the last document in the collection.", "labels": [], "entities": []}, {"text": "The second baseline, coverage baseline, took the first sentence in the first document, the first sentence in the second document and soon until it had a summary of 50, 100, 200, or 400 words.", "labels": [], "entities": [{"text": "coverage", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9451435208320618}]}, {"text": "NIST used the Summary Evaluation Environment (SEE) 2.0 developed by one of the authors (Lin 2001) to support its human evaluation process.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9636627435684204}]}, {"text": "Using SEE, the assessors evaluated the quality of the system's text (the peer text) as compared to an ideal (the model text).", "labels": [], "entities": [{"text": "SEE", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.6532407999038696}]}, {"text": "The two texts were broken into lists of units and displayed in separate windows.", "labels": [], "entities": []}, {"text": "In DUC-2001 the sentence was used as the smallest unit of evaluation.", "labels": [], "entities": [{"text": "DUC-2001", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.8997402787208557}]}, {"text": "SEE 2.0 provides interfaces for assessors to judge the quality of summaries in grammatically 3 , cohesion 4 , and coherence 5 at five different levels: all, most, some, hardly any, or none.", "labels": [], "entities": [{"text": "SEE 2.0", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7430402934551239}]}, {"text": "It also allow s assessors to step through each model unit, mark all system units sharing content with the current model unit, and specify that the marked system units 3 Does a summary follow the rule of English grammatical rules independent of its content?", "labels": [], "entities": []}, {"text": "Do sentences in a summary fit in with their surrounding sentences?", "labels": [], "entities": []}, {"text": "Is the content of a summary expressed and organized in an effective way?.", "labels": [], "entities": []}, {"text": "100 word summary with explicit time annotation.", "labels": [], "entities": []}, {"text": "<multi size=\"100\" docset=\"d45h\"> AP900625-0160 1 express all, most, some or hardly any of the content of the current model unit.", "labels": [], "entities": [{"text": "AP900625-0160", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.7298669815063477}]}, {"text": "One goal of DUC-2001 was to debug the evaluation procedures and identify stable metrics that could serve as common reference points.", "labels": [], "entities": [{"text": "DUC-2001", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.8450132608413696}]}, {"text": "NIST did not define any official performance metric in DUC-2001.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9851491451263428}, {"text": "DUC-2001", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9698693752288818}]}, {"text": "It released the raw evaluation results to DUC -2001 participants and encouraged them to propose metrics that would help progress the field.", "labels": [], "entities": [{"text": "DUC -2001", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.8945920070012411}]}], "tableCaptions": [{"text": " Table 1. Pseudo precision, unweighted retention, and weighted retention for all summary lengths: overall  average, 400, 200, 100, and 50 words.", "labels": [], "entities": [{"text": "Pseudo", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9906283020973206}, {"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.8917825818061829}, {"text": "weighted retention", "start_pos": 54, "end_pos": 72, "type": "METRIC", "confidence": 0.7708300650119781}]}, {"text": " Table 2. Averaged grammaticality, cohesion, and  coherence over all summary sizes.", "labels": [], "entities": []}]}