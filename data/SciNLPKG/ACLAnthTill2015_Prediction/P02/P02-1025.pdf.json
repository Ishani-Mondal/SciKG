{"title": [{"text": "A Study on Richer Syntactic Dependencies for Structured Language Modeling", "labels": [], "entities": [{"text": "Structured Language Modeling", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.6433887978394827}]}], "abstractContent": [{"text": "We study the impact of richer syntactic dependencies on the performance of the structured language model (SLM) along three dimensions: parsing accuracy (LP/LR), perplexity (PPL) and word-error-rate (WER, N-best re-scoring).", "labels": [], "entities": [{"text": "accuracy (LP/LR)", "start_pos": 143, "end_pos": 159, "type": "METRIC", "confidence": 0.9094091653823853}]}, {"text": "We show that our models achieve an improvement in LP/LR, PPL and/or WER over the reported baseline results using the SLM on the UPenn Treebank and Wall Street Journal (WSJ) corpora, respectively.", "labels": [], "entities": [{"text": "LP/LR", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.8939639131228129}, {"text": "WER", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9951268434524536}, {"text": "UPenn Treebank and Wall Street Journal (WSJ) corpora", "start_pos": 128, "end_pos": 180, "type": "DATASET", "confidence": 0.8893169701099396}]}, {"text": "Analysis of parsing performance shows correlation between the quality of the parser (as measured by pre-cision/recall) and the language model performance (PPL and WER).", "labels": [], "entities": [{"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.965272843837738}, {"text": "WER", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.9118734002113342}]}, {"text": "A remarkable fact is that the enriched SLM outperforms the baseline 3-gram model in terms of WER by 10% when used in isolation as a second pass (N-best re-scoring) language model.", "labels": [], "entities": [{"text": "WER", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9928800463676453}]}], "introductionContent": [{"text": "The structured language model uses hidden parse trees to assign conditional word-level language model probabilities.", "labels": [], "entities": []}, {"text": "As explained in), Section 4.4.1, if the final best parse is used to be the only parse, the reduction in PPL -relative to a 3-gram baseline-using the SLM's headword parametrization for word prediction is about 40%.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 184, "end_pos": 199, "type": "TASK", "confidence": 0.7554225623607635}]}, {"text": "The key to achieving this reduction is a good guess of the final best parse fora given sentence as it is being traversed left-to-right, which is much harder than finding the final best parse for the entire sentence, as it is sought by a regular statistical parser.", "labels": [], "entities": []}, {"text": "Nevertheless, it is expected that techniques developed in the statistical parsing community that aim at recovering the best parse for an entire sentence, i.e. as judged by a human annotator, should also be productive in enhancing the performance of a language model that uses syntactic structure.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7373881042003632}]}, {"text": "The statistical parsing community has used various ways of enriching the dependency structure underlying the parametrization of the probabilistic model used for scoring a given parse tree).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6614537239074707}]}, {"text": "Recently, such models)) have been shown to outperform the SLM in terms of both PPL and WER on the UPenn Treebank and WSJ corpora, respectively.", "labels": [], "entities": [{"text": "WER", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9861528873443604}, {"text": "UPenn Treebank and WSJ corpora", "start_pos": 98, "end_pos": 128, "type": "DATASET", "confidence": 0.8337685704231262}]}, {"text": "In (), a simple way of enriching the probabilistic dependencies in the CONSTRUC-TOR component of the SLM also showed better PPL and WER performance; the simple modification to the training procedure brought the WER performance of the SLM to the same level with the best as reported in.", "labels": [], "entities": [{"text": "WER", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9747887849807739}]}, {"text": "In this paper, we present three simple ways of enriching the syntactic dependency structure in the SLM, extending the work in).", "labels": [], "entities": [{"text": "SLM", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.904359757900238}]}, {"text": "The results show that an improved parser (as measured by LP/LR) is indeed helpful in reducing the PPL and WER.", "labels": [], "entities": [{"text": "LP/LR", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9040815432866415}, {"text": "WER", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9812719225883484}]}, {"text": "Another remarkable fact is that for the first time a language model exploiting elemen-tary syntactic dependencies obviates the need for interpolation with a 3-gram model in N-best rescoring.", "labels": [], "entities": []}], "datasetContent": [{"text": "With the three enrichment schemes described in Section 3 and their combinations, we evaluated the PPL performance of the resulting seven models on the UPenn Treebank and the WER performance on the WSJ setup, respectively.", "labels": [], "entities": [{"text": "UPenn Treebank", "start_pos": 151, "end_pos": 165, "type": "DATASET", "confidence": 0.9855169951915741}, {"text": "WER", "start_pos": 174, "end_pos": 177, "type": "METRIC", "confidence": 0.887731671333313}, {"text": "WSJ setup", "start_pos": 197, "end_pos": 206, "type": "DATASET", "confidence": 0.941556453704834}]}, {"text": "In order to seethe correspondence between parsing accuracy and PPL/WER performance, we also evaluated the labeled precision and recall statistics (LP/LR, the standard parsing accuracy measures) on the UPenn Treebank corpus.", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9743545651435852}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.6740966439247131}, {"text": "WER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.6004941463470459}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9013050198554993}, {"text": "recall statistics (LP/LR", "start_pos": 128, "end_pos": 152, "type": "METRIC", "confidence": 0.8017183939615885}, {"text": "UPenn Treebank corpus", "start_pos": 201, "end_pos": 222, "type": "DATASET", "confidence": 0.9917789101600647}]}, {"text": "For every model component in our experiments, deleted-interpolation was used for smoothing.", "labels": [], "entities": []}, {"text": "The interpolation weights were estimated from separate held-out data.", "labels": [], "entities": []}, {"text": "For example, in the UPenn Treebank setup, we used section 00-20 as training data, section 21-22 as held-out data, and section 23-24 as test data.", "labels": [], "entities": [{"text": "UPenn Treebank setup", "start_pos": 20, "end_pos": 40, "type": "DATASET", "confidence": 0.9726846218109131}]}], "tableCaptions": [{"text": " Table 1: Vocabulary size comparison of the models", "labels": [], "entities": [{"text": "Vocabulary size comparison", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7863599260648092}]}, {"text": " Table 2. The  SLM is interpolated with a 3-gram model as shown  in the equation:", "labels": [], "entities": []}, {"text": " Table 2: SLM PPL results", "labels": [], "entities": [{"text": "SLM PPL", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.8941429853439331}]}, {"text": " Table 3: Labeled precision/recall(%) results", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.915113627910614}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9625909924507141}]}, {"text": " Table 4: N-best re-scoring WER(%) results", "labels": [], "entities": [{"text": "N-best re-scoring", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7832612693309784}, {"text": "WER", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.5489305853843689}]}, {"text": " Table 5: PPL for training data", "labels": [], "entities": []}]}