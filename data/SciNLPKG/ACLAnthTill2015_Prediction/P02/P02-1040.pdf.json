{"title": [{"text": "BLEU: a Method for Automatic Evaluation of Machine Translation", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9759515523910522}, {"text": "Machine Translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.6862780004739761}]}], "abstractContent": [{"text": "Human evaluations of machine translation are extensive but expensive.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7974711358547211}]}, {"text": "Human evaluations can take months to finish and involve human labor that cannot be reused.", "labels": [], "entities": []}, {"text": "We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation , and that has little marginal cost per run.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.7836281955242157}]}, {"text": "We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The BLEU metric ranges from 0 to 1.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.971636027097702}]}, {"text": "Few translations will attain a score of 1 unless they are identical to a reference translation.", "labels": [], "entities": []}, {"text": "For this reason, even a human translator will not necessarily score 1.", "labels": [], "entities": []}, {"text": "It is important to note that the more reference translations per sentence there are, the higher the score is.", "labels": [], "entities": []}, {"text": "Thus, one must be cautious making even \"rough\" comparisons on evaluations with different numbers of reference translations: on a test corpus of about 500 sentences (40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references.", "labels": [], "entities": []}, {"text": "shows the BLEU scores of the 5 systems against two references on this test corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991980195045471}]}, {"text": "The MT systems S2 and S3 are very close in this metric.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.929468035697937}]}, {"text": "Hence, several questions arise:  \u2022 Is the difference in BLEU metric reliable?", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 56, "end_pos": 67, "type": "METRIC", "confidence": 0.9387608766555786}]}, {"text": "\u2022 What is the variance of the BLEU score?", "labels": [], "entities": [{"text": "variance", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.983819842338562}, {"text": "BLEU score", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9712303280830383}]}, {"text": "\u2022 If we were to pick another random set of 500 sentences, would we still judge S3 to be better than S2?", "labels": [], "entities": []}, {"text": "To answer these questions, we divided the test corpus into 20 blocks of 25 sentences each, and computed the BLEU metric on these blocks individually.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.9752547740936279}]}, {"text": "We thus have 20 samples of the BLEU metric for each system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9969555139541626}]}, {"text": "We computed the means, variances, and paired t-statistics which are displayed in.", "labels": [], "entities": []}, {"text": "The t-statistic compares each system with its left neighbor in the table.", "labels": [], "entities": []}, {"text": "For example, t = 6 for the pair S1 and S2.", "labels": [], "entities": []}, {"text": "Note that the numbers in are the BLEU metric on an aggregate of 500 sentences, but the means in are averages of the BLEU metric on aggregates of 25 sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9155675768852234}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.8254169225692749}]}, {"text": "As expected, these two sets of results are close for each system and differ only by small finite block size effects.", "labels": [], "entities": []}, {"text": "Since a paired t-statistic of 1.7 or above is 95% significant, the differences between the systems' scores are statistically very significant.", "labels": [], "entities": []}, {"text": "The reported variance on 25-sentence blocks serves as an upper bound to the variance of sizeable test sets like the 500 sentence corpus.", "labels": [], "entities": []}, {"text": "How many reference translations do we need?", "labels": [], "entities": []}, {"text": "We simulated a single-reference test corpus by randomly selecting one of the 4 reference translations as the single reference for each of the 40 stories.", "labels": [], "entities": []}, {"text": "In this way, we ensured a degree of stylistic variation.", "labels": [], "entities": []}, {"text": "The systems maintain the same rank order as with multiple references.", "labels": [], "entities": []}, {"text": "This outcome suggests that we may use a big test corpus with a single reference translation, provided that the translations are not all from the same translator.", "labels": [], "entities": []}, {"text": "We had two groups of human judges.", "labels": [], "entities": []}, {"text": "The first group, called the monolingual group, consisted of 10 native speakers of English.", "labels": [], "entities": []}, {"text": "The second group, called the bilingual group, consisted of 10 native speakers of Chinese who had lived in the United States for the past several years.", "labels": [], "entities": []}, {"text": "None of the human judges was a professional translator.", "labels": [], "entities": []}, {"text": "The humans judged our 5 standard systems on a Chinese sentence subset extracted at random from our 500 sentence test corpus.", "labels": [], "entities": [{"text": "500 sentence test corpus", "start_pos": 99, "end_pos": 123, "type": "DATASET", "confidence": 0.7095488607883453}]}, {"text": "We paired each source sentence with each of its 5 translations, fora total of 250 pairs of Chinese source and English translations.", "labels": [], "entities": []}, {"text": "We prepared a web page with these translation pairs randomly ordered to disperse the five translations of each source sentence.", "labels": [], "entities": []}, {"text": "All judges used this same webpage and saw the sentence pairs in the same order.", "labels": [], "entities": []}, {"text": "They rated each translation from 1 (very bad) to 5 (very good).", "labels": [], "entities": []}, {"text": "The monolingual group made their judgments based only on the translations' readability and fluency.", "labels": [], "entities": []}, {"text": "As must be expected, some judges were more liberal than others.", "labels": [], "entities": []}, {"text": "And some sentences were easier to translate than others.", "labels": [], "entities": []}, {"text": "To account for the intrinsic difference between judges and the sentences, we compared each judge's rating fora sentence across systems.", "labels": [], "entities": []}, {"text": "We performed four pairwise t-test comparisons between adjacent systems as ordered by their aggregate average score.", "labels": [], "entities": []}, {"text": "shows the mean difference between the scores of two consecutive systems and the 95% confidence interval about the mean.", "labels": [], "entities": [{"text": "mean difference", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9695064425468445}]}, {"text": "We see that S2 is quite a bit better than S1 (by a mean opinion score difference of 0.326 on the 5-point scale), while S3 is judged a little better (by 0.114).", "labels": [], "entities": [{"text": "mean opinion score difference", "start_pos": 51, "end_pos": 80, "type": "METRIC", "confidence": 0.6429613456130028}]}, {"text": "Both differences are significant at the 95% level.", "labels": [], "entities": []}, {"text": "The human H1 is much better than the best system, though a bit worse than human H2.", "labels": [], "entities": []}, {"text": "This is not surprising given that H1 is not a native speaker of either Chinese or English, whereas H2 is a native English speaker.", "labels": [], "entities": []}, {"text": "Again, the difference between the human translators is significant beyond the 95% level.", "labels": [], "entities": []}, {"text": "shows the same results for the bilingual group.", "labels": [], "entities": []}, {"text": "They also find that S3 is slightly better than S2 (at 95% confidence) though they judge that the human translations are much closer (indistinguishable at 95% confidence), suggesting that the bilinguals tended to focus more on adequacy than on fluency.", "labels": [], "entities": []}, {"text": "shows a linear regression of the monolingual group scores as a function of the BLEU score over two reference translations for the 5 systems.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9726231694221497}]}, {"text": "The high correlation coefficient of 0.99 indicates that BLEU tracks human judgment well.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 9, "end_pos": 32, "type": "METRIC", "confidence": 0.9705459773540497}, {"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9962601661682129}]}, {"text": "Particularly interesting is how well BLEU distinguishes between S2 and S3 which are quite close.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.995746910572052}]}, {"text": "shows the comparable regression results for the bilingual group.", "labels": [], "entities": []}, {"text": "The correlation coefficient is 0.96.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9986456036567688}]}], "tableCaptions": [{"text": " Table 1: BLEU on 500 sentences", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9978887438774109}]}, {"text": " Table 2: Paired t-statistics on 20 blocks", "labels": [], "entities": [{"text": "Paired t-statistics", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7672572433948517}]}]}