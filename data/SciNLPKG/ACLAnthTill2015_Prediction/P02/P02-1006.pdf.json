{"title": [{"text": "Learning Surface Text Patterns fora Question Answering System", "labels": [], "entities": [{"text": "Learning Surface Text Patterns", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5188355594873428}, {"text": "Question Answering", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7537831962108612}]}], "abstractContent": [{"text": "In this paper we explore the power of surface text patterns for open-domain question answering systems.", "labels": [], "entities": [{"text": "open-domain question answering", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.6503095030784607}]}, {"text": "In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically.", "labels": [], "entities": []}, {"text": "A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista.", "labels": [], "entities": []}, {"text": "Patterns are then automatically extracted from the returned documents and standardized.", "labels": [], "entities": []}, {"text": "We calculate the precision of each pattern, and the average precision for each question type.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9995276927947998}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9600436687469482}]}, {"text": "These patterns are then applied to find answers to new questions.", "labels": [], "entities": []}, {"text": "Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web.", "labels": [], "entities": [{"text": "TREC-10 question set", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.9316165447235107}, {"text": "TREC-10 corpus", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.9327848851680756}]}], "introductionContent": [{"text": "Most of the recent open domain questionanswering systems use external knowledge and tools for answer pinpointing.", "labels": [], "entities": [{"text": "answer pinpointing", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.8645412921905518}]}, {"text": "These may include named entity taggers, WordNet, parsers, hand-tagged corpora, and ontology lists (Srihari and Li, 00; Harabagiu et al., 01; Hovy et al., 01; Prager et al., 01).", "labels": [], "entities": []}, {"text": "However, at the recent TREC-10 QA evaluation (Voorhees, 01), the winning system used just one resource: a fairly extensive list of surface patterns (Soubbotin and Soubbotin, 01).", "labels": [], "entities": [{"text": "TREC-10 QA", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.526919960975647}]}, {"text": "The apparent power of such patterns surprised many.", "labels": [], "entities": []}, {"text": "We therefore decided to investigate their potential by acquiring patterns automatically and to measure their accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9970695972442627}]}, {"text": "It has been noted in several QA systems that certain types of answer are expressed using characteristic phrases (Lee et al., 01;.", "labels": [], "entities": []}, {"text": "For example, for BIRTHDATEs (with questions like \"When was X born?\"), typical answers are \"Mozart was born in 1756.\"", "labels": [], "entities": [{"text": "BIRTHDATEs", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9139020442962646}]}, {"text": "\"Gandhi (1869-1948)\u2026\" These examples suggest that phrases like \"<NAME> was born in <BIRTHDATE>\" \"<NAME> (<BIRTHDATE>-\" when formulated as regular expressions, can be used to locate the correct answer.", "labels": [], "entities": [{"text": "BIRTHDATE", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.933412492275238}]}, {"text": "In this paper we present an approach for automatically learning such regular expressions (along with determining their precision) from the web, forgiven types of questions.", "labels": [], "entities": [{"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9967113733291626}]}, {"text": "Our method uses the machine learning technique of bootstrapping to build a large tagged corpus starting with only a few examples of QA pairs.", "labels": [], "entities": []}, {"text": "Similar techniques have been investigated extensively in the field of information extraction (Riloff, 96).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.907077968120575}]}, {"text": "These techniques are greatly aided by the fact that there is no need to hand-tag a corpus, while the abundance of data on the web makes it easier to determine reliable statistical estimates.", "labels": [], "entities": []}, {"text": "Our system assumes each sentence to be a simple sequence of words and searches for repeated word orderings as evidence for useful answer phrases.", "labels": [], "entities": []}, {"text": "We use suffix trees for extracting substrings of optimal length.", "labels": [], "entities": []}, {"text": "We borrow the idea of suffix trees from computational biology where it is primarily used for detecting DNA sequences.", "labels": [], "entities": []}, {"text": "Suffix trees can be processed in time linear on the size of the corpus and, more importantly, they do not restrict the length of substrings.", "labels": [], "entities": []}, {"text": "We then test the patterns learned by our system on new unseen questions from the TREC-10 set and evaluate their results to determine the precision of the patterns.", "labels": [], "entities": [{"text": "TREC-10 set", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.878052145242691}, {"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9988206028938293}]}], "datasetContent": [{"text": "From our Webclopedia QA Typology ( For each question type, we extracted the corresponding questions from the TREC-10 set.", "labels": [], "entities": [{"text": "Webclopedia QA Typology", "start_pos": 9, "end_pos": 32, "type": "DATASET", "confidence": 0.8665046493212382}, {"text": "TREC-10 set", "start_pos": 109, "end_pos": 120, "type": "DATASET", "confidence": 0.8157224059104919}]}, {"text": "These questions were run through the testing phase of the algorithm.", "labels": [], "entities": []}, {"text": "Two sets of experiments were performed.", "labels": [], "entities": []}, {"text": "In the first case, the TREC corpus was used as the input source and IR was performed by the IR component of our QA system).", "labels": [], "entities": [{"text": "TREC corpus", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.7971005141735077}, {"text": "IR", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.6838029623031616}]}, {"text": "In the second case, the web was the input source and the IR was performed by the AltaVista search engine.", "labels": [], "entities": [{"text": "IR", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.777765691280365}]}, {"text": "Results of the experiments, measured by Mean Reciprocal Rank (MRR) score (Voorhees, 01), are: The results indicate that the system performs better on the Web data than on the TREC corpus.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR) score", "start_pos": 40, "end_pos": 72, "type": "METRIC", "confidence": 0.966556864125388}, {"text": "TREC corpus", "start_pos": 175, "end_pos": 186, "type": "DATASET", "confidence": 0.9103136658668518}]}, {"text": "The abundance of data on the web makes it easier for the system to locate answers with high precision scores (the system finds many examples of correct answers among the top 20 when using the Web as the input source).", "labels": [], "entities": [{"text": "precision scores", "start_pos": 92, "end_pos": 108, "type": "METRIC", "confidence": 0.9714246988296509}]}, {"text": "A similar result for QA was obtained by.", "labels": [], "entities": []}, {"text": "The TREC corpus does not have enough candidate answers with high precision score and has to settle for answers extracted from sentences matched by low precision patterns.", "labels": [], "entities": [{"text": "TREC corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7528156340122223}, {"text": "precision score", "start_pos": 65, "end_pos": 80, "type": "METRIC", "confidence": 0.9813347160816193}]}, {"text": "The WHY-FAMOUS question type is an exception and maybe due to the fact that the system was tested on a small number of questions.", "labels": [], "entities": [{"text": "WHY-FAMOUS question type", "start_pos": 4, "end_pos": 28, "type": "METRIC", "confidence": 0.7962825894355774}]}], "tableCaptions": []}