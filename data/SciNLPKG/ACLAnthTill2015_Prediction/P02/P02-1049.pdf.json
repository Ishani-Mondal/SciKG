{"title": [{"text": "What's the Trouble: Automatically Identifying Problematic Dialogues in DARPA Communicator Dialogue Systems", "labels": [], "entities": [{"text": "Automatically Identifying Problematic Dialogues in DARPA Communicator Dialogue Systems", "start_pos": 20, "end_pos": 106, "type": "TASK", "confidence": 0.6461844378047519}]}], "abstractContent": [{"text": "Spoken dialogue systems promise efficient and natural access to information services from any phone.", "labels": [], "entities": []}, {"text": "Recently, spoken dialogue systems for widely used applications such as email, travel information , and customer care have moved from research labs into commercial use.", "labels": [], "entities": []}, {"text": "These applications can receive millions of calls a month.", "labels": [], "entities": []}, {"text": "This huge amount of spoken dialogue data has led to a need for fully automatic methods for selecting a subset of caller dialogues that are most likely to be useful for further system improvement , to be stored, transcribed and further analyzed.", "labels": [], "entities": []}, {"text": "This paper reports results on automatically training a Problematic Dialogue Identifier to classify problematic human-computer dialogues using a corpus of 1242 DARPA Communicator dialogues in the travel planning domain.", "labels": [], "entities": []}, {"text": "We show that using fully automatic features we can identify classes of problematic dialogues with accuracies from 67% to 89%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9744016528129578}]}], "introductionContent": [{"text": "Spoken dialogue systems promise efficient and natural access to a large variety of information services from any phone.", "labels": [], "entities": []}, {"text": "Deployed systems and research prototypes exist for applications such as personal email and calendars, travel and restaurant information, personal banking, and customer care.", "labels": [], "entities": []}, {"text": "Within the last few years, several spoken dialogue systems for widely used applications have moved from research labs into commercial use ().", "labels": [], "entities": []}, {"text": "These applications can receive millions of calls a month.", "labels": [], "entities": []}, {"text": "There is a strong requirement for automatic methods to identify and extract dialogues that provide training data for further system development.", "labels": [], "entities": []}, {"text": "As a spoken dialogue system is developed, it is first tested as a prototype, then fielded in a limited setting, possibly running with human supervision (, and finally deployed.", "labels": [], "entities": []}, {"text": "At each stage from research prototype to deployed commercial application, the system is constantly undergoing further development.", "labels": [], "entities": []}, {"text": "When a system is prototyped in house or first tested in the field, human subjects are often paid to use the system and give detailed feedback on task completion and user satisfaction ().", "labels": [], "entities": []}, {"text": "Even when a system is deployed, it often keeps evolving, either because customers want to do different things with it, or because new tasks arise out of developments in the underlying application.", "labels": [], "entities": []}, {"text": "However, real customers of a deployed system may not be willing to give detailed feedback.", "labels": [], "entities": []}, {"text": "Thus, the widespread use of these systems has created a data management and analysis problem.", "labels": [], "entities": [{"text": "data management and analysis", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.6822468265891075}]}, {"text": "System designers need to constantly track system performance, identify problems, and fix them.", "labels": [], "entities": []}, {"text": "System modules such as automatic speech recognition (ASR), natural language understanding (NLU) and dialogue management may rely on training data collected at each phase.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.8148617247740427}, {"text": "natural language understanding (NLU)", "start_pos": 59, "end_pos": 95, "type": "TASK", "confidence": 0.8086679875850677}, {"text": "dialogue management", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.8887211680412292}]}, {"text": "ASR performance assessment relies on full transcription of the utterances.", "labels": [], "entities": [{"text": "ASR performance assessment", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8765800992647806}]}, {"text": "Dialogue manager assessment relies on a human interface expert reading a full transcription of the dialogue or listening to a recording of it, possibly while examining the logfiles to understand the interaction between all the components.", "labels": [], "entities": [{"text": "Dialogue manager assessment", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8366494178771973}]}, {"text": "However, because of the high volume of calls, spoken dialogue service providers typically can only afford to store, transcribe, and analyze a small fraction of the dialogues.", "labels": [], "entities": []}, {"text": "Therefore, there is a great need for methods for both automatically evaluating system performance, and for extracting subsets of dialogues that provide good training data for system improvement.", "labels": [], "entities": []}, {"text": "This is a difficult problem because by the time a system is deployed, typically over 90% of the dialogue interactions result in completed tasks and satisfied users.", "labels": [], "entities": []}, {"text": "Dialogues such as these do not provide very useful training data for further system development because there is little to be learned when the dialogue goes well.", "labels": [], "entities": []}, {"text": "Previous research on spoken dialogue evaluation proposed the application of automatic classifiers for identifying and predicting of problematic dialogues) for the purpose of automatically adapting the dialogue manager.", "labels": [], "entities": [{"text": "spoken dialogue evaluation", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.7283212145169576}, {"text": "identifying and predicting of problematic dialogues", "start_pos": 102, "end_pos": 153, "type": "TASK", "confidence": 0.688006674249967}]}, {"text": "Here we apply similar methods to the dialogue corpus data-mining problem described above.", "labels": [], "entities": []}, {"text": "We report results on automatically training a Problematic Dialogue Identifier (PDI) to classify problematic human-computer dialogues using the October-2001 DARPA Communicator corpus.", "labels": [], "entities": [{"text": "DARPA Communicator corpus", "start_pos": 156, "end_pos": 181, "type": "DATASET", "confidence": 0.8963988025983175}]}, {"text": "Section 2 describes our approach and the dialogue corpus.", "labels": [], "entities": []}, {"text": "Section 3 describes how we use the DATE dialogue act tagging scheme to define input features for the PDI.", "labels": [], "entities": [{"text": "DATE dialogue act tagging", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.6936130672693253}]}, {"text": "Section 4 presents a method and results for automatically predicting task completion.", "labels": [], "entities": [{"text": "predicting task completion", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.8557361960411072}]}, {"text": "Section 5 presents results for predicting problematic dialogues based on the user's satisfaction.", "labels": [], "entities": [{"text": "predicting problematic dialogues", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.9087114334106445}]}, {"text": "We show that we identify task failure dialogues with 85% accuracy (baseline 59%) and dialogues with low user satisfaction with up to 89% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9984371066093445}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9946900606155396}]}, {"text": "We discuss the application of the PDI to data mining in Section 6.", "labels": [], "entities": [{"text": "data mining", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.857304036617279}, {"text": "Section 6", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.9316050112247467}]}, {"text": "Finally, we summarize the paper and discuss future work.", "labels": [], "entities": [{"text": "summarize", "start_pos": 12, "end_pos": 21, "type": "TASK", "confidence": 0.9494222402572632}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 gives the results for Task Completion pre- diction accuracy using the various types of features. Baseline  Auto  ALF +  ALF +  Logfile  GC  GC+ DATE  TC  59%  59%  79%  85%  BTC  86%  86%  86%  92%", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9267255067825317}]}, {"text": " Table 2: Correlation results using logfile fea- tures (LF), adding unigram proportions and bigram  counts, for trees tested on either hand-labelled (HL)  or automatically derived Task Completion (TC) and  Binary Task Completion (BTC)", "labels": [], "entities": []}]}