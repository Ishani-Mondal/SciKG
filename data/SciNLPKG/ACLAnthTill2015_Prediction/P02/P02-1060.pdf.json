{"title": [{"text": "Named Entity Recognition using an HMM-based Chunk Tagger", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.659721573193868}]}], "abstractContent": [{"text": "This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities.", "labels": [], "entities": [{"text": "named entity (NE) recognition (NER)", "start_pos": 92, "end_pos": 127, "type": "TASK", "confidence": 0.7788870731989542}]}, {"text": "Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature.", "labels": [], "entities": []}, {"text": "In this way, the NER problem can be resolved effectively.", "labels": [], "entities": [{"text": "NER", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9535588622093201}]}, {"text": "Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.9481119513511658}, {"text": "MUC-7 English NE tasks", "start_pos": 38, "end_pos": 60, "type": "DATASET", "confidence": 0.8109607100486755}, {"text": "F-measures", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.999723494052887}]}, {"text": "It shows that the performance is significantly better than reported by any other machine-learning system.", "labels": [], "entities": []}, {"text": "Moreover, the performance is even consistently better than those based on handcrafted rules.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named Entity (NE) Recognition (NER) is to classify every word in a document into some predefined categories and \"none-of-the-above\".", "labels": [], "entities": [{"text": "Named Entity (NE) Recognition (NER)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7046129604180654}]}, {"text": "In the taxonomy of computational linguistics tasks, it falls under the domain of \"information extraction\", which extracts specific kinds of information from documents as opposed to the more general task of \"document management\" which seeks to extract all of the information found in a document.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7407563030719757}, {"text": "document management\"", "start_pos": 207, "end_pos": 227, "type": "TASK", "confidence": 0.7959391474723816}]}, {"text": "Since entity names form the main content of a document, NER is a very important step toward more intelligent information extraction and management.", "labels": [], "entities": [{"text": "NER", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.8867313265800476}, {"text": "information extraction", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.7605986297130585}]}, {"text": "The atomic elements of information extraction --indeed, of language as a whole --could be considered as the \"who\", \"where\" and \"how much\" in a sentence.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7350616902112961}]}, {"text": "NER performs what is known as surface parsing, delimiting sequences of tokens that answer these important questions.", "labels": [], "entities": [{"text": "surface parsing", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7245176136493683}]}, {"text": "NER can also be used as the first step in a chain of processors: a next level of processing could relate two or more NEs, or perhaps even give semantics to that relationship using a verb.", "labels": [], "entities": []}, {"text": "In this way, further processing could discover the \"what\" and \"how\" of a sentence or body of text.", "labels": [], "entities": []}, {"text": "While NER is relatively simple and it is fairly easy to build a system with reasonable performance, there are still a large number of ambiguous cases that make it difficult to attain human performance.", "labels": [], "entities": [{"text": "NER", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.97869873046875}]}, {"text": "There has been a considerable amount of work on NER problem, which aims to address many of these ambiguity, robustness and portability issues.", "labels": [], "entities": [{"text": "NER", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9395073056221008}]}, {"text": "During last decade, NER has drawn more and more attention from the NE tasks [Chinchor98a] in MUCs [MUC6], where person names, location names, organization names, dates, times, percentages and money amounts are to be delimited in text using SGML mark-ups.", "labels": [], "entities": [{"text": "NER", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.8536279797554016}, {"text": "NE tasks [Chinchor98a] in MUCs [MUC6]", "start_pos": 67, "end_pos": 104, "type": "DATASET", "confidence": 0.8241620719432831}]}, {"text": "Previous approaches have typically used manually constructed finite state patterns, which attempt to match against a sequence of words in much the same way as a general regular expression matcher.", "labels": [], "entities": []}, {"text": "These systems are mainly rule-based.", "labels": [], "entities": []}, {"text": "However, rule-based approaches lack the ability of coping with the problems of robustness and portability.", "labels": [], "entities": []}, {"text": "Each new source of text requires significant tweaking of rules to maintain optimal performance and the maintenance costs could be quite steep.", "labels": [], "entities": []}, {"text": "The current trend in NER is to use the machine-learning approach, which is more attractive in that it is trainable and adaptable and the maintenance of a machine-learning system is much cheaper than that of a rule-based one.", "labels": [], "entities": [{"text": "NER", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9435084462165833}]}, {"text": "The representative machine-learning approaches used in NER are HMM has been applied to the problem.", "labels": [], "entities": [{"text": "NER", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9721958041191101}]}, {"text": "Among these approaches, the evaluation performance of HMM is higher than those of others.", "labels": [], "entities": [{"text": "HMM", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.8933271169662476}]}, {"text": "The main reason maybe due to its better ability of capturing the locality of phenomena, which indicates names in text.", "labels": [], "entities": []}, {"text": "Moreover, HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm used in decoding the NE-class state sequence.", "labels": [], "entities": [{"text": "NE recognition", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.9746289253234863}]}, {"text": "However, the performance of a machine-learning system is always poorer than that of a rule-based one by about 2% [Chinchor98b].", "labels": [], "entities": [{"text": "Chinchor98b", "start_pos": 114, "end_pos": 125, "type": "DATASET", "confidence": 0.9582374691963196}]}, {"text": "This maybe because current machine-learning approaches capture important evidence behind NER problem much less effectively than human experts who handcraft the rules, although machine-learning approaches always provide important statistical information that is not available to human experts.", "labels": [], "entities": [{"text": "NER problem", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.9416904449462891}]}, {"text": "As defined in, there are two kinds of evidences that can be used in NER to solve the ambiguity, robustness and portability problems described above.", "labels": [], "entities": [{"text": "NER", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.8826532959938049}]}, {"text": "The first is the internal evidence found within the word and/or word string itself while the second is the external evidence gathered from its context.", "labels": [], "entities": []}, {"text": "In order to effectively apply and integrate internal and external evidences, we present a NER system using a HMM.", "labels": [], "entities": [{"text": "NER", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9166175723075867}]}, {"text": "The approach behind our NER system is based on the HMM-based chunk tagger in text chunking, which was ranked the best individual system [Zhou+00b] in CoNLL'2000.", "labels": [], "entities": [{"text": "HMM-based chunk tagger", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.5765914420286814}, {"text": "text chunking", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.7142686396837234}, {"text": "Zhou+00b] in CoNLL'2000", "start_pos": 137, "end_pos": 160, "type": "DATASET", "confidence": 0.7352201243241628}]}, {"text": "Here, a NE is regarded as a chunk, named \"NE-Chunk\".", "labels": [], "entities": []}, {"text": "To date, our system has been successfully trained and applied in English NER.", "labels": [], "entities": [{"text": "English NER", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.6127948760986328}]}, {"text": "To our knowledge, our system outperforms any published machine-learning systems.", "labels": [], "entities": []}, {"text": "Moreover, our system even outperforms any published rule-based systems.", "labels": [], "entities": []}, {"text": "The layout of this paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a description of the HMM and its application in NER: HMM-based chunk tagger.", "labels": [], "entities": [{"text": "HMM-based chunk tagger", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.6296879649162292}]}, {"text": "Section 3 explains the word feature used to capture both the internal and external evidences.", "labels": [], "entities": []}, {"text": "Section 4 describes the back-off schemes used to tackle the sparseness problem.", "labels": [], "entities": []}, {"text": "Section 5 gives the experimental results of our system.", "labels": [], "entities": []}, {"text": "Section 6 contains our remarks and possible extensions of the proposed work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we will report the experimental results of our system for English NER on MUC-6 and MUC-7 NE shared tasks, as shown in, and then for the impact of training data size on performance using MUC-7 training data.", "labels": [], "entities": [{"text": "MUC-7 NE shared tasks", "start_pos": 100, "end_pos": 121, "type": "DATASET", "confidence": 0.7919238060712814}, {"text": "MUC-7 training data", "start_pos": 203, "end_pos": 222, "type": "DATASET", "confidence": 0.8637686967849731}]}, {"text": "For each experiment, we have the MUC dry-run data as the held-out development data and the MUC formal test data as the held-out test data.", "labels": [], "entities": [{"text": "MUC dry-run data", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.8730541070302328}, {"text": "MUC formal test data", "start_pos": 91, "end_pos": 111, "type": "DATASET", "confidence": 0.8670061528682709}]}, {"text": "For both MUC-6 and MUC-7 NE tasks, shows the performance of our system using MUC evaluation while gives the comparisons of our system with others.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.8298503756523132}, {"text": "MUC-7 NE tasks", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.7125834226608276}]}, {"text": "Here, the precision (P) measures the number of correct NEs in the answer file over the total number of NEs in the answer file and the recall (R) measures the number of correct NEs in the answer file over the total number of NEs in the key file while F-measure is the weighted harmonic mean of precision and recall: for MUC-7 NE task.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9442007839679718}, {"text": "recall (R)", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.9713530838489532}, {"text": "F-measure", "start_pos": 250, "end_pos": 259, "type": "METRIC", "confidence": 0.9901660084724426}, {"text": "precision", "start_pos": 293, "end_pos": 302, "type": "METRIC", "confidence": 0.999190628528595}, {"text": "recall", "start_pos": 307, "end_pos": 313, "type": "METRIC", "confidence": 0.9988784193992615}, {"text": "MUC-7 NE task", "start_pos": 319, "end_pos": 332, "type": "TASK", "confidence": 0.6351486245791117}]}, {"text": "It shows that 200KB of training data would have given the performance of 90% while reducing to 100KB would have had a significant decrease in the performance.", "labels": [], "entities": []}, {"text": "It also shows that our system still has some room for performance improvement.", "labels": [], "entities": []}, {"text": "This maybe because of the complex word feature and the corresponding sparseness problem existing in our system.", "labels": [], "entities": []}, {"text": "Another important question is about the effect of different sub-features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Constraints between 1", "labels": [], "entities": []}, {"text": " Table 6: Statistics of Data from MUC-6  and MUC-7 NE Tasks  F  P  R  MUC-6  96.6  96.3  96.9  MUC-7  94.1  93.7  94.5  Table 7: Performance of our System on MUC-6  and MUC-7 NE Tasks  Composition  F  P  R f  f =  94.1  93.7  94.5", "labels": [], "entities": [{"text": "MUC-6  and MUC-7 NE Tasks  F  P  R  MUC-6  96.6  96.3  96.9  MUC-7  94.1  93.7  94.5", "start_pos": 34, "end_pos": 118, "type": "DATASET", "confidence": 0.8550714515149593}]}, {"text": " Table 8: Impact of Different Sub-Features  With any learning technique, one important  question is how much training data is required to  achieve acceptable performance. More generally  how does the performance vary as the training data  size changes? The result is shown in", "labels": [], "entities": []}]}