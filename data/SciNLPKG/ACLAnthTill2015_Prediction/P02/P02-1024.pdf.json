{"title": [{"text": "Exploring Asymmetric Clustering for Statistical Language Modeling", "labels": [], "entities": [{"text": "Statistical Language Modeling", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.8220785657564799}]}], "abstractContent": [{"text": "The n-gram model is a stochastic model, which predicts the next word (predicted word) given the previous words (conditional words) in a word sequence.", "labels": [], "entities": []}, {"text": "The cluster n-gram model is a variant of the n-gram model in which similar words are classified in the same cluster.", "labels": [], "entities": []}, {"text": "It has been demonstrated that using different clusters for predicted and conditional words leads to cluster models that are superior to classical cluster models which use the same clusters for both words.", "labels": [], "entities": []}, {"text": "This is the basis of the asymmetric cluster model (ACM) discussed in our study.", "labels": [], "entities": []}, {"text": "In this paper, we first present a formal definition of the ACM.", "labels": [], "entities": []}, {"text": "We then describe in detail the methodology of constructing the ACM.", "labels": [], "entities": []}, {"text": "The effectiveness of the ACM is evaluated on a realistic application, namely Japanese Kana-Kanji conversion.", "labels": [], "entities": [{"text": "Kana-Kanji conversion", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.7311610579490662}]}, {"text": "Experimental results show substantial improvements of the ACM in comparison with classical cluster models and word n-gram models at the same model size.", "labels": [], "entities": []}, {"text": "Our analysis shows that the high-performance of the ACM lies in the asymmetry of the model.", "labels": [], "entities": []}], "introductionContent": [{"text": "The n-gram model has been widely applied in many applications such as speech recognition, machine translation, and Asian language text input.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.8050760328769684}, {"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7990134060382843}, {"text": "Asian language text input", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.706342339515686}]}, {"text": "It is a stochastic model, which predicts the next word given the previous n-1 words (conditional words) in a word sequence.", "labels": [], "entities": []}, {"text": "The cluster n-gram model is a variant of the word n-gram model in which similar words are classified in the same cluster.", "labels": [], "entities": []}, {"text": "This has been demonstrated as an effective way to deal with the data sparseness problem and to reduce the memory sizes for realistic applications.", "labels": [], "entities": []}, {"text": "Recent research shows that using different clusters for predicted and conditional words can lead to cluster models that are superior to classical cluster models, which use the same clusters for both words.", "labels": [], "entities": []}, {"text": "This is the basis of the asymmetric cluster model (ACM), which will be formally defined and empirically studied in this paper.", "labels": [], "entities": []}, {"text": "Although similar models have been used in previous studies, several issues have not been completely investigated.", "labels": [], "entities": []}, {"text": "These include: (1) an effective methodology for constructing the ACM, (2) a thorough comparative study of the ACM with classical cluster models and word models when they are applied to a realistic application, and (3) an analysis of the reason why the ACM is superior.", "labels": [], "entities": []}, {"text": "The goal of this study is to address the above three issues.", "labels": [], "entities": []}, {"text": "We first present a formal definition of the ACM; then we describe in detail the methodology of constructing the ACM including (1) an asymmetric clustering algorithm in which different metrics are used for clustering the predicted and conditional words respectively; and (2) a method for model parameter optimization in which the optimal cluster numbers are found for different clusters.", "labels": [], "entities": [{"text": "model parameter optimization", "start_pos": 287, "end_pos": 315, "type": "TASK", "confidence": 0.6561925709247589}]}, {"text": "We evaluate the ACM on areal application, Japanese Kana-Kanji conversion, which converts phonetic Kana strings into proper Japanese orthography.", "labels": [], "entities": [{"text": "ACM", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.8812524080276489}, {"text": "Japanese Kana-Kanji conversion", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.5540018379688263}]}, {"text": "The performance is measured in terms of character error rate (CER).", "labels": [], "entities": [{"text": "character error rate (CER)", "start_pos": 40, "end_pos": 66, "type": "METRIC", "confidence": 0.9073361059029897}]}, {"text": "Our results show substantial improvements of the ACM in comparison with classical cluster models and word n-gram models at the same model size.", "labels": [], "entities": []}, {"text": "Our analysis shows that the high-performance of the ACM comes from better structure and better smoothing, both of which lie in the asymmetry of the model.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Section 1 introduces our research topic, and then Section 2 reviews related work.", "labels": [], "entities": []}, {"text": "Section 3 defines the ACM and describes in detail the method of model construction.", "labels": [], "entities": [{"text": "ACM", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.8134552240371704}, {"text": "model construction", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7297828495502472}]}, {"text": "Section 4 first introduces the Japanese Kana-Kanji conversion task; it then presents our main experiments and a discussion of our findings.", "labels": [], "entities": [{"text": "Kana-Kanji conversion task", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7866075038909912}]}, {"text": "Finally, conclusions are presented in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of different cluster types  with cluster-based models", "labels": [], "entities": []}, {"text": " Table 2: Sample parameter settings for the ACM", "labels": [], "entities": [{"text": "ACM", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.6213456988334656}]}, {"text": " Table 3: CER results of ACMs and word  trigram models at different model sizes  Now we discuss why the ACM is superior to  simple word trigrams. In addition to the better  structure as shown in Section 3.3, we assume here  that the benefit of our model also comes from its  better smoothing. Consider a probability such as  P(Tuesday| party on). If we put the word \"Tuesday\"  into the cluster WEEKDAY, we decompose the  probability", "labels": [], "entities": []}]}