{"title": [{"text": "Generative Models for Statistical Parsing with Combinatory Categorial Grammar", "labels": [], "entities": [{"text": "Statistical Parsing", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.9485314190387726}]}], "abstractContent": [{"text": "This paper compares a number of gen-erative probability models fora wide-coverage Combinatory Categorial Grammar (CCG) parser.", "labels": [], "entities": []}, {"text": "These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.", "labels": [], "entities": [{"text": "Penn Treebank trees", "start_pos": 76, "end_pos": 95, "type": "DATASET", "confidence": 0.9949994881947836}]}, {"text": "According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) fora linguistically less expressive grammar.", "labels": [], "entities": []}, {"text": "In contrast to Gildea (2001), we find a significant improvement from modeling word-word dependencies.", "labels": [], "entities": []}], "introductionContent": [{"text": "The currently best single-model statistical parser) achieves Parseval scores of over 89% on the Penn Treebank.", "labels": [], "entities": [{"text": "Parseval", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9935826659202576}, {"text": "Penn Treebank", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.9919454753398895}]}, {"text": "However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parseval measures without committing itself on certain semantically significant decisions, such as predicting null elements arising from deletion or movement.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9938537776470184}, {"text": "predicting null elements arising from deletion", "start_pos": 210, "end_pos": 256, "type": "TASK", "confidence": 0.8395720620950063}]}, {"text": "The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and its simple and semantically transparent capture of extraction and coordination.", "labels": [], "entities": [{"text": "wide-coverage parsing", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.649331271648407}]}, {"text": "We present a number of models over syntactic derivations of Combinatory Categorial Grammar (CCG, see and , this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 193, "end_pos": 206, "type": "DATASET", "confidence": 0.9927478432655334}]}, {"text": "CCG grammars are characterized by much larger category sets than standard Penn Treebank grammars, distinguishing for example between many classes of verbs with different subcategorization frames.", "labels": [], "entities": [{"text": "Penn Treebank grammars", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.9672505656878153}]}, {"text": "As a result, the categorial lexicon extracted for this purpose from the training corpus has 1207 categories, compared with the 48 POS-tags of the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 146, "end_pos": 159, "type": "DATASET", "confidence": 0.988257646560669}]}, {"text": "On the other hand, grammar rules in CCG are limited to a small number of simple unary and binary combinatory schemata such as function application and composition.", "labels": [], "entities": []}, {"text": "This results in a smaller and less overgenerating grammar than standard PCFGs (ca.", "labels": [], "entities": []}, {"text": "3,000 rules when instantiated with the above categories in sections 02-21, instead of \ud97b\udf5912,400 in the original Treebank representation).", "labels": [], "entities": [{"text": "Treebank representation", "start_pos": 110, "end_pos": 133, "type": "DATASET", "confidence": 0.9534878432750702}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The lexicalized models", "labels": [], "entities": []}, {"text": " Table 3: Performance of the models: LexCat indicates accuracy of the lexical categories; LP, LR, BP and  BR (the standard Parseval scores labeled/bracketed precision and recall) are not commensurate with other", "labels": [], "entities": [{"text": "LexCat", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.8650103211402893}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.999186098575592}, {"text": "LP", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9389098286628723}, {"text": "BP", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.8827143907546997}, {"text": "BR", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.7788451910018921}, {"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.8092043399810791}, {"text": "recall", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9969000816345215}]}, {"text": " Table 4: The impact of lexical coverage, using a different cutoff for rare words and smoothing (section 23)", "labels": [], "entities": []}]}