{"title": [{"text": "Using Similarity Scoring To Improve the Bilingual Dictionary for Word Alignment", "labels": [], "entities": [{"text": "Similarity Scoring", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.8943046927452087}, {"text": "Word Alignment", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.6956604570150375}]}], "abstractContent": [{"text": "We describe an approach to improve the bilingual cooccurrence dictionary that is used for word alignment, and evaluate the improved dictionary using aversion of the Competitive Linking algorithm.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.794177770614624}]}, {"text": "We demonstrate a problem faced by the Competitive Linking algorithm and present an approach to ameliorate it.", "labels": [], "entities": [{"text": "Competitive Linking algorithm", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.7696245312690735}]}, {"text": "In particular, we rebuild the bilingual dictionary by clustering similar words in a language and assigning them a higher cooccurrence score with a given word in the other language than each single word would have otherwise.", "labels": [], "entities": []}, {"text": "Experimental results show a significant improvement in precision and recall for word alignment when the improved dicitonary is used.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9993705153465271}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9992474317550659}, {"text": "word alignment", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.7874898910522461}]}], "introductionContent": [], "datasetContent": [{"text": "We trained three basic dictionaries using part of the Hansard data, around five megabytes of data (around 20k sentence pairs and 850k words).", "labels": [], "entities": [{"text": "Hansard data", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.9923786520957947}]}, {"text": "The basic dictionaries were built using the algorithm described in section 3, with three different thresholds: 0.005, 0.01, and 0.02.", "labels": [], "entities": []}, {"text": "In the following, we will refer to these dictionaries as as Dict0.005, Dict0.01, and Dict0.02. 50 sentences were held back for testing.", "labels": [], "entities": []}, {"text": "These sentences were hand-aligned by a fluent speaker of French.", "labels": [], "entities": []}, {"text": "No one-to-one assumption was enforced.", "labels": [], "entities": []}, {"text": "A word could thus align to zero or more words, where no upper limit was enforced (although there is a natural upper limit Thus varying the parameters, we have constructed various dictionaries by rebuilding the three baseline dictionaries.", "labels": [], "entities": []}, {"text": "Here, we report on results on three dictionaries where minsim was set to 0.7 and coocsratio was set to 0.003.", "labels": [], "entities": [{"text": "minsim", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9686476588249207}]}, {"text": "For these parameter settings, we observed robust results, although other parameter settings also yielded positive results.", "labels": [], "entities": []}, {"text": "Precision and recall was measured using the handaligned 50 sentences.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.988160252571106}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9984751343727112}]}, {"text": "Precision was defined as the percentage of links that were correctly proposed by our algorithm out of all links that were proposed.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9907940030097961}]}, {"text": "Recall is defined as the percentage of links that were found by our algorithm out of all links that should have been found.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9634077548980713}]}, {"text": "In both cases, the hand-aligned data was used as a gold standard.", "labels": [], "entities": []}, {"text": "The F-measure combines precision and recall: The following figures and tables illustrate that the Competitive Linking algorithm performs favorably when a rebuilt dictionary is used.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.980024516582489}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9991556406021118}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9988985061645508}, {"text": "Competitive Linking", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7507031857967377}]}, {"text": "lists the improvement in precision and recall for each of the dictionaries.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9995829463005066}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9995017051696777}]}, {"text": "The table shows the values when the minscore score is set to 50, and up to 1 link was allowed per word.", "labels": [], "entities": []}, {"text": "Furthermore, the p-values of a 1-tailed t-test are listed, indicating these performance boosts are in mostly highly statistically significant Dict0.005 Dict0.01 Dict0.02 P Improvement 0.060 0.067 0.057 P p-value 0.0003 0.0042 0.0126 R Improvement 0.094 0.11 0.087 R p-value 0.0026 0.0008 0.0037: Percent improvement and p-value for recall and precision, comparing baseline and rebuilt dictionaries at minscore 50 and maxlinks 1.", "labels": [], "entities": [{"text": "Dict0.005 Dict0.01 Dict0.02 P Improvement 0.060 0.067 0.057 P p-value 0.0003 0.0042 0.0126 R Improvement 0.094 0.11 0.087 R p-value", "start_pos": 142, "end_pos": 273, "type": "METRIC", "confidence": 0.847857865691185}, {"text": "recall", "start_pos": 332, "end_pos": 338, "type": "METRIC", "confidence": 0.999194324016571}, {"text": "precision", "start_pos": 343, "end_pos": 352, "type": "METRIC", "confidence": 0.996507465839386}]}, {"text": "for these parameter settings, where some of the best results were observed.", "labels": [], "entities": []}, {"text": "The following figures (figures 1-9) serve to illustrate the impact of the algorithm in greater detail.", "labels": [], "entities": []}, {"text": "All figures plot the precision, recall, and f-measure performance against different minscore settings, comparing rebuilt dictionaries to their baselines.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9996691942214966}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9988023042678833}, {"text": "f-measure", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9940112233161926}]}, {"text": "For each dictionary, three plots are given, one for each maxlinks setting, i.e. the maximum number of links allowed per word.", "labels": [], "entities": []}, {"text": "The curve names indicate the type of the curve (Precision, Recall, or F-measure), the maximum number of links allowed per word (1, 2, or 3), the dictionary used (Dict0.005, Dict0.01, or Dict0.02), and whether the run used the baseline dictionary or the rebuilt dictionary (Baseline or Cog7.3).", "labels": [], "entities": [{"text": "Recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9298286437988281}, {"text": "F-measure", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9751830101013184}]}, {"text": "It can be seen that our algorithm leads to stable improvement across parameter settings.", "labels": [], "entities": []}, {"text": "In few cases, it drops below the baseline when minscore is low.", "labels": [], "entities": [{"text": "minscore", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9758776426315308}]}, {"text": "Overall, however, our algorithm is robust -it improves alignment regardless of how many links are allowed per word, what baseline dictionary is used, and boosts both precision and recall, and thus also the f-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9995613694190979}, {"text": "recall", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.9994117021560669}]}, {"text": "To return briefly to the example cited in section , we can now show how the dictionary rebuild has affected these entries.", "labels": [], "entities": []}, {"text": "In dictionary \u00a7 \u00a6 \u00a1 they now look as follows: oil -p\u00e9trole 434 oil p\u00e9trol\u00ec ere 434 oil p\u00e9trol\u00ec eres 434 The fact that p\u00e9trole,p\u00e9trol\u00ec ere, andp\u00e9trol\u00ec eres now receive higher scores than et and dans is what causes the alignment performance to increase.", "labels": [], "entities": []}], "tableCaptions": []}