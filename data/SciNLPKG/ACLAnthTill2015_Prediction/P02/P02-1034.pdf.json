{"title": [{"text": "New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron", "labels": [], "entities": [{"text": "Parsing and Tagging", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8522704243659973}]}], "abstractContent": [{"text": "This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6586480339368185}]}, {"text": "We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the \"all sub-trees\" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence.", "labels": [], "entities": []}, {"text": "We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9765244722366333}, {"text": "Wall Street Journal text", "start_pos": 84, "end_pos": 108, "type": "DATASET", "confidence": 0.7374361753463745}, {"text": "named-entity extraction from web data", "start_pos": 114, "end_pos": 151, "type": "TASK", "confidence": 0.831716674566269}]}], "introductionContent": [{"text": "The perceptron algorithm is one of the oldest algorithms in machine learning, going back to.", "labels": [], "entities": []}, {"text": "It is an incredibly simple algorithm to implement, and yet it has been shown to be competitive with more recent learning methods such as support vector machines -see) for its application to image classification, for example.", "labels": [], "entities": [{"text": "image classification", "start_pos": 190, "end_pos": 210, "type": "TASK", "confidence": 0.8894035816192627}]}, {"text": "This paper describes how the perceptron and voted perceptron algorithms can be used for parsing and tagging problems.", "labels": [], "entities": [{"text": "parsing and tagging", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7099411090215048}]}, {"text": "Crucially, the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the \"all subtrees\" (DOP) representation described by, or a representation tracking all sub-fragments of a tagged sentence.", "labels": [], "entities": []}, {"text": "It might seem paradoxical to be able to efficiently learn and apply a model with an exponential number of features.", "labels": [], "entities": []}, {"text": "The key to our algorithms is the \"kernel\" trick (( discuss kernel methods at length).", "labels": [], "entities": []}, {"text": "We describe how the inner product between feature vectors in these representations can be calculated efficiently using dynamic programming algorithms.", "labels": [], "entities": []}, {"text": "This leads to polynomial time 2 algorithms for training and applying the perceptron.", "labels": [], "entities": []}, {"text": "The kernels we describe are related to the kernels over discrete structures in.", "labels": [], "entities": []}, {"text": "A previous paper (Collins and Duffy 2001) showed improvements over a PCFG in parsing the ATIS task.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.8337129354476929}, {"text": "parsing the ATIS task", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.6675399169325829}]}, {"text": "In this paper we show that the method scales to far more complex domains.", "labels": [], "entities": []}, {"text": "In parsing Wall Street Journal text, the method gives a 5.1% relative reduction in error rate over the model of).", "labels": [], "entities": [{"text": "parsing", "start_pos": 3, "end_pos": 10, "type": "TASK", "confidence": 0.9754044413566589}, {"text": "Wall Street Journal text", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.8390980362892151}, {"text": "error rate", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9846807420253754}]}, {"text": "In the second domain, detecting namedentity boundaries in web data, we show a 15.6% relative error reduction (an improvement in F-measure from 85.3% to 87.6%) over a state-of-the-art model, a maximum-entropy tagger.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 84, "end_pos": 108, "type": "METRIC", "confidence": 0.8780808647473654}, {"text": "F-measure", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9988002777099609}]}, {"text": "This result is derived using anew kernel, for tagged sequences, described in this paper.", "labels": [], "entities": []}, {"text": "Both results rely on anew approach that incorporates the log-probability from a baseline model, in addition to the \"all-fragments\" features.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}