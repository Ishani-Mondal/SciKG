{"title": [], "abstractContent": [{"text": "We describe a speedup for training conditional maximum entropy models.", "labels": [], "entities": []}, {"text": "The algorithm is a simple variation on Generalized Iterative Scaling, but converges roughly an order of magnitude faster, depending on the number of constraints, and the way speed is measured.", "labels": [], "entities": [{"text": "Generalized Iterative Scaling", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.7420612970987955}]}, {"text": "Rather than attempting to train all model parameters simultaneously, the algorithm trains them sequentially.", "labels": [], "entities": []}, {"text": "The algorithm is easy to implement, typically uses only slightly more memory, and will lead to improvements for most maximum entropy problems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conditional Maximum Entropy models have been used fora variety of natural language tasks, including Language Modeling, partof-speech tagging, prepositional phrase attachment, and parsing, word selection for machine translation, and finding sentence boundaries.", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.7774572372436523}, {"text": "partof-speech tagging", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.8128035068511963}, {"text": "prepositional phrase attachment", "start_pos": 142, "end_pos": 173, "type": "TASK", "confidence": 0.6274235049883524}, {"text": "word selection", "start_pos": 188, "end_pos": 202, "type": "TASK", "confidence": 0.7463932037353516}, {"text": "machine translation", "start_pos": 207, "end_pos": 226, "type": "TASK", "confidence": 0.725864440202713}]}, {"text": "Unfortunately, although maximum entropy (maxent) models can be applied very generally, the typical training algorithm for maxent, Generalized Iterative Scaling (GIS), can be extremely slow.", "labels": [], "entities": [{"text": "Generalized Iterative Scaling (GIS)", "start_pos": 130, "end_pos": 165, "type": "TASK", "confidence": 0.7383763541777929}]}, {"text": "We have personally used up to a month of computer time to train a single model.", "labels": [], "entities": []}, {"text": "There have been several attempts to speedup maxent training (Della).", "labels": [], "entities": [{"text": "maxent training", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.6592354774475098}]}, {"text": "However, as we describe later, each of these has suffered from applicability to a limited number of applications.", "labels": [], "entities": []}, {"text": "describe GIS for joint probabilities, and mention a fast variation, which appears to have been missed by the conditional maxent community.", "labels": [], "entities": []}, {"text": "We show that this fast variation can also be used for conditional probabilities, and that it is useful fora larger range of problems than traditional speedup techniques.", "labels": [], "entities": []}, {"text": "It achieves good speedups for all but the simplest models, and speedups of an order of magnitude or more for typical problems.", "labels": [], "entities": []}, {"text": "It has only one disadvantage: when there are many possible output values, the memory needed is prohibitive.", "labels": [], "entities": []}, {"text": "By combining this technique with another speedup technique, this disadvantage can be eliminated.", "labels": [], "entities": []}, {"text": "Conditional maxent models are of the form where x is an input vector, y is an output, the f i are the so-called indicator functions or feature values that are true if a particular property of x, y is true, and \u03bb i is a weight for the indicator f i . For instance, if trying to do word sense disambiguation for the word \"bank\", x would be the context around an occurrence of the word; y would be a particular sense, e.g. financial or river; f i (x, y) could be 1 if the context includes the word \"money\" and y is the financial sense; and \u03bb i would be a large positive number.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 280, "end_pos": 305, "type": "TASK", "confidence": 0.6234854658444723}]}, {"text": "Maxent models have several valuable properties.", "labels": [], "entities": []}, {"text": "The most important is constraint satisfaction.", "labels": [], "entities": [{"text": "constraint satisfaction", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.674329549074173}]}, {"text": "For a given f i , we can count how many times f i was observed in the training data, observed[i] = \ud97b\udf59 j f i (x j , y j ).", "labels": [], "entities": []}, {"text": "For a model P \u03bb with parameters \u03bb, we can see how many times the model predicts that f i would be expected: These equalities are called constraints.", "labels": [], "entities": []}, {"text": "An additional property is that, of models in the form of Equation 1, the maxent model maximizes the probability of the training data.", "labels": [], "entities": [{"text": "Equation", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9666547775268555}]}, {"text": "Yet another property is that maxent models are as close as possible to the uniform distribution, subject to constraint satisfaction.", "labels": [], "entities": []}, {"text": "Maximum entropy models are most commonly learned using GIS, which is actually a very simple algorithm.", "labels": [], "entities": []}, {"text": "At each iteration, a step is taken in a direction that increases the likelihood of the training data.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9700044393539429}]}, {"text": "The step size is guaranteed to be not too large and not too small: the likelihood of the training data increases at each iteration and eventually converges to the global optimum.", "labels": [], "entities": []}, {"text": "Unfortunately, this guarantee comes at a price: GIS takes a step size inversely proportional to the maximum number of active constraints.", "labels": [], "entities": []}, {"text": "Maxent models are interesting precisely because of their ability to combine many different kinds of information, so this weakness of GIS means that maxent models are slow to learn precisely when they are most useful.", "labels": [], "entities": []}, {"text": "We will describe a variation on GIS that works much faster.", "labels": [], "entities": []}, {"text": "Rather than learning all parameters of the model simultaneously, we learn them sequentially: one, then the next, etc., and then back to the beginning.", "labels": [], "entities": []}, {"text": "The new algorithm converges to the same point as the original one.", "labels": [], "entities": []}, {"text": "This sequential learning would not lead to much, if any, improvement, except that we also show how to cache subcomputations.", "labels": [], "entities": []}, {"text": "The combination leads to improvements of an order of magnitude or more.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we give experimental results, showing that SCGIS converges up to an order of magnitude faster than GIS, or more, depending on the number of non-zero indicator functions, and the method of measuring performance.", "labels": [], "entities": [{"text": "GIS", "start_pos": 116, "end_pos": 119, "type": "DATASET", "confidence": 0.6175085306167603}]}, {"text": "There are at least three ways in which one could measure performance of a maxent model: the objective function optimized by GIS/SCGIS; the entropy on test data; and the percent correct on test data.", "labels": [], "entities": []}, {"text": "The objective function for both SCGIS and GIS when smoothing is Equation 3: the probability of the training data times the probability of the model.", "labels": [], "entities": [{"text": "Equation", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9943828582763672}]}, {"text": "The most interesting measure, the percent correct on test data, tends to be noisy.", "labels": [], "entities": [{"text": "correct", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.6992002725601196}]}, {"text": "For a test corpus, we chose to use exactly the same training, test, problems, and feature sets used by.", "labels": [], "entities": []}, {"text": "These problems consisted of trying to guess which of two confusable words, e.g. \"their\" or \"there\", a user intended.", "labels": [], "entities": []}, {"text": "Banko and Brill chose this data to be representative of typical machine learning problems, and, by trying it across data sizes and different pairs of words, it exhibits a good deal of different behaviors.", "labels": [], "entities": []}, {"text": "Banko and Brill used a standard set of features, including words within a window of 2, part-of-speech tags within a window of 2, pairs of word or tag features, and whether or not a given word occurred within a window of 9.", "labels": [], "entities": []}, {"text": "Altogether, they had 55 feature types.", "labels": [], "entities": []}, {"text": "That is, there were many thousands of features in the model (depending on the exact model), but at most 55 could be \"true\" fora given training or test instance.", "labels": [], "entities": []}, {"text": "We examine the performance of SCGIS versus GIS across three different axes.", "labels": [], "entities": [{"text": "SCGIS", "start_pos": 30, "end_pos": 35, "type": "TASK", "confidence": 0.9104015231132507}]}, {"text": "The most important variable is the number of features.", "labels": [], "entities": []}, {"text": "In addition to trying Banko and Brill's 55 feature types, we tried using feature sets with 5 feature types (words within a window of 2, plus the \"unigram\" feature) and 15 feature types (words within a window of 2, tags within a window of 2, the unigram, and pairs of words within a window of 2).", "labels": [], "entities": []}, {"text": "We also tried not using smoothing, and we tried varying the training data size.", "labels": [], "entities": []}, {"text": "In, we present a \"typical\" configuration, using 55 feature types, and 10 million words of training, and smoothing with a Gaussian prior.", "labels": [], "entities": []}, {"text": "The first two columns show the different confusable words.", "labels": [], "entities": []}, {"text": "Each column shows the ratio of how much longer (in terms of elapsed time) it takes GIS to achieve the same results as 10 iterations of SCGIS.", "labels": [], "entities": []}, {"text": "An \"XXX\" denotes a casein which GIS did not achieve the performance level of SCGIS within 1000 iterations.", "labels": [], "entities": []}, {"text": "(XXXs were not included in averages.)", "labels": [], "entities": []}, {"text": "The \"objec\" column shows the ratio of time to achieve the same value of the objective function (Equation 3); the \"ent\" column show the ratio of time to achieve the same test entropy; and the \"cor\" column shows the ratio of time to achieve the same test error rate.", "labels": [], "entities": []}, {"text": "For all three measurements, the ratio can be up to a factor of 30, though the average is somewhat lower, and in two cases, GIS converged faster.", "labels": [], "entities": [{"text": "GIS", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.685040295124054}]}, {"text": "In we repeat the experiment, but without smoothing.", "labels": [], "entities": []}, {"text": "On the objective function -which with no smoothing is just the training entropy -the increase from SCGIS is even larger.: Same as baseline, except no smoothing criteria -test entropy and percentage correct -the increase from SCGIS is smaller than it was with smoothing, but still consistently large.", "labels": [], "entities": []}, {"text": "In, we show results with small and medium feature sets.", "labels": [], "entities": []}, {"text": "As can be seen, the speedups with smaller features sets (5 feature types) are less than the speedups with the medium sized feature set (15 feature types), which are smaller than the baseline speedup with 55 features.", "labels": [], "entities": []}, {"text": "Notice that across all experiments, there were no cases where GIS converged faster than SCGIS on the objective function; two cases where it coverged faster on test data entropy; and 5 cases where it converged faster on test data correctness.", "labels": [], "entities": []}, {"text": "The objective function measure is less noisy than test data entropy, and test data entropy is less noisy than test data error rate: the noisier the data, the more chance of an unexpected result.", "labels": [], "entities": []}, {"text": "Thus, one possibility is that these cases are simply due to noise.", "labels": [], "entities": []}, {"text": "Similarly, the four cases in which GIS never reached the test data   entropy of SCGIS and the four cases in which GIS never reached the test data error rate of SCGIS might also be attributable to noise.", "labels": [], "entities": []}, {"text": "There is an alternative explanation that might be worth exploring.", "labels": [], "entities": []}, {"text": "On a different data set, 20 newsgroups, we found that early stopping techniques were helpful, and that GIS and SCGIS benefited differently depending on the exact settings.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.5257071405649185}]}, {"text": "It is possible that effects similar to the smoothing effect of early stopping played a role in both the XXX cases (in which SCGIS presumably benefited more from the effects) and in the cases where GIS beat SCGIS (in which cases GIS presumably benefited more.)", "labels": [], "entities": [{"text": "early stopping", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.599897176027298}]}, {"text": "Additional research would be required to determine which explanation -early stopping or noise -is correct, although we suspect both explanations apply in some cases.", "labels": [], "entities": []}, {"text": "We also ran experiments that were the same as the baseline experiment, except changing the training data size to 50 million words and to 1 million words.", "labels": [], "entities": []}, {"text": "We found that the individual speedups were often different at the different sizes, but did not appear to be overall higher or lower or qualitatively different.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline: standard feature types (55), 10  million words, smoothed", "labels": [], "entities": []}, {"text": " Table 2: Same as baseline, except no smoothing", "labels": [], "entities": [{"text": "smoothing", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.7734447121620178}]}, {"text": " Table 3: Small feature set (5 feature types)", "labels": [], "entities": []}, {"text": " Table 4: Medium feature set (15 feature types)", "labels": [], "entities": []}]}