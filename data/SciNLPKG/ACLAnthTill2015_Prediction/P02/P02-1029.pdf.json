{"title": [{"text": "Inducing German Semantic Verb Classes from Purely Syntactic Subcategorisation Information", "labels": [], "entities": []}], "abstractContent": [{"text": "The paper describes the application of k-Means, a standard clustering technique, to the task of inducing semantic classes for German verbs.", "labels": [], "entities": []}, {"text": "Using probability distributions over verb subcategorisation frames, we obtained an intuitively plausible clustering of 57 verbs into 14 classes.", "labels": [], "entities": []}, {"text": "The automatic clustering was evaluated against independently motivated, hand-constructed semantic verb classes.", "labels": [], "entities": []}, {"text": "A series of post-hoc cluster analyses explored the influence of specific frames and frame groups on the coherence of the verb classes, and supported the tight connection between the syntactic behaviour of the verbs and their lexical meaning components .", "labels": [], "entities": []}], "introductionContent": [{"text": "A long-standing linguistic hypothesis asserts a tight connection between the meaning components of a verb and its syntactic behaviour: To a certain extent, the lexical meaning of a verb determines its behaviour, particularly with respect to the choice of its arguments.", "labels": [], "entities": []}, {"text": "The theoretical foundation has been established in extensive work on semantic verb classes such as for English and () for Spanish: each verb class contains verbs which are similar in their meaning and in their syntactic properties.", "labels": [], "entities": []}, {"text": "From a practical point of view, a verb classification supports Natural Language Processing tasks, since it provides a principled basis for filling gaps in available lexical knowledge.", "labels": [], "entities": []}, {"text": "For example, the English verb classification has been used for applications such as machine translation, word sense disambiguation, and document classification (.", "labels": [], "entities": [{"text": "English verb classification", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.6557691693305969}, {"text": "machine translation", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.8233897387981415}, {"text": "word sense disambiguation", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.6906673908233643}, {"text": "document classification", "start_pos": 136, "end_pos": 159, "type": "TASK", "confidence": 0.8003361523151398}]}, {"text": "Various attempts have been made to infer conveniently observable morpho-syntactic and semantic properties for English verb classes).", "labels": [], "entities": []}, {"text": "To our knowledge this is the first work to obtain German verb classes automatically.", "labels": [], "entities": []}, {"text": "We used a robust statistical parser) to acquire purely syntactic subcategorisation information for verbs.", "labels": [], "entities": []}, {"text": "The information was provided inform of probability distributions over verb frames for each verb.", "labels": [], "entities": []}, {"text": "There were two conditions: the first with relatively coarse syntactic verb subcategorisation frames, the second a more delicate classification subdividing the verb frames of the first condition using prepositional phrase information (case plus preposition).", "labels": [], "entities": []}, {"text": "In both conditions verbs were clustered using k-Means, an iterative, unsupervised, hard clustering method with well-known properties, cf. (. The goal of a series of cluster analyses was (i) to find good values for the parameters of the clustering process, and (ii) to explore the role of the syntactic frame descriptions in verb classification, to demonstrate the implicit induction of lexical meaning components from syntactic properties, and to suggest ways in which the syntactic information might further be refined.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 324, "end_pos": 343, "type": "TASK", "confidence": 0.7272156327962875}]}, {"text": "Our long term goal is to support the development of high-quality and large-scale lexical resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "The task of evaluating the result of a cluster analysis against the known gold standard of hand-constructed verb classes requires us to assess the similarity between two sets of equivalence relations.", "labels": [], "entities": []}, {"text": "As noted by (), it is useful to have an evaluation measure that does not depend on the choice of similarity measure or on the original dimensionality of the input data, since that allows meaningful comparison of results for which these parameters vary.", "labels": [], "entities": []}, {"text": "This is similar to the perspective of (, who present, in the context of the MUC co-reference evaluation scheme, a model-theoretic measure of the similarity between equivalence classes.", "labels": [], "entities": [{"text": "MUC co-reference evaluation", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.570711076259613}]}, {"text": "(4) This manipulation is designed to remove the bias towards small clusters: 2 using the 57 verbs from our study we generated 50 random clusters for each cluster size between 1 and 57, and evaluated the results against the gold standard, returning the best result for each replication.", "labels": [], "entities": []}, {"text": "We found that even using the scaling factor the measure favours smaller clusters.", "labels": [], "entities": []}, {"text": "But this bias is strongest at the extremes of the range, and does not appear to impact too heavily on our results.", "labels": [], "entities": []}, {"text": "Unfortunately none of Strehl et al's measures have all the properties which we intuitively require from a measure of linguistic cluster quality.", "labels": [], "entities": []}, {"text": "For example, if we restrict attention to the casein which all verbs in an inferred cluster are drawn from the same actual class, we would like it to be the case that the evaluation measure is a monotonically increasing function of the size of the inferred cluster.", "labels": [], "entities": []}, {"text": "We therefore introduced an additional, more suitable measure for the evaluation of individual clusters, based on the representation of equivalence classes as sets of pairs.", "labels": [], "entities": []}, {"text": "It turns out that pairwise precision and recall have some of the counter-intuitive properties that we objected to in Strehl et al's measures, so we adjust pairwise precision with a scaling factor based on the size In the absence of the penalty, mutual information would attain its maximum (which is the entropy of ) not only when A is correct but also when . The evaluation function is extremely non-linear, which leads to a severe loss of quality with the first few clustering mistakes, but does not penalise later mistakes to the same extent.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9727424383163452}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9982845187187195}, {"text": "precision", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.7530794739723206}]}, {"text": "From the methodological point of view, the clustering evaluation gave interesting insights into kMeans' behaviour on the syntactic frame data.", "labels": [], "entities": []}, {"text": "The more delicate verb-frame classification, i.e. the refinement of the syntactic verb frame descriptions by prepositional phrase specification, improved the clustering results.", "labels": [], "entities": [{"text": "verb-frame classification", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.7283630073070526}]}, {"text": "This does not go without saying: there was potential fora sparse data problem, since even frequent verbs can only be expected to inhabit a few frames.", "labels": [], "entities": []}, {"text": "For example, the verb anfangen with a corpus frequency of 2,554 has zero counts for 138 of the 171 frames.", "labels": [], "entities": []}, {"text": "Whether the improvement really matters in an application task is left to further research.", "labels": [], "entities": []}, {"text": "We found that randomised starting clusters usually give better results than initialisation from a hierarchical clustering.", "labels": [], "entities": []}, {"text": "Hierarchies imposing a strong structure on the clustering (such as single-linkage: the output clusterings contain few very large and many singleton clusters) are hardly improved by kMeans.", "labels": [], "entities": []}, {"text": "Their evaluation results are noticeably below those for random clusters.", "labels": [], "entities": []}, {"text": "But initialisation using Ward's method, which produces tighter clusters and a narrower range of cluster sizes does outperform random cluster initialisation.", "labels": [], "entities": [{"text": "initialisation", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9795799255371094}]}, {"text": "Presumably the issue is that the other hierarchical clustering methods place k-Means in a local minimum from which it cannot escape, and that uniformly shaped cluster initialisation gives k-Means a better chance of avoiding local minima, even with a high degree of perturbation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Probability distribution for glauben", "labels": [], "entities": []}, {"text": " Table 2: Refined np distribution for reden", "labels": [], "entities": [{"text": "reden", "start_pos": 38, "end_pos": 43, "type": "TASK", "confidence": 0.7721731662750244}]}]}