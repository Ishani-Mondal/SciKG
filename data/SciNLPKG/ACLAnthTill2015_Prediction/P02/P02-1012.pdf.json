{"title": [{"text": "Pronominalization in Generated Discourse and Dialogue", "labels": [], "entities": []}], "abstractContent": [{"text": "Previous approaches to pronominalization have largely been theoretical rather than applied in nature.", "labels": [], "entities": []}, {"text": "Frequently, such methods are based on Centering Theory, which deals with the resolution of anaphoric pronouns.", "labels": [], "entities": [{"text": "resolution of anaphoric pronouns", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.8463783413171768}]}, {"text": "But it is not clear that complex theoretical mechanisms, while having satisfying explanatory power, are necessary for the actual generation of pronouns.", "labels": [], "entities": []}, {"text": "We first illustrate examples of pronouns from various domains, describe a simple method for generating pronouns in an implemented multi-page generation system, and present an evaluation of its performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Pronominalization is an important element in the automatic creation of multi-paragraph and multi-page texts using natural language generation (NLG).", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 114, "end_pos": 147, "type": "TASK", "confidence": 0.7976269423961639}]}, {"text": "Authors routinely use pronouns in texts spanning all types of genres, such as newspaper copy, science fiction and even academic papers.", "labels": [], "entities": []}, {"text": "Indeed, without pronouns, texts quickly become confusing as readers begin to pay more attention to the writing style than to the content that makes the text informative or enjoyable.", "labels": [], "entities": []}, {"text": "Even worse, incorrect pronouns can lead readers to misinterpret the text or draw unsound inferences.", "labels": [], "entities": []}, {"text": "Furthermore, current pronominalization strategies are ill-equipped to deal with the wide variety of reasons that pronouns are used in naturally occurring texts.", "labels": [], "entities": []}, {"text": "Almost without exception, they focus on anaphoric pronouns as described in Focus/Centering Theory, ignoring the multitude of other possible types.", "labels": [], "entities": []}, {"text": "However, it is certainly true that authors make use of pronouns which are not motivated by anaphoric reference.", "labels": [], "entities": []}, {"text": "In addition, because such approaches are oriented towards anaphora resolution during parsing, they ignore structures such as the discourse plan which are present during generation but not parsing.", "labels": [], "entities": [{"text": "anaphora resolution during parsing", "start_pos": 58, "end_pos": 92, "type": "TASK", "confidence": 0.6709019988775253}]}, {"text": "A typical discourse plan can include vital information for pronominalization such as time and clause boundaries, ordering of propositions, and semantic details verbal arguments.", "labels": [], "entities": []}, {"text": "Current approaches based on Centering algorithms thus attempt to recreate a text coherence structure that duplicates work already done by the discourse planner.", "labels": [], "entities": []}, {"text": "Finally, there are significant obstacles to verifying the correctness of existing pronominalization algorithms for any pronominalization theory): the lack of natural language generation systems that can produce large enough texts to bring discourse-level processes into play.", "labels": [], "entities": []}, {"text": "Because of this, researchers are forced to simulate by hand how their algorithms will work on a given text.", "labels": [], "entities": []}, {"text": "It is also not sufficient to use template generation systems to perform this task because they lack the low-level discourse representation needed to provide the information upon which most algorithms base their decisions.", "labels": [], "entities": []}, {"text": "In this paper we first summarize related work in both anaphora resolution and anaphora generation.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8333212435245514}, {"text": "anaphora generation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.8119849860668182}]}, {"text": "We next describe the range of pronoun types that we found in a wide variety of texts.", "labels": [], "entities": []}, {"text": "We proceed to describe an algorithm for determining appropriate pronominalizations that uses existing NLG structures and simple numeric techniques.", "labels": [], "entities": []}, {"text": "We also briefly describe an implemented generation system that contains enough low-level discourse information to motivate pronominalization decisions using this method.", "labels": [], "entities": []}, {"text": "Finally, we quantitatively demonstrate the performance of this simple numerical approach in both a newspaper and fictional narrative domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "Now, it happened that a wolf \u00bd , a very cruel, greedy creature \u00be also heard Little Red Riding Hood \u00bf as she  Each noun element is processed in the order linearized from the discourse plan: 1.", "labels": [], "entities": []}, {"text": "The first mention of wolf \u00bd in the narrative resets its discourse history entry.", "labels": [], "entities": []}, {"text": "2. Creature \u00be is the second mention of wolf, but it is in an appositive structure (see pronoun category #9).", "labels": [], "entities": []}, {"text": "3. LRRH \u00bf was mentioned just before in the prior paragraph, but \"Now,\" is a prosodic discourse marker (see pronoun category #8), thus modifiers(NE, RS) \ud97b\udf59 \ud97b\udf59 \ud97b\udf59.", "labels": [], "entities": [{"text": "LRRH", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.8740848302841187}]}, {"text": "4. For LRRH \u00bf and LRRH \ud97b\udf59 , sentence-distance(NE, SC) = 0 resulting in a multiple-in-sentence-pronoun.", "labels": [], "entities": []}, {"text": "5. Sentence-distance(NE, SC) = 1, but recency(NE) = 2, resulting in a short-distance-pronoun.", "labels": [], "entities": [{"text": "recency(NE)", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9037223011255264}]}, {"text": "6. Similarly, LRRH \ud97b\udf59 is converted into a short-distance-pronoun.", "labels": [], "entities": [{"text": "LRRH", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9768664240837097}]}, {"text": "7. As with element #4, this is a case resulting in a multiple-in-sentence-pronoun.", "labels": [], "entities": []}, {"text": "9. As with element #5, this is a case resulting in a short-distance-pronoun.", "labels": [], "entities": []}, {"text": "10. The first mention of Hugh \u00bd\u00bc in the narrative resets its discourse history entry.", "labels": [], "entities": []}, {"text": "11. As with element #2, the discourse plan reports that this is an appositive.", "labels": [], "entities": []}, {"text": "13. Finally, Hugh \u00bd\u00bf is repeated in the same sentence.", "labels": [], "entities": []}, {"text": "However, there are significant practical obstacles to comparing the performance of different pronominalization algorithms using corpus matching criteria instead of \"quality\" as evaluated by human judges.", "labels": [], "entities": []}, {"text": "Because systems that can handle a large quantity of text are very recent and because it can require years to create and organize the necessary knowledge to produce even one multi-paragraph text, much research on anaphora generation has instead relied on one of two techniques: \u00af Checking algorithms by hand: One verification method is to manually examine a text, identifying candidates for pronominalization and simulating the rules of a particular theory.", "labels": [], "entities": [{"text": "anaphora generation", "start_pos": 212, "end_pos": 231, "type": "TASK", "confidence": 0.7629857063293457}]}, {"text": "However, this method is prone to human error.", "labels": [], "entities": []}, {"text": "\u00af Checking algorithms semiautomatically: Other researchers opt instead to annotate a corpus for pronominalization and their antecedents as well as the pronoun forms that should occur, and then simulate a pronominalization algorithm on the marked-up text ().", "labels": [], "entities": []}, {"text": "Similarly, this approach can suffer from interannotator agreement errors ().", "labels": [], "entities": []}, {"text": "To verify our pronominalization algorithm more rigorously, we instead used the STORYBOOK deep generation system to recreate pre-existing multipage texts with automatically selected pronouns.", "labels": [], "entities": [{"text": "STORYBOOK deep generation", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.46597932775815326}]}, {"text": "Without a full-scale implementation, it is impossible to determine whether an algorithm performs imperfectly due to human error, alack of available corpus data for making decisions, or if it is a fault with the algorithm itself.", "labels": [], "entities": []}], "tableCaptions": []}