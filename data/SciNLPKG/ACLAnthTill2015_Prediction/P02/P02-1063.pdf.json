{"title": [{"text": "Revision Learning and its Application to Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Revision Learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9279783368110657}, {"text": "Part-of-Speech Tagging", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7112235575914383}]}], "abstractContent": [{"text": "This paper presents a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost.", "labels": [], "entities": [{"text": "revision learning", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.9645976722240448}]}, {"text": "This method uses a high capacity model to revise the output of a small cost model.", "labels": [], "entities": []}, {"text": "We apply this method to English part-of-speech tagging and Japanese morphological analysis, and show that the method performs well.", "labels": [], "entities": [{"text": "English part-of-speech tagging", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.5920803348223368}, {"text": "Japanese morphological analysis", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.6881271004676819}]}], "introductionContent": [{"text": "Recently, corpus-based approaches have been widely studied in many natural language processing tasks, such as part-of-speech (POS) tagging, syntactic analysis, text categorization and word sense disambiguation.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.6684041976928711}, {"text": "syntactic analysis", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.7375533580780029}, {"text": "text categorization", "start_pos": 160, "end_pos": 179, "type": "TASK", "confidence": 0.7625991702079773}, {"text": "word sense disambiguation", "start_pos": 184, "end_pos": 209, "type": "TASK", "confidence": 0.6950540343920389}]}, {"text": "In corpus-based natural language processing, one important issue is to decide which learning model to use.", "labels": [], "entities": []}, {"text": "Various learning models have been studied such as Hidden Markov models (HMMs), decision trees ( and maximum entropy models.", "labels": [], "entities": []}, {"text": "Recently, Support Vector Machines (SVMs)) are getting to be used, which are supervised machine learning algorithm for binary classification.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 118, "end_pos": 139, "type": "TASK", "confidence": 0.6986730694770813}]}, {"text": "SVMs have good generalization performance and can handle a large number of features, and are applied to some tasks * Presently with Oki Electric Industry successfully).", "labels": [], "entities": [{"text": "Oki Electric Industry", "start_pos": 132, "end_pos": 153, "type": "DATASET", "confidence": 0.7354116241137186}]}, {"text": "However, their computational cost is large and is a weakness of SVMs.", "labels": [], "entities": []}, {"text": "In general, a trade-off between capacity and computational cost of learning models exists.", "labels": [], "entities": []}, {"text": "For example, SVMs have relatively high generalization capacity, but have high computational cost.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.9230639934539795}]}, {"text": "On the other hand, HMMs have lower computational cost, but have lower capacity and difficulty in handling data with a large number of features.", "labels": [], "entities": []}, {"text": "Learning models with higher capacity may not be of practical use because of their prohibitive computational cost.", "labels": [], "entities": []}, {"text": "This problem becomes more serious when a large amount of data is used.", "labels": [], "entities": []}, {"text": "To solve this problem, we propose a revision learning method which combines a model with high generalization capacity and a model with small computational cost to achieve high performance with small computational cost.", "labels": [], "entities": [{"text": "revision learning", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.96096470952034}]}, {"text": "This method is based on the idea that processing the entire target task using a model with higher capacity is wasteful and costly, that is, if a large portion of the task can be processed easily using a model with small computational cost, it should be processed by such a model, and only difficult portion should be processed by the model with higher capacity.", "labels": [], "entities": []}, {"text": "Revision learning can handle a general multiclass classification problem, which includes POS tagging, text categorization and many other tasks in natural language processing.", "labels": [], "entities": [{"text": "Revision learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9244116842746735}, {"text": "multiclass classification", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.7365648746490479}, {"text": "POS tagging", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.7833606600761414}]}, {"text": "We apply this method to English POS tagging and Japanese morphological analysis.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.8134539723396301}, {"text": "Japanese morphological analysis", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.6309646666049957}]}, {"text": "This paper is organized as follows: Section 2 describes the general multi-class classification problem and the one-versus-rest method which is known as one of the solutions for the problem.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.7338862419128418}]}, {"text": "Section 3 introduces revision learning, and discusses how to combine learning models.", "labels": [], "entities": [{"text": "revision learning", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.9879293441772461}]}, {"text": "Section 4 describes one way to conduct Japanese morphological analysis with revision learning.", "labels": [], "entities": [{"text": "Japanese morphological analysis", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.8139810363451639}, {"text": "revision learning", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.9197270274162292}]}, {"text": "Section 5 shows experimental results of English POS tagging and Japanese morphological analysis with revision learning.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.7555044293403625}, {"text": "Japanese morphological analysis", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.6474305391311646}]}, {"text": "Section 6 discusses related works, and Section 7 gives conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section gives experimental results of English POS tagging and Japanese morphological analysis with revision learning.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.7956391870975494}, {"text": "Japanese morphological analysis", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.6700997749964396}]}, {"text": "Experiments of English POS tagging with revision learning (RL) are performed on the Penn Treebank WSJ corpus.", "labels": [], "entities": [{"text": "English POS tagging", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.5446490744749705}, {"text": "revision learning (RL)", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6595953226089477}, {"text": "Penn Treebank WSJ corpus", "start_pos": 84, "end_pos": 108, "type": "DATASET", "confidence": 0.9830294847488403}]}, {"text": "The corpus is randomly separated into training data of 41,342 sentences and test data of 11,771 sentences.", "labels": [], "entities": []}, {"text": "The dictionary for HMMs is constructed from all the words in the training data.", "labels": [], "entities": []}, {"text": "T3 of ICOPOST release 0.9.0) is used as the stochastic model for ranking stage.", "labels": [], "entities": [{"text": "ICOPOST release 0.9.0", "start_pos": 6, "end_pos": 27, "type": "DATASET", "confidence": 0.8409149448076884}]}, {"text": "This is equivalent to POS-based second order HMMs.", "labels": [], "entities": []}, {"text": "SVMs with second order polynomial kernel are used as the binary classifier.", "labels": [], "entities": []}, {"text": "The results are compared with TnT) based on second order HMMs, and with POS tagger using SVMs with one-versus-rest (1-v-r) ().", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 72, "end_pos": 82, "type": "TASK", "confidence": 0.6510515809059143}]}, {"text": "The accuracies of those systems for known words, unknown words and all the words are shown in.", "labels": [], "entities": []}, {"text": "The accuracies for both known words and unknown words are improved through revision learning.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9812936186790466}, {"text": "revision learning", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.8970154821872711}]}, {"text": "However, revision learning could not surpass the one-versus-rest.", "labels": [], "entities": [{"text": "revision learning", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.8083861470222473}]}, {"text": "The main difference in the accuracies stems from those for unknown words.", "labels": [], "entities": []}, {"text": "The reason for that seems to be that the dictionary of HMMs for POS tagging is obtained from the training data, as a result, virtually no unknown words exist in the training data, and the HMMs never make mistakes for unknown words during the training.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.8657073974609375}]}, {"text": "So no example of unknown words is available in the training data for the SVM reviser.", "labels": [], "entities": [{"text": "SVM reviser", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.7372644543647766}]}, {"text": "This is problematic: Though the HMMs handles unknown words with an exceptional method, SVMs cannot learn about errors made by the unknown word processing in the HMMs.", "labels": [], "entities": []}, {"text": "To cope with this problem, we force the HMMs to make mistakes by eliminating low frequent words from the dictionary.", "labels": [], "entities": []}, {"text": "We eliminated the words appearing only once in the training data so as to make SVMs to learn about unknown words.", "labels": [], "entities": []}, {"text": "The results are shown in (row \"cutoff-1\").", "labels": [], "entities": []}, {"text": "Such procedure improves the accuracies for unknown words.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9740299582481384}]}, {"text": "One advantage of revision learning is its small computational cost.", "labels": [], "entities": [{"text": "revision learning", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.9862974882125854}]}, {"text": "We compare the computation time with the HMMs and the one-versusrest.", "labels": [], "entities": []}, {"text": "We also use SVMs with linear kernel function that has lower capacity but lower computational cost compared to the second order polynomial kernel SVMs.", "labels": [], "entities": []}, {"text": "The experiments are performed on an Alpha 21164A 500MHz processor.", "labels": [], "entities": []}, {"text": "shows the total number of training examples, training time, testing time and accuracy for each of the five systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.999742329120636}]}, {"text": "The training time and the testing time of revision learning are considerably smaller than those of the oneversus-rest.", "labels": [], "entities": [{"text": "revision learning", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.9601461589336395}]}, {"text": "Using linear kernel, the accuracy decreases a little, but the computational cost is much lower than the second order polynomial kernel.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995775818824768}]}, {"text": ") which is originally constructed for the Japanese morphological analyzer ChaSen ( . A POS bigram model and ChaSen version 2.2.8 based on variable length HMMs are used as the stochastic models for the ranking stage, and SVMs with the second order polynomial kernel are used as the binary classifier.", "labels": [], "entities": []}, {"text": "We use the following values to evaluate Japanese morphological analysis: The results of the original systems and those with revision learning are shown in, which provides the recalls, precisions and Fmeasures for two cases, namely segmentation (i.e. segmentation of the sentences into morphemes) and tagging (i.e. segmentation and POS tagging).", "labels": [], "entities": [{"text": "Japanese morphological analysis", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.7339727481206259}, {"text": "recalls", "start_pos": 175, "end_pos": 182, "type": "METRIC", "confidence": 0.9970032572746277}, {"text": "precisions", "start_pos": 184, "end_pos": 194, "type": "METRIC", "confidence": 0.9962343573570251}, {"text": "Fmeasures", "start_pos": 199, "end_pos": 208, "type": "METRIC", "confidence": 0.9985014200210571}, {"text": "POS tagging", "start_pos": 331, "end_pos": 342, "type": "TASK", "confidence": 0.62531878054142}]}, {"text": "The one-versus-rest method is not used because it is not applicable to morphological analysis of non-segmented languages directly.", "labels": [], "entities": []}, {"text": "When revision learning is used, all the measures are improved for both POS bigram and ChaSen.", "labels": [], "entities": [{"text": "revision learning", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.9154668748378754}, {"text": "POS bigram", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.7935225665569305}]}, {"text": "Improvement is particularly clear for the tagging task.", "labels": [], "entities": [{"text": "tagging task", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.9254977107048035}]}, {"text": "The numbers of correct morphemes for each POS category tag in the output of ChaSen with and without revision learning are shown in Table 4.", "labels": [], "entities": []}, {"text": "Many particles are correctly revised by revision learning.", "labels": [], "entities": [{"text": "revision learning", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.9229229986667633}]}, {"text": "The reason is that the POS tags for particles are often affected by the following words in Japanese, and SVMs can revise such particles because it uses the lexical forms of the following words as the features.", "labels": [], "entities": []}, {"text": "This is the advantage of our method compared to simple HMMs, because HMMs have difficulty in handling a lot of features such as the lexical forms of words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Result of English POS Tagging", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.6426950544118881}]}, {"text": " Table 2: Computational Cost of English POS Tagging", "labels": [], "entities": [{"text": "Computational Cost of English POS Tagging", "start_pos": 10, "end_pos": 51, "type": "TASK", "confidence": 0.5124415308237076}]}, {"text": " Table 3: Result of Morphological Analysis", "labels": [], "entities": [{"text": "Result of Morphological Analysis", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7254424393177032}]}, {"text": " Table 4: The Number of Correctly Tagged Morphemes for Each POS Category Tag", "labels": [], "entities": []}]}