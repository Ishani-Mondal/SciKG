{"title": [], "abstractContent": [{"text": "We consider the problem of parsing non-recursive context-free grammars, i.e., context-free grammars that generate finite languages.", "labels": [], "entities": [{"text": "parsing non-recursive context-free grammars", "start_pos": 27, "end_pos": 70, "type": "TASK", "confidence": 0.8346649408340454}]}, {"text": "In natural language processing , this problem arises in several areas of application, including natural language generation, speech recognition and machine translation.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6425794164339701}, {"text": "natural language generation", "start_pos": 96, "end_pos": 123, "type": "TASK", "confidence": 0.6419753233591715}, {"text": "speech recognition", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.7767352163791656}, {"text": "machine translation", "start_pos": 148, "end_pos": 167, "type": "TASK", "confidence": 0.7994364798069}]}, {"text": "We present two tabu-lar algorithms for parsing of non-recursive context-free grammars, and show that they perform well in practical settings, despite the fact that this problem is PSPACE-complete.", "labels": [], "entities": [{"text": "parsing of non-recursive context-free grammars", "start_pos": 39, "end_pos": 85, "type": "TASK", "confidence": 0.8711451530456543}]}], "introductionContent": [{"text": "Several applications in natural language processing require \"parsing\" of a large but finite set of candidate strings.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6809035738309225}]}, {"text": "Here parsing means some computation that selects those strings out of the finite set that are wellformed according to some grammar, or that are most likely according to some language model.", "labels": [], "entities": []}, {"text": "In these applications, the finite set is typically encoded in a compact way as a context-free grammar (CFG) that is non-recursive.", "labels": [], "entities": []}, {"text": "This is motivated by the fact that non-recursive CFGs allow very compact representations for finite languages, since the strings derivable from single nonterminals maybe substrings of many different strings in the language.", "labels": [], "entities": []}, {"text": "Unfolding such a grammar and parsing the generated strings \u00a1 Secondary affiliation is the German Research Center for Artificial Intelligence (DFKI).", "labels": [], "entities": []}, {"text": "one by one then leads to an unnecessary duplication of subcomputations, since each occurrence of a repeated substring has to be independently parsed.", "labels": [], "entities": []}, {"text": "As this approach maybe prohibitively expensive, it is preferable to find a parsing algorithm that shares subcomputations among different strings by working directly on the nonterminals and the rules of the non-recursive CFG.", "labels": [], "entities": [{"text": "CFG", "start_pos": 220, "end_pos": 223, "type": "DATASET", "confidence": 0.9592968821525574}]}, {"text": "In this way, \"parsing\" a nonterminal of the grammar amounts to shared parsing of all the substrings encoded by that nonterminal.", "labels": [], "entities": []}, {"text": "To give a few examples, in some natural language generation systems) nonrecursive CFGs are used to encode very large sets of candidate sentences realizing some input conceptual representation (Langkilde calls such grammars forests).", "labels": [], "entities": []}, {"text": "Each CFG is later \"parsed\" using a language model, in order to rank the sentences in the set according to their likelyhood.", "labels": [], "entities": []}, {"text": "Similarly, in some approaches to automatic speech understanding () the \u00a2 -best sentences obtained from the speech recognition module are \"compressed\" into a non-recursive CFG grammar, which is later provided as input to a parser.", "labels": [], "entities": [{"text": "automatic speech understanding", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.6491213341554006}]}, {"text": "Finally, in some machine translation applications related techniques are exploited to obtain sentences that simultaneously realize two different conceptual representations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7158618867397308}]}, {"text": "This is done in order to produce translations that preserve syntactic or semantic ambiguity in cases where the ambiguity could not be resolved when processing the source sentence.", "labels": [], "entities": []}, {"text": "To be able to describe the above applications in an abstract way, let us first fix some terminology.", "labels": [], "entities": []}, {"text": "The term \"recognition\" refers to the process of deciding whether an input string is in the language described by a grammar, the parsing grammar \u00a3 \u00a5 \u00a4 . We will generalize this notion in a natural way to input representing a set of strings, and here the goal of recognition is to decide whether at least one of the strings in the set is in the language described by are CFGs.", "labels": [], "entities": [{"text": "CFGs", "start_pos": 369, "end_pos": 373, "type": "DATASET", "confidence": 0.915712296962738}]}, {"text": "General CFGs have unfavourable computational properties with respect to intersection.", "labels": [], "entities": []}, {"text": "In particular, the problem of deciding whether the intersection of two CFGs is non-empty is undecidable.", "labels": [], "entities": [{"text": "CFGs", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.8946937918663025}]}, {"text": "Following the terminology adopted above, this means that parsing a context-free input grammar represents a regular language, more precisely an \u00a2 -gram model.", "labels": [], "entities": []}, {"text": "In this way the problem can be solved using a stochastic variant of an algorithm presented by, where it is shown that the intersection of a general context-free language and a regular language is still context-free.", "labels": [], "entities": []}, {"text": "In the present paper we leave the theoretical framework of unfolded into a lattice (acyclic finite automaton) and later parsed with \u00a3 \u00a5 \u00a4 using an algorithm close to the one by.", "labels": [], "entities": []}, {"text": "The algorithm proposed by involves copying of charts, and this makes it very similar in behaviour to the former approach.", "labels": [], "entities": []}, {"text": "Thus in both algorithms parts of the input grammar \u00a3 \u00a7 are copied where a nonterminal occurs more than once, which destroys the compactness of the representation.", "labels": [], "entities": []}, {"text": "In this paper we propose two alternative tabular algorithms that exploit the compactness of \u00a3 \u00a7 as much as possible.", "labels": [], "entities": []}, {"text": "Although a limited amount of copying is also done by our algorithms, this never happens in cases where the resulting structure is ungrammatical with respect to the parsing grammar \u00a3 \u00a4 . The structure of this paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we introduce some preliminary definitions, followed in Section 3 by a first algorithm based on CKY parsing.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 108, "end_pos": 119, "type": "TASK", "confidence": 0.7405069470405579}]}, {"text": "A more sophisticated algorithm, satisfying the equivalent of the correct-prefix property and based on Earley's algorithm, is presented in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents our experimental results and Section 6 closes with some discussion.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}