{"title": [{"text": "Teaching a Weaker Classifier: Named Entity Recognition on Upper Case Text", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.5968400835990906}]}], "abstractContent": [{"text": "This paper describes how a machine-learning named entity recognizer (NER) on uppercase text can be improved by using a mixed case NER and some unlabeled text.", "labels": [], "entities": [{"text": "machine-learning named entity recognizer (NER", "start_pos": 27, "end_pos": 72, "type": "TASK", "confidence": 0.6992291410764059}]}, {"text": "The mixed case NER can be used to tag some unlabeled mixed case text, which are then used as additional training material for the uppercase NER.", "labels": [], "entities": []}, {"text": "We show that this approach reduces the performance gap between the mixed case NER and the uppercase NER substantially, by 39% for MUC-6 and 22% for MUC-7 named entity test data.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 130, "end_pos": 135, "type": "DATASET", "confidence": 0.900574266910553}, {"text": "MUC-7 named entity test data", "start_pos": 148, "end_pos": 176, "type": "DATASET", "confidence": 0.8330272436141968}]}, {"text": "Our method is thus useful in improving the accuracy of NERs on uppercase text, such as transcribed text from automatic speech recognizers where case information is missing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9990975856781006}, {"text": "NERs", "start_pos": 55, "end_pos": 59, "type": "TASK", "confidence": 0.9843457341194153}, {"text": "transcribed text from automatic speech recognizers", "start_pos": 87, "end_pos": 137, "type": "TASK", "confidence": 0.6020686477422714}]}], "introductionContent": [{"text": "In this paper, we propose using a mixed case named entity recognizer (NER) that is trained on labeled text, to further train an uppercase NER.", "labels": [], "entities": []}, {"text": "In the Sixth and Seventh Message Understanding Conferences), the named entity task consists of labeling named entities with the classes PERSON, ORGANIZATION, LOCA-TION, DATE, TIME, MONEY, and PERCENT.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9750401973724365}, {"text": "ORGANIZATION", "start_pos": 144, "end_pos": 156, "type": "METRIC", "confidence": 0.9637740254402161}, {"text": "LOCA-TION", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.935815155506134}, {"text": "DATE", "start_pos": 169, "end_pos": 173, "type": "METRIC", "confidence": 0.9562642574310303}, {"text": "TIME", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.8744474053382874}, {"text": "MONEY", "start_pos": 181, "end_pos": 186, "type": "METRIC", "confidence": 0.9335317611694336}, {"text": "PERCENT", "start_pos": 192, "end_pos": 199, "type": "METRIC", "confidence": 0.9794597029685974}]}, {"text": "We conducted experiments on uppercase named entity recognition, and showed how unlabeled mixed case text can be used to improve the results of an uppercase NER on the official MUC-6 and MUC-7 Mixed Case: Consuela Washington, a longtime House staffer and an expert in securities laws, is a leading candidate to be chairwoman of the Securities and Exchange Commission in the Clinton administration.", "labels": [], "entities": [{"text": "uppercase named entity recognition", "start_pos": 28, "end_pos": 62, "type": "TASK", "confidence": 0.6141549870371819}, {"text": "MUC-6", "start_pos": 176, "end_pos": 181, "type": "DATASET", "confidence": 0.9486488699913025}, {"text": "MUC-7 Mixed Case", "start_pos": 186, "end_pos": 202, "type": "DATASET", "confidence": 0.8255684574445089}]}, {"text": "can also be applied on transcribed text from automatic speech recognizers in Speech Normalized Orthographic Representation (SNOR) format, or from optical character recognition (OCR) output.", "labels": [], "entities": [{"text": "Speech Normalized Orthographic Representation (SNOR) format", "start_pos": 77, "end_pos": 136, "type": "TASK", "confidence": 0.7132009640336037}, {"text": "optical character recognition (OCR) output", "start_pos": 146, "end_pos": 188, "type": "TASK", "confidence": 0.8028819986752102}]}, {"text": "For the English language, a word starting with a capital letter often designates a named entity.", "labels": [], "entities": []}, {"text": "Upper case NERs do not have case information to help them to distinguish named entities from non-named entities.", "labels": [], "entities": [{"text": "NERs", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.8974368572235107}]}, {"text": "When data is sparse, many named entities in the test data would be unknown words.", "labels": [], "entities": []}, {"text": "This makes uppercase named entity recognition more difficult than mixed case.", "labels": [], "entities": [{"text": "uppercase named entity recognition", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.6617888584733009}]}, {"text": "Even a human would experience greater difficulty in annotating uppercase text than mixed case text ().", "labels": [], "entities": []}, {"text": "We propose using a mixed case NER to \"teach\" an uppercase NER, by making use of unlabeled mixed case text.", "labels": [], "entities": []}, {"text": "With the abundance of mixed case un-labeled texts available in so many corpora and on the Internet, it will be easy to apply our approach to improve the performance of NER on uppercase text.", "labels": [], "entities": []}, {"text": "Our approach does not satisfy the usual assumptions of co-training.", "labels": [], "entities": []}, {"text": "Intuitively, however, one would expect some information to be gained from mixed case unlabeled text, where case information is helpful in pointing out new words that could be named entities.", "labels": [], "entities": []}, {"text": "We show empirically that such an approach can indeed improve the performance of an uppercase NER.", "labels": [], "entities": []}, {"text": "In Section 5, we show that for MUC-6, this way of using unlabeled text can bring a relative reduction in errors of 38.68% between the uppercase and mixed case NERs.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.7211452722549438}, {"text": "errors", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9845273494720459}]}, {"text": "For MUC-7 the relative reduction in errors is 22.49%.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.5281075835227966}, {"text": "reduction in", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9207320213317871}, {"text": "errors", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.6864941120147705}]}], "datasetContent": [{"text": "For manually labeled data (corpus C), we used only the official training data provided by the MUC-6 and MUC-7 conferences, i.e., using MUC-6 training data and testing on MUC-6 test data, and using MUC-7 training data and testing on MUC-7 test data.", "labels": [], "entities": [{"text": "MUC-6 and MUC-7 conferences", "start_pos": 94, "end_pos": 121, "type": "DATASET", "confidence": 0.7766139805316925}, {"text": "MUC-6 training data", "start_pos": 135, "end_pos": 154, "type": "DATASET", "confidence": 0.8622965017954508}, {"text": "MUC-6 test data", "start_pos": 170, "end_pos": 185, "type": "DATASET", "confidence": 0.9173912604649862}, {"text": "MUC-7 training data", "start_pos": 197, "end_pos": 216, "type": "DATASET", "confidence": 0.8910326560338339}, {"text": "MUC-7 test data", "start_pos": 232, "end_pos": 247, "type": "DATASET", "confidence": 0.9660720229148865}]}, {"text": "The task definitions for MUC-6 and MUC-7 are not exactly identical, so we could not combine the training data.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.8413812518119812}, {"text": "MUC-7", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.8691776990890503}]}, {"text": "The original MUC-6 training data has a total of approximately 160,000 tokens and The unlabeled text is drawn from the TREC (Text REtrieval Conference) corpus, 1992 Wall Street Journal section.", "labels": [], "entities": [{"text": "MUC-6 training data", "start_pos": 13, "end_pos": 32, "type": "DATASET", "confidence": 0.769358237584432}, {"text": "TREC (Text REtrieval Conference) corpus, 1992 Wall Street Journal section", "start_pos": 118, "end_pos": 191, "type": "DATASET", "confidence": 0.74259830896671}]}, {"text": "We have used a total of 4,893 articles with a total of approximately 2,161,000 tokens.", "labels": [], "entities": []}, {"text": "After example selection, this reduces the number of tokens to approximately 46,000 for MUC-6 and 67,000 for MUC-7. and show the results for MUC-6 and MUC-7 obtained, plotted against the number of unlabeled instances used.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.9524596333503723}, {"text": "MUC-7.", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.9647524952888489}, {"text": "MUC-6", "start_pos": 140, "end_pos": 145, "type": "DATASET", "confidence": 0.9379752278327942}, {"text": "MUC-7", "start_pos": 150, "end_pos": 155, "type": "DATASET", "confidence": 0.8875876069068909}]}, {"text": "As expected, it increases the recall in each domain, as more names or their contexts are learned from unlabeled data.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9995949864387512}]}, {"text": "However, as more unlabeled data is used, precision drops due to the noise introduced in the machine tagged data.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9994542002677917}]}, {"text": "For MUC-6, F-measure performance peaked at the point where 30,000 tokens of machine labeled data are added to the original manually tagged 160,000 tokens.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.7209673523902893}]}, {"text": "For MUC-7, performance peaked at 20,000 tokens of machine labeled data, added to the original manually tagged 180,000 tokens.", "labels": [], "entities": []}, {"text": "The improvements achieved are summarized in.", "labels": [], "entities": []}, {"text": "It is clear from the table that this method of using unlabeled data brings considerable improvement for both MUC-6 and MUC-7 named entity task.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.8525502681732178}, {"text": "MUC-7 named entity task", "start_pos": 119, "end_pos": 142, "type": "DATASET", "confidence": 0.734600841999054}]}, {"text": "The result of the teaching process for MUC-6 is a lot better than that of MUC-7.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8233492374420166}, {"text": "MUC-7", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9419209957122803}]}, {"text": "We think that this is: F-measure on MUC-6 and MUC-7 test data due to the following reasons: Better Mixed Case NER for MUC-6 than MUC-7.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.99928218126297}, {"text": "MUC-6", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.896292507648468}, {"text": "MUC-7 test data", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.930293063322703}, {"text": "Mixed Case NER", "start_pos": 99, "end_pos": 113, "type": "METRIC", "confidence": 0.7254531383514404}, {"text": "MUC-7", "start_pos": 129, "end_pos": 134, "type": "DATASET", "confidence": 0.8498741388320923}]}, {"text": "The mixed case NER trained on the MUC-6 officially released training data achieved an Fmeasure of 93.27% on the official MUC-6 test data, while that of MUC-7 (also trained on only the official MUC-7 training data) achieved an F-measure of only 87.24%.", "labels": [], "entities": [{"text": "MUC-6 officially released training data", "start_pos": 34, "end_pos": 73, "type": "DATASET", "confidence": 0.9240868806838989}, {"text": "Fmeasure", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9992191791534424}, {"text": "MUC-6 test data", "start_pos": 121, "end_pos": 136, "type": "DATASET", "confidence": 0.9827871123949686}, {"text": "MUC-7", "start_pos": 152, "end_pos": 157, "type": "DATASET", "confidence": 0.9732945561408997}, {"text": "MUC-7 training data", "start_pos": 193, "end_pos": 212, "type": "DATASET", "confidence": 0.9380376140276591}, {"text": "F-measure", "start_pos": 226, "end_pos": 235, "type": "METRIC", "confidence": 0.9988685846328735}]}, {"text": "As the mixed case NER is used as the teacher, a bad teacher does not help as much.", "labels": [], "entities": [{"text": "NER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.44032150506973267}]}, {"text": "Another possible cause is that there is a domain shift in MUC-7 for the formal test (training articles are aviation disasters articles and test articles are missile/rocket launch articles).", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.9413123726844788}]}, {"text": "The domain of the MUC-7 test data is also very specific, and hence it might exhibit different properties from the training and the unlabeled data.", "labels": [], "entities": [{"text": "MUC-7 test data", "start_pos": 18, "end_pos": 33, "type": "DATASET", "confidence": 0.9081219633420309}]}, {"text": "The Source of Unlabeled Data.", "labels": [], "entities": []}, {"text": "The unlabeled data used is from the same source as MUC-6, but different for MUC-7 (MUC-6 articles and the unlabeled articles are all Wall Street Journal articles, whereas MUC-7 articles are New York Times articles).", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.9675523042678833}, {"text": "MUC-7", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.9082127213478088}]}], "tableCaptions": [{"text": " Table 3: F-measure on MUC-6 and MUC-7 test data", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9977948665618896}, {"text": "MUC-6", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.8864264488220215}, {"text": "MUC-7 test data", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.9140468041102091}]}]}