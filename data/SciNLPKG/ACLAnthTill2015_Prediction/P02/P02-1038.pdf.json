{"title": [{"text": "Discriminative Training and Maximum Entropy Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.8462447921435038}]}], "abstractContent": [{"text": "We present a framework for statistical machine translation of natural languages based on direct maximum entropy models , which contains the widely used source channel approach as a special case.", "labels": [], "entities": [{"text": "statistical machine translation of natural languages", "start_pos": 27, "end_pos": 79, "type": "TASK", "confidence": 0.7924429674943289}]}, {"text": "All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables.", "labels": [], "entities": []}, {"text": "This approach allows a baseline machine translation system to be extended easily by adding new feature functions.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7126527577638626}]}, {"text": "We show that a baseline statistical machine translation system is significantly improved using this approach.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.6315495471159617}]}], "introductionContent": [{"text": "We are given a source ('French') sentence f J 1 = f 1 , . .", "labels": [], "entities": []}, {"text": ", f j , . .", "labels": [], "entities": []}, {"text": ", f J , which is to be translated into a target ('English') sentence e I 1 = e 1 , . .", "labels": [], "entities": []}, {"text": ", e i , . .", "labels": [], "entities": []}, {"text": ", e I . Among all possible target sentences, we will choose the sentence with the highest probability: 1 The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.", "labels": [], "entities": []}, {"text": "The notational convention will be as follows.", "labels": [], "entities": []}, {"text": "We use the symbol P r(\u00b7) to denote general probability distributions with (nearly) no specific assumptions.", "labels": [], "entities": []}, {"text": "In contrast, for model-based probability distributions, we use the generic symbol p(\u00b7).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Characteristics of training corpus (Train),  manual lexicon (Lex), development corpus (Dev),  test corpus (Test).", "labels": [], "entities": []}, {"text": " Table 2: Effect of maximum entropy training for alignment template approach (WP: word penalty feature,  CLM: class-based language model (five-gram), MX: conventional dictionary).  objective criteria [%]  subjective criteria [%]  SER WER PER mWER BLEU SSER  IER  Baseline(\u03bb m = 1)  86.9 42.8 33.0  37.7  43.9  35.9  39.0  ME  81.7 40.2 28.7  34.6  49.7  32.5  34.8  ME+WP  80.5 38.6 26.9  32.4  54.1  29.9  32.2  ME+WP+CLM  78.1 38.3 26.9  32.1  55.0  29.1  30.9  ME+WP+CLM+MX 77.8 38.4 26.8  31.9  55.2  28.8  30.9", "labels": [], "entities": [{"text": "SER WER PER mWER BLEU SSER  IER  Baseline", "start_pos": 230, "end_pos": 271, "type": "METRIC", "confidence": 0.8119309470057487}]}, {"text": " Table 3: Resulting model scaling factors of maxi- mum entropy training for alignment templates; \u03bb 1 :  trigram language model; \u03bb 2 : alignment template  model, \u03bb 3 : lexicon model, \u03bb 4 : alignment model  (normalized such that  4  m=1 \u03bb m = 4).  ME +WP +CLM +MX  \u03bb 1  0.86 0.98  0.75  0.77  \u03bb 2  2.33 2.05  2.24  2.24  \u03bb 3  0.58 0.72  0.79  0.75  \u03bb 4  0.22 0.25  0.23  0.24  WP  \u00b7  2.6  3.03  2.78  CLM  \u00b7  \u00b7  0.33  0.34  MX  \u00b7  \u00b7  \u00b7  2.92", "labels": [], "entities": []}]}