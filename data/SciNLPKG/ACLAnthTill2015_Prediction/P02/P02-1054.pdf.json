{"title": [{"text": "Is It the Right Answer? Exploiting Web Redundancy for Answer Validation", "labels": [], "entities": [{"text": "Answer Validation", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.9078191220760345}]}], "abstractContent": [{"text": "Answer Validation is an emerging topic in Question Answering, where open domain systems are often required to rank huge amounts of candidate answers.", "labels": [], "entities": [{"text": "Answer Validation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8998944163322449}, {"text": "Question Answering", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.8156917989253998}]}, {"text": "We present a novel approach to answer validation based on the intuition that the amount of implicit knowledge which connects an answer to a question can be quantitatively estimated by exploiting the redundancy of Web information.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8967534899711609}]}, {"text": "Experiments carried out on the TREC-2001 judged-answer collection show that the approach achieves a high level of performance (i.e. 81% success rate).", "labels": [], "entities": [{"text": "TREC-2001 judged-answer collection", "start_pos": 31, "end_pos": 65, "type": "DATASET", "confidence": 0.8376357356707255}]}, {"text": "The simplicity and the efficiency of this approach make it suitable to be used as a module in Question Answering systems.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.802380383014679}]}], "introductionContent": [{"text": "Open domain question-answering (QA) systems search for answers to a natural language question either on the Web or in a local document collection.", "labels": [], "entities": []}, {"text": "Different techniques, varying from surface patterns) to deep semantic analysis, are used to extract the text fragments containing candidate answers.", "labels": [], "entities": []}, {"text": "Several systems apply answer validation techniques with the goal of filtering out improper candidates by checking how adequate a candidate answer is with respect to a given question.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7836973369121552}]}, {"text": "These approaches rely on discovering semantic relations between the question and the answer.", "labels": [], "entities": []}, {"text": "As an example,) describes answer validation as an abductive inference process, where an answer is valid with respect to a question if an explanation for it, based on background knowledge, can be found.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.8808304965496063}]}, {"text": "Although theoretically well motivated, the use of semantic techniques on open domain tasks is quite expensive both in terms of the involved linguistic resources and in terms of computational complexity, thus motivating a research on alternative solutions to the problem.", "labels": [], "entities": []}, {"text": "This paper presents a novel approach to answer validation based on the intuition that the amount of implicit knowledge which connects an answer to a question can be quantitatively estimated by exploiting the redundancy of Web information.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.9319883286952972}]}, {"text": "The hypothesis is that the number of documents that can be retrieved from the Web in which the question and the answer co-occur can be considered a significant clue of the validity of the answer.", "labels": [], "entities": []}, {"text": "Documents are searched in the Web by means of validation patterns, which are derived from a linguistic processing of the question and the answer.", "labels": [], "entities": []}, {"text": "In order to test this idea a system for automatic answer validation has been implemented and a number of experiments have been carried out on questions and answers provided by the TREC-2001 participants.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.654980018734932}]}, {"text": "The advantages of this approach are its simplicity on the one hand and its efficiency on the other.", "labels": [], "entities": []}, {"text": "Automatic techniques for answer validation are of great interest for the development of open domain QA systems.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.9121390581130981}]}, {"text": "The availability of a completely automatic evaluation procedure makes it feasible QA systems based on generate and test approaches.", "labels": [], "entities": [{"text": "QA", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9595240354537964}]}, {"text": "In this way, until a given answer is automatically proved to be correct fora question, the system will carryout different refinements of its searching criteria checking the relevance of new candidate answers.", "labels": [], "entities": []}, {"text": "In addition, given that most of the QA systems rely on complex architectures and the evaluation of their performances requires a huge amount of work, the automatic assessment of the relevance of an answer with respect to a given question will speedup both algorithm refinement and testing.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the main features of the approach.", "labels": [], "entities": []}, {"text": "Section 3 describes how validation patterns are extracted from a question-answer pair by means of specific question answering techniques.", "labels": [], "entities": [{"text": "validation patterns", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8891771137714386}, {"text": "question answering", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7383600473403931}]}, {"text": "Section 4 explains the basic algorithm for estimating the answer validity score.", "labels": [], "entities": [{"text": "answer validity score", "start_pos": 58, "end_pos": 79, "type": "METRIC", "confidence": 0.5820085604985555}]}, {"text": "Section 5 gives the results of a number of experiments and discusses them.", "labels": [], "entities": []}, {"text": "Finally, Section 6 puts our approach in the context of related works.", "labels": [], "entities": []}], "datasetContent": [{"text": "A number of experiments have been carried out in order to check the validity of the proposed answer validation technique.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7883021533489227}]}, {"text": "As a data set, the 492 questions of the TREC-2001 database have been used.", "labels": [], "entities": [{"text": "TREC-2001 database", "start_pos": 40, "end_pos": 58, "type": "DATASET", "confidence": 0.9163588881492615}]}, {"text": "For each question, at most three correct answers and three wrong answers have been randomly selected from the TREC-2001 participants' submissions, resulting in a corpus of 2726 question-answer pairs (some question have less than three positive answers in the corpus).", "labels": [], "entities": []}, {"text": "As said before, AltaVista was used as search engine.", "labels": [], "entities": []}, {"text": "A baseline for the answer validation experiment was defined by considering how often an answer occurs in the top 10 documents among those (1000 for each question) provided by NIST to participants.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.8208408057689667}, {"text": "NIST", "start_pos": 175, "end_pos": 179, "type": "DATASET", "confidence": 0.934471845626831}]}, {"text": "An answer was judged correct fora question if it appears at least onetime in the first 10 documents retrieved for that question, otherwise it was judged not correct.", "labels": [], "entities": []}, {"text": "Baseline results are reported in.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9524142742156982}]}, {"text": "We carried out several experiments in order to check a number of working hypotheses.", "labels": [], "entities": []}, {"text": "Three independent factors were considered: Estimation method.", "labels": [], "entities": [{"text": "Estimation", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9940094947814941}]}, {"text": "We have implemented three measures (reported in Section 4.2) to estimate an answer validity score: PMI, MLHR and CCP.", "labels": [], "entities": [{"text": "answer validity score", "start_pos": 76, "end_pos": 97, "type": "METRIC", "confidence": 0.6740319927533468}, {"text": "PMI", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.8609189391136169}, {"text": "MLHR", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9256725907325745}]}, {"text": "We wanted to estimate the role of two different kinds of thresholds for the assessment of answer validation.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7637905478477478}]}, {"text": "In the case of an absolute threshold, if the answer validity score fora candidate answer is below the threshold, the answer is considered wrong, otherwise it is accepted as relevant.", "labels": [], "entities": []}, {"text": "Ina second type of experiment, for every question and its corresponding answers the program chooses the answer with the highest validity score and calculates a relative threshold on that basis ).", "labels": [], "entities": [{"text": "validity score", "start_pos": 128, "end_pos": 142, "type": "METRIC", "confidence": 0.9590688347816467}]}, {"text": "However the relative threshold should be larger than a certain minimum value.", "labels": [], "entities": [{"text": "relative threshold", "start_pos": 12, "end_pos": 30, "type": "METRIC", "confidence": 0.9201210737228394}]}, {"text": "We wanted to check performance variation based on different types of TREC-2001 questions.", "labels": [], "entities": []}, {"text": "In particular, we have separated definition and generic questions from true named entities questions.", "labels": [], "entities": []}, {"text": "report the results of the automatic answer validation experiments obtained respectively on all the TREC-2001 questions and on the subset of definition and generic questions.", "labels": [], "entities": [{"text": "TREC-2001 questions", "start_pos": 99, "end_pos": 118, "type": "DATASET", "confidence": 0.8890041708946228}]}, {"text": "For each estimation method we report precision, recall and success rate.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9997904896736145}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.99974125623703}]}, {"text": "Success rate best represents the performance of the system, being the percent of pairs where the result given by the system is the same as the TREC judges' opinion.", "labels": [], "entities": [{"text": "Success rate", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9733023941516876}, {"text": "TREC", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.5779917240142822}]}, {"text": "Precision is the percent of pairs estimated by the algorithm as relevant, for which the opinion of TREC judges was the same.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9956529140472412}, {"text": "TREC", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.673244059085846}]}, {"text": "Recall shows the percent of the relevant answers which the system also evaluates as relevant.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9117724299430847}]}, {"text": "The best results on the 492 questions corpus (CCP measure with relative threshold) show a success rate of 81.25%, i.e. in 81.25% of the pairs the system evaluation corresponds to the human evaluation, and confirms the initial working hypotheses.", "labels": [], "entities": [{"text": "492 questions corpus (CCP measure", "start_pos": 24, "end_pos": 57, "type": "DATASET", "confidence": 0.6944820682207743}]}, {"text": "This is 28% above the baseline success rate.", "labels": [], "entities": []}, {"text": "Precision and recall are respectively 20-30% and 68-87% above the baseline values.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9952386617660522}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9996719360351562}]}, {"text": "These results demonstrate that the intuition behind the approach is motivated and that the algorithm provides a workable solution for answer validation.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.8722761571407318}]}], "tableCaptions": [{"text": " Table 2: Results on all 492 TREC-2001 questions", "labels": [], "entities": [{"text": "492 TREC-2001 questions", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.5767738024393717}]}, {"text": " Table 3: Results on 249 named entity questions", "labels": [], "entities": []}]}