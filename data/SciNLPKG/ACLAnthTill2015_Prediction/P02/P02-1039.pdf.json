{"title": [{"text": "A Decoder for Syntax-based Statistical MT", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.7479206323623657}]}], "abstractContent": [{"text": "This paper describes a decoding algorithm fora syntax-based translation model (Ya-mada and Knight, 2001).", "labels": [], "entities": []}, {"text": "The model has been extended to incorporate phrasal translations as presented here.", "labels": [], "entities": []}, {"text": "In contrast to a conventional word-to-word statistical model, a decoder for the syntax-based model builds up an English parse tree given a sentence in a foreign language.", "labels": [], "entities": []}, {"text": "As the model size becomes huge in a practical setting, and the decoder considers multiple syntactic structures for each word alignment, several pruning techniques are necessary.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 120, "end_pos": 134, "type": "TASK", "confidence": 0.7343024015426636}]}, {"text": "We tested our de-coder in a Chinese-to-English translation system, and obtained better results than IBM Model 4.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.6734063476324081}]}, {"text": "We also discuss issues concerning the relation between this decoder and a language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "A statistical machine translation system based on the noisy channel model consists of three components: a language model (LM), a translation model (TM), and a decoder.  are not simple probability tables but are parameterized models, a decoder must conduct a search over the space defined by the models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 2, "end_pos": 33, "type": "TASK", "confidence": 0.6448235313097636}]}, {"text": "For the IBM models defined by a pioneering paper (, a decoding algorithm based on a left-to-right search was described in.", "labels": [], "entities": []}, {"text": "Recently) introduced a syntax-based TM which utilized syntactic structure in the channel input, and showed that it could outperform the IBM model in alignment quality.", "labels": [], "entities": []}, {"text": "In contrast to the IBM models, which are word-to-word models, the syntax-based model works on a syntactic parse tree, so the decoder builds up an English parse tree given a sentence in a foreign language.", "labels": [], "entities": []}, {"text": "This paper describes an algorithm for such a decoder, and reports experimental results.", "labels": [], "entities": []}, {"text": "Other statistical machine translation systems such as (Wu, 1997) and () also produce a tree given a sentence . unlike the noisy channel model.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 6, "end_pos": 37, "type": "TASK", "confidence": 0.6305893758932749}]}, {"text": "Section 2 briefly reviews the syntax-based TM, and Section 3 describes phrasal translation as an extension.", "labels": [], "entities": [{"text": "TM", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.732075572013855}, {"text": "phrasal translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.6790154278278351}]}, {"text": "Section 4 presents the basic idea for decoding.", "labels": [], "entities": []}, {"text": "As in other statistical machine translation systems, the decoder has to cope with a huge search space.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.6412633756796519}]}, {"text": "Section 5 describes how to prune the search space for practical decoding.", "labels": [], "entities": []}, {"text": "Section 6 shows experimental results.", "labels": [], "entities": []}, {"text": "Section 7 discusses LM issues, and is followed by conclusions.", "labels": [], "entities": [{"text": "LM", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9628043174743652}]}], "datasetContent": [{"text": "This section describes results from our experiment using the decoder as described in the previous section.", "labels": [], "entities": []}, {"text": "We used a Chinese-English translation corpus for the experiment.", "labels": [], "entities": []}, {"text": "After discarding long sentences (more than 20 words in English), the English side of the corpus consisted of about 3M words, and it was parsed with.", "labels": [], "entities": []}, {"text": "Training the TM took about 8 hours using a 54-node unix cluster.", "labels": [], "entities": [{"text": "TM", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9560048580169678}]}, {"text": "We selected 347 short sentences (less than 14 words in the reference English translation) from the held-out portion of the corpus, and they were used for evaluation.", "labels": [], "entities": []}, {"text": "shows the decoding performance for the test sentences.", "labels": [], "entities": []}, {"text": "The first system ibm4 is a reference system, which is based on IBM Model4.", "labels": [], "entities": [{"text": "IBM Model4", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.9336089789867401}]}, {"text": "The second and the third (syn and syn-nozf) are our decoders.", "labels": [], "entities": []}, {"text": "Both used the same decoding algorithm and pruning as described in the previous sections, except that syn-nozf allowed no zero-fertility insertions.", "labels": [], "entities": []}, {"text": "The average decoding speed was about 100 seconds 6 per sentence for both syn and syn-nozf.", "labels": [], "entities": []}, {"text": "As an overall decoding performance measure, we used the BLEU metric ().", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 56, "end_pos": 67, "type": "METRIC", "confidence": 0.973113089799881}]}, {"text": "This measure is a geometric average of n-gram accuracy, adjusted by a length penalty factor LP.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9828330874443054}, {"text": "length penalty factor LP", "start_pos": 70, "end_pos": 94, "type": "METRIC", "confidence": 0.9592369347810745}]}, {"text": "7 The n-gram accuracy (in percentage) is shown in as P1/P2/P3/P4 for unigram/bigram/trigram/4-gram.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9759978652000427}]}, {"text": "Overall, our decoder performed better than the IBM system, as indicated by the higher BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.965783566236496}]}, {"text": "We obtained better n-gram accuracy, but the lower LP score penalized the overall score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9869764447212219}, {"text": "LP score", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9739874303340912}]}, {"text": "Interestingly, the system with no explicit zero-fertility word insertion (syn-nozf) performed better than the one with zerofertility insertion (syn).", "labels": [], "entities": []}, {"text": "It seems that most zerofertility words were already included in the phrasal translations, and the explicit zero-fertility word insertion produced more garbage than expected words.", "labels": [], "entities": [{"text": "zero-fertility word insertion", "start_pos": 107, "end_pos": 136, "type": "TASK", "confidence": 0.6295507152875265}]}, {"text": "To verify that the pruning was effective, we relaxed the pruning threshold and checked the decoding coverage for the first 92 sentences of the test data.", "labels": [], "entities": []}, {"text": "On the left, the r-table pruning was relaxed from the 95% level to 98% or 100%.", "labels": [], "entities": []}, {"text": "On the right, the t-table pruning was relaxed from the top-5 (o , \u00f8 ) pairs to the top-10 or top-20 pairs.", "labels": [], "entities": []}, {"text": "The system r95 and w5 are identical to syn-nozf in.", "labels": [], "entities": []}, {"text": "When r-table pruning was relaxed from 95% to 98%, only about half (47/92) of the test sentences were decoded, others were aborted due to lack of memory.", "labels": [], "entities": []}, {"text": "When it was further relaxed to 100% (i.e., no pruning was done), only 20 sentences were decoded.", "labels": [], "entities": []}, {"text": "Similarly, when the t-table pruning threshold was relaxed, fewer sentences could be decoded due to the memory limitations.", "labels": [], "entities": []}, {"text": "Although our decoder performed better than the Using a single-CPU 800Mhz Pentium III unix system with 1GB memory.", "labels": [], "entities": []}, {"text": "IBM system in the BLEU score, the obtained gain was less than what we expected.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9639610350131989}]}, {"text": "We have thought the following three reasons.", "labels": [], "entities": []}, {"text": "First, the syntax of Chinese is not extremely different from English, compared with other languages such as Japanese or Arabic.", "labels": [], "entities": []}, {"text": "Therefore, the TM could not take advantage of syntactic reordering operations.", "labels": [], "entities": [{"text": "TM", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.8913734555244446}]}, {"text": "Second, our decoder looks fora decoded tree, not just fora decoded sentence.", "labels": [], "entities": []}, {"text": "Thus, the search space is larger than IBM models, which might lead to more search errors caused by pruning.", "labels": [], "entities": []}, {"text": "Third, the LM used for our system was exactly the same as the LM used by the IBM system.", "labels": [], "entities": []}, {"text": "Decoding performance might be heavily influenced by LM performance.", "labels": [], "entities": []}, {"text": "In addition, since the TM assumes an English parse tree as input, a trigram LM might not be appropriate.", "labels": [], "entities": []}, {"text": "We will discuss this point in the next section.", "labels": [], "entities": []}, {"text": "Phrasal translation worked pretty well.", "labels": [], "entities": [{"text": "Phrasal translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7293676137924194}]}, {"text": "shows the top-20 frequent phrase translations observed in the Viterbi alignment.", "labels": [], "entities": [{"text": "phrase translations", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7111757844686508}, {"text": "Viterbi alignment", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.8481864929199219}]}, {"text": "The leftmost column shows how many times they appeared.", "labels": [], "entities": []}, {"text": "Most of them are correct.", "labels": [], "entities": []}, {"text": "It even detected frequent sentenceto-sentence translations, since we only imposed a relative length limit for phrasal translations (Section 3).", "labels": [], "entities": [{"text": "sentenceto-sentence translations", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.7291601300239563}]}, {"text": "However, some of them, such as the one with (in cantonese), are wrong.", "labels": [], "entities": []}, {"text": "We expected that these junk phrases could be eliminated by phrase pruning (Section 5), however the junk phrases present many times in the corpus were not effectively filtered out.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Effect of pruning", "labels": [], "entities": []}]}