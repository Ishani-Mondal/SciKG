{"title": [{"text": "Improving Machine Learning Approaches to Coreference Resolution", "labels": [], "entities": [{"text": "Improving Machine Learning Approaches", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.900745689868927}, {"text": "Coreference Resolution", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.8547998368740082}]}], "abstractContent": [{"text": "We present a noun phrase coreference system that extends the work of Soon et al.", "labels": [], "entities": [{"text": "noun phrase coreference", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.6167062024275461}]}, {"text": "(2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets-F-measures of 70.4 and 63.4, respectively.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.9700585007667542}, {"text": "MUC-7 coreference resolution data sets-F-measures", "start_pos": 81, "end_pos": 130, "type": "DATASET", "confidence": 0.849763286113739}]}, {"text": "Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.", "labels": [], "entities": []}], "introductionContent": [{"text": "Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document.", "labels": [], "entities": [{"text": "Noun phrase coreference resolution", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7470698207616806}]}, {"text": "Machine learning approaches to this problem have been reasonably successful, operating primarily by recasting the problem as a classification task (e.g.,).", "labels": [], "entities": []}, {"text": "Specifically, a pair of NPs is classified as co-referring or not based on constraints that are learned from an annotated corpus.", "labels": [], "entities": []}, {"text": "A separate clustering mechanism then coordinates the possibly contradictory pairwise classifications and constructs a partition on the set of NPs., for example, apply an NP coreference system based on decision tree induction to two standard coreference resolution data sets), achieving performance comparable to the best-performing knowledge-based coreference engines.", "labels": [], "entities": []}, {"text": "Perhaps surprisingly, this was accomplished in a decidedly knowledge-lean manner -the learning algorithm has access to just 12 surface-level features.", "labels": [], "entities": []}, {"text": "This paper presents an NP coreference system that investigates two types of extensions to the Soon et al. corpus-based approach.", "labels": [], "entities": [{"text": "NP coreference", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.8289219737052917}]}, {"text": "First, we propose and evaluate three extra-linguistic modifications to the machine learning framework, which together provide substantial and statistically significant gains in coreference resolution precision.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 177, "end_pos": 199, "type": "TASK", "confidence": 0.9106696844100952}, {"text": "precision", "start_pos": 200, "end_pos": 209, "type": "METRIC", "confidence": 0.8811091184616089}]}, {"text": "Second, in an attempt to understand whether incorporating additional knowledge can improve the performance of a corpus-based coreference resolution system, we expand the Soon et al. feature set from 12 features to an arguably deeper set of 53.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 125, "end_pos": 147, "type": "TASK", "confidence": 0.8221762776374817}]}, {"text": "We propose additional lexical, semantic, and knowledge-based features; most notably, however, we propose 26 additional grammatical features that include a variety of linguistic constraints and preferences.", "labels": [], "entities": []}, {"text": "Although the use of similar knowledge sources has been explored in the context of both pronoun resolution (e.g.) and NP coreference resolution (e.g.,), most previous work treats linguistic constraints as broadly and unconditionally applicable hard constraints.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7465769350528717}, {"text": "NP coreference resolution", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.7761978308359782}]}, {"text": "Because sources of linguistic information in a learning-based system are represented as features, we can, in contrast, incorporate them selectively rather than as universal hard constraints.", "labels": [], "entities": []}, {"text": "Our results using an expanded feature set are mixed.", "labels": [], "entities": []}, {"text": "First, we find that performance drops significantly when using the full feature set, even though the learning algorithms investigated have built-in feature selection mechanisms.", "labels": [], "entities": []}, {"text": "We demonstrate em-pirically that the degradation in performance can be attributed, at least in part, to poor performance on common noun resolution.", "labels": [], "entities": [{"text": "common noun resolution", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.5960531731446584}]}, {"text": "A manually selected subset of 22-26 features, however, is shown to provide significant gains in performance when chosen specifically to improve precision on common noun resolution.", "labels": [], "entities": [{"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9968026876449585}, {"text": "common noun resolution", "start_pos": 157, "end_pos": 179, "type": "TASK", "confidence": 0.5776326457659403}]}, {"text": "Overall, the learning framework and linguistic knowledge source modifications boost performance of Soon's learning-based coreference resolution approach from an F-measure of 62.6 to 70.4, and from 60.4 to 63.4 for the MUC-6 and MUC-7 data sets, respectively.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.6984610110521317}, {"text": "F-measure", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9989215135574341}, {"text": "MUC-6", "start_pos": 218, "end_pos": 223, "type": "DATASET", "confidence": 0.9444828033447266}, {"text": "MUC-7 data sets", "start_pos": 228, "end_pos": 243, "type": "DATASET", "confidence": 0.9083252549171448}]}, {"text": "To our knowledge, these are the best results reported to date on these data sets for the full NP coreference problem.", "labels": [], "entities": [{"text": "NP coreference problem", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.9234356681505839}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In sections 2 and 3, we present the baseline coreference system and explore extra-linguistic modifications to the machine learning framework.", "labels": [], "entities": []}, {"text": "Section 4 describes and evaluates the expanded feature set.", "labels": [], "entities": []}, {"text": "We conclude with related and future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the Duplicated Soon Baseline system using the standard MUC-6 (1995) and MUC-7 (1998) coreference corpora, training the coreference classifier on the 30 \"dry run\" texts, and applying the coreference resolution algorithm on the 20-30 \"formal evaluation\" texts.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9499897360801697}, {"text": "MUC-7 (1998) coreference corpora", "start_pos": 84, "end_pos": 116, "type": "DATASET", "confidence": 0.8036137322584788}, {"text": "coreference resolution", "start_pos": 198, "end_pos": 220, "type": "TASK", "confidence": 0.8303763568401337}]}, {"text": "The MUC-6 corpus produces a training set of 26455 instances (5.4% positive) from 4381 NPs and a test set of 28443 instances (5.2% positive) from 4565 NPs.", "labels": [], "entities": [{"text": "MUC-6 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9696826338768005}]}, {"text": "For the MUC-7 corpus, we obtain a training set of 35895 instances (4.4% positive) from 5270 NPs and a test set of 22699 instances (3.9% positive) from 3558 NPs.", "labels": [], "entities": [{"text": "MUC-7 corpus", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.9105606079101562}]}, {"text": "Results are shown in (Duplicated Soon Baseline) where performance is reported in terms of recall, precision, and F-measure using the modeltheoretic MUC scoring program ( Y if NP$ starts with a demonstrative such as \"this,\" \"that,\" \"these,\" or \"those;\" else N.", "labels": [], "entities": [{"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9992582201957703}, {"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9991052746772766}, {"text": "F-measure", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9991024732589722}]}], "tableCaptions": [{"text": " Table 2: Results for the MUC-6 and MUC-7 data sets using C4.5 and RIPPER. Recall, Precision, and F-measure", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.8421408534049988}, {"text": "MUC-7 data sets", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.8969698746999105}, {"text": "RIPPER", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9799116253852844}, {"text": "Recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9923338890075684}, {"text": "Precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.998336911201477}, {"text": "F-measure", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.998390793800354}]}]}