{"title": [{"text": "A Generative Constituent-Context Model for Improved Grammar Induction", "labels": [], "entities": [{"text": "Improved Grammar Induction", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.7191327611605326}]}], "abstractContent": [{"text": "We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts.", "labels": [], "entities": []}, {"text": "Parameter search with EM produces higher quality analyses than previously exhibited by un-supervised systems, giving the best published un-supervised parsing results on the ATIS corpus.", "labels": [], "entities": [{"text": "Parameter search", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8468841016292572}, {"text": "ATIS corpus", "start_pos": 173, "end_pos": 184, "type": "DATASET", "confidence": 0.9800441563129425}]}, {"text": "Experiments on Penn treebank sentences of comparable length show an even higher F 1 of 71% on non-trivial brackets.", "labels": [], "entities": [{"text": "Penn treebank sentences", "start_pos": 15, "end_pos": 38, "type": "DATASET", "confidence": 0.9715152780214945}, {"text": "F 1", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9935964643955231}]}, {"text": "We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model.", "labels": [], "entities": []}, {"text": "We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of inducing hierarchical syntactic structure from observed yields alone has received a great deal of attention.", "labels": [], "entities": []}, {"text": "Researchers have explored this problem fora variety of reasons: to argue empirically against the poverty of the stimulus, to use induction systems as a first stage in constructing large treebanks), or to build better language models.", "labels": [], "entities": []}, {"text": "In previous work, we presented a conditional model over trees which gave the best published results for unsupervised parsing of the ATIS corpus ().", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.9140225052833557}]}, {"text": "However, it suffered from several drawbacks, primarily stemming from the conditional model used for induction.", "labels": [], "entities": []}, {"text": "Here, we improve on that model in several ways.", "labels": [], "entities": []}, {"text": "First, we construct a generative model which utilizes the same features.", "labels": [], "entities": []}, {"text": "Then, we extend the model to allow multiple constituent types and multiple prior distributions over trees.", "labels": [], "entities": []}, {"text": "The new model gives a 13% reduction in parsing error on WSJ sentence experiments, including a positive qualitative shift in error types.", "labels": [], "entities": [{"text": "parsing error", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.7967363893985748}]}, {"text": "Additionally, it produces much more stable results, does not require heavy smoothing, and exhibits a reliable correspondence between the maximized objective and parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 161, "end_pos": 168, "type": "TASK", "confidence": 0.961736261844635}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.6090158224105835}]}, {"text": "It is also much faster, not requiring a fitting phase for each iteration. and take treebank part-of-speech sequences as input.", "labels": [], "entities": []}, {"text": "We followed this for most experiments, but in section 4.3, we use distributionally induced tags as input.", "labels": [], "entities": []}, {"text": "Performance with induced tags is somewhat reduced, but still gives better performance than previous models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed most experiments on the 7422 sentences in the Penn treebank Wall Street Journal section which contained no more than 10 words after the removal of punctuation and null elements (WSJ-10).", "labels": [], "entities": [{"text": "Penn treebank Wall Street Journal section", "start_pos": 59, "end_pos": 100, "type": "DATASET", "confidence": 0.9383580883344015}, {"text": "WSJ-10", "start_pos": 191, "end_pos": 197, "type": "DATASET", "confidence": 0.8693898320198059}]}, {"text": "Evaluation was done by measuring unlabeled precision, recall, and their harmonic mean F 1 against the treebank parses.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.994304358959198}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9994863271713257}, {"text": "harmonic mean F 1", "start_pos": 72, "end_pos": 89, "type": "METRIC", "confidence": 0.7898247689008713}]}, {"text": "Constituents which could not begotten wrong (single words and entire sentences) were discarded.", "labels": [], "entities": []}, {"text": "The basic experiments, as described above, do not label constituents.", "labels": [], "entities": []}, {"text": "An advantage to having only a single constituent class is that it encourages constituents of one type to be found even when they occur in a context which canonically holds another type.", "labels": [], "entities": []}, {"text": "For example, NPs and PPs both occur between a verb and the end of the sentence, and they can transfer constituency to each other through that context.", "labels": [], "entities": []}, {"text": "shows the F 1 score for various methods of parsing.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9893229405085245}, {"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9651376605033875}]}, {"text": "RANDOM chooses a tree uniformly Since reproducible evaluation is important, a few more notes: this is different from the original (unlabeled) bracketing measures proposed in the PARSEVAL standard, which did not count single words as constituents, but did give points for putting a bracket over the entire sentence.", "labels": [], "entities": []}, {"text": "Secondly, bracket labels and multiplicity are just ignored.", "labels": [], "entities": []}, {"text": "Below, we also present results using the EVALB program for comparability, but we note that while one can get results from it that ignore bracket labels, it never ignores bracket multiplicity.", "labels": [], "entities": []}, {"text": "Both these alternatives seem less satisfactory to us as measures for evaluating unsupervised constituency decisions.", "labels": [], "entities": []}, {"text": "Figure 5: Accuracy scores for CCM-induced structures by span size.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9973244667053223}]}, {"text": "The drop in precision for span length 2 is largely due to analysis inside NPs which is omitted by the treebank.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9993945360183716}]}, {"text": "Also shown is F 1 for the induced PCFG.", "labels": [], "entities": [{"text": "F 1", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9941231906414032}, {"text": "PCFG", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.8248975872993469}]}, {"text": "The PCFG shows higher accuracy on small spans, while the CCM is more even.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8838452696800232}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9993718266487122}]}, {"text": "at random from the set of binary trees.", "labels": [], "entities": []}, {"text": "4 This is the unsupervised baseline.", "labels": [], "entities": []}, {"text": "DEP-PCFG is the result of duplicating the experiments of, using EM to train a dependencystructured PCFG.", "labels": [], "entities": [{"text": "DEP-PCFG", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.762954592704773}]}, {"text": "LBRANCH and RBRANCH choose the left-and right-branching structures, respectively.", "labels": [], "entities": [{"text": "LBRANCH", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8407687544822693}, {"text": "RBRANCH", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9925923943519592}]}, {"text": "RBRANCH is a frequently used baseline for supervised parsing, but it should be stressed that it encodes a significant fact about English structure, and an induction system need not beat it to claim a degree of success.", "labels": [], "entities": [{"text": "RBRANCH", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.926369845867157}, {"text": "supervised parsing", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.5168232619762421}]}, {"text": "CCM is our system, as described above.", "labels": [], "entities": [{"text": "CCM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9149945974349976}]}, {"text": "SUP-PCFG is a supervised PCFG parser trained on a 90-10 split of this data, using the treebank grammar, with the Viterbi parse rightbinarized.", "labels": [], "entities": []}, {"text": "5 UBOUND is the upper bound of how well a binary system can do against the treebank sentences, which are generally flatter than binary, limiting the maximum precision.", "labels": [], "entities": [{"text": "UBOUND", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9890266060829163}, {"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9964937567710876}]}, {"text": "CCM is doing quite well at 71.1%, substantially better than right-branching structure.", "labels": [], "entities": [{"text": "CCM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7969515323638916}]}, {"text": "One common issue with grammar induction systems is a tendency to chunk in a bottom-up fashion.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7021541148424149}]}, {"text": "Especially since the CCM does not model recursive structure explicitly, one might be concerned that the high overall accuracy is due to a high accuracy on short-span constituents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9979408383369446}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9927600622177124}]}, {"text": "shows that this is not true.", "labels": [], "entities": []}, {"text": "Recall drops slightly for mid-size constituents, but longer constituents are as reliably proposed as short ones.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9908806085586548}]}, {"text": "Another effect illustrated in this graph is that, for span 2, constituents have low precision for their recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9988917708396912}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.980455756187439}]}, {"text": "This contrast is primarily due to the single largest difference between the system's induced structures and those in the treebank: the treebank does not parse into NPs such as DT JJ NN, while our system does, and generally does so correctly, identifying N units like JJ NN.", "labels": [], "entities": []}, {"text": "This overproposal drops span-2 precision.", "labels": [], "entities": [{"text": "span-2", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9745368957519531}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.8949301242828369}]}, {"text": "In contrast, figure 5 also shows the F 1 for DEP-PCFG, which does exhibit a drop in F 1 over larger spans.", "labels": [], "entities": [{"text": "F 1", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9898074567317963}, {"text": "DEP-PCFG", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.8686478734016418}, {"text": "F 1", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.989946722984314}]}, {"text": "The top row of shows the recall of nontrivial brackets, split according the brackets' labels in the treebank.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9974194765090942}]}, {"text": "Unsurprisingly, NP recall is highest, but other categories are also high.", "labels": [], "entities": [{"text": "NP", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9203121066093445}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9163205027580261}]}, {"text": "Because we ignore trivial constituents, the comparatively low S represents only embedded sentences, which are somewhat harder even for supervised systems.", "labels": [], "entities": []}, {"text": "To facilitate comparison to other recent work, figure 6 shows the accuracy of our system when trained on the same WSJ data, but tested on the ATIS corpus, and evaluated according to the EVALB program.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9995872378349304}, {"text": "WSJ data", "start_pos": 114, "end_pos": 122, "type": "DATASET", "confidence": 0.9313808381557465}, {"text": "ATIS corpus", "start_pos": 142, "end_pos": 153, "type": "DATASET", "confidence": 0.9809962809085846}, {"text": "EVALB program", "start_pos": 186, "end_pos": 199, "type": "DATASET", "confidence": 0.8764105439186096}]}, {"text": "The F 1 numbers are lower for this corpus and evaluation method.", "labels": [], "entities": [{"text": "F 1 numbers", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9407995939254761}]}, {"text": "Still, CCM beats not only RBRANCH (by 8.3%), but also the previous conditional COND-CCM and the next closest unsupervised system (which does not beat RBRANCH in F 1 ).", "labels": [], "entities": [{"text": "RBRANCH", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9926512837409973}, {"text": "COND-CCM", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9716785550117493}, {"text": "RBRANCH", "start_pos": 150, "end_pos": 157, "type": "METRIC", "confidence": 0.8756396770477295}, {"text": "F 1", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.8422325849533081}]}, {"text": "6 EMILE and ABL are lexical systems described in).", "labels": [], "entities": [{"text": "EMILE", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9976840019226074}, {"text": "ABL", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.993683934211731}]}, {"text": "CDC-40, from, reflects training on much more data (12M words).", "labels": [], "entities": [{"text": "CDC-40", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9433237910270691}]}, {"text": "The primary cause of the lower F 1 is that the ATIS corpus is replete with span-one NPs; adding an extra bracket around all single words raises our EVALB recall to 71.9; removing all unaries from the ATIS gold standard gives an F: Constituents most frequently over-and underproposed by our system.", "labels": [], "entities": [{"text": "F 1", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9870690107345581}, {"text": "ATIS corpus", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.9745276570320129}, {"text": "EVALB", "start_pos": 148, "end_pos": 153, "type": "METRIC", "confidence": 0.981599748134613}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.5007613897323608}, {"text": "ATIS gold standard", "start_pos": 200, "end_pos": 218, "type": "DATASET", "confidence": 0.9498348434766134}]}], "tableCaptions": []}