{"title": [{"text": "Combining Unsupervised Lexical Knowledge Methods for Word Sense Disambiguation *", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7867699066797892}]}], "abstractContent": [{"text": "This paper presents a method to combine a set of unsupervised algorithms that can accurately disambiguate word senses in a large, completely untagged corpus.", "labels": [], "entities": []}, {"text": "Although most of the techniques for word sense resolution have been presented as stand-alone, it is our belief that full-fledged lexical ambiguity resolution should combine several information sources and techniques.", "labels": [], "entities": [{"text": "word sense resolution", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.8167168895403544}, {"text": "full-fledged lexical ambiguity resolution", "start_pos": 116, "end_pos": 157, "type": "TASK", "confidence": 0.6427945047616959}]}, {"text": "The set of techniques have been applied in a combined way to disambiguate the genus terms of two machine-readable dictionaries (MRD), enabling us to construct complete taxonomies for Spanish and French.", "labels": [], "entities": []}, {"text": "Tested accuracy is above 80% overall and 95% for two-way ambiguous genus terms, showing that taxonomy building is not limited to structured dictionaries such as LDOCE.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9996032118797302}, {"text": "taxonomy building", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.8904410600662231}]}], "introductionContent": [{"text": "While in English the \"lexical bottleneck\" problem seems to be softened (e.g. WordNet, Alvey Lexicon (), COMLEX (, etc.) there are no available wide range lexicons for natural language processing (NLP) for other languages.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.9557544589042664}, {"text": "natural language processing (NLP)", "start_pos": 167, "end_pos": 200, "type": "TASK", "confidence": 0.7155595322450002}]}, {"text": "Manual construction of lexicons is the most reliable technique for obtaining structured lexicons but is costly and highly time-consuming.", "labels": [], "entities": []}, {"text": "This is the reason for many researchers having focused on the massive acquisition of lexical knowledge and semantic information from pre-existing structured lexical resources as automatically as possible.", "labels": [], "entities": []}, {"text": "*This research has been partially funded by CICYT TIC96-1243-C03-02 (ITEM project) and the European Comission LE-4003 (EuroWordNet project).", "labels": [], "entities": [{"text": "CICYT", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.8587248921394348}, {"text": "TIC96-1243-C03-02", "start_pos": 50, "end_pos": 67, "type": "METRIC", "confidence": 0.4209159016609192}, {"text": "European Comission LE-4003 (EuroWordNet project", "start_pos": 91, "end_pos": 138, "type": "DATASET", "confidence": 0.8639057874679565}]}, {"text": "As dictionaries are special texts whose subject matter is a language (or a pair of languages in the case of bilingual dictionaries) they provide a wide range of information about words by giving definitions of senses of words, and, doing that, supplying knowledge not just about language, but about the world itself.", "labels": [], "entities": []}, {"text": "One of the most important relation to be extracted from machine-readable dictionaries (MRD) is the hyponym/hypernym relation among dictionary senses (e.g., ) not only because of its own importance as the backbone of taxonomies, but also because this relation acts as the support of main inheritance mechanisms helping, thus, the acquisition of other relations and semantic features, providing formal structure and avoiding redundancy in the lexicon ().", "labels": [], "entities": [{"text": "machine-readable dictionaries (MRD)", "start_pos": 56, "end_pos": 91, "type": "TASK", "confidence": 0.7183165192604065}]}, {"text": "For instance, following the natural chain of dictionary senses described in the Diccionario General Ilustrado de la Lengua Espadola we can discover that a bonsai is a cultivated plant or bush.", "labels": [], "entities": []}, {"text": "bonsai_l_2 planta y arbusto asi cultivado.", "labels": [], "entities": []}, {"text": "(bonsai, plant and bush cultivated in that way) The hyponym/hypernym relation appears between the entry word (e.g. bonsai) and the genus term, or the core of the phrase (e.g. planta and arbusto).", "labels": [], "entities": []}, {"text": "Thus, usually a dictionary definition is written to employ a genus term combined with differentia which distinguishes the word being defined from other words with the same genus term 1.", "labels": [], "entities": []}, {"text": "As lexical ambiguity pervades language in texts, the words used in dictionary are themselves lexically ambiguous.", "labels": [], "entities": []}, {"text": "Thus, when constructing complete disambiguated taxonomies, the correct dictionary sense of the genus term must be selected in each dictionary :For other kind of definition patterns not based on genus, a genus-like term was added after studying those patterns.", "labels": [], "entities": []}, {"text": "Although a large set of dictionaries have been exploited as lexicM resources, the most widely used monolingual MRD for NLP is LDOCE which was designed for learners of English.", "labels": [], "entities": []}, {"text": "It is clear that different dictionaries do not contain the same explicit information.", "labels": [], "entities": []}, {"text": "The information placed in LDOCE has allowed to extract other implicit information easily, e.g. taxonomies.", "labels": [], "entities": []}, {"text": "Does it mean that only highly structured dictionaries like LDOCE are suitable to be exploited to provide lexical resources for NLP systems?", "labels": [], "entities": []}, {"text": "We explored this question probing two disparate dictionaries: Diccionario General Ilustrado de la Lengua Espa~ola for Spanish, and Le Plus Petit Larousse (LPPL, 1980) for French.", "labels": [], "entities": []}, {"text": "Both are substantially poorer in coded information than LDOCE 3.", "labels": [], "entities": []}, {"text": "These dictionaries are very different in number of headwords, polysemy degree, size and length of definitions (c.f. table 1).", "labels": [], "entities": [{"text": "length", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.962852418422699}]}, {"text": "While DGILE is a good example of a large sized dictionary, LPPL shows to what extent the smallest dictionary is useful.", "labels": [], "entities": []}, {"text": "Even if most of the techniques for WSD are presented as stand-alone, it is our belief, following the ideas of, that full-fledged lexical ambiguity resolution should combine several information sources and techniques.", "labels": [], "entities": [{"text": "WSD", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9815769791603088}, {"text": "full-fledged lexical ambiguity resolution", "start_pos": 116, "end_pos": 157, "type": "TASK", "confidence": 0.6321929469704628}]}, {"text": "This work does not address all the heuristics cited in her paper, but profits from techniques that were at hand, without any claim of them being complete.", "labels": [], "entities": []}, {"text": "In fact we use unsupervised techniques, i.e. those that do not require hand-coding of any kind, that draw knowledge from a variety of sources -the source dictionaries, bilingual dictionaries and WordNet -in diverse ways.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 195, "end_pos": 202, "type": "DATASET", "confidence": 0.9473646283149719}]}, {"text": "3In LDOCE, dictionary senses are explicitly ordered by frequency, 86% dictionary senses have semantic codes and 44% of dictionary senses have pragmatic codes.", "labels": [], "entities": []}, {"text": "This paper tries to proof that using an appropriate method to combine those heuristics we can disambiguate the genus terms with reasonable precision, and thus construct complete taxonomies from any conventional dictionary in any language.", "labels": [], "entities": [{"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9914267063140869}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "After this short introduction, section 2 shows the methods we have applied.", "labels": [], "entities": []}, {"text": "Section 3 describes the test sets and shows the results.", "labels": [], "entities": []}, {"text": "Section 4 explains the construction of the lexical knowledge resources used.", "labels": [], "entities": []}, {"text": "Section 5 discusses previous work, and finally, section 6 faces some conclusions and comments on future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The difference in performance between the two dictionaries show that quality and size of resources is a key issue.", "labels": [], "entities": []}, {"text": "Apparently the task of disambiguating LPPL seems easier: less polysemy, more monosemous genus and high precision of the sense ordering heuristic.", "labels": [], "entities": [{"text": "disambiguating LPPL", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.6947093904018402}, {"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9974914789199829}]}, {"text": "However, the heuristics that depend only on the size of the data (5, 6) perform poorly on LPPL, while they are powerful methods for DGILE.", "labels": [], "entities": []}, {"text": "The results show that the combination of heuristics is useful, even if the performance of some of the heuristics is low.", "labels": [], "entities": []}, {"text": "The combination performs better than isolated heuristics, and allows to disambiguate all the genus of the test set with a success rate of 83% in DGILE and 82% in LPPL.", "labels": [], "entities": [{"text": "DGILE", "start_pos": 145, "end_pos": 150, "type": "METRIC", "confidence": 0.528495728969574}, {"text": "LPPL", "start_pos": 162, "end_pos": 166, "type": "DATASET", "confidence": 0.7955336570739746}]}, {"text": "All the heuristics except heuristic 3 can readily be applied to any other dictionary.", "labels": [], "entities": []}, {"text": "Minimal parameter adjustment (window size, cooccurrence weigth formula and vector similarity function) should be done to fit the characteristics of the dictionary, but according to our results it does not alter significantly the results after combining the heuristics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results for polysemous genus.", "labels": [], "entities": [{"text": "polysemous genus", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.6473741233348846}]}, {"text": " Table 5: Knowledge provided by each heuristic (overall results).", "labels": [], "entities": []}]}