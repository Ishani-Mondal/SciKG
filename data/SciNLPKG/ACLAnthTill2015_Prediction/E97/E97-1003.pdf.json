{"title": [{"text": "Three Generative, Lexicalised Models for Statistical Parsing", "labels": [], "entities": [{"text": "Statistical Parsing", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.9210991263389587}]}], "abstractContent": [{"text": "In this paper we first propose anew statistical parsing model, which is a genera-tive model of lexicalised context-free grammar.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.6993976831436157}]}, {"text": "We then extend the model to include a probabilistic treatment of both sub-categorisation and wh-movement.", "labels": [], "entities": []}, {"text": "Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).", "labels": [], "entities": [{"text": "Wall Street Journal text", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.9772110730409622}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.8531103134155273}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9475148916244507}]}], "introductionContent": [{"text": "Generative models of syntax have been central in linguistics since they were introduced in (Chomsky 57).", "labels": [], "entities": []}, {"text": "Each sentence-tree pair (S,T) in a language has an associated top-down derivation consisting of a sequence of rule applications of a grammar.", "labels": [], "entities": []}, {"text": "These models can be extended to be statistical by defining probability distributions at points of non-determinism in the derivations, thereby assigning a probability 7)(S, T) to each (S, T) pair.", "labels": [], "entities": []}, {"text": "Probabilistic context free grammar was an early example of a statistical grammar.", "labels": [], "entities": []}, {"text": "A PCFG can be lexicalised by associating a headword with each non-terminal in a parse tree; thus far, (Magerman 95;) and (Collins 96), which both make heavy use of lexical information, have reported the best statistical parsing performance on Wall Street Journal text.", "labels": [], "entities": [{"text": "Wall Street Journal text", "start_pos": 243, "end_pos": 267, "type": "DATASET", "confidence": 0.9826158583164215}]}, {"text": "Neither of these models is generative, instead they both estimate 7)(T] S) directly.", "labels": [], "entities": []}, {"text": "This paper proposes three new parsing models.", "labels": [], "entities": []}, {"text": "Model 1 is essentially a generative version of the model described in.", "labels": [], "entities": []}, {"text": "In Model 2, we extend the parser to make the complement/adjunct distinction by adding probabilities over subcategorisation frames for head-words.", "labels": [], "entities": []}, {"text": "In Model 3 we give a probabilistic treatment of wh-movement, which This research was supported by ARPA Grant N6600194-C6043. is derived from the analysis given in Generalized Phrase Structure Grammar ().", "labels": [], "entities": [{"text": "ARPA Grant N6600194-C6043.", "start_pos": 98, "end_pos": 124, "type": "DATASET", "confidence": 0.8538671731948853}]}, {"text": "The work makes two advances over previous models: First, Model 1 performs significantly better than (Collins 96), and Models 2 and 3 give further improvements --our final results are 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over.", "labels": [], "entities": [{"text": "Collins 96", "start_pos": 101, "end_pos": 111, "type": "DATASET", "confidence": 0.9453182518482208}, {"text": "constituent", "start_pos": 194, "end_pos": 205, "type": "METRIC", "confidence": 0.9050946235656738}, {"text": "precision", "start_pos": 206, "end_pos": 215, "type": "METRIC", "confidence": 0.9059585332870483}, {"text": "recall", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.9614485502243042}]}, {"text": "Second, the parsers in (Collins 96) and) produce trees without information about whmovement or subcategorisation.", "labels": [], "entities": [{"text": "Collins 96)", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9470024903615316}]}, {"text": "Most NLP applications will need this information to extract predicateargument structure from parse trees.", "labels": [], "entities": []}, {"text": "In the remainder of this paper we describe the 3 models in section 2, discuss practical issues in section 3, give results in section 4, and give conclusions in section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results on Section 23 of the WSJ Treebank. LR/LP = labeled recall/precision. CBs is the average  number of crossing brackets per sentence. 0 CBs, < 2 CBs are the percentage of sentences with 0 or < 2  crossing brackets respectively.", "labels": [], "entities": [{"text": "WSJ Treebank", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.8820779919624329}, {"text": "LR/LP", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.8658422231674194}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.7811849117279053}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.84521484375}, {"text": "CBs", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9589835405349731}]}]}