{"title": [{"text": "A Trainable Rule-based Algorithm for Word Segmentation", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7747209370136261}]}], "abstractContent": [{"text": "This paper presents a trainable rule-based algorithm for performing word segmen-tation.", "labels": [], "entities": []}, {"text": "The algorithm provides a simple , language-independent alternative to large-scale lexicai-based segmenters requiring large amounts of knowledge engineering.", "labels": [], "entities": []}, {"text": "As a stand-alone segmenter, we show our algorithm to produce high performance Chinese segmentation.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.7335204780101776}]}, {"text": "In addition, we show the transformation-based algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7581866681575775}]}, {"text": "1 Introduction This paper presents a trainable rule-based algorithm for performing word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.753564327955246}]}, {"text": "Our algorithm is effective both as a high-accuracy stand-alone seg-menter and as a postprocessor that improves the output of existing word segmentation algorithms.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.7401138842105865}]}, {"text": "In the writing systems of many languages, including Chinese, Japanese, and Thai, words are not delimited by spaces.", "labels": [], "entities": []}, {"text": "Determining the word boundaries , thus tokenizing the text, is usually one of the first necessary processing steps, making tasks such as part-of-speech tagging and parsing possible.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.7335927784442902}]}, {"text": "A variety of methods have recently been developed to perform word segmentation and the results have been published widely.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8020009100437164}]}, {"text": "1 A major difficulty in evaluating segmentation algorithms is that there are no widely-accepted guidelines as to what constitutes a word, and there is therefore no agreement on how to \"correctly\" segment a text in an unsegmented language.", "labels": [], "entities": []}, {"text": "It is 1Most published segmentation work has been done for Chinese.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.9740158319473267}]}, {"text": "For a discussion of recent Chinese segmentation work, see Sproat et al.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.5204649120569229}]}, {"text": "frequently mentioned in segmentation papers that native speakers of a language do not always agree about the \"correct\" segmentation and that the same text could be segmented into several very different (and equally correct) sets of words by different native speakers.", "labels": [], "entities": []}, {"text": "Sproat et a1.(1996) and Wu and Fung (1994) give empirical results showing that an agreement rate between native speakers as low as 75% is common.", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 82, "end_pos": 96, "type": "METRIC", "confidence": 0.9581730961799622}]}, {"text": "Consequently, an algorithm which scores extremely well compared to one native segmentation may score dismally compared to other, equally \"cor-rect\" segmentations.", "labels": [], "entities": []}, {"text": "We will discuss some other issues in evaluating word segmentation in Section 3.1.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7497195303440094}]}, {"text": "One solution to the problem of multiple correct segmentations might be to establish specific guidelines for what is and is not a word in unsegmented languages.", "labels": [], "entities": []}, {"text": "Given these guidelines, all corpora could theoretically be uniformly segmented according to the same conventions, and we could directly compare existing methods on the same corpora.", "labels": [], "entities": []}, {"text": "While this approach has been successful in driving progress in NLP tasks such as part-of-speech tagging and parsing , there are valid arguments against adopting it for word segmentation.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.6816222369670868}, {"text": "parsing", "start_pos": 108, "end_pos": 115, "type": "TASK", "confidence": 0.7884058952331543}, {"text": "word segmentation", "start_pos": 168, "end_pos": 185, "type": "TASK", "confidence": 0.7486169636249542}]}, {"text": "For example, since word seg-mentation is merely a preprocessing task fora wide variety of further tasks such as parsing, information extraction, and information retrieval, different seg-mentations can be useful or even essential for the different tasks.", "labels": [], "entities": [{"text": "parsing", "start_pos": 112, "end_pos": 119, "type": "TASK", "confidence": 0.9677633047103882}, {"text": "information extraction", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.7105571925640106}, {"text": "information retrieval", "start_pos": 149, "end_pos": 170, "type": "TASK", "confidence": 0.7820563912391663}]}, {"text": "In this sense, word segmentation is similar to speech recognition, in which a system must be robust enough to adapt to and recognize the multiple speaker-dependent \"correct\" pronunciations of words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7648286521434784}, {"text": "speech recognition", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7484798431396484}]}, {"text": "In some cases, it may also be necessary to allow multiple \"correct\" segmentations of the same text, depending on the requirements of further processing steps.", "labels": [], "entities": []}, {"text": "However, many algorithms use extensive domain-specific word lists and intricate name recognition routines as well as hard-coded morphological analysis modules to produce a predetermined segmentation output.", "labels": [], "entities": [{"text": "name recognition", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7820886075496674}]}], "introductionContent": [{"text": "This paper presents a trainable rule-based algorithm for performing word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7570532560348511}]}, {"text": "Our algorithm is effective both as a high-accuracy stand-alone segmenter and as a postprocessor that improves the output of existing word segmentation algorithms.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.74484783411026}]}, {"text": "In the writing systems of many languages, including Chinese, Japanese, and Thai, words are not delimited by spaces.", "labels": [], "entities": []}, {"text": "Determining the word boundaries, thus tokenizing the text, is usually one of the first necessary processing steps, making tasks such as part-of-speech tagging and parsing possible.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.7335927784442902}]}, {"text": "A variety of methods have recently been developed to perform word segmentation and the results have been published widely.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8020009100437164}]}, {"text": "A major difficulty in evaluating segmentation algorithms is that there are no widely-accepted guidelines as to what constitutes a word, and there is therefore no agreement on how to \"correctly\" segment a text in an unsegmented language.", "labels": [], "entities": []}, {"text": "It is 1Most published segmentation work has been done for Chinese.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.9740158319473267}]}, {"text": "For a discussion of recent Chinese segmentation work, see.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.5674488544464111}]}, {"text": "frequently mentioned in segmentation papers that native speakers of a language do not always agree about the \"correct\" segmentation and that the same text could be segmented into several very different (and equally correct) sets of words by different native speakers. and give empirical results showing that an agreement rate between native speakers as low as 75% is common.", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 311, "end_pos": 325, "type": "METRIC", "confidence": 0.9484034776687622}]}, {"text": "Consequently, an algorithm which scores extremely well compared to one native segmentation may score dismally compared to other, equally \"correct\" segmentations.", "labels": [], "entities": []}, {"text": "We will discuss some other issues in evaluating word segmentation in Section 3.1.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7497195303440094}]}, {"text": "One solution to the problem of multiple correct segmentations might be to establish specific guidelines for what is and is not a word in unsegmented languages.", "labels": [], "entities": []}, {"text": "Given these guidelines, all corpora could theoretically be uniformly segmented according to the same conventions, and we could directly compare existing methods on the same corpora.", "labels": [], "entities": []}, {"text": "While this approach has been successful in driving progress in NLP tasks such as part-of-speech tagging and parsing, there are valid arguments against adopting it for word segmentation.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.6816222369670868}, {"text": "parsing", "start_pos": 108, "end_pos": 115, "type": "TASK", "confidence": 0.7884058952331543}, {"text": "word segmentation", "start_pos": 167, "end_pos": 184, "type": "TASK", "confidence": 0.7486169636249542}]}, {"text": "For example, since word segmentation is merely a preprocessing task fora wide variety of further tasks such as parsing, information extraction, and information retrieval, different segmentations can be useful or even essential for the different tasks.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7720837891101837}, {"text": "parsing", "start_pos": 111, "end_pos": 118, "type": "TASK", "confidence": 0.9718675017356873}, {"text": "information extraction", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.6960723698139191}, {"text": "information retrieval", "start_pos": 148, "end_pos": 169, "type": "TASK", "confidence": 0.7819701135158539}]}, {"text": "In this sense, word segmentation is similar to speech recognition, in which a system must be robust enough to adapt to and recognize the multiple speaker-dependent \"correct\" pronunciations of words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7648291289806366}, {"text": "speech recognition", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7484799325466156}]}, {"text": "In some cases, it may also be necessary to allow multiple \"correct\" segmentations of the same text, depending on the requirements of further processing steps.", "labels": [], "entities": []}, {"text": "However, many algorithms use extensive domain-specific word lists and intricate name recognition routines as well as hard-coded morphological analysis modules to produce a predetermined segmentation output.", "labels": [], "entities": [{"text": "name recognition", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7820895612239838}]}, {"text": "Modifying or retargeting an existing segmentation algorithm to produce a different segmentation can be difficult, especially if it is not clear what and where the systematic differences in segmentation are.", "labels": [], "entities": []}, {"text": "It is widely reported in word segmentation papers, 2 that the greatest barrier to accurate word segmentation is in recognizing words that are not in the lexicon of the segmenter.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7314826250076294}, {"text": "word segmentation", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.6619581580162048}]}, {"text": "Such a problem is dependent both on the source of the lexicon as well as the correspondence (in vocabulary) between the text in question and the lexicon.", "labels": [], "entities": []}, {"text": "demonstrate that segmentation accuracy is significantly higher when the lexicon is constructed using the same type of corpus as the corpus on which it is tested.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.9508615732192993}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9671433568000793}]}, {"text": "We argue that rather than attempting to construct a single exhaustive lexicon or even a series of domain-specific lexica, it is more practical to develop a robust trainable means of compensating for lexicon inadequacies.", "labels": [], "entities": []}, {"text": "Furthermore, developing such an algorithm will allow us to perform segmentation in many different languages without requiring extensive morphological resources and domain-specific lexica in any single language.", "labels": [], "entities": []}, {"text": "For these reasons, we address the problem of word segmentation from a different direction.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7284941375255585}]}, {"text": "We introduce a rule-based algorithm which can produce an accurate segmentation of a text, given a rudimentary initial approximation to the segmentation.", "labels": [], "entities": []}, {"text": "Recognizing the utility of multiple correct segmentations of the same text, our algorithm also allows the output of a wide variety of existing segmentation algorithms to be adapted to different segmentation schemes.", "labels": [], "entities": []}, {"text": "In addition, our rule-based algorithm can also be used to supplement the segmentation of an existing algorithm in order to compensate for an incomplete lexicon.", "labels": [], "entities": []}, {"text": "Our algorithm is trainable and language independent, so it can be used with any unsegmented language.", "labels": [], "entities": []}], "datasetContent": [{"text": "Despite the number of papers on the topic, the evaluation and comparison of existing segmentation algorithms is virtually impossible.", "labels": [], "entities": []}, {"text": "In addition to the problem of multiple correct segmentations of the same texts, the comparison of algorithms is difficult because of the lack of a single metric for reporting scores.", "labels": [], "entities": []}, {"text": "Two common measures of performance are recall and precision, where recall is defined as the percent of words in the hand-segmented text identified by the segmentation algorithm, and precision is defined as the percentage of words returned by the algorithm that also occurred in the hand-segmented text in the same position.", "labels": [], "entities": [{"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9991148114204407}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9991647005081177}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9980844259262085}, {"text": "precision", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9990436434745789}]}, {"text": "The component recall and precision scores are then used to calculate an F-measure, where F = (1 +/~)PR/(~P + R).", "labels": [], "entities": [{"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.992006242275238}, {"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9972755312919617}, {"text": "F-measure", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.991544246673584}, {"text": "F", "start_pos": 89, "end_pos": 90, "type": "METRIC", "confidence": 0.995486319065094}]}, {"text": "In this paper we will report all scores as a balanced F-measure (precision and recall weighted equally) with/~ = 1, such that F = 2PR/(P + R)  For an initial experiment, segmentation was performed using the maximum matching algorithm, with a large lexicon of 34272 English words compiled from the WSJ.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9454342126846313}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9939893484115601}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.96890789270401}, {"text": "F", "start_pos": 126, "end_pos": 127, "type": "METRIC", "confidence": 0.9843340516090393}, {"text": "segmentation", "start_pos": 170, "end_pos": 182, "type": "TASK", "confidence": 0.9765006303787231}, {"text": "WSJ", "start_pos": 297, "end_pos": 300, "type": "DATASET", "confidence": 0.8526129126548767}]}, {"text": "l\u00b0 In contrast to the low initial Thai score, the greedy algorithm gave an initial English segmentation score of F=73.2.", "labels": [], "entities": [{"text": "English segmentation", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.5921636670827866}, {"text": "F", "start_pos": 113, "end_pos": 114, "type": "METRIC", "confidence": 0.9947221279144287}]}, {"text": "Our rule-based algorithm learned a sequence of 800 transformations, 9The average length of a word in our English data was 4.46.", "labels": [], "entities": []}, {"text": "characters, compared to 5.01 for Thai and 1.60 for Chinese.", "labels": [], "entities": []}, {"text": "1\u00b0Note that the portion of the WSJ corpus used to compile the word list was independent of both the training and test sets used in the segmentation experiments.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 31, "end_pos": 41, "type": "DATASET", "confidence": 0.9156348407268524}]}, {"text": "which improved the score from 73.2 to 79.0, a 21.6% error reduction.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 52, "end_pos": 67, "type": "METRIC", "confidence": 0.9917405247688293}]}, {"text": "The difference in the greedy scores for English and Thai demonstrates the dependence on the word list in the greedy algorithm.", "labels": [], "entities": []}, {"text": "For example, an experiment in which we randomly removed half of the words from the English list reduced the performance of the greedy algorithm from 73.2 to 32.3; although this reduced English word list was nearly twice the size of the Thai word list (17136 vs. 9939), the longest match segmentation utilizing the list was much lower.", "labels": [], "entities": []}, {"text": "Successive experiments in which we removed different random sets of half the words from the original list resulted in greedy algorithm performance of 39.2, 35.1, and 35.5.", "labels": [], "entities": []}, {"text": "Yet, despite the disparity in initial segmentation scores, the transformation sequences effect a significant error reduction in all cases, which indicates that the transformation sequences are effectively able to compensate (to some extent) for weaknesses in the lexicon.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 109, "end_pos": 124, "type": "METRIC", "confidence": 0.9506770968437195}]}, {"text": "provides a summary of the results using the greedy algorithm for each of the three languages.", "labels": [], "entities": []}, {"text": "As mentioned above, lexical resources are more readily available for English than for Thai.", "labels": [], "entities": []}, {"text": "We can use these resources to provide an informed initial segmentation approximation separate from the greedy algorithm.", "labels": [], "entities": []}, {"text": "Using our native knowledge of English as well as a shortlist of common English prefixes and suffixes, we developed a simple algorithm for initial segmentation of English which placed boundaries after any of the suffixes and before any of the prefixes, as well as segmenting punctuation characters.", "labels": [], "entities": [{"text": "initial segmentation of English", "start_pos": 138, "end_pos": 169, "type": "TASK", "confidence": 0.8122205436229706}]}, {"text": "In most cases, this simple approach was able to locate only one of the two necessary boundaries for recognizing full words, and the initial score was understandably low, F=29.8.", "labels": [], "entities": [{"text": "F", "start_pos": 170, "end_pos": 171, "type": "METRIC", "confidence": 0.9988815188407898}]}, {"text": "Nevertheless, even from this flawed initial approximation, our rule-based algorithm learned a sequence of 632 transformations which nearly doubled the word recall, improving the score from 29.8 to 53.3, a 33.5% error reduction.", "labels": [], "entities": [{"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.9568905830383301}, {"text": "error", "start_pos": 211, "end_pos": 216, "type": "METRIC", "confidence": 0.9570885896682739}]}], "tableCaptions": [{"text": " Table 3: English training set sizes. Initial score of  test data (700 sentences) was 73.2.", "labels": [], "entities": [{"text": "Initial score", "start_pos": 38, "end_pos": 51, "type": "METRIC", "confidence": 0.9709039330482483}]}, {"text": " Table 2: Summary of maximum matching results.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8519474864006042}]}]}