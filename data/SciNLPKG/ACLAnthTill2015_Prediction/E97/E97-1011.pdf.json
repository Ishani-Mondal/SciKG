{"title": [{"text": "Learning Features that Predict Cue Usage", "labels": [], "entities": [{"text": "Predict Cue Usage", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.8314434289932251}]}], "abstractContent": [{"text": "Our goal is to identify the features that predict the occurrence and placement of discourse cues in tutorial explanations in order to aid in the automatic generation of explanations.", "labels": [], "entities": []}, {"text": "Previous attempts to devise rules for text generation were based on intuition or small numbers of constructed examples.", "labels": [], "entities": [{"text": "text generation", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7957175076007843}]}, {"text": "We apply a machine learning program , C4.5, to induce decision trees for cue occurrence and placement from a corpus of data coded fora variety of features previously thought to affect cue usage.", "labels": [], "entities": [{"text": "cue occurrence and placement", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.7528686597943306}]}, {"text": "Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 158, "end_pos": 173, "type": "TASK", "confidence": 0.80915167927742}]}], "introductionContent": [{"text": "Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities.", "labels": [], "entities": []}, {"text": "They play a crucial role in many discourse processing tasks, including plan recognition, text comprehension, and anaphora resolution (.", "labels": [], "entities": [{"text": "plan recognition", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.8403200209140778}, {"text": "anaphora resolution", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7230082601308823}]}, {"text": "Moreover, research in reading comprehension indicates that felicitous use of cues improves comprehension and recall, but that their indiscriminate use may have detrimental effects on recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9979742169380188}, {"text": "recall", "start_pos": 183, "end_pos": 189, "type": "METRIC", "confidence": 0.9938399195671082}]}, {"text": "Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation.", "labels": [], "entities": [{"text": "cue usage", "start_pos": 47, "end_pos": 56, "type": "TASK", "confidence": 0.7665710151195526}, {"text": "automatic text generation", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.5982874631881714}]}, {"text": "From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not to include a cue in the generated text, (2) placement: where the cue should be placed in the text, and (3) selection: what lexical item(s) should be used.", "labels": [], "entities": [{"text": "cue usage", "start_pos": 33, "end_pos": 42, "type": "TASK", "confidence": 0.6746582537889481}]}, {"text": "Prior work in text generation has focused on cue selection, or on the relation between *Learning Research & Development Center tComputer Science Department, and Learning Research ~z Development Center tlntelllgent Systems Program cue occurrence and placement and specific rhetorical structures.", "labels": [], "entities": [{"text": "text generation", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.7764898836612701}, {"text": "cue selection", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.7538548111915588}, {"text": "cue occurrence and placement", "start_pos": 230, "end_pos": 258, "type": "TASK", "confidence": 0.7392569407820702}]}, {"text": "Other hypotheses about cue usage derive from work on discourse coherence and structure.", "labels": [], "entities": [{"text": "cue usage", "start_pos": 23, "end_pos": 32, "type": "TASK", "confidence": 0.6727588176727295}]}, {"text": "Previous research, which has been largely descriptive, suggests factors such as structural features of the discourse (e.g., level of embedding and segment complexity), intentional and informational relations in that structure, ordering of relata, and syntactic form of discourse constituents.", "labels": [], "entities": []}, {"text": "coded a corpus of naturally occurring tutorial explanations for the range of features identified in prior work.", "labels": [], "entities": []}, {"text": "Because they were also interested in the contrast between occurrence and non-occurrence of cues, they exhaustively coded for all of the factors thought to contribute to cue usage in all of the text.", "labels": [], "entities": []}, {"text": "From their study, Moscr and Moore identified several interesting correlations between particular features and specific aspects of cue usage, and were able to test specific hypotheses from the hterature that were based on constructed examples.", "labels": [], "entities": []}, {"text": "In this paper, we focus on cue occurrence and placement, and present an empirical study of the hypotheses provided by previous research, which have never been systematically evaluated with naturally occurring data.", "labels": [], "entities": [{"text": "cue occurrence and placement", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.7475230544805527}]}, {"text": "Wc use a machine learning program, C4.5, on the tagged corpus of Moser and Moore to induce decision trees.", "labels": [], "entities": []}, {"text": "The number of coded features and their interactions makes the manual construction of rules that predict cue occurrence and placement an intractable task.", "labels": [], "entities": []}, {"text": "Our results largely confirm the suggestions from the hterature, and clarify them by highhghting the most influential features fora particular task.", "labels": [], "entities": []}, {"text": "Discourse structure, in terms of both segment structure and levels of embedding, affects cue occurrence the most; intentional relations also play an important role.", "labels": [], "entities": []}, {"text": "For cue placement, the most important factors are syntactic structure and segment complexity.", "labels": [], "entities": [{"text": "cue placement", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8270292282104492}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we discuss previous research in more detail.", "labels": [], "entities": []}, {"text": "Section 3 provides an overview of Moser and Moore's coding scheme.", "labels": [], "entities": []}, {"text": "In Section 4 we present our learning experiments, and in Section 5 we discuss our results and conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "Initially, we performed learning on all 406 instances of core:contributor relations.", "labels": [], "entities": []}, {"text": "We quickly determined that this approach would not lead to useful decision trees.", "labels": [], "entities": []}, {"text": "First, the trees we obtained were extremely complex (at least 50 nodes).", "labels": [], "entities": []}, {"text": "Second, some of the subtrees corresponded to clearly identifiable subclasses of the data, such as relations with an implicit core, which suggested that we should apply learning to these independently identifiable subclasses.", "labels": [], "entities": []}, {"text": "Thus, we subdivided the data into three subsets: \u2022 Core/: core:contributor relations with the core in first position \u2022 Core~: core:contributor relations with the core in second position \u2022 Impl(icit)-core: core:contributor relations with an implicit core While this has the disadvantage of smaller training sets, the trees we obtain are more manageable and more meaningful.", "labels": [], "entities": []}, {"text": "summarizes the cardinality of these sets, and the frequencies of cue occurrence.", "labels": [], "entities": []}, {"text": "We ran four sets of experiments.", "labels": [], "entities": []}, {"text": "In three of them we predict cue occurrence and in one cue placement.", "labels": [], "entities": []}, {"text": "4 summarizes our main results concerning cue occurrence, and includes the error rates associated with different feature sets.", "labels": [], "entities": []}, {"text": "We adopt Litman's approach (1906) to determine whether two error rates El and \u00a32 are significantly different.", "labels": [], "entities": []}, {"text": "We compute 05% confidence intervals for the two error rates using a t-test.", "labels": [], "entities": []}, {"text": "\u00a31 is significantly better than \u00a3~ if the upper bound of the 95% confidence interval for \u00a31 is lower than the lower bound of the 95% confidence interval for g2-~ For each set of experiments, we report the following:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distributions of relations and cue occurrences", "labels": [], "entities": []}, {"text": " Table 3: Distributions of relations and cue occurrences", "labels": [], "entities": []}, {"text": " Table 4: Summary of learning results", "labels": [], "entities": []}]}