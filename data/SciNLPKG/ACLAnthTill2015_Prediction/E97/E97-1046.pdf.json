{"title": [{"text": "A Comparison of Head Transducers and Transfer fora Limited Domain Translation Application", "labels": [], "entities": [{"text": "Limited Domain Translation", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.648121843735377}]}], "abstractContent": [{"text": "We compare the effectiveness of two related \u2022 machine translation models applied to the same limited-domain task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7111567109823227}]}, {"text": "One is a transfer model with monolingual head automata for analysis and generation; the other is a direct transduction model based on bilingual head transducers.", "labels": [], "entities": []}, {"text": "We conclude that the head transducer model is more effective according to measures of accuracy, computational requirements, model size, and development effort.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9989318251609802}]}], "introductionContent": [{"text": "In this paper we describe an experimental machine translation system based on head transducer models and compare it to a related transfer system, described in Alshawi 1996a, based on monolingual head automata.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7194354236125946}]}, {"text": "Head transducer models consist of collections of finite state machines that are associated with pairs of lexical items in a bilingual lexicon.", "labels": [], "entities": []}, {"text": "The transfer system follows the familiar analysis-transfer-generation architecture. with mapping of dependency representations)in the transfer phase.", "labels": [], "entities": []}, {"text": "In contrast, the head transducer approach is more closely aligned with earlier direct translation methods: no explicit representations of the source language (interlingua or otherwise) are created in the process of deriving the target string.", "labels": [], "entities": []}, {"text": "Despite ~he simple direct architecture, the head transducer model does embody modern principles of lexicalized recursive grammars and statistical language processing.", "labels": [], "entities": [{"text": "statistical language processing", "start_pos": 134, "end_pos": 165, "type": "TASK", "confidence": 0.7302897969881693}]}, {"text": "The context for evaluating both the transducer and transfer models was the development of experimental prototypes for speechto-speech translation.", "labels": [], "entities": [{"text": "speechto-speech translation", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.8031549453735352}]}, {"text": "In the case of text translation for publishing, it is reasonable to adopt economic measures of the", "labels": [], "entities": [{"text": "text translation", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.7366229742765427}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Accuracy. time, and space comparison", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9732427597045898}]}, {"text": " Table 2: Lexicon and model size comparison", "labels": [], "entities": []}]}