{"title": [{"text": "Finite State Transducers Approximating Hidden Markov Models", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the conversion of a Hidden Markov Model into a sequential transducer that closely approximates the behavior of the stochastic model.", "labels": [], "entities": []}, {"text": "This transformation is especially advantageous for part-of-speech tagging because the resulting transducer can be composed with other transducers that encode correction rules for the most frequent tagging errors.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7120391428470612}]}, {"text": "The speed of tagging is also improved.", "labels": [], "entities": [{"text": "speed", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9876168370246887}, {"text": "tagging", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9778043627738953}]}, {"text": "The described methods have been implemented and successfully tested on six languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Finite-state automata have been successfully applied in many areas of computational linguistics.", "labels": [], "entities": []}, {"text": "This paper describes two algorithms 1 which approximate a Hidden Markov Model (HMM) used for part-of-speech tagging by a finite-state transducer (FST).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.6823648810386658}]}, {"text": "These algorithms maybe useful beyond the current description on any kind of analysis of written or spoken language based on both finite-state technology and HMMs, such as corpus analysis, speech recognition, etc.", "labels": [], "entities": [{"text": "corpus analysis", "start_pos": 171, "end_pos": 186, "type": "TASK", "confidence": 0.8335261642932892}, {"text": "speech recognition", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.7548973262310028}]}, {"text": "Both algorithms have been fully implemented.", "labels": [], "entities": []}, {"text": "An HMM used for tagging encodes, like a transducer, a relation between two languages.", "labels": [], "entities": []}, {"text": "One language contains sequences of ambiguity classes obtained by looking up in a lexicon all words of a sentence.", "labels": [], "entities": []}, {"text": "The other language contains sequences of tags obtained by statistically disambiguating the class sequences.", "labels": [], "entities": []}, {"text": "From the outside, an HMM tagger behaves like a sequential transducer that deterministically 1There is a different (unpublished) algorithm by Julian M. Kupiec and John T. Maxwell (p.c.).", "labels": [], "entities": [{"text": "HMM tagger", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.7825113236904144}]}, {"text": "maps every class sequence to a tag sequence, e.g.: The aim of the conversion is not to generate FSTs that behave in the same way, or in as similar away as possible like IIMMs, but rather FSTs that perform tagging in as accurate away as possible.", "labels": [], "entities": []}, {"text": "The motivation to derive these FSTs from HMMs is that HMMs can be trained and converted with little manual effort.", "labels": [], "entities": [{"text": "FSTs from HMMs", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.6622736056645712}]}, {"text": "The tagging speed when using transducers is up to five times higher than when using the underlying HMMs.", "labels": [], "entities": []}, {"text": "The main advantage of transforming an HMM is that the resulting transducer can be handled by finite state calculus.", "labels": [], "entities": []}, {"text": "Among others, it can be composed with transducers that encode: \u2022 correction rules for the most frequent tagging errors which are automatically generated or manually written, in order to significantly improve tagging accuracy 2.", "labels": [], "entities": [{"text": "correction", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9946692585945129}, {"text": "accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.841040849685669}]}, {"text": "These rules may include long-distance dependencies not handled by HMM taggers, and can conveniently be expressed by the replace operator ().", "labels": [], "entities": [{"text": "HMM taggers", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.7939380407333374}]}, {"text": "\u2022 further steps of text analysis, e.g. light parsing or extraction of noun phrases or other phrases (Ait-Mokhtar and Chanod, 1997).", "labels": [], "entities": [{"text": "text analysis", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.7563475668430328}, {"text": "light parsing or extraction of noun phrases or other phrases", "start_pos": 39, "end_pos": 99, "type": "TASK", "confidence": 0.8203175514936447}]}, {"text": "These compositions enable complex text analysis to be performed by a single transducer.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.7299274802207947}]}, {"text": "An IIMM transducer builds on the data (probability matrices) of the underlying HMM.", "labels": [], "entities": []}, {"text": "The accuracy 2Automatically derived rules require less work than manually written ones but are unlikely to yield better results because they would consider relatively limited context and simple relations only. of this data has an impact on the tagging accuracy of both the HMM itself and the derived transducer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9962084293365479}, {"text": "tagging", "start_pos": 244, "end_pos": 251, "type": "TASK", "confidence": 0.9504067301750183}, {"text": "accuracy", "start_pos": 252, "end_pos": 260, "type": "METRIC", "confidence": 0.8730593323707581}]}, {"text": "The training of the HMM can be done on either a tagged or untagged corpus, and is not a topic of this paper since it is exhaustively described in the literature ().", "labels": [], "entities": [{"text": "HMM", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.8676037192344666}]}, {"text": "An HMM can be identically represented by a weighted FST in a straightforward way.", "labels": [], "entities": []}, {"text": "We are, however, interested in non-weighted transducers.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Accuracy, speed, size and creation time of some HMM transducers", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9971737861633301}]}, {"text": " Table 2: Accuracy of some HMM transducers for different languages", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9977521300315857}]}]}