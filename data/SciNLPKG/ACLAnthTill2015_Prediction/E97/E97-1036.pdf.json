{"title": [], "abstractContent": [{"text": "Recent empirical research has shown conclusive advantages of multimodal interaction over speech-only interaction for map-based tasks.", "labels": [], "entities": []}, {"text": "This paper describes a mul-timodal language processing architecture which supports interfaces allowing simultaneous input from speech and gesture recognition.", "labels": [], "entities": [{"text": "gesture recognition", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.7011809349060059}]}, {"text": "Integration of spoken and gestural input is driven by unification of typed feature structures representing the semantic contributions of the different modes.", "labels": [], "entities": []}, {"text": "This integration method allows the component modalities to mutually compensate for each others' errors.", "labels": [], "entities": []}, {"text": "It is implemented in Quick-Set, a multimodal (pen/voice) system that enables users to setup and control distributed interactive simulations.", "labels": [], "entities": []}, {"text": "1 Introduction By providing a number of channels through which information may pass between user and computer, multimodal interfaces promise to significantly increase the bandwidth and fluidity of the interface between humans and machines.", "labels": [], "entities": []}, {"text": "In this work, we are concerned with the addition of multimodal input to the interface.", "labels": [], "entities": []}, {"text": "In particular, we focus on interfaces which support simultaneous input from speech and pen, utilizing speech recognition and recognition of gestures and drawings made with a pen on a complex visual display, such as a map.", "labels": [], "entities": []}, {"text": "Our focus on multimodal interfaces is motivated, in part, by the trend toward portable computing devices for which complex graphical user interfaces are infeasible.", "labels": [], "entities": []}, {"text": "For such devices, speech and gesture will be the primary means of user input.", "labels": [], "entities": []}, {"text": "Recent empirical results (Oviatt 1996) demonstrate clear task performance and user preference advantages for mul-timodal interfaces over speech only interfaces, in particular for spatial tasks such as those involving maps.", "labels": [], "entities": []}, {"text": "Specifically, in a within-subject experiment during which the same users performed the same tasks in various conditions using only speech, only pen, or both speech and pen-based input, users' multimodal input to maps resulted in 10% faster task completion time, 23% fewer words, 35% fewer spoken dis-fluencies, and 36% fewer task errors compared to unimodal spoken input.", "labels": [], "entities": []}, {"text": "Of the user errors, 48% involved location errors on the map-errors that were nearly eliminated by the simple ability to use pen-based input.", "labels": [], "entities": []}, {"text": "Finally, 100% of users indicated a preference for multimodal interaction over speech-only interaction with maps.", "labels": [], "entities": []}, {"text": "These results indicate that for map-based tasks, users would both perform better and be more satisfied when using a multimodal interface.", "labels": [], "entities": []}, {"text": "As an illustrative example, in the distributed simulation application we describe in this paper, one user task is to add a \"phase line\" to a map.", "labels": [], "entities": []}, {"text": "In the existing unimodal interface for this application (CommandTalk, Moore 1997), this is accomplished with a spoken utterance such as 'CRE-ATE A LINE FROM COORDINATES NINE FOUR THREE NINE THREE ONE TO NINE EIGHT NINE NINE FIVE ZERO AND CALL IT PHASE LINE GREEN'.", "labels": [], "entities": [{"text": "CommandTalk, Moore 1997)", "start_pos": 57, "end_pos": 81, "type": "DATASET", "confidence": 0.8999338626861573}, {"text": "CRE-ATE A LINE FROM COORDINATES NINE FOUR THREE NINE THREE ONE TO NINE EIGHT NINE NINE FIVE ZERO", "start_pos": 137, "end_pos": 233, "type": "METRIC", "confidence": 0.7903689642747244}, {"text": "CALL IT PHASE LINE GREEN", "start_pos": 238, "end_pos": 262, "type": "METRIC", "confidence": 0.7099124431610108}]}, {"text": "In contrast the same task can be accomplished by saying 'PHASE LINE GREEN' and simultaneously drawing the gesture in Figure 1.", "labels": [], "entities": [{"text": "PHASE LINE GREEN", "start_pos": 57, "end_pos": 73, "type": "METRIC", "confidence": 0.7423099676767985}]}, {"text": "J Figure 1: Line gesture The multimodal command involves speech recognition of only a three word phrase, while the equivalent unimodal speech command involves recognition of a complex twenty four word expression.", "labels": [], "entities": []}, {"text": "Furthermore , using unimodal speech to indicate more com-281", "labels": [], "entities": []}], "introductionContent": [{"text": "By providing a number of channels through which information may pass between user and computer, multimodal interfaces promise to significantly increase the bandwidth and fluidity of the interface between humans and machines.", "labels": [], "entities": []}, {"text": "In this work, we are concerned with the addition of multimodal input to the interface.", "labels": [], "entities": []}, {"text": "In particular, we focus on interfaces which support simultaneous input from speech and pen, utilizing speech recognition and recognition of gestures and drawings made with a pen on a complex visual display, such as a map.", "labels": [], "entities": []}, {"text": "Our focus on multimodal interfaces is motivated, in part, by the trend toward portable computing devices for which complex graphical user interfaces are infeasible.", "labels": [], "entities": []}, {"text": "For such devices, speech and gesture will be the primary means of user input.", "labels": [], "entities": []}, {"text": "Recent empirical results) demonstrate clear task performance and user preference advantages for multimodal interfaces over speech only interfaces, in particular for spatial tasks such as those involving maps.", "labels": [], "entities": []}, {"text": "Specifically, in a within-subject experiment during which the same users performed the same tasks in various conditions using only speech, only pen, or both speech and pen-based input, users' multimodal input to maps resulted in 10% faster task completion time, 23% fewer words, 35% fewer spoken disfluencies, and 36% fewer task errors compared to unimodal spoken input.", "labels": [], "entities": []}, {"text": "Of the user errors, 48% involved location errors on the map--errors that were nearly eliminated by the simple ability to use penbased input.", "labels": [], "entities": []}, {"text": "Finally, 100% of users indicated a preference for multimodal interaction over speech-only interaction with maps.", "labels": [], "entities": []}, {"text": "These results indicate that for map-based tasks, users would both perform better and be more satisfied when using a multimodal interface.", "labels": [], "entities": []}, {"text": "As an illustrative example, in the distributed simulation application we describe in this paper, one user task is to add a \"phase line\" to a map.", "labels": [], "entities": []}, {"text": "In the existing unimodal interface for this application, this is accomplished with a spoken utterance such as 'CRE-ATE A LINE FROM COORDINATES NINE FOUR THREE NINE THREE ONE TO NINE EIGHT NINE NINE FIVE ZERO AND CALL IT PHASE LINE GREEN'.", "labels": [], "entities": [{"text": "CRE-ATE A LINE FROM COORDINATES NINE FOUR THREE NINE THREE ONE TO NINE EIGHT NINE NINE FIVE ZERO", "start_pos": 111, "end_pos": 207, "type": "METRIC", "confidence": 0.8295829461680518}, {"text": "CALL IT PHASE LINE GREEN", "start_pos": 212, "end_pos": 236, "type": "METRIC", "confidence": 0.6890629172325134}]}, {"text": "In contrast the same task can be accomplished by saying 'PHASE LINE GREEN' and simultaneously drawing the gesture in The multimodal command involves speech recognition of only a three word phrase, while the equivalent unimodal speech command involves recognition of a complex twenty four word expression.", "labels": [], "entities": [{"text": "PHASE LINE GREEN", "start_pos": 57, "end_pos": 73, "type": "METRIC", "confidence": 0.799570639928182}]}, {"text": "Furthermore, using unimodal speech to indicate more com-plex spatial features such as routes and areas is practically infeasible if accuracy of shape is important.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9974051117897034}]}, {"text": "Another significant advantage of multimodal over unimodal speech is that it allows the user to switch modes when environmental noise or security concerns make speech an unacceptable input medium, or for avoiding and repairing recognition errors.", "labels": [], "entities": []}, {"text": "Multimodality also offers the potential for input modes to mutually compensate for each others' errors.", "labels": [], "entities": []}, {"text": "We will demonstrate :~'~.,, in our system, multimodal integration allows speech input to compensate for errors in gesture recognition and vice versa.", "labels": [], "entities": [{"text": "gesture recognition", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7789613008499146}]}, {"text": "Systems capable of integration of speech and gesture have existed since the early 80's.", "labels": [], "entities": []}, {"text": "One of the first such systems was the \"Put-That-There\" system.", "labels": [], "entities": []}, {"text": "However, in the sixteen years since then, research on multimodal integration has not yielded a reusable scalable architecture for the construction of multimodal systems that integrate gesture and voice.", "labels": [], "entities": [{"text": "multimodal integration", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7529330253601074}]}, {"text": "There are four major limiting factors in previous approaches to multimodal integration: (1) The majority of approaches limit the bandwidth of the gestural mode to simple deictic pointing gestures made with a mouse (), Wauchope 1994) or with the hand (.", "labels": [], "entities": [{"text": "multimodal integration", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7917058169841766}]}, {"text": "(ii) Most previous approaches have been primarily speech-driven ~ , treating gesture as a secondary dependent mode),.", "labels": [], "entities": []}, {"text": "In these systems, integration of gesture is triggered by the appearance of expressions in the speech stream whose reference needs to be resolved, such as definite and deictic noun phrases (e.g. 'this one', 'the red cube').", "labels": [], "entities": []}, {"text": "(iii) None of the existing approaches provide a wellunderstood generally applicable common meaning representation for the different modes, or, (iv) A general and formally-welldefined mechanism for multimodal integration.", "labels": [], "entities": [{"text": "multimodal integration", "start_pos": 197, "end_pos": 219, "type": "TASK", "confidence": 0.7357552945613861}]}, {"text": "I Koons et al 1993 describe two different systems.", "labels": [], "entities": []}, {"text": "The first uses input from hand gestures and eye gaze in order to aid in determining the reference of noun phrases in the speech stream.", "labels": [], "entities": []}, {"text": "The second allows users to manipulate objects in a blocks world using iconic and pantomimic gestures in addition to deictic gestures.", "labels": [], "entities": []}, {"text": "~More precisely, they are 'verbal language'-driven.", "labels": [], "entities": []}, {"text": "Either spoken or typed linguistic expressions are the driving force of interpretation.", "labels": [], "entities": []}, {"text": "We present an approach to multimodal integration which overcomes these limiting factors.", "labels": [], "entities": [{"text": "multimodal integration", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.7889524102210999}]}, {"text": "A wide base of continuous gestural input is supported and integration maybe driven by either mode.", "labels": [], "entities": []}, {"text": "Typed feature structures) are used to provide a clearly defined and well understood common meaning representation for the modes, and multimodal integration is accomplished through unification.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}