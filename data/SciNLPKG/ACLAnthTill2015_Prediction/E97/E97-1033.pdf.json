{"title": [{"text": "Intonational Boundaries, Speech Repairs and Discourse Markers: Modeling Spoken Dialog", "labels": [], "entities": [{"text": "Speech Repairs", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7107788026332855}, {"text": "Modeling Spoken Dialog", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7760435938835144}]}], "abstractContent": [{"text": "To understand a speaker's turn of a conversation , one needs to segment it into in-tonational phrases, cleanup any speech repairs that might have occurred, and identify discourse markers.", "labels": [], "entities": []}, {"text": "In this paper, we argue that these problems must be resolved together, and that they must be resolved early in the processing stream.", "labels": [], "entities": []}, {"text": "We put forward a statistical language model that resolves these problems, does POS tagging, and can be used as the language model of a speech recognizer.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 79, "end_pos": 90, "type": "TASK", "confidence": 0.7652336061000824}, {"text": "speech recognizer", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.6749003380537033}]}, {"text": "We find that by accounting for the interactions between these tasks that the performance on each task improves, as does POS tagging and per-plexity.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.7671267092227936}]}], "introductionContent": [{"text": "Interactive spoken dialog provides many new challenges for natural language understanding systems.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.6504368086655935}]}, {"text": "One of the most critical challenges is simply determining the speaker's intended utterances: both segmenting the speaker's turn into utterances and determining the intended words in each utterance.", "labels": [], "entities": []}, {"text": "Since there is no well-agreed to definition of what an utterance is, we instead focus on intonational phrases, which end with an acoustically signaled boundary lone.", "labels": [], "entities": []}, {"text": "Even assuming perfect word recognition, the problem of determining the intended words is complicated due to the occurrence of speech repairs, which occur where the speaker goes back and changes (or repeats) something she just said.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7661905586719513}]}, {"text": "The words that are replaced or repeated are no longer part of the intended utterance, and so need to be identified.", "labels": [], "entities": []}, {"text": "The following example, from the Trains corpus , gives an example of a speech repair with the words that the speaker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term.", "labels": [], "entities": [{"text": "Trains corpus", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.941644698381424}, {"text": "speech repair", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.7257936596870422}]}, {"text": "Much work has been done on both detecting boundary tones (e.g. () and on speech repair detection and correction (e.g.).", "labels": [], "entities": [{"text": "detecting boundary tones", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.8532861073811849}, {"text": "speech repair detection and correction", "start_pos": 73, "end_pos": 111, "type": "TASK", "confidence": 0.8079993903636933}]}, {"text": "This work has focused on one of the issues in isolation of the other.", "labels": [], "entities": []}, {"text": "However, these two issues are intertwined.", "labels": [], "entities": []}, {"text": "Cues such as the presence of silence, final syllable lengthening, and presence of filled pauses tend to mark both events.", "labels": [], "entities": []}, {"text": "Even the presence of word correspondences, a tradition cue for detecting and correcting speech repairs, sometimes marks boundary tones as well, as illustrated by the following example where the intonational phrase boundary is marked with the ToBI symbol %.", "labels": [], "entities": [{"text": "detecting and correcting speech repairs", "start_pos": 63, "end_pos": 102, "type": "TASK", "confidence": 0.8387392520904541}]}, {"text": "Example 2 (d93-83.3 utt73) that's all you need % you only need one boxcar Intonational phrases and speech repairs also interact with the identification of discourse markers.", "labels": [], "entities": [{"text": "identification of discourse markers", "start_pos": 137, "end_pos": 172, "type": "TASK", "confidence": 0.768466979265213}]}, {"text": "Discourse markers) are used to relate new speech to the current discourse state.", "labels": [], "entities": []}, {"text": "Lexical items that can function as discourse markers, such as \"well\" and \"okay,\" are ambiguous as to whether they are being used as discourse markers or not.", "labels": [], "entities": []}, {"text": "The complication is that discourse markers tend to be used to introduce anew utterance, or can bean utterance all to themselves (such as the acknowledgment \"okay\" or \"alright\"), or can be used as part of the editing term of a speech repair, or to begin the alteration.", "labels": [], "entities": []}, {"text": "Hence, the problem of identifying discourse markers also needs to be addressed with the segmentation and speech repair problems.", "labels": [], "entities": [{"text": "speech repair", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.7097540944814682}]}, {"text": "These three phenomena of spoken dialog, however, cannot be resolved without recourse to syntactic information.", "labels": [], "entities": []}, {"text": "Speech repairs, for example, are often signaled by syntactic anomalies.", "labels": [], "entities": [{"text": "Speech repairs", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7007392197847366}]}, {"text": "Furthermore, in order to determine the extent of the reparanduin, one needs to take into account the parallel structure that typically exists between the reparandum and alteration, which relies on at identifying the s:?ntactic roles, or part-of-speech (POS) tags, of the words involved.", "labels": [], "entities": []}, {"text": "However, speech repairs disrupt the context that is needed to determine the POS tags.", "labels": [], "entities": []}, {"text": "Hence, speech repairs, as well as boundary tones and discourse markers, must be resolved during syntactic disambiguation.", "labels": [], "entities": [{"text": "syntactic disambiguation", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.7011101245880127}]}, {"text": "Of course when dealing with spoken dialogue, one cannot forget the initial problem of determining the actual words that the speaker is saying.", "labels": [], "entities": []}, {"text": "Speech recognizers rely on being able to predict the probability of what word will be said next.", "labels": [], "entities": [{"text": "Speech recognizers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.727235957980156}]}, {"text": "Just as intonational phrases and speech repairs disrupt the local context that is needed for syntactic disambiguation, the same holds for predicting what word will come next.", "labels": [], "entities": [{"text": "syntactic disambiguation", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.7381594181060791}]}, {"text": "If a speech repair or intonational phrase occurs, this will alter the probability estimate.", "labels": [], "entities": [{"text": "speech repair", "start_pos": 5, "end_pos": 18, "type": "TASK", "confidence": 0.7404409945011139}]}, {"text": "But more importantly, speech repairs and intonational phrases have acoustic correlates such as the presence of silence.", "labels": [], "entities": [{"text": "speech repairs", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7231080085039139}]}, {"text": "Current speech recognition language models camlot account for the presence of silence, and tend to simply ignore it.", "labels": [], "entities": [{"text": "speech recognition language", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.7753007511297861}]}, {"text": "By modeling speech repairs and intonational boundaries, we can take into account the acoustic correlates and hence use more of the available information.", "labels": [], "entities": [{"text": "speech repairs", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7346470057964325}]}, {"text": "From the above discussion, it is clear that we need to model these dialogue phenomena together and very early on in the speech processing stream, in fact, during speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.7598205804824829}]}, {"text": "Currently, the approaches that work best in speech recognition are statistical approaches that are able to assign probability estimates for what word will occur next given the previous words.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7961109280586243}]}, {"text": "Hence, in this paper, we introduce a statistical language model that can detect speech repairs, boundary tones, and discourse markers, and can assign POS tags, and can use this information to better predict what word will occur next.", "labels": [], "entities": []}, {"text": "In the rest of the paper, we first introduce the Trains corpus.", "labels": [], "entities": [{"text": "Trains corpus", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.9366373717784882}]}, {"text": "We then introduce a statistical language model that incorporates POS tagging and the identification of discourse markers.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.8204313516616821}]}, {"text": "We then augmeat this model with speech repair detection and correction and intonational boundary tone detection.", "labels": [], "entities": [{"text": "speech repair detection", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.8600760102272034}]}, {"text": "We then present the results of this model on the Trains corpus and show that it can better account for these discourse events than can be achieved by modeling them individually.", "labels": [], "entities": [{"text": "Trains corpus", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.9448420107364655}]}, {"text": "We also show that by modeling these two phenomena that we can increase our POS tagging performance by 8.6%, and improve our ability to predict the next word.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.7509838044643402}, {"text": "predict the next word", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.8767973333597183}]}, {"text": "As part of the TRAINS project (, which is along term research project to build a conversationally proficient planning assistant, we have collected a corpus of problem solving dialogs.", "labels": [], "entities": [{"text": "TRAINS", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.6542418003082275}, {"text": "problem solving dialogs", "start_pos": 159, "end_pos": 182, "type": "TASK", "confidence": 0.7735081513722738}]}, {"text": "The dialogs involve two human participants, one who is playing the role of a user and has a certain task to accomplish, and another who is playing the role of the system by acting as a planning assistant.", "labels": [], "entities": []}, {"text": "The collection methodology was designed to make the setting as close to humancomputer interaction as possible, but was not a wizard scenario, where one person pretends to be a computer.", "labels": [], "entities": []}, {"text": "Rathor, the user knows that he is talking to another person.", "labels": [], "entities": [{"text": "Rathor", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9138307571411133}]}, {"text": "The TaAINS corpus consists of about six and half hours of speech.", "labels": [], "entities": [{"text": "TaAINS corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7409007698297501}]}, {"text": "gives some general statistics about the corpus, including the number of dialogs, speakers, words, speaker turns, and occurrences of discourse markers, boundary tones and speech repairs.", "labels": [], "entities": []}, {"text": "The speech repairs in the Trains corpus have been hand-annotated.", "labels": [], "entities": [{"text": "Trains corpus", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.9775832891464233}]}, {"text": "We have divided the repairs into three types: fresh starts, modification repairs, and abridged repairs.", "labels": [], "entities": []}, {"text": "1 A fresh start is where the speaker abandons the current utterance and starts again, where the abandonment seems acoustically signaled.", "labels": [], "entities": []}, {"text": "Example 3 (d93-12.1 utt30) so it'll take um so you want to do what reparandum| editing term alteration interruption point The second type of repairs are the modification repairs.", "labels": [], "entities": [{"text": "editing term alteration interruption point", "start_pos": 79, "end_pos": 121, "type": "METRIC", "confidence": 0.7989474534988403}]}, {"text": "These include all other repairs in which the reparandum is not empty.", "labels": [], "entities": []}, {"text": "Example 4 (d92a-l.3 utt65) so that will total will take seven hours to do that reparandumT alteration interruption point 1This classification is similar to that of Hindle (1983) and.", "labels": [], "entities": [{"text": "reparandumT alteration interruption point", "start_pos": 79, "end_pos": 120, "type": "METRIC", "confidence": 0.8566554933786392}]}, {"text": "The third type of repairs are the abridged repairs, which consist solely of an editing term.", "labels": [], "entities": []}, {"text": "Note that utterance initial filled pauses are not treated as abridged repairs.", "labels": [], "entities": []}, {"text": "Example 5 (d93-14.3 utt42) we need to um manage to get the bananas to Dansville", "labels": [], "entities": [{"text": "Dansville", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.7720133066177368}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Frequency of Tones, Repairs and Editing  Terms in the Trains Corpus", "labels": [], "entities": []}, {"text": " Table 2: POS Tagging and Perplexity Results", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.775361955165863}]}, {"text": " Table 3: Detecting Intonational Phrases", "labels": [], "entities": [{"text": "Detecting Intonational Phrases", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.8987895647684733}]}, {"text": " Table 4: Detecting and Correcting Speech Repairs", "labels": [], "entities": [{"text": "Detecting and Correcting Speech Repairs", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.8026259899139404}]}]}