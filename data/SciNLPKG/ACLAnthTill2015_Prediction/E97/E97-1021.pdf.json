{"title": [{"text": "A DOP Model for Semantic Interpretation*", "labels": [], "entities": [{"text": "Semantic Interpretation", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.7884926199913025}]}], "abstractContent": [{"text": "In data-oriented language processing, an annotated language corpus is used as a stochastic grammar.", "labels": [], "entities": []}, {"text": "The most probable analysis of anew sentence is constructed by combining fragments from the corpus in the most probable way.", "labels": [], "entities": []}, {"text": "This approach has been successfully used for syntactic analysis , using corpora with syntactic annotations such as the Penn Tree-bank.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8807442486286163}, {"text": "Penn Tree-bank", "start_pos": 119, "end_pos": 133, "type": "DATASET", "confidence": 0.9959771931171417}]}, {"text": "If a corpus with semantically annotated sentences is used, the same approach can also generate the most probable semantic interpretation of an input sentence.", "labels": [], "entities": []}, {"text": "The present paper explains this semantic interpretation method.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.7813780605792999}]}, {"text": "A data-oriented semantic interpretation algorithm was tested on two semantically annotated corpora: the English ATIS corpus and the Dutch OVIS corpus.", "labels": [], "entities": [{"text": "data-oriented semantic interpretation", "start_pos": 2, "end_pos": 39, "type": "TASK", "confidence": 0.6252662042776743}, {"text": "English ATIS corpus", "start_pos": 104, "end_pos": 123, "type": "DATASET", "confidence": 0.795406719048818}, {"text": "Dutch OVIS corpus", "start_pos": 132, "end_pos": 149, "type": "DATASET", "confidence": 0.8316434621810913}]}, {"text": "Experiments show an increase in semantic accuracy if larger corpus-fragments are taken into consideration.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9827859997749329}]}], "introductionContent": [{"text": "Data-oriented models of language processing embody the assumption that human language perception and production works with representations of concrete past language experiences, rather than with abstract grammar rules.", "labels": [], "entities": []}, {"text": "Such models therefore maintain large corpora of linguistic representations of previously occurring utterances.", "labels": [], "entities": []}, {"text": "When processing anew input utterance, analyses of this utterance are constructed by combining fragments from the corpus; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one.", "labels": [], "entities": []}, {"text": "* This work was partially supported by NWO, the Netherlands Organization for Scientific Research (Priority Programme Language and Speech Technology).", "labels": [], "entities": [{"text": "NWO", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.9480215907096863}]}, {"text": "For the syntactic dimension of language, various instantiations of this data-oriented processing or \"DOP\" approach have been worked out (e.g.;;;;;; Rajman (1995ab);;).", "labels": [], "entities": []}, {"text": "A method for extending it to the semantic domain was first introduced by van den.", "labels": [], "entities": []}, {"text": "In the present paper we discuss a computationally effective version of that method, and an implemented system that uses it.", "labels": [], "entities": []}, {"text": "We first summarize the first fully instantiated DOP model as presented in.", "labels": [], "entities": []}, {"text": "Then we show how this method can be straightforwardly extended into a semantic analysis method, if corpora are created in which the trees are enriched with semantic annotations.", "labels": [], "entities": []}, {"text": "Finally, we discuss an implementation and report on experiments with two semantically analyzed corpora (ATIS and OVIS).", "labels": [], "entities": [{"text": "ATIS", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9102075099945068}]}], "datasetContent": [{"text": "We performed a number of experiments, using a random division of the tree-bank data into test-and training-set.", "labels": [], "entities": []}, {"text": "No provisions were taken for unknown words.", "labels": [], "entities": []}, {"text": "The results reported here, are obtained by randomly selecting 300 trees from the tree-bank.", "labels": [], "entities": []}, {"text": "All utterances of length greater than one in this selection are used as testing material.", "labels": [], "entities": []}, {"text": "We varied the size of the training-set, and the maximal depth of the subtrees.", "labels": [], "entities": []}, {"text": "The average length of the test-sentences was 4.74 words.", "labels": [], "entities": []}, {"text": "There was a constraint on the extraction of subtrees from the training-set trees: subtrees could have a maximum of two substitution-sites, and no more than three contiguous lexical nodes (Experience has shown that such limitations improve prob6In the example in, the pragmatic operators #, denial, and !, correction, axe used ability estimations, while retaining the full power of DOP).", "labels": [], "entities": [{"text": "correction", "start_pos": 305, "end_pos": 315, "type": "METRIC", "confidence": 0.9906692504882812}, {"text": "DOP", "start_pos": 381, "end_pos": 384, "type": "METRIC", "confidence": 0.8882012963294983}]}, {"text": "show results using a training set size of 8500 trees.", "labels": [], "entities": []}, {"text": "The maximal depth of subtrees involved in the parsing process was varied from 1 to 5.", "labels": [], "entities": [{"text": "maximal depth", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9731946587562561}, {"text": "parsing process", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.9094563126564026}]}, {"text": "Results in concern a match with the total analysis in the test-set, whereas shows success on just the resulting interpretation.", "labels": [], "entities": []}, {"text": "Only exact matches with the trees and interpretations in the test-set were counted as successes.", "labels": [], "entities": []}, {"text": "The experiments show that involving larger fragments in the parsing process leads to higher accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9900962114334106}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9980224370956421}]}, {"text": "Apparently, for this domain fragments of depth 5 are too large, and deteriorate probability estimations 7.", "labels": [], "entities": []}, {"text": "The results also confirm our earlier findings, that semantic parsing is robust.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.8413212299346924}]}, {"text": "Quite a few analysis trees that did not exactly match with their counterparts in the test-set, yielded a semantic interpretation that did match.", "labels": [], "entities": []}, {"text": "Finally, figures 12 and 13 show results for differing training-set sizes, using subtrees of maximal depth 4.", "labels": [], "entities": []}], "tableCaptions": []}