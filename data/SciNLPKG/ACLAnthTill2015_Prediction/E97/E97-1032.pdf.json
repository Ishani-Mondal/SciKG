{"title": [{"text": "Comparing a Linguistic and a Stochastic Tagger", "labels": [], "entities": []}], "abstractContent": [{"text": "Concerning different approaches to automatic PoS tagging: EngCG-2, a constraint-based morphological tagger, is compared in a double-blind test with a state-of-the-art statistical tagger on a common disambigua-tion task using a common tag set.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.8930370211601257}]}, {"text": "The experiments show that for the same amount of remaining ambiguity, the error rate of the statistical tagger is one order of magnitude greater than that of the rule-based one.", "labels": [], "entities": [{"text": "error rate", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9878050982952118}]}, {"text": "The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed.", "labels": [], "entities": []}], "introductionContent": [{"text": "There are currently two main methods for automatic part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7042404413223267}]}, {"text": "The prevailing one uses essentially statistical language models automatically derived from usually hand-annotated corpora.", "labels": [], "entities": []}, {"text": "These corpus-based models can be represented e.g. as collocational matrices (, Hidden Markov models (cf.), local rules (e.g. Hindle 1989) and neural networks (e.g..", "labels": [], "entities": []}, {"text": "Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text.", "labels": [], "entities": []}, {"text": "using tag sets ranging from some dozens to about 130 tags.", "labels": [], "entities": []}, {"text": "The less popular approach is based on hand-coded linguistic rules.", "labels": [], "entities": []}, {"text": "Pioneering work was done in the 1960\"s (e.g..", "labels": [], "entities": []}, {"text": "Recently, new interest in the linguistic approach has been shown e.g. in the work of.", "labels": [], "entities": []}, {"text": "The first serious linguistic competitor to data-driven statistical taggers is the English Constraint Grammar parser.", "labels": [], "entities": [{"text": "English Constraint Grammar parser", "start_pos": 82, "end_pos": 115, "type": "TASK", "confidence": 0.7069422900676727}]}, {"text": "The tagger consists of the following sequentially applied modules: 1.", "labels": [], "entities": []}, {"text": "Morphological analysis (a) Lexical component (b) Rule-based guesser for unknown words 3.", "labels": [], "entities": []}, {"text": "Resolution of morphological ambiguities The tagger uses a two-level morphological analyser with a large lexicon and a morphological description that introduces about 180 different ambiguity-forming morphological analyses, as a result of which each word gets 1.7-2.2 different analyses on an average.", "labels": [], "entities": [{"text": "Resolution of morphological ambiguities", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.886665552854538}]}, {"text": "Morphological analyses are assigned to unknown words with an accurate rulebased 'guesser'.", "labels": [], "entities": []}, {"text": "The morphological disambiguator uses constraint rules that discard illegitimate morphological analyses on the basis of local or global context conditions.", "labels": [], "entities": []}, {"text": "The rules can be grouped as ordered subgrammars: e.g. heuristic subgrammar 2 can be applied for resolving ambiguities left pending by the more \"careful' subgrammar 1.", "labels": [], "entities": []}, {"text": "Older versions of EngCG (using about 1,150 constraints) are reported to assign a correct analysis to about 99.7% of all words while each word in the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remait~ unresolved.", "labels": [], "entities": [{"text": "correct analysis", "start_pos": 81, "end_pos": 97, "type": "METRIC", "confidence": 0.9426238238811493}]}, {"text": "These results have been seriously questioned.", "labels": [], "entities": []}, {"text": "One doubt concerns the notion 'correct analysis\".", "labels": [], "entities": []}, {"text": "For example argues that linguists who manually perform the tagging task using the doubleblind method disagree about the correct analysis in at least 3% of all words even after they have negotiated about the initial disagreements.", "labels": [], "entities": [{"text": "tagging task", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.9076873362064362}]}, {"text": "If this were the case, reporting accuracies above this 97% \"upper bound' would make no sense.", "labels": [], "entities": []}, {"text": "However, empirically show that an interjudge agreement virtually of 1()0% is possible, at least with the EngCG tag set if not with the original Brown Corpus tag set.", "labels": [], "entities": [{"text": "EngCG tag set", "start_pos": 105, "end_pos": 118, "type": "DATASET", "confidence": 0.9589206576347351}, {"text": "Brown Corpus tag set", "start_pos": 144, "end_pos": 164, "type": "DATASET", "confidence": 0.9806004613637924}]}, {"text": "This consistent applicability of the EngCG tag set is explained by characterising it as grammatically rather than semantically motivated.", "labels": [], "entities": [{"text": "EngCG tag set", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.7919894258181254}]}, {"text": "Another main reservation about the EngCG is the suspicion that, perhaps partly due to the somewhat underspecific nature of the EngCG tag set, it must be so easy to disambiguate that also a statistical tagger using the EngCG tags would reach at least as good results.", "labels": [], "entities": [{"text": "EngCG tag set", "start_pos": 127, "end_pos": 140, "type": "DATASET", "confidence": 0.823212742805481}]}, {"text": "This argument will be examined in this paper.", "labels": [], "entities": []}, {"text": "It will be empirically shown (i) that the EngCG tag set is about as difficult fora probabilistic tagger as more generally used tag sets and (ii) that the EngCG disambiguator has a clearly smaller error rate than the probabilistic tagger when a similar (small) amount of ambiguity is permitted in the output.", "labels": [], "entities": [{"text": "EngCG tag set", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8073047598203024}]}, {"text": "A state-of-the-art statistical tagger is trained on a corpus of over 350,000 words hand-annotated with EngCG tags.", "labels": [], "entities": []}, {"text": "then both taggers (a new version known as En~CG-21 with 3,600 constraints as five subgrammars-, and a statistical tagger) are applied to the same held-out benchmark corpus of 55,000 words, and their performances are compared.", "labels": [], "entities": []}, {"text": "The results disconfirm the suspected 'easiness' of the EngCG tag set: the statistical tagger's performance figures are no better than is the case with better known tag sets.", "labels": [], "entities": [{"text": "EngCG tag set", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.8517487645149231}]}, {"text": "Two caveats are in order.", "labels": [], "entities": []}, {"text": "What we are not addressing in this paper is the workload required for making a rule-based or a data-driven tagger.", "labels": [], "entities": []}, {"text": "The rules in EngCG certainly took a considerable effort to write, and though at the present state of knowledge rules could be written and tested with less effort, it may well be the case that a tagger with an accuracy of 95-97% can be produced with less effort by using data-driven techniques.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.997347354888916}]}, {"text": "3 Another caveat is that EngCG alone does not resolve all ambiguities, so it cannot be compared to atypical statistical tagger if full disambiguation is required. has shown that EngCG combined with a syntactic parser produces morphologically unambiguous output with an accuracy of 99.3%, a figure clearly better than that of the statistical tagger in the experiments below (however. the test data was not the same).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 269, "end_pos": 277, "type": "METRIC", "confidence": 0.9987843632698059}]}, {"text": "Before examining the statistical tagger, two practical points are addressed: the annotation of tile corpora used. and the modification of the EngCG tag set for use in a statistical tagger.", "labels": [], "entities": [{"text": "EngCG tag set", "start_pos": 142, "end_pos": 155, "type": "DATASET", "confidence": 0.8756274183591207}]}, {"text": "1An online version of EngCG-2 can be found at, ht tp://www.ling.helsinki.fi/\"avoutila/engcg-2.ht ml.", "labels": [], "entities": []}, {"text": ":The first three subgrammars are generally highly reliable and almost all of the total grammar development time was spent on them: the last two contain rather rough heuristic constraints.", "labels": [], "entities": []}, {"text": "3However, for an interesting experiment suggesting otherwise, see. that was annotated using the EngCG tags.", "labels": [], "entities": [{"text": "EngCG tags", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.8943111598491669}]}, {"text": "The corpus was first analysed with the EngCG lexical analyser, and then it was fully disambiguated and, when necessary, corrected by a human expert.", "labels": [], "entities": [{"text": "EngCG lexical analyser", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.9226906100908915}]}, {"text": "This annotation took place a few years ago.", "labels": [], "entities": []}, {"text": "Since then, it has been used in the development of new EngCG constraints (the present version, EngCG-2, contains about 3,600 constraints): new constraints were applied to the training corpus, and whenever a reading marked as correct was discarded, either the analysis in the corpus, or the constraint itself, was corrected.", "labels": [], "entities": []}, {"text": "In this way, the tagging quality of the corpus was continuously improved.", "labels": [], "entities": []}], "datasetContent": [{"text": "The statistical tagger was trained on 357,000 words from the Brown corpus (, reannotated using the EngCG annotation scheme (see above).", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.9682305157184601}]}, {"text": "Ina first set of experiments, a 35,000 word subset of this corpus was set aside and used to evaluate the tagger's performance when trained on successively larger portions of the remaining 322,000 words.", "labels": [], "entities": []}, {"text": "The learning curve, showing the error rate alter full disambiguation as a function of the amount of training data used, see, has levelled off at 322,000 words, indicating that little is to be gained from further training.", "labels": [], "entities": [{"text": "error rate", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9555940628051758}]}, {"text": "We also note that the absolute value of the error rate is 3.51% --a typical state-of-the-art Here, previously unseen words contribute 1.08% to the total error rate, while the contribution from lexical tag omissions is 0.08% 95% confidence intervals for the error rates would range from + 0.30% for 30,000 words to + 0.20~c at 322.000 words.", "labels": [], "entities": []}, {"text": "The tagger was then trained on the entire set of 357,000 words and confronted with the separate 55,000-word benchmark corpus, and run both in full Error-rate-ambiguity tradeoff for both taggets on the benchmark corpus.", "labels": [], "entities": []}, {"text": "Parenthesized numbers are interpolated. and partial disambiguation mode.", "labels": [], "entities": []}, {"text": "shows the error rate as a function of remaining ambiguity (tags/word) both for the statistical tagger, and for the EngCG-2 tagger.", "labels": [], "entities": [{"text": "error rate", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.95926833152771}, {"text": "EngCG-2 tagger", "start_pos": 115, "end_pos": 129, "type": "DATASET", "confidence": 0.8560804128646851}]}, {"text": "The error rate for full disanabiguation using the 6 variables is 4.72% and using the 7 variables is 4.68%, both -4-0.18% with confidence degree 95%.", "labels": [], "entities": [{"text": "error", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9886589646339417}]}, {"text": "Note that the optimal tag sequence obtained using the 7 variables need not equal the optimal tag sequence obtained using the 6 variables.", "labels": [], "entities": []}, {"text": "In fact, the former sequence maybe assigned zero probability by the HMM, namely if one of its state transitions has zero probability.", "labels": [], "entities": []}, {"text": "Previously unseen words account for 2.01%, and lexical tag omissions for 0.15% of the total error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9264808595180511}]}, {"text": "These two error sources are together exactly 1.00% higher on the benchmark corpus than on the Brown corpus, and account for almost the entire difference in error rate.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.8820006251335144}, {"text": "error rate", "start_pos": 156, "end_pos": 166, "type": "METRIC", "confidence": 0.9339461326599121}]}, {"text": "They stem from using less complete lexical information sources, and are most likely the effect of a larger vocabulary overlap between the test and training portions of the Brown corpus than between the Brown and benchmark corpora.", "labels": [], "entities": []}, {"text": "The ratio between the error rates of the two taggets with the same amount of remaining ambiguity ranges from 8.6 at 1.026 tags/word to 28,0 at 1.070 tags/word.", "labels": [], "entities": [{"text": "error rates", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9626106321811676}]}, {"text": "The error rate of the statistical tagger can be further decreased, at the price of increased remaining ambiguity, see.", "labels": [], "entities": [{"text": "error rate", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9783416092395782}]}, {"text": "In the limit of retaining all possible tags, the residual error rate is entirely due to lexical tag omissions, i.e., it is 0.15%, within average 14.24 tags per word.", "labels": [], "entities": [{"text": "residual error rate", "start_pos": 49, "end_pos": 68, "type": "METRIC", "confidence": 0.7748433947563171}]}, {"text": "The reason that this figure is so high is that the unknown words, which comprise 10% of the corpus, are assigned all possible tags as they are backed off all the way to the root of the reverse-suffix tree.", "labels": [], "entities": []}, {"text": "Figure 2: Error-rate-ambiguity tradeoff for the statistical tagger on the benchmark corpus.", "labels": [], "entities": [{"text": "Error-rate-ambiguity", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9821764230728149}, {"text": "benchmark corpus", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.6915203928947449}]}], "tableCaptions": []}