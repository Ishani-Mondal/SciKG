{"title": [{"text": "Similarity-Based Methods For Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.7808331251144409}]}], "abstractContent": [{"text": "We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disam-biguation task in which we controlled for both unigram and bigram frequency.", "labels": [], "entities": []}, {"text": "The similarity-based methods perform up to 40% better on this particular task.", "labels": [], "entities": []}, {"text": "We also conclude that events that occur only once in the training set have major impact on similarity-based estimates.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of data sparseness affects all statistical methods for natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.653602123260498}]}, {"text": "Even large training sets tend to misrepresent low-probability events, since rare events may not appear in the training corpus at all.", "labels": [], "entities": []}, {"text": "We concentrate hereon the problem of estimating the probability of unseen word pairs, that is, pairs that do not occur in the training set.", "labels": [], "entities": []}, {"text": "Katz's back-off scheme, widely used in bigram language modeling, estimates the probability of an unseen bigram by utilizing unigram estimates.", "labels": [], "entities": []}, {"text": "This has the undesirable result of assigning unseen bigrams the same probability if they are made up of unigrams of the same frequency.", "labels": [], "entities": []}, {"text": "Class-based methods cluster words into classes of similar words, so that one can base the estimate of a word pair's probability on the averaged cooccurrence probability of the classes to which the two words belong.", "labels": [], "entities": []}, {"text": "However, a word is therefore modeled by the average behavior of many words, which may cause the given word's idiosyncrasies to be ignored.", "labels": [], "entities": []}, {"text": "For instance, the word \"red\" might well act like a generic color word inmost cases, but it has distinctive cooccurrence patterns with respect to words like \"apple,\" \"banana,\" and soon.", "labels": [], "entities": []}, {"text": "We therefore consider similarity-based estimation schemes that do not require building general word classes.", "labels": [], "entities": []}, {"text": "Instead, estimates for the most similar words to a word ware combined; the evidence provided byword w' is weighted by a function of its similarity tow.", "labels": [], "entities": []}, {"text": "propose such a scheme for predicting which unseen cooccurrences are more likely than others.", "labels": [], "entities": []}, {"text": "However, their scheme does not assign probabilities.", "labels": [], "entities": []}, {"text": "In what follows, we focus on probabilistic similarity-based estimation methods.", "labels": [], "entities": []}, {"text": "We compared several such methods, including that of and the cooccurrence smoothing method of, against classical estimation methods, including that of Katz, in a decision task involving unseen pairs of direct objects and verbs, where unigram frequency was eliminated from being a factor.", "labels": [], "entities": []}, {"text": "We found that all the similarity-based schemes performed almost 40% better than back-off, which is expected to yield about 50% accuracy in our experimental setting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9990804195404053}]}, {"text": "Furthermore, a scheme based on the total divergence of empirical dis-tributions to their average 1 yielded statistically significant improvement in error rate over cooccurrence smoothing.", "labels": [], "entities": [{"text": "error rate", "start_pos": 148, "end_pos": 158, "type": "METRIC", "confidence": 0.9709649085998535}]}, {"text": "We also investigated the effect of removing extremely low-frequency events from the training set.", "labels": [], "entities": []}, {"text": "We found that, in contrast to backoff smoothing, where such events are often discarded from training with little discernible effect, similarity-based smoothing methods suffer noticeable performance degradation when singletons (events that occur exactly once) are omitted.", "labels": [], "entities": [{"text": "backoff smoothing", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.6610803008079529}]}], "datasetContent": [{"text": "We evaluated the similarity measures listed above on a word sense disambiguation task, in which each method is presented with a noun and two verbs, and decides which verb is more likely to have the noun as a direct object.", "labels": [], "entities": [{"text": "word sense disambiguation task", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.7524533793330193}]}, {"text": "Thus, we do not measure the absolute quality of the assignment of probabilities, as would be the casein a perplexity evaluation, but rather the relative quality.", "labels": [], "entities": []}, {"text": "We are therefore able to ignore constant factors, and so we neither normalize the similarity measures nor calculate the denominator in equation (3).", "labels": [], "entities": []}, {"text": "The performances of the four base language models are shown in table 3.", "labels": [], "entities": []}, {"text": "MLE-1 and MLE-ol both have error rates of exactly .5 because the test sets consist of unseen bigrams, which are all assigned a probability of 0 by maximum-likelihood estimates, and thus are all ties for this method.", "labels": [], "entities": [{"text": "MLE-1", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7950225472450256}, {"text": "error rates", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9753922820091248}]}, {"text": "The back-off models BO-1 and BO-ol also perform similarly.", "labels": [], "entities": [{"text": "BO-1", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.6672484874725342}, {"text": "BO-ol", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.8459482192993164}]}, {"text": "Since the back-off models consistently performed worse than the MLE models, we chose to use only the MLE models in our subsequent experiments.", "labels": [], "entities": []}, {"text": "Therefore, we only ran comparisons between the measures that could utilize unsmoothed data, namely, the Lt norm, L(wx, w~); the total divergence to the average, A(wx, w~); and the confusion probability, Pc(w~lwx).", "labels": [], "entities": [{"text": "Lt norm", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9788462817668915}, {"text": "L", "start_pos": 113, "end_pos": 114, "type": "METRIC", "confidence": 0.7974010705947876}, {"text": "A", "start_pos": 161, "end_pos": 162, "type": "METRIC", "confidence": 0.9843651056289673}, {"text": "confusion probability", "start_pos": 180, "end_pos": 201, "type": "METRIC", "confidence": 0.9507707357406616}]}, {"text": "3 In the full paper, we give detailed examples showing the different neighborhoods induced by the different measures, which we omit here for reasons of space.", "labels": [], "entities": []}, {"text": "shows the results on the five test sets, using MLE-1 as the base language model.", "labels": [], "entities": [{"text": "MLE-1", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.5970327854156494}]}, {"text": "The parameter/3 was always set to the optimal value for the corresponding training set.", "labels": [], "entities": []}, {"text": "RAND, which is shown for comparison purposes, simply chooses the weights W(wl,w~) randomly.", "labels": [], "entities": [{"text": "RAND", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6337077617645264}]}, {"text": "S(wl) was set equal to Vt in all cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Base Language Model Error Rates", "labels": [], "entities": [{"text": "Base Language Model Error Rates", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.4808123290538788}]}]}