{"title": [{"text": "A Word-to-Word Model of Translational Equivalence", "labels": [], "entities": [{"text": "Translational Equivalence", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.9226166307926178}]}], "abstractContent": [{"text": "Many multilingual NLP applications need to translate words between different languages , but cannot afford the computational expense of inducing or applying a full translation model.", "labels": [], "entities": []}, {"text": "For these applications, we have designed a fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level.", "labels": [], "entities": []}, {"text": "The model's preci-sion/recall trade-off can be directly controlled via one threshold parameter.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9940956830978394}]}, {"text": "This feature makes the model more suitable for applications that are not fully statistical.", "labels": [], "entities": []}, {"text": "The model's hidden parameters can be easily conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge such as part-of-speech, dictionaries, word order, etc..", "labels": [], "entities": []}, {"text": "Our model can link word tokens in parallel texts as well as other translation models in the literature.", "labels": [], "entities": []}, {"text": "Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9965052604675293}]}], "introductionContent": [{"text": "Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.8274264335632324}]}, {"text": "However, the IBM models, which attempt to capture abroad range of translation phenomena, are computationally expensive to apply.", "labels": [], "entities": []}, {"text": "Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web, certain machine-assisted translation tools (e.g.), concordancing for bilingual lexicography, computerassisted language learning, corpus linguistics, and cross-lingual information retrieval.", "labels": [], "entities": [{"text": "cross-lingual information retrieval", "start_pos": 319, "end_pos": 354, "type": "TASK", "confidence": 0.6698114077250162}]}, {"text": "In this paper, we present a fast method for inducing accurate translation lexicons.", "labels": [], "entities": []}, {"text": "The method assumes that words are translated one-to-one.", "labels": [], "entities": []}, {"text": "This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models.", "labels": [], "entities": [{"text": "explanatory", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.9502832293510437}]}, {"text": "Section 3.1 also shows how the oneto-one assumption enables us to use anew greedy competitive linking algorithm for re-estimating the model's parameters, instead of more expensive algorithms that consider a much larger set of word correspondence possibilities.", "labels": [], "entities": []}, {"text": "The model uses two hidden parameters to estimate the confidence of its own predictions.", "labels": [], "entities": []}, {"text": "The confidence estimates enable direct control of the balance between the model's precision and recall via a simple threshold.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9991017580032349}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9987645149230957}]}, {"text": "The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9971369504928589}]}], "datasetContent": [{"text": "A word-to-word model of translational equivalence can be evaluated either over types or over tokens.", "labels": [], "entities": [{"text": "translational equivalence", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.9118435084819794}]}, {"text": "It is impossible to replicate the experiments used to evaluate other translation models in the literature, because neither the models nor the programs that induce them are generally available.", "labels": [], "entities": []}, {"text": "For each kind of evaluation, we have found one case where we can come close.", "labels": [], "entities": []}, {"text": "We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards, aligned using the method in . One class represented content-word links and the other represented function-word links 4.", "labels": [], "entities": [{"text": "Canadian Hansards", "start_pos": 100, "end_pos": 117, "type": "DATASET", "confidence": 0.9100569188594818}]}, {"text": "Link types with negative log-likelihood were discarded after each iteration.", "labels": [], "entities": []}, {"text": "Both classes' parameters converged after six iterations.", "labels": [], "entities": []}, {"text": "The value of class-based models was demonstrated by the differences between the hidden parameters for the two classes.", "labels": [], "entities": []}, {"text": "(A +,A-) converged at (.78,00016) for content-class links and at (.43,.000094) for function-class links.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Erroneous link tokens generated by two  translation models.", "labels": [], "entities": []}, {"text": " Table 1. The \"wrong  link\" and \"missing link\" error categories should be  self-explanatory. \"Partial links\" are those where one  French word resulted from multiple English words,  but the model only links the French word to one of  its English sources. \"Class conflict\" errors resulted  from our model's refusal to link content words with  function words. Usually, this is the desired behavior,  but words like English auxiliary verbs are sometimes  used as content words, giving rise to content words  in French. Such errors could be overcome by a model  that classifies each word token, for example using a", "labels": [], "entities": []}]}