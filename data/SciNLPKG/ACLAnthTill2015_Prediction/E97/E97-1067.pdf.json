{"title": [{"text": "Choosing the Word Most Typical in Context Using a Lexical Co-occurrence Network", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a partial solution to a component of the problem of lexical choice: choosing the synonym most typical, or expected, in context.", "labels": [], "entities": []}, {"text": "We apply anew statistical approach to representing the context of a word through lexical co-occurrence networks.", "labels": [], "entities": []}, {"text": "The implementation was trained and evaluated on a large corpus, and results show that the inclusion of second-order co-occurrence relations improves the performance of our implemented lexical choice program.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work views lexical choice as the process of mapping fi'om a set of concepts (in some representation of knowledge) to a word or phrase.", "labels": [], "entities": [{"text": "lexical choice as the process of mapping fi'om a set of concepts (in some representation of knowledge) to a word or phrase", "start_pos": 18, "end_pos": 140, "type": "Description", "confidence": 0.773546539247036}]}, {"text": "When the same concept admits more than one lexicalization, it is often difficult to choose which of these 'synonyms' is the most appropriate for achieving the desired pragmatic goals: but this is necessary for highquality machine translation and natural language generation.", "labels": [], "entities": [{"text": "highquality machine translation", "start_pos": 210, "end_pos": 241, "type": "TASK", "confidence": 0.6019854346911112}, {"text": "natural language generation", "start_pos": 246, "end_pos": 273, "type": "TASK", "confidence": 0.6470646957556406}]}, {"text": "Knowledge-based approaches to representing the potentially subtle differences between synonyms have suffered from a serious lexical acquisition bottleneck.", "labels": [], "entities": []}, {"text": "Statistical approaches, which have sought to explicitly represent differences between pairs of synonyms with respect to their occurrence with other specific words (, are inefficient in time and space.", "labels": [], "entities": []}, {"text": "This paper presents anew statistical approach to modeling context that provides a preliminary solution to an important sub-problem, that of determining the nearsynonym that is most typical, or expected, if any, in a given context.", "labels": [], "entities": []}, {"text": "Although weaker than full lexical choice, because it doesn't choose the 'best' word, we believe that it is a necessary first step, because it would allow one to determine the effects of choosing a non-typical word in place of the typical word.", "labels": [], "entities": []}, {"text": "The approach relies on a generalization of lexical co-occurrence that allows for an implicit representation of the differences between two (or more) words with respect to any actual context.", "labels": [], "entities": []}, {"text": "For example, our implemented lexical choice program selects mistake as most typical for the 'gap' in sentence (1), and error in (2).", "labels": [], "entities": [{"text": "error", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.9560802578926086}]}, {"text": "(1) However, such a move also would run the risk of cutting deeply into U.S. economic growth, which is why some economists think it would be a big {error I mistake [ oversight}.", "labels": [], "entities": []}, {"text": "(2) The {error I mistake t oversight} was magnified when the Army failed to charge the standard percentage rate for packing and handling.", "labels": [], "entities": [{"text": "packing and handling", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.8124512434005737}]}], "datasetContent": [{"text": "To evaluate the lexical choice program, we selected several sets of near-synonyms, shown in table 1, that have low polysemy in the corpus, and that occur with similar frequencies.", "labels": [], "entities": []}, {"text": "This is to reduce the confounding effects of lexical ambiguity.", "labels": [], "entities": []}, {"text": "For each set, we collected all sentences from the yetunseen 1987 Wall Street Journal (part-of-speech-tagged) that contained any of the members of the set, ignoring word sense.", "labels": [], "entities": [{"text": "yetunseen 1987 Wall Street Journal", "start_pos": 50, "end_pos": 84, "type": "DATASET", "confidence": 0.7957838177680969}]}, {"text": "We replaced each occurrence by a 'gap' that the program then had to fill.", "labels": [], "entities": []}, {"text": "We compared the 'correctness' of the choices made by our program to the baseline of always choosing the most frequent synonym according to the training corpus.", "labels": [], "entities": []}, {"text": "But what are the 'correct' responses?", "labels": [], "entities": []}, {"text": "Ideally, they should be chosen by a credible human informant.", "labels": [], "entities": []}, {"text": "But regrettably, we are not in a position to undertake a study of how humans judge typical usage, so we will turn instead to a less ideal source: the authors of the Wall Street Journal.", "labels": [], "entities": [{"text": "authors of the Wall Street Journal", "start_pos": 150, "end_pos": 184, "type": "DATASET", "confidence": 0.784013549486796}]}, {"text": "The problem is, of course, that authors aren't always typical.", "labels": [], "entities": []}, {"text": "A particular word might occur in a 'pattern' in which another synonym was seen more often, making it the typical choice.", "labels": [], "entities": []}, {"text": "Thus, we cannot expect perfect accuracy in this evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.999243974685669}]}, {"text": "shows the results for all seven sets of synonyms under different versions of the program.", "labels": [], "entities": []}, {"text": "We varied two parameters: (1) the window size used during the construction of the network: either narrow (4-4 words), medium (4-10 words), or wide (4-50 words); (2) the maximum order of co-occurrence relation allowed: 1, 2, or 3.", "labels": [], "entities": []}, {"text": "The results show that at least second-order cooccurrences are necessary to achieve better than baseline accuracy in this task; regular co-occurrence relations are insufficient.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9896933436393738}]}, {"text": "This justifies our assumption that we need: Accuracy of several different versions of the iexical choice program.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9869471788406372}]}, {"text": "The best score for each set is in boldface.", "labels": [], "entities": []}, {"text": "Size refers to the size of the sample collection.", "labels": [], "entities": []}, {"text": "All differences from baseline are significant at the 5% level according to Pearson's X 2 test, unless indicated.", "labels": [], "entities": [{"text": "Pearson's X 2 test", "start_pos": 75, "end_pos": 93, "type": "METRIC", "confidence": 0.6135279774665833}]}, {"text": "more than the surrounding context to build adequate contextual representations.", "labels": [], "entities": []}, {"text": "Also, the narrow window gives consistently higher accuracy than the other sizes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.999206006526947}]}, {"text": "This can be explained, perhaps, by the fact that differences between near-synonyms often involve differences in short-distance collocations with neighboring words, e.g., face the task.", "labels": [], "entities": []}, {"text": "There are two reasons why the approach doesn't do as well as an automatic approach ought to.", "labels": [], "entities": []}, {"text": "First, as mentioned above, our method of evaluation is not ideal; it may make our results just seem poor.", "labels": [], "entities": []}, {"text": "Perhaps our results actually show the level of 'typical usage' in the newspaper.", "labels": [], "entities": []}, {"text": "Second, lexical ambiguity is a major problem, affecting both evaluation and the construction of the co-occurrence network.", "labels": [], "entities": []}, {"text": "For example, in sentence (3), above, it turns out that the program uses safety as evidence for choosing job (because job safety is a frequent collocation), but this is the wrong sense of job.", "labels": [], "entities": []}, {"text": "Syntactic and collocational red herrings can add noise too.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy of several different versions of the iexical choice program. The best score for each set is in boldface.  Size refers to the size of the sample collection. All differences from baseline are significant at the 5% level according  to Pearson's X 2 test, unless indicated.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9736045598983765}, {"text": "Pearson's X 2 test", "start_pos": 251, "end_pos": 269, "type": "DATASET", "confidence": 0.8097643613815307}]}]}