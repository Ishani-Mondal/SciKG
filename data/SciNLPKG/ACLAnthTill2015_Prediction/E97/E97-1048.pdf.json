{"title": [{"text": "A Model of Lexical Attraction and Repulsion*", "labels": [], "entities": [{"text": "Lexical Attraction and Repulsion", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.7601718306541443}]}], "abstractContent": [{"text": "This paper introduces new methods based on exponential families for modeling the correlations between words in text and speech.", "labels": [], "entities": []}, {"text": "While previous work assumed the effects of word co-occurrence statistics to be constant over a window of several hundred words, we show that their influence is nonstationary on a much smaller timescale.", "labels": [], "entities": []}, {"text": "Empirical data drawn from En-glish and Japanese text, as well as conversational speech, reveals that the \"attrac-tion\" between words decays exponentially, while stylistic and syntactic contraints create a \"repulsion\" between words that discourages close co-occurrence.", "labels": [], "entities": []}, {"text": "We show that these characteristics are well described by simple mixture models based on two-stage exponential distributions which can be trained using the EM algorithm.", "labels": [], "entities": []}, {"text": "The resulting distance distributions can then be incorporated as penalizing features in an exponential language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the fundamental characteristics of language, viewed as a stochastic process, is that it is highly nonstationary.", "labels": [], "entities": []}, {"text": "Throughout a written document and during the course of spoken conversation, the topic evolves, effecting local statistics on word occurrences.", "labels": [], "entities": []}, {"text": "The standard trigram model disregards this nonstationarity, as does any stochastic grammar whichassigns probabilities to sentences in a contextindependent fashion.", "labels": [], "entities": []}, {"text": "*Research supported in part by NSF grant IRI-9314969, DARPA AASERT award DAAH04-95-1-0475, and the ATR Interpreting Telecommunications Research Laboratories.", "labels": [], "entities": [{"text": "NSF grant IRI-9314969", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.588303397099177}, {"text": "DARPA", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.40999361872673035}, {"text": "AASERT award DAAH04-95-1-0475", "start_pos": 60, "end_pos": 89, "type": "METRIC", "confidence": 0.7924292087554932}, {"text": "ATR Interpreting Telecommunications Research Laboratories", "start_pos": 99, "end_pos": 156, "type": "DATASET", "confidence": 0.8993120074272156}]}, {"text": "Stationary models are used to describe such a dynamic source for at least two reasons.", "labels": [], "entities": []}, {"text": "The first is convenience: stationary models require a relatively small amount of computation to train and to apply.", "labels": [], "entities": []}, {"text": "The second is ignorance: we know so little about how to model effectively the nonstationary characteristics of language that we have for the most part completely neglected the problem.", "labels": [], "entities": [{"text": "ignorance", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9437260031700134}]}, {"text": "From a theoretical standpoint, we appeal to the Shannon-McMillanBreiman theorem whenever computing perplexities on test data; yet this result only rigorously applies to stationary and ergodic sources.", "labels": [], "entities": []}, {"text": "To allow a language model to adapt to its recent context, some researchers have used techniques to update trigram statistics in a dynamic fashion by creating a cache of the most recently seen n-grams which is smoothed together (typically by linear interpolation) with the static model; see for example).", "labels": [], "entities": []}, {"text": "Another approach, using maximum entropy methods similar to those that we present here, introduces a parameter for trigger pairs of mutually informative words, so that the occurrence of certain words in recent context boosts the probability of the words that they trigger.", "labels": [], "entities": []}, {"text": "Triggers have also been incorporated through different methods.", "labels": [], "entities": [{"text": "Triggers", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9296650886535645}]}, {"text": "All of these techniques treat the recent context as a \"bag of words,\" so that a word that appears, say, five positions back makes the same contribution to prediction as words at distances of 50 or 500 positions back in the history.", "labels": [], "entities": []}, {"text": "In this paper we introduce new modeling techniques based on exponential families for capturing the long-range correlations between occurrences of words in text and speech.", "labels": [], "entities": []}, {"text": "We show how for both written text and conversational speech, the empirical distribution of the distance between trig-s t ger words exhibits a striking behavior in which the \"attraction\" between words decays exponentially, while stylistic and syntactic constraints create a \"repulsion\" between words that discourages close cooccurrence.", "labels": [], "entities": []}, {"text": "We have discovered that this observed behavior is well described by simple mixture models based on two-stage exponential distributions.", "labels": [], "entities": []}, {"text": "Though in common use in queueing theory, such distributions have not, to our knowledge, been previously exploited in speech and language processing.", "labels": [], "entities": [{"text": "queueing theory", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.9020356833934784}]}, {"text": "It is remarkable that the behavior of a highly complex stochastic process such as the separation between word cooccurrences is well modeled by such a simple parametric family, just as it is surprising that Zipf's law can so simply capture the distribution of word frequencies inmost languages.", "labels": [], "entities": []}, {"text": "In the following section we present examples of the empirical evidence for the effects of distance.", "labels": [], "entities": []}, {"text": "In Section 3 we outline the class of statistical models that we propose to model this data.", "labels": [], "entities": []}, {"text": "After completing this work we learned of a related paper) which constructs similar models.", "labels": [], "entities": []}, {"text": "In Section 4 we present a parameter estimation algorithm, based on the EM algorithm, for determining the maximum likelihood estimates within the class.", "labels": [], "entities": []}, {"text": "In Section 5 we explain how distance models can be incorporated into an exponential language model, and present sample perplexity results we have obtained using this class of models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}