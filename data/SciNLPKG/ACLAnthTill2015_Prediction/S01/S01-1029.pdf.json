{"title": [{"text": "Disambiguating Noun and Verb Senses Using Automatically Acquired Selectional Preferences*", "labels": [], "entities": [{"text": "Disambiguating Noun and Verb Senses", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6868190288543701}]}], "abstractContent": [{"text": "Our system for the SENSEVAL-2 all words task uses automatically acquired selectional preferences to sense tag subject and object head nouns, along with the associated verbal predicates.", "labels": [], "entities": [{"text": "SENSEVAL-2 all words task", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.8450734168291092}]}, {"text": "The selectional preferences comprise probability distributions over WordN et nouns, and these distributions are conditioned on WordNet verb classes.", "labels": [], "entities": [{"text": "WordN et nouns", "start_pos": 68, "end_pos": 82, "type": "DATASET", "confidence": 0.9439653555552164}, {"text": "WordNet verb classes", "start_pos": 127, "end_pos": 147, "type": "DATASET", "confidence": 0.8816525936126709}]}, {"text": "The conditional distributions are used directly to disambiguate the head nouns.", "labels": [], "entities": []}, {"text": "We use prior distributions and Bayes rule to compute the highest probability verb class, given a noun class.", "labels": [], "entities": []}, {"text": "We also use anaphora resolution and the 'one sense per dis-course' heuristic to cover nouns and verbs not occurring in these relationships in the target text.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.6719022989273071}]}, {"text": "The selectional preferences are acquired without recourse to sense tagged data so our system is unsupervised.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the first SENSEVAL, we used automatically acquired selectional preferences to disambiguate head nouns occurring in specific grammatical relationships).", "labels": [], "entities": []}, {"text": "The selectional preference models provided co-occurrence behaviour between WordNet synsets 1 in the noun hyponym hierarchy and verbal predicates.", "labels": [], "entities": []}, {"text": "Preference scores, based on mutual information, were attached to the classes in the models.", "labels": [], "entities": [{"text": "Preference", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9713941216468811}]}, {"text": "These scores were conditioned on the verbal context and the grammatical relationship in which the nouns for training had occurred.", "labels": [], "entities": []}, {"text": "The system performed compara- \u2022 This work was supported by UK EPSRC projects GR/153175 'PSET: Practical Simplification of English Text' and GR/N36462/93 'Robust Accurate Statistical Parsing (RASP)'.", "labels": [], "entities": [{"text": "UK EPSRC projects GR/153175", "start_pos": 59, "end_pos": 86, "type": "DATASET", "confidence": 0.8178824086983999}, {"text": "PSET: Practical Simplification of English Text", "start_pos": 88, "end_pos": 134, "type": "TASK", "confidence": 0.680539105619703}, {"text": "GR/N36462/93 'Robust Accurate Statistical Parsing (RASP)'", "start_pos": 140, "end_pos": 197, "type": "TASK", "confidence": 0.7111980319023132}]}, {"text": "We will hereafter refer to WordN et synsets as classes.", "labels": [], "entities": [{"text": "WordN et synsets", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.9299179514249166}]}, {"text": "bly to the other system using selectional preferences alone.", "labels": [], "entities": []}, {"text": "The work here is an extension of this earlier work, this time applied to the English all words task.", "labels": [], "entities": []}, {"text": "We use probability distributions rather than mutual information to quantify the preferences.", "labels": [], "entities": []}, {"text": "The preference models are modifications of the Tree Cut Models (TCMs) originally proposed by.", "labels": [], "entities": []}, {"text": "A TCM is a set of classes cutting across the WordNet noun hypernym hierarchy which covers all the nouns of WordNet disjointly, i.e. the classes in the set are not hyponyms of one another.", "labels": [], "entities": [{"text": "WordNet noun hypernym hierarchy", "start_pos": 45, "end_pos": 76, "type": "DATASET", "confidence": 0.9003363102674484}]}, {"text": "The set of classes is associated with a probability distribution.", "labels": [], "entities": []}, {"text": "In our work, we acquire TCMs conditioned on a verb class, rather than a verb form.", "labels": [], "entities": []}, {"text": "We then use Bayes rule to obtain probability estimates for verb classes conditioned on cooccurring noun classes.", "labels": [], "entities": []}, {"text": "Using selectional preferences alone for disambiguation enables us to investigate the situations when they are useful, as well as cases when they are not.", "labels": [], "entities": []}, {"text": "However, this means we loose out in cases where preferences do not provide the necessary information and other complementary information would help.", "labels": [], "entities": []}, {"text": "Another disadvantage of using selectional preferences alone for disambiguation is that the preferences only apply to the grammatical slots for which they have been acquired.", "labels": [], "entities": []}, {"text": "In addition, selectional preferences only help disambiguation for slots where there is a strong enough tie between predicate and argument.", "labels": [], "entities": []}, {"text": "In this work, we use subject and object relationships, since these appear to work better than other relationships), and we use argument heads, rather than the entire argument phrase.", "labels": [], "entities": []}, {"text": "Our basic system is restricted to using only selectional information, and no other source of disambiguating information.", "labels": [], "entities": []}, {"text": "However, we ex-perimented with two methods of extending the coverage to include other grammatical contexts.", "labels": [], "entities": []}, {"text": "The first of these methods is the 'one sense per discourse' heuristic (.", "labels": [], "entities": []}, {"text": "With this method a sense tag fora given word is applied to other occurrences of the same word within the discourse.", "labels": [], "entities": []}, {"text": "The second method uses anaphora resolution to link pronouns to their antecedents.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.6830196380615234}]}, {"text": "Using the anaphoric links we are able to use the preferences fora verb cooccurring with a pronoun with the antecedent of that pronoun.", "labels": [], "entities": []}], "datasetContent": [{"text": "VVe entered three systems for the SENSEVAL-2 English all words task: s ussex-sel Selectional preferences were used alone.", "labels": [], "entities": [{"text": "VVe", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9617019891738892}, {"text": "SENSEVAL-2 English all words task", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.7797419905662537}]}, {"text": "Preferences at the subject slot were applied first, if these were not applicable then the direct object slot was tried.: Analysis of sussex-sel precision for polysemous nouns and verbs sussex-sel-ospd The selectional preferences were applied first, followed by the one sense per discourse heuristic.", "labels": [], "entities": [{"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.889757513999939}]}, {"text": "In the English all words task a discourse was demarcated by a unique text identifier.", "labels": [], "entities": []}, {"text": "sussex-sel-ospd-ana The selectional preferences were used, then the anaphoric links were applied to extend coverage, and finally the one sense per discourse was applied.", "labels": [], "entities": []}, {"text": "The results are shown in table 1.", "labels": [], "entities": []}, {"text": "We only attempted disambiguation for head nouns and verbs in subject and direct object relationships, those tagged using anaphoric links to antecedents in these relationships and those tagged using the one sense per discourse heuristic.", "labels": [], "entities": []}, {"text": "vVe do not include the coarse-grained results which are just slightly better than the finegrained results, and this seems to be typical of other systems.", "labels": [], "entities": []}, {"text": "We did not take advantage of the coarse grained classification as this was not available a.t the time of acquiring the selectional preferences.", "labels": [], "entities": []}, {"text": "From analysis of the fine-grained results of the selectional preference results for system sussex-sel, we see that nouns performed better than verbs because there were more monosemous nouns than verbs.", "labels": [], "entities": []}, {"text": "However, if we remove the monosemous cases, and rely on the preferences, the verbs were disambiguated more accurately than the nouns, having only a.", "labels": [], "entities": []}, {"text": "1% higher random baseline.", "labels": [], "entities": [{"text": "random baseline", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9626697897911072}]}, {"text": "Also, the direct object slot outperformed the subject slot.", "labels": [], "entities": []}, {"text": "In future it would be better to use the preferences from this slot first.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: English all words fine-grained results", "labels": [], "entities": []}, {"text": " Table 2: Analysis of sussex-sel precision for pol- ysemous nouns and verbs", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.6009835600852966}]}]}