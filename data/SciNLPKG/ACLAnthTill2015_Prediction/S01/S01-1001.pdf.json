{"title": [], "abstractContent": [{"text": "SENSEV AL-2: The Second International Workshop on Evaluating Word Sense Disambiguation Systems was held on July 5-6, 2001.", "labels": [], "entities": [{"text": "SENSEV AL-2: The Second International Workshop on Evaluating Word Sense Disambiguation Systems", "start_pos": 0, "end_pos": 94, "type": "TASK", "confidence": 0.6208352973827949}]}, {"text": "This paper gives an overview of SENSEV AL-2, discussing the evaluation exercise, the tasks, the scoring system, and the results.", "labels": [], "entities": [{"text": "SENSEV AL-2", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.7094288766384125}]}, {"text": "It ends with some recommendations for future evaluation exercises.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense disambiguation (WSD) is the problem of automatically deciding which sense a word has in any particular context.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.835054968794187}]}, {"text": "The success of any project in WSD is clearly tied to the evaluation of WSD systems.", "labels": [], "entities": [{"text": "WSD", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.929716944694519}]}, {"text": "SENSEV AL was started in 1997, under the auspices of ACL-SIGLEX, to bring together researchers to discuss and solve the WSD-evaluation problem.", "labels": [], "entities": [{"text": "SENSEV AL", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.5092292726039886}, {"text": "ACL-SIGLEX", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.8948838114738464}, {"text": "WSD-evaluation problem", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.9251252710819244}]}, {"text": "Its aim is to evaluate the strengths and weaknesses of WSD algorithms and systems with respect to different words, different varieties of language, and different languages.", "labels": [], "entities": [{"text": "WSD algorithms", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.8983950018882751}]}, {"text": "SENSEV AL is independent from other evaluation programs in the language technology community, such as TREC and MUC.", "labels": [], "entities": [{"text": "SENSEV AL", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.4317467361688614}, {"text": "TREC", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.5311073660850525}, {"text": "MUC", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.8311658501625061}]}, {"text": "Unlike these programs, SENSEV AL is a 'freelance' program is run entirely by volunteers.", "labels": [], "entities": [{"text": "SENSEV AL", "start_pos": 23, "end_pos": 32, "type": "TASK", "confidence": 0.49222126603126526}]}, {"text": "We'd like to remind everyone that while SENSEV AL takes the guise of a competition, its main function is not to determine a winner but to explore the scientific aspects of word sense disambiguation.", "labels": [], "entities": [{"text": "SENSEV AL", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.5394321829080582}, {"text": "word sense disambiguation", "start_pos": 172, "end_pos": 197, "type": "TASK", "confidence": 0.7525697151819865}]}, {"text": "SENSEV AL held its first evaluation exercise in the summer of 1998, culminating in a workshop at Herstmonceux Castle, England on September 2-4 (.", "labels": [], "entities": [{"text": "SENSEV AL", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.6482332050800323}, {"text": "Herstmonceux Castle, England on September 2-4", "start_pos": 97, "end_pos": 142, "type": "DATASET", "confidence": 0.9121960060937064}]}, {"text": "Following the success of the first workshop, SENSEV AL-2, supported by EURALEX, This paper gives an overview of SENSEV AL-2, discussing the evaluation exercise, the tasks, the scoring system, and the results.", "labels": [], "entities": [{"text": "SENSEV AL-2", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.5161372870206833}, {"text": "EURALEX", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9442188143730164}, {"text": "SENSEV AL-2", "start_pos": 112, "end_pos": 123, "type": "TASK", "confidence": 0.7346523106098175}]}, {"text": "It ends with some recommendations for future evaluation exercises.", "labels": [], "entities": []}], "datasetContent": [{"text": "The same answer format and scoring program was used for SENSEV AL-2 as was used in the first SENSEV AL.", "labels": [], "entities": [{"text": "SENSEV AL-2", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.7279267907142639}, {"text": "SENSEV AL", "start_pos": 93, "end_pos": 102, "type": "DATASET", "confidence": 0.7365925014019012}]}, {"text": "Systems were allowed to tag a word with as many senses as appropriate, giving probabilities, if desired.", "labels": [], "entities": []}, {"text": "If the task had a sense hierarchy or grouping, then fine-and coarse-grained scoring was done.", "labels": [], "entities": []}, {"text": "In fine-grained scoring, a system had to give at least one of the Gold Standard senses.", "labels": [], "entities": [{"text": "Gold Standard", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.8033832311630249}]}], "tableCaptions": [{"text": " Table 1 Submissiom to SENSEV AL-2", "labels": [], "entities": [{"text": "SENSEV AL-2", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.5094017684459686}]}]}