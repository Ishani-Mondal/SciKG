{"title": [{"text": "Combining Heterogeneous Classifiers for Word-Sense Disambiguation", "labels": [], "entities": [{"text": "Word-Sense Disambiguation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.6795616298913956}]}], "abstractContent": [{"text": "The Stanford-CS224N system is an ensemble of simple classifiers.", "labels": [], "entities": []}, {"text": "The first-tier systems are heterogeneous , consisting primarily of naive-Bayes variants, but also including vector space, memory-based, and other classifier types.", "labels": [], "entities": []}, {"text": "These simple classifiers are combined by a second-tier classifier, which variously uses majority voting, weighted voting, or a maximum entropy model.", "labels": [], "entities": []}, {"text": "Results from SENSEVAL-2 lexical sample tasks indicate that, while the individual classifiers perform at a level comparable to middle-scoring team's systems, the combination achieves high performance.", "labels": [], "entities": [{"text": "SENSEVAL-2 lexical sample tasks", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.7214695811271667}]}, {"text": "In this paper, we discuss both our system and lessons learned from its behavior.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of supervised word sense disambiguation (wsD) has been approached using many different classification algorithms, including naive Bayes, decision trees, decision lists, and memory-based learners.", "labels": [], "entities": [{"text": "supervised word sense disambiguation (wsD)", "start_pos": 15, "end_pos": 57, "type": "TASK", "confidence": 0.7558222029890332}]}, {"text": "While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see), it seems to be the case that, given similar features as input, various algorithms do not behave dramatically differently.", "labels": [], "entities": [{"text": "WSD problem", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9311510324478149}]}, {"text": "This was seen in the SENSEVAL-2 results where a large fraction of the systems had scores clustered in a fairly narrow region.", "labels": [], "entities": [{"text": "SENSEVAL-2", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.8091334104537964}]}, {"text": "We began building our system with 23 supervised WSD systems, each submitted by a student taking the natural language processing course (CS224N) at Stanford University.", "labels": [], "entities": [{"text": "WSD", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9532290101051331}]}, {"text": "Students were free to imple1pent whatever WSD This paper is based on work supported in part by the National Science Foundation under Grants IIS-0085896 and IIS-9982226, by an NSF Graduate Fellowship, and by the Research Collaboration between NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation and CSLI, Stanford University.", "labels": [], "entities": [{"text": "NTT Communication Science Laboratories", "start_pos": 242, "end_pos": 280, "type": "DATASET", "confidence": 0.7429737150669098}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results by word. Single classifiers: base  = most-frequent-sense baseline, sngl = best single  first-tier classifier as chosen on held-out data for that  word. Fixed combinations: vot = majority vote, wei", "labels": [], "entities": []}, {"text": " Table 2: Results by part-of-speech, and overall.", "labels": [], "entities": []}]}