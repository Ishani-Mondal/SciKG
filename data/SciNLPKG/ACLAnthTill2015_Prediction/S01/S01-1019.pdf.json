{"title": [{"text": "Semantic Tagging Using W ordN et Examples", "labels": [], "entities": [{"text": "Semantic Tagging", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8287218809127808}, {"text": "W ordN", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.7746332883834839}]}], "abstractContent": [{"text": "This paper describes IITl, IIT2, and IIT3, three versions of a semantic tagging system basing its sense discriminations on WordNet examples.", "labels": [], "entities": [{"text": "IITl", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.8614179491996765}, {"text": "IIT2", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.814163327217102}, {"text": "IIT3", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.9343546628952026}, {"text": "semantic tagging", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.7149989306926727}]}, {"text": "The system uses WordNet relations aggressively, both in identifying examples of words with similar lexical constraints and matching those examples to the context.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ability of natural language understanding systems to determine the meaning of words in context has long been suggested as a necessary precursor to a deep understanding of the context.", "labels": [], "entities": []}, {"text": "Competitions such as SENSEV AL) and SENSEV AL-2 (SENSEV AL-2,) model the determination of word meaning as a choice of one or more items from a fixed sense inventory, comparing a gold standard based on human judgment to the performance of computational word sense disambiguation systems.", "labels": [], "entities": [{"text": "determination of word meaning", "start_pos": 73, "end_pos": 102, "type": "TASK", "confidence": 0.821096196770668}, {"text": "word sense disambiguation", "start_pos": 252, "end_pos": 277, "type": "TASK", "confidence": 0.6828469038009644}]}, {"text": "Statistically based systems that train on tagged data have regularly performed best on these tasks).", "labels": [], "entities": []}, {"text": "The difficulty with these supervised systems is their insatiable need for reliable annotated data, frequently called the \"data acquisition bottleneck.\"", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 122, "end_pos": 138, "type": "TASK", "confidence": 0.6862072348594666}]}, {"text": "The systems described here avoid the data acquisition bottleneck by using only a sense repository, or more specifically the examples and relationships contained in the sense repository.", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.7500457465648651}]}, {"text": "WordNet version 1.7) was chosen as the sense repository for the English Lexical Sample task (where systems disambiguate a single word or collocation in context) and the English All Word task (where systems disambiguate all content words) of the SENSEV AL-2 competition.", "labels": [], "entities": [{"text": "WordNet version 1.7", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.9338577191034952}, {"text": "English Lexical Sample task", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.5665731281042099}, {"text": "SENSEV AL-2 competition", "start_pos": 245, "end_pos": 268, "type": "TASK", "confidence": 0.5718843936920166}]}, {"text": "WordNet defmes a word sense (or synset) as a collection of words that can express the sense, a definition of the sense (called a gloss), zero or more examples of the use of the word sense, and a set of tuples that defme relations between synsets or synset words.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4  SENSEV AL-2 English Lexical Sample Results", "labels": [], "entities": [{"text": "SENSEV AL-2 English Lexical Sample", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.6353475153446198}]}]}