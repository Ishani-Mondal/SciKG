{"title": [], "abstractContent": [{"text": "The English lexical sample task (adjectives and nouns) for SENSEVAL 2 was setup according to the same principles as for SENSEVAL-1, as reported in (Kilgarriff and Rosenzweig, 2000).", "labels": [], "entities": [{"text": "SENSEVAL 2", "start_pos": 59, "end_pos": 69, "type": "TASK", "confidence": 0.8352692127227783}]}, {"text": "(Adjectives and nouns only, because the data preparation for the verbs lexical sample was undertaken alongside that for the English all-words task, and is reported in Palmer et al (this volume).", "labels": [], "entities": []}, {"text": "All discussion below up to the Results section covers only adjectives and nouns.)", "labels": [], "entities": []}, {"text": "1 Lexical sample The lexicon was sampled to give a range of low, medium and high frequency words (see Table 1).", "labels": [], "entities": []}, {"text": "These were all different words to the ones used in SENSEVAL 1. 2 Corpus choic~ For the most part, the British National Corpus (New edition) was used.", "labels": [], "entities": [{"text": "SENSEVAL 1. 2 Corpus choic", "start_pos": 51, "end_pos": 77, "type": "DATASET", "confidence": 0.7252992510795593}, {"text": "British National Corpus (New edition)", "start_pos": 102, "end_pos": 139, "type": "DATASET", "confidence": 0.9143098081861224}]}, {"text": "(The new edition has the advantage that it is available worldwide , so all participants had the opportunity of obtaining it for system training.)", "labels": [], "entities": []}, {"text": "Our goal was to match this source, containing British En-glish, with another, of American English.", "labels": [], "entities": [{"text": "British En-glish", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.8257587254047394}]}, {"text": "In the event, only limited quantities of corpus data for American English were available without copyright complications, so the lion's share of the data was from the BNC with a limited quantity from the Wall Street Journal.", "labels": [], "entities": [{"text": "BNC", "start_pos": 167, "end_pos": 170, "type": "DATASET", "confidence": 0.9492533802986145}, {"text": "Wall Street Journal", "start_pos": 204, "end_pos": 223, "type": "DATASET", "confidence": 0.954221506913503}]}, {"text": "In accordance with standard SENSEVAL procedure , the goal was to have 75 + 15n + 6m instances for each lexical-sample word, where n is the number of senses the word has and m is the number of multiword expressions that the word is part of (both, of course, relative to a specific lexicon).", "labels": [], "entities": []}, {"text": "In practice numbers varied slightly, as instances were deleted because they had the wrong part of speech or were otherwise unus-17 able.", "labels": [], "entities": []}, {"text": "See Table 1 for actual numbers of senses, multiwords expressions and instances.", "labels": [], "entities": []}, {"text": "3 Lexicon choice Here lay the biggest contrast with the SENSEVAL-1 task, which had used Oxford University Press's experimental HECTOR lexicon.", "labels": [], "entities": [{"text": "SENSEVAL-1 task", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.6160692125558853}, {"text": "Oxford University Press's experimental HECTOR lexicon", "start_pos": 88, "end_pos": 141, "type": "DATASET", "confidence": 0.9039598277636937}]}, {"text": "This time, in response to popular acclaim, WordNet was used.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.959718644618988}]}, {"text": "Since SENSEVAL was first mooted, in 1997, WordNet-or-not-WordNet has been a recurring theme.", "labels": [], "entities": [{"text": "SENSEVAL", "start_pos": 6, "end_pos": 14, "type": "TASK", "confidence": 0.9246839284896851}, {"text": "WordNet-or-not-WordNet", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.9372180104255676}]}, {"text": "In favour was the argument that it was already very widely used, almost a de facto standard.", "labels": [], "entities": []}, {"text": "The argument against concerned its sense distinctions.", "labels": [], "entities": []}, {"text": "WordNet, like thesauruses but unlike standard dictionaries, is organised around groups of words of similar meanings (synsets), not around words (with their various meanings).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9401801824569702}]}, {"text": "This means that the priority for the lexicographer is building coherent synsets rather than the coherent analysis of the various meanings of a particular word.", "labels": [], "entities": []}, {"text": "The writer of a thesaurus does not need to pay as much attention to the distinction between two senses of a word, as the writer of a dictionary.", "labels": [], "entities": []}, {"text": "Word sense disambiguation is a task which needs clear and well-motivated sense distinctions.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6444966395696005}]}, {"text": "In English SENSEVAL-1, Word-Net was not used because of concerns that it did not provide clean enough sense distinctions.", "labels": [], "entities": [{"text": "Word-Net", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.9183136820793152}]}, {"text": "While HECTOR provided good sense distinctions , it was unsatisfactory in that it did not cover the whole lexicon so there was no possibility of scaling up.", "labels": [], "entities": []}, {"text": "The case for WordNet-that it was already integrated into so much NLP and WSD work-still stood, so the decision was made to use WordNet.", "labels": [], "entities": [{"text": "WordNet-that", "start_pos": 13, "end_pos": 25, "type": "DATASET", "confidence": 0.9208973050117493}, {"text": "WordNet", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.9634056687355042}]}, {"text": "To guard against cases where WordNet made a distinction between two meanings, but it was not clear what the distinction was, all the words in the lexical sample had their entries reviewed by a", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Lexical sample: rubric for column  headers: Ss=number of fine-grained senses;  Mwe =number of multi-word expressions which  the word participates in (as bear participates in  WordNet headword polar bear); inst = number  of instances tagged; ITA = inter-tagger agree- ment (fine-grained).", "labels": [], "entities": [{"text": "Mwe", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9114014506340027}, {"text": "WordNet headword polar bear", "start_pos": 185, "end_pos": 212, "type": "DATASET", "confidence": 0.9497978687286377}]}, {"text": " Table 3: PR=system precision; ATT= percent- age of cases for which an answer was returned  (\"attempted\").", "labels": [], "entities": [{"text": "PR", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9953157901763916}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.907402753829956}, {"text": "ATT", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9992577433586121}, {"text": "percent- age", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.7413336634635925}]}]}