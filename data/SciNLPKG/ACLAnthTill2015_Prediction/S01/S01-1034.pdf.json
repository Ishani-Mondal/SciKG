{"title": [{"text": "Machine Learning with Lexical Features: The Duluth Approach to Senseval-2", "labels": [], "entities": [{"text": "Senseval-2", "start_pos": 63, "end_pos": 73, "type": "TASK", "confidence": 0.467372328042984}]}], "abstractContent": [{"text": "This paper describes the sixteen Duluth entries in the SENSEVAL-2 comparative exercise among word sense disambiguation systems.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.6935679018497467}]}, {"text": "There were eight pairs of Duluth systems entered in the Spanish and English lexical sample tasks.", "labels": [], "entities": []}, {"text": "These are all based on standard machine learning algorithms that induce classifiers from sense-tagged training text where the context in which ambiguous words occur are represented by simple lexical features.", "labels": [], "entities": []}, {"text": "These are highly portable, robust methods that can serve as a foundation for more tailored approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Duluth systems in SENSEVAL-2 take a supervised learning approach to the Spanish and English lexical sample tasks.", "labels": [], "entities": []}, {"text": "They learn decision trees and Naive Bayesian classifiers from sense-tagged training examples where the context in which an ambiguous word occurs is represented by lexical features.", "labels": [], "entities": []}, {"text": "These include unigrams and bigrams that occur anywhere in the context, and co-occurrences within just a few words of the target word.", "labels": [], "entities": []}, {"text": "These are the only types of features used.", "labels": [], "entities": []}, {"text": "There are no syntactic features, nor is the structure or content of WordNet employed.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9536788463592529}]}, {"text": "As a result these systems are highly portable, and can serve as a foundation for systems that are tailored to particular languages and sense inventories.", "labels": [], "entities": []}, {"text": "The word sense disambiguation literature provides ample evidence that many different kinds of features contribute to the resolution of word meaning.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7052460412184397}, {"text": "resolution of word meaning", "start_pos": 121, "end_pos": 147, "type": "TASK", "confidence": 0.8852199763059616}]}, {"text": "These include part-of-speech, morphology, verb-object relationships, selectional restrictions, lexical features, etc.", "labels": [], "entities": []}, {"text": "When used in combination it is often unclear to what degree each type of feature contributes to overall performance.", "labels": [], "entities": []}, {"text": "It is also unclear to what 139 extent adding new features allows for the disambiguation of previously unresolvable test instances.", "labels": [], "entities": []}, {"text": "One of the long term objectives of our research is to determine which types of features are complementary and cover increasing numbers of test instances as they are added to a representation of context.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training and test data for the English and Spanish lexical sample tasks is split into separate training and test files per word.", "labels": [], "entities": []}, {"text": "A supervised learning algorithm induces a classifier from the training examples fora word, which is then used to assign sense tags to the test instances for that word.", "labels": [], "entities": []}, {"text": "The context in which an ambiguous word occurs is represented by lexical features that are identified using the Bigram Statistics Package (BSP) version 0.4.", "labels": [], "entities": [{"text": "Bigram Statistics Package (BSP) version 0.4", "start_pos": 111, "end_pos": 154, "type": "DATASET", "confidence": 0.9195297881960869}]}, {"text": "This is free software that extracts unigrams and bigrams from text using a variety of statistical methods.", "labels": [], "entities": []}, {"text": "Each unigram or bigram that is identified in the training data is treated as a binary feature that indicates whether or not it occurs in the context of the word being disambiguated.", "labels": [], "entities": []}, {"text": "The free software package SenseTools (version 0.1) converts training and test data into a feature vector representation, based on the output from BSP.", "labels": [], "entities": [{"text": "BSP", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.9838467836380005}]}, {"text": "This becomes the input to the Weka suite of supervised learning algorithms.", "labels": [], "entities": []}, {"text": "Weka induces classifiers from the training examples and applies the sense tags to the test instances.", "labels": [], "entities": []}, {"text": "The same software is used for the English and Spanish text.", "labels": [], "entities": []}, {"text": "BSP and SenseTools are written in Perl and are freely available from www.d.umn.edu;-tpederse/code.html.", "labels": [], "entities": [{"text": "BSP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9583685994148254}]}, {"text": "Weka is written in Java and is freely available from www.cs.waikato.ac.nz/-ml.", "labels": [], "entities": [{"text": "Weka", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8838764429092407}]}, {"text": "There were eight pairs of Duluth systems in the English and Spanish lexical sample tasks.", "labels": [], "entities": []}, {"text": "The only language dependent components are the tokenizers and stop-lists.", "labels": [], "entities": []}, {"text": "For both English and Spanish a stop-list is made up of all words that occur tenor more times in five randomly selected word training files of comparable size.", "labels": [], "entities": []}, {"text": "All Duluth systems exclude the words in the stop-list from being features.", "labels": [], "entities": []}, {"text": "Each pair of systems is summarized below.", "labels": [], "entities": []}, {"text": "All performance results are based on accuracy (correct/total) using fine-grained scoring.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.99949049949646}]}, {"text": "The name of the English system appears first, followed by the Spanish system.", "labels": [], "entities": []}, {"text": "Duluthl/Duluth6 create an ensemble of three Naive Bayesian classifiers, where each is based on a different set of features.", "labels": [], "entities": [{"text": "Duluthl", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.949003279209137}]}, {"text": "The hope is that these different views of the training examples will result in classifiers that make complementary errors, and that their combined performance will be better than any of the individual classifiers.", "labels": [], "entities": []}, {"text": "Separate Naive Bayesian classifiers are learned from each representation of the training examples.", "labels": [], "entities": []}, {"text": "Each classifier assigns probabilities to each of the possible senses of a test instance.", "labels": [], "entities": []}, {"text": "These are summed and the sense with the largest value is used.", "labels": [], "entities": []}, {"text": "This technique is used in many of our ensembles and will be referred to as a weighted vote.", "labels": [], "entities": []}, {"text": "The first feature set is made up of bigrams, i.e., consecutive two word sequences, that can occur anywhere in the context with the ambiguous word.", "labels": [], "entities": []}, {"text": "To be selected as a feature, a bigram must occur two or more times in the training examples and have a log-likelihood ratio ( G 2 ) value 2:: 6.635, which is associated with a p-value of .01.", "labels": [], "entities": [{"text": "log-likelihood ratio ( G 2 ) value 2", "start_pos": 103, "end_pos": 139, "type": "METRIC", "confidence": 0.7954573594033718}]}, {"text": "The second feature set is based on unigrams, i.e., one word sequences, that occur five or more times in the training data.", "labels": [], "entities": []}, {"text": "The third feature set is made up of cooccurrence features that represent words that occur on the immediate left or right of the target word.", "labels": [], "entities": []}, {"text": "In effect, these are bigrams that include the target word.", "labels": [], "entities": []}, {"text": "They must also occur two or more times and have a log-likelihood ratio 2:: 2.706, which is associated with a p-value of .10.", "labels": [], "entities": []}, {"text": "These systems are inspired by, which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features.", "labels": [], "entities": []}, {"text": "However, the current systems only use a three member ensemble to capture the spirit of simplicity and portability that underlies the Duluth approach to SENSEVAL-2.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9855265617370605}]}, {"text": "English accuracy was 53%, Spanish was 58%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9561313390731812}]}, {"text": "Duluth2/Duluth7 learn an ensemble of decision trees via bagging.", "labels": [], "entities": [{"text": "Duluth2", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9373469948768616}]}, {"text": "Ten samples are drawn, with replacement, from the training examples fora word.", "labels": [], "entities": []}, {"text": "A decision tree is learned from each of these permutations of the training examples, and each of these trees becomes a member of the ensemble.", "labels": [], "entities": []}, {"text": "A test instance is assigned a sense based on a weighted vote among the members of the ensemble.", "labels": [], "entities": []}, {"text": "In general decision tree learning can be overly influenced by a small percentage of the training examples, so the goal of bagging is to smooth out this instability.", "labels": [], "entities": [{"text": "decision tree learning", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.7309365669886271}]}, {"text": "There is only one kind of feature used in these systems, bigrams that occur two or more times and have a log-likelihood ratio ;:::: 6.635.", "labels": [], "entities": []}, {"text": "This is one of the three feature sets used in the Duluth1/Duluth6 systems.", "labels": [], "entities": [{"text": "Duluth1/Duluth6", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.7332971493403116}]}, {"text": "The set of bigrams that meet these criteria become candidate features for the J48 decision tree learning algorithm, which is the Weka implementation of the C4.5 algorithm.", "labels": [], "entities": [{"text": "J48 decision tree learning algorithm", "start_pos": 78, "end_pos": 114, "type": "TASK", "confidence": 0.6535756707191467}]}, {"text": "The decision tree learner first constructs a tree of features that characterizes the training data exactly, and then prunes features away to avoid over-fitting and allow it to generalize to the previously unseen test instances.", "labels": [], "entities": []}, {"text": "Thus, a decision tree learner performs a second cycle of feature selection and is not likely to use all of the features that we identify prior to learning with BSP.", "labels": [], "entities": [{"text": "BSP", "start_pos": 160, "end_pos": 163, "type": "DATASET", "confidence": 0.8842644095420837}]}, {"text": "The default C4.5 parameter settings are used for pruning.", "labels": [], "entities": []}, {"text": "These systems are an extension of), which learns a single decision tree where the representation of context is based on bigrams.", "labels": [], "entities": []}, {"text": "This earlier work does not use bagging, and the top 100 bigrams according to the log-likelihood ratio are the candidate features.", "labels": [], "entities": []}, {"text": "English accuracy was 54%, Spanish was 60%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.958023190498352}]}, {"text": "Duluth3 /Duluth8 rely on the same features .as Duluthl/Duluth6, but learn an ensemble of three bagged decision trees instead of an ensemble of Naive Bayesian classifiers.", "labels": [], "entities": [{"text": "Duluth3", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9402256608009338}]}, {"text": "There is a strong contrast between these techniques, since decision tree learners attempt to characterize the training examples and find relationships among the features, while a Naive Bayesian classifier is based on an assumption of conditional independence among the features.", "labels": [], "entities": []}, {"text": "The feature set used in these systems is from Duluthl/Duluth6 and consists of bigrams, unigrams and co-occurrences.", "labels": [], "entities": [{"text": "Duluthl/Duluth6", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.8767388264338175}]}, {"text": "A bagged decision tree is learned for each of the three kinds of features.", "labels": [], "entities": []}, {"text": "The test instances are classified by each of the bagged decision trees, and a majority vote is taken among the members to assign senses to the test instances.", "labels": [], "entities": []}, {"text": "These are the most accurate of the Duluth systems for both English (57%) and Spanish (61%).", "labels": [], "entities": []}, {"text": "These are within 7% of the most accurate overall approaches for English (64%) and Spanish (68%).", "labels": [], "entities": []}], "tableCaptions": []}