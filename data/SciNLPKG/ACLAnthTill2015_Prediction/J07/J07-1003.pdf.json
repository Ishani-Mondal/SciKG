{"title": [{"text": "Word-Level Confidence Estimation for Machine Translation", "labels": [], "entities": [{"text": "Word-Level Confidence Estimation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7549319763978323}, {"text": "Machine Translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.74745774269104}]}], "abstractContent": [{"text": "This article introduces and evaluates several different word-level confidence measures for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.8084577918052673}]}, {"text": "These measures provide a method for labeling each word in an automatically generated translation as corrector incorrect.", "labels": [], "entities": []}, {"text": "All approaches to confidence estimation presented here are based on word posterior probabilities.", "labels": [], "entities": []}, {"text": "Different concepts of word posterior probabilities as well as different ways of calculating them will be introduced and compared.", "labels": [], "entities": []}, {"text": "They can be divided into two categories: System-based methods that explore knowledge provided by the translation system that generated the translations, and direct methods that are independent of the translation system.", "labels": [], "entities": []}, {"text": "The system-based techniques make use of system output, such as word graphs or N-best lists.", "labels": [], "entities": []}, {"text": "The word posterior probability is determined by summing the probabilities of the sentences in the translation hypothesis space that contains the target word.", "labels": [], "entities": [{"text": "word posterior probability", "start_pos": 4, "end_pos": 30, "type": "METRIC", "confidence": 0.6027652025222778}]}, {"text": "The direct confidence measures take other knowledge sources, such as word or phrase lexica, into account.", "labels": [], "entities": []}, {"text": "They can be applied to output from nonstatistical machine translation systems as well.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7549014687538147}]}, {"text": "Experimental assessment of the different confidence measures on various translation tasks and in several language pairs will be presented.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.904698520898819}]}, {"text": "Moreover, the application of confidence measures for rescoring of translation hypotheses will be investigated.", "labels": [], "entities": [{"text": "rescoring of translation hypotheses", "start_pos": 53, "end_pos": 88, "type": "TASK", "confidence": 0.8226882964372635}]}], "introductionContent": [{"text": "The work presented in this article deals with confidence estimation for machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.8424100160598755}]}, {"text": "Because sentences generated by a machine translation system are often incorrect but may contain correct substrings, a method for identifying these correct substrings and finding possible errors is desirable.", "labels": [], "entities": []}, {"text": "For this purpose, each word in the generated target sentence is assigned a value expressing the confidence that it is correct.", "labels": [], "entities": []}, {"text": "Confidence measures have been extensively studied for speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8821517825126648}]}, {"text": "Only recently have researchers started to investigate confidence measures for machine translation ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.8141132593154907}]}, {"text": "In this article, we will develop a sound theoretical framework for calculating and evaluating word confidence measures.", "labels": [], "entities": []}, {"text": "Possible applications of confidence measures include: r marking words with low confidence as potential errors for post-editing r improving translation prediction accuracy in TransType-style interactive machine translation ( r combining output from different machine translation systems: Hypotheses with low confidence can be discarded before selecting one of the system translations (), or the word confidence scores can be used in the generation of new hypotheses from the output of different systems (Jayaraman and Lavie 2005), or the sentence confidence value can be employed for reranking ().", "labels": [], "entities": [{"text": "translation prediction", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.9013360440731049}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.674182116985321}, {"text": "TransType-style interactive machine translation", "start_pos": 174, "end_pos": 221, "type": "TASK", "confidence": 0.5406107380986214}]}, {"text": "The article is organized as follows: In Section 2, we briefly review the statistical approach to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.8153810501098633}]}, {"text": "The phrase-based translation system, which serves as the basis for one of the direct confidence measures, will be presented.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7023074626922607}]}, {"text": "Section 3 gives an overview of related work on confidence estimation for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8162306845188141}]}, {"text": "Moreover, word posterior probabilities will be introduced, and we will explain how they can be used as word-level confidence measures.", "labels": [], "entities": []}, {"text": "In Section 4, we describe so-called systembased methods for confidence estimation, which make use of the output of a statistical machine translation system, such as word graphs or N-best lists.", "labels": [], "entities": [{"text": "confidence estimation", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.6632988750934601}]}, {"text": "In Section 5, we present confidence measures based on direct models.", "labels": [], "entities": []}, {"text": "The combination of several confidence measures into one is described in Section 6.", "labels": [], "entities": []}, {"text": "Experimental evaluation and comparison of the different confidence measures is provided in Section 7.", "labels": [], "entities": []}, {"text": "Section 8 deals with the rescoring of translation hypotheses using confidence measures.", "labels": [], "entities": [{"text": "rescoring of translation hypotheses", "start_pos": 25, "end_pos": 60, "type": "TASK", "confidence": 0.8410677164793015}]}, {"text": "The article concludes in Section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments were performed on three translation tasks in different language pairs.", "labels": [], "entities": []}, {"text": "The corpora were compiled in the EU projects TransType2 (TransType2 2005) and TC-STAR (TC-STAR 2005), and for the NIST MT evaluation campaign).", "labels": [], "entities": [{"text": "TransType2 (TransType2 2005)", "start_pos": 45, "end_pos": 73, "type": "DATASET", "confidence": 0.8349164605140686}, {"text": "TC-STAR (TC-STAR 2005)", "start_pos": 78, "end_pos": 100, "type": "DATASET", "confidence": 0.8115315914154053}, {"text": "NIST MT evaluation", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.5388519763946533}]}, {"text": "The TransType2 corpora consist of technical manuals for Xerox devices such as printers.", "labels": [], "entities": []}, {"text": "They are available in three different language pairs.", "labels": [], "entities": []}, {"text": "This domain is very specialized with respect to terminology and style.", "labels": [], "entities": []}, {"text": "The corpus statistics are given in.", "labels": [], "entities": []}, {"text": "The TC-STAR corpus consists of proceedings of the European Parliament.", "labels": [], "entities": [{"text": "TC-STAR corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.7346032857894897}]}, {"text": "It is a spoken language translation corpus containing the verbatim transcriptions of the speeches in the European Parliament Plenary Sessions (EPPS).", "labels": [], "entities": [{"text": "European Parliament Plenary Sessions (EPPS)", "start_pos": 105, "end_pos": 148, "type": "DATASET", "confidence": 0.7446935772895813}]}, {"text": "The domain is basically unrestricted because a wide range of different topics is covered in the sessions.", "labels": [], "entities": []}, {"text": "The translation direction is from Spanish into English.", "labels": [], "entities": []}, {"text": "For corpus statistics, see.", "labels": [], "entities": []}, {"text": "The NIST corpus was compiled for the yearly MT evaluation campaign carried out since 2001.", "labels": [], "entities": [{"text": "NIST corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9740609228610992}, {"text": "MT evaluation", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9035089313983917}]}, {"text": "Chinese news articles are translated into English.", "labels": [], "entities": []}, {"text": "Similarly to the EPPS data, the domain is basically unrestricted, because a wide range of different topics is covered.", "labels": [], "entities": [{"text": "EPPS data", "start_pos": 17, "end_pos": 26, "type": "DATASET", "confidence": 0.9751544296741486}]}, {"text": "However, the vocabulary size and the training corpus are much larger than in the EPPS collection, as the corpus statistics presented in show.", "labels": [], "entities": [{"text": "EPPS collection", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.9773299992084503}]}, {"text": "Additionally to the bilingual data, a monolingual English corpus consisting of 636M running words was used for language model training.", "labels": [], "entities": []}, {"text": "The SMT systems that generated the translations for which confidence estimation was performed were trained on these corpora.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9861106872558594}]}, {"text": "The same holds for the probability models that were used to estimate the word confidences.", "labels": [], "entities": []}, {"text": "We translated the development and test corpora using several different MT systems for testing the confidence measures: r The phrase-based translation system described in Section 2.2 (denoted as PBT in the tables); a large part of the results will be presented for output of this system.", "labels": [], "entities": []}, {"text": "r The alignment template system (Och and Ney 2004) (denoted as AT in the tables), which is also a state-of-the art phrase-based translation system.", "labels": [], "entities": [{"text": "AT", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9941680431365967}]}, {"text": "r The Systran version available at http://babelfish.altavista.com/tr in June 2005.", "labels": [], "entities": []}, {"text": "These hypotheses were used to investigate whether the direct confidence measures perform well on translations generated by a structurally different system.", "labels": [], "entities": []}, {"text": "The translation quality on the TransType2 task in terms of WER, PER, BLEU score (, and NIST score (NIST 2002) is given in.", "labels": [], "entities": [{"text": "WER", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9970691800117493}, {"text": "PER", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9872279167175293}, {"text": "BLEU score", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9847866594791412}, {"text": "NIST score (NIST 2002)", "start_pos": 87, "end_pos": 109, "type": "DATASET", "confidence": 0.7243634760379791}]}, {"text": "We see that the best results are obtained on Spanish to English translation, followed by French to English and German to English.", "labels": [], "entities": [{"text": "Spanish to English translation", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.596088744699955}]}, {"text": "The reason that Systran generates translations of much lower quality than the SMT systems is due to the fact that the technical manuals are very specific in terminology.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.962536096572876}]}, {"text": "The SMT systems were trained on similar corpora so that they are familiar with the terminology.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9859559535980225}]}, {"text": "The table additionally shows the translation quality achieved by the system PBT on the NIST test set.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9490209817886353}, {"text": "NIST test set", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.9804186622301737}]}, {"text": "On the EPPS task from TC-STAR, the confidence measures were tested on output from the phrase-based translation system.", "labels": [], "entities": [{"text": "EPPS task from TC-STAR", "start_pos": 7, "end_pos": 29, "type": "DATASET", "confidence": 0.6770054847002029}, {"text": "phrase-based translation", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.6865156441926956}]}, {"text": "The hypotheses are generated by the version of the system that participated in the TC-STAR evaluation round in March 2005 and that was ranked first there.", "labels": [], "entities": [{"text": "TC-STAR evaluation round in March 2005", "start_pos": 83, "end_pos": 121, "type": "DATASET", "confidence": 0.7851937413215637}]}, {"text": "The translation quality can be seen in later in this article.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.929824709892273}]}, {"text": "After computing the confidence measure, each generated word is tagged as either corrector incorrect, depending on whether its confidence exceeds the tagging threshold that was optimized on the development set beforehand.", "labels": [], "entities": []}, {"text": "The performance of the confidence measures is evaluated using the following three measures: r Classification or Confidence Error Rate (CER): This is defined as the number of incorrect tags divided by the total number of generated words in the translated sentence.", "labels": [], "entities": [{"text": "r Classification or Confidence Error Rate (CER)", "start_pos": 92, "end_pos": 139, "type": "METRIC", "confidence": 0.9098427560594347}]}, {"text": "The baseline CER is determined by assigning the most frequent class (in the whole development or test corpus) to all words.", "labels": [], "entities": [{"text": "CER", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.4867075979709625}]}, {"text": "Assume that the correct classes of the words are defined on the basis of WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9908214807510376}]}, {"text": "If the overall WER on the considered development or test corpus is below 50%, the baseline CER is calculated by tagging all words as correct.", "labels": [], "entities": [{"text": "WER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9933100938796997}, {"text": "CER", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.7444777488708496}]}, {"text": "The baseline CER then corresponds to the number of substitutions and insertions, divided by the number of generated words.", "labels": [], "entities": [{"text": "CER", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.8361512422561646}]}, {"text": "The CER strongly depends on the tagging threshold.", "labels": [], "entities": [{"text": "CER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.980659544467926}]}, {"text": "Therefore, the tagging threshold is adjusted beforehand (to minimize CER) on a development corpus distinct from the test set.", "labels": [], "entities": [{"text": "CER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9980411529541016}]}, {"text": "Moreover, we will present significance bounds for the baseline CER.", "labels": [], "entities": [{"text": "CER", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.6950306296348572}]}, {"text": "They were determined using the bootstrap estimation method described in. r Receiver Operating Characteristic (ROC) curve: The ROC curve plots the correct rejection rate versus the correct acceptance rate for different values of the tagging threshold.", "labels": [], "entities": [{"text": "r Receiver Operating Characteristic (ROC) curve", "start_pos": 73, "end_pos": 120, "type": "METRIC", "confidence": 0.8813285380601883}]}, {"text": "The correct rejection rate is the number of incorrectly translated words that were tagged as false, divided by the total number of incorrectly translated words.", "labels": [], "entities": [{"text": "correct rejection rate", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.9179677565892538}]}, {"text": "The correct acceptance rate is the ratio of correctly translated words that were tagged as correct.", "labels": [], "entities": [{"text": "correct acceptance rate", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.8076975544293722}]}, {"text": "These two rates depend on each other: If one of them is restricted by a lower bound, the other one cannot be restricted.", "labels": [], "entities": []}, {"text": "The further the ROC curve lies away from the diagonal (and away from the point of origin), the better the performance of the confidence measure.", "labels": [], "entities": [{"text": "ROC", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.8254351615905762}]}, {"text": "Unlike the CER, the ROC curve is independent of the prior probability of the two classes correct and incorrect.", "labels": [], "entities": [{"text": "CER", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.6275105476379395}, {"text": "ROC curve", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9530672132968903}]}, {"text": "This means that ROC curves from different data sets can be compared directly.", "labels": [], "entities": []}, {"text": "r Integral of the ROC curve (IROC): ROC curves provide fora qualitative analysis of classifier performance; a related quantitative metric is IROC, defined as the area under a ROC curve.", "labels": [], "entities": [{"text": "ROC curve (IROC)", "start_pos": 18, "end_pos": 34, "type": "METRIC", "confidence": 0.6325053453445435}, {"text": "IROC", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9944139719009399}]}, {"text": "The IROC takes on values in, with 0.5 corresponding to a random separation of correct and incorrect words, 1.0 corresponding to a perfect separation, and 0.0 the opposite.", "labels": [], "entities": [{"text": "IROC", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.5385387539863586}]}, {"text": "Rescoring was carried out on EPPS data using the direct phrase-based confidence measures.", "labels": [], "entities": [{"text": "EPPS data", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.9790935218334198}]}, {"text": "Within the project TC-STAR, an MT evaluation campaign was performed in March 2005 to compare the research systems of the consortium members ( ).", "labels": [], "entities": [{"text": "TC-STAR", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.7868372201919556}, {"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9742529392242432}]}, {"text": "Different conditions concerning the input data were defined.", "labels": [], "entities": []}, {"text": "In the following, rescoring results on the verbatim transcriptions will be presented.", "labels": [], "entities": []}, {"text": "The translations that RWTH submitted to this evaluation were generated by the phrase-based translation system described in Section 2.2.", "labels": [], "entities": [{"text": "RWTH", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.8803472518920898}]}, {"text": "N-best lists were generated for development and test corpus, with a maximum length of 20,000 and 15,000, respectively.", "labels": [], "entities": []}, {"text": "These were then rescored with an IBM model 1, a 4-gram language model, and a deletion model based on IBM-1.", "labels": [], "entities": [{"text": "IBM-1", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.9556750655174255}]}, {"text": "The weights for all these models and for the sentence probability assigned by the SMT system were optimized with respect to BLEU score on the development corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9835140705108643}, {"text": "BLEU score", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.977710098028183}]}, {"text": "For a detailed description of the system, see.", "labels": [], "entities": []}, {"text": "This system was ranked first in the evaluation round according to all evaluation criteria ( ).", "labels": [], "entities": []}, {"text": "Two different sets of rescoring experiments were performed.", "labels": [], "entities": []}, {"text": "They differ only in their starting points: The first one starts from the baseline system without rescoring.", "labels": [], "entities": []}, {"text": "The sub-model weights of this system were optimized with respect to BLEU on the development set, but no additional models were used for rescoring the N-best list.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.998778760433197}]}, {"text": "This experiment was performed to analyze the maximum improvement that can be achieved through rescoring with confidence measures.", "labels": [], "entities": []}, {"text": "The second experiment starts from the system that has already been rescored with the three different models mentioned above.", "labels": [], "entities": []}, {"text": "This is the system that was used in the TC-STAR evaluation campaign, and that was ranked first there.", "labels": [], "entities": [{"text": "TC-STAR evaluation campaign", "start_pos": 40, "end_pos": 67, "type": "DATASET", "confidence": 0.6838751037915548}]}, {"text": "In this setting, it can be seen whether the rescoring with confidence measures manages to improve upon the best available system as well.", "labels": [], "entities": [{"text": "rescoring", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.966079592704773}]}, {"text": "Furthermore, it is possible to analyze whether the gains from all rescoring models are additive.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The upper block evaluates the translation quality without considering case, and the second one contains the case-sensitive evaluation.", "labels": [], "entities": []}, {"text": "These different figures are presented herein order to separate the effect of the translation and the true-casing process.", "labels": [], "entities": []}, {"text": "The translation system was trained on a lower-cased corpus, and the true-casing is performed as an additional post-processing step.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9704593420028687}]}, {"text": "Let us first consider the case-insensitive results.", "labels": [], "entities": []}, {"text": "The baseline is the single best output of the translation system.", "labels": [], "entities": []}, {"text": "This system can be improved through rescoring with confidence measures by 1 BLEU point.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.999290943145752}]}, {"text": "This is only 0.1 BLEU points less than the gain achieved from rescoring with the three other models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.99968421459198}]}, {"text": "The system from the second setup (rescored with IBM model 1, the language and the deletion model) improves the BLEU score by 1.1 points over the baseline.", "labels": [], "entities": [{"text": "IBM model 1", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.8958863218625387}, {"text": "BLEU score", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9783824980258942}]}, {"text": "Another 0.6 BLEU points can be gained through additional rescoring with the direct phrase-based confidence measures.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.999556839466095}]}, {"text": "The improvement is consistent across all four automatic evaluation criteria.", "labels": [], "entities": []}, {"text": "Naturally, the gain in BLEU score is higher than for the other measures, because the system was optimized with respect to BLEU.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9696226716041565}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9870085120201111}]}, {"text": "In the TC-STAR evaluation campaign, case information was taken into account.", "labels": [], "entities": [{"text": "TC-STAR evaluation", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.552812248468399}]}, {"text": "The corresponding results are presented in the second block of the table.", "labels": [], "entities": []}, {"text": "The overall translation quality is lower if case is considered.", "labels": [], "entities": [{"text": "translation", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.956462562084198}]}, {"text": "For all models applied here, the gain achieved through rescoring is not as big as in the case-insensitive evaluation.", "labels": [], "entities": []}, {"text": "If only the confidence measures are used for rescoring, the BLEU score is increased by 0.5 points.", "labels": [], "entities": [{"text": "rescoring", "start_pos": 45, "end_pos": 54, "type": "TASK", "confidence": 0.9509859681129456}, {"text": "BLEU score", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9806913733482361}]}, {"text": "The NIST score and the error measures change only slightly.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.757447361946106}, {"text": "error", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9801637530326843}]}, {"text": "However, when all four rescoring models are applied, the system is significantly improved.", "labels": [], "entities": []}, {"text": "The models used in the TC-STAR evaluation yield an increase of 0.8 BLEU points.", "labels": [], "entities": [{"text": "TC-STAR", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.6697178483009338}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9994572997093201}]}, {"text": "The word posterior probabilities add another 0.3 points to this.", "labels": [], "entities": []}, {"text": "This change is rather small, but comparable to the contribution of each single rescoring model used in the evaluation campaign.", "labels": [], "entities": []}, {"text": "For comparison, the translation quality of the second best system in this campaign is reported in the last row of the table.", "labels": [], "entities": [{"text": "translation", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9443991184234619}]}, {"text": "The difference in BLEU score between the RWTH system and the second best can be significantly improved through rescoring.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9806616008281708}, {"text": "RWTH", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.6223178505897522}]}], "tableCaptions": [{"text": " Table 1  Statistics of the training, development, and test corpora for the TransType2 task.", "labels": [], "entities": []}, {"text": " Table 2  Statistics of the training, development, and test corpora for the TC-STAR EPPS Spanish-English  task. Both development and test corpus are provided with two English references.", "labels": [], "entities": [{"text": "TC-STAR EPPS Spanish-English  task", "start_pos": 76, "end_pos": 110, "type": "TASK", "confidence": 0.6434673219919205}]}, {"text": " Table 4  Translation quality of different MT systems on the TransType2 and the NIST test corpora.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9454265236854553}, {"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9858842492103577}, {"text": "NIST test corpora", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.9643392165501913}]}, {"text": " Table 5  Ratio of correct words (%) in the EPPS Spanish \u2192 English development and test corpora,  according to different word error measures.", "labels": [], "entities": [{"text": "EPPS Spanish \u2192 English development", "start_pos": 44, "end_pos": 78, "type": "DATASET", "confidence": 0.8733021497726441}]}, {"text": " Table 6  Classification performance in terms of CER (%) and IROC (%) for different confidence measures.  TransType2 French \u2192 English test set. References based on WER and PER, confidence measures  optimized accordingly. Hypotheses from the phrase-based system.", "labels": [], "entities": [{"text": "CER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9954440593719482}, {"text": "IROC", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9984375834465027}, {"text": "TransType2 French \u2192 English test set", "start_pos": 106, "end_pos": 142, "type": "DATASET", "confidence": 0.7733727395534515}, {"text": "WER", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.8387845754623413}, {"text": "PER", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.9577540755271912}]}, {"text": " Table 7  Classification performance in terms of CER (%) and IROC (%) for different system-independent  confidence measures. TransType2 test sets. Reference based on WER. Hypotheses from different  MT systems.", "labels": [], "entities": [{"text": "CER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9963333606719971}, {"text": "IROC", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.998155415058136}, {"text": "TransType2 test sets", "start_pos": 125, "end_pos": 145, "type": "DATASET", "confidence": 0.9120301604270935}, {"text": "WER", "start_pos": 166, "end_pos": 169, "type": "METRIC", "confidence": 0.4678601026535034}, {"text": "MT", "start_pos": 198, "end_pos": 200, "type": "TASK", "confidence": 0.958817720413208}]}, {"text": " Table 8  Classification performance in terms of CER (%) and IROC (%) for different confidence measures.  EPPS Spanish \u2192 English test set. Reference based on m-WER and m-PER, confidence measures  optimized accordingly. Hypotheses from the phrase-based system.", "labels": [], "entities": [{"text": "CER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9948491454124451}, {"text": "IROC", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9983848333358765}, {"text": "EPPS Spanish \u2192 English test set", "start_pos": 106, "end_pos": 137, "type": "DATASET", "confidence": 0.8876693646113077}]}, {"text": " Table 9  Classification performance in terms of CER (%) and IROC (%) for different confidence measures.  EPPS Spanish \u2192 English test set. Reference based on pooled WER and PER, confidence  measures optimized accordingly. Hypotheses from the phrase-based system.", "labels": [], "entities": [{"text": "CER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9953967928886414}, {"text": "IROC", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9985470175743103}, {"text": "EPPS Spanish \u2192 English test set", "start_pos": 106, "end_pos": 137, "type": "DATASET", "confidence": 0.8766254782676697}, {"text": "WER", "start_pos": 165, "end_pos": 168, "type": "METRIC", "confidence": 0.8818141222000122}, {"text": "PER", "start_pos": 173, "end_pos": 176, "type": "METRIC", "confidence": 0.8637081980705261}]}, {"text": " Table 10  Classification performance in terms of CER (%) and IROC (%) for different confidence measures.  NIST04 Chinese \u2192 English test set. Reference based on m-WER and m-PER, confidence  measures optimized accordingly. Hypotheses from the phrase-based system.", "labels": [], "entities": [{"text": "CER", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9960122108459473}, {"text": "IROC", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9986227750778198}, {"text": "NIST04 Chinese \u2192 English test set", "start_pos": 107, "end_pos": 140, "type": "DATASET", "confidence": 0.928074578444163}]}, {"text": " Table 11  Classification performance in terms of CER (%) and IROC (%) for a log-linear combination of  word posterior probabilities. Test sets from all three tasks. References based on m-WER and  m-PER. Hypotheses from the phrase-based system.", "labels": [], "entities": [{"text": "CER", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9960357546806335}, {"text": "IROC", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9987623691558838}]}, {"text": " Table 12  Translation quality for rescoring with confidence measures. EPPS Spanish \u2192 English test set.  Optimized for BLEU.", "labels": [], "entities": [{"text": "Translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9429047703742981}, {"text": "EPPS Spanish \u2192 English test set", "start_pos": 71, "end_pos": 102, "type": "DATASET", "confidence": 0.824118842681249}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9929890632629395}]}]}