{"title": [], "abstractContent": [{"text": "We present a statistical machine translation model that uses hierarchical phrases-phrases that contain subphrases.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6192346513271332}]}, {"text": "The model is formally asynchronous context-free grammar but is learned from a parallel text without any syntactic annotations.", "labels": [], "entities": []}, {"text": "Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.7599359154701233}]}, {"text": "We describe our system's training and decoding methods in detail, and evaluate it for translation speed and translation accuracy.", "labels": [], "entities": [{"text": "translation", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.9603554606437683}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.872534990310669}]}, {"text": "Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrase-based system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9980556964874268}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.7785986065864563}]}], "introductionContent": [{"text": "The alignment template translation model) and related phrase-based models advanced the state of the art in machine translation by expanding the basic unit of translation from words to phrases, that is, substrings of potentially unlimited size (but not necessarily phrases in any syntactic theory).", "labels": [], "entities": [{"text": "alignment template translation", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.6500122348467509}, {"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7479293644428253}]}, {"text": "These phrases allow a model to learn local reorderings, translations of multiword expressions, or insertions and deletions that are sensitive to local context.", "labels": [], "entities": []}, {"text": "This makes them a simple and powerful mechanism for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.977125883102417}]}, {"text": "The basic phrase-based model is an instance of the noisy-channel approach.", "labels": [], "entities": []}, {"text": "Following convention, we call the source language \"French\" and the target language \"English\"; the translation of a French sentence f into an English sentence e is modeled as: = arg max The phrase-based translation model P( f | e) \"encodes\" e into f by the following steps: 1.", "labels": [], "entities": []}, {"text": "segment e into phrases \u00af e 1 \u00b7 \u00b7 \u00b7 \u00af e I , typically with a uniform distribution over segmentations;", "labels": [], "entities": []}], "datasetContent": [{"text": "The implementation of our system, named Hiero, is in Python, a bytecode-interpreted language, and optimized using Psyco, a just-in-time compiler, and Pyrex, a Python-like compiled language, with C++ code from the SRI Language Modeling Toolkit.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.8351865410804749}, {"text": "SRI Language Modeling Toolkit", "start_pos": 213, "end_pos": 242, "type": "TASK", "confidence": 0.6794394254684448}]}, {"text": "In this section we report on experiments with Mandarin-toEnglish translation.", "labels": [], "entities": [{"text": "Mandarin-toEnglish translation", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.6258157789707184}]}, {"text": "Our evaluation metric is case-insensitive BLEU-4 (, as defined by NIST, that is, using the shortest (as opposed to closest) reference sentence length for the brevity penalty.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9861366748809814}, {"text": "NIST", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.9204076528549194}]}, {"text": "We ran the grammar extractor of Section 3.2 on the parallel corpora listed in with the exception of the United Nations data, fora total of 28 million words (English side).", "labels": [], "entities": [{"text": "United Nations data", "start_pos": 104, "end_pos": 123, "type": "DATASET", "confidence": 0.9117180109024048}]}, {"text": "We then filtered this grammar for our development set, which was the 2002 NIST MT evaluation dry-run data, and our test sets, which were the data from the NIST MT evaluations.", "labels": [], "entities": [{"text": "NIST MT evaluation dry-run data", "start_pos": 74, "end_pos": 105, "type": "DATASET", "confidence": 0.8842640161514282}, {"text": "NIST MT evaluations", "start_pos": 155, "end_pos": 174, "type": "DATASET", "confidence": 0.7971116503079733}]}, {"text": "Some example rules are shown in, and the sizes of the filtered grammars are shown in.", "labels": [], "entities": []}, {"text": "We also used the SRI Language Modeling Toolkit to train two trigram language models with modified Kneser-Ney smoothing: one on 2.8 billion words from the English Gigaword corpus, and the other on the English side of the parallel text (28 million words).", "labels": [], "entities": [{"text": "SRI Language Modeling Toolkit", "start_pos": 17, "end_pos": 46, "type": "DATASET", "confidence": 0.6843856796622276}, {"text": "English Gigaword corpus", "start_pos": 154, "end_pos": 177, "type": "DATASET", "confidence": 0.8190516630808512}]}, {"text": "shows the average decoding time on part of the development set for the three LM-incorporation methods described in Section 5.3, on a single processor of a dual 3 GHz Xeon machine.", "labels": [], "entities": []}, {"text": "For these experiments, only the Gigaword language model was used.", "labels": [], "entities": []}, {"text": "We set b = 30, \u03b2 = 1 for X cells, b = 15, \u03b2 = 1 for S cells, and b = 100 for rules except where noted in.", "labels": [], "entities": []}, {"text": "Note that values for \u03b2 and \u03b5 are only meaningful relative to the scale of the feature weights; here, the language model weight was 0.06.", "labels": [], "entities": []}, {"text": "The feature weights were obtained by minimum-error-rate training using the cube-pruning (\u03b5 = 0.1) decoder.", "labels": [], "entities": []}, {"text": "For the LM rescoring decoder, parsing and k-best list generation used feature weights optimized for the \u2212LM model, but rescoring used the same weights as the other experiments.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9856265187263489}]}], "tableCaptions": [{"text": " Table 2  Corpora used in training data. Sizes are approximate and in millions of words.", "labels": [], "entities": []}, {"text": " Table 4  Test set sizes, with grammar sizes for two systems.", "labels": [], "entities": []}, {"text": " Table 5  Comparison of three methods for decoding with a language model. Time = mean per-sentence  user+system time, in seconds. BLEU = case-insensitive BLEU-4. All tests were on the first 400  sentences of the development set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9986779093742371}, {"text": "BLEU-4", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9320278167724609}]}, {"text": " Table 6  Feature weights obtained by minimum-error-rate training.", "labels": [], "entities": []}, {"text": " Table 7  Results on baseline systems and hierarchical system. Also shown are the 95% confidence  intervals, obtained using bootstrap resampling.", "labels": [], "entities": []}]}