{"title": [{"text": "Weighted and Probabilistic Context-Free Grammars Are Equally Expressive", "labels": [], "entities": []}], "abstractContent": [{"text": "This article studies the relationship between weighted context-free grammars (WCFGs), where each production is associated with a positive real-valued weight, and probabilistic context-free grammars (PCFGs), where the weights of the productions associated with a nonterminal are constrained to sum to one.", "labels": [], "entities": []}, {"text": "Because the class of WCFGs properly includes the PCFGs, one might expect that WCFGs can describe distributions that PCFGs cannot.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9499605894088745}]}, {"text": "542-549, College Park, MD) proved that every WCFG distribution is equivalent to some PCFG distribution.", "labels": [], "entities": []}, {"text": "We extend their results to conditional distributions, and show that every WCFG conditional distribution of parses given strings is also the conditional distribution defined by some PCFG, even when the WCFG's partition function diverges.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 181, "end_pos": 185, "type": "DATASET", "confidence": 0.9290984869003296}]}, {"text": "This shows that any parsing or labeling accuracy improvement from conditional estimation of WCFGs or conditional random fields (CRFs) over joint estimation of PCFGs or hidden Markov models (HMMs) is due to the estimation procedure rather than the change in model class, because PCFGs and HMMs are exactly as expressive as WCFGs and chain-structured CRFs, respectively.", "labels": [], "entities": [{"text": "parsing or labeling", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.799919605255127}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9155213236808777}]}], "introductionContent": [{"text": "In recent years the field of computational linguistics has turned to machine learning to aid in the development of accurate tools for language processing.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.749314546585083}, {"text": "language processing", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.701430082321167}]}, {"text": "A widely used example, applied to parsing and tagging tasks of various kinds, is a weighted grammar.", "labels": [], "entities": [{"text": "parsing and tagging tasks", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.7905929088592529}]}, {"text": "Adding weights to a formal grammar allows disambiguation (more generally, ranking of analyses) and can lead to more efficient parsing.", "labels": [], "entities": []}, {"text": "Machine learning comes in when we wish to choose those weights empirically.", "labels": [], "entities": [{"text": "Machine learning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7966956496238708}]}, {"text": "The predominant approach for many years was to select a probabilistic modelsuch as a hidden Markov model (HMM) or probabilistic context-free grammar (PCFG)-that defined a distribution over the structures allowed by a grammar.", "labels": [], "entities": []}, {"text": "Given a Unless otherwise specified, we assume a fixed underlying context-free grammar G.", "labels": [], "entities": []}, {"text": "Let \u2126(G) be the set of (finite) trees that G generates.", "labels": [], "entities": []}, {"text": "For any \u03c4 \u2208 \u2126(G), the score s \u0398 (\u03c4) of \u03c4 is defined as follows: where f (X \u2192 \u03b1; \u03c4) is the number of times X \u2192 \u03b1 is used in the derivation of the tree \u03c4.", "labels": [], "entities": []}, {"text": "The partition function Z(\u0398) is the sum of the scores of the trees in \u2126(G).", "labels": [], "entities": []}, {"text": "Because we have imposed no constraints on \u0398, the partition function need not equal one; indeed, as we show subsequently the partition function need not even exist.", "labels": [], "entities": []}, {"text": "If Z(\u0398) is finite then we say that the WCFG is convergent, and we can define a Gibbs probability distribution over \u2126(G) by dividing by Z(\u0398): A probabilistic CFG, or PCFG, is a WCFG in which the sum of the weights of the rules expanding each nonterminal is one:", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}