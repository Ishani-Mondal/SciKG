{"title": [{"text": "Unsupervised Acquisition of Predominant Word Senses", "labels": [], "entities": []}], "abstractContent": [{"text": "There has been a great deal of recent research into word sense disambiguation, particularly since the inception of the Senseval evaluation exercises.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.7775851686795553}, {"text": "Senseval evaluation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7904975116252899}]}, {"text": "Because a word often has more than one meaning, resolving word sense ambiguity could benefit applications that need some level of semantic interpretation of language input.", "labels": [], "entities": []}, {"text": "A major problem is that the accuracy of word sense disambiguation systems is strongly dependent on the quantity of manually sense-tagged data available, and even the best systems, when tagging every word token in a document, perform little better than a simple heuristic that guesses the first, or predominant, sense of a word in all contexts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9965444207191467}, {"text": "word sense disambiguation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.7108473976453146}]}, {"text": "The success of this heuristic is due to the skewed nature of word sense distributions.", "labels": [], "entities": []}, {"text": "Data for the heuristic can come from either dictionaries or a sample of sense-tagged data.", "labels": [], "entities": []}, {"text": "However, there is a limited supply of the latter, and the sense distributions and predominant sense of a word can depend on the domain or source of a document.", "labels": [], "entities": []}, {"text": "(The first sense of \"star\" for example would be different in the popular press and scientific journals).", "labels": [], "entities": []}, {"text": "In this article, we expand on a previously proposed method for determining the predominant sense of a word automatically from raw text.", "labels": [], "entities": [{"text": "determining the predominant sense of a word automatically from raw text", "start_pos": 63, "end_pos": 134, "type": "TASK", "confidence": 0.6770035163922743}]}, {"text": "We look at a number of different data sources and parameterizations of the method, using evaluation results and error analyses to identify where the method performs well and also where it does not.", "labels": [], "entities": []}, {"text": "In particular, we find that the method does notwork as well for verbs and adverbs as nouns and adjectives, but produces more accurate predominant sense information than the widely used SemCor corpus for nouns with low coverage in that corpus.", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 185, "end_pos": 198, "type": "DATASET", "confidence": 0.7140963822603226}]}, {"text": "We further show that the method is able to adapt successfully to domains when using domain specific corpora as input and where the input can either be hand-labeled for domain or automatically classified.", "labels": [], "entities": []}], "introductionContent": [{"text": "In word sense disambiguation, the \"first sense\" heuristic (choosing the first, or predominant sense of a word) is used by most state-of-the-art systems as a back-off method when information from the context is not sufficient to make a more informed choice.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.6933790246645609}]}, {"text": "In this article, we present an in-depth study of a method for automatically acquiring predominant senses for words from raw text ().", "labels": [], "entities": []}, {"text": "The method uses distributionally similar words listed as \"nearest neighbors\" in automatically acquired thesauruses (e.g., Lin 1998a), and takes advantage of the observation that the more prevalent a sense of a word, the more neighbors will relate to that sense, and the higher their distributional similarity scores will be.", "labels": [], "entities": [{"text": "distributional similarity scores", "start_pos": 283, "end_pos": 315, "type": "METRIC", "confidence": 0.6984244783719381}]}, {"text": "The senses of a word are defined in a sense inventory.", "labels": [], "entities": []}, {"text": "We use WordNet) because this is widely used, is publicly available, and has plenty of gold-standard evaluation data available).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9609512686729431}]}, {"text": "The distributional strength of the neighbors is associated with the senses of a word using a measure of semantic similarity which relies on the relationships between word senses, such as hyponyms (available in an inventory such as WordNet) or overlap in the definitions of word senses (available inmost dictionaries), or both.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 231, "end_pos": 238, "type": "DATASET", "confidence": 0.952034056186676}]}, {"text": "In this article we provide a detailed discussion and quantitative analysis of the motivation behind the first sense heuristic, and a full description of our method.", "labels": [], "entities": []}, {"text": "We extend previously reported work in a number of different directions: r We evaluate the method on all parts of speech (PoS) on.", "labels": [], "entities": []}, {"text": "Previous experiments () evaluated only nouns on SemCor, or all PoS but only on the Senseval-2 ( and Senseval-3 (Mihalcea and Edmonds 2004) data.", "labels": [], "entities": [{"text": "Senseval-3 (Mihalcea and Edmonds 2004) data", "start_pos": 100, "end_pos": 143, "type": "DATASET", "confidence": 0.7133883014321327}]}, {"text": "The evaluation on all PoS is much more extensive because the SemCor corpus is composed of 220,000 words in contrast to the 6 documents in the Senseval-2 and -3 English all words data (10,000 words).", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.7631740570068359}, {"text": "Senseval-2 and -3 English all words data", "start_pos": 142, "end_pos": 182, "type": "DATASET", "confidence": 0.6010540574789047}]}, {"text": "r We compare two WordNet similarity measures in our evaluation on nouns, and also contrast performance using two publicly available thesauruses, both produced from the same NEWSWIRE corpus, but one derived using a proximity-based approach and the other using dependency relations from a parser.", "labels": [], "entities": [{"text": "NEWSWIRE corpus", "start_pos": 173, "end_pos": 188, "type": "DATASET", "confidence": 0.884871631860733}]}, {"text": "It turns out that the results from the proximity-based thesaurus are comparable to those from the dependencybased thesaurus; this is encouraging for applying the method to languages without sophisticated analysis tools.", "labels": [], "entities": []}, {"text": "r We manually analyze a sample of errors from the SemCor evaluation.", "labels": [], "entities": [{"text": "SemCor evaluation", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.6777860969305038}]}, {"text": "A small number of errors can be traced back to inherent shortcomings of our method, but the main source of error is due to noise from related senses.", "labels": [], "entities": []}, {"text": "This is a common problem for all WSD systems) but one which is only recently starting to be addressed by the WSD community r One motivation for an automatic method for acquiring predominant senses is that there will always be words for which there are insufficient data available in manually sense-tagged resources.", "labels": [], "entities": []}, {"text": "We compare the performance of our automatic method with the first sense heuristic derived from SemCor on nouns in the Senseval-2 data.", "labels": [], "entities": [{"text": "Senseval-2 data", "start_pos": 118, "end_pos": 133, "type": "DATASET", "confidence": 0.7615531384944916}]}, {"text": "We find that the automatic method outperforms the one obtained from manual annotations in SemCor for nouns with fewer than five occurrences in r Aside from the lack of coverage of manually annotated data, there is a need for first sense heuristics to be specific to domain.", "labels": [], "entities": []}, {"text": "We explore the potential for applying the method with domain-specific text for all PoS in an experiment using a gold-standard domain-specific resource which we have used previously only with nouns.", "labels": [], "entities": []}, {"text": "We show that although there is a little mileage to be had from domain-specific first sense heuristics for verbs, nouns benefit greatly from domain-specific training.", "labels": [], "entities": []}, {"text": "r In previous work we produced manually sense-annotated domain-specific test corpora fora lexical sample, and demonstrated that predominant senses acquired (from hand-classified corpora) in the same domain as the test data outperformed the SemCor first sense.", "labels": [], "entities": []}, {"text": "We further this exploration by contrasting with results from training on automatically categorized text from the English Gigaword Corpus and show that the results are comparable to those using hand-classified domain data.", "labels": [], "entities": [{"text": "English Gigaword Corpus", "start_pos": 113, "end_pos": 136, "type": "DATASET", "confidence": 0.9026361107826233}]}, {"text": "The article is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section we motivate the use of predominant sense information in WSD systems and the need for acquiring this information automatically.", "labels": [], "entities": [{"text": "WSD", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9361932277679443}]}, {"text": "In Section 3 we give an overview of related work in WSD, focusing on the acquisition of prior sense distributions and domain-specific sense information.", "labels": [], "entities": [{"text": "WSD", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9744383096694946}]}, {"text": "Section 4 describes our acquisition method.", "labels": [], "entities": [{"text": "acquisition", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9510293006896973}]}, {"text": "Section 5 describes the experimental setup for the work reported in this article.", "labels": [], "entities": []}, {"text": "Section 6 describes four experiments.", "labels": [], "entities": []}, {"text": "The first evaluates the first sense heuristic using predominant sense information acquired for all PoS on SemCor; for nouns we compare two semantic similarity methods and three different types of distributional thesaurus.", "labels": [], "entities": []}, {"text": "We also report an error analysis for all PoS of our method.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.9556768536567688}]}, {"text": "The second experiment compares the performance of the automatic method to the manually produced data in SemCor, on nouns in the Senseval-2 data, looking particularly at nouns which have a low frequency in SemCor.", "labels": [], "entities": [{"text": "Senseval-2 data", "start_pos": 128, "end_pos": 143, "type": "DATASET", "confidence": 0.8638664186000824}]}, {"text": "The third uses corpora in restricted domains and the subject field code gold standard of to investigate the potential for domain-specific rankings for different PoS.", "labels": [], "entities": []}, {"text": "The fourth compares results when we train and test on domain-specific corpora, where the training data is (1) manually categorized for domain and from the same corpus as the test data, and (2) where the training data is harvested automatically from another corpus which is categorized automatically.", "labels": [], "entities": []}, {"text": "Finally, we conclude (Section 7) and discuss directions for future work (Section 8).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe four experiments using our method for acquiring predominant sense information.", "labels": [], "entities": []}, {"text": "The first experiment evaluates automatically acquired predominant senses for all parts of speech, using SemCor as the test corpus.", "labels": [], "entities": []}, {"text": "This extends previous work which had only evaluated all PoS on Senseval-2 () and Senseval-3 (Mihalcea and Edmonds 2004) data.", "labels": [], "entities": [{"text": "Senseval-3 (Mihalcea and Edmonds 2004) data", "start_pos": 81, "end_pos": 124, "type": "DATASET", "confidence": 0.7268903702497482}]}, {"text": "The SemCor corpus is composed of 220,000 words, in contrast to the 6 documents in the Senseval-2 and -3 English all-words data (10,000 words).", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7219955027103424}, {"text": "Senseval-2 and -3 English all-words data", "start_pos": 86, "end_pos": 126, "type": "DATASET", "confidence": 0.6012404816491264}]}, {"text": "We examine the effects of using the two different semantic similarity scores that performed well in previous work: jcn is quick to compute but lesk has the advantage that it is applicable to all PoS and can be implemented for any dictionary with sense definitions.", "labels": [], "entities": []}, {"text": "We compare three thesauruses: one is derived from the BNC and two from the NEWSWIRE corpus.", "labels": [], "entities": [{"text": "BNC", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.9417746067047119}, {"text": "NEWSWIRE corpus", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.9584383070468903}]}, {"text": "The two from the NEWSWIRE corpus examine the requirement fora parser by contrasting results obtained when the thesaurus is built using parsed data compared to a proximity approach.", "labels": [], "entities": [{"text": "NEWSWIRE corpus", "start_pos": 17, "end_pos": 32, "type": "DATASET", "confidence": 0.9281355142593384}]}, {"text": "We contrast the results of the BNC thesaurus with a simplified version of the prevalence score which uses the number of the k neighbors closest to a sense for ranking without using the dss and without sharing the credit fora neighbor between senses.", "labels": [], "entities": [{"text": "BNC thesaurus", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9095934331417084}, {"text": "prevalence score", "start_pos": 78, "end_pos": 94, "type": "METRIC", "confidence": 0.9638416767120361}]}, {"text": "We also perform an error analysis on a random sample of words for which a predominant sense was found that differed from that given by SemCor, identifying and giving an indication of the frequencies of the main sources of error.", "labels": [], "entities": []}, {"text": "The second experiment is on nouns in the Senseval-2 all-words data, again using predominant senses acquired using each of the three distributional thesauruses, but in this experiment we explore the benefits of an automatic first sense heuristic when there is inadequate data in available resources.", "labels": [], "entities": [{"text": "Senseval-2 all-words data", "start_pos": 41, "end_pos": 66, "type": "DATASET", "confidence": 0.7246178388595581}]}, {"text": "Although show that on Senseval-2 and Senseval-3 test data a first sense heuristic derived from SemCor outperforms the automatic method, we look at whether the method's performance is relatively stronger on words for which there is little data in SemCor.", "labels": [], "entities": [{"text": "Senseval-3 test data", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.8247756361961365}]}, {"text": "This is important because, as we have shown in, low frequency words are used often in senses other than the sense that is ranked first according to SemCor.", "labels": [], "entities": [{"text": "SemCor", "start_pos": 148, "end_pos": 154, "type": "DATASET", "confidence": 0.8787539601325989}]}, {"text": "In addition to the issue of lack of coverage of manually annotated resources, sense frequency will depend on the domain of the data.", "labels": [], "entities": []}, {"text": "In the third experiment, we revisit some previous work on noun senses and domain) using corpora of news text about sports and finance.", "labels": [], "entities": []}, {"text": "Using distributional thesauruses computed from these corpora and a gold standard domain labeling of word senses we look at the potential for computing domain-specific predominant senses for parts of speech other than nouns.", "labels": [], "entities": []}, {"text": "Continuing the line of research on automatic acquisition of domain-specific predominant senses, the fourth experiment compares results when we train and test on domain-specific corpora, where the training data is (1) manually categorized for domain and from the same corpus as the gold-standard test data, and (2) where the training data is harvested automatically from another corpus which is categorized automatically.", "labels": [], "entities": []}, {"text": "In this experiment, we evaluate the accuracy of automatically acquired predominant senses for all open class parts of speech, taking SemCor as the gold standard.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9990027546882629}]}, {"text": "For nouns we use the semantic similarity measures lesk and jcn, and for other parts of speech, lesk.", "labels": [], "entities": []}, {"text": "We use the three distributional thesauruses BNC, DEP, and PROX.", "labels": [], "entities": [{"text": "BNC", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.43575024604797363}]}, {"text": "The gold standard is derived from the Brown Corpus files publicly released as part of SemCor, rather than the processed data provided in the cntlist file in the WordNet distribution.", "labels": [], "entities": [{"text": "Brown Corpus files publicly released", "start_pos": 38, "end_pos": 74, "type": "DATASET", "confidence": 0.9734363675117492}, {"text": "WordNet distribution", "start_pos": 161, "end_pos": 181, "type": "DATASET", "confidence": 0.9461301267147064}]}, {"text": "The released SemCor files contain only the tagged data from the Brown Corpus and do not include data from The Red Badge of Courage.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9768040478229523}, {"text": "The Red Badge of Courage", "start_pos": 106, "end_pos": 130, "type": "DATASET", "confidence": 0.8851516962051391}]}, {"text": "We use the released data rather than that in cntlist because this includes the actual tagged examples which are marked for genre by the Brown files.", "labels": [], "entities": []}, {"text": "We envisage the possibility of further experiments with these genre markers.", "labels": [], "entities": []}, {"text": "We only evaluate on instances where a single, unique sense is supplied by the annotators.", "labels": [], "entities": []}, {"text": "So, for example, we ignore instances like the following with multiple wnsn values: <wf cmd=done pos=NN lemma=tooth wnsn=3;1 lexsn=1:05:02::;1:08:00::>tooth</wf> We also only evaluate on polysemous words (according to WordNet) having one sense in SemCor which is more frequent than any other, and for which both SemCor and our thesauruses have at least a minimal amount of data.", "labels": [], "entities": []}, {"text": "Specifically, a word must occur three or more times in SemCor; it must also occur in tenor more grammatical relations in the parsed version of the BNC and have neighbors in the distributional thesaurus, or be present in Dekang Lin's thesaurus.", "labels": [], "entities": []}, {"text": "We evaluate on nouns, verbs, adjectives, and adverbs separately, computing a number of accuracy measures, both type-based and token-based.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.998835027217865}]}, {"text": "PS acc is calculated over word types in SemCor which have one sense which occurs more than any other.", "labels": [], "entities": [{"text": "PS acc", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8594037890434265}]}, {"text": "It is the accuracy of identifying the predominant sense in SemCor.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995323419570923}]}, {"text": "If the automatic ranking has a tie for the top ranked sense then we score that word as incorrect.", "labels": [], "entities": []}, {"text": "So we have where types mf are the types in SemCor such that one sense is more frequent than any other, the word has occurred at least three times in SemCor and has an entry in the thesaurus.", "labels": [], "entities": []}, {"text": "|correct typ | is the number of these where the automatically acquired predominant sense matches the first sense in SemCor.", "labels": [], "entities": [{"text": "correct typ", "start_pos": 1, "end_pos": 12, "type": "METRIC", "confidence": 0.8995968401432037}]}, {"text": "PS acc BL is the predominant sense random baseline, obtained as follows: WSD sc is a token-based measure.", "labels": [], "entities": [{"text": "PS acc BL", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.6744283636411031}]}, {"text": "It is the WSD accuracy that would be obtained by using the first sense heuristic with the automatically acquired predominant sense information, in cases where there was a unique automatic top ranked sense: where |correct tok | is the number of tokens disambiguated correctly out of the tokens in SemCor having an automatically acquired first sense (SCtokens afs ).", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7035625576972961}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.7726814150810242}]}, {"text": "SC FS is the WSD accuracy of the SemCor first sense heuristic on the same set of tokens (SCtokens afs ), which is the upper bound because the information it uses is derived from the test data itself.", "labels": [], "entities": [{"text": "FS", "start_pos": 3, "end_pos": 5, "type": "METRIC", "confidence": 0.5667723417282104}, {"text": "WSD", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.7416589856147766}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.8243869543075562}]}, {"text": "RBL is the random baseline for the WSD task, calculated by splitting the credit for each token to be tagged in the test data evenly between all of the word's senses.", "labels": [], "entities": [{"text": "RBL", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9541741013526917}, {"text": "WSD task", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.9345913827419281}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We examined differences between the semantic similarity measures (lesk and jcn), the BNC and DEP thesauruses, and the DEP and PROX thesauruses using the \u03c7 2 test of significance with one degree of freedom (.", "labels": [], "entities": [{"text": "BNC and DEP thesauruses", "start_pos": 85, "end_pos": 108, "type": "DATASET", "confidence": 0.7768710851669312}]}, {"text": "None of the differences between the different combinations of similarity measures and thesauruses for the type-based measure PS acc are significant.", "labels": [], "entities": []}, {"text": "The differences between lesk and jcn are significant for the token-based measure WSD sc for both the BNC and PROX thesauruses (both p < .001), however not when comparing lesk and jcn for the DEP thesaurus.", "labels": [], "entities": [{"text": "BNC and PROX thesauruses", "start_pos": 101, "end_pos": 125, "type": "DATASET", "confidence": 0.7217433899641037}, {"text": "DEP thesaurus", "start_pos": 191, "end_pos": 204, "type": "DATASET", "confidence": 0.9098777770996094}]}, {"text": "Although lesk is more accurate than jcn, at least on the WSD task, jcn is much faster because of the precompilation of IC in the WordNet similarity package; however, lesk has the additional benefit of being applicable to other parts of speech.", "labels": [], "entities": [{"text": "accurate", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9628929495811462}, {"text": "WSD task", "start_pos": 57, "end_pos": 65, "type": "TASK", "confidence": 0.5555061101913452}, {"text": "WordNet similarity package", "start_pos": 129, "end_pos": 155, "type": "DATASET", "confidence": 0.843553344408671}]}, {"text": "The method gives particularly good results for adjectives, given that they have a similar random baseline to nouns.", "labels": [], "entities": []}, {"text": "It does not do so well for adverbs and verbs, but still performs well above the random baseline which is low for verbs due to their high degree of polysemy.", "labels": [], "entities": []}, {"text": "Given that the first sense heuristic from SemCor is particularly strong for adverbs, it is disappointing that the automatic method does not perform as well as it does on adjectives.", "labels": [], "entities": []}, {"text": "One possible reason for this might be that adverbs are often less strongly associated to the verbs that they modify than adjectives are to the nouns that they modify, so the distributional thesaurus information is less reliable.", "labels": [], "entities": []}, {"text": "Another reason maybe that less data are available for adverbs, both in the thesaurus and also in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9781026244163513}]}, {"text": "Comparing the results for the DEP and the PROX thesauruses, we see that although there is no significant difference in PS acc (with either lesk or jcn), there is for WSD sc when using jcn (p < .001), but not when comparing the lesk values for these thesauruses.", "labels": [], "entities": [{"text": "DEP", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.9290898442268372}, {"text": "PROX thesauruses", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.8765190243721008}, {"text": "PS acc", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9493004083633423}]}, {"text": "Even though the differences between jcn DEP and jcn PROX are significant, the absolute differences are nevertheless relatively small; this bodes well for applying the automatic predominant sense method to languages less well resourced than English, because the PROX thesaurus was produced without using a parser.", "labels": [], "entities": []}, {"text": "The differences in results between jcn BNC and jcn DEP for nouns are statistically significant (p < .001).", "labels": [], "entities": []}, {"text": "The better accuracy with DEP maybe because the NEWSWIRE corpus is larger than the BNC.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9993815422058105}, {"text": "DEP", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.6346432566642761}, {"text": "NEWSWIRE corpus", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9523230791091919}, {"text": "BNC", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.9727363586425781}]}, {"text": "We intend to investigate the effects of corpus size in the future.", "labels": [], "entities": [{"text": "corpus size", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.6780950874090195}]}, {"text": "The differences in results between lesk BNC and lesk DEP for nouns are not significant.", "labels": [], "entities": [{"text": "BNC", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.7390812635421753}]}, {"text": "In the previous section we described an evaluation of the accuracy of automatically acquired predominant sense information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9986666440963745}]}, {"text": "We carried out the evaluation with respect to SemCor in order to have as much test data as possible.", "labels": [], "entities": [{"text": "SemCor", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.9003472328186035}]}, {"text": "To obtain reasonably reliable gold-standard first-sense data and first-sense heuristic upper bounds, we limited the evaluation to words occurring at least three times in SemCor.", "labels": [], "entities": []}, {"text": "Clearly this scenario is unrealistic.", "labels": [], "entities": []}, {"text": "For many words, and particularly for nouns, there is very little or no data in SemCor; shows that 81.9% of nouns (excluding multiwords) listed in WordNet do not occur at all in SemCor.", "labels": [], "entities": []}, {"text": "Thus, even for English, which has substantial manually sense-tagged resources, coverage is severely limited for many words.", "labels": [], "entities": [{"text": "coverage", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9556741118431091}]}, {"text": "For a more realistic comparison of automatic and manual heuristics, we therefore now change to a different test corpus, the Senseval-2 English all-words task data set.", "labels": [], "entities": [{"text": "Senseval-2 English all-words task data set", "start_pos": 124, "end_pos": 166, "type": "DATASET", "confidence": 0.608888566493988}]}, {"text": "We focus on nouns and evaluate using all words regardless of their frequencies in SemCor.", "labels": [], "entities": []}, {"text": "We examine the effect of frequency in SemCor on performance of a SemCor-derived heuristic in comparison to results from our automatic method on the same words.", "labels": [], "entities": []}, {"text": "Our hypothesis is that although automatically acquired predominant sense information may not outperform first-sense data obtained from a hand-tagged resource overall words in a text, the information may well be more accurate for low frequency items.", "labels": [], "entities": []}, {"text": "We use a mapping between different WordNet versions 25 (Daud\u00e9, Padr\u00f3Padr\u00b4Padr\u00f3, and Rigau 2000) to obtain the Senseval-2 all words noun data (originally distributed with 1.7 sense numbers) with 1.6 sense numbers.", "labels": [], "entities": []}, {"text": "As well as examining the performance of our method in contrast to the SemCor heuristic, we calculate an upper bound for this using the first sense heuristic from the Senseval-2 all-words data itself.", "labels": [], "entities": []}, {"text": "This is obtained for nouns with two or more occurrences in the Senseval-2 data and where one sense occurs more than any of the others.", "labels": [], "entities": [{"text": "Senseval-2 data", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.9277135133743286}]}, {"text": "We calculate type, precision, and recall, using this Senseval-2 first-sense as the gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9994507431983948}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9995239973068237}]}, {"text": "The recall measure is the same as PS acc described previously, except that we include items which do not have entries in the thesaurus, scoring them incorrect.", "labels": [], "entities": [{"text": "recall measure", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9817974269390106}]}, {"text": "Precision only includes items where there is a sense ranked higher than any other for that word with the prevalence score, that is, it does not include items with a joint automatic ranking.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8455955386161804}, {"text": "prevalence score", "start_pos": 105, "end_pos": 121, "type": "METRIC", "confidence": 0.9739914536476135}]}, {"text": "We also calculate token precision and recall (WSD).", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.8607045412063599}, {"text": "recall (WSD)", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.94715416431427}]}, {"text": "These measures relate to WSD sc , but again, recall includes words not in the thesaurus which are scored incorrect, and precision does not include items with a joint automatic ranking.", "labels": [], "entities": [{"text": "WSD sc", "start_pos": 25, "end_pos": 31, "type": "TASK", "confidence": 0.7521909177303314}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9985236525535583}, {"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.999277651309967}]}, {"text": "We also separately compute WSD precision for words not in SemCor (NISC).", "labels": [], "entities": [{"text": "WSD", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.8127919435501099}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.6129672527313232}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The automatically acquired predominant sense results (the first six lines of results in the table) are approaching the SemCor-derived results (third line from the bottom of the table).", "labels": [], "entities": []}, {"text": "The NISC results are particularly encouraging, but with the caveat that there are only 17 such words in the data.", "labels": [], "entities": [{"text": "NISC", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8810915350914001}]}, {"text": "The precision for these items is higher than the This is because the nouns involved are less frequent so tend to be less polysemous and consequently have a higher random baseline.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995288848876953}]}, {"text": "There area few nouns that are not in the automatic ranking, but this is due to the fact that neighbors were not collected for these nouns in the thesaurus because of tagging or parser errors or the particular set of grammatical relations used.", "labels": [], "entities": []}, {"text": "It should be possible to extend the range of grammatical relations, or use proximity-based relations, so that neighbors can be obtained in these cases.", "labels": [], "entities": []}, {"text": "It would also be possible to assign some credit in the case of joint top ranked senses to increase coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.971712589263916}]}, {"text": "Looking at in more detail, it seems to be the case that although the BNC thesaurus does well in identifying the first sense of a word (the type results), the PROX and DEP thesauruses from the NEWSWIRE corpus return better WSD results when used with the jcn measure.", "labels": [], "entities": [{"text": "BNC thesaurus", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.9425060749053955}, {"text": "identifying the first sense of a word", "start_pos": 96, "end_pos": 133, "type": "TASK", "confidence": 0.8148325341088432}, {"text": "NEWSWIRE corpus", "start_pos": 192, "end_pos": 207, "type": "DATASET", "confidence": 0.9178245067596436}, {"text": "WSD", "start_pos": 222, "end_pos": 225, "type": "METRIC", "confidence": 0.9196830987930298}]}, {"text": "This is possibly because jcn works well for more frequent items due to its incorporation of frequency information, and the NEWSWIRE corpus has more data for frequent words, although coverage is not as good as the BNC as seen by the bigger differences in precision and recall and the figures in.", "labels": [], "entities": [{"text": "NEWSWIRE corpus", "start_pos": 123, "end_pos": 138, "type": "DATASET", "confidence": 0.9605342745780945}, {"text": "coverage", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9671242833137512}, {"text": "BNC", "start_pos": 213, "end_pos": 216, "type": "DATASET", "confidence": 0.7356287240982056}, {"text": "precision", "start_pos": 254, "end_pos": 263, "type": "METRIC", "confidence": 0.9983766078948975}, {"text": "recall", "start_pos": 268, "end_pos": 274, "type": "METRIC", "confidence": 0.9872686862945557}]}, {"text": "The lower coverage maybe due to the narrower domain and genre of the NEWSWIRE corpus, though spelling and capitalization differences probably also account for some differences.", "labels": [], "entities": [{"text": "coverage", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9634485840797424}, {"text": "NEWSWIRE corpus", "start_pos": 69, "end_pos": 84, "type": "DATASET", "confidence": 0.9089235365390778}]}, {"text": "shows results on the Senseval-2 nouns for the best similarity measure and thesaurus combinations in for nouns at or below various frequencies in SemCor.", "labels": [], "entities": []}, {"text": "(The differences between the DEP and PROX thesauruses are negligible at frequencies of 10 or below, so for those we report only the results for DEP.)", "labels": [], "entities": [{"text": "PROX thesauruses", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.6876222491264343}, {"text": "DEP", "start_pos": 144, "end_pos": 147, "type": "DATASET", "confidence": 0.8993679285049438}]}, {"text": "As we anticipated, for low frequency words the automatic methods do give more accurate predominant sense information than SemCor.", "labels": [], "entities": []}, {"text": "The low number of test items at frequency five or less means that results for jcn with the BNC thesaurus are not significantly better when compared with SemCor ( p = .05); however the lesk WSD results are significantly better ( p < .01 for the \u2264 1 threshold and p < .05 for the \u2264 5 threshold).", "labels": [], "entities": [{"text": "BNC thesaurus", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.937117725610733}]}, {"text": "On the whole, we see that the automatic method, using either jcn or lesk and any of the three thesauruses, tend to give better results than SemCor on nouns which have low coverage in SemCor.", "labels": [], "entities": []}, {"text": "show the precision for type and token (WSD) evaluation where the items have a frequency at or below given thresholds in SemCor.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9992488026618958}]}, {"text": "Although the manually  In this experiment, we investigate the potential of the automatic ranking method for computing predominant senses with respect to particular domains.", "labels": [], "entities": []}, {"text": "We have previously demonstrated that the method produces intuitive domain-specific models for nouns (, and that these can be more accurate than first senses derived from SemCor for words salient to a domain.", "labels": [], "entities": []}, {"text": "Here we investigate the behavior for other parts of speech, using a similar experimental setup to that of McCarthy et al.", "labels": [], "entities": []}, {"text": "That work used the subject field codes (SFC) (Magnini andCavagl\u00ec a 2000) as a gold standard.", "labels": [], "entities": []}, {"text": "In SFC the Princeton English WordNet is augmented with some domain labels.", "labels": [], "entities": [{"text": "Princeton English WordNet", "start_pos": 11, "end_pos": 36, "type": "DATASET", "confidence": 0.9312812288602194}]}, {"text": "Every synset in WordNet's sense inventory is annotated with at least one domain label, selected from a set of about 200 labels.", "labels": [], "entities": [{"text": "WordNet's sense inventory", "start_pos": 16, "end_pos": 41, "type": "DATASET", "confidence": 0.8877722471952438}]}, {"text": "These labels are organized in a tree structure.", "labels": [], "entities": []}, {"text": "Each synset of WordNet 1.6 is labeled with one or more labels.", "labels": [], "entities": [{"text": "WordNet 1.6", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.9415456056594849}]}, {"text": "The label factotum is assigned if any other is inadequate.", "labels": [], "entities": []}, {"text": "The first level consists of five main categories (e.g., doctrines and social science) and factotum.doctrines In that work we produced sense rankings fora set of 38 nouns which have at least one synset with an economy SFC label and one with a sport SFC label.", "labels": [], "entities": []}, {"text": "We then demonstrated that there were more sport labels assigned to the predominant senses acquired from the SPORTS corpus and more economy labels assigned to those from the FINANCE corpus.", "labels": [], "entities": [{"text": "SPORTS corpus", "start_pos": 108, "end_pos": 121, "type": "DATASET", "confidence": 0.8160198628902435}, {"text": "FINANCE corpus", "start_pos": 173, "end_pos": 187, "type": "DATASET", "confidence": 0.7488549649715424}]}, {"text": "The predominant senses from both domains had a similarly high percentage of factotum (domain-independent) labels.", "labels": [], "entities": []}, {"text": "We reproduce the results here (in for ease of reference, and for comparison with other results presented in this section.", "labels": [], "entities": []}, {"text": "The y-axis in this figure shows the percentage of the predominant sense labels for these 38 nouns that have the SFC label indicated by the x-axis.", "labels": [], "entities": []}, {"text": "We envisaged running the same experiment with verbs, adjectives, and adverbs, although we suspected that these would show less domain-specific tendencies and there would be fewer candidate words to work with.", "labels": [], "entities": []}, {"text": "The SFC labels for all senses of polysemous words (excluding multiwords) in the various parts of speech are shown in.", "labels": [], "entities": []}, {"text": "We see from the distribution of factotum labels across the parts of speech that nouns are certainly the PoS most likely to be influenced by domain.", "labels": [], "entities": []}, {"text": "To produce results like for each PoS, we needed words having at least one synset with a sport label and one with an economy label.", "labels": [], "entities": []}, {"text": "There were 20 such verbs but only two adjectives and no adverbs meeting this condition.", "labels": [], "entities": []}, {"text": "We therefore performed the experiment only with verbs.", "labels": [], "entities": []}, {"text": "To do this we used the SPORTS and FINANCE corpora as before, computing thesauruses for verbs using the grammatical relations specified in.", "labels": [], "entities": [{"text": "SPORTS", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9490296840667725}, {"text": "FINANCE", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9876195192337036}]}, {"text": "The results for the distribution of domain labels of the predominant senses.", "labels": [], "entities": []}, {"text": "We seethe same tendency for sport labels for predominant senses from the SPORTS corpus and economy labels for the predominant senses from the FINANCE corpus, but the relationship is less marked compared with nouns because of the high proportions of factotum senses in both corpora for verbs.", "labels": [], "entities": [{"text": "SPORTS corpus", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.8891255855560303}, {"text": "FINANCE corpus", "start_pos": 142, "end_pos": 156, "type": "DATASET", "confidence": 0.7475140392780304}]}, {"text": "We believe that acquisition of domain-specific predominant senses should be focused on those words which show domain-specific tendencies.", "labels": [], "entities": []}, {"text": "We hope to put more work into automatic detection of these tendencies using indicators such as domain salience and words that have different sense rankings in a given domain compared to the BNC (as discussed by.", "labels": [], "entities": [{"text": "BNC", "start_pos": 190, "end_pos": 193, "type": "DATASET", "confidence": 0.7041237354278564}]}, {"text": "In the final set of experiments we evaluate the acquired predominant senses for domainspecific corpora.", "labels": [], "entities": []}, {"text": "The first of the two experiments was reported by, but we extend it by the second experiment reported subsequently.", "labels": [], "entities": []}, {"text": "Because there are no publicly available domain-specific manually sense-tagged corpora, we created our own gold standard.", "labels": [], "entities": []}, {"text": "The two chosen domains (SPORTS and FINANCE) and the domain-neutral corpus (BNC) are the same as we used in the previous experiment.", "labels": [], "entities": [{"text": "FINANCE", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9824674725532532}]}, {"text": "We selected 40 words and we sampled (randomly) sentences containing these words from the three corpora and asked annotators to choose the correct sense for the target words.", "labels": [], "entities": []}, {"text": "The set consists of 17 words which have at least one sense assigned an economy domain label and at least one sense assigned a sports label: club, manager, record, right, bill, check, competition, conversion, crew, delivery, division, fishing, reserve, return, score, receiver, running; eight words that are particular salient in the SPORTS domain: fan, star, transfer, striker, goal, title, tie, coach; eight words that are particular salient in the give further details of the construction of the gold standard.", "labels": [], "entities": [{"text": "receiver", "start_pos": 267, "end_pos": 275, "type": "METRIC", "confidence": 0.9301729202270508}, {"text": "SPORTS domain: fan, star, transfer, striker, goal, title, tie, coach", "start_pos": 333, "end_pos": 401, "type": "Description", "confidence": 0.7014016840193007}, {"text": "gold standard", "start_pos": 498, "end_pos": 511, "type": "DATASET", "confidence": 0.8168507218360901}]}, {"text": "In the first experiment, we train on a corpus of documents with manually assigned domain labels (i.e., sub-corpora of the Reuters corpus, see Section 6.3), and we test on data from the same source.", "labels": [], "entities": [{"text": "Reuters corpus", "start_pos": 122, "end_pos": 136, "type": "DATASET", "confidence": 0.9629226922988892}]}, {"text": "Ina second experiment we build a text classifier, use the text classifier to obtain SPORTS and FINANCE corpora (using general newswire text from the English Gigaword Corpus; Graff 2003) and test on the gold-standard data from the Reuters corpus.", "labels": [], "entities": [{"text": "SPORTS", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9773203134536743}, {"text": "FINANCE", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9911547899246216}, {"text": "English Gigaword Corpus; Graff 2003", "start_pos": 149, "end_pos": 184, "type": "DATASET", "confidence": 0.8875941137472788}, {"text": "Reuters corpus", "start_pos": 230, "end_pos": 244, "type": "DATASET", "confidence": 0.8570608198642731}]}, {"text": "The second experiment eliminates issues about dependencies between training and test data and will shed light on the question of how robust the acquired predominant sense method is with respect to noise in the input data.", "labels": [], "entities": []}, {"text": "At the same time, the second experiment paves the way towards creating predominant sense inventories for any conceivable domain.", "labels": [], "entities": []}, {"text": "In this section we focus on the predominant sense evaluation of the experiments described by.", "labels": [], "entities": []}, {"text": "After running the predominant sense finding algorithms on the raw text of the two domain corpora (SPORTS and FINANCE) and the domain-neutral corpus (BNC), we evaluate the accuracy of performing WSD on the sample of 40 words purely with the first sense heuristic using all nine combinations of training and test corpora.", "labels": [], "entities": [{"text": "sense finding", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.7382641136646271}, {"text": "FINANCE", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9802723526954651}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9980114698410034}, {"text": "WSD", "start_pos": 194, "end_pos": 197, "type": "TASK", "confidence": 0.9593779444694519}]}, {"text": "The results (as given in) are compared with a random baseline (\"Random BL\") and the accuracy using the first sense heuristic from SemCor (\"SemCor FS\").", "labels": [], "entities": [{"text": "Random BL\")", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.7811909715334574}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9996581077575684}]}, {"text": "The results in show that the best results are obtained when the predominant senses are acquired using the appropriate domain (i.e., test and training data from the same domain).", "labels": [], "entities": []}, {"text": "Moreover, when trained on the domain-relevant corpora, the random baseline as well as the baseline provided by SemCor are comfortably beaten.", "labels": [], "entities": []}, {"text": "It can be observed from these results that apparently the BNC is more similar to the FINANCE corpus than it is to the SPORTS corpus.", "labels": [], "entities": [{"text": "BNC", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.4520638883113861}, {"text": "FINANCE corpus", "start_pos": 85, "end_pos": 99, "type": "DATASET", "confidence": 0.7708750069141388}, {"text": "SPORTS corpus", "start_pos": 118, "end_pos": 131, "type": "DATASET", "confidence": 0.8136185705661774}]}, {"text": "The results for the SPORTS domain lag behind the results for the FINANCE domain by almost 6 percentage points.", "labels": [], "entities": [{"text": "SPORTS domain", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.6369964331388474}, {"text": "FINANCE domain", "start_pos": 65, "end_pos": 79, "type": "DATASET", "confidence": 0.7097458094358444}]}, {"text": "This could be because The random baseline is i\u2208tokens 1 #senses(i) . 29 The precision is given alongside in brackets because a predominant sense for the word striker is not supplied by SemCor.", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9991912245750427}]}, {"text": "The automatic method proposes a predominant sense in every case. of the smaller amount of training data available (32M words versus 9M words), but it could also bean artifact of this particular selection of words.", "labels": [], "entities": []}, {"text": "Although the previous experiment shows that it is possible to acquire domain-specific predominant senses successfully, the usefulness of doing this will be far greater if there is no need to classify corpora with respect to domain by hand.", "labels": [], "entities": []}, {"text": "There is no such thing as a standard domain specification because the definition of a domain depends on user and application.", "labels": [], "entities": []}, {"text": "It would be advantageous if we could automatically obtain a user-/application-specific corpus from which to acquire predominant senses.", "labels": [], "entities": []}, {"text": "In this section we describe an experiment where we build a text classifier using WordNet as a sense inventory and the SFC domain extension (see Section 6.3).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9627926349639893}, {"text": "SFC domain extension", "start_pos": 118, "end_pos": 138, "type": "DATASET", "confidence": 0.8112074534098307}]}, {"text": "We extracted bags of domain-specific words from WordNet for all the defined domains by collecting all the word senses (synsets) and corresponding glosses associated with each domain label.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9577057361602783}]}, {"text": "These bags of words are the fingerprints for the domains and we used them to train a Support Vector Machine (SVM) text classifier using TwentyOne.", "labels": [], "entities": [{"text": "TwentyOne", "start_pos": 136, "end_pos": 145, "type": "DATASET", "confidence": 0.9712015986442566}]}, {"text": "The classifier distinguishes between 48 classes (the first and second levels of the SFC hierarchy).", "labels": [], "entities": [{"text": "SFC hierarchy", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.7584022879600525}]}, {"text": "When a document is evaluated by the classifier, it returns a list of all the classes (domains) it recognizes and an associated confidence score reflecting the certainty that the document belongs to that particular domain.", "labels": [], "entities": []}, {"text": "We classified 10 months' worth of data from the English Gigaword Corpus using this classifier and assigned each document to the corpus belonging to the highest scoring class of the classifier's output.", "labels": [], "entities": [{"text": "English Gigaword Corpus", "start_pos": 48, "end_pos": 71, "type": "DATASET", "confidence": 0.9044986168543497}]}, {"text": "The level of confidence was ignored at this stage.", "labels": [], "entities": []}, {"text": "This resulted in a SPORTS corpus comprising about 11M words and a FINANCE corpus of about 27M words.", "labels": [], "entities": [{"text": "FINANCE", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9864646196365356}]}, {"text": "The predominant sense finding algorithm was run on the raw text of these two corpora and we followed exactly the same evaluation strategy as in the previous section.", "labels": [], "entities": [{"text": "sense finding", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.8058273196220398}]}, {"text": "The results are summarized in and are very similar to those based on hand-labeled corpora.", "labels": [], "entities": []}, {"text": "Again, the best results are obtained when test and training data are derived from the same domain.", "labels": [], "entities": []}, {"text": "The FINANCE-FINANCE result is slightly worse, but is still well above both Random and the SemCor baseline.", "labels": [], "entities": [{"text": "FINANCE-FINANCE", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9979124665260315}, {"text": "Random", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9859719276428223}, {"text": "SemCor baseline", "start_pos": 90, "end_pos": 105, "type": "DATASET", "confidence": 0.8143771588802338}]}, {"text": "The SPORTS-SPORTS result has slightly improved over the result reported in the previous section.", "labels": [], "entities": [{"text": "SPORTS-SPORTS", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9234275817871094}]}, {"text": "The reason for these differences may well be because the FINANCE corpus used for this experiment is smaller and the SPORTS corpus is slightly larger than those used in the hand-labeled experiment.", "labels": [], "entities": [{"text": "FINANCE", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9661093950271606}, {"text": "SPORTS corpus", "start_pos": 116, "end_pos": 129, "type": "DATASET", "confidence": 0.7284604907035828}]}, {"text": "Automatically classifying documents inherently introduces noise in the training corpora.", "labels": [], "entities": []}, {"text": "This experiment to test the robustness of our method for finding predominant senses suggests that it deals well with the noise.", "labels": [], "entities": []}, {"text": "Further experiments that take the confidence levels of the classifier into account will allow us to create corpora with less noise and will allow us to find the right balance between corpus size and corpus quality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  The best two performing systems of each type (according to fine-grained recall) in Senseval-2  and -3.", "labels": [], "entities": [{"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9558315277099609}]}, {"text": " Table 2  Words (excluding multiwords) in WordNet 1.7.1 and the BNC without any data in SemCor.", "labels": [], "entities": [{"text": "WordNet 1.7.1", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9534556269645691}, {"text": "BNC", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.94404536485672}]}, {"text": " Table 4  Most frequent sense analysis for Senseval-2 and -3 polysemous lemmas occurring more than  once in a document (adverb data is only from Senseval-2).", "labels": [], "entities": []}, {"text": " Table 5  Most frequent sense analysis for all polysemous lemmas in the Senseval-2 and -3 test data,  broken down by their frequencies of occurrence in SemCor (adverb data is only from  Senseval-2).", "labels": [], "entities": [{"text": "Senseval-2 and -3 test data", "start_pos": 72, "end_pos": 99, "type": "DATASET", "confidence": 0.671629011631012}]}, {"text": " Table 6  Example dss and sss scores for star and its neighbors.", "labels": [], "entities": []}, {"text": " Table 8  Thesaurus coverage of polysemous words (excluding multiwords) in WordNet 1.6.", "labels": [], "entities": [{"text": "WordNet 1.6", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.9681165218353271}]}, {"text": " Table 9  Evaluation on SemCor, polysemous words only.", "labels": [], "entities": []}, {"text": " Table 10  Simplified prevalence score, evaluation on SemCor, polysemous words only.", "labels": [], "entities": [{"text": "Simplified prevalence score", "start_pos": 11, "end_pos": 38, "type": "METRIC", "confidence": 0.7742485006650289}]}, {"text": " Table 12  SemCor results for Nouns using jcn.", "labels": [], "entities": []}, {"text": " Table 13  Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words  task data.", "labels": [], "entities": [{"text": "Senseval-2 all-words  task data", "start_pos": 80, "end_pos": 111, "type": "DATASET", "confidence": 0.6904538497328758}]}, {"text": " Table 14  Senseval-2 results, polysemous nouns only, broken down by their frequencies of occurrence in  SemCor.", "labels": [], "entities": []}, {"text": " Table 15  Most frequent SFC labels for all senses of polysemous words in WordNet, by part of speech.", "labels": [], "entities": [{"text": "SFC", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8077148795127869}, {"text": "WordNet", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9647996425628662}]}, {"text": " Table 16  WSD using predominant senses, training, and testing on all domain combinations  (hand-classified corpora).", "labels": [], "entities": [{"text": "WSD", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.7079390287399292}]}, {"text": " Table 17  WSD using predominant senses, training, and testing on all domain combinations (automatically  classified corpora).", "labels": [], "entities": [{"text": "WSD", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.6652762293815613}]}]}