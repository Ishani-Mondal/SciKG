{"title": [{"text": "Answering Clinical Questions with Knowledge-Based and Statistical Techniques", "labels": [], "entities": [{"text": "Answering Clinical Questions", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9145878553390503}]}], "abstractContent": [{"text": "The combination of recent developments in question-answering research and the availability of unparalleled resources developed specifically for automatic semantic processing of text in the medical domain provides a unique opportunity to explore complex question answering in the domain of clinical medicine.", "labels": [], "entities": [{"text": "question answering", "start_pos": 253, "end_pos": 271, "type": "TASK", "confidence": 0.7188888788223267}]}, {"text": "This article presents a system designed to satisfy the information needs of physicians practicing evidence-based medicine.", "labels": [], "entities": []}, {"text": "We have developed a series of knowledge extractors, which employ a combination of knowledge-based and statistical techniques, for automatically identifying clinically relevant aspects of MEDLINE abstracts.", "labels": [], "entities": [{"text": "automatically identifying clinically relevant aspects of MEDLINE abstracts", "start_pos": 130, "end_pos": 204, "type": "TASK", "confidence": 0.5218895971775055}]}, {"text": "These extracted elements serve as the input to an algorithm that scores the relevance of citations with respect to structured representations of information needs, in accordance with the principles of evidence-based medicine.", "labels": [], "entities": []}, {"text": "Starting with an initial list of citations retrieved by PubMed, our system can bring relevant abstracts into higher ranking positions, and from these abstracts generate responses that directly answer physicians' questions.", "labels": [], "entities": []}, {"text": "We describe three separate evaluations: one focused on the accuracy of the knowledge extractors, one conceptualized as a document reranking task, and finally, an evaluation of answers by two physicians.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9973621964454651}]}, {"text": "Experiments on a collection of real-world clinical questions show that our approach significantly outperforms the already competitive PubMed baseline.", "labels": [], "entities": [{"text": "PubMed baseline", "start_pos": 134, "end_pos": 149, "type": "DATASET", "confidence": 0.9097735583782196}]}], "introductionContent": [{"text": "Recently, the focus of question-answering research has shifted away from simple factbased questions that can be answered with relatively little linguistic knowledge to \"harder\" questions that require fine-grained text analysis, reasoning capabilities, and the ability to synthesize information from multiple sources.", "labels": [], "entities": []}, {"text": "General purpose reasoning on anything other than superficial lexical relations is exceedingly difficult because there is avast amount of world knowledge that must be encoded, either manually or automatically, to overcome the brittleness often associated with long chains of evidence.", "labels": [], "entities": []}, {"text": "This situation poses a serious bottleneck to \"advanced\" question-answering systems.", "labels": [], "entities": []}, {"text": "However, the availability of existing knowledge sources and ontologies in certain domains provides exciting opportunities to experiment with knowledge-rich approaches.", "labels": [], "entities": []}, {"text": "How might one go about leveraging these resources effectively?", "labels": [], "entities": []}, {"text": "How might one integrate statistical techniques to overcome the brittleness often associated with knowledgebased approaches?", "labels": [], "entities": []}, {"text": "We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.", "labels": [], "entities": []}, {"text": "This domain is well-suited for exploring the posed research questions for several reasons.", "labels": [], "entities": []}, {"text": "First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS).", "labels": [], "entities": [{"text": "Unified Medical Language System (UMLS)", "start_pos": 80, "end_pos": 118, "type": "DATASET", "confidence": 0.6154467293194362}]}, {"text": "Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep ( extracts relations between the concepts.", "labels": [], "entities": []}, {"text": "Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.", "labels": [], "entities": []}, {"text": "The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and 5 million concept names from more than 100 controlled vocabularies.", "labels": [], "entities": [{"text": "UMLS Metathesaurus", "start_pos": 24, "end_pos": 42, "type": "DATASET", "confidence": 0.9131314158439636}]}, {"text": "The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.", "labels": [], "entities": [{"text": "UMLS Metathesaurus", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.9331347942352295}]}, {"text": "Third, the paradigm of evidence-based medicine) provides a task-based model of the clinical information-seeking process.", "labels": [], "entities": []}, {"text": "The PICO framework () for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.", "labels": [], "entities": []}, {"text": "The confluence of these many factors makes clinical question answering a very exciting area of research.", "labels": [], "entities": [{"text": "question answering", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7995670735836029}]}, {"text": "Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented).", "labels": [], "entities": []}, {"text": "MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians' questions, and is commonly used in that capacity.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8145372271537781}]}, {"text": "However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.6732381582260132}, {"text": "PubMed", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.9136554598808289}]}, {"text": "Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7150456309318542}]}, {"text": "Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of healthcare.", "labels": [], "entities": [{"text": "question answering", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.6844354271888733}]}, {"text": "Our question-answering system supports the practice of evidence-based medicine (EBM), a widely accepted paradigm for medical practice that stresses the importance of evidence from patient-centered clinical research in the healthcare process.", "labels": [], "entities": [{"text": "evidence-based medicine (EBM)", "start_pos": 55, "end_pos": 84, "type": "TASK", "confidence": 0.6886658012866974}]}, {"text": "EBM prescribes an approach to structuring clinical information needs and identifies elements (for example, the problem at hand and the interventions under consideration) that factor into the assessment of clinically relevant studies for medical practice.", "labels": [], "entities": []}, {"text": "The foundation of our question-answering strategy is built on knowledge extractors that automatically identify these elements in MEDLINE abstracts.", "labels": [], "entities": []}, {"text": "Using these knowledge extractors, we have developed algorithms for scoring the relevance", "labels": [], "entities": []}], "datasetContent": [{"text": "Ninety of the 100 fully annotated abstracts in our collection were agreed upon by the annotators as being clinical in nature, and were used as test data for our population extractor.", "labels": [], "entities": [{"text": "population extractor", "start_pos": 161, "end_pos": 181, "type": "TASK", "confidence": 0.6522371023893356}]}, {"text": "Because these abstracts were not examined in the process of developing the extractor rules, they can be viewed as a blind held-out test set.", "labels": [], "entities": []}, {"text": "The output of our popu- lation extractor was judged to be correct if it occurred in a sentence that was annotated as containing the population in the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 150, "end_pos": 163, "type": "DATASET", "confidence": 0.8719417750835419}]}, {"text": "Note that this evaluation presents an upper bound on the performance of the population extractor, whose outputs are noun phrases.", "labels": [], "entities": []}, {"text": "We adopted such a lenient evaluation setup because of the boundary issues previously discussed, and also to forestall potential difficulties with scoring partially overlapping string matches.", "labels": [], "entities": []}, {"text": "For comparison, our baseline simply returned the first three sentences of the abstract.", "labels": [], "entities": []}, {"text": "We considered the baseline correct if anyone of the sentences were annotated as containing the population in the gold standard (an even more lenient criterion).", "labels": [], "entities": []}, {"text": "This baseline was motivated by the observation that the aim and methods sections of structured abstracts are likely to contain the population information-for structured abstracts, explicit headings provide structural cues; for unstructured abstracts, positional information serves as a surrogate.", "labels": [], "entities": []}, {"text": "The performance of the population extractor is shown in.", "labels": [], "entities": []}, {"text": "A manual error analysis revealed three sources of error: First, not all population descriptions contain a number explicitly, for example, The medical charts of all patients who were treated with etanercept for back or neck pain at a single private medical clinic in 2003.", "labels": [], "entities": []}, {"text": "Second, not all study populations are population groups, as for example in All primary care trusts in England.", "labels": [], "entities": []}, {"text": "Finally, tagging and chunking errors propagate to the semantic type assignment level and affect the quality of MetaMap output.", "labels": [], "entities": []}, {"text": "For example, consider the following sentence: We have compared the LD and recombination patterns defined by single-nucleotide polymorphisms in ENCODE region ENm010, chromosome 7p15 2, in Korean, Japanese, and Chinese samples.", "labels": [], "entities": [{"text": "ENCODE region ENm010", "start_pos": 143, "end_pos": 163, "type": "DATASET", "confidence": 0.8414531548817953}]}, {"text": "Both Korean and Japanese were mistagged as nouns, which lead to the following erroneous chunking:  This resulted in the tagging of Japanese as a population.", "labels": [], "entities": []}, {"text": "Errors of this type affect other extractors as well.", "labels": [], "entities": []}, {"text": "For example, lead was mistagged as a noun in the phrase Echocardiographic findings lead to the right diagnosis, which caused MetaMap to identify the word as a PHARMACOLOGICAL SUBSTANCE (lead is sometimes used as a homeopathic preparation).", "labels": [], "entities": []}, {"text": "Although our problem extractor returns a list of clinical problems, we only evaluate performance on identification of the primary problem.", "labels": [], "entities": [{"text": "problem extractor", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7347125709056854}]}, {"text": "For some abstracts, MeSH headings can be used as ground truth, because one of the human indexers' tasks in assigning terms is to identify the main topic of the article (sometimes a disorder).", "labels": [], "entities": []}, {"text": "For this evaluation, we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 127, "end_pos": 133, "type": "DATASET", "confidence": 0.9733881950378418}]}, {"text": "We applied our problem extractor on different segments of the abstract: the title only, the title and first two sentences, and the entire abstract.", "labels": [], "entities": []}, {"text": "These results are shown in.", "labels": [], "entities": []}, {"text": "Here, a problem was considered correctly identified only if it shared the same concept ID as the ground truth problem (from the MeSH heading).", "labels": [], "entities": [{"text": "MeSH heading", "start_pos": 128, "end_pos": 140, "type": "DATASET", "confidence": 0.9402145445346832}]}, {"text": "The performance of our best variant (abstract title and first two sentences) approaches the upper bound on MetaMap performance-which is limited by human agreement on the identification of semantic concepts in medical texts, as established in.", "labels": [], "entities": [{"text": "identification of semantic concepts in medical texts", "start_pos": 170, "end_pos": 222, "type": "TASK", "confidence": 0.8189283439091274}]}, {"text": "Although problem extraction largely depends on disease coverage in UMLS and MetaMap performance, the error rate could be further reduced by more sophisticated recognition of implicitly stated problems.", "labels": [], "entities": [{"text": "problem extraction", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.8201096951961517}, {"text": "error rate", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9680972099304199}]}, {"text": "For example, with respect to a question about immunization in children, an abstract about the measles-mumps-rubella vaccination never mentioned the disease without the word vaccination; hence, no concept of the type DISEASE OR SYNDROME was identified.", "labels": [], "entities": [{"text": "DISEASE OR SYNDROME", "start_pos": 216, "end_pos": 235, "type": "METRIC", "confidence": 0.7437489827473959}]}, {"text": "The intervention extractor was evaluated in the same manner as the population extractor and compared to the same baseline.", "labels": [], "entities": []}, {"text": "To iterate, 90 held-out clinical abstracts that contained human-annotated interventions served as ground truth.", "labels": [], "entities": []}, {"text": "The output of our intervention extractor was judged to be correct if it occurred in a sentence that was annotated as containing the intervention in the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 152, "end_pos": 165, "type": "DATASET", "confidence": 0.9534015655517578}]}, {"text": "As with the evaluation of the population extractor, this represents an upper bound on performance.", "labels": [], "entities": []}, {"text": "Some of the errors were caused by ambiguity of terms.", "labels": [], "entities": []}, {"text": "For example, in the clause serum levels of anti-HBsAg and presence of autoantibodies (ANA, ENA) were evaluated, serum is recognized as a TISSUE, levels as INTELLECTUAL PRODUCT, and autoantibodies and ANA as IMMUNOLOGIC FACTORS.", "labels": [], "entities": [{"text": "ANA, ENA)", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.7127108871936798}, {"text": "TISSUE", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9964599013328552}, {"text": "INTELLECTUAL PRODUCT", "start_pos": 155, "end_pos": 175, "type": "METRIC", "confidence": 0.8880755603313446}, {"text": "ANA", "start_pos": 200, "end_pos": 203, "type": "METRIC", "confidence": 0.8573058843612671}, {"text": "IMMUNOLOGIC FACTORS", "start_pos": 207, "end_pos": 226, "type": "METRIC", "confidence": 0.7681334614753723}]}, {"text": "In this case, however, autoantibodies should be considered a LABORATORY OR TEST RESULT.", "labels": [], "entities": [{"text": "LABORATORY", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9764214754104614}, {"text": "TEST", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9967114925384521}, {"text": "RESULT", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.5870456695556641}]}, {"text": "3 In other cases, extraction errors were caused by summary sentences that were very similar to intervention statements, for example, This study compared the effects of 52 weeks' treatment with pioglitazone, a thiazolidinedione that reduces insulin resistance, and glibenclamide, on insulin sensitivity, glycaemic control, and lipids in patients with Type 2 diabetes.", "labels": [], "entities": []}, {"text": "For this particular abstract, the correct interventions are contained in the sentence Patients with Type 2 diabetes were randomized to receive either pioglitazone (initially 30 mg QD, n = 91) or micronized glibenclamide (initially 1.75 mg QD, n = 109) as monotherapy.", "labels": [], "entities": []}, {"text": "Because outcome statements were annotated in each of the 633 citations in our collection, it was possible to evaluate the outcome extractor on a broader set of abstracts.", "labels": [], "entities": []}, {"text": "From those not used in training the outcome classifiers, 153 citations pertaining to therapy were selected.", "labels": [], "entities": []}, {"text": "Of these, 143 contained outcome statements and were used as the blind held-out test set.", "labels": [], "entities": []}, {"text": "In addition, outcome statements in abstracts pertaining to diagnosis (57), prognosis (111), and etiology (37) were also used.", "labels": [], "entities": []}, {"text": "The output of our outcome extractor is a ranked list of sentences sorted by confidence.", "labels": [], "entities": []}, {"text": "Based on the observation that human annotators typically mark two to three sentences in each abstract as outcomes, we evaluated the performance of our extractor at cutoffs of two and three sentences.", "labels": [], "entities": []}, {"text": "These results are shown in: The columns marked AH2 and AH3 show performance of the weighted linear interpolation approach with ad hoc weight assignment at two-and three-sentence cutoffs, respectively; the columns marked LR2 and LR3 show performance of the least squares linear regression model at the same cutoffs.", "labels": [], "entities": [{"text": "AH2", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9081164598464966}, {"text": "AH3", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.5380475521087646}]}, {"text": "In the evaluation, our outcome extractor was considered correct if the returned sentences intersected with sentences judged as outcomes by our human annotators.", "labels": [], "entities": []}, {"text": "Although this is a lenient criterion, it does roughly capture the performance of our knowledge extractor.", "labels": [], "entities": []}, {"text": "Because outcome statements are typically found in the conclusion of a structured abstract (or near the end of the abstract in the case of unstructured abstracts), we compared our answer extractor to the baseline of returning either the final two or final three sentences in the abstract (B2 and B3 in).", "labels": [], "entities": []}, {"text": "As can be seen, variants of our outcome extractor performed better than the baseline at the two-sentence cutoff, for the most part.", "labels": [], "entities": [{"text": "outcome extractor", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.6336372941732407}]}, {"text": "Bigger improvements, however, can be seen at the three-sentence cutoff level.", "labels": [], "entities": []}, {"text": "It is evident that the assignment of weights in our ad hoc model is primarily geared towards therapy questions, perhaps overly so.", "labels": [], "entities": []}, {"text": "Better overall performance is obtained with the least squares linear regression model.", "labels": [], "entities": []}, {"text": "Examples of strength of evidence categories based on Publication Type and MeSH headings.", "labels": [], "entities": [{"text": "MeSH headings", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.7517009675502777}]}, {"text": "The previous section describes a relevance-scoring algorithm for MEDLINE citations that attempts to capture the principles of EBM.", "labels": [], "entities": [{"text": "MEDLINE citations", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7092430293560028}]}, {"text": "In this section, we present an evaluation of this algorithm.", "labels": [], "entities": []}, {"text": "Ideally, questions should be answered by directly comparing queries to knowledge structures derived from MEDLINE citations.", "labels": [], "entities": [{"text": "MEDLINE citations", "start_pos": 105, "end_pos": 122, "type": "DATASET", "confidence": 0.8223477303981781}]}, {"text": "However, knowledge extraction on such large scales is impractical given our computational resources, so we opted for an IR-based pipeline approach.", "labels": [], "entities": [{"text": "knowledge extraction", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.8361229300498962}]}, {"text": "Under this strategy, an existing search engine would be employed to generate a candidate list of citations to be rescored, according to our algorithm.", "labels": [], "entities": []}, {"text": "PubMed is a logical choice for gathering this initial list of citations because it represents one of the most widely used tools employed by physicians and other health professionals today.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9447897672653198}]}, {"text": "The system supports boolean operators and sorts results chronologically, most recent citations first.", "labels": [], "entities": []}, {"text": "This two-stage retrieval process immediately suggests an evaluation methodology for our citation scoring algorithm-as a document reranking task.", "labels": [], "entities": [{"text": "citation scoring", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.8206462264060974}]}, {"text": "Given an initial hit list, can our algorithm automatically re-sort the results such that relevant documents are brought to higher ranks?", "labels": [], "entities": []}, {"text": "Not only is such a task intuitive to understand, this conceptualization also lends itself to an evaluation based on widely accepted practices in information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 145, "end_pos": 166, "type": "TASK", "confidence": 0.7399781346321106}]}, {"text": "For each question in our test collection, PubMed queries were manually crafted to fetch an initial set of hits.", "labels": [], "entities": []}, {"text": "These queries took advantage of existing advanced search features to simulate the types of results that would be currently available to a knowledgeable physician.", "labels": [], "entities": []}, {"text": "Specifically, widely accepted tools for narrowing down PubMed search results such as Clinical Queries were employed whenever appropriate.", "labels": [], "entities": []}, {"text": "As a concrete example, consider the following question: What is the best treatment for analgesic rebound headaches?", "labels": [], "entities": [{"text": "analgesic rebound headaches", "start_pos": 87, "end_pos": 114, "type": "TASK", "confidence": 0.6341474453608195}]}, {"text": "The search started with the initial terms \"analgesic rebound headache\" with a \"narrow therapy filter.\"", "labels": [], "entities": []}, {"text": "In PubMed, this query is: Note that PubMed automatically identifies concepts and attempts matching both in abstract text and MeSH headings.", "labels": [], "entities": []}, {"text": "We always restrict searches to articles that have abstracts, are published in English, and are assigned the MeSH term humans (as opposed to say, experiments on animals)-these are all strategies commonly used by clinicians.", "labels": [], "entities": []}, {"text": "In this case, because none of the top 20 results were relevant, the query was expanded with the term side effects to emphasize the aspect of the problem requiring an intervention.", "labels": [], "entities": []}, {"text": "The final query for the question became: The first author, who is a medical doctor, performed the query formulation process manually for every question in our collection; she verified that each hit list contained at least some relevant documents and that the results were as good as could be reasonably achieved.", "labels": [], "entities": [{"text": "query formulation", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.6785628944635391}]}, {"text": "The process of generating queries averaged about 40 minutes per question.", "labels": [], "entities": []}, {"text": "The top 50 results for each query were retained for our experiments.", "labels": [], "entities": []}, {"text": "In total, 2,309 citations were gathered because some queries returned fewer than 50 hits.", "labels": [], "entities": []}, {"text": "The process of generating a \"good\" PubMed query is not a trivial task, which we have side-stepped in this work by placing a human in the loop.", "labels": [], "entities": []}, {"text": "We return to this issue in Section 12.", "labels": [], "entities": []}, {"text": "All abstracts gathered by this process were exhaustively examined for relevance by the first author.", "labels": [], "entities": []}, {"text": "It is important to note that relevance assessment in the clinical domain requires significant medical knowledge (in short, a medical degree).", "labels": [], "entities": [{"text": "relevance assessment", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.8859123587608337}]}, {"text": "After careful consideration, we decided to assess only topical relevance, with the understanding that the applicability of information from a specific citation in real-world settings depends on a variety of other factors (see Section 10 for further discussion).", "labels": [], "entities": []}, {"text": "Each citation was assigned one of four labels:  Because all abstracts were judged, we did not have to worry about impartiality issues when comparing different systems.", "labels": [], "entities": []}, {"text": "In total, the relevance assessment process took approximately 100 hours, or about an average of 2 hours per question.", "labels": [], "entities": [{"text": "relevance assessment process", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.9178843895594279}]}, {"text": "Our reranking experiment compared four different systems: r The baseline PubMed results.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.9237476587295532}]}, {"text": "r A term-based reranker that computes term overlap between the natural language question and the citation (i.e., counted words shared between the two strings).", "labels": [], "entities": []}, {"text": "Each term match was weighted by the outcome score of the sentence from which it came (see Section 5.7).", "labels": [], "entities": []}, {"text": "This simple algorithm favors term matches that occur in sentences recognized as outcome statements.", "labels": [], "entities": []}, {"text": "r A reranker based on the EBM scorer described in the previous section.", "labels": [], "entities": []}, {"text": "r A reranker that combines normalized scores from the term-based reranker and the EBM-based reranker (weighted linear interpolation).", "labels": [], "entities": []}, {"text": "Questions in the development set were used to debug the EBM-based reranker as we implemented the scoring algorithm.", "labels": [], "entities": [{"text": "EBM-based reranker", "start_pos": 56, "end_pos": 74, "type": "DATASET", "confidence": 0.7860994040966034}]}, {"text": "The development questions were also used to tune the weight for combining scores from the term-based scorer and EBM-based scorer; by simply trying all possible values, we settled on a \u03bb of 0.8, that is, 80% weight to the EBM score, and 20% weight to the term-based score.", "labels": [], "entities": []}, {"text": "As we shall see later, it is unclear if evidence combination in this simple manner helps at all; for one, it is debatable which metric should be optimized.", "labels": [], "entities": []}, {"text": "The test questions were hidden during the system development phase and served as a blind held-out test set for assessing the generality of our algorithm.", "labels": [], "entities": []}, {"text": "In our experiment, we collected the following metrics, all computed automatically using our relevance judgments: r Precision at ten retrieved documents (P10) measures the fraction of relevant documents in the top ten results.", "labels": [], "entities": [{"text": "Precision at ten retrieved documents (P10)", "start_pos": 115, "end_pos": 157, "type": "METRIC", "confidence": 0.6281868815422058}]}, {"text": "r Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 2, "end_pos": 30, "type": "METRIC", "confidence": 0.961035837729772}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9945553541183472}]}, {"text": "It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.8083051443099976}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9991589784622192}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9937325119972229}]}, {"text": "r Mean Reciprocal Rank (MRR) is a measure of how far down a hit list the user must browse before encountering the first relevant result.", "labels": [], "entities": [{"text": "r Mean Reciprocal Rank (MRR)", "start_pos": 0, "end_pos": 28, "type": "METRIC", "confidence": 0.8904373475483486}]}, {"text": "The score is equal to the reciprocal of the rank, that is, a relevant document at rank 1 gets a score of 1, 1/2 at rank 2, 1/3 at rank 3, and soon.", "labels": [], "entities": []}, {"text": "Note that this measure only captures the appearance of the first relevant document.", "labels": [], "entities": []}, {"text": "Furthermore, due to its discretization, MRR values are noisy on small collections.", "labels": [], "entities": [{"text": "MRR", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.6437830924987793}]}, {"text": "r Total Document Reciprocal Rank (TDRR) is the sum of the reciprocal ranks of all relevant documents.", "labels": [], "entities": [{"text": "Total Document Reciprocal Rank (TDRR)", "start_pos": 2, "end_pos": 39, "type": "METRIC", "confidence": 0.8317663882459913}]}, {"text": "For example, if relevant documents were found at ranks 2 and 5, the TDRR would be 1/2 + 1/5 = 0.7.", "labels": [], "entities": [{"text": "TDRR", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.5396674871444702}]}, {"text": "TDRR provides an advantage over MRR in that it captures the ranks of all relevant documents-emphasizing their appearance at higher ranks.", "labels": [], "entities": [{"text": "TDRR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8782588839530945}, {"text": "MRR", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8282607793807983}]}, {"text": "The downside, however, is that TDRR does not have an intuitive interpretation.", "labels": [], "entities": [{"text": "TDRR", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.726904571056366}]}, {"text": "For our reranking experiment, we applied the Wilcoxon signed-rank test to determine the statistical significance of the results.", "labels": [], "entities": []}, {"text": "This testis commonly used in information retrieval research because it makes minimal assumptions about the underlying distribution of differences.", "labels": [], "entities": [{"text": "information retrieval research", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.8468929131825765}]}, {"text": "For each evaluation metric, significance at the 1% level is indicated by either \ud97b\udf59 or \ud97b\udf59 , depending on the direction of change; significance at the 5% level is indicated by \ud97b\udf59 or \ud97b\udf59 , depending on the direction of change.", "labels": [], "entities": [{"text": "significance", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.9887194037437439}, {"text": "significance", "start_pos": 127, "end_pos": 139, "type": "METRIC", "confidence": 0.9631003737449646}]}, {"text": "Differences that are not statistically significant are marked with the symbol \u2022 . We report results under two different scoring criteria.", "labels": [], "entities": []}, {"text": "Under the lenient condition, documents marked \"contains answer\" and \"relevant\" were given credit; these results are shown in (for the development set) and (for the blind held-out test set).", "labels": [], "entities": []}, {"text": "Across all questions, both the EBM-based reranker and combination reranker significantly outperform the PubMed baseline on all metrics.", "labels": [], "entities": [{"text": "PubMed baseline", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.9304026663303375}]}, {"text": "In many cases, the differences are particularly noteworthy-for example, our EBM citation scoring algorithm more than doubles the baseline in terms of MAP and P10 on the test set.", "labels": [], "entities": [{"text": "EBM citation scoring", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.5041196147600809}, {"text": "MAP", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.9504038095474243}]}, {"text": "There are enough therapy questions to achieve statistical significance in the task-specific results; however, due to the smaller number of questions for the other clinical tasks, those results are not statistically significant.", "labels": [], "entities": []}, {"text": "Results also show that the simple term-based reranker outperforms the PubMed baseline, demonstrating the importance of recognizing outcome statements in MEDLINE abstracts.", "labels": [], "entities": [{"text": "PubMed baseline", "start_pos": 70, "end_pos": 85, "type": "DATASET", "confidence": 0.9493012130260468}]}, {"text": "Are the differences in performance between the term-based, EBM, and combination rerankers statistically significant?", "labels": [], "entities": []}, {"text": "Results of Wilcoxon signed-rank tests are shown in.", "labels": [], "entities": []}, {"text": "Both the EBM and combination rerankers significantly outperform the termbased reranker (at the 1% level, on all metrics, on both development and test set), with the exception of MRR on the development set.", "labels": [], "entities": [{"text": "MRR", "start_pos": 178, "end_pos": 181, "type": "METRIC", "confidence": 0.7515226602554321}]}, {"text": "However, for all metrics, on both the development set and test set, there is no significant difference between the EBM and combination reranker (which combines both term-based and EBM-based evidence).", "labels": [], "entities": []}, {"text": "In the parameter tuning process, we could not find a weight where performance across all measures was higher; in the end, we settled on what we felt was a reasonable weight that improved P10 and MRR on the development set.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7900456786155701}, {"text": "P10", "start_pos": 187, "end_pos": 190, "type": "METRIC", "confidence": 0.5804897546768188}, {"text": "MRR", "start_pos": 195, "end_pos": 198, "type": "METRIC", "confidence": 0.9305763840675354}]}, {"text": "Under the strict condition, only documents marked \"contains answer\" were given credit; these results are shown in (for the development set) and (for the blind held-out test set).", "labels": [], "entities": []}, {"text": "The same trend is observed-in fact, larger relative gains were achieved under the strict scoring criteria for our EBM and combination rerankers.", "labels": [], "entities": []}, {"text": "Results of Wilcoxon signed-rank tests on the term-based, EBM, and combination rerankers are also shown in for the strict scoring condition.", "labels": [], "entities": []}, {"text": "In most cases, combining term scoring with EBM scoring does not help.", "labels": [], "entities": [{"text": "term scoring", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.742472767829895}, {"text": "EBM scoring", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.49045267701148987}]}, {"text": "In almost all cases, the EBM and combination reranker perform significantly better than the term-based reranker.", "labels": [], "entities": []}, {"text": "How does better ranking of citations impact end-to-end question answering performance?", "labels": [], "entities": [{"text": "question answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7250943183898926}]}, {"text": "We shall return to this issue in Sections 9 and 10, which describe and evaluate the answer generation module, respectively.", "labels": [], "entities": [{"text": "answer generation", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.8758030235767365}]}, {"text": "In the next section, we describe more detailed experiments with our EBM citation scoring algorithm.", "labels": [], "entities": [{"text": "EBM citation scoring", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7112935284773508}]}, {"text": "(b) Performance across all clinical tasks \ud97b\udf59\ud97b\udf59 Significance at the 1% level, depending on direction of change.", "labels": [], "entities": [{"text": "\ud97b\udf59\ud97b\udf59", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.944348931312561}, {"text": "Significance", "start_pos": 45, "end_pos": 57, "type": "METRIC", "confidence": 0.6057682037353516}]}, {"text": "\ud97b\udf59\ud97b\udf59 Significance at the 5% level, depending on direction of change.", "labels": [], "entities": [{"text": "\ud97b\udf59\ud97b\udf59", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9395850896835327}, {"text": "Significance", "start_pos": 3, "end_pos": 15, "type": "METRIC", "confidence": 0.9380722641944885}]}, {"text": "\u2022 Difference not statistically significant.", "labels": [], "entities": [{"text": "Difference", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9854556918144226}]}, {"text": "Evaluation of answers within a clinical setting involves a complex decision that must not only take into account topical relevance (i.e., \"Does the answer address the information need?\"), but also situational relevance (e.g.,.", "labels": [], "entities": []}, {"text": "The latter factor includes many issues such as the strength of evidence, recency of results, and reputation of the journal.", "labels": [], "entities": [{"text": "recency", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.991942286491394}]}, {"text": "Clinicians need to carefully consider all these elements before acting on any information for the purposes of patient care.", "labels": [], "entities": []}, {"text": "Within the framework of evidence-based medicine, the physician is the final arbiter of how clinical answers are integrated into the broader activities of medical care, but this complicates any attempt to evaluate answers generated by our system.", "labels": [], "entities": []}, {"text": "In assessing answers produced by our system, we decided to focus only on the evaluation of topical relevance-assessors were only presented with answer strings, generated in the manner described in the previous section.", "labels": [], "entities": []}, {"text": "Metadata that would contribute to judgments about situational relevance, such as the strength of evidence, names of the authors and the journal, and soon, were purposefully suppressed.", "labels": [], "entities": []}, {"text": "Our evaluation compared the top five answers generated from the original PubMed hit list and the top five answers generated from our reranked list of citations.", "labels": [], "entities": [{"text": "PubMed hit list", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.9543164372444153}]}, {"text": "Answers were prepared for all 24 questions in our development set.", "labels": [], "entities": []}, {"text": "We recruited two medical doctors (one family practitioner, one surgeon) from the National Library of Medicine to evaluate the textual answers.", "labels": [], "entities": [{"text": "National Library of Medicine", "start_pos": 81, "end_pos": 109, "type": "DATASET", "confidence": 0.9630686342716217}]}, {"text": "Our instructions clearly stated that only topical relevance was to be assessed.", "labels": [], "entities": [{"text": "topical relevance", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7664066851139069}]}, {"text": "We asked the physicians to provide three-valued judgments: r A plus (+) indicates that the response directly answers the question.", "labels": [], "entities": []}, {"text": "Naturally, the physicians would need to followup and examine the source citation in more detail.", "labels": [], "entities": []}, {"text": "r A check ( \u221a ) indicates that the response provides clinically relevant information that may factor into decisions about patient treatment, and that the source citation was worth examining in more detail.", "labels": [], "entities": []}, {"text": "r A minus (\u2212) indicates that the response does not provide useful information in answering the clinical question, and that the source citation was not worth examining.", "labels": [], "entities": []}, {"text": "We purposely avoided short linguistic labels for the judgments so as to sidestep the question of \"What exactly is an answer to a clinical question?\"", "labels": [], "entities": []}, {"text": "Informally, answers marked with a plus can be considered \"actionable\" clinical advice.", "labels": [], "entities": []}, {"text": "Answers marked with a check provide relevant information that may influence the physician's actions.", "labels": [], "entities": []}, {"text": "We adopted a double-blind study design for the actual assessment process: Answers from both systems were presented in a randomized order without any indication of which system the response came from (duplicates were suppressed).", "labels": [], "entities": []}, {"text": "A paper printout, containing each question followed by the blinded answers, was presented to each assessor.", "labels": [], "entities": []}, {"text": "We then coded the relevance judgments in a plain text file manually.", "labels": [], "entities": []}, {"text": "During this entire time, the key that maps answers to systems was kept in a separate file and hidden from everyone, including the authors.", "labels": [], "entities": []}, {"text": "All scores were computed automatically without human intervention.", "labels": [], "entities": []}, {"text": "Answer precision was calculated for two separate conditions: Under the strict condition, only \"plus\" judgments were considered good; under the lenient condition, both \"plus\" and \"check\" judgments were considered good.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.902057409286499}]}, {"text": "As can be seen, our EBM algorithm significantly outperforms the baseline under both the strict and lenient conditions, according to both assessors.", "labels": [], "entities": []}, {"text": "On average, the length of answers generated from the original PubMed list of citations was 90 words; answers generated from the reranked list of citations averaged 87 words.", "labels": [], "entities": [{"text": "PubMed list of citations", "start_pos": 62, "end_pos": 86, "type": "DATASET", "confidence": 0.9237609207630157}]}, {"text": "Answers from both sources  Lenient answer precision (considering both \"plus\" and \"check\" judgments). were significantly shorter than the abstracts from which they were extracted (250 word average for original PubMed results and 270 word average for reranked results).", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.8616508841514587}]}], "tableCaptions": [{"text": " Table 1  Evaluation of the population extractor.", "labels": [], "entities": [{"text": "population extractor", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.721361517906189}]}, {"text": " Table 2  Evaluation of the problem extractor.", "labels": [], "entities": [{"text": "problem extractor", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7623483240604401}]}, {"text": " Table 4  Evaluation of the outcome extractor. B = baseline, returns last sentences in abstract; AH = ad hoc  weight assignment; LR = least squares linear regression. Statistically significant improvement  over the baseline at the 1% level is indicated by \ud97b\udf59 .", "labels": [], "entities": [{"text": "AH", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9926204681396484}]}, {"text": " Table 5  Examples of strength of evidence categories based on Publication Type and MeSH headings.", "labels": [], "entities": [{"text": "MeSH headings", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.7484983205795288}]}, {"text": " Table 12  Results of optimizing \u03bb 1 and \u03bb 2 on therapy questions.", "labels": [], "entities": []}, {"text": " Table 13  Results of optimizing \u03bb 1 and \u03bb 2 on all questions.", "labels": [], "entities": []}, {"text": " Table 15  Strict answer precision (considering only \"plus\" judgments).", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.8508123159408569}]}]}