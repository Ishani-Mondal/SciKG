{"title": [{"text": "Squib Prepositional Phrase Attachment without Oracles", "labels": [], "entities": [{"text": "Squib Prepositional Phrase Attachment without Oracles", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7922995785872141}]}], "abstractContent": [{"text": "Work on prepositional phrase (PP) attachment resolution generally assumes that there is an oracle that provides the two hypothesized structures that we want to choose between.", "labels": [], "entities": [{"text": "prepositional phrase (PP) attachment resolution", "start_pos": 8, "end_pos": 55, "type": "TASK", "confidence": 0.71548478943961}]}, {"text": "The information that there are two possible attachment sites and the information about the lexical heads of those phrases is usually extracted from gold-standard parse trees.", "labels": [], "entities": []}, {"text": "We show that the performance of reattachment methods is higher with such an oracle than without.", "labels": [], "entities": []}, {"text": "Because oracles are not available in NLP applications, this indicates that the current evaluation methodology for PP attachment does not produce realistic performance numbers.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.8820687830448151}]}, {"text": "We argue that PP attachment should not be evaluated in isolation, but instead as an integral component of a parsing system, without using information from the gold-standard oracle.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.8996391594409943}]}], "introductionContent": [{"text": "One of the main challenges in natural language parsing is the resolution of ambiguity.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6364739139874777}]}, {"text": "One frequently studied type of ambiguity is prepositional phrase (PP) attachment.", "labels": [], "entities": [{"text": "prepositional phrase (PP) attachment", "start_pos": 44, "end_pos": 80, "type": "TASK", "confidence": 0.6412401249011358}]}, {"text": "Given the quadruple (v,n1,p,n2), where v is the head of a verb phrase, n1 is the head of an NP1 dominated by v, p is the head of a prepositional phrase, and n2 the head of an NP2 embedded in the PP, the task of PP attachment is to determine whether we should attach the PP to the verb v or the noun n1.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 211, "end_pos": 224, "type": "TASK", "confidence": 0.7381917536258698}]}, {"text": "Work on PP attachment resolution generally assumes that there is an oracle that provides the quadruple (v,n1,p,n2), where we define an oracle as a mechanism that provides information that is not present in the data in their naturally occurring form.", "labels": [], "entities": [{"text": "PP attachment resolution", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.955849270025889}]}, {"text": "In PP attachment, the oracle is usually implemented by extracting the quadruple (v,n1,p,n2) from the gold-standard parse trees.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.9176472425460815}]}, {"text": "In an application, a PP attachment module would be used to change the attachment of prepositional phrases in preliminary syntactic analyses produced by a parser-or reattach them.", "labels": [], "entities": []}, {"text": "The problem with oracle-based work on PP attachment is that when the parser does not find the gold-standard NP1 or PP, for instance, the attachment ambiguity is not recognized, and the correct solution cannot be found by the PP attachment method.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.8293024003505707}]}, {"text": "Likewise, it is harder for the reattachment method to find the correct attachment if the heads n1 or n2 are not correctly identified by the parser.", "labels": [], "entities": []}, {"text": "The oracle approach to PP attachment differs from many other tasks in NLP like part-of-speech tagging or parsing, in which usually no data based on manual annotation are part of the input.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.963736355304718}, {"text": "part-of-speech tagging", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.7155117094516754}]}, {"text": "Thus, published evaluation results for tagging and parsing accurately reflect the performance we would expect in an application whereas PP reattachment results arguably do not.", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.6753741006056467}, {"text": "PP reattachment", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.7379232347011566}]}, {"text": "We will show in this article that the performance of reattachment methods is higher when an oracle is available.", "labels": [], "entities": []}, {"text": "This means that the current evaluation methodology for PP attachment does not produce realistic performance numbers.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.9608495831489563}]}, {"text": "In particular, it is not clear whether reattachment methods improve the parses of state-of-the-art parsers.", "labels": [], "entities": [{"text": "parses", "start_pos": 72, "end_pos": 78, "type": "TASK", "confidence": 0.9526179432868958}]}, {"text": "This argues fora new evaluation methodology for PP attachment, one that does not assume that an oracle is available.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.9751766324043274}]}, {"text": "To create a more realistic setup for PP reattachment, we replace the oracle with Bikel's parser).", "labels": [], "entities": [{"text": "PP reattachment", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.887218713760376}]}, {"text": "With the removal of the oracle and the introduction of the parser, the baseline performance also changes.", "labels": [], "entities": []}, {"text": "It is no longer the performance of always choosing the most frequent attachment.", "labels": [], "entities": []}, {"text": "Instead, it is the attachment performance of the parser.", "labels": [], "entities": []}, {"text": "In fact, we will see that it is surprisingly difficult for reattachment methods to beat this baseline performance.", "labels": [], "entities": []}, {"text": "The fact that standard parsers perform well on PP attachment also prompted us to investigate the performance of a standard parser on traditional oracle-based PPattachment.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.8328251540660858}]}, {"text": "We find that standard parsers do well, further questioning the soundness of the traditional evaluation methodology.", "labels": [], "entities": []}, {"text": "We compare our results with three other approaches: Collins and Brooks (1995; henceforth C&B),.", "labels": [], "entities": [{"text": "Collins and Brooks (1995; henceforth C&B)", "start_pos": 52, "end_pos": 93, "type": "DATASET", "confidence": 0.8484728011217985}]}, {"text": "We call these three methods PP reattachers.", "labels": [], "entities": []}, {"text": "We selected these three methods because they perform best on the widely used PP attachment evaluation set created by.", "labels": [], "entities": [{"text": "PP attachment evaluation", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.8385007381439209}]}, {"text": "We call this data set RRR.", "labels": [], "entities": [{"text": "RRR", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.4649810492992401}]}, {"text": "RRR consists of 20,801 training and 3,097 test quadruples of the form (v,n1,p,n2).", "labels": [], "entities": [{"text": "RRR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.4513586461544037}]}, {"text": "The performance of the three reattachers on RRR is shown in.", "labels": [], "entities": [{"text": "RRR", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.9239514470100403}]}, {"text": "This article is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 compares the performance of the reattachers on oracle-based reattachment with that of a standard parser (using artificial sentences built from the RRR set) and finds no significant difference in performance.", "labels": [], "entities": [{"text": "RRR set", "start_pos": 157, "end_pos": 164, "type": "DATASET", "confidence": 0.8437466323375702}]}, {"text": "In Section 3, we look at reattachment without oracles (using the Penn Treebank) and argue that realistic performance numbers can only be produced in experiments without oracles.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9966039955615997}]}, {"text": "Sections 4 and 5 discuss the experiments and related work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Bikel parser) with default settings for training and parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.9587096571922302}]}, {"text": "In Experiment 1, we convert the RRR set into artificial sentences.", "labels": [], "entities": [{"text": "RRR set", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.6852134764194489}]}, {"text": "The data consist of quadruples of the form (v,n1,p,n2): verb, noun, preposition, and embedded noun.", "labels": [], "entities": []}, {"text": "To create sentences, we add the generic subject they.", "labels": [], "entities": []}, {"text": "For example, the quadruple join board as director V becomes: Note that these artificial sentences are not necessarily grammatically correct.", "labels": [], "entities": []}, {"text": "For example, subject and verb need not agree.", "labels": [], "entities": []}, {"text": "When we train on these artificial sentences, attachment decisions are independent of the embedded noun n2 because the Bikel parser does not percolate non-head information up the tree.", "labels": [], "entities": []}, {"text": "We therefore call this experiment bilexical because we only use standard lexical head-head relationships.", "labels": [], "entities": []}, {"text": "To simulate a parser that takes into account all four elements of the quadruple, we annotate the preposition with the embedded noun: We call this mode trilexical because we effectively model three-word dependencies (e.g., have-of-million vs. revenue-of-million).", "labels": [], "entities": []}, {"text": "To avoid problems with sparseness, we restrict the annotation to the N most frequent n2 nouns of the RRR train data.", "labels": [], "entities": [{"text": "RRR train data", "start_pos": 101, "end_pos": 115, "type": "DATASET", "confidence": 0.9562911987304688}]}, {"text": "We chose N = 20 based on the performance on the development set.", "labels": [], "entities": []}, {"text": "shows the accuracy of Bikel's parser in bilexical and trilexical mode.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99973064661026}]}, {"text": "The parser sometimes does not recognize the attachment ambiguity.", "labels": [], "entities": []}, {"text": "For example, it may fail to correctly identify the PP.", "labels": [], "entities": []}, {"text": "We call these cases NAs (non-attachment cases) and compute three different evaluation measures:  The setup in Experiment 1 is typical of work on PP attachment, but it is not a realistic model of PP attachment in practice.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 145, "end_pos": 158, "type": "TASK", "confidence": 0.8692097067832947}, {"text": "PP attachment", "start_pos": 195, "end_pos": 208, "type": "TASK", "confidence": 0.8681256175041199}]}, {"text": "In Experiment 2, we look at reattachment in naturally occurring sentences if an oracle that identifies the two alternative attachments is not available.", "labels": [], "entities": []}, {"text": "We first tried to convert RRR into a set of sentences by identifying for each quadruple the sentence in the Penn Treebank (PTB) 0.5 it was extracted from.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) 0.5", "start_pos": 108, "end_pos": 131, "type": "DATASET", "confidence": 0.9781216283639272}]}, {"text": "However, we were notable to train the Bikel parser on this set because of inconsistencies and other problems (truncated incomplete sentences, etc.) with the 0.5 treebank.", "labels": [], "entities": []}, {"text": "On the other hand, it was not straightforward to create training, test, and development sets from PTB 3, which contains the corresponding sentences.", "labels": [], "entities": [{"text": "PTB 3", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.914539635181427}]}, {"text": "Due to changes in the treebank versions, a considerable number of the quadruples was missing in PTB 3.", "labels": [], "entities": [{"text": "PTB 3", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.9570934772491455}]}, {"text": "For these reasons, we identified as our test data the 3,097 PTB 0.5 sentences the RRR quadruples had been extracted from because for testing we only need string inputs (and the gold-standard attachment annotations of the quadruples).", "labels": [], "entities": [{"text": "PTB 0.5 sentences the RRR quadruples", "start_pos": 60, "end_pos": 96, "type": "DATASET", "confidence": 0.7520950734615326}]}, {"text": "Our training data consist of those 45,156 PTB 3 sentences that remain of the total number of 49,208 PTB 3 sentences after removing (i) the 3,097 test sentences and (ii) PTB 3 sentences that correspond to RRR development set quadruples.", "labels": [], "entities": [{"text": "RRR development", "start_pos": 204, "end_pos": 219, "type": "TASK", "confidence": 0.6800875067710876}]}, {"text": "(Note that some test sentences had no corresponding sentence in PTB 3.)", "labels": [], "entities": [{"text": "PTB 3", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.8580791652202606}]}, {"text": "We call this evaluation set (45,156 PTB 3 training sentences, 3,097 PTB 0.5 test sentences) RRR-sent.", "labels": [], "entities": [{"text": "PTB 0.5 test sentences", "start_pos": 68, "end_pos": 90, "type": "METRIC", "confidence": 0.801856279373169}, {"text": "RRR-sent", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.4958387017250061}]}, {"text": "The RRR-sent training set contains a mixture of sentences with and without PP attachment ambiguities.", "labels": [], "entities": [{"text": "RRR-sent training set", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.6921404302120209}]}, {"text": "We believe that this is the optimal experimental setup because parsers are usually not trained on a subset of sentences with a particular construction.", "labels": [], "entities": []}, {"text": "RRR-sent is available at http://ifnlp.org/\u223cschuetze/rrr-sent.", "labels": [], "entities": [{"text": "RRR-sent", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.4249056875705719}]}], "tableCaptions": [{"text": " Table 1  Performance of Collins and Brooks (C&B), Olteanu and Moldovan (O&M) and Toutanova,  Manning, and Ng (TM&N) on RRR when no resources other than the training set are used.", "labels": [], "entities": [{"text": "RRR", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.5272414684295654}]}]}