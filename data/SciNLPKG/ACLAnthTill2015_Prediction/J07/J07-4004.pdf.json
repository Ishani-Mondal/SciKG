{"title": [{"text": "Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models", "labels": [], "entities": [{"text": "Statistical Parsing", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8017070889472961}]}], "abstractContent": [{"text": "This article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar.", "labels": [], "entities": []}, {"text": "The models are \"full\" parsing models in the sense that probabilities are defined for complete parses, rather than for independent events derived by decomposing the parse tree.", "labels": [], "entities": []}, {"text": "Discriminative training is used to estimate the models, which requires incorrect parses for each sentence in the training data as well as the correct parse.", "labels": [], "entities": []}, {"text": "The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 158, "end_pos": 171, "type": "DATASET", "confidence": 0.9931472539901733}]}, {"text": "The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement (up to 25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithm running on a Beowulf cluster.", "labels": [], "entities": [{"text": "BFGS optimization", "start_pos": 197, "end_pos": 214, "type": "TASK", "confidence": 0.6746180653572083}, {"text": "Beowulf cluster", "start_pos": 238, "end_pos": 253, "type": "DATASET", "confidence": 0.9259815514087677}]}, {"text": "Dynamic programming over a packed chart, in combination with the parallel implementation, allows us to solve one of the largest-scale estimation problems in the statistical parsing literature in under three hours.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.7163013815879822}]}, {"text": "A key component of the parsing system, for both training and testing, is a Maximum En-tropy supertagger which assigns CCG lexical categories to words in a sentence.", "labels": [], "entities": []}, {"text": "The super-tagger makes the discriminative training feasible, and also leads to a highly efficient parser.", "labels": [], "entities": []}, {"text": "Surprisingly, given CCG's \"spurious ambiguity,\" the parsing speeds are significantly higher than those reported for comparable parsers in the literature.", "labels": [], "entities": [{"text": "parsing", "start_pos": 52, "end_pos": 59, "type": "TASK", "confidence": 0.9546211957931519}]}, {"text": "We also extend the existing parsing techniques for CCG by developing anew model and efficient parsing algorithm which exploits all derivations, including CCG's nonstandard derivations.", "labels": [], "entities": []}, {"text": "This model and parsing algorithm, when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of predicate-argument dependencies from CCGbank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.965171754360199}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9992128610610962}, {"text": "CCGbank", "start_pos": 166, "end_pos": 173, "type": "DATASET", "confidence": 0.8829960227012634}]}, {"text": "The parser is also evaluated on DepBank and compared against the RASP parser, outperforming RASP overall and on the majority of relation types.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9797296524047852}]}, {"text": "The evaluation on DepBank raises a number of issues regarding parser evaluation.", "labels": [], "entities": [{"text": "DepBank", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.9436938762664795}, {"text": "parser evaluation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.8952420949935913}]}, {"text": "This article provides a comprehensive blueprint for building a wide-coverage CCG parser.", "labels": [], "entities": []}, {"text": "We demonstrate that both accurate and highly efficient parsing is possible with CCG.", "labels": [], "entities": [{"text": "parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.9753661155700684}]}], "introductionContent": [{"text": "Log-linear models have been applied to a number of problems in NLP, for example, POS tagging, named entity recognition), chunking, and parsing).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.8588469624519348}, {"text": "named entity recognition", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.637350728114446}, {"text": "chunking", "start_pos": 121, "end_pos": 129, "type": "TASK", "confidence": 0.9594674706459045}, {"text": "parsing", "start_pos": 135, "end_pos": 142, "type": "TASK", "confidence": 0.9339704513549805}]}, {"text": "Log-linear models are also referred to as maximum entropy models and random fields in the NLP literature.", "labels": [], "entities": []}, {"text": "They are popular because of the ease with which complex discriminating features can be included in the model, and have been shown to give good performance across a range of NLP tasks.", "labels": [], "entities": []}, {"text": "Log-linear models have previously been applied to statistical parsing (), but typically under the assumption that all possible parses fora sentence can be enumerated.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7687589824199677}]}, {"text": "For manually constructed grammars, this assumption is usually sufficient for efficient estimation and decoding.", "labels": [], "entities": []}, {"text": "However, for wide-coverage grammars extracted from a treebank, enumerating all parses is infeasible.", "labels": [], "entities": []}, {"text": "In this article we apply the dynamic programming method of to a packed chart; however, because the grammar is automatically extracted, the packed charts require a considerable amount of memory: up to 25 GB.", "labels": [], "entities": []}, {"text": "We solve this massive estimation problem by developing a parallelized version of the estimation algorithm which runs on a Beowulf cluster.", "labels": [], "entities": [{"text": "Beowulf cluster", "start_pos": 122, "end_pos": 137, "type": "DATASET", "confidence": 0.9680362045764923}]}, {"text": "The lexicalized grammar formalism we use is Combinatory Categorial Grammar (CCG; Steedman 2000).", "labels": [], "entities": []}, {"text": "A number of statistical parsing models have recently been developed for CCG and used in parsers applied to newspaper text).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.5779736638069153}]}, {"text": "In this article we extend existing parsing techniques by developing log-linear models for CCG, as well as anew model and efficient parsing algorithm which exploits all CCG's derivations, including the nonstandard ones.", "labels": [], "entities": [{"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.965445339679718}]}, {"text": "Estimating a log-linear model involves computing expectations of feature values.", "labels": [], "entities": []}, {"text": "For the conditional log-linear models used in this article, computing expectations requires a sum overall derivations for each sentence in the training data.", "labels": [], "entities": []}, {"text": "Because there can be a massive number of derivations for some sentences, enumerating all derivations is infeasible.", "labels": [], "entities": []}, {"text": "To solve this problem, we have adapted the dynamic programming method of to packed CCG charts.", "labels": [], "entities": []}, {"text": "A packed chart efficiently represents all derivations fora sentence.", "labels": [], "entities": []}, {"text": "The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside-outside algorithm for estimating the parameters of a PCFG from unlabeled data (.", "labels": [], "entities": []}, {"text": "Generalized Iterative Scaling () is a common choice in the NLP literature for estimating a log-linear model (e.g.,.", "labels": [], "entities": []}, {"text": "Initially we used generalized iterative scaling (GIS) for the parsing models described here, but found that convergence was extremely slow; present a similar finding for globally optimized log-linear models for sequences.", "labels": [], "entities": [{"text": "convergence", "start_pos": 108, "end_pos": 119, "type": "METRIC", "confidence": 0.9818859100341797}]}, {"text": "As an alternative to GIS, we use the limited-memory BFGS algorithm (Nocedal and Wright 1999).", "labels": [], "entities": [{"text": "BFGS", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.7516646385192871}]}, {"text": "As Malouf (2002) demonstrates, general purpose numerical optimization algorithms such as BFGS can converge much faster than iterative scaling algorithms (including Improved Iterative Scaling; Della Pietra, Della Pietra, and.", "labels": [], "entities": [{"text": "Improved Iterative Scaling", "start_pos": 164, "end_pos": 190, "type": "TASK", "confidence": 0.5087595482667288}, {"text": "Della Pietra", "start_pos": 206, "end_pos": 218, "type": "DATASET", "confidence": 0.8783044815063477}]}, {"text": "Despite the use of a packed representation, the complete set of derivations for the sentences in the training data requires up to 25 GB of RAM for some of the models in this article.", "labels": [], "entities": []}, {"text": "There area number of ways to solve this problem.", "labels": [], "entities": []}, {"text": "Possibilities include using a subset of the training data; repeatedly parsing the training data for each iteration of the estimation algorithm; or reading the packed charts from disk for each iteration.", "labels": [], "entities": []}, {"text": "These methods are either too slow or sacrifice parsing performance, and so we use a parallelized version of BFGS running on an 18-node Beowulf cluster to perform the estimation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.9705454707145691}, {"text": "BFGS", "start_pos": 108, "end_pos": 112, "type": "DATASET", "confidence": 0.7772822976112366}, {"text": "Beowulf cluster", "start_pos": 135, "end_pos": 150, "type": "DATASET", "confidence": 0.9613487124443054}]}, {"text": "Even given the large number of derivations and the large feature sets in our models, the estimation time for the best-performing model is less than three hours.", "labels": [], "entities": [{"text": "estimation", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9093589186668396}]}, {"text": "This gives us a practical framework for developing a statistical parser.", "labels": [], "entities": []}, {"text": "A corollary of CCG's base-generative treatment of long-range dependencies in relative clauses and coordinate constructions is that the standard predicate-argument relations can be derived via nonstandard surface derivations.", "labels": [], "entities": []}, {"text": "The addition of \"spurious\" derivations in CCG complicates the modeling and parsing problems.", "labels": [], "entities": []}, {"text": "In this article we consider two solutions.", "labels": [], "entities": []}, {"text": "The first, following, is to define a model in terms of normal-form derivations.", "labels": [], "entities": []}, {"text": "In this approach we recover only one derivation leading to a given set of predicate-argument dependencies and ignore the rest.", "labels": [], "entities": []}, {"text": "The second approach is to define a model over the predicate-argument dependencies themselves, by summing the probabilities of all derivations leading to a given set of dependencies.", "labels": [], "entities": []}, {"text": "We also define anew efficient parsing algorithm for such a model, based on, which maximizes the expected recall of dependencies.", "labels": [], "entities": [{"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9925128221511841}]}, {"text": "The development of this model allows us to test, for the purpose of selecting the correct predicate-argument dependencies, whether there is useful information in the additional derivations.", "labels": [], "entities": []}, {"text": "We also compare the performance of our best log-linear model against existing CCG parsers, obtaining the highest results to date for the recovery of predicateargument dependencies from CCGbank.", "labels": [], "entities": []}, {"text": "A key component of the parsing system is a Maximum Entropy CCG supertagger which assigns lexical categories to words in a sentence.", "labels": [], "entities": []}, {"text": "The role of the supertagger is twofold.", "labels": [], "entities": []}, {"text": "First, it makes discriminative estimation feasible by limiting the number of incorrect derivations for each training sentence; the supertagger can bethought of as supplying a number of incorrect but plausible lexical categories for each word in the sentence.", "labels": [], "entities": []}, {"text": "Second, it greatly increases the efficiency of the parser, which was the original motivation for supertagging (Bangalore and Joshi 1999).", "labels": [], "entities": []}, {"text": "One possible criticism of CCG has been that highly efficient parsing is not possible because of the additional \"spurious\" derivations.", "labels": [], "entities": []}, {"text": "In fact, we show that a novel method which tightly integrates the supertagger and parser leads to parse times significantly faster than those reported for comparable parsers in the literature.", "labels": [], "entities": []}, {"text": "The parser is evaluated on CCGbank (available through the Linguistic Data Consortium).", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9572776556015015}, {"text": "Linguistic Data Consortium", "start_pos": 58, "end_pos": 84, "type": "DATASET", "confidence": 0.749621202548345}]}, {"text": "In order to facilitate comparisons with parsers using different formalisms, we also evaluate on the publicly available DepBank (), using the Briscoe and Carroll annotation consistent with the RASP parser).", "labels": [], "entities": [{"text": "DepBank", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9349585771560669}]}, {"text": "The dependency annotation is designed to be as theory-neutral as possible to allow easy comparison.", "labels": [], "entities": []}, {"text": "However, there are still considerable difficulties associated with a cross-formalism comparison, which we describe.", "labels": [], "entities": []}, {"text": "Even though the CCG dependencies are being mapped into another representation, the accuracy of the CCG parser is over 81% F-score on labeled dependencies, against an upper bound of 84.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9997127652168274}, {"text": "F-score", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9990473389625549}]}, {"text": "The CCG parser also outperforms RASP overall and on the majority of dependency types.", "labels": [], "entities": [{"text": "RASP", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.8901225924491882}]}, {"text": "The contributions of this article are as follows.", "labels": [], "entities": []}, {"text": "First, we explain how to estimate a full log-linear parsing model for an automatically extracted grammar, on a scale as large as that reported anywhere in the NLP literature.", "labels": [], "entities": []}, {"text": "Second, the article provides a comprehensive blueprint for building a wide-coverage CCG parser, including theoretical and practical aspects of the grammar, the estimation process, and decoding.", "labels": [], "entities": []}, {"text": "Third, we investigate the difficulties associated with cross-formalism parser comparison, evaluating the parser on DepBank.", "labels": [], "entities": [{"text": "cross-formalism parser comparison", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.7053738037745158}, {"text": "DepBank", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.9791298508644104}]}, {"text": "And finally, we develop new models and decoding algorithms for", "labels": [], "entities": []}], "datasetContent": [{"text": "The statistics relating to model estimation were obtained using Sections 02-21 of CCGbank as training data.", "labels": [], "entities": [{"text": "model estimation", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.6167021691799164}, {"text": "CCGbank", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.8162462115287781}]}, {"text": "The results for parsing accuracy were obtained using Section 00 as development data and Section 23 as the final test data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.9887616038322449}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9678117632865906}]}, {"text": "The results for parsing speed were obtained using Section 23.", "labels": [], "entities": [{"text": "parsing", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.985584557056427}, {"text": "speed", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.6319701075553894}, {"text": "Section", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9271827340126038}]}, {"text": "There are various hyperparameters in the parsing system, for example the frequency cutoff for features, the \u03c3 parameter in the Gaussian prior term, the \u03b2 values used in the supertagger, and soon.", "labels": [], "entities": []}, {"text": "All of these were set experimentally using Section 00 as development data.", "labels": [], "entities": [{"text": "Section 00 as development data", "start_pos": 43, "end_pos": 73, "type": "DATASET", "confidence": 0.8046615839004516}]}], "tableCaptions": [{"text": " Table 4  Supertagger ambiguity and accuracy on Section 00.", "labels": [], "entities": [{"text": "ambiguity", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.6114766001701355}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9989857077598572}, {"text": "Section 00", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.8057076632976532}]}, {"text": " Table 6  Results for dependency and normal-form models on Section 00.", "labels": [], "entities": [{"text": "Section 00", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.8911899626255035}]}, {"text": " Table 7  Results on Section 00 with both models using normal-form constraints.", "labels": [], "entities": []}, {"text": " Table 8  Results for the hybrid dependency model on Section 00 by dependency relation.", "labels": [], "entities": []}, {"text": " Table 9  Results for the hybrid dependency model on Section 23.", "labels": [], "entities": []}, {"text": " Table 10  Parse times for Section 23.", "labels": [], "entities": [{"text": "Parse", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9652169942855835}]}, {"text": " Table 11  Supertagger \u03b2 levels used on Section 00.", "labels": [], "entities": [{"text": "Supertagger \u03b2", "start_pos": 11, "end_pos": 24, "type": "METRIC", "confidence": 0.782539039850235}, {"text": "Section 00", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.7745793163776398}]}, {"text": " Table 12  Comparing parser speeds on Section 23 of the WSJ Penn Treebank.", "labels": [], "entities": [{"text": "Section 23 of the WSJ Penn Treebank", "start_pos": 38, "end_pos": 73, "type": "DATASET", "confidence": 0.8659202797072274}]}, {"text": " Table 15  Examples of the many-to-many nature of the CCG dependency to GRs mapping, and a  terniary GR.", "labels": [], "entities": []}, {"text": " Table 16  Accuracy on DepBank.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.997525155544281}, {"text": "DepBank", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.8842021822929382}]}]}