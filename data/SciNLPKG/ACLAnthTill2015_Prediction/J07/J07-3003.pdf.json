{"title": [{"text": "A Sketch Algorithm for Estimating Two-Way and Multi-Way Associations", "labels": [], "entities": []}], "abstractContent": [{"text": "We should not have to look at the entire corpus (e.g., the Web) to know if two (or more) words are strongly associated or not.", "labels": [], "entities": []}, {"text": "One can often obtain estimates of associations from a small sample.", "labels": [], "entities": []}, {"text": "We develop a sketch-based algorithm that constructs a contingency table fora sample.", "labels": [], "entities": []}, {"text": "One can estimate the contingency table for the entire population using straightforward scaling.", "labels": [], "entities": []}, {"text": "However, one can do better by taking advantage of the margins (also known as document frequencies).", "labels": [], "entities": []}, {"text": "The proposed method cuts the errors roughly in half over Broder's sketches.", "labels": [], "entities": [{"text": "errors", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9722450375556946}]}], "introductionContent": [{"text": "We develop an algorithm for efficiently computing associations, for example, word associations.", "labels": [], "entities": []}, {"text": "Word associations (co-occurrences, or joint frequencies) have a wide range of applications including: speech recognition, optical character recognition, and information retrieval (IR).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7501482665538788}, {"text": "optical character recognition", "start_pos": 122, "end_pos": 151, "type": "TASK", "confidence": 0.726458470026652}, {"text": "information retrieval (IR)", "start_pos": 157, "end_pos": 183, "type": "TASK", "confidence": 0.8319610476493835}]}, {"text": "The Know-It-All project computes such associations at Web scale ().", "labels": [], "entities": []}, {"text": "It is easy to compute a few association scores fora small corpus, but more challenging to compute lots of scores for lots of data (e.g., the Web), with billions of Web pages (D) and millions of word types.", "labels": [], "entities": []}, {"text": "Web search engines produce estimates of page hits, as illustrated in shows hits for two high frequency words, a and the, suggesting that the total number of English documents is roughly D \u2248 10 10 . In addition to the two highfrequency words, there are three low-frequency words selected from The New Oxford Dictionary of English).", "labels": [], "entities": [{"text": "The New Oxford Dictionary of English", "start_pos": 292, "end_pos": 328, "type": "DATASET", "confidence": 0.9581924776236216}]}, {"text": "The low-frequency words demonstrate that there are many hits, even for relatively rare words.", "labels": [], "entities": []}, {"text": "How many page hits do \"ordinary\" words have?", "labels": [], "entities": []}, {"text": "To address this question, we randomly picked 15 pages from a learners' dictionary, and selected the first entry on each page.", "labels": [], "entities": []}, {"text": "According to Google, there are 10 million pages/word (median value, aggregated over the 15 words).", "labels": [], "entities": []}, {"text": "To compute all two-way associations for the 57,100 entries in this dictionary would probably be infeasible, let alone all multi-way associations.", "labels": [], "entities": []}, {"text": "Sampling can make it possible to work in physical memory, avoiding disk accesses.", "labels": [], "entities": [{"text": "Sampling", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9366686940193176}]}, {"text": "reported an inverted index of 37.2 GBs for 24 million pages.", "labels": [], "entities": [{"text": "inverted index", "start_pos": 12, "end_pos": 26, "type": "METRIC", "confidence": 0.9688025712966919}]}, {"text": "By extrapolation, we should expect the size of the inverted indexes for current Web scale to be 1.5 TBs/billion pages, probably too large for physical memory.", "labels": [], "entities": []}, {"text": "A sample is more manageable.", "labels": [], "entities": []}, {"text": "When estimating associations, it is desirable that the estimates be consistent.", "labels": [], "entities": []}, {"text": "Joint frequencies ought to decrease monotonically as we add terms to the query.", "labels": [], "entities": []}, {"text": "shows that estimates produced by current search engines are not always consistent.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our two-way association sampling/estimation algorithm with a chunk of Web crawls (D = 2 16 ) produced by the crawler for MSN.com.", "labels": [], "entities": [{"text": "association sampling/estimation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.784321129322052}, {"text": "D", "start_pos": 95, "end_pos": 96, "type": "METRIC", "confidence": 0.973362147808075}, {"text": "MSN.com", "start_pos": 134, "end_pos": 141, "type": "DATASET", "confidence": 0.939241886138916}]}, {"text": "We collected two sets of English words which we will refer to as the small data set and the large data set.", "labels": [], "entities": []}, {"text": "The small data set contains just four high frequency words: THIS, HAVE, HELP and PROGRAM (see), whereas the large data set contains 968 words (i.e., 468,028 pairs).", "labels": [], "entities": [{"text": "THIS", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9903400540351868}, {"text": "HAVE", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9812705516815186}, {"text": "HELP", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9900264739990234}, {"text": "PROGRAM", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9249078631401062}]}, {"text": "The large data set was constructed by taking a random sample of English words that appeared in at least 20 documents in the collection.", "labels": [], "entities": []}, {"text": "The histograms of the margins and co-occurrences have long tails, as expected (see).", "labels": [], "entities": []}, {"text": "For the small data set, we applied 10 5 independent random permutations to the D = 2 16 document IDs, \u2126 = {1, 2, . .", "labels": [], "entities": []}, {"text": "High-frequency words were selected so we could study a large range of sampling rates ( k f ), from 0.002 to 0.95.", "labels": [], "entities": []}, {"text": "A pair of sketches was constructed for each of the 6 pairs of words in, each of the 10 5 permutations and each sampling rate.", "labels": [], "entities": []}, {"text": "The sketches were then used to compute a sample contingency table, leading to an estimate of co-occurrence, \u02c6 a.", "labels": [], "entities": []}, {"text": "An error was computed by comparing this estimate, \u02c6 a, to the appropriate gold standard value fora in.", "labels": [], "entities": []}, {"text": "Mean square errors (MSE = E(\u02c6 a \u2212 a) 2 ) and other statistics were computed by aggregating over the 10 5 Small dataset: co-occurrences and margins for the population.", "labels": [], "entities": [{"text": "Mean square errors", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.9565689166386923}, {"text": "MSE = E", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.7488133311271667}]}, {"text": "The task is to estimate these values, which will be referred to as the gold standard, from a sample.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.8257165849208832}]}, {"text": "In this way, the small data set experiment made it possible to verify our theoretical results, including the approximations in the variance formulas.", "labels": [], "entities": []}, {"text": "The larger experiment contains many words with a large range of frequencies; and hence the experiment was repeated just six times (i.e., six different permutations).", "labels": [], "entities": []}, {"text": "With such a large range of frequencies and sampling rates, there is a danger that some samples would be too small, especially for very rare words and very low sampling rates.", "labels": [], "entities": []}, {"text": "A floor was imposed to make sure that every sample contains at least 20 documents.", "labels": [], "entities": []}, {"text": "shows that the proposed methods (solid lines) are better than the baselines (dashed lines), in terms of MSE, estimated by the large Monte Carlo experiment over the small data set, as described herein.", "labels": [], "entities": [{"text": "MSE", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.8845577836036682}]}, {"text": "Note that errors generally decrease with sampling rate, as one would expect, at least for the methods that take advantage of the sample.", "labels": [], "entities": []}, {"text": "The independence baseline (\u02c6 a IND ), which does not take advantage of the sample, has very large errors.", "labels": [], "entities": [{"text": "independence baseline (\u02c6 a IND )", "start_pos": 4, "end_pos": 36, "type": "METRIC", "confidence": 0.7264882070677621}]}, {"text": "The sample is a very useful source of information; even a small sample is much better than no sample.", "labels": [], "entities": []}, {"text": "The recommended quadratic approximation, \u02c6 a MLE,a , is remarkably close to the exact MLE solution.", "labels": [], "entities": []}, {"text": "Both of the proposed methods, \u02c6 a MLE,a and\u00e2and\u02c6and\u00e2 MLE (solid lines), have much smaller MSE than the margin-free baselin\u00ea a MF (dashed lines), especially at low sampling rates.", "labels": [], "entities": [{"text": "MSE", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9814438819885254}]}, {"text": "When we know the margins, we ought to use them.", "labels": [], "entities": []}, {"text": "Note that MSE can be decomposed into variance and bias: MSE(\u02c6 a) = E (\u02c6 a \u2212 a) 2 = Var (\u02c6 a) +Bias 2 (\u02c6 a).", "labels": [], "entities": [{"text": "variance", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9544848203659058}, {"text": "Var", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9042418003082275}, {"text": "Bias", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.689252495765686}]}, {"text": "If\u00e2If\u02c6If\u00e2 is unbiased, MSE(\u02c6 a) = Var (\u02c6 a) = SE 2 (\u02c6 a), where SE is called \"standard error.\"", "labels": [], "entities": [{"text": "Var", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9217140674591064}, {"text": "SE 2 (\u02c6 a)", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9334659079710642}, {"text": "SE", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9941638112068176}]}, {"text": "In, the large data set experiment confirms the findings of the large Monte Carlo experiment: The proposed MLE method is better than the margin-free and independence baselines.", "labels": [], "entities": [{"text": "MLE", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.5846998691558838}]}, {"text": "The recommended quadratic approximation, \u02c6 a MLE,a , is close to the exact solution, \u02c6 a MLE .  We use the same four words as in to evaluate the multi-way association algorithm, as merely a sanity check.", "labels": [], "entities": []}, {"text": "There are four different combinations of three-way associations and one four-way association, as listed in.", "labels": [], "entities": []}, {"text": "We present results for x 1 (i.e., a in two-way associations) for all cases.", "labels": [], "entities": []}, {"text": "The evaluations for four three-way cases are presented in.", "labels": [], "entities": []}, {"text": "From these figures, we see that the proposed MLE has lower MSE than the MF.", "labels": [], "entities": [{"text": "MLE", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.538956880569458}, {"text": "MSE", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9968395233154297}, {"text": "MF", "start_pos": 72, "end_pos": 74, "type": "DATASET", "confidence": 0.7683255076408386}]}, {"text": "As in the two-way case, smoothing helps MLE but still hurts MF inmost cases.", "labels": [], "entities": [{"text": "MLE", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9599380493164062}, {"text": "MF", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.8391674160957336}]}, {"text": "Also, the experiments verify that our approximate variance formulas are fairly accurate.", "labels": [], "entities": []}, {"text": "presents the evaluation results for the four-way association case, including MSE, smoothing, and variance.", "labels": [], "entities": [{"text": "MSE", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.4621060788631439}]}, {"text": "The results are similar to the three-way case.", "labels": [], "entities": []}, {"text": "The same four words as in are used for evaluating multi-way associations.", "labels": [], "entities": []}, {"text": "There are in total four three-way combinations and one four-way combination.", "labels": [], "entities": []}, {"text": "Combining the results of two-way associations for the same four words, we can study the trend how the proposed MLE improve the MF baseline.(a) sug-  We have theoretically shown that our proposed method is a considerable improvement over Broder's sketch.", "labels": [], "entities": []}, {"text": "Next, we would like to evaluate these theoretical results using the same experiment data as in evaluating two-way associations (i.e.,).", "labels": [], "entities": []}, {"text": "Here we assume equal samples and later we will show that proportional samples could further improve the results.", "labels": [], "entities": []}, {"text": "The figure shows that our MLE estimator is consistently better than Broder's sketch.", "labels": [], "entities": [{"text": "MLE estimator", "start_pos": 26, "end_pos": 39, "type": "METRIC", "confidence": 0.8317663967609406}]}, {"text": "In addition, the approximate MLE\u00e2MLE\u02c6MLE\u00e2 MLE,a still gives very close answers to the exact MLE, and the simple \"add-one\" smoothing improves the estimations at low sampling rates, quite substantially.", "labels": [], "entities": [{"text": "MLE,a", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.8605461120605469}, {"text": "MLE", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.8836488723754883}]}, {"text": "As expected, estimating resemblance from\u00e2from\u02c6from\u00e2 MLE introduces a small bias.", "labels": [], "entities": [{"text": "MLE", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.7896909117698669}]}, {"text": "This bias will be ignored since it is small compared to the MSE.", "labels": [], "entities": [{"text": "MSE", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.8875373601913452}]}, {"text": "verifies that the variance of our estimator is always smaller than Broder's sketch.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4  Small dataset: co-occurrences and margins for the population. The task is to estimate these  values, which will be referred to as the gold standard, from a sample.", "labels": [], "entities": []}]}