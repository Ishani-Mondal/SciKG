{"title": [], "abstractContent": [{"text": "Default unification operations combine strict information with information from one or more defeasible feature structures.", "labels": [], "entities": [{"text": "Default unification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7255085706710815}]}, {"text": "Many such operations require finding the maximal subsets of a set of atomic constraints that are consistent with each other and with the strict feature structure, where a subset is maximally consistent with respect to the subsumption ordering if no constraint can be added to it without creating an inconsistency.", "labels": [], "entities": []}, {"text": "Although this problem is NP-complete, there area number of heuristic optimizations that can be used to substantially reduce the size of the search space.", "labels": [], "entities": []}, {"text": "In this article, we propose a novel optimization, leaf pruning, which in some cases yields an improvement in running time of several orders of magnitude over previously described algorithms.", "labels": [], "entities": []}, {"text": "This makes default unification efficient enough to be practical fora wide range of problems and applications.", "labels": [], "entities": [{"text": "default unification", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7607176899909973}]}], "introductionContent": [{"text": "In unification-based grammatical frameworks, it often desirable to combine information from possibly inconsistent sources.", "labels": [], "entities": [{"text": "unification-based grammatical frameworks", "start_pos": 3, "end_pos": 43, "type": "TASK", "confidence": 0.8571045200030009}]}, {"text": "Over the years, a number of default unification operations have been proposed 1 , which combine a strict feature structure with one or more defeasible feature structures.", "labels": [], "entities": []}, {"text": "These operations preserve all information in the strict feature structure, while bringing in as much information as possible from the defeasible structures.", "labels": [], "entities": []}, {"text": "Default unification has been used to address a wide variety of linguistic knowledge representation problems, including lexical inheritance hierarchies, lexical semantics (, grammar induction), anaphora resolution (, and discourse processing (), among many others.", "labels": [], "entities": [{"text": "Default unification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8163501620292664}, {"text": "grammar induction", "start_pos": 173, "end_pos": 190, "type": "TASK", "confidence": 0.7167667299509048}, {"text": "anaphora resolution", "start_pos": 193, "end_pos": 212, "type": "TASK", "confidence": 0.7188817411661148}, {"text": "discourse processing", "start_pos": 220, "end_pos": 240, "type": "TASK", "confidence": 0.6955066025257111}]}, {"text": "Although the various default unification operators differ in their particulars, most involve something like's credulous default unification as one step.", "labels": [], "entities": []}, {"text": "The result of credulously adding the default information in G to the strict information in F is:,,,,,.", "labels": [], "entities": []}, {"text": "See Bouma (2006) fora recent overview.", "labels": [], "entities": [{"text": "Bouma (2006)", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.6793536245822906}]}, {"text": "In other words, cred-unify(F, G) is the result of unifying F with the maximal consistent subset(s) of the atomic constraints in G.", "labels": [], "entities": []}, {"text": "A subset of constraints is maximally consistent with respect to the subsumption ordering if no constraint can be added to it without creating an inconsistency.", "labels": [], "entities": []}, {"text": "In general, there maybe more than one subset of the constraints in G that is consistent with F and maximal, so the result of credulous default unification is a set of feature structures.", "labels": [], "entities": []}, {"text": "One example of the use of credulous default unification for the resolution of discourse anaphora comes from.", "labels": [], "entities": [{"text": "resolution of discourse anaphora", "start_pos": 64, "end_pos": 96, "type": "TASK", "confidence": 0.8169325441122055}]}, {"text": "Consider the mini-discourse: Jessy likes her brother.", "labels": [], "entities": []}, {"text": "To resolve the anaphoric predicate in the second sentence, we can setup meaning representations for the source and the target: and credulously unify the source with the strict information in the target.", "labels": [], "entities": []}, {"text": "To proceed, we first decompose the source feature structure into the following five atomic ground constraints:", "labels": [], "entities": []}], "datasetContent": [{"text": "The graphs in and show an empirical comparison between a breadthfirst search with root pruning (BFS-R) and a breadth-first search with root and leaf pruning (BFS-RL) on randomly generated problems.", "labels": [], "entities": []}, {"text": "The graphs show the number of subsets that were checked for consistency, as it relates to |C|, the number of constraints, and p, the probability that two members of C are consistent.", "labels": [], "entities": [{"text": "consistency", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.969475269317627}]}, {"text": "Larger values for p generally lead to fewer but larger maximal consistent subsets.", "labels": [], "entities": []}, {"text": "All counts are averaged across 100 randomly generated sets of ground constraints.", "labels": [], "entities": []}, {"text": "In generating random problems, we make the simplifying assumptions that all constraints are consistent with any indefeasible information, and that a subset of constraints that are pairwise consistent is a consistent subset.", "labels": [], "entities": []}, {"text": "The first thing to note in these graphs is that root pruning by itself provides very little benefit.", "labels": [], "entities": []}, {"text": "For most values of p, the number of subsets checked by BFS-R is very close to the worst case maximum 2 |C| . A possible reason for this is that root pruning  Comparison of breadth-first search using root pruning alone (BFS-R) and in combination with leaf pruning (BFS-RL).", "labels": [], "entities": [{"text": "BFS-R", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.732968270778656}, {"text": "BFS-RL", "start_pos": 264, "end_pos": 270, "type": "DATASET", "confidence": 0.8199936747550964}]}, {"text": "|C| is the number of ground constraints, p is the fraction of the ground constraints which are pairwise consistent, and \"Subsets visited\" is the number of subsets of C which were checked for consistency (on a logarithmic scale).", "labels": [], "entities": []}, {"text": "All counts are based on the average of 100 randomly generated problems.", "labels": [], "entities": []}, {"text": "will have the greatest effect when consistent subsets are found in the interior nodes of the binomial search tree.", "labels": [], "entities": []}, {"text": "However, the configuration of the search space is such that most nodes are either leaves or very close to leaves, and only a few nodes have a large number of descendants.", "labels": [], "entities": []}, {"text": "Therefore, root pruning mostly removes very small subtrees and has only a small effect on the overall cost of the algorithm.", "labels": [], "entities": []}, {"text": "The next observation to make is that for small values of |C| (in these experiments, less than 7), BFS-RL is very slightly more expensive than BFS-R.", "labels": [], "entities": [{"text": "BFS-RL", "start_pos": 98, "end_pos": 104, "type": "DATASET", "confidence": 0.5807290077209473}, {"text": "BFS-R", "start_pos": 142, "end_pos": 147, "type": "DATASET", "confidence": 0.8657031655311584}]}, {"text": "In these cases, the advantages of leaf pruning do not outweigh the cost of the extra consistency checks required to implement it.", "labels": [], "entities": []}, {"text": "As |C| increases, though, leaf pruning can offer substantial improvements.", "labels": [], "entities": []}, {"text": "For |C| = 19 and p = 0.1, leaf pruning eliminates more than 99.5% of the search space, leading to a 185-fold improvement in running time.", "labels": [], "entities": []}, {"text": "As p increases, the benefits of leaf pruning do become more modest.", "labels": [], "entities": []}, {"text": "Larger values of p mean fewer inconsistent leaf nodes, so fewer subtrees are able to be eliminated.", "labels": [], "entities": []}, {"text": "Even so, the savings from leaf pruning can still be dramatic: at |C| = 19 and p = 0.9, leaf pruning yields a nearly five-fold improvement in speed.", "labels": [], "entities": []}, {"text": "Values of |C| and p that can be realistically expected will vary widely from application to application.", "labels": [], "entities": []}, {"text": "An anonymous reviewer reports that in one application, the resolution of non-monotonic lexical inheritance for constraint-based grammars, p is generally greater than 0.7.", "labels": [], "entities": [{"text": "resolution", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9707128405570984}]}, {"text": "This maybe due in part to the fact that most constraintbased grammar development platforms do not support defaults LKB is a notable exception), and so grammar engineers tend to avoid the use of default overriding.", "labels": [], "entities": []}, {"text": "propose a more comprehensive use of defaults, and grammars written following these principles would likely have a much lower value of p.", "labels": [], "entities": []}, {"text": "To my knowledge, however, these ideas have not yet made their way into any largescale grammar implementations.", "labels": [], "entities": []}, {"text": "Ninomiya, Miyao, and Tsujii (2002) describe experiments using default unification for robust parsing and automatic grammar augmentation via a kind of explanationbased learning.", "labels": [], "entities": []}, {"text": "For this application, all features of a rule in the base XHPSG grammar () are considered defaults that can be overridden if necessary to get a successful parse of a sentence.", "labels": [], "entities": []}, {"text": "In this case, |C| is likely very large and grows quickly with the length of the sentences being parsed.", "labels": [], "entities": []}, {"text": "The value of p will depend on the coverage of the base grammar, but can be expected to be fairly close to 1.0 for most domains.", "labels": [], "entities": []}, {"text": "In situations such as this, where p is expected to fall close to the worst case for leaf pruning, one could consider inverting the search direction of the algorithm in.", "labels": [], "entities": []}, {"text": "Rather than beginning with C and removing constraints until a consistent subset is found, we could instead begin with the empty subset and add constraints until an inconsistency is found.", "labels": [], "entities": []}, {"text": "In either case, the frontier in the search space between consistent and inconsistent subsets is where maximally consistent subsets will be found, and leaf pruning can be used to eliminate regions of the search space that contain only consistent or inconsistent subsets.", "labels": [], "entities": []}], "tableCaptions": []}