{"title": [{"text": "Using Suffix Arrays to Compute Term Frequency and Document Frequency for All Substrings in a Corpus", "labels": [], "entities": []}], "abstractContent": [{"text": "Bigrams and trigrams are commonly used in statistical natural language processing; this paper will describe techniques for working with much longer n-grams.", "labels": [], "entities": [{"text": "statistical natural language processing", "start_pos": 42, "end_pos": 81, "type": "TASK", "confidence": 0.687379889190197}]}, {"text": "Suffix arrays (Manber and My-ers 1990) were /irst introduced to compute the frequency and location of a substring (n-gram) in a sequence (corpus) of length N.", "labels": [], "entities": []}, {"text": "To compute frequencies overall N(N + 1)/2 substrings in a corpus, the substrings are grouped into a manageable number of equivalence classes.", "labels": [], "entities": []}, {"text": "In this way, a prohibitive computation over substrings is reduced to a manageable computation over classes.", "labels": [], "entities": []}, {"text": "This paper presents both the algorithms and the code that were used to compute term frequency (tf) and document frequency (dr)for all n-grams in two large corpora, an English corpus of 50 million words of Wall Street Journal and a Japanese corpus of 216 million characters of Mainichi Shimbun.", "labels": [], "entities": [{"text": "term frequency (tf)", "start_pos": 79, "end_pos": 98, "type": "METRIC", "confidence": 0.8567925453186035}, {"text": "document frequency (dr)", "start_pos": 103, "end_pos": 126, "type": "METRIC", "confidence": 0.8474575877189636}, {"text": "Wall Street Journal", "start_pos": 205, "end_pos": 224, "type": "DATASET", "confidence": 0.926034947236379}, {"text": "Mainichi Shimbun", "start_pos": 276, "end_pos": 292, "type": "DATASET", "confidence": 0.9256387948989868}]}, {"text": "The second half of the paper uses these frequencies to find \"interesting\" substrings.", "labels": [], "entities": []}, {"text": "Lexicographers have been interested in n-grams with high mutual information (MI) where the joint term frequency is higher than what would be expected by chance, assuming that the parts of the n-gram combine independently.", "labels": [], "entities": [{"text": "mutual information (MI", "start_pos": 57, "end_pos": 79, "type": "METRIC", "confidence": 0.7333352193236351}, {"text": "joint term frequency", "start_pos": 91, "end_pos": 111, "type": "METRIC", "confidence": 0.7420828739802042}]}, {"text": "Residual inverse document frequency (RIDF) compares document frequency to another model of chance where terms with a particular term frequency are distributed randomly throughout the collection.", "labels": [], "entities": [{"text": "inverse document frequency (RIDF)", "start_pos": 9, "end_pos": 42, "type": "METRIC", "confidence": 0.7004188944896063}]}, {"text": "MI tends to pick out phrases with noncompo-sitional semantics (which often violate the independence assumption) whereas RIDF tends to highlight technical terminology, names, and good keywords for information retrieval (which tend to exhibit nonrandom distributions over documents).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 196, "end_pos": 217, "type": "TASK", "confidence": 0.7413480281829834}]}, {"text": "The combination of both MI and RIDF is better than either by itself in a Japanese word extraction task.", "labels": [], "entities": [{"text": "MI", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.8721613883972168}, {"text": "RIDF", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.938961923122406}, {"text": "Japanese word extraction task", "start_pos": 73, "end_pos": 102, "type": "TASK", "confidence": 0.6660175025463104}]}], "introductionContent": [{"text": "We will use suffix arrays to compute a number of type/token statistics of interest, including term frequency and document frequency, for all n-grams in large corpora.", "labels": [], "entities": []}, {"text": "Type/token statistics model the corpus as a sequence of N tokens (characters, words, terms, n-grams, etc.) drawn from a vocabulary of V types.", "labels": [], "entities": []}, {"text": "Different tokenizing rules will be used for different corpora and for different applications.", "labels": [], "entities": []}, {"text": "In this work, the English text is tokenized into a sequence of English words delimited by white space and the Japanese text is tokenized into a sequence of Japanese characters (typically one or two bytes each).", "labels": [], "entities": []}, {"text": "Term frequency (tf) is the standard notion of frequency in corpus-based natural language processing (NLP); it counts the number of times that a type (term/word/ngram) appears in a corpus.", "labels": [], "entities": [{"text": "Term frequency (tf)", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.8793168306350708}]}, {"text": "Document frequency (df) is borrowed for the information retrieval literature; it counts the number of documents that contain a type at least once.", "labels": [], "entities": [{"text": "Document frequency (df)", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.7570277214050293}, {"text": "information retrieval", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.742702066898346}]}, {"text": "Term frequency is an integer between 0 and N; document frequency is an integer between 0 and D, the number of documents in the corpus.", "labels": [], "entities": [{"text": "Term frequency", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8381789028644562}]}, {"text": "The statistics, tf and df, and functions of these statistics such as mutual information (MI) and inverse document frequency (IDF), are usually computed over short n-grams such as unigrams, bigrams, and trigrams (substrings of 1-3 tokens).", "labels": [], "entities": [{"text": "mutual information (MI)", "start_pos": 69, "end_pos": 92, "type": "METRIC", "confidence": 0.7404945850372314}, {"text": "inverse document frequency (IDF)", "start_pos": 97, "end_pos": 129, "type": "METRIC", "confidence": 0.8814845184485117}]}, {"text": "This paper will show how to work with much longer n-grams, including million-grams and even billion-grams.", "labels": [], "entities": []}, {"text": "In corpus-based NLP, term frequencies are often converted into probabilities, using the maximum likelihood estimator (MLE), the Good-Turing method, or Deleted Interpolation (.", "labels": [], "entities": [{"text": "maximum likelihood estimator (MLE)", "start_pos": 88, "end_pos": 122, "type": "METRIC", "confidence": 0.8354916125535965}]}, {"text": "These probabilities are used in noisy channel applications such as speech recognition to distinguish more likely sequences from less likely sequences, reducing the search space (perplexity) for the acoustic recognizer.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7778422236442566}]}, {"text": "In information retrieval, document frequencies are converted into inverse document frequency (IDF), which plays an important role in term weighting.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.7957931756973267}, {"text": "inverse document frequency (IDF)", "start_pos": 66, "end_pos": 98, "type": "METRIC", "confidence": 0.8750008742014567}, {"text": "term weighting", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.6279280483722687}]}, {"text": "dr(t) IDF(t) = -log 2 D can be interpreted as the number of bits of information the system is given if it is told that the document in question contains the term t.", "labels": [], "entities": [{"text": "IDF", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.948958158493042}]}, {"text": "Rare terms contribute more bits than common terms.", "labels": [], "entities": []}, {"text": "Mutual information (MI) and residual IDF (RIDF) (Church and Gale 1995) both compare tf and df to what would be expected by chance, using two different notions of chance.", "labels": [], "entities": [{"text": "Mutual information (MI)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6641793668270111}, {"text": "residual IDF (RIDF)", "start_pos": 28, "end_pos": 47, "type": "METRIC", "confidence": 0.9206588625907898}]}, {"text": "MI compares the term frequency of an n-gram to what would be expected if the parts combined independently, whereas RIDF combines the document frequency of a term to what would be expected if a term with a given term frequency were randomly distributed throughout the collection.", "labels": [], "entities": [{"text": "RIDF", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9344989061355591}]}, {"text": "MI tends to pick out phrases with noncompositional semantics (which often violate the independence assumption) whereas RIDF tends to highlight technical terminology, names, and good keywords for information retrieval (which tend to exhibit nonrandom distributions over documents).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 195, "end_pos": 216, "type": "TASK", "confidence": 0.73584845662117}]}, {"text": "Assuming a random distribution of a term (Poisson model), the probability p~(k) that a document will have exactly k instances of the term is:", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1  Statistics of the English and Japanese corpora.", "labels": [], "entities": [{"text": "English and Japanese corpora", "start_pos": 28, "end_pos": 56, "type": "DATASET", "confidence": 0.7479982674121857}]}]}