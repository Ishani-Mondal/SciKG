{"title": [{"text": "The Need for Accurate Alignment in Natural Language System Evaluation", "labels": [], "entities": [{"text": "Accurate Alignment", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8415287733078003}]}], "abstractContent": [{"text": "SRI International As evaluations of computational linguistics technology progress toward higher-level interpretation tasks, the problem o/determining alignments between system responses and answer key entries may become less straightforward.", "labels": [], "entities": [{"text": "SRI International", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.8972482085227966}]}, {"text": "We present an extensive analysis o/the alignment procedure used in the MUC-6 evaluation o/information extraction technology, which reveals effects that interfere with the stated goals of the evaluation.", "labels": [], "entities": [{"text": "MUC-6 evaluation o/information extraction", "start_pos": 71, "end_pos": 112, "type": "TASK", "confidence": 0.7011193633079529}]}, {"text": "These effects are shown to be pervasive enough that they have the potential to adversely impact the technology development process.", "labels": [], "entities": []}, {"text": "These results argue strongly/or the use o/accurate alignment criteria in natural language evaluations, and/or maintaining the independence o/alignment criteria and mechanisms used to calculate scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "It would be hard to overestimate the influence of evaluation on current natural language processing (NLP) research and development.", "labels": [], "entities": []}, {"text": "In contrast to the primarily qualitative methodologies that characterized research in the 1980's, purely quantitative evaluation methods now pervade all aspects of the research and development process in many areas of NLP.", "labels": [], "entities": []}, {"text": "These roles are well summarized by, who refer specifically to the methods used for the U.S.-government-sponsored Message Understanding Conference (MUC) evaluations of information extraction (IE) technology: The resulting scores [assigned by the MUC evaluation process] are used for decision-making over the entire evaluation cycle, including refinement of the task definition based on interannotator comparisons, technology development using training data, validating answer keys, and benchrnarking both system and human capabilities on the test data.", "labels": [], "entities": [{"text": "U.S.-government-sponsored Message Understanding Conference (MUC) evaluations of information extraction (IE)", "start_pos": 87, "end_pos": 194, "type": "TASK", "confidence": 0.7116133663803339}]}, {"text": "This passage highlights three major roles an evaluation method can serve.", "labels": [], "entities": []}, {"text": "First, the method maybe used during the process of task definition, to assess interannotator agreement on proposed task specifications and revise them accordingly, and to subsequently validate the final answer keys.", "labels": [], "entities": [{"text": "task definition", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.702118992805481}]}, {"text": "Second, the method maybe used to drive the system development process, as system developers and machine learning algorithms rely heavily on the feedback it provides to determine whether proposed system changes should be adopted.", "labels": [], "entities": []}, {"text": "Third, the results maybe used for final cross-system comparison, on which judgments concerning the adequacy of competing technologies are based.", "labels": [], "entities": []}, {"text": "Given their pervasiveness in the entire technology development process, the need for adequate evaluation methods is of the essence, and has thus become a prominent topic for research in itself.", "labels": [], "entities": []}, {"text": "The need to serve all these roles has necessitated the development of automated evaluation methods that are capable of being run repeatedly with little time and cost overhead.", "labels": [], "entities": []}, {"text": "Automated methods are commonly used for application tasks, including speech recognition, information retrieval, and IE, as well as for natural language component technologies, including part-of-speech tagging, syntactic annotation, and coreference resolution.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7305000275373459}, {"text": "information retrieval", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7631022930145264}, {"text": "IE", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9705314636230469}, {"text": "part-of-speech tagging", "start_pos": 186, "end_pos": 208, "type": "TASK", "confidence": 0.7232683449983597}, {"text": "coreference resolution", "start_pos": 236, "end_pos": 258, "type": "TASK", "confidence": 0.9572152495384216}]}, {"text": "Such methods generally consist of a two-step process--first, systemgenerated responses are aligned with corresponding human-generated responses encoded in an answer key, and then a predetermined scoring procedure is applied to the aligned response pairs.", "labels": [], "entities": []}, {"text": "The second of these tasks (scoring procedure) has been the focus of most previous research on evaluation.", "labels": [], "entities": [{"text": "scoring procedure)", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.6729613641897837}]}, {"text": "In this paper, we focus instead on the less well studied problem of alignment.", "labels": [], "entities": [{"text": "alignment", "start_pos": 68, "end_pos": 77, "type": "TASK", "confidence": 0.9793070554733276}]}, {"text": "The relative inattention to problems in alignment is no doubt a result of the fact that alignment is relatively unproblematic in many natural language evaluation scenarios.", "labels": [], "entities": []}, {"text": "In evaluations of component technologies such as part-of-speech tagging and treebank-style syntactic annotation systems, for instance, a system-generated annotation is simply scored against the human-generated annotation for the same word or sentence, regardless of whether matching it to the annotation fora different word or sentence would improve the overall score assigned.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7144953459501266}]}, {"text": "Alignment is similarly trivial in evaluations of applications such as information retrieval, since the notion of document identity is well defined.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9652515053749084}, {"text": "information retrieval", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.7543153166770935}]}, {"text": "Alignment in speech recognition evaluations can be a bit more complex, but constraints inherent in the methods nonetheless prohibit clear misalignments, such that a system cannot receive credit for recognizing a word from an acoustic signal that occurred several utterances later, for instance.", "labels": [], "entities": [{"text": "Alignment in speech recognition evaluations", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7259076952934265}]}, {"text": "As the field progresses to address higher-level interpretation tasks, however, the problem of determining alignments may become less straightforward.", "labels": [], "entities": []}, {"text": "Such tasks may require that the output contain information that is synthesized (and perhaps even inferred) from disparate parts of the input signal, making the correspondence between information in the system output and information in the answer key more difficult to recover.", "labels": [], "entities": []}, {"text": "A casein point is the evaluation IE technology, as most prominently carried out by the series of MUCs.", "labels": [], "entities": [{"text": "IE", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.7194674015045166}, {"text": "MUCs", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.8495061993598938}]}, {"text": "For atypical MUC-style IE task, a text may contain several extractable events, and thus a method is required for aligning the (often only partially correct) event descriptions extracted by a system to the appropriate ones in the answer key.", "labels": [], "entities": [{"text": "MUC-style IE task", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7879064281781515}]}, {"text": "It is therefore important to investigate the issues involved in the definition of alignment criteria in such tasks, and we can use the MUC experience as a basis for such an investigation.", "labels": [], "entities": [{"text": "definition of alignment criteria", "start_pos": 68, "end_pos": 100, "type": "TASK", "confidence": 0.7819820046424866}, {"text": "MUC experience", "start_pos": 135, "end_pos": 149, "type": "DATASET", "confidence": 0.8989339768886566}]}, {"text": "In this paper, we focus specifically on the criterion used for alignment in MUC-6.", "labels": [], "entities": [{"text": "alignment", "start_pos": 63, "end_pos": 72, "type": "TASK", "confidence": 0.9690791368484497}, {"text": "MUC-6", "start_pos": 76, "end_pos": 81, "type": "TASK", "confidence": 0.5187624096870422}]}, {"text": "In light of difficulties in identifying a perfect alignment criterion for the MUC-6 task, the MUC-6 community agreed upon on a rather weak and forgiving criterion, leaving the resolution of alignment ambiguities to a mechanism that sought to maximize the score assigned.", "labels": [], "entities": [{"text": "MUC-6 task", "start_pos": 78, "end_pos": 88, "type": "TASK", "confidence": 0.5783744156360626}, {"text": "MUC-6 community", "start_pos": 94, "end_pos": 109, "type": "DATASET", "confidence": 0.9079908132553101}]}, {"text": "While this decision may have been thought to be relatively benign, we report on the results of an extensive, post hoc analysis of a 13 1/2-month effort focused on the MUC-6 task which reveals several unforeseen and negative consequences associated with this decision, particularly with respect to its influence on the incremental system development process.", "labels": [], "entities": [{"text": "MUC-6 task", "start_pos": 167, "end_pos": 177, "type": "TASK", "confidence": 0.6164210140705109}]}, {"text": "While we do not argue that these consequences are so severe that they call the integrity of the MUC-6 evaluation into question, they are substantial enough that they demonstrate the potential of such an alignment strategy to have a significantly adverse impact on the goals of an evaluation.", "labels": [], "entities": [{"text": "MUC-6 evaluation", "start_pos": 96, "end_pos": 112, "type": "DATASET", "confidence": 0.8851317465305328}]}, {"text": "It is therefore important that these lessons be brought to bear in the design of future evaluations for IE and other high-level language processing tasks.", "labels": [], "entities": [{"text": "IE", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.9806822538375854}]}, {"text": "We begin with an overview of IE tasks, systems, and evaluation, including the alignment procedure used for MUC-6.", "labels": [], "entities": [{"text": "IE tasks", "start_pos": 29, "end_pos": 37, "type": "TASK", "confidence": 0.9202533662319183}, {"text": "MUC-6", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.6428210735321045}]}, {"text": "We then provide an example from the MUC-6 development corpus that illustrates properties of the alignment process that interfere with the stated goals of the evaluation.", "labels": [], "entities": [{"text": "MUC-6 development corpus", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.9019729892412821}]}, {"text": "We assess the pervasiveness of these problems based on data compiled from an extended development effort centered on the MUC-6 task, and conclude that the effect of the alignment criterion is robust enough that it could potentially undermine the technology development process.", "labels": [], "entities": [{"text": "MUC-6 task", "start_pos": 121, "end_pos": 131, "type": "TASK", "confidence": 0.6612794399261475}]}, {"text": "We conclude that these results argue strongly for the use of strict and accurate alignment criteria in future natural language evaluations---evaluations in which alignment problems will become exacerbated as the natural language applications addressed become more complex--and for maintaining the independence of alignment criteria and the mechanisms used to calculate scores.", "labels": [], "entities": []}], "datasetContent": [{"text": "The rightmost column of shows hypothetical output of an IE system.", "labels": [], "entities": [{"text": "IE", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9101803302764893}]}, {"text": "The first step of the evaluation algorithm, alignment, determines which templates in the system's output correspond to which ones in the key.", "labels": [], "entities": []}, {"text": "Generally speaking, there can be any number of templates in the key and system response fora given document; all, some, or no pairs of which maybe descriptions of the same event.", "labels": [], "entities": []}, {"text": "The alignment algorithm must thus determine the correct template pairing.", "labels": [], "entities": []}, {"text": "This process is not necessarily straightforward, since there maybe no slot in a template that uniquely identifies the event or object which it describes.", "labels": [], "entities": []}, {"text": "In response to this problem, the MUC community decided to adopt a relatively lax alignment criterion, leaving it to the alignment algorithm to find the alignment that optimizes the resulting score.", "labels": [], "entities": [{"text": "MUC community", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.7957754135131836}]}, {"text": "The procedure has two major steps.", "labels": [], "entities": []}, {"text": "First, it determines which pairs of templates are possible candidates for alignment; the criterion for candidacy was only that the templates share a common value for at least one slot.", "labels": [], "entities": [{"text": "alignment", "start_pos": 74, "end_pos": 83, "type": "TASK", "confidence": 0.9775106310844421}]}, {"text": "(Pointer fills share a common value if they point to objects that are aligned by the algorithm.)", "labels": [], "entities": []}, {"text": "This criterion often results in alignment ambiguities--a key template will often share a common slot value with several templates in the system's output, and vice versa--and thus a method for selecting among the alternative mappings is necessary.", "labels": [], "entities": []}, {"text": "The candidate pairs are rank ordered by a mapping score, which simply counts the number of slot values the templates have in common.", "labels": [], "entities": []}, {"text": "1 The scoring algorithm then considers key and response template pairs according to this order, aligning them when neither member has already been mapped to another template.", "labels": [], "entities": []}, {"text": "Ties between pairs with the same number of common slot values are broken arbitrarily.", "labels": [], "entities": []}, {"text": "Because this algorithm is heuristic--with many combinations of alignments never being considered--the result may not be the globally optimal alignment in terms of score assigned.", "labels": [], "entities": []}, {"text": "In our example, there is only one template of each type in each response, and thus there are no mapping ambiguities.", "labels": [], "entities": []}, {"text": "The only requirement is that each pair share a common slot value, which is the case, and so the algorithm aligns each as shown in.", "labels": [], "entities": []}, {"text": "Once the templates are aligned, the scoring algorithm performs slot-by-slot comparisons to determine errors.", "labels": [], "entities": []}, {"text": "The leftmost column in shows examples of the three types of errors that the algorithm will mark.", "labels": [], "entities": []}, {"text": "First, while our hypothetical system recognized that the correct PERSON and ORGANIZATION are Julian Mounter and Star TV, respectively, it missed the ORG_ALIAS, ORG_DESCRIPTOR, PER_ALIAS, and PER_TITLE values that appear later in the passage, resulting in four missing slot fills (denoted by mis in the left hand column).", "labels": [], "entities": [{"text": "PERSON", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.947850227355957}, {"text": "ORGANIZATION", "start_pos": 76, "end_pos": 88, "type": "METRIC", "confidence": 0.9673581719398499}, {"text": "Julian Mounter", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.9017867743968964}, {"text": "Star TV", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.8078959286212921}, {"text": "ORG_ALIAS", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9044336080551147}, {"text": "ORG_DESCRIPTOR", "start_pos": 160, "end_pos": 174, "type": "METRIC", "confidence": 0.8641536235809326}, {"text": "PER_ALIAS", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.8606598377227783}, {"text": "PER_TITLE", "start_pos": 191, "end_pos": 200, "type": "METRIC", "confidence": 0.8727024992307028}]}, {"text": "Next, it also erroneously assigned a value to the ORG_LOCALE and ORG_COUNTRY slots in the ORGANIZATION, resulting in two spurious slot fills (denoted by spu).", "labels": [], "entities": [{"text": "ORG_LOCALE", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.8495996991793314}, {"text": "ORG_COUNTRY", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.6655261119206747}]}, {"text": "Finally, the system got three of the set fill slots wrong--the VACANCY_REASON slot in the SUCCESSION_EVENT, and the NEW_STATUS and ON_THE_JOB slots of the IN_AND_OuT template--resulting in three incorrect slot fills (denoted by inc).", "labels": [], "entities": [{"text": "VACANCY_REASON slot", "start_pos": 63, "end_pos": 82, "type": "METRIC", "confidence": 0.7711310088634491}]}, {"text": "The remainder of the slot fills are correct (denoted by cor).", "labels": [], "entities": []}, {"text": "2 Again, pointer slots are scored as correct when they point to templates that have been aligned.", "labels": [], "entities": []}, {"text": "The possible fills are those in the key which contribute to the final score, and the actual fills are those in the system's response which contribute to the final score:", "labels": [], "entities": []}], "tableCaptions": []}