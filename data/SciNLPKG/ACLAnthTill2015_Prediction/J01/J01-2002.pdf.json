{"title": [{"text": "Improving Accuracy in Word Class Tagging through the Combination of Machine Learning Systems", "labels": [], "entities": [{"text": "Improving Accuracy in Word Class Tagging", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6598959714174271}]}], "abstractContent": [{"text": "We examine how differences in language models, learned by different data-driven systems performing the same NLP task, can be exploited to yield a higher accuracy than the best individual system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9962384700775146}]}, {"text": "We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora.", "labels": [], "entities": [{"text": "word class tagging", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.6506684223810831}]}, {"text": "Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data.", "labels": [], "entities": []}, {"text": "After comparison, their outputs are combined using several voting strategies and second-stage classifiers.", "labels": [], "entities": []}, {"text": "All combination taggers outperform their best component.", "labels": [], "entities": [{"text": "combination taggers", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7033877074718475}]}, {"text": "The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus.", "labels": [], "entities": [{"text": "error rate", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9582089781761169}, {"text": "LOB corpus", "start_pos": 103, "end_pos": 113, "type": "DATASET", "confidence": 0.9606424272060394}]}], "introductionContent": [{"text": "In all natural language processing (NLP) systems, we find one or more language models that are used to predict, classify, or interpret language-related observations.", "labels": [], "entities": []}, {"text": "Because most real-world NLP tasks require something that approaches full language understanding in order to be perfect, but automatic systems only have access to limited (and often superficial) information, as well as limited resources for reasoning with that information, such language models tend to make errors when the system is tested on new material.", "labels": [], "entities": []}, {"text": "The engineering task in NLP is to design systems that make as few errors as possible with as little effort as possible.", "labels": [], "entities": []}, {"text": "Common ways to reduce the error rate are to devise better representations of the problem, to spend more time on encoding language knowledge (in the case of hand-crafted systems), or to find more training data (in the case of data-driven systems).", "labels": [], "entities": [{"text": "error rate", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9476819038391113}]}, {"text": "However, given limited resources, these options are not always available.", "labels": [], "entities": []}, {"text": "Rather than devising anew representation for our task, in this paper, we combine different systems employing known representations.", "labels": [], "entities": []}, {"text": "The observation that suggests this approach is that systems that are designed differently, either because they use a different formalism or because they contain different knowledge, will typically produce different errors.", "labels": [], "entities": []}, {"text": "We hope to make use of this fact and reduce the number of errors with tagging experiments before moving onto other tasks.", "labels": [], "entities": []}, {"text": "Since then the method has also been applied to other NLP tasks with good results (see Section 6).", "labels": [], "entities": []}, {"text": "In the remaining sections, we first introduce classifier combination on the basis of previous work in the machine learning literature and present the combination methods we use in our experiments (Section 2).", "labels": [], "entities": []}, {"text": "Then we explain our experimental setup (Section 3), also describing the corpora (3.1) and tagger generators used in the experiments.", "labels": [], "entities": []}, {"text": "In Section 4, we goon to report the overall results of the experiments, starting with a comparison between the component taggers (and hence between the underlying tagger generators) and continuing with a comparison of the combination methods.", "labels": [], "entities": []}, {"text": "The results are examined in more detail in Section 5, where we discuss such aspects as accuracy on specific words or tags, the influence of inconsistent training data, training set size, the contribution of individual component taggers, and tagset granularity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9984526634216309}]}, {"text": "In Section 6, we discuss the results in the light of related work, after which we conclude (Section 7) with a summary of the most important observations and interesting directions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to test the potential of system combination, we obviously need systems to combine, i.e., a number of different taggers.", "labels": [], "entities": []}, {"text": "As we are primarily interested in the combination of classifiers trained on the same data sets, we are in fact looking for data sets (in this case, tagged corpora) and systems that can automatically generate a tagger on the basis of those data sets.", "labels": [], "entities": []}, {"text": "For the current experiments, we have selected three tagged corpora and four tagger generators.", "labels": [], "entities": []}, {"text": "Before giving a detailed description of each of these, we first describe how the ingredients are used in the experiments.", "labels": [], "entities": []}, {"text": "Each corpus is used in the same way to test tagger and combiner performance.", "labels": [], "entities": []}, {"text": "First of all, it is split into a 90% training set and a 10% test set.", "labels": [], "entities": []}, {"text": "We can evaluate the base taggers by using the whole training set to train the tagger generators and the test set to test the resulting tagger.", "labels": [], "entities": []}, {"text": "For the combiners, a more complex strategy must be followed, since combiner training must be done on material unseen by the base taggers involved.", "labels": [], "entities": [{"text": "combiners", "start_pos": 8, "end_pos": 17, "type": "TASK", "confidence": 0.941620945930481}]}, {"text": "Rather than setting apart a fixed combiner training set, we use a ninefold training strategy?", "labels": [], "entities": []}, {"text": "The 90% trai1~ing set is split into nine equal parts.", "labels": [], "entities": []}, {"text": "Each part is tagged with component taggers that have been trained on the other eight parts.", "labels": [], "entities": []}, {"text": "All results are then concatenated for use in combiner training, so that, in contrast to our earlier work, all of the training set is effectively available for the training of the combiner.", "labels": [], "entities": []}, {"text": "Finally, the resulting combiners are tested on the test set.", "labels": [], "entities": []}, {"text": "Since the test set is identical for all methods, we can compute the statistical significance of the results using McNemar's chi-squared test).", "labels": [], "entities": []}, {"text": "As we will see, the increase in combiner training set size (90% of the corpus versus the fixed 10% tune set in the earlier experiments) indeed results in better performance.", "labels": [], "entities": [{"text": "combiner training set size", "start_pos": 32, "end_pos": 58, "type": "METRIC", "confidence": 0.5813864544034004}]}, {"text": "On the other hand, the increased amount of data also increases time and space requirements for some systems to such a degree that we had to exclude them from (some parts of) the experiments.", "labels": [], "entities": []}, {"text": "The data in the training set is the only information used in tagger and combiner construction: all components of all taggers and combiners (lexicon, context statistics, etc.) are entirely data driven, and no manual adjustments are made.", "labels": [], "entities": [{"text": "tagger and combiner construction", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.6676345020532608}]}, {"text": "If any tagger or combiner construction method is parametrized, we use default settings where available.", "labels": [], "entities": []}, {"text": "If there is no default, we choose intuitively appropriate values without preliminary testing.", "labels": [], "entities": []}, {"text": "In these cases, we report such parameter settings in the introduction to the system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 7  Error rates for the most confusing words. For each word, we list the total number of instances  in the test set (n), the number of tags associated with the word (tags), and then, for each base  tagger and WPDV(Tags+Context), the rank in the error list (rank), the absolute number of  errors (err), and the percentage of instances that is mistagged (%).", "labels": [], "entities": [{"text": "absolute number of  errors (err)", "start_pos": 274, "end_pos": 306, "type": "METRIC", "confidence": 0.7064775739397321}]}]}