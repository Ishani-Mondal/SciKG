{"title": [{"text": "A Machine Learning Approach to Coreference Resolution of Noun Phrases", "labels": [], "entities": [{"text": "Coreference Resolution of Noun Phrases", "start_pos": 31, "end_pos": 69, "type": "TASK", "confidence": 0.9531597256660461}]}], "abstractContent": [{"text": "In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text.", "labels": [], "entities": [{"text": "coreference resolution of noun phrases in unrestricted text", "start_pos": 49, "end_pos": 108, "type": "TASK", "confidence": 0.8961782529950142}]}, {"text": "The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases.", "labels": [], "entities": []}, {"text": "It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of \"organization,\" \"person,\" or other types.", "labels": [], "entities": []}, {"text": "We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.8785866498947144}, {"text": "MUC-7 coreference corpora", "start_pos": 68, "end_pos": 93, "type": "DATASET", "confidence": 0.8027220169703165}, {"text": "noun phrase coreference task", "start_pos": 158, "end_pos": 186, "type": "TASK", "confidence": 0.6582100763916969}, {"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.9991955161094666}]}, {"text": "Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9203950464725494}]}, {"text": "It is an important subtask in natural language processing systems.", "labels": [], "entities": []}, {"text": "In particular, information extraction (IE) systems like those builtin the DARPA Message Understanding Conferences have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.8548495173454285}, {"text": "DARPA Message Understanding Conferences", "start_pos": 74, "end_pos": 113, "type": "DATASET", "confidence": 0.7069834321737289}, {"text": "coreference resolution", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.9201117157936096}]}, {"text": "In this paper, we focus on the task of determining coreference relations as defined in and MUC-7.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.8538213968276978}]}, {"text": "Specifically, a coreference relation denotes an identity of reference and holds between two textual elements known as markables, which can be definite noun phrases, demonstrative noun phrases, proper names, appositives, sub-noun phrases that act as modifiers, pronouns, and soon.", "labels": [], "entities": []}, {"text": "Thus, our coreference task resolves general noun phrases and is not restricted to a certain type of noun phrase such as pronouns.", "labels": [], "entities": []}, {"text": "Also, we do not place any restriction on the possible candidate markables; that is, all markables, whether they are \"organization,\" \"person,\" or other entity types, are considered.", "labels": [], "entities": []}, {"text": "The ability to link coreferring noun phrases both within and across sentences is critical to discourse analysis and language understanding in general.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7170421034097672}, {"text": "language understanding", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.7474184334278107}]}], "datasetContent": [{"text": "In order to evaluate the performance of our learning approach to coreference resolution on common data sets, we utilized the annotated corpora and scoring programs from MUC-6 and MUC-7, which assembled a set of newswire documents annotated with coreference chains.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.9567909240722656}, {"text": "MUC-6", "start_pos": 169, "end_pos": 174, "type": "DATASET", "confidence": 0.9691140055656433}, {"text": "MUC-7", "start_pos": 179, "end_pos": 184, "type": "DATASET", "confidence": 0.9140190482139587}]}, {"text": "Although we did not participate in either MUC-6 or MUC-7, we were able to obtain the training and test corpora for both years from the MUC organizers for research purposes.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.9238340258598328}, {"text": "MUC-7", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.8282997012138367}, {"text": "MUC", "start_pos": 135, "end_pos": 138, "type": "DATASET", "confidence": 0.8914885520935059}]}, {"text": "1 To our knowledge, these are the only publicly available annotated corpora for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.9706512093544006}]}, {"text": "For MUC-6, 30 dry-run documents annotated with coreference :information were used as the training documents for our coreference engine.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.8874673247337341}]}, {"text": "There are also 30 annotated training documents from MUC-7.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.8635463118553162}]}, {"text": "The total size of the 30 training documents is close to 12,400 words for MUC-6 and 19,000 words for MUC-7.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.9241129159927368}, {"text": "MUC-7", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.9491074085235596}]}, {"text": "There are altogether 20,910 (48,872) training examples used for MUC-6 (MUC-7), of which only 6.5% (4.4%) are positive examples in MUC-6 (MUC-7).", "labels": [], "entities": [{"text": "MUC-6 (MUC-7)", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.6667529046535492}, {"text": "MUC-6 (MUC-7)", "start_pos": 130, "end_pos": 143, "type": "DATASET", "confidence": 0.8269733190536499}]}, {"text": "2 After training a separate classifier for each year, we tested the performance of each classifier on its corresponding test corpus.", "labels": [], "entities": []}, {"text": "For MUC-6, the C5 pruning confidence is set at 20% and the minimum number of instances per leaf node is set at 5.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.5677051544189453}, {"text": "pruning confidence", "start_pos": 18, "end_pos": 36, "type": "METRIC", "confidence": 0.8762755990028381}]}, {"text": "For MUC-7, the pruning confidence is 60% and the minimum number of instances is 2.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.5032411813735962}, {"text": "pruning confidence", "start_pos": 15, "end_pos": 33, "type": "METRIC", "confidence": 0.9678255319595337}]}, {"text": "The parameters are determined by performing 10-fold cross-validation on the whole training set for each MUC year.", "labels": [], "entities": [{"text": "MUC year", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.6696439385414124}]}, {"text": "The possible pruning confidence values that we tried are 10%, 20%, 40%, 60%, 80%, and 100%, and for minimum instances, we tried 2, 5, 10, 15, and 20.", "labels": [], "entities": []}, {"text": "Thus, a total of 30 (6 x 5) cross-validation runs were executed.", "labels": [], "entities": []}, {"text": "One advantage of using a decision tree learning algorithm is that the resulting decision tree classifier can be interpreted by humans.", "labels": [], "entities": [{"text": "decision tree learning algorithm", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.7316477000713348}]}, {"text": "The decision tree generated for MUC-6, shown in, seems to encapsulate a reasonable rule of thumb that matches our intuitive linguistic notion of when two noun phrases can corefer.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.6138561964035034}]}, {"text": "It is also interesting to note that only 8 out of the 12 available features in the training examples are actually used in the final decision tree built.", "labels": [], "entities": []}, {"text": "MUC-6 has a standard set of 30 test documents, which is used by all systems that participated in the evaluation.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9724404811859131}]}, {"text": "Similarly, MUC-7 has a test corpus of 20 documents.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.9161554574966431}]}, {"text": "We compared our system's MUC-6 and MUC-7 performance with that of the systems that took part in MUC-6 and MUC-7, respectively.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.853036642074585}, {"text": "MUC-7", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.6625237464904785}, {"text": "MUC-6", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.8503972887992859}, {"text": "MUC-7", "start_pos": 106, "end_pos": 111, "type": "DATASET", "confidence": 0.6032270193099976}]}, {"text": "When the coreference engine is given new test documents, its output is in the form of SGML files with the coreference chains properly annotated according to the guidelines.", "labels": [], "entities": []}, {"text": "3 We then used the scoring programs 1 See http://www.itl.nist.gov/iad/894.02/related_projects/muc/index.html for details on obtaining the corpora.", "labels": [], "entities": []}, {"text": "2 Our system runs on a Pentium III 550MHz PC.", "labels": [], "entities": []}, {"text": "It took less than 5 minutes to generate the training examples from the training documents for MUC-6, and about 7 minutes for MUC-7.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.9166052341461182}, {"text": "MUC-7", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.9448790550231934}]}, {"text": "The training time for the C5 algorithm to generate a decision tree from all the training examples was less than 3 seconds for both MUC years.", "labels": [], "entities": [{"text": "MUC", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.7126201391220093}]}, {"text": "3 The time taken to generate the coreference chains for the 30 MUC-6 test documents of close to 13,400 words was less than 3 minutes, while it took less than 2 minutes for the 20 MUC-7 test documents of about 10,000 words.", "labels": [], "entities": [{"text": "MUC-6 test documents", "start_pos": 63, "end_pos": 83, "type": "DATASET", "confidence": 0.947014590104421}, {"text": "MUC-7 test documents", "start_pos": 179, "end_pos": 199, "type": "DATASET", "confidence": 0.9365755319595337}]}, {"text": "The decision tree classifier learned for MUC-6.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.7209804058074951}]}, {"text": "for the respective years to generate the recall and precision scores for our coreference engine.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9993084669113159}, {"text": "precision scores", "start_pos": 52, "end_pos": 68, "type": "METRIC", "confidence": 0.9685271978378296}]}, {"text": "Our coreference engine achieves a recall of 58.6% and a precision of 67.3%, yielding a balanced F-measure of 62.6% for MUC-6.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9997228980064392}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9994891881942749}, {"text": "F-measure", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9986360669136047}, {"text": "MUC-6", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.8244365453720093}]}, {"text": "For MUC-7, the recall is 56.1%, the precision is 65.5%, and the balanced F-measure is 60.4%.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.6763735413551331}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9998277425765991}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9997633099555969}, {"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9791314601898193}]}, {"text": "4 We plotted the scores of our coreference engine (square-shaped) against the official test scores of the other systems (crossshaped) in and.", "labels": [], "entities": []}, {"text": "We also plotted the learning curves of our coreference engine in and, showing its accuracy averaged over three random trials when trained on 1, 2, 3, 4, 5, 10, 15, 20, 25, and 30 training documents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9991587400436401}]}, {"text": "The learning curves indicate that our coreference engine achieves its peak performance with about 25 training documents, or about 11,000 to 17,000 words of training documents.", "labels": [], "entities": []}, {"text": "This number of training documents would generate tens of thousands of training examples, sufficient for the decision tree learning algorithm to learn a good classifier.", "labels": [], "entities": []}, {"text": "At higher numbers of training documents, our system seems to start overfitting the training data.", "labels": [], "entities": []}, {"text": "For example, on MUC-7 data, training on the full set of 30 training documents results in a more complex decision tree.", "labels": [], "entities": [{"text": "MUC-7 data", "start_pos": 16, "end_pos": 26, "type": "DATASET", "confidence": 0.9366940557956696}]}, {"text": "Our system's scores are in the upper region of the MUC-6 and MUC-7 systems.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.9603862762451172}, {"text": "MUC-7", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.8502136468887329}]}, {"text": "We performed a simple one-tailed, paired sample t-test at significance level p = 0.05 to determine whether the difference between our system's F-measure score and each of the other systems' F-measure score on the test documents is statistically significant.", "labels": [], "entities": [{"text": "significance level p", "start_pos": 58, "end_pos": 78, "type": "METRIC", "confidence": 0.9124696652094523}, {"text": "F-measure score", "start_pos": 143, "end_pos": 158, "type": "METRIC", "confidence": 0.9503167271614075}]}, {"text": "We found that at the 95% significance level (p = 0.05), our system performed better than three MUC-6 systems, and as well as the rest of the MUC-6 systems.", "labels": [], "entities": [{"text": "significance level", "start_pos": 25, "end_pos": 43, "type": "METRIC", "confidence": 0.9497942626476288}, {"text": "MUC-6", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.8695380091667175}, {"text": "MUC-6", "start_pos": 141, "end_pos": 146, "type": "DATASET", "confidence": 0.8721572160720825}]}, {"text": "Using the 4 Note that MUC-6 did not use balanced F-measure as the official evaluation measure, but MUC-7 did.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.9660176038742065}, {"text": "F-measure", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9601830840110779}, {"text": "MUC-7", "start_pos": 99, "end_pos": 104, "type": "DATASET", "confidence": 0.9730846881866455}]}, {"text": "5 Though the McNemar testis shown to have low Type I error compared with the paired t-test), we did not carryout this test in the context of coreference.", "labels": [], "entities": [{"text": "Type I error", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.9160371025403341}]}, {"text": "This is because an example instance defines a coreference link between two noun phrases, and since this link is transitive in nature, it is unclear how the number of links misclassified by System A but not by System B and vice versa can be obtained to execute the McNemar test.", "labels": [], "entities": [{"text": "McNemar test", "start_pos": 264, "end_pos": 276, "type": "DATASET", "confidence": 0.782938152551651}]}, {"text": "Coreference scores of MUC-7 systems and our system.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.8166311979293823}]}, {"text": "same significance level, our system performed better than four MUC-7 systems, and as well as the rest of the MUC-7 systems.", "labels": [], "entities": [{"text": "significance level", "start_pos": 5, "end_pos": 23, "type": "METRIC", "confidence": 0.9703137874603271}, {"text": "MUC-7", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.8410089015960693}, {"text": "MUC-7", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.8897634744644165}]}, {"text": "Our result is encouraging since it indicates that a learning approach using relatively shallow features can achieve scores comparable to those of systems built using nonlearning approaches.", "labels": [], "entities": []}, {"text": "Number of MUC-6 Documents Trained On", "labels": [], "entities": [{"text": "MUC-6 Documents", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.611220121383667}]}], "tableCaptions": [{"text": " Table 3  MUC-6 results of complete and baseline systems to study the contribution of the features.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.7009366750717163}]}, {"text": " Table 4  MUC-7 results of complete and baseline systems to study the contribution of the features.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.7292935252189636}]}, {"text": " Table 6  The types and frequencies of errors that affect recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9909058809280396}]}]}