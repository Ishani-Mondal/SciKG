{"title": [{"text": "The Interaction of Knowledge Sources in Word Sense Disambiguation", "labels": [], "entities": [{"text": "Interaction of Knowledge Sources in Word Sense Disambiguation", "start_pos": 4, "end_pos": 65, "type": "TASK", "confidence": 0.6499354913830757}]}], "abstractContent": [{"text": "Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial intelligence research.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8343344728151957}]}, {"text": "An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results.", "labels": [], "entities": []}, {"text": "We present a sense tagger which uses several knowledge sources.", "labels": [], "entities": [{"text": "sense tagger", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.6963943988084793}]}, {"text": "Tested accuracy exceeds 94% on our evaluation corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9979463219642639}]}, {"text": "Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words.", "labels": [], "entities": []}, {"text": "It is argued that this approach is more likely to assist the creation of practical systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense disambiguation (WSD) is a problem long recognised in computational linguistics and there has been a recent resurgence of interest, including a special issue of this journal devoted to the topic (.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8534706383943558}]}, {"text": "Despite this there is still a considerable diversity of methods employed by researchers, as well as differences in the definition of the problems to be tackled.", "labels": [], "entities": []}, {"text": "The SENSEVAL evaluation framework) was a DARPA-style competition designed to bring some conformity to the field of WSD, although it has yet to achieve that aim completely.", "labels": [], "entities": [{"text": "WSD", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9626562595367432}]}, {"text": "The main sources of divergence are the choice of computational paradigm, the proportion of text words disambiguated, the granularity of the meanings assigned to them, and the knowledge sources used.", "labels": [], "entities": []}, {"text": "We will discuss each in turn.", "labels": [], "entities": []}, {"text": "noted that, for the most part, part-of-speech tagging is tackled using the noisy channel model, although transformation rules and grammaticostatistical methods have also had some success.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.684214174747467}]}, {"text": "There has been far less consensus as to the best approach to WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9846840500831604}]}, {"text": "Currently, machine learning methods (Yarowsky 1995; Rigau, Atserias, and Agirre 1997) and combinations of classifiers (McRoy 1992) have been popular.", "labels": [], "entities": []}, {"text": "This paper reports a WSD system employing elements of both approaches.", "labels": [], "entities": [{"text": "WSD", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9852297902107239}]}, {"text": "Another source of difference in approach is the proportion of the vocabulary disambiguated.", "labels": [], "entities": []}, {"text": "Some researchers have concentrated on producing WSD systems that base results on a limited number of words, for example and who quoted results for 12 words, and a second group, including Leacock, Towell, and and, who gave results for just one, namely interest.", "labels": [], "entities": [{"text": "WSD", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9738762974739075}]}, {"text": "But limiting the vocabulary on which a system is evaluated can have two serious drawbacks.", "labels": [], "entities": []}, {"text": "First, the words used were not chosen by frequency-based sampling techniques and so we have noway of knowing whether or not they are special cases, a point emphasised by.", "labels": [], "entities": []}, {"text": "Secondly, there is no guarantee that the techniques employed will be applicable when a larger vocabulary is tackled.", "labels": [], "entities": []}, {"text": "However it is likely that mark-up fora restricted vocabulary can be carried out more rapidly since the subject has to learn the possible senses of fewer words.", "labels": [], "entities": []}, {"text": "Among the researchers mentioned above, one must distinguish between, on the one hand, supervised approaches that are inherently limited in performance to the words over which they evaluate because of limited training data and, on the other hand, approaches whose unsupervised learning methodology is applied to only small numbers of words for evaluation, but which could in principle have been used to tag all content words in a text.", "labels": [], "entities": []}, {"text": "Others, such as and ourselves, have concentrated on approaches that disambiguate all content words.", "labels": [], "entities": []}, {"text": "1 In addition to avoiding the problems inherent in restricted vocabulary systems, wide coverage systems are more likely to be useful for NLP applications, as discussed by.", "labels": [], "entities": []}, {"text": "A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense (see Section 3.1).", "labels": [], "entities": [{"text": "WSD", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9684002995491028}]}, {"text": "Like Cowie, Guthrie, and Guthrie (1992), we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g., and see Section 2.3) and Roget categories correspond broadly to the wider, homograph, distinctions.", "labels": [], "entities": []}, {"text": "In this paper we attempt to show that the high level of results more typical of systems trained on many instances of a restricted vocabulary can also be obtained by large vocabulary systems, and that the best results are to be obtained from an optimization of a combination of types of lexical knowledge (see Section 2).", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of WSD algorithms has recently become a much-studied area.,, and Melamed and Resnik (2000) each presented arguments for adopting various evaluation strategies, with Resnik and Yarowsky's proposal directly influencing the set-up of SENSEVAL.", "labels": [], "entities": [{"text": "WSD algorithms", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.8545480072498322}, {"text": "SENSEVAL", "start_pos": 246, "end_pos": 254, "type": "TASK", "confidence": 0.7976800799369812}]}, {"text": "At the heart of their proposals is the ability of human subjects to markup text with the phenomenon in question (WSD in this case) and evaluate the results of computation.", "labels": [], "entities": []}, {"text": "This linguistic phenomenon has proved to be far more elusive and complex than many others.", "labels": [], "entities": []}, {"text": "We have discussed this at length elsewhere (Wilks 1997) and will assume here that humans can markup text for senses to a sufficient degree.", "labels": [], "entities": []}, {"text": "questioned the possibility of creating sense-tagged texts, claiming the task to be impossible.", "labels": [], "entities": []}, {"text": "However, it should be borne in mind that no alternative has yet been widely accepted and that Kilgarriff himself used the markup-and-test model for SENSEVAL.", "labels": [], "entities": [{"text": "SENSEVAL", "start_pos": 148, "end_pos": 156, "type": "TASK", "confidence": 0.7195596098899841}]}, {"text": "In the following discussion we compare the evaluation methodology adopted herewith those proposed by others.", "labels": [], "entities": []}, {"text": "The standard evaluation procedure for WSD is to compare the output of the system against gold standard texts, but these are very labor-intensive to obtain; lexical semantic markup is generally considered to be a more difficult and time-consuming task than part-of-speech markup).", "labels": [], "entities": [{"text": "WSD", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9541658163070679}]}, {"text": "Rather than expend avast amount of effort on manual tagging we decided to combine two existing resources: SEMCOR.", "labels": [], "entities": [{"text": "manual tagging", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.6139121949672699}, {"text": "SEMCOR", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.6047016382217407}]}, {"text": "SEMCOR is a 200,000 word corpus with the content words manually tagged as part of the WordNet project.", "labels": [], "entities": [{"text": "SEMCOR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8560385704040527}]}, {"text": "The semantic tagging was carried out by trained lexicographers under disciplined conditions that attempted to keep tagging inconsistencies to a minimum.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7624127864837646}]}, {"text": "SENSUS is a large-scale ontology designed for machine-translation and was itself produced by merging the ontological hierarchies of WordNet, LDOCE (as derived by Bruce and Guthrie, see Section 4.4), and the Penman Upper Model () from ISI.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 132, "end_pos": 139, "type": "DATASET", "confidence": 0.9487821459770203}]}, {"text": "To facilitate the merging of these three resources to produce SENSUS, Knight and Luk were required to derive a mapping between the senses in the two lexical resources.", "labels": [], "entities": [{"text": "SENSUS", "start_pos": 62, "end_pos": 68, "type": "TASK", "confidence": 0.7974854707717896}]}, {"text": "We used this mapping to translate the WordNet-tagged content words in SEMCOR to LDOCE tags.", "labels": [], "entities": [{"text": "WordNet-tagged content words in SEMCOR", "start_pos": 38, "end_pos": 76, "type": "DATASET", "confidence": 0.8283706903457642}]}, {"text": "The mapping of senses is not one-to-one, and some WordNet synsets are mapped onto two or three LDOCE senses when WordNet does not distinguish between them.", "labels": [], "entities": []}, {"text": "The mapping also contained significant gaps, chiefly words and senses not in the translation scheme.", "labels": [], "entities": []}, {"text": "SEMCOR contains 91,808 words tagged with WordNet synsets, 6,071 of which are proper names, which we ignored, leaving 85,737 words which could potentially be translated.", "labels": [], "entities": [{"text": "SEMCOR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8566453456878662}, {"text": "WordNet synsets", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.7652872502803802}]}, {"text": "The translation contains only 36,869 words tagged with LDOCE senses; however, this is a reasonable size for an evaluation corpus for the task, and it is several orders of magnitude larger than those used by other researchers working in large vocabulary WSD, for example Cowie, Guthrie, and Guthrie (1992),, and.", "labels": [], "entities": []}, {"text": "This corpus was also constructed without the excessive cost of additional hand-tagging and does not introduce any of the inconsistencies that can occur with a poorly controlled tagging strategy.", "labels": [], "entities": []}, {"text": "proposed to evaluate large vocabulary WSD systems by choosing a set of test words and providing annotated test and training examples for just these words, allowing supervised and unsupervised algorithms to be tested on the same vocabulary.", "labels": [], "entities": [{"text": "WSD", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9468538165092468}]}, {"text": "This model was implemented in SENSEVAL).", "labels": [], "entities": []}, {"text": "However, for the evaluation of the system presented here, there would have been no benefit from using this strategy since it still involves the manual tagging of large amounts of data and this effort could be used to create a gold standard corpus in which all content words are disambiguated.", "labels": [], "entities": []}, {"text": "It is possible that some computational techniques may evaluate well over a small vocabulary but may notwork fora large set of words, and the evaluation strategy proposed by Resnik and Yarowsky will not discriminate between these cases.", "labels": [], "entities": []}, {"text": "In our evaluation corpus, the most frequent ambiguous type is have, which appears 604 times.", "labels": [], "entities": [{"text": "have", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9626631140708923}]}, {"text": "A large number of words (2407) occur only once, and nearly 95% have 25 occurrences or less.", "labels": [], "entities": []}, {"text": "shows the distribution of ambiguous types by number of corpus tokens.", "labels": [], "entities": []}, {"text": "It is worth noting that, as would be expected, the observed distribution is highly Zipfian.", "labels": [], "entities": []}, {"text": "Differences in evaluation corpora makes comparison difficult.", "labels": [], "entities": [{"text": "comparison", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.965787947177887}]}, {"text": "However, some idea of the difficulty of WSD can be gained by calculating properties of the evaluation corpus.", "labels": [], "entities": [{"text": "WSD", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9696395993232727}]}, {"text": "suggest that the lowest level of performance which can be reasonably expected from a WSD system is that achieved by assigning the most likely sense in all cases.", "labels": [], "entities": [{"text": "WSD", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9397685527801514}]}, {"text": "Since the first sense in LDOCE is usually the most frequent, we calculate this baseline figure using a heuristic which assumes the first sense is always correct.", "labels": [], "entities": []}, {"text": "This is the same baseline heuristic we used for the experiments Occurrence of ambiguous words in the evaluation corpus.", "labels": [], "entities": []}, {"text": "The choice of scoring metric is an important one in the evaluation of WSD algorithms.", "labels": [], "entities": [{"text": "WSD", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9849587678909302}]}, {"text": "The most commonly used metric is the ratio of words for which the system has assigned the correct sense compared to those which it attempted to disambiguate.", "labels": [], "entities": []}, {"text": "dubbed this the exact match metric, which is usually expressed as a percentage calculated according to the formula in (12).", "labels": [], "entities": [{"text": "exact match metric", "start_pos": 16, "end_pos": 34, "type": "METRIC", "confidence": 0.864355723063151}]}], "tableCaptions": [{"text": " Table 1  Relative contribution of knowledge sources in LEXAS.", "labels": [], "entities": [{"text": "LEXAS", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.7107996344566345}]}, {"text": " Table 1. They found that the local collocations were the most useful knowl- edge source in their system. However, it must be remembered that this experiment  was carried out on a data set consisting of a single word and may, therefore, not be  generalizable.", "labels": [], "entities": []}]}