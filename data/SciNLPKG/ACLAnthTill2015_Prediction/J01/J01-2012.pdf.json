{"title": [], "abstractContent": [{"text": "There are several ways in which the algorithmic acquisition of language knowledge and behavior can be studied.", "labels": [], "entities": []}, {"text": "One important area of research is the computational modeling of human language acquisition using statistical, machine learning, or neural network methods.", "labels": [], "entities": [{"text": "computational modeling of human language acquisition", "start_pos": 38, "end_pos": 90, "type": "TASK", "confidence": 0.6715453416109085}]}, {"text": "See Broeder and Murre (2000) fora recent collection of this type of research.", "labels": [], "entities": []}, {"text": "And there is of course the use of statistical and machine learning methods in computational linguistics and language technology (Manning and Schtitze 1999).", "labels": [], "entities": []}, {"text": "The theoretical study of the learnability of formal grammars in close relationship with linguistic explanation in generative grammar (\"the logical problem of language acqui-sition\") is yet another relevant area of research.", "labels": [], "entities": [{"text": "generative grammar", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.8767933547496796}]}, {"text": "The new book by Bruce Tesar and Paul Smolensky (T&S) is an example of the latter approach, but supported by computational modeling.", "labels": [], "entities": [{"text": "T&S)", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.8149068504571915}]}, {"text": "In the book, an approach to learning in optimality theory (OT) is proposed.", "labels": [], "entities": [{"text": "learning in optimality theory (OT)", "start_pos": 28, "end_pos": 62, "type": "TASK", "confidence": 0.7841304881232125}]}, {"text": "OT is an alternative to the principles and parameters (P&P) approach to Universal Grammar.", "labels": [], "entities": [{"text": "OT", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.6153150796890259}]}, {"text": "OT claims that all languages have in common a set of constraints on well-formedness, and differ only in which constraints have priority in case of conflict.", "labels": [], "entities": [{"text": "OT", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7712680697441101}]}, {"text": "Priorities for each language are characterized in a language-specific ranking (dominance hierarchy).", "labels": [], "entities": []}, {"text": "This hierarchy assigns harmony values to structural descriptions of language input (depending on which constraints are broken), and the analyses with maximal harmony are the well-formed ones.", "labels": [], "entities": []}, {"text": "In OT, the problem of language learning is therefore transformed from a parameter-setting problem into a constraint-ranking problem.", "labels": [], "entities": [{"text": "OT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9478235244750977}]}, {"text": "In P&P approaches to learning, either comprehensive structural detail has to be encoded in the learning principles (as in cue learning), or the search is completely uninformed (as in triggering learning).", "labels": [], "entities": []}, {"text": "The central claim of T&S is that OT provides sufficient structure to allow efficient and grammatically informed learning, and therefore offers an optimal trade-off.", "labels": [], "entities": [{"text": "T&S", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8245550990104675}]}, {"text": "Chapter 1 introduces the problem of language learning and sketches its solution.", "labels": [], "entities": [{"text": "language learning", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7212348133325577}]}, {"text": "The problem is that a learner cannot parse the language input until a grammar has been learned, but a grammar cannot be learned until the input can be parsed.", "labels": [], "entities": []}, {"text": "The solution proposed for this unsupervised learning problem will be familiar to researchers in statistical natural language processing: a variant of expectation-maximization in which the model (grammar) and the input-output pairing (parse) are iteratively optimized.", "labels": [], "entities": [{"text": "statistical natural language processing", "start_pos": 96, "end_pos": 135, "type": "TASK", "confidence": 0.6187791675329208}]}, {"text": "In Chapter 2, OT is explained as a series of general principles.", "labels": [], "entities": [{"text": "OT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9588648676872253}]}, {"text": "These principles are illustrated by means of a phonological example (syllable parsing) and a syntactic example (distribution of clausal subjects).", "labels": [], "entities": [{"text": "syllable parsing", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.7513419389724731}]}, {"text": "This is an excellent chapter for those wanting a concise and clear introduction to OT.", "labels": [], "entities": [{"text": "OT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.8359577655792236}]}, {"text": "Chapter 3 introduces constraint demotion (CD) as the grammar-learning part in the expectation-maximization setup explained earlier.", "labels": [], "entities": [{"text": "constraint demotion (CD)", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.6199358582496644}]}, {"text": "The basic mechanism is simple and elegant: given an initial constraint ordering, and pairs of well-formed structural descriptions (winners) and competing not-well-formed structural descriptions (losers), demote the constraints violated by a winner down in 316", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}