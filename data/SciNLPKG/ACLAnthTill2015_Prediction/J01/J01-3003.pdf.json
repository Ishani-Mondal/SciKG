{"title": [{"text": "Automatic Verb Classification Based on Statistical Distributions of Argument Structure", "labels": [], "entities": [{"text": "Automatic Verb Classification", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5399637818336487}]}], "abstractContent": [{"text": "Automatic acquisition of lexical knowledge is critical to a wide range of natural language processing tasks.", "labels": [], "entities": [{"text": "Automatic acquisition of lexical knowledge", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8177614808082581}, {"text": "natural language processing", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.6738112171490988}]}, {"text": "Especially important is knowledge about verbs, which are the primary source of relational information in a sentence-the predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom).", "labels": [], "entities": []}, {"text": "In this work, we report on supervised learning experiments to automatically classify three major types of English verbs, based on their argument structure-specifically, the thematic roles they assign to participants.", "labels": [], "entities": []}, {"text": "We use linguistically-motivated statistical indicators extracted from large annotated corpora to train the classifier, achieving 69.8% accuracy fora task whose baseline is 34%, and whose expert-based upper bound we calculate at 86.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9988219141960144}]}, {"text": "A detailed analysis of the performance of the algorithm and of its errors con~'rms that the proposed features capture properties related to the argument structure of the verbs.", "labels": [], "entities": []}, {"text": "Our results validate our hypotheses that knowledge about thematic relations is crucial for verb classification, and that it can be gleaned from a corpus by automatic means.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7324701100587845}]}, {"text": "We thus demonstrate an effective combination of deeper linguistic knowledge with the robustness and scalability of statistical techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic acquisition of lexical knowledge is critical to a wide range of natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "Automatic acquisition of lexical knowledge", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8122012495994568}]}, {"text": "Especially important is knowledge about verbs, which are the primary source of relational information in a sentence--the predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom).", "labels": [], "entities": []}, {"text": "In facing the task of automatic acquisition of knowledge about verbs, two basic questions must be addressed: What information about verbs and their relational properties needs to be learned?", "labels": [], "entities": [{"text": "automatic acquisition of knowledge about verbs", "start_pos": 22, "end_pos": 68, "type": "TASK", "confidence": 0.7504422763983408}]}, {"text": "What information can in practice be learned through automatic means?", "labels": [], "entities": []}, {"text": "In answering these questions, some approaches to lexical acquisition have focused on learning syntactic information about verbs, by automatically extracting subcategorization frames from a corpus or machine-readable dictionary.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7587401866912842}]}, {"text": "The question then is what underlies these distinctions.", "labels": [], "entities": []}, {"text": "We identify the property that precisely distinguishes among these three classes as that of argument structure--i.e., the thematic roles assigned by the verbs.", "labels": [], "entities": []}, {"text": "The thematic roles for each class, and their mapping to subject and object positions, are summarized in.", "labels": [], "entities": []}, {"text": "Note that verbs across these three classes allow the same subcategorization frames (taking an NP objector occurring intransitively); thus, classification based on subcategorization alone would not distinguish them.", "labels": [], "entities": []}, {"text": "On the other hand, each of the three classes is comprised of multiple Levin classes, because the latter reflect more detailed semantic distinctions among the verbs (Levin 1993); thus, classification based on Levin's labeling would miss generalizations across the three broader classes.", "labels": [], "entities": []}, {"text": "By contrast, as shown in, each class has a unique pattern of thematic assignments, which categorize the verbs precisely into the three classes of interest.", "labels": [], "entities": []}, {"text": "Although the granularity of our classification differs from Levin's, we draw on her hypothesis that semantic properties of verbs are reflected in their syntactic behavior.", "labels": [], "entities": []}, {"text": "The behavior that Levin focuses on is the notion of diathesis alternation--an alternation in the expression of the arguments of a verb, such as the different mappings between transitive and intransitive that our verbs undergo.", "labels": [], "entities": []}, {"text": "Whether a verb participates in a particular diathesis alternation or not is a key factor in Levin's approach to classification.", "labels": [], "entities": []}, {"text": "We, like others in a computational framework, have extended this idea by showing that statistics over the alternants of a verb effectively capture information about its class).", "labels": [], "entities": []}, {"text": "In our specific task, we analyze the pattern of thematic assignments given in to develop statistical indicators that are able to determine the class of an optionally intransitive verb by capturing information across its transitive and intransitive alternants.", "labels": [], "entities": []}, {"text": "These indicators serve as input to a machine learning algorithm, under a supervised training methodology, which produces an automatic classification system for our three verb classes.", "labels": [], "entities": []}, {"text": "Since we rely on patterns of behavior across multiple occurrences of a verb, we begin with the problem of assigning a single class to the entire set of usages of a verb within the corpus.", "labels": [], "entities": []}, {"text": "For example, we measure properties across all occurrences of a word, such as raced, in order to assign a single classification to the lexical entry for the verb race.", "labels": [], "entities": []}, {"text": "This contrasts with work classifying individual occurrences of a verb in each local context, which have typically relied on training that includes instances of the verbs to be classified--essentially developing a bias that is used in conjunction with the local context to determine the best classification for new instances of previously seen verbs.", "labels": [], "entities": []}, {"text": "By contrast, our method assigns a classification to verbs that have not previously been seen in the training data.", "labels": [], "entities": []}, {"text": "Thus, while we do not as yet assign different classes to the instances of a verb, we can assign a single predominant class to new verbs that have never been encountered.", "labels": [], "entities": []}, {"text": "To preview our results, we demonstrate that combining just five numerical indicators, automatically extracted from large text corpora, is sufficient to reduce the error rate in this classification task by more than 50% over chance.", "labels": [], "entities": [{"text": "error rate", "start_pos": 163, "end_pos": 173, "type": "METRIC", "confidence": 0.979330450296402}]}, {"text": "Specifically, we achieve almost 70% accuracy in a task whose baseline (chance) performance is 34%, and whose expert-based upper bound is calculated at 86.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9990457892417908}]}, {"text": "Beyond the interest for the particular classification task at hand, this work addresses more general issues concerning verb class distinctions based in argument structure.", "labels": [], "entities": [{"text": "verb class distinctions based in argument structure", "start_pos": 119, "end_pos": 170, "type": "TASK", "confidence": 0.6379744751112801}]}, {"text": "We evaluate our hypothesis that such distinctions are reflected in statistics over corpora through a computational experimental methodology in which we investigate as indicated each of the subhypotheses below, in the context of the three verb classes under study: \u2022 Lexical features capture argument structure differences between verb classes.", "labels": [], "entities": []}, {"text": "1 \u2022 The linguistically distinctive features exhibit distributional differences across the verb classes that are apparent within linguistic experience (i.e., they can be collected from text).", "labels": [], "entities": []}, {"text": "\u2022 The statistical distributions of (some of) the features contribute to learning the classifications of the verbs.", "labels": [], "entities": []}, {"text": "In the following sections, we show that all three hypotheses above are borne out.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the argument structure distinctions of our three verb classes in more detail.", "labels": [], "entities": []}, {"text": "In support of the first hypothesis above, we discuss lexical correlates of the underlying differences in thematic assignments that distinguish the three verb classes under investigation.", "labels": [], "entities": []}, {"text": "In Section 3, we show how to approximate these features by simple syntactic counts, and how to perform these counts on available corpora.", "labels": [], "entities": []}, {"text": "We confirm the second hypothesis above, by showing that the differences in distribution predicted by the underlying argument structures are largely found in the data.", "labels": [], "entities": []}, {"text": "In Section 4, in a series of machine learning experiments and a detailed analysis of errors, we confirm the third hypothesis by showing that the differences in the distribution of the extracted features are successfully used for verb classification.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 229, "end_pos": 248, "type": "TASK", "confidence": 0.7730575203895569}]}, {"text": "Section 5 evaluates the significance of these results by comparing the program's accuracy to an expertbased upper bound.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9992349147796631}]}, {"text": "We conclude the paper with a discussion of its contributions, comparison to related work, and suggestions for future extensions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we turn to our computational experiments that investigate whether the statistical indicators of thematic properties that we have developed can in fact be used to classify verbs.", "labels": [], "entities": []}, {"text": "Recall that the task we have set ourselves is that of automatically learning the best class fora set of usages of a verb, as opposed to classifying individual occurrences of the verb.", "labels": [], "entities": []}, {"text": "The frequency distributions of our features yield a vector for each verb that represents the estimated values for the verb on each dimension across the entire corpus: The resulting set of 59 vectors constitutes the data for our machine learning experiments.", "labels": [], "entities": []}, {"text": "We use this data to train an automatic classifier to determine, given the feature values fora new verb (not from the training set), which of the three major classes of English optionally intransitive verbs it belongs to.", "labels": [], "entities": []}, {"text": "In pilot experiments on a subset of the features, we investigated a number of supervised machine learning methods that produce automatic classifiers (decision tree induction, rule learning, and two types of neural networks), as well as hierarchical clustering; see for more detail.", "labels": [], "entities": [{"text": "decision tree induction", "start_pos": 150, "end_pos": 173, "type": "TASK", "confidence": 0.6289200286070505}]}, {"text": "Because we achieved approximately the same level of performance in all cases, we narrowed our further experimentation to the publicly available version of the C5.0 machine learning system (http://www.rulequest.com), a newer version of C4.5 (Quinlan 1992), due to its ease of use and wide availability.", "labels": [], "entities": []}, {"text": "The C5.0 system generates both decision trees and corresponding rule sets from a training set of known classifications.", "labels": [], "entities": []}, {"text": "In our experiments, we found little to no difference in performance between the trees and rule sets, and report only the rule set results.", "labels": [], "entities": []}, {"text": "In the experiments below, we follow two methodologies in training and testing, each of which tests a subset of cases held out from the training data.", "labels": [], "entities": []}, {"text": "Thus, in all cases, the results we report are on test data that was never seen in training.", "labels": [], "entities": []}, {"text": "7 The first training and testing methodology we follow is 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "In this approach, the system randomly divides the data into ten parts, and runs ten times on a different 90%-training-data/10%-test-data split, yielding an average accuracy and standard error across the ten test sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9994003772735596}, {"text": "standard error", "start_pos": 177, "end_pos": 191, "type": "METRIC", "confidence": 0.9729840457439423}]}, {"text": "This training methodology is very useful for 7 One anonymous reviewer raised the concern that we do not test on verbs that were unseen by the authors prior to finalizing the specific features to count.", "labels": [], "entities": []}, {"text": "However, this does not reduce the generality of our results.", "labels": [], "entities": []}, {"text": "The features we use are motivated by linguistic theory, and derived from the set of thematic properties that discriminate the verb classes.", "labels": [], "entities": []}, {"text": "It is therefore very unlikely that they are skewed to the particular verbs we have chosen.", "labels": [], "entities": []}, {"text": "Furthermore, our cross-validation experiments, described in the next subsection, show that our results hold across a very large number of randomly selected subsets of this sample of verbs.", "labels": [], "entities": []}, {"text": "our application, as it yields performance measures across a large number of training data/test data sets, avoiding the problems of outliers in a single random selection from a relatively small data set such as ours.", "labels": [], "entities": [{"text": "training data/test data sets", "start_pos": 76, "end_pos": 104, "type": "DATASET", "confidence": 0.7249231189489365}]}, {"text": "The second methodology is a single hold-out training and testing approach.", "labels": [], "entities": []}, {"text": "Here, the system is run N times, where N is the size of the data set (i.e., the 59 verbs in our case), each time holding out a single data vector as the test case and using the remaining N-1 vectors as the training set.", "labels": [], "entities": []}, {"text": "The single hold-out methodology yields an overall accuracy rate (when the results are averaged across all N trials), but also--unlike cross-validation--gives us classification results on each individual data vector.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9990895986557007}]}, {"text": "This property enables us to analyze differential performance on the individual verbs and across the different verb classes.", "labels": [], "entities": []}, {"text": "Under both training and testing methodologies, the baseline (chance) performance in this task--a three-way classification--is 33.9%.", "labels": [], "entities": []}, {"text": "In the single hold-out methodology, there are 59 test cases, with 20, 19, and 20 verbs each from the unergative, unaccusative, and object-drop classes, respectively.", "labels": [], "entities": []}, {"text": "Chance performance of picking a single class label as a default and assigning it to all cases would yield at most 20 out of the 59 cases correct, or 33.9%.", "labels": [], "entities": []}, {"text": "For the cross-validation methodology, the determination of a baseline is slightly more complex, as we are testing on a random selection of 10% of the full data set in each run.", "labels": [], "entities": []}, {"text": "The 33.9% figure represents the expected relative proportion of a test set that would be labelled correctly by assignment of a default class label to the entire test set.", "labels": [], "entities": []}, {"text": "Although the precise make-up of the test cases vary, on average the test set will represent the class membership proportions of the entire set of verbs.", "labels": [], "entities": []}, {"text": "Thus, as with the single hold-out approach, chance accuracy corresponds to a maximum of 20/59, or 33.9%, of the test set being labelled correctly.", "labels": [], "entities": [{"text": "chance", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9151751399040222}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.8739293813705444}]}, {"text": "The theoretical maximum accuracy for the task is, of course, 100%, although in Section 5 we discuss some classification results from human experts that indicate that a more realistic expectation is much lower (around 87%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9988371729850769}]}], "tableCaptions": [{"text": " Table 6  Aggregated relative frequency data for the five features. E = unergatives, A = unaccusatives,  O = object-drops.", "labels": [], "entities": []}]}