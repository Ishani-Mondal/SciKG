{"title": [{"text": "SPRITE: Generalizing Topic Models with Structured Priors", "labels": [], "entities": [{"text": "SPRITE", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9359279274940491}, {"text": "Generalizing Topic Models", "start_pos": 8, "end_pos": 33, "type": "TASK", "confidence": 0.8639915784200033}]}], "abstractContent": [{"text": "We introduce SPRITE, a family of topic models that incorporates structure into model priors as a function of underlying components.", "labels": [], "entities": []}, {"text": "The structured priors can be constrained to model topic hierarchies, factorizations, correlations, and supervision , allowing SPRITE to be tailored to particular settings.", "labels": [], "entities": [{"text": "SPRITE", "start_pos": 126, "end_pos": 132, "type": "TASK", "confidence": 0.9385706186294556}]}, {"text": "We demonstrate this flexibility by constructing a SPRITE-based model to jointly infer topic hierarchies and author perspective, which we apply to corpora of political debates and online reviews.", "labels": [], "entities": []}, {"text": "We show that the model learns intuitive topics, outperforming several other topic models at predictive tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic models can be a powerful aid for analyzing large collections of text by uncovering latent interpretable structures without manual supervision.", "labels": [], "entities": []}, {"text": "Yet people often have expectations about topics in a given corpus and how they should be structured fora particular task.", "labels": [], "entities": []}, {"text": "It is crucial for the user experience that topics meet these expectations ) yet black box topic models provide no control over the desired output.", "labels": [], "entities": []}, {"text": "This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured.", "labels": [], "entities": [{"text": "SPRITE", "start_pos": 20, "end_pos": 26, "type": "TASK", "confidence": 0.726642370223999}]}, {"text": "SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (, factorizations), sparsity (, correlations between topics (), preferences over word choices (, and associations between topics and document attributes.", "labels": [], "entities": []}, {"text": "SPRITE builds on a standard topic model, adding structure to the priors over the model parameters.", "labels": [], "entities": [{"text": "SPRITE", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.7008255124092102}]}, {"text": "The priors are given by log-linear functions of underlying components ( \u00a72), which provide additional latent structure that we will show can enrich the model in many ways.", "labels": [], "entities": []}, {"text": "By applying particular constraints and priors to the component hyperparameters, a variety of structures can be induced such as hierarchies and factorizations ( \u00a73), and we will show that this framework captures many existing topic models ( \u00a74).", "labels": [], "entities": []}, {"text": "After describing the general form of the model, we show how SPRITE can be tailored to particular settings by describing a specific model for the applied task of jointly inferring topic hierarchies and perspective ( \u00a76).", "labels": [], "entities": [{"text": "SPRITE", "start_pos": 60, "end_pos": 66, "type": "TASK", "confidence": 0.9678349494934082}]}, {"text": "We experiment with this topic+perspective model on sets of political debates and online reviews ( \u00a77), and demonstrate that SPRITE learns desired structures while outperforming many baselines at predictive tasks.", "labels": [], "entities": [{"text": "SPRITE", "start_pos": 124, "end_pos": 130, "type": "TASK", "confidence": 0.9534522294998169}]}], "datasetContent": [{"text": "We applied our models to two corpora: \u2022 Debates: A set of floor debates from the 109th-112th U.S. Congress, collected by, who also applied a hierarchical topic model to this data.", "labels": [], "entities": []}, {"text": "Each document is a transcript of one speaker's turn in a debate, and each document includes the first dimension of the DW-NOMINATE score (), a real-valued score indicating how conservative (positive) or liberal (negative) the speaker is.", "labels": [], "entities": [{"text": "DW-NOMINATE score", "start_pos": 119, "end_pos": 136, "type": "METRIC", "confidence": 0.9755415916442871}]}, {"text": "This value is \u03b1 (P ) . We took a sample of 5,000 documents from the House debates (850,374 tokens; 7,426 types), balanced across party affilia-49 tion.", "labels": [], "entities": []}, {"text": "We sampled from the most partisan speakers, removing scores below the median value.", "labels": [], "entities": []}, {"text": "\u2022 Reviews: Doctor reviews from RateMDs.com, previously analyzed using FLDA ().", "labels": [], "entities": [{"text": "FLDA", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.7180253267288208}]}, {"text": "The reviews contain ratings on a 1-5 scale for multiple aspects.", "labels": [], "entities": []}, {"text": "We centered the ratings around the middle value 3, then took reviews that had the same sign for all aspects, and averaged the scores to produce a value for \u03b1 (P ) . Our corpus contains 20,000 documents (476,991 tokens; 10,158 types), balanced across positive/negative scores.", "labels": [], "entities": []}, {"text": "Unless otherwise specified, K=50 topics and C=10 components (excluding the perspective component) for Debates, and K=20 and C=5 for Reviews.", "labels": [], "entities": []}, {"text": "These values were chosen as a qualitative preference, not optimized for predictive performance, but we experiment with different values in \u00a77.2.2.", "labels": [], "entities": []}, {"text": "We set the step size \u03b7 t according to, where the step size is the inverse of the sum of squared historical gradients.", "labels": [], "entities": []}, {"text": "We place a sparse Dirichlet(\u03c1=0.01) prior on the b variables, and apply weak regularization to all other hyperparameters via a N (0, 10 2 ) prior.", "labels": [], "entities": []}, {"text": "These hyperparameters were chosen after only minimal tuning, and were selected because they showed stable and reasonable output qualitatively during preliminary development.", "labels": [], "entities": []}, {"text": "We ran our inference algorithm for 5000 iterations, estimating the parameters \u03b8 and \u03c6 by averaging the final 100 iterations.", "labels": [], "entities": [{"text": "\u03c6", "start_pos": 84, "end_pos": 85, "type": "METRIC", "confidence": 0.9559719562530518}]}, {"text": "Our results are averaged across 10 randomly initialized samplers.", "labels": [], "entities": []}, {"text": "shows examples of topics learned from the Reviews corpus.", "labels": [], "entities": [{"text": "Reviews corpus", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.8387786746025085}]}, {"text": "The figure includes the highest probability words in various topics as well as the highest weight words in the supertopic components and perspective component, which feed into the priors over the topic parameters.", "labels": [], "entities": []}, {"text": "We see that one supertopic includes many words related to surgery, such as procedure and performed, and has multiple children, including a topic about dental work.", "labels": [], "entities": []}, {"text": "Another supertopic includes words describing family members such as kids and husband.", "labels": [], "entities": []}, {"text": "We evaluated the model on two predictive tasks as well as topic quality.", "labels": [], "entities": []}, {"text": "The first metric is perplexity of held-out text.", "labels": [], "entities": []}, {"text": "The held-out set is based on tokens rather than documents: we trained on even numbered tokens and tested on odd tokens.", "labels": [], "entities": []}, {"text": "This is a type of \"document completion\" evaluation () which measures how well the model can predict held-out tokens of a document after observing only some.", "labels": [], "entities": []}, {"text": "We also evaluated how well the model can predict the attribute value (DW-NOMINATE score or user rating) of the document.", "labels": [], "entities": [{"text": "DW-NOMINATE score or user rating)", "start_pos": 70, "end_pos": 103, "type": "METRIC", "confidence": 0.7977400372425715}]}, {"text": "We trained a linear regression model using the document topic distributions \u03b8 as features.", "labels": [], "entities": []}, {"text": "We held out half of the documents for testing and measured the mean absolute error.", "labels": [], "entities": [{"text": "mean absolute error", "start_pos": 63, "end_pos": 82, "type": "METRIC", "confidence": 0.879520038763682}]}, {"text": "When estimating document-specific SPRITE parameters for held-out documents, we fix the feature value \u03b1 (P ) m = 0 for that document.", "labels": [], "entities": [{"text": "SPRITE", "start_pos": 34, "end_pos": 40, "type": "TASK", "confidence": 0.7524045705795288}, {"text": "feature value \u03b1 (P ) m", "start_pos": 87, "end_pos": 109, "type": "METRIC", "confidence": 0.833572907107217}]}, {"text": "These predictive experiments do not directly measure performance at many of the particular tasks that topic models are well suited for, like data exploration, summarization, and visualization.", "labels": [], "entities": [{"text": "data exploration", "start_pos": 141, "end_pos": 157, "type": "TASK", "confidence": 0.806640088558197}, {"text": "summarization", "start_pos": 159, "end_pos": 172, "type": "TASK", "confidence": 0.9508833289146423}]}, {"text": "We therefore also include a metric that more directly measures the quality and interpretability of topics.", "labels": [], "entities": []}, {"text": "We use the topic coherence metric introduced by , which is based on co-occurrence statistics among each topic's most probable words and has been shown to correlate with human judgments of topic quality.", "labels": [], "entities": []}, {"text": "This metric measures the quality of each topic, and we  Note that this is a \"soft\" hierarchy because the tree structure is not strictly enforced, so some topics have multiple parent components.", "labels": [], "entities": []}, {"text": "shows how strict trees can be learned by tuning the annealing parameter.", "labels": [], "entities": []}, {"text": "measure the average coherence across all topics: where DF (v, w) is the document frequency of words v and w (the number of documents in which they both occur), DF (v) is the document frequency of word v, and v ki is the ith most probable word in topic k.", "labels": [], "entities": [{"text": "DF", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9804869294166565}, {"text": "DF (v)", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9461841583251953}]}, {"text": "We use the top M = 20 words.", "labels": [], "entities": []}, {"text": "This metric is limited to measuring only the quality of word clusters, ignoring the potentially improved interpretability of organizing the data into certain structures.", "labels": [], "entities": []}, {"text": "However, it is still useful as an alternative measure of performance and utility, independent of the models' predictive abilities.", "labels": [], "entities": []}, {"text": "Using these three metrics, we compared to several variants (denoted in bold) of the full model to understand how the different parts of the model affect performance: \u2022 Variants that contain the hierarchy components but not the perspective component (Hierarchy only), and vice versa (Perspective only).", "labels": [], "entities": []}, {"text": "\u2022 The \"hierarchy only\" model using only document components \u03b4 and no topic components.", "labels": [], "entities": []}, {"text": "This is a PAM-style model because it exhibits similar behavior to PAM ( \u00a74.4).", "labels": [], "entities": []}, {"text": "We also compared to the original PAM model.", "labels": [], "entities": []}, {"text": "\u2022 The \"hierarchy only\" model using only topic components \u03c9 and no document components.", "labels": [], "entities": []}, {"text": "This is a SCTM-style model because it exhibits similar behavior to SCTM ( \u00a74.2).", "labels": [], "entities": []}, {"text": "\u2022 The full model where \u03b1 (P ) is learned rather than given as input.", "labels": [], "entities": []}, {"text": "This is a FLDA-style model that has similar behavior to FLDA ( \u00a74.3).", "labels": [], "entities": [{"text": "FLDA", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.8671702742576599}]}, {"text": "We also compared to the original FLDA model.", "labels": [], "entities": [{"text": "FLDA", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.8581476807594299}]}, {"text": "\u2022 The \"perspective only\" model but without the \u03c9 (P ) topic component, so the attribute value affects only the topic distributions and not the word distributions.", "labels": [], "entities": []}, {"text": "This is identical to the DMR model of Mimno and McCallum (2008) ( \u00a74.5).", "labels": [], "entities": []}, {"text": "\u2022 A model with no components except for the bias vectors \u03c9 (B) and \u03b4 (B) . This is equivalent to LDA with optimized hyperparameters (learned).", "labels": [], "entities": []}, {"text": "We also experimented with using fixed symmetric hyperparameters, using values suggested by: 50/K and 0.01 for topic and word distributions.", "labels": [], "entities": []}, {"text": "To put the results in context, we also compare to two types of baselines: (1) \"bag of words\" baselines, where we measure the perplexity of add-one smoothed unigram language models, we measure: Perplexity of held-out tokens and mean absolute error for attribute prediction using various models (\u00b1 std. error).", "labels": [], "entities": [{"text": "mean absolute error", "start_pos": 227, "end_pos": 246, "type": "METRIC", "confidence": 0.7901571393013}, {"text": "attribute prediction", "start_pos": 251, "end_pos": 271, "type": "TASK", "confidence": 0.6942594647407532}]}, {"text": "\u2020 indicates significant improvement (p < 0.05) over optimized LDA under a two-sided t-test.", "labels": [], "entities": []}, {"text": "the prediction error using bag of words features, and we measure coherence of the unigram distribution; (2) naive baselines, where we measure the perplexity of the uniform distribution over each dataset's vocabulary, the prediction error when simply predicting each attribute as the mean value in the training set, and the coherence of 20 randomly selected words (repeated for 10 trials).", "labels": [], "entities": []}, {"text": "shows that the full SPRITE model substantially outperforms the LDA baseline at both predictive tasks.", "labels": [], "entities": [{"text": "SPRITE", "start_pos": 20, "end_pos": 26, "type": "TASK", "confidence": 0.7850415110588074}]}, {"text": "Generally, model variants with more structure perform better predictively.", "labels": [], "entities": []}, {"text": "The difference between SCTM-style and PAM-style is that the former uses only topic components (for word distributions) and the latter uses only document components (for the topic distributions).", "labels": [], "entities": []}, {"text": "Results show that the structured priors are more important for topic than word distributions, since PAM-style has lower perplexity on both datasets.", "labels": [], "entities": []}, {"text": "However, models with both topic and document components generally outperform either alone, including comparing the Perspective only and DMR models.", "labels": [], "entities": []}, {"text": "The former includes both topic and document perspective components, while DMR has only a document level component.", "labels": [], "entities": [{"text": "DMR", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.7617241144180298}]}, {"text": "PAM does not significantly outperform optimized LDA inmost measures, likely because it updates the hyperparameters using a moment-based approximation, which is less accurate than our gradient-based optimization.", "labels": [], "entities": []}, {"text": "FLDA perplexity is 2.3% higher than optimized LDA on Reviews, comparable to the 4% reported by Paul and Dredze (2012) on a different corpus.", "labels": [], "entities": [{"text": "FLDA", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6950472593307495}]}, {"text": "The FLDA-style SPRITE variant, which is more flexible, significantly outperforms FLDA inmost measures.", "labels": [], "entities": [{"text": "FLDA-style SPRITE", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.35646553337574005}, {"text": "FLDA", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.6859378814697266}]}, {"text": "The results are quite different under the coherence metric.", "labels": [], "entities": []}, {"text": "It seems that topic components (which influence the word distributions) improve coherence over LDA, while document components worsen coherence.", "labels": [], "entities": []}, {"text": "SCTM-style (which uses only topic components) does the best in both datasets, while PAM-style (which uses only documents) does the worst.", "labels": [], "entities": []}, {"text": "PAM also significantly improves over LDA, despite worse perplexity.", "labels": [], "entities": [{"text": "PAM", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9016071557998657}]}, {"text": "The LDA (learned) baseline substantially outperforms LDA (fixed) in all cases, highlighting the importance of optimizing hyperparameters, consistent with prior research ().", "labels": [], "entities": []}, {"text": "Surprisingly, many SPRITE variants also outperform the bag of words regression baseline, even though the latter was tuned to optimize performance using heavy 2 regularization, which we applied only weakly (without tuning) to the topic model features.", "labels": [], "entities": []}, {"text": "We also point out that the \"bag of words\" version of the coherence metric (the coherence of the top 20 words) is higher than the average topic coherence, which is an artifact of how the metric is defined: the most probable words in the corpus also tend to co-occur together inmost documents, so these words are considered to be highly coherent when grouped together.", "labels": [], "entities": []}, {"text": "Parameter Sensitivity We evaluated the full model at the two predictive tasks with varying numbers of topics ({12,25,50,100} for Debates and {5,10,20,40} for Reviews) and components ({2,5,10,20}).", "labels": [], "entities": []}, {"text": "shows that performance is more sensitive to the number of topics than components, with generally less variance among the latter.", "labels": [], "entities": []}, {"text": "More topics improve performance monotonically on Debates, while performance declines at 40 topics on Reviews.", "labels": [], "entities": []}, {"text": "The middle range of components (5-10) tends to perform better than too few (2) or too many  .88 .92 Figure 5: Predictive performance of full model with different numbers of topics K across different numbers of components, represented on the x-axis (log scale).", "labels": [], "entities": []}, {"text": "choice of parameters may depend on the end application and the particular structures that the user has in mind, if interpretability is important.", "labels": [], "entities": []}, {"text": "For example, if the topic model is used as a visualization tool, then 2 components would not likely result in an interesting hierarchy to the user, even if this setting produces low perplexity.", "labels": [], "entities": []}, {"text": "Structured Sparsity We use a relaxation of the binary b that induces a \"soft\" tree structure.", "labels": [], "entities": []}, {"text": "shows the percentage of b values which are within = .001 of 0 or 1 under various annealing schedules, increasing the inverse temperature \u03c4 by 0.1% after each iteration (i.e. \u03c4 t = 1.001 t ) as well as 0.3% and no annealing at all (\u03c4 = 1).", "labels": [], "entities": [{"text": "inverse temperature \u03c4", "start_pos": 117, "end_pos": 138, "type": "METRIC", "confidence": 0.9236999154090881}]}, {"text": "At \u03c4 = 0, we model a DAG rather than a tree, because the model has no preference that b is sparse.", "labels": [], "entities": []}, {"text": "Many of the values are binary in the DAG case, but the sparse prior substantially increases the number of binary values, obtaining fully binary structures with sufficient annealing.", "labels": [], "entities": []}, {"text": "We compare the DAG and tree structures more in the next subsection.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The percentage of indicator values that are sparse  (near 0 or 1) when using different annealing schedules.", "labels": [], "entities": []}, {"text": " Table 4: Quantitative results for different structures (columns) and different components (rows) for two metrics (\u00b1 std. error)  across three datasets. The best (structure, component) pair for each dataset and metric is in bold.", "labels": [], "entities": []}]}