{"title": [{"text": "A Sense-Topic Model for Word Sense Induction with Unsupervised Data Enrichment", "labels": [], "entities": [{"text": "Word Sense Induction", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7257076899210612}]}], "abstractContent": [{"text": "Word sense induction (WSI) seeks to automatically discover the senses of a word in a corpus via unsupervised methods.", "labels": [], "entities": [{"text": "Word sense induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8334149171908697}]}, {"text": "We propose a sense-topic model for WSI, which treats sense and topic as two separate latent variables to be inferred jointly.", "labels": [], "entities": [{"text": "WSI", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.7063226103782654}]}, {"text": "Topics are informed by the entire document, while senses are informed by the local context surrounding the ambiguous word.", "labels": [], "entities": []}, {"text": "We also discuss unsu-pervised ways of enriching the original corpus in order to improve model performance, including using neural word embeddings and external corpora to expand the context of each data instance.", "labels": [], "entities": []}, {"text": "We demonstrate significant improvements over the previous state-of-the-art, achieving the best results reported to date on the SemEval-2013 WSI task.", "labels": [], "entities": [{"text": "SemEval-2013 WSI task", "start_pos": 127, "end_pos": 148, "type": "TASK", "confidence": 0.5521486600240072}]}], "introductionContent": [{"text": "Word sense induction (WSI) is the task of automatically discovering all senses of an ambiguous word in a corpus.", "labels": [], "entities": [{"text": "Word sense induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8185131351153055}]}, {"text": "The inputs to WSI are instances of the ambiguous word with its surrounding context.", "labels": [], "entities": []}, {"text": "The output is a grouping of these instances into clusters corresponding to the induced senses.", "labels": [], "entities": []}, {"text": "WSI is generally conducted as an unsupervised learning task, relying on the assumption that the surrounding context of a word indicates its meaning.", "labels": [], "entities": [{"text": "WSI", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9709889888763428}]}, {"text": "Most previous work assumed that each instance is best labeled with a single sense, and therefore, that each instance belongs to exactly one sense cluster.", "labels": [], "entities": []}, {"text": "However, recent work ( has shown that more than one sense can be used to interpret certain instances, due to context ambiguity and sense relatedness.", "labels": [], "entities": []}, {"text": "To handle these characteristics of WSI (unsupervised, senses represented by token clusters, multiple senses per instance), we consider approaches based on topic models.", "labels": [], "entities": []}, {"text": "A topic model is an unsupervised method that discovers the semantic topics underlying a collection of documents.", "labels": [], "entities": []}, {"text": "The most popular is latent Dirichlet allocation (LDA;, in which each topic is represented as a multinomial distribution over words, and each document is represented as a multinomial distribution over topics.", "labels": [], "entities": [{"text": "latent Dirichlet allocation", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.5692994991938273}]}, {"text": "One approach would be to run LDA on the instances for an ambiguous word, then simply interpret topics as induced senses.", "labels": [], "entities": []}, {"text": "However, while sense and topic are related, they are distinct linguistic phenomena.", "labels": [], "entities": []}, {"text": "Topics are assigned to entire documents and are expressed by all word tokens, while senses relate to a single ambiguous word and are expressed through the local context of that word.", "labels": [], "entities": []}, {"text": "One possible approach would be to only keep the local context of each ambiguous word, discarding the global context.", "labels": [], "entities": []}, {"text": "However, the topical information contained in the broader context, though it may not determine the sense directly, might still be useful for narrowing down the likely senses of the ambiguous word.", "labels": [], "entities": []}, {"text": "Consider the ambiguous word cold.", "labels": [], "entities": []}, {"text": "In the sentence \"His reaction to the experiments was cold\", the possible senses for cold include cold temperature, a cold sensation, common cold, or a negative emotional reaction.", "labels": [], "entities": []}, {"text": "However, if we know that the topic of the document concerns the effects of low temperatures on physical health, then the negative emotional reaction sense should become less likely.", "labels": [], "entities": []}, {"text": "Therefore, in this case, knowing the topic helps narrow down the set of plausible senses.", "labels": [], "entities": []}, {"text": "At the same time, knowing the sense can also help determine possible topics.", "labels": [], "entities": []}, {"text": "Consider a set of texts that all include the word cold.", "labels": [], "entities": []}, {"text": "Without further information, the texts might discuss any of a number of possible topics.", "labels": [], "entities": []}, {"text": "However, if the sense of cold is that of cold ischemia, then the most probable topics would be those related to organ transplantation.", "labels": [], "entities": [{"text": "organ transplantation", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.7498492896556854}]}, {"text": "In this paper, we propose a sense-topic model for WSI, which treats sense and topic as two separate latent variables to be inferred jointly ( \u00a74).", "labels": [], "entities": [{"text": "WSI", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.7287481427192688}]}, {"text": "When relating the sense and topic variables, a bidirectional edge is drawn between them to represent their cyclic dependence ().", "labels": [], "entities": []}, {"text": "We perform inference using collapsed Gibbs sampling ( \u00a74.2), then estimate the sense distribution for each instance as the solution to the WSI task.", "labels": [], "entities": [{"text": "WSI task", "start_pos": 139, "end_pos": 147, "type": "TASK", "confidence": 0.7483811676502228}]}, {"text": "We conduct experiments on the SemEval-2013 Task 13 WSI dataset, showing improvements over several strong baselines and task systems ( \u00a75).", "labels": [], "entities": [{"text": "SemEval-2013 Task 13 WSI dataset", "start_pos": 30, "end_pos": 62, "type": "DATASET", "confidence": 0.6923457503318786}]}, {"text": "We also present unsupervised ways of enriching our dataset, including using neural word embeddings () and external Web-scale corpora to enrich the context of each data instance or to add more instances ( \u00a76).", "labels": [], "entities": []}, {"text": "Each data enrichment method gives further gains, resulting in significant improvements over existing state-of-the-art WSI systems.", "labels": [], "entities": []}, {"text": "Overall, we find gains of up to 22% relative improvement in fuzzy B-cubed and 50% relative improvement in fuzzy normalized mutual information ().", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our sense-topic model and compare it to several strong baselines and stateof-the-art systems.", "labels": [], "entities": []}, {"text": "Evaluation Metrics To evaluate WSI systems, Jurgens and Klapaftis (2013) propose two metrics: fuzzy B-cubed and fuzzy normalized mutual information (NMI).", "labels": [], "entities": [{"text": "WSI", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.983836829662323}]}, {"text": "They are each computed separately for each target word, then averaged across target words.", "labels": [], "entities": []}, {"text": "Fuzzy B-cubed prefers labeling all instances with the same sense, while fuzzy NMI prefers the opposite extreme of labeling all instances with distinct senses.", "labels": [], "entities": []}, {"text": "Hence, we report both fuzzy B-cubed (%) and fuzzy NMI (%) in our evaluation.", "labels": [], "entities": [{"text": "B-cubed", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.791893720626831}, {"text": "NMI", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.645462691783905}]}, {"text": "For ease of comparison, we also report the geometric mean of the 2 metrics, which we denote by AVG.", "labels": [], "entities": [{"text": "AVG", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9950000047683716}]}, {"text": "SemEval-2013 Task 13 also provided atrial dataset (TRIAL) that consists of eight target ambiguous words, each with 50 instances ).", "labels": [], "entities": [{"text": "SemEval-2013 Task 13", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.782825748125712}, {"text": "TRIAL", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9370143413543701}]}, {"text": "We use it for preliminary experiments of our model and for tuning certain hyperparameters, and evaluate final performance on the SemEval-2013 dataset (TEST) with 50 target words.", "labels": [], "entities": [{"text": "SemEval-2013 dataset (TEST)", "start_pos": 129, "end_pos": 156, "type": "DATASET", "confidence": 0.8522783577442169}]}, {"text": "Hyperparameter Tuning We use TRIAL to analyze performance of our sense-topic model under different settings for the numbers of senses (S) and topics (T ); see.", "labels": [], "entities": [{"text": "TRIAL", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9224387407302856}]}, {"text": "We always set T = 2S for simplicity.", "labels": [], "entities": [{"text": "T", "start_pos": 14, "end_pos": 15, "type": "METRIC", "confidence": 0.9581878781318665}]}, {"text": "We find that small S values work best, which is unsurprising considering the relatively small number of instances and small size of each instance.", "labels": [], "entities": []}, {"text": "When evaluating on TEST, we use S = 3 (which gives the best AVG results on TRIAL).", "labels": [], "entities": [{"text": "AVG", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9952274560928345}, {"text": "TRIAL", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.616436779499054}]}, {"text": "Later, when we add larger context or more instances (see \u00a76), tuning on TRIAL chooses a larger S value.", "labels": [], "entities": [{"text": "TRIAL", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.6851583123207092}]}, {"text": "During inference, the Gibbs sampler was run for 4,000 iterations for each target word, setting the first 500 iterations as the burn-in period.", "labels": [], "entities": []}, {"text": "In order to get a representative set of samples, every 13th sample (after burn-in) is saved to prevent correlations among samples.", "labels": [], "entities": []}, {"text": "Due to the randomized nature of the inference procedure, all reported results are average scores over 5 runs.", "labels": [], "entities": []}, {"text": "The hyperparameters (\u03b1) for all Dirichlet priors in our model are set to the (untuned) value of 0.01, following prior work on topic modeling ().", "labels": [], "entities": []}, {"text": "Baselines We include two na\u00a8\u0131vena\u00a8\u0131ve baselines corresponding to the two extremes (biases) preferred by fuzzy B-cubed and NMI, respectively: 1 sense (label each instance with the same single sense) and all distinct (label each instance with its own sense).", "labels": [], "entities": []}, {"text": "We also consider two baselines based on LDA.", "labels": [], "entities": []}, {"text": "We run LDA for each target word in TEST, using the set of instances as the set of documents.", "labels": [], "entities": []}, {"text": "We treat the learned topics as induced senses.", "labels": [], "entities": []}, {"text": "When setting the number of topics (senses), we use the gold-standard number of senses for each target word, making this baseline unreasonably strong.", "labels": [], "entities": []}, {"text": "We run LDA both with full context (FULL) and local context (LOCAL), using the same window size as above (10 words before and after the target word).", "labels": [], "entities": [{"text": "FULL", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9224043488502502}]}, {"text": "We also present results for the two best systems in the SemEval-2013 task (according to fuzzy Bcubed and fuzzy NMI, respectively): unimelb and AI-KU.", "labels": [], "entities": []}, {"text": "As described in Section 2, unimelb uses hierarchical Dirichlet processes (HDPs).", "labels": [], "entities": []}, {"text": "It extracts 50,000 extra instances for each target word as training data from the ukWac corpus\u2212a web corpus of approximately 2 billion tokens.", "labels": [], "entities": [{"text": "ukWac corpus\u2212a web corpus", "start_pos": 82, "end_pos": 107, "type": "DATASET", "confidence": 0.8215375045935313}]}, {"text": "Among all systems in the task, it performs best according to fuzzy Bcubed.", "labels": [], "entities": []}, {"text": "AI-KU is based on a lexical substitution method; a language model is built to identify lexical substitutes for target words from the dataset and the ukWac corpus.", "labels": [], "entities": [{"text": "ukWac corpus", "start_pos": 149, "end_pos": 161, "type": "DATASET", "confidence": 0.9931066632270813}]}, {"text": "It performed best among all systems according to fuzzy NMI.", "labels": [], "entities": []}, {"text": "Results In, we present results for these systems and compare them to our basic (i.e., without any data enrichment) sense-topic model with S = 3 (row 9).", "labels": [], "entities": []}, {"text": "According to both fuzzy B-cubed and fuzzy NMI, our model outperforms the other WSI systems (LDA, AI-KU, and unimelb).", "labels": [], "entities": []}, {"text": "Hence, we are able to achieve state-of-the-art results on the SemEval-2013 task even when only using the single sentence of context given in each instance (while AI-KU and unimelb use large training sets from ukWac).", "labels": [], "entities": [{"text": "ukWac", "start_pos": 209, "end_pos": 214, "type": "DATASET", "confidence": 0.9761457443237305}]}, {"text": "We found similar performance improvements when only tested on instances labeled with a single sense.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on TRIAL for the sense-topic  model with different numbers of senses (S). Best score  in each column is bold.", "labels": [], "entities": [{"text": "TRIAL", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.7179070711135864}]}, {"text": " Table 2: Performance on TEST for baselines and our sense-topic model. Best score in each column is bold.", "labels": [], "entities": [{"text": "TEST", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.5981165170669556}]}, {"text": " Table 3: Performance on TEST for the sense-topic model  with ablation of links between sense and topic variables.", "labels": [], "entities": [{"text": "TEST", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.6263375282287598}]}]}