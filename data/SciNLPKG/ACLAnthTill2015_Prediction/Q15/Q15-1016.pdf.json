{"title": [{"text": "Improving Distributional Similarity with Lessons Learned from Word Embeddings", "labels": [], "entities": [{"text": "Improving Distributional Similarity", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8407351573308309}]}], "abstractContent": [{"text": "Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks.", "labels": [], "entities": [{"text": "analogy detection tasks", "start_pos": 154, "end_pos": 177, "type": "TASK", "confidence": 0.82948237657547}]}, {"text": "We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations , rather than the embedding algorithms themselves.", "labels": [], "entities": []}, {"text": "Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains.", "labels": [], "entities": []}, {"text": "In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding the meaning of a word is at the heart of natural language processing (NLP).", "labels": [], "entities": [{"text": "Understanding the meaning of a word", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8531928062438965}, {"text": "natural language processing (NLP)", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.7569486300150553}]}, {"text": "While a deep, human-like, understanding remains elusive, many methods have been successful in recovering certain aspects of similarity between words.", "labels": [], "entities": []}, {"text": "Recently, neural-network based approaches in which words are \"embedded\" into a lowdimensional space were proposed by various authors ().", "labels": [], "entities": []}, {"text": "These models represent each word as a ddimensional vector of real numbers, and vectors that are close to each other are shown to be semantically related.", "labels": [], "entities": []}, {"text": "In particular, a sequence of papers by culminated in the skip-gram with negative-sampling training method (SGNS): an efficient embedding algorithm that provides state-of-the-art results on various linguistic tasks.", "labels": [], "entities": []}, {"text": "It was popularized via word2vec, a program for creating word embeddings.", "labels": [], "entities": []}, {"text": "A recent study by conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see and for comprehensive surveys).", "labels": [], "entities": []}, {"text": "These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks.", "labels": [], "entities": []}, {"text": "However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words.", "labels": [], "entities": []}, {"text": "Furthermore, analysis by shows that word2vec's SGNS is implicitly factorizing a word-context PMI matrix.", "labels": [], "entities": []}, {"text": "That is, the mathematical objective and the sources of information available to SGNS are in fact very similar to those employed by the more traditional methods.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 80, "end_pos": 84, "type": "TASK", "confidence": 0.9195939898490906}]}, {"text": "What, then, is the source of superiority (or perceived superiority) of these recent embeddings?", "labels": [], "entities": []}, {"text": "While the focus of the presentation in the wordembedding literature is on the mathematical model and the objective being optimized, other factors affect the results as well.", "labels": [], "entities": []}, {"text": "In particular, embedding algorithms suggest some natural hyperparameters that can be tuned; many of which were already tuned to some extent by the algorithms' designers.", "labels": [], "entities": []}, {"text": "Some hyperparameters, such as the number of negative samples to use, are clearly marked as tunable.", "labels": [], "entities": []}, {"text": "Other modifications, such as smoothing the negative-sampling distribution, are reported in passing and considered thereafter as part of the algorithm.", "labels": [], "entities": []}, {"text": "Others still, such as dynamically-sized context windows, are not even mentioned in some of the papers, but are part of the canonical implementation.", "labels": [], "entities": []}, {"text": "All of these modifications and system design choices, which we collectively denote as hyperparameters, are part of the final algorithm, and, as we show, have a substantial impact on performance.", "labels": [], "entities": []}, {"text": "In this work, we make these hyperparameters explicit, and show how they can be adapted and transferred into the traditional count-based approach.", "labels": [], "entities": []}, {"text": "To asses how each hyperparameter contributes to the algorithms' performance, we conduct a comprehensive set of experiments and compare four different representation methods, while controlling for the various hyperparameters.", "labels": [], "entities": []}, {"text": "Once adapted across methods, hyperparameter tuning significantly improves performance in every task.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.6735346764326096}]}, {"text": "In many cases, changing the setting of a single hyperparameter yields a greater increase in performance than switching to a better algorithm or training on a larger corpus.", "labels": [], "entities": []}, {"text": "In particular, word2vec's smoothing of the negative sampling distribution can be adapted to PPMI-based methods by introducing a novel, smoothed variant of the PMI association measure (see Section 3.2).", "labels": [], "entities": []}, {"text": "Using this variant increases performance by over 3 points per task, on average.", "labels": [], "entities": []}, {"text": "We suspect that this smoothing partially addresses the \"Achilles' heel\" of PMI: its bias towards cooccurrences of rare words.", "labels": [], "entities": [{"text": "PMI", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.8492558598518372}]}, {"text": "We also show that when all methods are allowed to tune a similar set of hyperparameters, their performance is largely comparable.", "labels": [], "entities": []}, {"text": "In fact, there is no consistent advantage to one algorithmic approach over another, a result that contradicts the claim that embeddings are superior to count-based methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We explored a large space of hyperparameters, representations, and evaluation datasets.", "labels": [], "entities": []}, {"text": "We evaluated each word representation on eight datasets covering similarity and analogy tasks.", "labels": [], "entities": []}, {"text": "Analogy The two analogy datasets present questions of the form \"a is to a * as b is to b * \", where b * is hidden, and must be guessed from the entire vocabulary.", "labels": [], "entities": []}, {"text": "MSR's analogy dataset () contains 8000 morpho-syntactic analogy questions, such as \"good is to best as smart is to smartest\".", "labels": [], "entities": [{"text": "MSR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7919688820838928}]}, {"text": "Google's analogy dataset () contains 19544 questions, about half of the same kind as in MSR (syntactic analogies), and another half of a more semantic nature, such as capital cities (\"Paris is to France as Tokyo is to Japan\").", "labels": [], "entities": []}, {"text": "After filtering questions involving outof-vocabulary words, i.e. words that appeared in English Wikipedia less than 100 times, we remain with 7118 instances in MSR and 19258 instances in Google.", "labels": [], "entities": [{"text": "MSR", "start_pos": 160, "end_pos": 163, "type": "DATASET", "confidence": 0.9673023223876953}, {"text": "Google", "start_pos": 187, "end_pos": 193, "type": "DATASET", "confidence": 0.9456480145454407}]}, {"text": "The analogy questions are answered using 3CosAdd (addition and subtraction):", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of each method across different tasks in the \"vanilla\" scenario (all hyperparameters set to default):  win = 2; dyn = none; sub = none; neg = 1; cds = 1; w+c = only w; eig = 0.0.", "labels": [], "entities": []}, {"text": " Table 3: Performance of each method across different tasks using word2vec's recommended configuration: win = 2;  dyn = with; sub = dirty; neg = 5; cds = 0.75; w+c = only w; eig = 0.0. CBOW is presented for comparison.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 185, "end_pos": 189, "type": "DATASET", "confidence": 0.8125748038291931}]}, {"text": " Table 4: Performance of each method across different tasks using the best configuration for that method and task combination,  assuming win = 2.", "labels": [], "entities": []}, {"text": " Table 5: Performance of each method across different tasks using 2-fold cross-validation for hyperparameter tuning. Configu- rations on large-scale (LS) corpora are also presented for comparison.", "labels": [], "entities": [{"text": "Configu- rations", "start_pos": 117, "end_pos": 133, "type": "METRIC", "confidence": 0.9025872349739075}]}, {"text": " Table 6: The average performance of SVD on word similarity  tasks given different values of eig, in the vanilla scenario.", "labels": [], "entities": [{"text": "word similarity  tasks", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7519257962703705}]}, {"text": " Table 7: The impact of each hyperparameter, measured by the number of tasks in which the best configuration had that hyper- parameter setting. Non-applicable combinations are marked by \"-\".", "labels": [], "entities": []}, {"text": " Table 8: The added value versus the risk of setting each hyperparameter. The figures show the differences in performance  between the best achievable configurations when restricting a hyperparameter to different values. This difference indicates the  potential gain of tuning a given hyperparameter, as well as the risks of decreased performance when not tuning it. For example,  an entry of +9.2% in", "labels": [], "entities": []}]}