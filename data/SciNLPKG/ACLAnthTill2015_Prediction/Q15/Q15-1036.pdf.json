{"title": [{"text": "Plato: A Selective Context Model for Entity Resolution", "labels": [], "entities": []}], "abstractContent": [{"text": "We present Plato, a probabilistic model for entity resolution that includes a novel approach for handling noisy or uninformative features, and supplements labeled training data derived from Wikipedia with a very large unlabeled text corpus.", "labels": [], "entities": [{"text": "entity resolution", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7943481802940369}]}, {"text": "Training and inference in the proposed model can easily be distributed across many servers, allowing it to scale to over 10 7 entities.", "labels": [], "entities": []}, {"text": "We evaluate Plato on three standard datasets for entity resolution.", "labels": [], "entities": [{"text": "entity resolution", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7904800176620483}]}, {"text": "Our approach achieves the best results to-date on TAC KBP 2011 and is highly competitive on both the CoNLL 2003 and TAC KBP 2012 datasets.", "labels": [], "entities": [{"text": "TAC KBP 2011", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.9206567605336508}, {"text": "CoNLL 2003 and TAC KBP 2012 datasets", "start_pos": 101, "end_pos": 137, "type": "DATASET", "confidence": 0.858718101467405}]}], "introductionContent": [{"text": "Given a document collection and a knowledge base (KB) of entities, entity resolution, also known as entity disambiguation or entity linking, is the process of mapping each entity mention in a document to the corresponding entity record in the KB (.", "labels": [], "entities": [{"text": "entity resolution", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7395410686731339}, {"text": "entity disambiguation or entity linking", "start_pos": 100, "end_pos": 139, "type": "TASK", "confidence": 0.7145721554756165}]}, {"text": "Entity resolution is challenging because referring expressions are often ambiguous on their own and can only be disambiguated by their surrounding context.", "labels": [], "entities": [{"text": "Entity resolution", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8860425353050232}]}, {"text": "Consider the name Newcastle; it can refer to the city of Newcastle upon Tyne in UK, to the football (soccer for US readers) club Newcastle United F.C., to a popular beverage (Newcastle Brown Ale), or to several other entities.", "labels": [], "entities": []}, {"text": "The ambiguity can only be resolved with appropriate context.", "labels": [], "entities": []}, {"text": "Another complicating factor is that no KB is complete, and so a name in a document may refer to an entity that is missing from the KB.", "labels": [], "entities": []}, {"text": "This problem is commonly called NIL detection.", "labels": [], "entities": [{"text": "NIL detection", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9047601222991943}]}, {"text": "In this paper we present a probabilistic model for entity resolution.", "labels": [], "entities": [{"text": "entity resolution", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.8058942556381226}]}, {"text": "Our system, hereafter referred to as Plato, is designed to be resilient to irrelevant features and can be seen as a selective extension of the na\u00a8\u0131vena\u00a8\u0131ve Bayes model.", "labels": [], "entities": []}, {"text": "Specifically, we assume that most of the context features of a mention are irrelevant to its disambiguation.", "labels": [], "entities": []}, {"text": "This contrasts with the na\u00a8\u0131vena\u00a8\u0131ve Bayes assumption that all features are generated from a class-conditional distribution and are thus all relevant to the class assignment.", "labels": [], "entities": []}, {"text": "Our empirical results support this modeling choice.", "labels": [], "entities": []}, {"text": "We train Plato in a semi-supervised manner, starting with labeled data derived from Wikipedia, and continuing with a very large unlabeled corpus of Web documents.", "labels": [], "entities": []}, {"text": "The use of unlabeled data enables us to obtain a better estimate of feature distributions and discover new features that are not present in the (labeled) training data.", "labels": [], "entities": []}, {"text": "Plato scales up easily to very large KBs with millions of entities and includes NIL detection as a natural by-product of inference.", "labels": [], "entities": [{"text": "NIL detection", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.7891804575920105}]}, {"text": "We named our system after the Greek philosopher because the system's inference of real underlying entities from imperfect evidence reminds us of Plato's Theory of Forms.", "labels": [], "entities": []}, {"text": "Previous entity resolution studies) have typically relied on three main components: a mention model, a context model, and a coherency model.", "labels": [], "entities": [{"text": "entity resolution", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7821589112281799}]}, {"text": "The mention model, perhaps the most important component (, estimates the prior belief that a particular phrase refers to a particular entity 503 in the KB.", "labels": [], "entities": [{"text": "KB", "start_pos": 152, "end_pos": 154, "type": "DATASET", "confidence": 0.9154698848724365}]}, {"text": "In addition to providing a prior, the mention model also helps efficient inference by giving zero probability to entities that are never referred to by a particular name.", "labels": [], "entities": []}, {"text": "The context model helps disambiguate the entity using the textual context of the mention.", "labels": [], "entities": []}, {"text": "This includes both features extracted from the immediate context (such as the enclosing sentence) and from the overall discourse (such as the most salient noun phrases in the document).", "labels": [], "entities": []}, {"text": "Finally, the coherency model encourages all referring expressions in a document to resolve to entities that are related to each other in the KB.", "labels": [], "entities": []}, {"text": "For example, a mention of Sunderland A.F.C.", "labels": [], "entities": []}, {"text": "(a rival football club to Newcastle United F.C.) may reduce the uncertainty about the mention Newcastle.", "labels": [], "entities": []}, {"text": "Since a coherency model introduces dependencies between the resolutions of all the mentions in a document, it is seen as a global model, while mention and context models are usually referred to as local ().", "labels": [], "entities": []}, {"text": "Coherency models typically have an increased inference cost, as they require access to the relevant entity relationships in the KB.", "labels": [], "entities": []}, {"text": "Plato does not include a full coherency component.", "labels": [], "entities": [{"text": "Plato", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8967262506484985}]}, {"text": "Instead, mentions in a given document are sorted into coreference clusters by a simple within-document coreference algorithm similar to that of.", "labels": [], "entities": []}, {"text": "Each coreference cluster is then resolved to the KB independently of the resolution of the other clusters in the document.", "labels": [], "entities": []}, {"text": "The context features for each mention cluster in our model include the names of other referring phrases in the document.", "labels": [], "entities": []}, {"text": "Since many referring phrases are unambiguous, our hypothesis is that such context can capture much of the discourse coherence usually represented by a coherency model.", "labels": [], "entities": []}, {"text": "Plato detects and links both nominal and named mentions, but following previous work, we evaluate it only on the resolution of gold named entity mentions to either KB or NIL.", "labels": [], "entities": []}, {"text": "We train Plato with expectation-maximization (EM), which is easily parallelizable and thus can scale up to very large KBs and unlabeled training corpora.", "labels": [], "entities": [{"text": "expectation-maximization (EM)", "start_pos": 20, "end_pos": 49, "type": "METRIC", "confidence": 0.6855005770921707}]}, {"text": "Indeed, our efficient distributed implementation allows the system to scale up to KBs with over 10 7 entities.", "labels": [], "entities": []}, {"text": "Plato achieves highly competitive results on several benchmarks: Most importantly, this performance is \"out-of-the-box\": we did not use any of the corresponding training sets, labeled or not, to train the model or tune hyperparameters.", "labels": [], "entities": []}], "datasetContent": [{"text": "Mention Prior We initialized the mention phrase parameters {\u03c4 e } from links in Wikipedia by counting how many times a given phrase w is used to refer to entity e, and normalizing appropriately.", "labels": [], "entities": []}, {"text": "We used the following sources to obtain (w, e) pairs for the above estimates: (a) w is the title of e's page after removing parenthesized disambiguation terms; (b) w is the title of a Wikipedia: Candidate generation recall on the three evaluation datasets: the percentage of linkable gold mentions for which the correct entity was in the set of candidates generated by Plato.", "labels": [], "entities": [{"text": "recall", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.9564698934555054}]}, {"text": "This is an upper bound on our in-KB accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9565970301628113}]}, {"text": "redirect page linking to e's page; (c) w is a Freebase alias (property /common/topic/alias) for Freebase topic e; (d) w is the title of a disambiguation page that links toe as a possible disambiguation.", "labels": [], "entities": []}, {"text": "For all the aliases obtained from the above sources, we used the Wikilinks corpus ( as an additional source of counts.", "labels": [], "entities": [{"text": "Wikilinks corpus", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.9571979343891144}]}, {"text": "In addition to the above sources, we also used anchors of Wikipedia pages as aliases if they occurred more than 500 times.", "labels": [], "entities": []}, {"text": "Unlabeled data was used to reestimate the parameters \u03c4 e using Equation 4; however, we did not introduce any new aliases.", "labels": [], "entities": [{"text": "Equation", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9643728733062744}]}, {"text": "Context Features To extract context features, all documents were processed as follows.", "labels": [], "entities": []}, {"text": "The free text in each document was POS-tagged and dependencyparsed using a parser that is a reimplementation of the MaltParser () with a linear kernel SVM.", "labels": [], "entities": []}, {"text": "When trained on Sections 02-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank (, our parser achieves an unlabeled attachment score (UAS) of 88.24 and a labeled attachment score (LAS) of 84.69 on WSJ Section 22.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) portion of the Penn Treebank (", "start_pos": 38, "end_pos": 94, "type": "DATASET", "confidence": 0.9178533752759298}, {"text": "unlabeled attachment score (UAS)", "start_pos": 119, "end_pos": 151, "type": "METRIC", "confidence": 0.8001365462938944}, {"text": "labeled attachment score (LAS)", "start_pos": 167, "end_pos": 197, "type": "METRIC", "confidence": 0.8757918973763784}, {"text": "WSJ Section 22", "start_pos": 210, "end_pos": 224, "type": "DATASET", "confidence": 0.9721623460451762}]}, {"text": "Named mentions (such as Barack Obama) and common noun phrases (such as the president) were identified using a simple rulebased tagger with rules over POS tag sequences and dependency parses.", "labels": [], "entities": []}, {"text": "We then used a withindocument coreference resolver comparable to that of to cluster all identified mentions.", "labels": [], "entities": []}, {"text": "As context features in our model, we used the phrases of all mentions in each coreference cluster in the document.", "labels": [], "entities": []}, {"text": "We did not differentiate between phrases corresponding to the same coreference cluster as the query string, and phrases in other clusters.", "labels": [], "entities": []}, {"text": "Adding other types of local features, such as words and phrases near the mentions in a cluster and dependency paths involving mentions in a cluster, did not lead to significant performance improvements in either our proposed model or na\u00a8\u0131vena\u00a8\u0131ve Bayes, and so we did not include them.", "labels": [], "entities": []}, {"text": "We initialized the context parameters {\u03c1 k } using only labeled data, and re-estimated them using unlabeled data, as detailed in Equation 5.", "labels": [], "entities": []}, {"text": "Inference To determine the set of candidate entities for each coreference cluster, we use the mention with the longest phrase in the cluster.", "labels": [], "entities": []}, {"text": "This phrase is used to generate the candidates which are then scored using Equation 2.", "labels": [], "entities": [{"text": "Equation", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9829661846160889}]}, {"text": "We copy the label of the highest-scoring entity to all mentions in the cluster.", "labels": [], "entities": []}, {"text": "Note that clusters will often include proper names mentions, referential common noun phrases, and referential pronouns.", "labels": [], "entities": []}, {"text": "Thus Plato detects and links both nominal and named mentions.", "labels": [], "entities": []}, {"text": "However, following most existing work, we only evaluate the resolution of gold named entity mentions to either KB or NIL.", "labels": [], "entities": [{"text": "resolution of gold named entity mentions", "start_pos": 60, "end_pos": 100, "type": "TASK", "confidence": 0.8219116032123566}, {"text": "NIL", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.6723961234092712}]}, {"text": "While in the case of CoNLL all gold mentions can be resolved to the KB, in TAC NIL is a valid label.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.8912481069564819}, {"text": "TAC NIL", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.7325680553913116}]}], "tableCaptions": [{"text": " Table 1: Candidate generation recall on the three evalu- ation datasets: the percentage of linkable gold mentions  for which the correct entity was in the set of candidates  generated by Plato. This is an upper bound on our in-KB  accuracy.", "labels": [], "entities": [{"text": "Candidate generation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7732213735580444}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.977681040763855}, {"text": "accuracy", "start_pos": 232, "end_pos": 240, "type": "METRIC", "confidence": 0.9116496443748474}]}, {"text": " Table 2: Mention-averaged accuracy on the CoNLL 2003 dataset in our experiments and previous best work. The  results of the best system are shown in bold-face.", "labels": [], "entities": [{"text": "Mention-averaged", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9905351996421814}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9084702134132385}, {"text": "CoNLL 2003 dataset", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.9672943353652954}]}, {"text": " Table 3: TAC KBP evaluation results for our model and previous highest-accuracy systems. The best results are  shown in bold-face; this includes the highest-accuracy system and systems whose performance was not statistically  significantly different, according to a two-tailed t-test with p = 0.05.", "labels": [], "entities": [{"text": "TAC KBP", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.43525098264217377}]}]}