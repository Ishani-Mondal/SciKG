{"title": [{"text": "Domain Adaptation for Syntactic and Semantic Dependency Parsing Using Deep Belief Networks", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6717294007539749}, {"text": "Syntactic and Semantic Dependency Parsing", "start_pos": 22, "end_pos": 63, "type": "TASK", "confidence": 0.6319020807743072}]}], "abstractContent": [{"text": "In current systems for syntactic and semantic dependency parsing, people usually define a very high-dimensional feature space to achieve good performance.", "labels": [], "entities": [{"text": "syntactic and semantic dependency parsing", "start_pos": 23, "end_pos": 64, "type": "TASK", "confidence": 0.658738660812378}]}, {"text": "But these systems often suffer severe performance drops on out-of-domain test data due to the diversity of features of different domains.", "labels": [], "entities": []}, {"text": "This paper fo-cuses on how to relieve this domain adaptation problem with the help of unlabeled target domain data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7660780847072601}]}, {"text": "We propose a deep learning method to adapt both syntactic and semantic parsers.", "labels": [], "entities": []}, {"text": "With additional unlabeled target domain data, our method can learn a latent feature representation (LFR) that is beneficial to both domains.", "labels": [], "entities": []}, {"text": "Experiments on English data in the CoNLL 2009 shared task show that our method largely reduced the performance drop on out-of-domain test data.", "labels": [], "entities": [{"text": "CoNLL 2009 shared task", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.8832483887672424}]}, {"text": "Moreover, we get a Macro F1 score that is 2.32 points higher than the best system in the CoNLL 2009 shared task in out-of-domain tests.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.882424920797348}, {"text": "CoNLL 2009 shared task", "start_pos": 89, "end_pos": 111, "type": "DATASET", "confidence": 0.8206019699573517}]}], "introductionContent": [{"text": "Both syntactic and semantic dependency parsing are the standard tasks in the NLP community.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.6402277946472168}]}, {"text": "The stateof-the-art model performs well if the test data comes from the domain of the training data.", "labels": [], "entities": []}, {"text": "But if the test data comes from a different domain, the performance drops severely.", "labels": [], "entities": []}, {"text": "The results of the shared tasks of) also substantiates the argument.", "labels": [], "entities": []}, {"text": "To relieve the domain adaptation, in this paper, we propose a deep learning method for both syntactic and semantic parsers.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7328363806009293}]}, {"text": "We focus on the situation that, besides source domain training data and target domain test data, we also have some unlabeled target domain data.", "labels": [], "entities": []}, {"text": "Many syntactic and semantic parsers are developed using a supervised learning paradigm, where each data sample is represented as a vector of features, usually a high-dimensional feature.", "labels": [], "entities": [{"text": "syntactic and semantic parsers", "start_pos": 5, "end_pos": 35, "type": "TASK", "confidence": 0.6496561765670776}]}, {"text": "The performance degradation on target domain test data is mainly caused by the diversity of features of different domains, i.e., many features in target domain test data are never seen in source domain training data.", "labels": [], "entities": []}, {"text": "Previous work have shown that using word clusters to replace the sparse lexicalized features (, helps relieve the performance degradation on the target domain.", "labels": [], "entities": []}, {"text": "But for syntactic and semantic parsing, people also use a lot of syntactic features, i.e., features extracted from syntactic trees.", "labels": [], "entities": [{"text": "syntactic and semantic parsing", "start_pos": 8, "end_pos": 38, "type": "TASK", "confidence": 0.6956425085663795}]}, {"text": "For example, the relation path between a predicate and an argument is a syntactic feature used in semantic dependency parsing.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.653745432694753}]}, {"text": "shows an example of this relation path feature.", "labels": [], "entities": []}, {"text": "Obviously, syntactic features like this are also very sparse and usually specific to each domain.", "labels": [], "entities": []}, {"text": "The method of clustering fails in generalizing these kinds of features.", "labels": [], "entities": []}, {"text": "Our method, however, is very different from clustering specific features and substituting these features using their clusters.", "labels": [], "entities": []}, {"text": "Instead, we attack the domain adaption problem by learning a latent feature representation (LFR) for different domains, which is similar to: A path feature example.", "labels": [], "entities": []}, {"text": "The red edges are the path between She and visit and thus the relation path feature between them is SBJ\u2191OPRD\u2193IM\u2193OBJ\u2193 based on the data sample's original feature vector.", "labels": [], "entities": [{"text": "SBJ", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9015499949455261}, {"text": "OPRD", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.8821784257888794}]}, {"text": "Our DBN model is trained unsupervisedly on original feature vectors of data in both domains: training data from the source domain, and unlabeled data from the target domain.", "labels": [], "entities": []}, {"text": "So our DBN model can produce a common feature representation for data from both domains.", "labels": [], "entities": []}, {"text": "A common feature representation can make two domains more similar and thus is very helpful for domain adaptation).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7515373826026917}]}, {"text": "Discriminative models using our latent features adapt better to the target domain than models using original features.", "labels": [], "entities": []}, {"text": "Discriminative models in syntactic and semantic parsers usually use millions of features.", "labels": [], "entities": [{"text": "syntactic and semantic parsers", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.6530574783682823}]}, {"text": "Applying atypical DBN to learn a sensible LFR on that many original features is computationally too expensive and impractical).", "labels": [], "entities": []}, {"text": "Therefore, we constrain the DBN by splitting the original features into groups.", "labels": [], "entities": []}, {"text": "In this way, we largely reduce the computational cost and make LFR learning practical.", "labels": [], "entities": []}, {"text": "We carried out experiments on the English data of the CoNLL 2009 shared task.", "labels": [], "entities": [{"text": "English data of the CoNLL 2009 shared task", "start_pos": 34, "end_pos": 76, "type": "DATASET", "confidence": 0.8360204547643661}]}, {"text": "We use a basic pipelined system and compare the effectiveness of the two feature representations: original feature representation and our LFR.", "labels": [], "entities": []}, {"text": "Using the original features, the performance drop on out-of-domain test data is 10.58 points in Macro F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9215467274188995}]}, {"text": "In contrast, using the LFR, the performance drop is only 4.97 points.", "labels": [], "entities": [{"text": "LFR", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.7460820078849792}]}, {"text": "And we have achieved a Macro F1 score of 80.83% on the out-of-domain test data.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.926937609910965}]}, {"text": "As far as we know, this is the best result on this data set to date.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the English data in the CoNLL 2009 shared task for experiments.", "labels": [], "entities": [{"text": "CoNLL 2009 shared task", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.9103037863969803}]}, {"text": "The training data and in-domain test data are from the WSJ corpus, whereas the out-of-domain test data is from the Brown corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.9731714129447937}, {"text": "Brown corpus", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.9235386252403259}]}, {"text": "We also use unlabeled data consisting of the following sections of the Brown corpus: K, L, M, N, P.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.9624709188938141}]}, {"text": "The test data are excerpts from fictions.", "labels": [], "entities": []}, {"text": "The unlabeled data are also excerpts from fictions or stories, which are similar to the test data.", "labels": [], "entities": []}, {"text": "Although the unlabeled data is actually annotated in Release 3 of the Penn Treebank, we do not use any information contained in the annotation, only using the raw texts.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.9831313192844391}]}, {"text": "The training, test and unlabeled data contains 39279, 425, and 16407 sentences respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The results of our basic and adapted systems", "labels": [], "entities": []}, {"text": " Table 2: Results of different splitting configurations on  in-domain WSJ development data", "labels": [], "entities": [{"text": "WSJ development data", "start_pos": 70, "end_pos": 90, "type": "DATASET", "confidence": 0.7181323965390524}]}, {"text": " Table 3: Results of different splitting configurations on  out-of-domain Brown test data", "labels": [], "entities": []}, {"text": " Table 4: Comparison with other methods.", "labels": [], "entities": []}]}