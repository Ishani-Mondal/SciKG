{"title": [{"text": "Combining Minimally-supervised Methods for Arabic Named Entity Recognition", "labels": [], "entities": [{"text": "Arabic Named Entity Recognition", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.6331455186009407}]}], "abstractContent": [{"text": "Supervised methods can achieve high performance on NLP tasks, such as Named Entity Recognition (NER), but new annotations are required for every new domain and/or genre change.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.8132456044356028}]}, {"text": "This has motivated research in minimally supervised methods such as semi-supervised learning and distant learning, but neither technique has yet achieved performance levels comparable to those of supervised methods.", "labels": [], "entities": []}, {"text": "Semi-supervised methods tend to have very high precision but comparatively low recall, whereas distant learning tends to achieve higher recall but lower precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9978179931640625}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9989149570465088}, {"text": "recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9978394508361816}, {"text": "precision", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9932979941368103}]}, {"text": "This complementarity suggests that better results maybe obtained by combining the two types of minimally supervised methods.", "labels": [], "entities": []}, {"text": "In this paper we present a novel approach to Arabic NER using a combination of semi-supervised and distant learning techniques.", "labels": [], "entities": [{"text": "Arabic NER", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.4944060742855072}]}, {"text": "We trained a semi-supervised NER classifier and another one using distant learning techniques, and then combined them using a variety of classifier combination schemes, including the Bayesian Classifier Combination (BCC) procedure recently proposed for sentiment analysis.", "labels": [], "entities": [{"text": "NER classifier", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.8678329885005951}, {"text": "sentiment analysis", "start_pos": 253, "end_pos": 271, "type": "TASK", "confidence": 0.961257815361023}]}, {"text": "According to our results, the BCC model leads to an increase in performance of 8 percentage points over the best base classifiers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised learning techniques are very effective and widely used to solve many NLP problems, including NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9241229891777039}]}, {"text": "The main disadvantage of supervised techniques, however, is the need fora large annotated corpus.", "labels": [], "entities": []}, {"text": "Although a considerable amount of annotated data is available for many languages, including Arabic, changing the domain or expanding the set of classes always requires domain-specific experts and new annotated data, both of which demand time and effort.", "labels": [], "entities": []}, {"text": "Therefore, much of the current research on NER focuses on approaches that require minimal human intervention to export the named entity (NE) classifiers to new domains and to expand NE classes).", "labels": [], "entities": [{"text": "NER", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9445209503173828}]}, {"text": "Semi-supervised and distant learning approaches () are alternatives to supervised methods that do not require manually annotated data.", "labels": [], "entities": []}, {"text": "These approaches have proved to be effective and easily adaptable to new NE types.", "labels": [], "entities": []}, {"text": "However, the performance of such methods tends to be lower than that achieved with supervised methods ().", "labels": [], "entities": []}, {"text": "We propose combining these two minimally supervised methods in order to exploit their respective strengths and thereby obtain better results.", "labels": [], "entities": []}, {"text": "Semisupervised learning tends to be more precise than distant learning, which in turn leads to higher recall than semi-supervised learning.", "labels": [], "entities": [{"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9989159107208252}]}, {"text": "In this work, we use various classifier combination schemes to combine the minimal supervision methods.", "labels": [], "entities": []}, {"text": "Most previous studies have examined classifier combination schemes to combine multiple supervisedlearning systems (, but this research is the first to combine minimal supervision approaches.", "labels": [], "entities": []}, {"text": "In addition, we report our results from testing the recently proposed Independent Bayesian Classifier Combination (IBCC) scheme) and comparing it with traditional voting methods for ensemble combination.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the IBCC model, the validation data was used as known ti to ground the estimates of model parameters.", "labels": [], "entities": [{"text": "IBCC model", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9020114243030548}]}, {"text": "The hyper-parameters were set as \u03b1 (k) j = 1 and \u03bd j = 1).", "labels": [], "entities": []}, {"text": "The initial values for random variables were set as follows: (a) the class proportion \u03b4 was initialised to the result of counting ti and (b) the confusion matrix \u03c0 was initialised to the result of counting ti and the output of each classifier c (k) . Gibbs sampling was run well past stability (i.e., 1000 iterations).", "labels": [], "entities": []}, {"text": "Stability was actually reached in approximately 100 iterations.", "labels": [], "entities": []}, {"text": "All parameters required in voting methods were specified using the validation set.", "labels": [], "entities": []}, {"text": "We examined two different voting methods: equal voting and weighted voting.", "labels": [], "entities": []}, {"text": "In the case of equal voting, each classifier was given an equal weight, (1/K) where K was the number of classifiers to be combined.", "labels": [], "entities": []}, {"text": "In weighted voting, total precision was used in order to give preference to classifiers with good quality.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.6338289976119995}]}], "tableCaptions": [{"text": " Table 1: The results of SSL and DL classifiers on the  ANERcorp test set.", "labels": [], "entities": [{"text": "ANERcorp test set", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.9178619384765625}]}, {"text": " Table 2: The results of the baseline", "labels": [], "entities": []}, {"text": " Table 3: The performances of various combination meth- ods.", "labels": [], "entities": []}, {"text": " Table 4: The performances of various combination meth- ods when restricting the combination process.", "labels": [], "entities": []}, {"text": " Table 5: The sign test results (exact p values) for the pair- wise comparisons of the combination methods.", "labels": [], "entities": []}]}