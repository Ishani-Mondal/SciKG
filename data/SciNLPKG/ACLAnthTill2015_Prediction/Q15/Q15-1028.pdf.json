{"title": [{"text": "Unsupervised Lexicon Discovery from Acoustic Input", "labels": [], "entities": [{"text": "Unsupervised Lexicon Discovery", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5886534154415131}]}], "abstractContent": [{"text": "We present a model of unsupervised phono-logical lexicon discovery-the problem of simultaneously learning phoneme-like and word-like units from acoustic input.", "labels": [], "entities": [{"text": "unsupervised phono-logical lexicon discovery-the", "start_pos": 22, "end_pos": 70, "type": "TASK", "confidence": 0.7430361807346344}]}, {"text": "Our model builds on earlier models of unsuper-vised phone-like unit discovery from acoustic data (Lee and Glass, 2012), and unsuper-vised symbolic lexicon discovery using the Adaptor Grammar framework (Johnson et al., 2006), integrating these earlier approaches using a probabilistic model of phonological variation.", "labels": [], "entities": [{"text": "phone-like unit discovery", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.7599610090255737}, {"text": "unsuper-vised symbolic lexicon discovery", "start_pos": 124, "end_pos": 164, "type": "TASK", "confidence": 0.6720385253429413}]}, {"text": "We show that the model is competitive with state-of-the-art spoken term discovery systems, and present analyses exploring the model's behavior and the kinds of linguistic structures it learns.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the most basic problems of language acquisition is accounting for how children learn the inventory of word forms from speech-phonological lexicon discovery.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7441301941871643}]}, {"text": "In learning a language, children face a number of challenging, mutually interdependent inference problems.", "labels": [], "entities": []}, {"text": "Words are represented in terms of phonemes, the basic phonological units of a language.", "labels": [], "entities": []}, {"text": "However, phoneme inventories vary from language to language, and the underlying phonemes which makeup individual words often have variable acoustic realizations due to systematic phonetic and phonological variation, dialect differences, speech style, environmental noise, and other factors.", "labels": [], "entities": []}, {"text": "To learn the phonological form of words in their language children must determine the phoneme inventory of their language, identify which parts of the acoustic signal correspond to which phonemeswhile discounting surface variation in the realization of individual units-and infer which sequences of phonemes correspond to which words (amongst other challenges).", "labels": [], "entities": []}, {"text": "Understanding the solution to this complex jointlearning problem is not only of fundamental scientific interest, but also has important applications in Spoken Language Processing (SLP).", "labels": [], "entities": [{"text": "Spoken Language Processing (SLP)", "start_pos": 152, "end_pos": 184, "type": "TASK", "confidence": 0.8530534108479818}]}, {"text": "Even setting aside additional grammatical and semantic information available to child learners, there is still a sharp contrast between the type of phonological learning done by humans and current SLP methods.", "labels": [], "entities": []}, {"text": "Tasks that involve recognizing words from acoustic input-such as automatic speech recognition and spoken term discovery-only tackle parts of the overall problem, and typically rely on linguistic resources such as phoneme inventories, pronunciation dictionaries, and annotated speech data.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 65, "end_pos": 93, "type": "TASK", "confidence": 0.670617034037908}, {"text": "spoken term discovery-only", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.6197360356648763}]}, {"text": "Such resources are unavailable for many languages, and expensive to create.", "labels": [], "entities": []}, {"text": "Thus, a model that can jointly learn the sound patterns and the lexicon of a language would open up the possibility of automatically developing SLP capabilities for any language.", "labels": [], "entities": []}, {"text": "In this paper, we present a first step towards an unsupervised model of phonological lexicon discovery that is able to jointly learn, from unannotated speech, an underlying phoneme-like inventory, the pattern of surface realizations of those units, and a set of lexical units fora language.", "labels": [], "entities": [{"text": "phonological lexicon discovery", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.7340943813323975}]}, {"text": "Our model builds on earlier work addressing the unsupervised discovery of phone-like units from acoustic data-in particular the Dirichlet Process Hidden Markov Model (DPHMM) of-and the un- Spoken Term Discovery Spoken term discovery is the problem of using unsupervised pattern discovery methods to find previously unknown keywords in speech.", "labels": [], "entities": [{"text": "Spoken Term Discovery Spoken term discovery", "start_pos": 189, "end_pos": 232, "type": "TASK", "confidence": 0.5658958703279495}]}, {"text": "Most models in this literature have typically made use of a two-stage procedure: First, subsequences of the input that are similar in an acoustic feature space are identified, and, then clustered to discover categories corresponding to lexical items).", "labels": [], "entities": []}, {"text": "This problem was first examined by who used Dynamic Time Warping to identify similar acoustic sequences across utterances.", "labels": [], "entities": []}, {"text": "The input sequences discovered by this method were then treated as nodes in a similarity-weighted graph, and graph clustering algorithms were applied to produce a number of densely connected groups of acoustic sequences, corresponding to lexical items.", "labels": [], "entities": []}, {"text": "Building on this work, and proposed robust features that allowed lexical units to be discovered from spoken documents generated by different speakers.", "labels": [], "entities": []}, {"text": "present a similar framework for finding repeated acoustic patterns, based on line-segment detection in dotplots.", "labels": [], "entities": []}, {"text": "Other variants of this approach include McInnes and Goldwater (2011) who compute similarity incrementally, and who integrates a simplified, symbolic representation of visual information associated with each utterance.", "labels": [], "entities": []}, {"text": "Word Segmentation In contrast to spoken term discovery, models of word (or morpheme) segmentation start from unsegmented strings of symbols and attempt to identify subsequences corresponding to lexical items.", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6541695296764374}, {"text": "spoken term discovery", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7256284753481547}, {"text": "word (or morpheme) segmentation", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.7030294239521027}]}, {"text": "The problem has been the focus of many years of intense research, and there area large variety of proposals in the literature).", "labels": [], "entities": []}, {"text": "Of particular interest here are models which treat segmentation as a secondary consequence of discovering a compact lexicon which explains the distribution of phoneme sequences in the input.", "labels": [], "entities": []}, {"text": "Recently, a number of such models have been introduced which make use of Bayesian nonparametric distributions such as the Dirichlet Process) or its two-parameter generalization, the Pitman-Yor Process, to define a prior which favors smaller lexicons with more reusable lexical items.", "labels": [], "entities": []}, {"text": "The first such models were proposed in and, subsequently, have been extended in a number of ways (.", "labels": [], "entities": []}, {"text": "One important lesson that has emerged from this literature is that models which jointly represent multiple levels of linguistic structure often benefit from synergistic interactions where different levels of linguistic structure provide mutual constraints on one another which can be exploited simultaneously.", "labels": [], "entities": []}, {"text": "For example, show that explicitly modeling symbolic variation in phoneme realization improves lexical learning-we use a similar idea in this paper.", "labels": [], "entities": [{"text": "phoneme realization", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7172873318195343}]}, {"text": "An important tool for studying such synergies has been the Adaptor Grammars framework of.", "labels": [], "entities": []}, {"text": "Adaptor Grammars area generalization of Probabilistic Context-free Grammars (PCFGs) which allow the lexical storage of complete subtrees.", "labels": [], "entities": []}, {"text": "Using Adaptor Grammars, it is possible to learn lexica which contain stored units at multiple levels of abstraction (e.g., phonemes, onsets, codas, syllables, morphemes, words, and multiword collocations).", "labels": [], "entities": []}, {"text": "A series of studies using the framework has shown that including such additional structure can markedly improve lexicon discovery.", "labels": [], "entities": [{"text": "lexicon discovery", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.713171049952507}]}], "datasetContent": [{"text": "To the best of our knowledge, there are no standard corpora for evaluating models of unsupervised lexicon discovery.", "labels": [], "entities": []}, {"text": "In this paper, we perform experiments on the six lecture recordings used in), apart of the MIT Lecture corpus (.", "labels": [], "entities": [{"text": "MIT Lecture corpus", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.9644051591555277}]}, {"text": "A brief summary of the six lectures is listed in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The F1 scores for the phone segmentation task obtained by the full systems and their corresponding  initialization systems.", "labels": [], "entities": [{"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9994547963142395}, {"text": "phone segmentation task", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.8486684560775757}]}, {"text": " Table 3: F1 scores for word segmentation obtained by the full systems and their ablated systems.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9994706511497498}, {"text": "word segmentation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7548398971557617}]}, {"text": " Table 5: A subset of the lexical units that the FullDP system discovers for the economics lecture. The  number of independent speech segments that are associated with each lexical unit is denoted as |Word|.", "labels": [], "entities": [{"text": "FullDP", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.8266360759735107}]}]}