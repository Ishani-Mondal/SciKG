{"title": [{"text": "Higher-order Lexical Semantic Models for Non-factoid Answer Reranking", "labels": [], "entities": [{"text": "Answer Reranking", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.8296976089477539}]}], "abstractContent": [{"text": "Lexical semantic models provide robust performance for question answering, but, in general, can only capitalize on direct evidence seen during training.", "labels": [], "entities": [{"text": "Lexical semantic", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8026046454906464}, {"text": "question answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8812922835350037}]}, {"text": "For example, monolingual alignment models acquire term alignment probabilities from semi-structured data such as question-answer pairs; neural network language models learn term embeddings from unstructured text.", "labels": [], "entities": []}, {"text": "All this knowledge is then used to estimate the semantic similarity between question and answer candidates.", "labels": [], "entities": []}, {"text": "We introduce a higher-order formalism that allows all these lexical semantic models to chain direct evidence to construct indirect associations between question and answer texts, by casting the task as the traversal of graphs that encode direct term associations.", "labels": [], "entities": []}, {"text": "Using a corpus of 10,000 questions from Yahoo!", "labels": [], "entities": []}, {"text": "Answers, we experimentally demonstrate that higher-order methods are broadly applicable to alignment and language models, across both word and syntactic representations.", "labels": [], "entities": []}, {"text": "We show that an important criterion for success is controlling for the semantic drift that accumulates during graph traversal.", "labels": [], "entities": []}, {"text": "All in all, the proposed higher-order approach improves five out of the six lexical semantic models investigated, with relative gains of up to +13% over their first-order variants.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open-domain question answering (QA), which finds short textual answers to natural language questions, is often viewed as the successor to keyword search) and one of the most difficult and widely applicable end-user applications of natural language processing (NLP).", "labels": [], "entities": [{"text": "Open-domain question answering (QA)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7715700566768646}]}, {"text": "From syntactic parsing, discourse processing, and lexical semantics, QA necessitates a level of functionality across a variety of topics that make it a natural, yet challenging, proving ground for many aspects of NLP.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7458761632442474}, {"text": "discourse processing", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7052562236785889}]}, {"text": "Here, we address a particularly challenging QA subtask: open-domain nonfactoid QA, where queries take the form of complex questions (e.g., manner or How questions), and answers range from single sentences to entire paragraphs.", "labels": [], "entities": []}, {"text": "Because this task is so complex and large in scope, current state-of-the-art opendomain systems perform at only about 30% P@1, or answering roughly one out of three questions correctly (.", "labels": [], "entities": [{"text": "P@1", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9494040608406067}]}, {"text": "In this paper we focus on answer ranking (AR), a key component of non-factoid QA that focuses on ordering candidate answers based on the likelihood that they capture the information needed to answer a question.", "labels": [], "entities": [{"text": "answer ranking (AR)", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.6085647284984589}]}, {"text": "Unlike keyword search, observed that lexical matching methods are generally insufficient for QA, where questions and answers often have little to no lexical overlap (as in the case of Where should we go for breakfast? and Zoe's Diner has great pancakes).", "labels": [], "entities": [{"text": "keyword search", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.772697925567627}]}, {"text": "Previous work has shown that lexical semantics (LS) models are well suited to bridging this \"lexical chasm\", and at least two flavors of lexical semantics have been successfully applied to QA.", "labels": [], "entities": []}, {"text": "The first treats QA as a monolingual alignment problem, learning associations between words (or other structures) that appear in question-answer pairs).", "labels": [], "entities": []}, {"text": "The second computes the semantic similarity between question and answer using language models acquired from relevant texts).", "labels": [], "entities": []}, {"text": "Here we argue that while these models begin to bridge the \"lexical chasm\", many still suffer from sparsity and only capitalize on direct evidence.", "labels": [], "entities": []}, {"text": "Returning to our example question, if we also train on the QA pair What goes well with pancakes? and hashbrowns and toast, we can use the indirect association between breakfast -pancakes and pancakes -hashbrowns to locate additional answers for whereto visit for breakfast, including Regee's has the best hashbrowns in town.", "labels": [], "entities": []}, {"text": "We can represent LS models as a graph, as in.", "labels": [], "entities": []}, {"text": "For a word-based alignment model, the graph nodes represent individual words, and the (directed) edges capture the likelihood of a word w a appearing in a gold answer given word w q in the question.", "labels": [], "entities": []}, {"text": "Grounding this in our example, w 1 may represent breakfast, w 2 pancakes, and w 4 hashbrowns.", "labels": [], "entities": []}, {"text": "For a language model, the edges capture the similarity of contexts between the words in the nodes.", "labels": [], "entities": []}, {"text": "For both alignment and language model graphs, we observe that semantic association between two words (or structures) stored in the nodes can be determined by investigating the different paths that connect these two nodes, even when they are not directly connected.", "labels": [], "entities": []}, {"text": "We call this class of models higher-order lexical models, in contrast to the first-order lexical models introduced in previous work, which rely only on direct evidence to estimate association strength.", "labels": [], "entities": []}, {"text": "For example, all alignment models in previous work can estimate P (w q |w a ), i.e., the probability of generating the question word w q if the answer contains the word w a , only if this pair has been seen at least once in training.", "labels": [], "entities": [{"text": "P", "start_pos": 64, "end_pos": 65, "type": "METRIC", "confidence": 0.9719832539558411}]}, {"text": "On the other hand, our approach can estimate P (w q |w a ) from indirect evidence, e.g., from chaining two distinct question/answer pairs that contain the words (w q , w i ) and (w i , w a ), respectively.", "labels": [], "entities": [{"text": "P", "start_pos": 45, "end_pos": 46, "type": "METRIC", "confidence": 0.9757198691368103}]}, {"text": "The contributions of this work are: 1.", "labels": [], "entities": []}, {"text": "This is the first work to our knowledge that proposes higher-order LS models for QA.", "labels": [], "entities": []}, {"text": "We show that an important criterion for success is controlling for the semantic drift that accumulates in the graph traversal paths, which plagues traditional random walk algorithms.", "labels": [], "entities": []}, {"text": "For example, we empirically demonstrate that paths up to a length of three are generally beneficial, but longer paths hurt or do not improve performance.", "labels": [], "entities": []}, {"text": "2. We show that a variety of LS models and representations, including alignment and language models, over both words and syntactic structures, can be adapted to the proposed higher-order formalism.", "labels": [], "entities": []}, {"text": "In this latter respect we introduce a novel syntax-based variant of the neural network language model (NNLM) of that models syntactic dependencies rather than words, which allows it to capture knowledge that is complementary to that of word-based NNLMs.", "labels": [], "entities": []}, {"text": "3. The training process for alignment models requires a large corpus of QA pairs.", "labels": [], "entities": [{"text": "alignment models", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.9440812170505524}]}, {"text": "Due to these resource requirements, we evaluate our higher-order LS models on a community question answering (CQA) task () across thousands of how questions, and show that most higher-order models perform significantly better than their first-order variants.", "labels": [], "entities": [{"text": "question answering (CQA) task", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.8259029587109884}]}, {"text": "4. We demonstrate that language models and alignment models capture complementary information, and can be combined to improve the performance of the CQA system for manner questions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Although the main focus of this work is on higherorder LS models, shows that even our first-order models perform better than the previous state of the art.", "labels": [], "entities": []}, {"text": "This is caused by the novel features proposed over alignment models and NNLMs.", "labels": [], "entities": []}, {"text": "To understand the contribution of each of these features, we performed an ablation experiment, summarized in, for two models: one alignment model (WA), and one NNLM (WN).", "labels": [], "entities": []}, {"text": "This analysis indicates that many of the features proposed are important.", "labels": [], "entities": []}, {"text": "For example, two of the novel JSD features for WA have a higher contribution to overall QA performance than the alignment probability (P (Q|A)) proposed in previous work).", "labels": [], "entities": [{"text": "JSD", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9341253042221069}, {"text": "WA", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.965820848941803}, {"text": "alignment probability (P (Q|A))", "start_pos": 112, "end_pos": 143, "type": "METRIC", "confidence": 0.7920576400227017}]}, {"text": "For WN, the two new features proposed here (maximum and minimum cosine similarity between embedding vectors) also have a positive contribution to overall performance, but less than the other two features proposed in previous work).", "labels": [], "entities": [{"text": "WN", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9410675168037415}]}], "tableCaptions": [{"text": " Table 1: Overall results on the Yahoo! Answers dataset using first-order representations (1) and first-order in combination", "labels": [], "entities": [{"text": "Yahoo! Answers dataset", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.8809842020273209}]}, {"text": " Table 3: Performance on the Yahoo! Answers dataset for word, dependency, and models that combine the word and de-", "labels": [], "entities": [{"text": "Yahoo! Answers dataset", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.8823659867048264}]}, {"text": " Table 4: Ablation experiments for first-order word models.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9771643280982971}]}]}