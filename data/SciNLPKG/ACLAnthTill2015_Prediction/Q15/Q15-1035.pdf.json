{"title": [{"text": "Approximation-Aware Dependency Parsing by Belief Propagation", "labels": [], "entities": [{"text": "Approximation-Aware Dependency Parsing", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7812555034955343}]}], "abstractContent": [{"text": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9963685274124146}]}, {"text": "This parser can consider higher-order interactions among edges while retaining O(n 3) runtime.", "labels": [], "entities": []}, {"text": "It outputs the parse with maximum expected recall-but for speed, this expectation is taken under a posterior distribution that is constructed only approximately , using loopy belief propagation through structured factors.", "labels": [], "entities": [{"text": "recall-but", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9947052597999573}, {"text": "speed", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9802565574645996}]}, {"text": "We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data.", "labels": [], "entities": []}, {"text": "We find this gradient by back-propagation.", "labels": [], "entities": []}, {"text": "That is, we treat the entire parser (approximations and all) as a differentiable circuit, as others have done for loopy CRFs (Domke, 2010; Stoyanov et al., 2011; Domke, 2011; Stoyanov and Eisner, 2012).", "labels": [], "entities": []}, {"text": "The resulting parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9992017149925232}, {"text": "belief propagation", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7136030346155167}]}], "introductionContent": [{"text": "Recent improvements to dependency parsing accuracy have been driven by higher-order features.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.8816779851913452}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9069074988365173}]}, {"text": "Such a feature can look beyond just the parent and child words connected by a single edge to also consider siblings, grandparents, etc.", "labels": [], "entities": []}, {"text": "By including increasingly global information, these features provide more information for the parser-but they also complicate inference.", "labels": [], "entities": []}, {"text": "The resulting higher-order parsers depend on approximate inference and decoding procedures, which may prevent them from predicting the best parse.", "labels": [], "entities": []}, {"text": "For example, consider the dependency parser we will train in this paper, which is based on the work of.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7525337636470795}]}, {"text": "Ostensibly, this parser finds the minimum Bayes risk (MBR) parse under a probability distribution defined by a higher-order dependency parsing model.", "labels": [], "entities": [{"text": "minimum Bayes risk (MBR) parse", "start_pos": 34, "end_pos": 64, "type": "METRIC", "confidence": 0.7379727746759143}]}, {"text": "In reality, it achieves O(n 3 t max ) runtime by relying on three approximations during inference: (1) variational inference by loopy belief propagation (BP) on a factor graph, (2) truncating inference after t max iterations prior to convergence, and (3) a first-order pruning model to limit the number of edges considered in the higherorder model.", "labels": [], "entities": [{"text": "O", "start_pos": 24, "end_pos": 25, "type": "METRIC", "confidence": 0.9957960844039917}]}, {"text": "Such parsers are traditionally trained as if the inference had been exact.", "labels": [], "entities": []}, {"text": "In contrast, we train the parser such that the approximate system performs well on the final evaluation function.", "labels": [], "entities": []}, {"text": "We treat the entire parsing computation as a differentiable circuit, and backpropagate the evaluation function through our approximate inference and decoding methods to improve its parameters by gradient descent.", "labels": [], "entities": []}, {"text": "The system also learns to cope with model misspecification, where the model couldn't perfectly fit the distribution even absent the approximations.", "labels": [], "entities": []}, {"text": "For standard graphical models, call this approach ERMA, for \"empirical risk minimization under approximations.\"", "labels": [], "entities": [{"text": "empirical risk minimization", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.6326750119527181}]}, {"text": "For objectives besides empirical risk, Domke (2011) refers to it as \"learning with truncated message passing.\"", "labels": [], "entities": []}, {"text": "Our primary contribution is the application of this approximation-aware learning method in the parsing setting, for which the graphical model involves a global constraint.", "labels": [], "entities": [{"text": "parsing", "start_pos": 95, "end_pos": 102, "type": "TASK", "confidence": 0.9663599133491516}]}, {"text": "previously showed how to run BP in this setting (by calling the inside-outside algorithm as a subroutine).", "labels": [], "entities": []}, {"text": "We must backpropagate the downstream objective function through their algorithm so that we can follow its gradient.", "labels": [], "entities": []}, {"text": "We carefully define an empirical risk objective function (` a la ERMA) to be smooth and differentiable, yet equivalent to accuracy of the minimum Bayes risk (MBR) parse in the limit.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9995954632759094}, {"text": "minimum Bayes risk (MBR) parse", "start_pos": 138, "end_pos": 168, "type": "METRIC", "confidence": 0.7850713133811951}]}, {"text": "Finding this difficult to optimize, we introduce anew simpler objective function based on the L 2 distance between the approximate marginals and the \"true\" marginals from the gold data.", "labels": [], "entities": [{"text": "L 2 distance", "start_pos": 94, "end_pos": 106, "type": "METRIC", "confidence": 0.8763332565625509}]}, {"text": "The goal of this work is to account for the approximations made by a system rooted in structured belief propagation.", "labels": [], "entities": [{"text": "structured belief propagation", "start_pos": 86, "end_pos": 115, "type": "TASK", "confidence": 0.7929689486821493}]}, {"text": "Taking such approximations into account during training enables us to improve the speed and accuracy of inference attest time.", "labels": [], "entities": [{"text": "speed", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9965985417366028}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.998659610748291}]}, {"text": "We compare our training method with the standard approach of conditional log-likelihood (CLL) training.", "labels": [], "entities": []}, {"text": "We evaluate our parser on 19 languages from the) and) Shared Tasks as well as the English Penn Treebank ().", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 82, "end_pos": 103, "type": "DATASET", "confidence": 0.8379429578781128}]}, {"text": "On English, the resulting parser obtains higher accuracy with fewer iterations of BP than CLL.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9991969466209412}, {"text": "BP", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9729591012001038}]}, {"text": "On the CoNLL languages, we find that on average it yields higher accuracy parsers than CLL, particularly when limited to few BP iterations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.994827926158905}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The impact of exact vs. approximate inference  on a 2nd-order model with grandparent factors only. Re- sults are for the development ( \u00a7 22) and test ( \u00a7 23) sec- tions of PTB-YM.", "labels": [], "entities": [{"text": "Re- sults", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9719405174255371}, {"text": "PTB-YM", "start_pos": 182, "end_pos": 188, "type": "DATASET", "confidence": 0.9391797780990601}]}, {"text": " Table 2: Results on 19 languages from CoNLL-2006/2007. For languages appearing in both datasets, the 2006 version  was used, except for Chinese (ZH). Evaluation follows the 2006 conventions and excludes punctuation. We report  absolute UAS for the baseline (CLL) and the improvement in UAS for L 2 over CLL (L 2 \u2212 CLL) with positive/negative  differences in blue/red. The average UAS and average difference across all languages (AVG.) is given.", "labels": [], "entities": [{"text": "CoNLL-2006/2007", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.9092636108398438}, {"text": "UAS", "start_pos": 237, "end_pos": 240, "type": "METRIC", "confidence": 0.8614957332611084}, {"text": "UAS", "start_pos": 287, "end_pos": 290, "type": "METRIC", "confidence": 0.9696715474128723}, {"text": "UAS", "start_pos": 381, "end_pos": 384, "type": "METRIC", "confidence": 0.9960823655128479}, {"text": "AVG.", "start_pos": 430, "end_pos": 434, "type": "METRIC", "confidence": 0.994817316532135}]}]}