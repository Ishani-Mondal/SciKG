{"title": [{"text": "Cross-Document Co-Reference Resolution using Sample-Based Clustering with Knowledge Enrichment", "labels": [], "entities": [{"text": "Cross-Document Co-Reference Resolution", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6140146950880686}]}], "abstractContent": [{"text": "Identifying and linking named entities across information sources is the basis of knowledge acquisition and at the heart of Web search, recommendations , and analytics.", "labels": [], "entities": [{"text": "Identifying and linking named entities across information sources", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.680608257651329}, {"text": "knowledge acquisition", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.719605028629303}]}, {"text": "An important problem in this context is cross-document co-reference resolution (CCR): computing equivalence classes of textual mentions denoting the same entity, within and across documents.", "labels": [], "entities": [{"text": "cross-document co-reference resolution (CCR)", "start_pos": 40, "end_pos": 84, "type": "TASK", "confidence": 0.7890709191560745}]}, {"text": "Prior methods employ ranking, clustering, or probabilistic graphical models using syntactic features and distant features from knowledge bases.", "labels": [], "entities": []}, {"text": "However, these methods exhibit limitations regarding run-time and robustness.", "labels": [], "entities": []}, {"text": "This paper presents the CROCS framework for unsupervised CCR, improving the state of the art in two ways.", "labels": [], "entities": []}, {"text": "First, we extend the way knowledge bases are harnessed, by constructing a notion of semantic summaries for intra-document co-reference chains using co-occurring entity mentions belonging to different chains.", "labels": [], "entities": []}, {"text": "Second, we reduce the computational cost by anew algorithm that embeds sample-based bisection, using spectral clustering or graph partitioning, in a hierarchical clustering process.", "labels": [], "entities": []}, {"text": "This allows scaling up CCR to large corpora.", "labels": [], "entities": []}, {"text": "Experiments with three datasets show significant gains in output quality , compared to the best prior methods, and the run-time efficiency of CROCS.", "labels": [], "entities": [{"text": "CROCS", "start_pos": 142, "end_pos": 147, "type": "DATASET", "confidence": 0.77177494764328}]}], "introductionContent": [], "datasetContent": [{"text": "Benchmark Datasets: We performed experiments with the following three publicly available benchmarking datasets, thereby comparing the performance of CROCS against state-of-the-art baselines under various input characteristics.", "labels": [], "entities": [{"text": "Benchmark Datasets", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.746791273355484}]}, {"text": "\u2022 John Smith corpus: the classical benchmark for CCR articles selected from the New York Times.", "labels": [], "entities": [{"text": "John Smith corpus", "start_pos": 2, "end_pos": 19, "type": "DATASET", "confidence": 0.9364388982454935}, {"text": "CCR articles selected from the New York Times", "start_pos": 49, "end_pos": 94, "type": "DATASET", "confidence": 0.6742103695869446}]}, {"text": "It includes mentions of 35 different \"John Smith\" person entities.", "labels": [], "entities": [{"text": "John Smith\" person", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.838988646864891}]}, {"text": "All mentions pertaining to John Smith within a document refer to the same person.", "labels": [], "entities": [{"text": "John Smith", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.8264191150665283}]}, {"text": "This provides a small-scale but demanding setting for CCR, as most John Smiths are longtail entities unknown to Wikipedia or any KB.", "labels": [], "entities": [{"text": "CCR", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9408358335494995}]}, {"text": "\u2022 WePS-2 collection: a set of 4,500 Web pages used in the Web People Search 2 competition ().", "labels": [], "entities": [{"text": "WePS-2 collection", "start_pos": 2, "end_pos": 19, "type": "DATASET", "confidence": 0.9367644786834717}]}, {"text": "The documents comprise the top 150 Web search results (using Yahoo!", "labels": [], "entities": []}, {"text": "search (as of 2008)) for each of 30 different people (obtained from Wikipedia, ACL'08, and US Census), covering both prominent entities (e.g., Ivan Titov, computer science researcher) and long-tailed entities (e.g., Ivan Titov, actor).", "labels": [], "entities": [{"text": "US Census", "start_pos": 91, "end_pos": 100, "type": "DATASET", "confidence": 0.8825457990169525}]}, {"text": "\u2022 New York Times (NYT) archive: a set of around 1.8 million news article from the archives of the newspaper (Sandhaus, 2008) extracted between January 1987 and June 2007.", "labels": [], "entities": [{"text": "New York Times (NYT) archive", "start_pos": 2, "end_pos": 30, "type": "DATASET", "confidence": 0.755713871547154}, {"text": "Sandhaus, 2008) extracted between January 1987", "start_pos": 109, "end_pos": 155, "type": "DATASET", "confidence": 0.8939322531223297}]}, {"text": "We randomly select 220, 000 articles from the time range of January 1, 2004 through June 19, 2007, which contain about 3.71 million mentions, organized into 1.57 million local mention chains after the intra-document CR step.", "labels": [], "entities": []}, {"text": "In our experiments, we consider mentions of person entities as this is the most predominant and demanding class of entities in the datasets.", "labels": [], "entities": []}, {"text": "The John Smith and WePS-2 datasets have explicit ground truth annotations, while the NYT contains editorial annotations for entities present in each article.", "labels": [], "entities": [{"text": "John Smith and WePS-2 datasets", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.8656076550483703}, {"text": "NYT", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.9569483399391174}]}, {"text": "For knowledge enrichment, we used Freebase; although sensitivity studies explore alternative setups with Yago.", "labels": [], "entities": [{"text": "knowledge enrichment", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7947245538234711}, {"text": "Freebase", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9446656107902527}]}, {"text": "Evaluation Measures: We use the established measures to assess output quality of CCR methods: \u2022 B 3 F1 score: measures the F1 score as a harmonic mean of precision and recall of the final equivalence classes.", "labels": [], "entities": [{"text": "B 3 F1 score", "start_pos": 96, "end_pos": 108, "type": "METRIC", "confidence": 0.8657337427139282}, {"text": "F1 score", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9849680960178375}, {"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9973915815353394}, {"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.998819887638092}]}, {"text": "Precision is defined as the ratio of the number of correctly reported co-references (for each mention) to the total number; while recall computes the fraction of actual co-references correctly identified.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9926971793174744}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9972463846206665}]}, {"text": "Both the final precision and recall are computed by averaging overall mention groups.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9487499594688416}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.999618649482727}]}, {"text": "\u2022 \u03c6 3 -CEAF score: an alternate way of computing precision, recall, and F1 scores using the best 1-to-1 mapping between the equivalence classes obtained and those in the ground truth.", "labels": [], "entities": [{"text": "\u03c6 3 -CEAF score", "start_pos": 2, "end_pos": 17, "type": "METRIC", "confidence": 0.938875961303711}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9978681802749634}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.997475802898407}, {"text": "F1 scores", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9765232801437378}]}, {"text": "The best mapping of ground-truth to output classes exhibits the highest mention overlap.", "labels": [], "entities": [{"text": "mention overlap", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.8611361682415009}]}, {"text": "All experiments were conducted on a 4 core Intel i5 2.50 GHz processor with 8 GB RAM running Ubuntu 12.04.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: B 3 F1 results on John Smith dataset.", "labels": [], "entities": [{"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.6393404603004456}, {"text": "John Smith dataset", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.9488813281059265}]}, {"text": " Table 2: B 3 F1 results on WePS-2 dataset.", "labels": [], "entities": [{"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.5127545595169067}, {"text": "WePS-2 dataset", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9761781096458435}]}, {"text": " Table 3: B 3 F1 scores for CROCS enrichment variants.", "labels": [], "entities": [{"text": "B 3 F1 scores", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.8525703251361847}, {"text": "CROCS enrichment", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.6655156314373016}]}, {"text": " Table 4: B 3 F1 scores (%) for different choices of \u03b8.", "labels": [], "entities": [{"text": "B 3 F1 scores", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.8644320964813232}]}, {"text": " Table 5: \u03b8 error sensitivity of CROCS", "labels": [], "entities": [{"text": "\u03b8 error sensitivity", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9280627568562826}, {"text": "CROCS", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.6696211099624634}]}, {"text": " Table 6: B 3 F1 scores (%) for different # sub-clusters k.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.921552449464798}]}, {"text": " Table 7: CROCS B 3 F1 scores with Freebase vs. Yago", "labels": [], "entities": [{"text": "CROCS B 3", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8108304540316263}, {"text": "F1", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.5021817684173584}, {"text": "Yago", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.6366754174232483}]}, {"text": " Table 8: Accuracy and scalability of various algorithms embedded in CROCS", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9931463003158569}, {"text": "CROCS", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.7212589383125305}]}]}