{"title": [{"text": "Gappy Pattern Matching on GPUs for On-Demand Extraction of Hierarchical Translation Grammars", "labels": [], "entities": [{"text": "Gappy Pattern Matching", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5761635998884836}, {"text": "Extraction of Hierarchical Translation Grammars", "start_pos": 45, "end_pos": 92, "type": "TASK", "confidence": 0.6951484620571137}]}], "abstractContent": [{"text": "Grammars for machine translation can be materialized on demand by finding source phrases in an indexed parallel corpus and extracting their translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7675892114639282}]}, {"text": "This approach is limited in practical applications by the computational expense of online lookup and extraction.", "labels": [], "entities": []}, {"text": "For phrase-based models, recent work has shown that on-demand grammar extraction can be greatly accelerated by parallelization on general purpose graphics processing units (GPUs), but these algorithms do notwork for hierarchical models, which require matching patterns that contain gaps.", "labels": [], "entities": [{"text": "grammar extraction", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7731691896915436}]}, {"text": "We address this limitation by presenting a novel GPU algorithm for on-demand hierarchical grammar extraction that is at least an order of magnitude faster than a comparable CPU algorithm when processing large batches of sentences.", "labels": [], "entities": [{"text": "hierarchical grammar extraction", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.6986894408861796}]}, {"text": "In terms of end-to-end translation, with decoding on the CPU, we increase throughput by roughly two thirds on a standard MT evaluation dataset.", "labels": [], "entities": [{"text": "MT evaluation dataset", "start_pos": 121, "end_pos": 142, "type": "DATASET", "confidence": 0.8442074855168661}]}, {"text": "The GPU necessary to achieve these improvements increases the cost of a server by about a third.", "labels": [], "entities": []}, {"text": "We believe that GPU-based extraction of hierarchical grammars is an attractive proposition, particularly for MT applications that demand high throughput.", "labels": [], "entities": [{"text": "GPU-based extraction of hierarchical grammars", "start_pos": 16, "end_pos": 61, "type": "TASK", "confidence": 0.860283899307251}, {"text": "MT", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.9806267619132996}]}], "introductionContent": [{"text": "Most machine translation systems extract a large, fixed translation model from parallel text that is accessed from memory or disk.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.740243673324585}]}, {"text": "An alternative is to store the indexed parallel text in memory and extract translation units on demand only when they are needed to decode new input.", "labels": [], "entities": []}, {"text": "This architecture has several advantages: It requires only a few gigabytes to represent a model that would otherwise require a terabyte.", "labels": [], "entities": []}, {"text": "It can adapt incrementally to new training data (, making it useful for interactive translation.", "labels": [], "entities": [{"text": "interactive translation", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.5779772996902466}]}, {"text": "It supports rule extraction that is sensitive to the input sentence, enabling leave-oneout training) and the use of sentence similarity features).", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.7821497619152069}]}, {"text": "On-demand extraction can be slow, but for phrase-based models, massive parallelization on general purpose graphics processing units (GPUs) can dramatically accelerate performance.", "labels": [], "entities": []}, {"text": "demonstrated orders of magnitude speedup inexact pattern matching with suffix arrays, the algorithms at the heart of on-demand extraction).", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.7330105304718018}, {"text": "on-demand extraction", "start_pos": 117, "end_pos": 137, "type": "TASK", "confidence": 0.7970797121524811}]}, {"text": "However, some popular translation models use \"gappy\" phrases, and the GPU algorithm of He et al. does notwork for these models since it is limited to contiguous phrases.", "labels": [], "entities": []}, {"text": "Instead, we need pattern matching and phrase extraction that is able to handle variable-length gaps.", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7547134160995483}, {"text": "phrase extraction", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.814934641122818}]}, {"text": "This paper presents a novel GPU algorithm for on-demand extraction of hierarchical translation models based on matching and extracting gappy phrases.", "labels": [], "entities": [{"text": "on-demand extraction of hierarchical translation", "start_pos": 46, "end_pos": 94, "type": "TASK", "confidence": 0.6464261829853057}]}, {"text": "Our experiments examine both grammar extraction and end-to-end translation, comparing quality, speed, and memory use.", "labels": [], "entities": [{"text": "grammar extraction", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8078207969665527}, {"text": "speed", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.953534722328186}]}, {"text": "We compare against the GPU system for phrase-based translation by and cdec, a state-of-the-art CPU system for hierarchical translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.789043664932251}, {"text": "hierarchical translation", "start_pos": 110, "end_pos": 134, "type": "TASK", "confidence": 0.7092517614364624}]}, {"text": "Our system outperforms the former on translation quality by 2.3 BLEU (replicating previously-known results) and outperforms the latter on speed, improving grammar extraction throughput by at least an order of magnitude on large batches of sentences while maintaining the same level of translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9989734888076782}, {"text": "speed", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9949288368225098}, {"text": "grammar extraction throughput", "start_pos": 155, "end_pos": 184, "type": "TASK", "confidence": 0.7497776051362356}]}, {"text": "Our contribution is to show, complete with an open-source implementation, how GPUs can vastly increase the speed of hierarchical grammar extraction, particularly for high-throughput MT applications.", "labels": [], "entities": [{"text": "hierarchical grammar extraction", "start_pos": 116, "end_pos": 147, "type": "TASK", "confidence": 0.653026799360911}]}], "datasetContent": [{"text": "We tested our algorithms in an end-to-end ChineseEnglish translation task using data conditions similar to those of Lopez (2008b) and.", "labels": [], "entities": [{"text": "ChineseEnglish translation task", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.8011717398961385}]}, {"text": "Our implementation of hierarchical grammar extraction on the GPU, as detailed in the previous section, is written in C, using CUDA library v5.5 and GCC v4.8, compiled with the -O3 optimization flag.", "labels": [], "entities": [{"text": "hierarchical grammar extraction", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6319199999173483}, {"text": "GPU", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.9428635239601135}]}, {"text": "Our code is open source and available for researchers to download and tryout.", "labels": [], "entities": []}, {"text": "We used NVIDIA's Tesla K20c GPU (Kepler Generation), which has 2496 CUDA cores and 5 GB memory, with a peak memory bandwidth of 208 GB/s.", "labels": [], "entities": [{"text": "Tesla K20c GPU", "start_pos": 17, "end_pos": 31, "type": "DATASET", "confidence": 0.7304043769836426}]}, {"text": "The server hosting the GPU has two Intel Xeon E5-2690 CPUs, each with eight cores at 2.90 GHz (a total of 16 physical cores; 32 logical cores with hyperthreading).", "labels": [], "entities": []}, {"text": "Both were released in 2012 and represent comparable generation hardware technology.", "labels": [], "entities": []}, {"text": "All GPU and CPU experiments were conducted on the same machine, which runs Red Hat Enterprise Linux (RHEL) 6.", "labels": [], "entities": [{"text": "Red Hat Enterprise Linux (RHEL) 6", "start_pos": 75, "end_pos": 108, "type": "DATASET", "confidence": 0.7289488166570663}]}, {"text": "We used two training sets: The first consists of news articles from the Xinhua Agency, with 27 million words of Chinese (around one million sentences).", "labels": [], "entities": [{"text": "Xinhua Agency", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.8946219980716705}]}, {"text": "The second adds parallel text from the United Nations, with 81 million words of Chinese (around four million sentences).", "labels": [], "entities": []}, {"text": "For performance evaluations, we ran tests on sentence batches of varying sizes: 100, 500, 1k, 2k, 4k, 6k, 8k, 16k and 32k.", "labels": [], "entities": []}, {"text": "These sentences are drawn from the NIST 2002-2008 MT evaluations (on average 27 words each) and then the Chinese side of the Hong Kong Parallel Text (LDC2004T08) when the NIST data are smaller than the target batch 1 http://hohocode.github.io/cgx/ size.", "labels": [], "entities": [{"text": "NIST 2002-2008 MT evaluations", "start_pos": 35, "end_pos": 64, "type": "DATASET", "confidence": 0.8519028127193451}, {"text": "Hong Kong Parallel Text (LDC2004T08)", "start_pos": 125, "end_pos": 161, "type": "DATASET", "confidence": 0.924650524343763}, {"text": "NIST data", "start_pos": 171, "end_pos": 180, "type": "DATASET", "confidence": 0.9733007848262787}]}, {"text": "Large batch sizes are necessary to saturate the processing power of the GPU.", "labels": [], "entities": []}, {"text": "The size of the complete batch of 32k test sentences is 4892 KB.", "labels": [], "entities": []}, {"text": "We compared our GPU implementation for on-demand extraction of hierarchical grammars against the corresponding CPU implementation ( found in pycdec (, an extension of cdec (.", "labels": [], "entities": [{"text": "on-demand extraction of hierarchical grammars", "start_pos": 39, "end_pos": 84, "type": "TASK", "confidence": 0.7551133513450623}]}, {"text": "We also compared our GPU algorithms against Moses (, representing a standard phrase-based SMT baseline.", "labels": [], "entities": []}, {"text": "Phrase tables generated by Moses are essentially the same as the GPU implementation of on-demand extraction for phrase-based translation by.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.7035057693719864}]}], "tableCaptions": [{"text": " Table 1: Comparison of translation quality. The  hierarchical system is cdec. Online CPU extraction  is the baseline, part of the standard cdec package.  Online GPU extraction is this work.", "labels": [], "entities": [{"text": "Online CPU extraction", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.6165716250737509}, {"text": "Online GPU extraction", "start_pos": 155, "end_pos": 176, "type": "TASK", "confidence": 0.5676521360874176}]}, {"text": " Table 2. Experiments used different  numbers of threads under different data conditions  (Xinhua or Xinhua + UN), with and without sam- pling. Our server has a total of 16 physical cores,  but supports 32 logical cores via hyperthreading. We  obtained the CPU sampling results by running cdec  over 16k query sentences. For the non-sampling  runs, since the throughput is so low, we measured", "labels": [], "entities": []}, {"text": " Table 3: GPU grammar extraction throughput (words/second) under different batch sizes, data conditions,  with and without sampling. Speedup is computed with respect to the CPU baseline running on 32 threads.", "labels": [], "entities": [{"text": "GPU grammar extraction throughput", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.6869012489914894}]}, {"text": " Table 4: Sentence-by-sentence GPU grammar  extraction throughput (words/second) vs. a single  thread on the CPU (X: Xinhua, X+U: Xinhua + UN).", "labels": [], "entities": [{"text": "Sentence-by-sentence GPU grammar  extraction throughput", "start_pos": 10, "end_pos": 65, "type": "TASK", "confidence": 0.7158225655555726}]}, {"text": " Table 7: Detailed timings (in seconds) for 6k  queries. Passes in gray occur on the GPU; all  others on the CPU. Passes needed for hierarchical  grammars are in italics, which are not present in He  et al. (2013).", "labels": [], "entities": []}]}