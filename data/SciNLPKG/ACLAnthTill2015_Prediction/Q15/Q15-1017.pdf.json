{"title": [{"text": "Learning Composition Models for Phrase Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Lexical embeddings can serve as useful representations for words fora variety of NLP tasks, but learning embeddings for phrases can be challenging.", "labels": [], "entities": []}, {"text": "While separate embeddings are learned for each word, this is infeasible for every phrase.", "labels": [], "entities": []}, {"text": "We construct phrase em-beddings by learning how to compose word embeddings using features that capture phrase structure and context.", "labels": [], "entities": []}, {"text": "We propose efficient unsupervised and task-specific learning objectives that scale our model to large datasets.", "labels": [], "entities": []}, {"text": "We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7211731523275375}, {"text": "phrase semantic similarity", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.7461479306221008}]}, {"text": "We make the implementation of our model and the datasets available for general use.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings learned by neural language models () have been successfully applied to a range of tasks, including syntax) and semantics ().", "labels": [], "entities": []}, {"text": "However, phrases are critical for capturing lexical meaning for many tasks.", "labels": [], "entities": []}, {"text": "For example, showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind.", "labels": [], "entities": [{"text": "SRL", "start_pos": 158, "end_pos": 161, "type": "TASK", "confidence": 0.9110684394836426}]}, {"text": "We propose anew method for compositional semantics that learns to compose word embeddings into phrases.", "labels": [], "entities": []}, {"text": "In contrast to a common approach to phrase embeddings that uses pre-defined composition operators), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context.", "labels": [], "entities": []}, {"text": "Other work on learning compositions relies on matrices/tensors as transformations).", "labels": [], "entities": []}, {"text": "However, this work suffers from two primary disadvantages.", "labels": [], "entities": []}, {"text": "First, these methods have high computational complexity for dense embeddings: O(d 2 ) or O(d 3 ) for composing every two components with d dimensions.", "labels": [], "entities": []}, {"text": "The high computational complexity restricts these methods to use very low-dimensional embeddings (25 or 50).", "labels": [], "entities": []}, {"text": "While low-dimensional embeddings perform well for syntax) and sentiment) tasks, they do poorly on semantic tasks.", "labels": [], "entities": []}, {"text": "Second, because of the complexity, they use supervised training with small task-specific datasets.", "labels": [], "entities": []}, {"text": "An exception is the unsupervised objective of recursive auto-encoders).", "labels": [], "entities": []}, {"text": "Yet this work cannot utilize contextual features of phrases and still poses scaling challenges.", "labels": [], "entities": []}, {"text": "In this work we propose a novel compositional transformation called the Feature-rich Compositional Transformation (FCT) model.", "labels": [], "entities": [{"text": "Feature-rich Compositional Transformation (FCT)", "start_pos": 72, "end_pos": 119, "type": "TASK", "confidence": 0.7232586840788523}]}, {"text": "FCT produces phrases from their word components.", "labels": [], "entities": [{"text": "FCT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6720833778381348}]}, {"text": "In contrast to previous work, our approach to phrase composition can efficiently utilize high dimensional embeddings (e.g. d = 200) with an unsupervised objective, both of which are critical to doing well on semantics tasks.", "labels": [], "entities": [{"text": "phrase composition", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.843305766582489}]}, {"text": "Our composition function is parameter-ized to allow the inclusion of features based on the phrase structure and contextual information, including positional indicators of the word components.", "labels": [], "entities": []}, {"text": "The phrase composition is a weighted summation of embeddings of component words, where the summation weights are defined by the features, which allows for fast composition.", "labels": [], "entities": []}, {"text": "We discuss a range of training settings for FCT.", "labels": [], "entities": [{"text": "FCT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9402467608451843}]}, {"text": "For tasks with labeled data, we utilize task-specific training.", "labels": [], "entities": []}, {"text": "We begin with embeddings trained on raw text and then learn compositional phrase parameters as well as fine-tune the embeddings for the specific task's objective.", "labels": [], "entities": []}, {"text": "For tasks with unlabeled data (e.g. most semantic tasks) we can train on a large corpus of unlabeled data.", "labels": [], "entities": []}, {"text": "For tasks with both labeled and unlabeled data, we consider a joint training scheme.", "labels": [], "entities": []}, {"text": "Our model's efficiency ensures we can incorporate large amounts of unlabeled data, which helps mitigate over-fitting and increases vocabulary coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.7630542516708374}]}, {"text": "We begin with a presentation of FCT ( \u00a72), including our proposed features for the model.", "labels": [], "entities": [{"text": "FCT", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.8877732753753662}]}, {"text": "We then present three training settings ( \u00a73) that cover language modeling (unsupervised), task-specific training (supervised), and joint (semi-supervised) settings.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7239900529384613}]}, {"text": "The remainder of the paper is devoted to evaluation of each of these settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We begin with experiments on FCT for language modeling tasks (Section 3.1).", "labels": [], "entities": [{"text": "FCT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9194269776344299}, {"text": "language modeling tasks", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.8180757959683737}]}, {"text": "The resultant embeddings can then be used for pre-training in taskspecific settings (Section 6).", "labels": [], "entities": []}, {"text": "Data We use the 1994-97 subset from the New York Times (NYT) portion of Gigaword v5.0 (Parker et al., 2011).", "labels": [], "entities": [{"text": "1994-97 subset from the New York Times (NYT) portion of Gigaword v5.0", "start_pos": 16, "end_pos": 85, "type": "DATASET", "confidence": 0.8590774855443409}]}, {"text": "Sentences are tokenized using OpenNLP.", "labels": [], "entities": [{"text": "Sentences", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9377618432044983}, {"text": "OpenNLP", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9715914726257324}]}, {"text": "We removed words with frequencies less than 5, yielding a vocabulary of 518,235 word forms and 515,301,382 tokens for training word embeddings.", "labels": [], "entities": []}, {"text": "This dataset is used for both training baseline word embeddings and evaluating our models trained with the LM objective.", "labels": [], "entities": []}, {"text": "When evaluating the LM task we consider bigram NPs in isolation (see the \"Phrases\" column in).", "labels": [], "entities": []}, {"text": "For FCT features that require syntactic information, we extract the NYT portion of Annotated Gigaword (, which uses the Stanford parser's annotations.", "labels": [], "entities": [{"text": "NYT portion of Annotated Gigaword", "start_pos": 68, "end_pos": 101, "type": "DATASET", "confidence": 0.781605315208435}]}, {"text": "We use all bigram noun phrases (obtained from the annotated data) as the input phrases for Eq.", "labels": [], "entities": []}, {"text": "A subset from January 1998 of NYT data is withheld for evaluation.", "labels": [], "entities": [{"text": "January 1998 of NYT data", "start_pos": 14, "end_pos": 38, "type": "DATASET", "confidence": 0.606054139137268}]}, {"text": "Baselines We include two baselines.", "labels": [], "entities": []}, {"text": "The first is to use each component word to predict the context of the phrase with the skip gram model () and then average the scores to get the probability (denoted as word2vec).", "labels": [], "entities": []}, {"text": "The second is to use SUM of the skip-gram embeddings to predict the scores.", "labels": [], "entities": [{"text": "SUM", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9921450018882751}]}, {"text": "Training the FCT models with pre-trained word embeddings requires running the skip-gram model on NYT data for 2 iterations: one for word2vec training and one for learning FCT.", "labels": [], "entities": [{"text": "NYT data", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.9038173258304596}]}, {"text": "Therefore, we also run the word2vec model for two epochs to provide embeddings for the baselines.", "labels": [], "entities": []}, {"text": "Data We consider several phrase similarity datasets for evaluating task-specific training.", "labels": [], "entities": []}, {"text": "summarizes these datasets and shows examples of inputs and outputs for each task.", "labels": [], "entities": []}, {"text": "PPDB The Paraphrase Database (PPDB) 4 (Ganitkevitch et al., 2013) contains tens of millions of automatically extracted paraphrase pairs, including words and phrases.", "labels": [], "entities": [{"text": "PPDB The Paraphrase Database (PPDB) 4", "start_pos": 0, "end_pos": 37, "type": "DATASET", "confidence": 0.8447815477848053}]}, {"text": "We extract all paraphrases containing a bigram noun phrase and a noun word from PPDB.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.9466119408607483}]}, {"text": "Since articles usually have little contributions to the phrase meaning, we removed the easy cases of all pairs in which the phrase is composed of an article and a noun.Next, we removed duplicate pairs: if <A,B> occurred in PPDB, we removed relations of <B,A>.", "labels": [], "entities": []}, {"text": "PPDB is organized into 6 parts, ranging from S (small) to XXXL.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8416585326194763}]}, {"text": "Division into these sets is based on an automatically derived accuracy metric.", "labels": [], "entities": [{"text": "Division", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9592154622077942}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9972054362297058}]}, {"text": "We extracted paraphrases from the XXL set.", "labels": [], "entities": [{"text": "XXL set", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9039101302623749}]}, {"text": "The most accurate (i.e. first) 1,000 pairs are used for evaluation and divided into a dev set (500 pairs) and test set (500 pairs); the remaining pairs were used for training.", "labels": [], "entities": []}, {"text": "Our PPDB task is an extension of measuring PPDB semantic similarity between words (Yu contribution of the european union eu contribution: Examples of phrase similarity tasks.", "labels": [], "entities": [{"text": "Yu", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9934537410736084}]}, {"text": "(1) PPDB is a ranking task, in which an input bigram and a output noun are given, and the goal is to rank the output word over other words in the vocabulary.", "labels": [], "entities": []}, {"text": "(2) SemEval2013 is a binary classification task: determine whether an input pair of a bigram and a word form a paraphrase (True) or not (False).", "labels": [], "entities": [{"text": "False", "start_pos": 137, "end_pos": 142, "type": "METRIC", "confidence": 0.9809867739677429}]}, {"text": "(3) Turney2012 is a multi-class classification task: determine the word most similar to the input phrase (in bold) from the five output candidates.", "labels": [], "entities": [{"text": "Turney2012", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8247020840644836}, {"text": "multi-class classification task", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.7556004027525584}]}, {"text": "For the 10-choice task, the goal is to select the most similar pair between the combination of one bigram phrase, i.e., the input phrase or the swapped input (\"word monosyllabic\" for this example), and the five output candidates.", "labels": [], "entities": []}, {"text": "The correct answer in this case should still be the pair of original input phrase and the original correct output candidate (in bold).", "labels": [], "entities": []}, {"text": "(4) PPDB (ngram) is similar to PPDB, but in which both inputs and outputs becomes noun phrases with arbitrary lengths. and  to that between phrases.", "labels": [], "entities": []}, {"text": "Phrase Similarity Datasets We use a variety of human annotated datasets to evaluate phrase semantic similarity: the SemEval2013 shared task (, and the noun-modifier problem (Turney2012) in Turney (2012).", "labels": [], "entities": [{"text": "phrase semantic similarity", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.7488444447517395}, {"text": "Turney2012) in Turney (2012)", "start_pos": 174, "end_pos": 202, "type": "DATASET", "confidence": 0.8262153949056353}]}, {"text": "Both tasks provide evaluation data and training data.", "labels": [], "entities": []}, {"text": "SemEval2013 Task 5(a) is a classification task to determine if a word phrase pair are semantically similar.", "labels": [], "entities": []}, {"text": "Turney2012 is a task to select the closest matching candidate word fora given phrase from candidate words.", "labels": [], "entities": [{"text": "Turney2012", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8508411645889282}]}, {"text": "The original task contained seven candidates, two of which are component words of the input phrase (seven-choice task).", "labels": [], "entities": []}, {"text": "Followup work has since removed the components words from the candidates (five-choice task).", "labels": [], "entities": []}, {"text": "Turney (2012) also propose a 10-choice task based on this same dataset.", "labels": [], "entities": []}, {"text": "In this task, the input bigram noun phrase will have its component words swapped.", "labels": [], "entities": []}, {"text": "Then all the pairs of swapped phrase and a candidate word will be treated as a negative example.", "labels": [], "entities": []}, {"text": "Therefore, each input phrase will correspond to 10 test examples where only one of them is the positive one.", "labels": [], "entities": []}, {"text": "Longer Phrases: PPDB (ngram-to-ngram) To show the generality of our approach we evaluate our method on phrases longer than bigrams.", "labels": [], "entities": []}, {"text": "We extract arbitrary length noun phrase pairs from PPDB.", "labels": [], "entities": []}, {"text": "We only include phrase pairs that differ by more than one word; otherwise the task would reduce to evaluating unigram similarity.", "labels": [], "entities": []}, {"text": "Similar to the bigram-tounigram task, we used the XXL set and removed duplicate pairs.", "labels": [], "entities": []}, {"text": "We used the most accurate pairs for development (2,821 pairs) and test (2,920 pairs); the remaining 148,838 pairs were used for training.", "labels": [], "entities": [{"text": "test", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9457417130470276}]}, {"text": "As before, we rely on negative sampling to efficiently compute the objective during training.", "labels": [], "entities": []}, {"text": "For each source/target n-gram pair, we sample negative noun phrases as outputs.", "labels": [], "entities": []}, {"text": "Both the target phrase and the negative phrases are transformed to their phrase embeddings with the current parameters.", "labels": [], "entities": []}, {"text": "We then compute inner products between embedding of the source phrase and these output embeddings, and update the parameters according to the NCE objective.", "labels": [], "entities": [{"text": "NCE", "start_pos": 142, "end_pos": 145, "type": "DATASET", "confidence": 0.8305485844612122}]}, {"text": "We use the same feature templates as in.", "labels": [], "entities": []}, {"text": "Notice that the XXL set contains several subsets (e.g., M, L ,XL) ranked by accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9988380074501038}]}, {"text": "In the experiments we also investigate their performance on dev data.", "labels": [], "entities": []}, {"text": "Unless otherwise specified, the full set is selected (performs best on dev set) for training.", "labels": [], "entities": []}, {"text": "Baselines We compare to the common and effective point-wise addition (SUM) method.", "labels": [], "entities": []}, {"text": "We additionally include Weighted SUM, which learns overall dimension specific weights from task-specific training, the equivalent of FCT with \u03b1 jk =0 and b ij learned from data.", "labels": [], "entities": [{"text": "FCT", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.5392554402351379}]}, {"text": "Furthermore, we compare to dataset specific baselines: we re-implemented the recursive neural network model (RNN)) and the Dual VSM algorithm in Turney (2012) 6 so that they can be trained on our dataset.", "labels": [], "entities": [{"text": "Turney (2012) 6", "start_pos": 145, "end_pos": 160, "type": "DATASET", "confidence": 0.8052049040794372}]}, {"text": "We also include results for fine-tuning word embeddings in SUM and Weighted SUM with TASK-SPEC objectives, which demonstrate improvements over the corresponding methods without fine-tuning.", "labels": [], "entities": [{"text": "TASK-SPEC", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9488187432289124}]}, {"text": "As before, word embeddings are pre-trained with word2vec.", "labels": [], "entities": []}, {"text": "RNNs serve as another way to model the compositionally of bigrams.", "labels": [], "entities": []}, {"text": "We run an RNN on bigrams and associated sub-trees, the same setting In the experiments we have 60 different matrices in total for bigram NPs.", "labels": [], "entities": []}, {"text": "The number is larger than that in due to incorrect tags in automatic parses.", "labels": [], "entities": []}, {"text": "Since the RNN model has time complexity O(n 2 ), we compare RNNs with different sized embeddings.", "labels": [], "entities": []}, {"text": "The first one uses embeddings with 50 dimensions, which has the same size as the embeddings used in, and has similar complexity to our model with 200 dimension embeddings.", "labels": [], "entities": []}, {"text": "The second model uses the same 200 dimension embeddings as our model but is significantly more computationally expensive.", "labels": [], "entities": []}, {"text": "For all models, we normalize the embeddings so that the L-2 norm equals 1, which is important in measuring semantic similarity via inner product.", "labels": [], "entities": []}, {"text": "So far our experiments have focused on bigram phrases.", "labels": [], "entities": []}, {"text": "We now show that FCT improves for longer n-gram phrases.", "labels": [], "entities": [{"text": "FCT", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.38734960556030273}]}, {"text": "Without fine-tuning, FCT performs significantly better than the other models, showing that the model can better capture the context and annotation information related to phrase semantics with the help of rich features.", "labels": [], "entities": [{"text": "FCT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.7739715576171875}]}, {"text": "With different amounts of training data, we found that WSum and FCT both perform better when trained on the PPDB-: Ablation study on dev set of the PPDB ngram-to-ngram task (MRR @ 10k). by the quality of single word semantics.", "labels": [], "entities": [{"text": "WSum", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.8277543783187866}, {"text": "FCT", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.8866274952888489}]}, {"text": "Therefore, we expect larger gains from FCT on tasks where single word embeddings are less important, such as relation extraction (long distance dependencies) and question understanding (intentions are largely dependent on interrogatives).", "labels": [], "entities": [{"text": "FCT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9425722360610962}, {"text": "relation extraction", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.8500458598136902}, {"text": "question understanding", "start_pos": 162, "end_pos": 184, "type": "TASK", "confidence": 0.8273808658123016}]}, {"text": "Finally, we demonstrate the efficacy of different features in FCT) with an ablation study (Table 9).", "labels": [], "entities": [{"text": "FCT", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9397457242012024}, {"text": "ablation", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9508746862411499}]}, {"text": "Word cluster features contribute most, because the point-wise product between word embedding and its context word cluster representation is actually an approximation of the word-word interaction, which is believed important for phrase compositions.", "labels": [], "entities": [{"text": "phrase compositions", "start_pos": 228, "end_pos": 247, "type": "TASK", "confidence": 0.7430533468723297}]}, {"text": "Head features, though few, also make a big difference, reflecting the importance of syntactic information.", "labels": [], "entities": []}, {"text": "Compound features do not have much of an impact, possibly because the simpler features capture enough information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of NYT and PPDB data. \"Training pairs\" are pairs of bigram phrase and word used in experiments.", "labels": [], "entities": [{"text": "NYT", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.6854116916656494}, {"text": "PPDB data", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.770212709903717}]}, {"text": " Table 3: Language model perplexity and NCE loss on a subset of train, dev, and test NYT data.", "labels": [], "entities": [{"text": "NCE loss", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9105471968650818}, {"text": "NYT data", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.7385968267917633}]}, {"text": " Table 7: Performance on SemEval2013 and Turney2012 semantic similarity tasks. Dual Space 1 : Our reimple- mentation of the method in (Turney, 2012). Dual Space 2 : The result reported in", "labels": [], "entities": []}, {"text": " Table 8: Results on PPDB ngram-to-ngram task.", "labels": [], "entities": [{"text": "PPDB ngram-to-ngram task", "start_pos": 21, "end_pos": 45, "type": "DATASET", "confidence": 0.6260904669761658}]}, {"text": " Table 9: Ablation study on dev set of the PPDB  ngram-to-ngram task (MRR @ 10k).", "labels": [], "entities": []}]}