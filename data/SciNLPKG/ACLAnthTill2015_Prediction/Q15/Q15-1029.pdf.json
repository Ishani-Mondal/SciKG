{"title": [], "abstractContent": [{"text": "Machine learning approaches to coreference resolution vary greatly in the modeling of the problem: while early approaches operated on the mention pair level, current research fo-cuses on ranking architectures and antecedent trees.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.9494718313217163}]}, {"text": "We propose a unified representation of different approaches to coreference resolution in terms of the structure they operate on.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.9706586301326752}]}, {"text": "We represent several coreference resolution approaches proposed in the literature in our framework and evaluate their performance.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.8633754253387451}]}, {"text": "Finally, we conduct a systematic analysis of the output of these approaches, highlighting differences and similarities.", "labels": [], "entities": []}], "introductionContent": [{"text": "Coreference resolution is the task of determining which mentions in a text are used to refer to the same real-world entity.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9222011864185333}]}, {"text": "The era of statistical natural language processing saw the shift from rule-based approaches to increasingly sophisticated machine learning models.", "labels": [], "entities": [{"text": "statistical natural language processing", "start_pos": 11, "end_pos": 50, "type": "TASK", "confidence": 0.6935550570487976}]}, {"text": "While early approaches cast the problem as binary classification of mention pairs (), recent approaches make use of complex structures to represent coreference relations ().", "labels": [], "entities": []}, {"text": "The aim of this paper is to devise a framework for coreference resolution that leads to a unified representation of different approaches to coreference resolution in terms of the structure they operate on.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.9571848809719086}, {"text": "coreference resolution", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.9213933348655701}]}, {"text": "Previous work in other areas of natural language processing such as parsing) and machine translation has shown that providing unified representations of approaches to a problem deepens its understanding and can also lead to empirical improvements.", "labels": [], "entities": [{"text": "natural language processing such as parsing", "start_pos": 32, "end_pos": 75, "type": "TASK", "confidence": 0.6035660207271576}, {"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.8038576245307922}]}, {"text": "By implementing popular approaches in this framework, we can highlight structural differences and similarities between them.", "labels": [], "entities": []}, {"text": "Furthermore, this establishes a setting to systematically analyze the contribution of the underlying structure to performance, while fixing parameters such as preprocessing and features.", "labels": [], "entities": []}, {"text": "In particular, we analyze approaches to coreference resolution and point out that they mainly differ in the structures they operate on.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.9768655598163605}]}, {"text": "We then note that these structures are not annotated in the training data (Section 2).", "labels": [], "entities": []}, {"text": "Motivated by this observation, we develop a machine learning framework for structured prediction with latent variables for coreference resolution (Section 3).", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.6630122363567352}, {"text": "coreference resolution", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.9815837442874908}]}, {"text": "We formalize the mention pair model (), mention ranking architectures) and antecedent trees) in our framework and highlight key differences and similarities.", "labels": [], "entities": []}, {"text": "Finally, we present an extensive comparison and analysis of the implemented approaches, both quantitative and qualitative (Sections 5 and 6).", "labels": [], "entities": []}, {"text": "Our analysis shows that a mention ranking architecture with latent antecedents performs best, mainly due to its ability to structurally model determining anaphoricity.", "labels": [], "entities": []}, {"text": "Finally, we briefly describe how entity-centric approaches fit into our framework).", "labels": [], "entities": []}, {"text": "An open source toolkit which implements the machine learning framework and the approaches discussed in this paper is available for download 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "We now evaluate model variants based on different latent structures on a large benchmark corpus.", "labels": [], "entities": []}, {"text": "The aim of this section is to compare popular approaches to coreference only in terms of the structure they operate on, fixing preprocessing and feature set.", "labels": [], "entities": []}, {"text": "In Section 6 we complement this comparison with a qualitative analysis of the influence of the structures on the output.", "labels": [], "entities": []}, {"text": "The aim of our evaluation is to assess the effectiveness and competitiveness of the models implemented in our framework in a realistic coreference setting, i.e. without using gold information such as gold mentions.", "labels": [], "entities": []}, {"text": "As all models we consider share the same preprocessing and features, this allows fora fair comparison of the individual structures.", "labels": [], "entities": []}, {"text": "We train, evaluate and analyze the models on the English data of the CoNLL-2012 shared task on multilingual coreference resolution ( We work in a setting that corresponds to the shared task's closed track ().", "labels": [], "entities": [{"text": "CoNLL-2012 shared task", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.8249629735946655}, {"text": "multilingual coreference resolution", "start_pos": 95, "end_pos": 130, "type": "TASK", "confidence": 0.6485584576924642}]}, {"text": "That is, we make use of the automatically created annotation layers (parse trees, NE information, ...) shipped with the data.", "labels": [], "entities": []}, {"text": "As additional resources we use only WordNet 3.0  We evaluate the models on the development and the test sets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.9279059171676636}]}, {"text": "When evaluating on the test set, we train on the concatenation of the training and development set.", "labels": [], "entities": []}, {"text": "After preliminary experiments with the ranking model with closest antecedents on the development set, we set the number of perceptron epochs to 5 and set \u03bb = 100 in the cost function.", "labels": [], "entities": []}, {"text": "We assess statistical significance of the difference in F 1 score for two approaches via an approximate randomization test.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9583113789558411}]}, {"text": "We say an improvement is statistically significant if p < 0.05.", "labels": [], "entities": []}, {"text": "shows the result of all model configurations discussed in the previous section on CoNLL'12 English development and test data.", "labels": [], "entities": [{"text": "CoNLL'12 English development and test data", "start_pos": 82, "end_pos": 124, "type": "DATASET", "confidence": 0.8615752458572388}]}, {"text": "In order to put the numbers into context, we also report the results of, who present a system that implements an antecedent tree model with non-local features.", "labels": [], "entities": []}, {"text": "Their system is the highestperforming system on the CoNLL data which operates in a closed track setting.", "labels": [], "entities": [{"text": "CoNLL data", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.9867790341377258}]}, {"text": "We also compare with, the winning system of the CoNLL-2012 shared task (Pradhan et al., 2012) . Both systems were trained on training data for evaluating on the development set, and on the concatena- We do not compare with the system of  tion of training and development data for evaluating on the test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of different systems and model variants on CoNLL-2012 English development and test data. Models  below the dashed lines are implemented in our framework. The best F 1 score results for each dataset and metric are  boldfaced.  *  indicates significant improvements in F 1 score of Ranking: Closest compared to Mention Pair;  \u2020 indicates  significant improvements of Ranking: Latent compared to Ranking: Closest; indicates significant improvements of  Ranking: Latent compared to Antecedent Trees; \u00d7 indicates significant improvements of Ranking: Latent compared  to Bj\u00f6rkelund and Kuhn (2014). We do not perform significance tests on differences in average F 1 since this measure  constitutes an average over other F 1 scores.", "labels": [], "entities": [{"text": "CoNLL-2012 English development and test data", "start_pos": 61, "end_pos": 105, "type": "DATASET", "confidence": 0.9068910479545593}, {"text": "F 1 score", "start_pos": 181, "end_pos": 190, "type": "METRIC", "confidence": 0.9459803104400635}, {"text": "F 1 score", "start_pos": 285, "end_pos": 294, "type": "METRIC", "confidence": 0.9500627517700195}]}, {"text": " Table 2: Overview of recall and precision errors.", "labels": [], "entities": [{"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9979124665260315}, {"text": "precision errors", "start_pos": 33, "end_pos": 49, "type": "METRIC", "confidence": 0.959326446056366}]}]}