{"title": [{"text": "Improving Topic Models with Latent Feature Word Representations", "labels": [], "entities": [{"text": "Improving Topic Models", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8510885636011759}]}], "abstractContent": [{"text": "Probabilistic topic models are widely used to discover latent topics in document collections , while latent feature vector representations of words have been used to obtain high performance in many NLP tasks.", "labels": [], "entities": []}, {"text": "In this paper , we extend two different Dirichlet multino-mial topic models by incorporating latent feature vector representations of words trained on very large corpora to improve the word-topic mapping learnt on a smaller corpus.", "labels": [], "entities": []}, {"text": "Experimental results show that by using information from the external corpora, our new models produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.7350820899009705}, {"text": "document classification", "start_pos": 171, "end_pos": 194, "type": "TASK", "confidence": 0.7155865281820297}]}], "introductionContent": [{"text": "Topic modeling algorithms, such as Latent Dirichlet Allocation ( and related methods, are often used to learn a set of latent topics fora corpus, and predict the probabilities of each word in each document belonging to each topic (.", "labels": [], "entities": [{"text": "Topic modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7340047061443329}]}, {"text": "Conventional topic modeling algorithms such as these infer document-to-topic and topic-to-word distributions from the co-occurrence of words within documents.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7374375760555267}]}, {"text": "But when the training corpus of documents is small or when the documents are short, the resulting distributions might be based on little evidence. and show that it helps to exploit external knowledge to improve the topic representations.", "labels": [], "entities": []}, {"text": "employed web search results to improve the information in short texts.", "labels": [], "entities": []}, {"text": "assumed that the small corpus is a sample of topics from a larger corpus like Wikipedia, and then use the topics discovered in the larger corpus to help shape the topic representations in the small corpus.", "labels": [], "entities": []}, {"text": "However, if the larger corpus has many irrelevant topics, this will \"use up\" the topic space of the model.", "labels": [], "entities": []}, {"text": "In addition, proposed an extension of LDA that uses external information about word similarity, such as thesauri and dictionaries, to smooth the topic-to-word distribution.", "labels": [], "entities": []}, {"text": "Topic models have also been constructed using latent features.", "labels": [], "entities": []}, {"text": "Latent feature (LF) vectors have been used fora wide range of NLP tasks).", "labels": [], "entities": []}, {"text": "The combination of values permitted by latent features forms a high dimensional space which makes it is well suited to model topics of very large corpora.", "labels": [], "entities": []}, {"text": "Rather than relying solely on a multinomial or latent feature model, as in, and, we explore how to take advantage of both latent feature and multinomial models by using a latent feature representation trained on a large external corpus to supplement a multinomial topic model estimated from a smaller corpus.", "labels": [], "entities": []}, {"text": "Our main contribution is that we propose two new latent feature topic models which integrate latent feature word representations into two Dirichlet multinomial topic models: a Latent Dirichlet Allocation (LDA) model () and a onetopic-per-document Dirichlet Multinomial Mixture (DMM) model ().", "labels": [], "entities": []}, {"text": "Specifically, we replace the topic-to-word Dirichlet multinomial component which generates the words from topics in each Dirichlet multinomial topic model by a twocomponent mixture of a Dirichlet multinomial component and a latent feature component.", "labels": [], "entities": []}, {"text": "In addition to presenting a sampling procedure for the new models, we also compare using two different sets of pre-trained latent feature word vectors with our models.", "labels": [], "entities": []}, {"text": "We achieve significant improvements on topic coherence evaluation, document clustering and document classification tasks, especially on corpora of short documents and corpora with few documents.", "labels": [], "entities": [{"text": "topic coherence evaluation", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.7787618239720663}, {"text": "document clustering", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7362620830535889}, {"text": "document classification", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.7155060470104218}]}], "datasetContent": [{"text": "To investigate the performance of our new LF-LDA and LF-DMM models, we compared their performance against baseline LDA and DMM models on topic coherence, document clustering and document classification evaluations.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.705165445804596}, {"text": "document classification evaluations", "start_pos": 178, "end_pos": 213, "type": "TASK", "confidence": 0.7730349004268646}]}, {"text": "The topic coherence evaluation measures the coherence of topic-word associations, i.e. it directly evaluates how coherent the assignment of words to topics is.", "labels": [], "entities": []}, {"text": "The document clustering and document classification tasks evaluate how useful the topics assigned to documents are in clustering and classification tasks.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6991198360919952}, {"text": "document classification", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.7004950195550919}]}, {"text": "Because we expect our new models to perform comparatively well in situations where there is little data about topic-to-word distributions, our experiments focus on corpora with few or short documents.", "labels": [], "entities": []}, {"text": "We also investigated which values of \u03bb perform well, and compared the performance when using two different sets of pre-trained word vectors in these new models.", "labels": [], "entities": []}, {"text": "We conducted experiments on the 20-Newsgroups dataset, the TagMyNews news dataset and the Sanders Twitter corpus.", "labels": [], "entities": [{"text": "20-Newsgroups dataset", "start_pos": 32, "end_pos": 53, "type": "DATASET", "confidence": 0.7751202881336212}, {"text": "TagMyNews news dataset", "start_pos": 59, "end_pos": 81, "type": "DATASET", "confidence": 0.9578848481178284}, {"text": "Sanders Twitter corpus", "start_pos": 90, "end_pos": 112, "type": "DATASET", "confidence": 0.9432951807975769}]}, {"text": "The 20-Newsgroups dataset 5 contains about 19,000 newsgroup documents evenly grouped into 20 different categories.", "labels": [], "entities": []}, {"text": "The TagMyNews news dataset consists of about 32,600 English RSS news items grouped into 7 categories, where each news document has a news title and a short description.", "labels": [], "entities": [{"text": "TagMyNews news dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.9306931098302206}]}, {"text": "In our experiments, we also used a news title dataset which consists of just the news titles from the TagMyNews news dataset.", "labels": [], "entities": [{"text": "TagMyNews news dataset", "start_pos": 102, "end_pos": 124, "type": "DATASET", "confidence": 0.9618187546730042}]}, {"text": "Each dataset was down-cased, and we removed non-alphabetic characters and stop-words found in the stop-word list in the Mallet toolkit).", "labels": [], "entities": [{"text": "Mallet toolkit", "start_pos": 120, "end_pos": 134, "type": "DATASET", "confidence": 0.9739221930503845}]}, {"text": "We also removed words shorter than 3 characters and words appearing less than 10 times in the 20-Newsgroups corpus, and under 5 times in the TagMyNews news and news titles datasets.", "labels": [], "entities": [{"text": "TagMyNews news and news titles datasets", "start_pos": 141, "end_pos": 180, "type": "DATASET", "confidence": 0.9327045480410258}]}, {"text": "In addition, words not found in both Google and Stanford vector representations were also removed.", "labels": [], "entities": []}, {"text": "We refer to the cleaned 20-Newsgroups, TagMyNews news and news title datasets as N20, TMN and TMNtitle, respectively.", "labels": [], "entities": [{"text": "TagMyNews news and news title datasets", "start_pos": 39, "end_pos": 77, "type": "DATASET", "confidence": 0.7482266326745352}, {"text": "N20", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.9390965700149536}]}, {"text": "We also performed experiments on two subsets of the N20 dataset.", "labels": [], "entities": [{"text": "N20 dataset", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.9704971313476562}]}, {"text": "The N20short dataset consists of all documents from the N20 dataset with less than 21 words.", "labels": [], "entities": [{"text": "N20short dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.986221581697464}, {"text": "N20 dataset", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.9548206329345703}]}, {"text": "The N20small dataset contains 400 documents consisting of 20 randomly selected documents from each group of the N20 dataset.", "labels": [], "entities": [{"text": "N20small dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.981320321559906}, {"text": "N20 dataset", "start_pos": 112, "end_pos": 123, "type": "DATASET", "confidence": 0.9522100985050201}]}, {"text": "Finally, we also experimented on the publicly available Sanders Twitter corpus.", "labels": [], "entities": [{"text": "Sanders Twitter corpus", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.9308774669965109}]}, {"text": "8 This corpus consists of 5,512 Tweets grouped into four different topics (Apple, Google, Microsoft, and Twitter).", "labels": [], "entities": []}, {"text": "Due to restrictions in Twitter's Terms of Service, the actual Tweets need to be downloaded using 5,512 Tweet IDs.", "labels": [], "entities": []}, {"text": "There are 850 Tweets not available to download.", "labels": [], "entities": []}, {"text": "After removing the non-English Tweets, 3,115 Tweets remain.", "labels": [], "entities": []}, {"text": "In addition to converting into lowercase and removing non-alphabetic characters, words were normalized by using a lexical normalization dictionary for microblogs).", "labels": [], "entities": []}, {"text": "We then removed stop-words, words shorter than 3 characters or appearing less than 3 times in the corpus.", "labels": [], "entities": []}, {"text": "The four words apple, google, microsoft and twitter were removed as these four words occur in every Tweet in the corresponding topic.", "labels": [], "entities": []}, {"text": "Moreover, words not found in both Google and Stanford vector lists were also removed.", "labels": [], "entities": []}, {"text": "In all our experiments, after removing words from documents, any document with a zero word count was also removed from the corpus.", "labels": [], "entities": []}, {"text": "For the Twitter corpus, this resulted in just 2,520 remaining Tweets.", "labels": [], "entities": []}, {"text": "This section examines the quality of the topic-word mappings induced by our models.", "labels": [], "entities": []}, {"text": "In our models, topics are distributions over words.", "labels": [], "entities": []}, {"text": "The topic coherence evaluation measures to what extent the highprobability words in each topic are semantically coherent ().", "labels": [], "entities": []}, {"text": "The method presented in uses the normalized pointwise mutual information (NPMI) score and has a strong correlation with humanjudged coherence.", "labels": [], "entities": []}, {"text": "A higher NPMI score indicates that the topic distributions are semantically more coherent.", "labels": [], "entities": []}, {"text": "Given a topic t represented by its top-N topic words w 1 , w 2 , ..., w N , the NPMI score fort is: NPMI-Score(t) = 1i<jN log P(wi,wj ) P(wi)P(wj )  We compared our models to the baseline models in a document clustering task.", "labels": [], "entities": [{"text": "document clustering task", "start_pos": 200, "end_pos": 224, "type": "TASK", "confidence": 0.7323769728342692}]}, {"text": "After using a topic model to calculate the topic probabilities of a document, we assign every document the topic with the highest probability given the document ().", "labels": [], "entities": []}, {"text": "We use two common metrics to evaluate clustering performance: Purity and normalized mutual information (NMI): see (3) for details of these evaluations.", "labels": [], "entities": [{"text": "normalized mutual information (NMI)", "start_pos": 73, "end_pos": 108, "type": "METRIC", "confidence": 0.6613481044769287}]}, {"text": "Purity and NMI scores always range from 0.0 to 1.0, and higher scores reflect better clustering performance.", "labels": [], "entities": [{"text": "Purity", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9881970286369324}, {"text": "NMI", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.6559886932373047}]}, {"text": "Figures 5 and 6 present Purity and NMI results obtained by the LDA,-LDA and glove-LDA models on the N20short dataset with the numbers of topics T set to either 20 or 40, and the value of the mixture weight \u03bb varied from 0.0 to 1.0.", "labels": [], "entities": [{"text": "Purity", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9810715317726135}, {"text": "N20short dataset", "start_pos": 100, "end_pos": 116, "type": "DATASET", "confidence": 0.9885903596878052}]}, {"text": "We found that setting \u03bb to 1.0 (i.e. using only the latent features to model words), the glove-LDA produced 1%+ higher scores on both Purity and NMI results than the w2v-LDA when using 20 topics.", "labels": [], "entities": []}, {"text": "However, the two models glove-LDA and w2v-LDA returned equivalent results with 40 topics where they sample if we do not take the order of the most probable words into account.", "labels": [], "entities": []}, {"text": "Unlike the document clustering task, the document classification task evaluates the distribution over topics for each document.", "labels": [], "entities": [{"text": "document clustering task", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.7806450227896372}, {"text": "document classification task", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.753108928600947}]}, {"text": "Just as in the document clustering task, the mixture weight \u03bb = 0.6 obtains the highest classification performances on the N20short dataset.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7252373099327087}, {"text": "N20short dataset", "start_pos": 123, "end_pos": 139, "type": "DATASET", "confidence": 0.9846794903278351}]}, {"text": "For example with T = 40, our w2v-LDA and glove-LDA obtain F 1 scores at 40.0% and 38.9% which are 4.5% and 3.4% higher than F 1 score at 35.5% obtained by the LDA model, respectively.", "labels": [], "entities": [{"text": "T", "start_pos": 17, "end_pos": 18, "type": "METRIC", "confidence": 0.9827026128768921}, {"text": "F 1 scores", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9793784221013387}, {"text": "F 1 score", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9635826746622721}]}, {"text": "We report classification results on the remaining experimental datasets with mixture weight \u03bb = 0.6 in tables 9, 10 and 11.", "labels": [], "entities": []}, {"text": "Unlike the clustering results, the LDA model does better than the DMM model for classification on the TMN dataset.", "labels": [], "entities": [{"text": "TMN dataset", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.8980644345283508}]}, {"text": "uations, our models perform better than the baseline models.", "labels": [], "entities": []}, {"text": "In particular, on the small N20small and Twitter datasets, when the number of topics T is equal to number of ground truth labels (i.e. 20 and 4 correspondingly), our w2v-LDA obtains 5 + % higher F 1 score than the LDA model.", "labels": [], "entities": [{"text": "N20small and Twitter datasets", "start_pos": 28, "end_pos": 57, "type": "DATASET", "confidence": 0.8075763881206512}, {"text": "F 1 score", "start_pos": 195, "end_pos": 204, "type": "METRIC", "confidence": 0.9885083039601644}]}, {"text": "In addition, our w2v-DMM model achieves 5.4% and 2.9% higher F 1 score than the DMM model on short TMN and TMNtitle datasets with T = 80, respectively.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9922804832458496}, {"text": "TMNtitle datasets", "start_pos": 107, "end_pos": 124, "type": "DATASET", "confidence": 0.7982339560985565}, {"text": "T", "start_pos": 130, "end_pos": 131, "type": "METRIC", "confidence": 0.9838799834251404}]}, {"text": "Google word2vec vs. Stanford glove word vectors: The comparison of the Google and Stanford pre-trained word vectors for classification is similar to the one for clustering.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of experimental datasets. #g: number of  ground truth labels; #docs: number of documents; #w/d:  the average number of words per document; V : the num- ber of word types", "labels": [], "entities": []}, {"text": " Table 6: Purity and NMI results (mean and standard deviation) on the N20 and N20small datasets with \u03bb = 0.6.  Improve. row denotes the difference between the best result obtained by our model and the baseline model.", "labels": [], "entities": [{"text": "Purity", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9960739612579346}, {"text": "NMI", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.8971098065376282}, {"text": "N20 and N20small datasets", "start_pos": 70, "end_pos": 95, "type": "DATASET", "confidence": 0.7999181002378464}, {"text": "Improve", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9948359727859497}]}, {"text": " Table 7: Purity and NMI results on the TMN and TMNtitle datasets with the mixture weight \u03bb = 0.6.", "labels": [], "entities": [{"text": "Purity", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9932004809379578}, {"text": "NMI", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9280534982681274}, {"text": "TMN", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.8619316220283508}, {"text": "TMNtitle datasets", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.8198830187320709}]}, {"text": " Table 8: Purity and NMI results on the Twitter dataset with the mixture weight \u03bb = 0.6.", "labels": [], "entities": [{"text": "Purity", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9933858513832092}, {"text": "NMI", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9709689021110535}, {"text": "Twitter dataset", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.9210339188575745}]}, {"text": " Table 9: F 1 scores (mean and standard deviation) for  N20 and N20small datasets.", "labels": [], "entities": [{"text": "F 1 scores (mean and standard deviation)", "start_pos": 10, "end_pos": 50, "type": "METRIC", "confidence": 0.8406091398662991}, {"text": "N20", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9664404988288879}, {"text": "N20small datasets", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.8191749751567841}]}, {"text": " Table 10: F 1 scores for TMN and TMNtitle datasets.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9798116087913513}, {"text": "TMNtitle datasets", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.8409239649772644}]}, {"text": " Table 11: F 1 scores for Twitter dataset.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9761846860249838}, {"text": "Twitter dataset", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.9612355828285217}]}]}