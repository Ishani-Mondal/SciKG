{"title": [], "abstractContent": [{"text": "We present the first large-scale, corpus based verification of Dowty's seminal theory of proto-roles.", "labels": [], "entities": [{"text": "Dowty's seminal theory", "start_pos": 63, "end_pos": 85, "type": "DATASET", "confidence": 0.946430966258049}]}, {"text": "Our results demonstrate both the need for and the feasibility of a property-based annotation scheme of semantic relationships, as opposed to the currently dominant notion of categorical roles.", "labels": [], "entities": []}], "introductionContent": [{"text": "For decades researchers have debated the number and character of thematic roles required fora theory of the syntax/semantics interface.", "labels": [], "entities": []}, {"text": "AGENT and PA-TIENT are canonical examples, but questions emerge such as: should we have a distinct role for BENE-FICIARY?", "labels": [], "entities": [{"text": "AGENT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8288382291793823}, {"text": "PA-TIENT", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9445635080337524}, {"text": "BENE-FICIARY", "start_pos": 108, "end_pos": 120, "type": "METRIC", "confidence": 0.987618088722229}]}, {"text": "What are the boundaries between these roles?", "labels": [], "entities": []}, {"text": "And so on., in a seminal article, responded to this debate by constructing the notion of a ProtoAgent and Proto-Patient, based on entailments that can be mapped to questions, such as: \"Did the argument change state?\", or \"Did the argument have volitional involvement in the event?\".", "labels": [], "entities": []}, {"text": "Dowty argued that these properties group together in the lexicon non-categorically, in away that aligns with classic Agent/Patient intuitions.", "labels": [], "entities": []}, {"text": "For instance, a ProtoPatient often both changes state (but might not), and often is causally affected by another participant.", "labels": [], "entities": []}, {"text": "Various resources have been developed for computational linguists working on 'Semantic Role Labeling' (SRL), largely under the classical, categorical notion of role.", "labels": [], "entities": [{"text": "Semantic Role Labeling' (SRL)", "start_pos": 78, "end_pos": 107, "type": "TASK", "confidence": 0.8085396587848663}]}, {"text": "Here we revisit Dowty's re- * Corresponding authors.", "labels": [], "entities": [{"text": "Dowty's re- * Corresponding authors", "start_pos": 16, "end_pos": 51, "type": "DATASET", "confidence": 0.9253774200166974}]}, {"text": "search as computational linguists desiring data fora new task, Semantic Proto-Role Labeling (SPRL), in which existing coarse-grained categorical roles are replaced by scalar judgements of Dowty-inspired properties.", "labels": [], "entities": [{"text": "Semantic Proto-Role Labeling (SPRL)", "start_pos": 63, "end_pos": 98, "type": "TASK", "confidence": 0.7876302798589071}]}, {"text": "As the availability of supporting data is a critical component of such a task, much of our efforts here are focused on showing that everyday English speakers (untrained annotators) are able to answer basic questions about semantic relationships.", "labels": [], "entities": []}, {"text": "In this work we consider the following questions: (i) can crowdsourcing methods be used to empirically validate the formal linguistic theory of Dowty, following prior work in psycholinguistics)?", "labels": [], "entities": [{"text": "Dowty", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.9392099380493164}]}, {"text": "(ii) How might existing semantic annotation efforts be used in such a pursuit?", "labels": [], "entities": []}, {"text": "(iii) Can the pursuit of Dowty's semantic properties be turned into a practical and scalable annotation task?", "labels": [], "entities": [{"text": "Dowty's semantic", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9033161203066508}]}, {"text": "(iv) Do the results of such an annotation task (at various scales, including over very large corpora) continue to confirm Dowty's proto-role hypothesis?", "labels": [], "entities": [{"text": "Dowty", "start_pos": 122, "end_pos": 127, "type": "DATASET", "confidence": 0.9013687372207642}]}, {"text": "And finally, (v) how do the resulting configurations of finegrained role properties compare to coarser annotated roles in resources such as VerbNet?", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 140, "end_pos": 147, "type": "DATASET", "confidence": 0.9014502167701721}]}, {"text": "We first derive a set of basic semantic questions pertaining to Dowty-inspired properties.", "labels": [], "entities": []}, {"text": "These questions are used in two Mechanical Turk HITs that address the above issuess.", "labels": [], "entities": [{"text": "Mechanical Turk HITs", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.5662254393100739}]}, {"text": "In the first HIT, we build on psycholinguistic work) to directly access 'type-level' intuitions about a lexical item, by asking subjects property-questions using made-up (\"nonce\") words in argument positions.", "labels": [], "entities": []}, {"text": "Our results replicate these previous experiments, and demonstrate that what can be done in this domain in a controlled lab experiment can be done via crowdsourcing.", "labels": [], "entities": []}, {"text": "We extend this to a large-scale MTurk annotation task using corpus data.", "labels": [], "entities": [{"text": "MTurk annotation task", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.9086164037386576}]}, {"text": "This task presents an annotator with a particular ('token-level') sentence from PropBank () and a highlighted argument, and asks them fora likelihood judgment about a property; for example, \"How likely or unlikely is it that ARG is sentient?\".", "labels": [], "entities": [{"text": "PropBank", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.9477697610855103}]}, {"text": "By looking across many token-level instances of a verb, we can then infer type-level information about the verb.", "labels": [], "entities": []}, {"text": "We discuss results from this task over 11 role properties annotated by a single (trusted) annotator on approximately 5000 verb tokens.", "labels": [], "entities": []}, {"text": "Our results represent the first large-scale corpus study explicitly aimed at confirming Dowty's proto-role hypothesis: Proto-Agent properties predict the mapping of semantic arguments to subject and object.", "labels": [], "entities": []}, {"text": "We show that this allows us to both capture and discover finegrained details of semantic roles that coarser annotation schemes such as VerbNet do not: empirically, this data set shows a great degree of role fragmentation, much greater than any existing annotation scheme allows.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 135, "end_pos": 142, "type": "DATASET", "confidence": 0.932295024394989}]}, {"text": "The results of this task represent anew large-scale annotated resource, involving close to 345 hours of human effort.", "labels": [], "entities": []}], "datasetContent": [{"text": "The literature review makes clear that understanding and annotating fine-grained role properties is valuable in both linguistic theory and in computational linguistics: under many sets of assumptions, such properties ground out the theory of coarse-grained roles.", "labels": [], "entities": [{"text": "linguistic theory", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.7026438415050507}]}, {"text": "We follow in directly addressing fine-grained properties, herein the context of the proto-role theory.", "labels": [], "entities": []}, {"text": "The proto-role approach gives us a set of testable questions to assess on a corpus.", "labels": [], "entities": []}, {"text": "We focus on two main issues: (i) whether the proto-role solution to the mapping problem scales up to very large sets of data, and (ii) the prediction that there will be a very large set of property configurations attested as roles in a large data set.", "labels": [], "entities": []}, {"text": "If the predictions from the proto-role theory are true, then we conclude that a large data set annotated with fine-grained role properties maybe valuable in tasks related to semantic roles and event detection.", "labels": [], "entities": [{"text": "event detection", "start_pos": 193, "end_pos": 208, "type": "TASK", "confidence": 0.7674897313117981}]}, {"text": "To assess these predictions, we broadly follow Kako (2006b) in operationalizing proto-roles using likelihood questions targeting specific role properties in sentences of English.", "labels": [], "entities": []}, {"text": "This paper presents two experiments that implement this strategy.", "labels": [], "entities": []}, {"text": "In the remainder of this section we describe the general setup of the experiments.", "labels": [], "entities": []}, {"text": "In particular, we describe a pro-  cess for arriving at the specific fine-grained property questions we ask, the creation of the data set that we ask the questions about, the task that Mechanical Turkers are presented with, and the manner in which we analyze and display the results.", "labels": [], "entities": []}, {"text": "We first inspected the role hierarchy of Bonial et al.", "labels": [], "entities": []}, {"text": "(2011) along with the associated textual definitions: these were manually decomposed into a set of explict binary properties.", "labels": [], "entities": []}, {"text": "For example, we define the SemLink ACTOR role as a participant that has the binary property of INSTIGATION.", "labels": [], "entities": []}, {"text": "From these properties we subselected those that were most similar to the original questions proposed by Dowty (see).", "labels": [], "entities": [{"text": "Dowty", "start_pos": 104, "end_pos": 109, "type": "DATASET", "confidence": 0.9580797553062439}]}, {"text": "For each such property we then generated a question in natural language to be posed to annotators given an example sentence (see).", "labels": [], "entities": []}, {"text": "The set we report on here represents a subset of the questions we have tested; in ongoing work we are evaluating whether we can expand Dowty's set of questions, e.g. to capture roles such as INSTRUMENT.", "labels": [], "entities": [{"text": "Dowty's set of questions", "start_pos": 135, "end_pos": 159, "type": "DATASET", "confidence": 0.9244142055511475}, {"text": "INSTRUMENT", "start_pos": 191, "end_pos": 201, "type": "TASK", "confidence": 0.7577229142189026}]}, {"text": "Methods Because we are interested in the potential impact of Dowty's proto-roles theory on human language technologies, we perform a number of related crowdsourcing experiments, with the dual aim of validating the existing (psycho-)linguistic literature on proto-roles as well as piloting this highly scalable framework for future decompositional semantic annotation efforts.", "labels": [], "entities": []}, {"text": "All of the crowdsourcing experiments in this paper are run using Amazon Mechanical Turk, and (ex-479 cept for the kappa scores reported for experiment 2) all workers were recruited from the MTurk worker pool.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 65, "end_pos": 87, "type": "DATASET", "confidence": 0.9340943694114685}, {"text": "MTurk worker pool", "start_pos": 190, "end_pos": 207, "type": "DATASET", "confidence": 0.8535441756248474}]}, {"text": "The basic setup of the experiments in Sections 4 and 5 is the same.", "labels": [], "entities": []}, {"text": "The Mechanical Turk worker is presented with a single sentence with a highlighted verb and one highlighted argument of that verb.", "labels": [], "entities": []}, {"text": "Then the worker answers all of the questions in for that verb-argument pair using a Likert scale from 1 to 5, with the response labels: very unlikely, somewhat unlikely, not enough information, somewhat likely, and very likely (See.", "labels": [], "entities": []}, {"text": "Each Mechanical Turk HIT yields responses for all the questions in applied to a single verbargument pair.", "labels": [], "entities": []}, {"text": "The Mechanical Turk experiments are run with two types of sentences: those with real verbs and nonsense (\"nonce\") arguments, and those with entirely real English sentences.", "labels": [], "entities": []}, {"text": "Section 4 discusses the former \"type-level\" HIT with nonce arguments, while Section 5 discusses the latter \"tokenlevel\" annotation task with real arguments.", "labels": [], "entities": []}, {"text": "Data To obtain verb-argument pairs for the task described here, we drew sentences from the subset of PropBank that SemLink annotates for VerbNet roles.", "labels": [], "entities": []}, {"text": "From these, we removed verbs annotated as participles, verbs with trace arguments, verbs under negation or modal auxiliaries, and verbs in embedded clauses to ensure that annotators only saw verbs in veridical contexts -contexts where logical operations such as negation do not interfere with direct judgments about the verbs.", "labels": [], "entities": []}, {"text": "For example, in John didn't die, negation reverses the change-ofstate judgment for the whole sentence, despite that being part of the meaning of the verb die.", "labels": [], "entities": [{"text": "negation", "start_pos": 33, "end_pos": 41, "type": "TASK", "confidence": 0.9644620418548584}]}, {"text": "We also removed clausal arguments, as most of the questions in do not make sense when applied to clauses; in ongoing work we are considering how to extend this approach to such arguments.", "labels": [], "entities": []}, {"text": "A total of 7,045 verb tokens with 11,913 argument spans from 6,808 sentences remained after applying these filters.", "labels": [], "entities": []}, {"text": "Analysis To evaluate whether the results of the following experiments accord with Dowty's proposal, we follow Kako (2006b) in taking the mean difference between the property ratings of the subject and object across sentences; see \u00a72.1.", "labels": [], "entities": []}, {"text": "We present these differences in the same format as in.", "labels": [], "entities": []}, {"text": "Here we stick with Kako's evaluation of the results, in order to demonstrate the convergence of the linguistic and psycholinguistic evidence with computational linguistic approaches; our immediate goal in the present work is not to advance the methodology, but to show that these techniques can be pursued through large-scale crowdsourcing.", "labels": [], "entities": []}, {"text": "We perform two Mechanical Turk experiments on verbs: one with nonce arguments, and one with real data in Section 5.", "labels": [], "entities": []}, {"text": "Because nonce arguments have no meaning in their own right, we assume that the properties that annotators assign these arguments area function of the verb and role, not the argument itself.", "labels": [], "entities": []}, {"text": "Hence, we assume that these annotations are at the verb-role type level.", "labels": [], "entities": []}, {"text": "Conversely, the experiment in Section 5 are at the token level, because all arguments have real English instantiations.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.7099365741014481}]}, {"text": "The first experiment we run with nonce arguments is an attempt to replicate the results of.", "labels": [], "entities": []}, {"text": "Recall that Kako (2006b) upholds the psychological validity of Dowty (1991)'s Argument Selection Principle, by demonstrating that human subjects assign Proto-Agent and Proto-Patient properties to grammatical subject and object arguments according to In this experiment, we generate simple transitive sentences with a small set of real verbs and nonce arguments.", "labels": [], "entities": []}, {"text": "The set of verbs are precisely those selected by in his first experiment: add, deny, discover, finish, find, help, maintain, mention, pass, remove, show, write.", "labels": [], "entities": []}, {"text": "The questions we ask workers to answer come from a slightly expanded set of proto-role properties.", "labels": [], "entities": []}, {"text": "There were 16 partic-ipants in the experiment, recruited from the MTurk worker pool, each completing 7.5 HITs on average.", "labels": [], "entities": [{"text": "MTurk worker pool", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.8643972675005595}, {"text": "HITs", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.8753001689910889}]}, {"text": "The results of this experiment, broadly, replicate Kako (2006b)'s earlier findings: human annotators on average indicate that, within the same sentence, the subject-position argument is more likely to have Proto-Agent properties than the objectposition argument, and the object-position argument is more likely to have Proto-Patient properties than the subject-position argument.", "labels": [], "entities": []}, {"text": "This finding is illustrated in.", "labels": [], "entities": []}, {"text": "In addition, the basic facts match Kako's original finding; compare Mean difference (subject \u2212 object) Our ability to replicate Kako (2006b) is significant for two reason: (i) it lends further credence to the proto-role hypothesis, and (ii) it establishes that crowd-sourcing with non-experts in a less controlled situation than a formal experiment results in reasonable annotations for this task with minimal training.", "labels": [], "entities": [{"text": "Mean", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9607044458389282}]}, {"text": "Can this result extend to real corpus data?", "labels": [], "entities": []}, {"text": "If so, the proto-role theory can lead to a valuable source of annotation information about thematic roles.", "labels": [], "entities": []}, {"text": "To assess this, we moved from a synthetic nonce task to a much larger scale version of the task using data from).", "labels": [], "entities": []}, {"text": "Each item in this task presents the annotator with a PropBank sentence with the predicate and argument highlighted, and asks them the same questions about that actual sentence.", "labels": [], "entities": []}, {"text": "The sentences were sampled from PropBank as described in \u00a73.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.965912401676178}]}, {"text": "Our primary goal in this collection effort was to obtain internally consistent, broad-coverage annotations.", "labels": [], "entities": []}, {"text": "Thus we worked through a number of pilot annotation efforts to determine cross-annotator reliability between annotators and with our own judgements.", "labels": [], "entities": []}, {"text": "From the final version of our pilot we selected a single annotator with strong-pairwise agreement amongst the other most prolific annotators.", "labels": [], "entities": []}, {"text": "Compared to the five other most prolific annotators in our final pilot, the pair-wise average Cohen's Kappa with squared metric on an ordinal interpretation of the Likert scale was 0.576.", "labels": [], "entities": []}, {"text": "In our large-scale annotation task, we have collected property judgments on over 9,000 arguments of near 5,000 verb tokens, spanning 1,610 PropBank rolesets.", "labels": [], "entities": []}, {"text": "This represents close to 350 hours of annotation effort.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Because some arguments in PropBank are abstract, for which many of the questions in do not make sense, we added an additional response field that asks \"Does this question make sense\" if the worker gives a response lower than 3 ().", "labels": [], "entities": [{"text": "PropBank", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.9045231342315674}]}, {"text": "shows the results with N/A responses removed.", "labels": [], "entities": []}, {"text": "For presentation purposes, we convert the temporal existence properties to CREATION and DESTRUCTION.", "labels": [], "entities": [{"text": "CREATION", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.944793164730072}, {"text": "DESTRUCTION", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.8355292677879333}]}], "tableCaptions": [{"text": " Table 3: STATIONARY examples from experiment 2.", "labels": [], "entities": [{"text": "STATIONARY", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9136033058166504}]}, {"text": " Table 4: Comparison of role annotations for kill across resources. Ratings: 1=very unlikely, 5=very likely.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of role annotations for split across resources.", "labels": [], "entities": []}, {"text": " Table 6: High and low frequency VerbNet roles (via SemLink) aligned with mean property ratings when excluding  N/A judgments. Freq provides the number of annotations that overlapped with a role. In parenthesis is the number of  cases for that property which were judged applicable (not N/A). E.g. we annotated 1,546 arguments that SemLink calls  AGENT, where 1,355 of those were deemed applicable for the instigation property, with a mean response of 4.9. 12  mid-frequency roles are omitted here for space reasons; the full alignment is provided with the dataset for this paper.", "labels": [], "entities": [{"text": "Freq", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.618531346321106}]}, {"text": " Table 8: Test classification accuracies for each property.", "labels": [], "entities": []}]}