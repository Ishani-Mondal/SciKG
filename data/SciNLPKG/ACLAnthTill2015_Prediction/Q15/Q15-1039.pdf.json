{"title": [{"text": "Imitation Learning of Agenda-based Semantic Parsers", "labels": [], "entities": [{"text": "Imitation Learning of Agenda-based Semantic Parsers", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.9003717402617136}]}], "abstractContent": [{"text": "Semantic parsers conventionally construct logical forms bottom-up in a fixed order, resulting in the generation of many extraneous partial logical forms.", "labels": [], "entities": []}, {"text": "In this paper, we combine ideas from imitation learning and agenda-based parsing to train a semantic parser that searches partial logical forms in a more strategic order.", "labels": [], "entities": [{"text": "agenda-based parsing", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.7356981337070465}]}, {"text": "Empirically, our parser reduces the number of constructed partial logical forms by an order of magnitude, and obtains a 6x-9x speedup over fixed-order parsing, while maintaining comparable accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9978336691856384}]}], "introductionContent": [{"text": "Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems) and other natural language interfaces.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8652262091636658}, {"text": "question answering", "start_pos": 173, "end_pos": 191, "type": "TASK", "confidence": 0.7448527812957764}]}, {"text": "Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.8099927306175232}]}, {"text": "The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers); this lessens the annotation burden and has been instrumental in fueling the first trend: A parsing chart for the utterance \"what city was abraham lincoln born in\".", "labels": [], "entities": []}, {"text": "Numbers in chart cells indicate the number of possible semantic parses constructed over that span, and arrows point to some of the logical forms that were constructed.", "labels": [], "entities": []}, {"text": "There are more than one million possible semantic parses for this utterance.", "labels": [], "entities": []}, {"text": "In this paper, we are interested in training semantic parsers from denotations on large KBs.", "labels": [], "entities": []}, {"text": "The challenge in this setting is that the vocabulary of the target logical language often contains thousands of logical predicates, and there is a mismatch between the structure of the natural language and the logical language.", "labels": [], "entities": []}, {"text": "As a result, the space of possible semantic parses for even a short utterance grows quickly.", "labels": [], "entities": []}, {"text": "For example, consider the utterance \"what city was abraham lincoln born in\".", "labels": [], "entities": []}, {"text": "illustrates the number of possible semantic parses that can be constructed over some of the utterance spans.", "labels": [], "entities": []}, {"text": "Just by combining semantic parses over the spans \"city\", \"lincoln\" and \"born\" we already obtain 362\u00b7391\u00b720 possible parses; at the root, we get over a million parses.", "labels": [], "entities": []}, {"text": "The ambiguity of language thus results in a Even when type constraints are used to prune parses, we still produce more than a million possible parses at the root.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our semantic parser on the WEBQUES-TIONS dataset), which contains 5,810 question-answer pairs.", "labels": [], "entities": [{"text": "WEBQUES-TIONS dataset", "start_pos": 39, "end_pos": 60, "type": "DATASET", "confidence": 0.9578960239887238}]}, {"text": "The questions are about popular topics (e.g., \"what movies does taylor lautner play in?\") and answers are sets of entities obtained through crowdsourcing (all questions are answerable by Freebase).", "labels": [], "entities": []}, {"text": "We use the provided traintest split and perform three random 80%-20% splits of the training data for development.", "labels": [], "entities": []}, {"text": "We perform lexical lookup for Freebase entities using the Freebase Search API and obtain 20 candidate entities for every named entity identified by Stanford CoreNLP ().", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 148, "end_pos": 164, "type": "DATASET", "confidence": 0.9227865934371948}]}, {"text": "We use the lexicon released by to retrieve unary and binary predicates.", "labels": [], "entities": []}, {"text": "We execute \u03bb-DCS logical forms by converting them to SPARQL and querying our local Virtuoso-backed copy of Freebase.", "labels": [], "entities": [{"text": "Virtuoso-backed copy of Freebase", "start_pos": 83, "end_pos": 115, "type": "DATASET", "confidence": 0.8382270634174347}]}, {"text": "During training, we use L 1 regularization, and crudely tune hyperparameters on the development set (beam size K = 200, tolerance for the lazy agenda = 0.01, local reweighting \u03b2 = 1000, and L 1 regularization strength \u03bb = 10 \u22125 ).", "labels": [], "entities": [{"text": "tolerance", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9708650708198547}]}, {"text": "We evaluated our semantic parser using the reward of the predictions, i.e., average F 1 score on predicted vs. true entities overall test examples.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9830932219823202}]}, {"text": "provides our key result comparing the fixed-order parser (FIXEDORDER) and our proposed agenda-based parser (AGENDAIL).", "labels": [], "entities": [{"text": "FIXEDORDER", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9759505391120911}, {"text": "AGENDAIL", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9403404593467712}]}, {"text": "In all subsequent tables, Train, Dev., and Test denote training, development and test accuracies, |Act.| denotes   We found that AGENDAIL is 6x faster than FIXE-DORDER, performs 13x fewer parsing actions, and reduces the number of featurized derivations by an order of magnitude, without loss of accuracy.", "labels": [], "entities": [{"text": "Test", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9360800385475159}, {"text": "AGENDAIL", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9446969032287598}, {"text": "FIXE-DORDER", "start_pos": 156, "end_pos": 167, "type": "METRIC", "confidence": 0.8777667880058289}, {"text": "accuracy", "start_pos": 296, "end_pos": 304, "type": "METRIC", "confidence": 0.9969857335090637}]}, {"text": "presents test set results of our systems, compared to recently published results.", "labels": [], "entities": []}, {"text": "We note that most systems perform question answering without semantic parsing.", "labels": [], "entities": [{"text": "question answering", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7715054452419281}, {"text": "semantic parsing", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7373703420162201}]}, {"text": "Our fixed-order parser, FIXE-DORDER, and agenda-based parser, AGENDAIL, obtain an accuracy of 49.6 and 49.7 respectively.", "labels": [], "entities": [{"text": "FIXE-DORDER", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9648277163505554}, {"text": "AGENDAIL", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9763567447662354}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.999466598033905}]}, {"text": "This improves accuracy compared to all previous systems, except fora recently published semantic parser presented by, whose accuracy is 52.5.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9990243911743164}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9991602897644043}]}, {"text": "We attribute our accuracy improvement compared to previous systems to the new features and changes to the model, as we discuss below.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9991146922111511}]}], "tableCaptions": [{"text": " Table 1: Test set results for the standard fixed-order  parser (FIXEDORDER) and our new agenda-based parser  (AGENDAIL), which substantially reduces parsing time and the  number of parsing actions at no cost to accuracy.", "labels": [], "entities": [{"text": "FIXEDORDER", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.8396393060684204}, {"text": "AGENDAIL", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9118512868881226}, {"text": "parsing", "start_pos": 150, "end_pos": 157, "type": "TASK", "confidence": 0.9746271371841431}, {"text": "accuracy", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.9948508143424988}]}, {"text": " Table 3: Development set results for variants of AGENDAIL.", "labels": [], "entities": [{"text": "AGENDAIL", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.486686646938324}]}, {"text": " Table 3. We see that for \u03b1 =  1000, we get a faster algorithm, but a minor drop in  performance compared to FIXEDORDER. However,  this baseline still featurizes 6x more derivations and  is 6x slower than AGENDAIL.", "labels": [], "entities": [{"text": "FIXEDORDER", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.7317832708358765}, {"text": "AGENDAIL", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.6107501983642578}]}, {"text": " Table 4: Accuracy, number of featurized derivations, and pars- ing time for both the training set and development set when  varying the value of the tolerance parameter .", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999131977558136}, {"text": "pars- ing time", "start_pos": 58, "end_pos": 72, "type": "METRIC", "confidence": 0.9353922307491302}]}]}