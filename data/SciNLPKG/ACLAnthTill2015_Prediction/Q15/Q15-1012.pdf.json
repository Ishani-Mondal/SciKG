{"title": [{"text": "An Unsupervised Method for Uncovering Morphological Chains", "labels": [], "entities": [{"text": "Uncovering Morphological Chains", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.7560407718022665}]}], "abstractContent": [{"text": "Most state-of-the-art systems today produce morphological analysis based only on ortho-graphic patterns.", "labels": [], "entities": []}, {"text": "In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words.", "labels": [], "entities": []}, {"text": "We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations.", "labels": [], "entities": [{"text": "word formation", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7248211205005646}]}, {"text": "We use log-linear models with morpheme and word-level features to predict possible parents, including their modifications, for each word.", "labels": [], "entities": []}, {"text": "The limited set of candidate parents for each word render contrastive estimation feasible.", "labels": [], "entities": [{"text": "contrastive estimation", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.84261354804039}]}, {"text": "Our model consistently matches or outper-forms five state-of-the-art systems on Arabic, English and Turkish.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphologically related words exhibit connections at multiple levels, ranging from orthographical patterns to semantic proximity.", "labels": [], "entities": []}, {"text": "For instance, the words playing and played share the same stem, but also carry similar meaning.", "labels": [], "entities": []}, {"text": "Ideally, all these complementary sources of information would betaken into account when learning morphological structures.", "labels": [], "entities": []}, {"text": "Most state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.8509197533130646}]}, {"text": "In these approaches, words are commonly modeled as concatenations of morphemes.", "labels": [], "entities": []}, {"text": "This morpheme-centric view is well-suited for uncovering distributional properties of stems and affixes.", "labels": [], "entities": []}, {"text": "But it is not well-equipped to capture semantic relatedness at the word level.", "labels": [], "entities": []}, {"text": "In contrast, earlier approaches that capture semantic similarity in morphological variants operate solely at the word level (Schone and Jurafsky,;).", "labels": [], "entities": []}, {"text": "Given two candidate words, the proximity is assessed using standard word-distributional measures such as mutual information.", "labels": [], "entities": []}, {"text": "However, the fact that these models do not model morphemes directly greatly limits their performance.", "labels": [], "entities": []}, {"text": "In this paper, we propose a model to integrate orthographic and semantic views.", "labels": [], "entities": []}, {"text": "Our goal is to build a chain of derivations fora current word from its base form.", "labels": [], "entities": []}, {"text": "For instance, given a word playfully, the corresponding chain is play \u2192 playful \u2192 playfully.", "labels": [], "entities": []}, {"text": "The wordplay is abase form of this derivation as it cannot be reduced any further.", "labels": [], "entities": []}, {"text": "Individual derivations are obtained by adding a morpheme (ex. -ful ) to a parent word (ex. play).", "labels": [], "entities": []}, {"text": "This addition maybe implemented via a simple concatenation, or it may involve transformations.", "labels": [], "entities": []}, {"text": "At every step of the chain, the model aims to find a parent-child pair (ex. playplayful ) such that the parent also constitutes a valid entry in the lexicon.", "labels": [], "entities": []}, {"text": "This allows the model to directly compare the semantic similarity of the parentchild pair, while also considering the orthographic properties of the morphemic combination.", "labels": [], "entities": []}, {"text": "We model each step of a morphological chain by means of a log-linear model that enables us to incorporate a wide range of features.", "labels": [], "entities": []}, {"text": "At the semantic level, we consider the relatedness between two words using the corresponding vector embeddings.", "labels": [], "entities": []}, {"text": "At the orthographic level, features capture whether the words in the chain actually occur in the corpus, how affixes are reused, as well as how the words are altered during the addition of morphemes.", "labels": [], "entities": []}, {"text": "We use Contrastive Estimation ( to efficiently learn this model in an unsupervised manner.", "labels": [], "entities": []}, {"text": "Specifically, we require that each word has greater support among its bounded set of candidate parents than an artificially constructed neighboring word would.", "labels": [], "entities": []}, {"text": "We evaluate our model on datasets in three languages: Arabic, English and Turkish.", "labels": [], "entities": []}, {"text": "We compare our performance against five state-of-the-art unsupervised systems: Morfessor Baseline (, Morfessor CatMAP (), AGMorph (, the Lee Segmenter () and the system of.", "labels": [], "entities": [{"text": "AGMorph", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.8799516558647156}]}, {"text": "Our model consistently equals or outperforms these systems across the three languages.", "labels": [], "entities": []}, {"text": "For instance, on English, we obtain an 8.5% gain in F-measure over Morfessor.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.991787850856781}]}, {"text": "Our experiments also demonstrate the value of semantic information.", "labels": [], "entities": []}, {"text": "While the contribution varies from 3% on Turkish to 11% on the English dataset, it nevertheless improves performance across all the languages.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.9308052361011505}]}], "datasetContent": [{"text": "Data We run experiments on three different languages: English, Turkish and Arabic.", "labels": [], "entities": []}, {"text": "For each language, we utilize corpora for training, testing and learning word vectors.", "labels": [], "entities": []}, {"text": "The training data consists of an unannotated wordlist with frequency information, while the test data is a set of gold morphological segmentations.", "labels": [], "entities": []}, {"text": "For the word vectors, we train the word2vec tool () on large text corpora and obtain 200-dimensional vectors for all three languages.", "labels": [], "entities": []}, {"text": "provides information about each dataset.", "labels": [], "entities": []}, {"text": "Evaluation measure We test our model on the task of morphological segmentation.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.714467853307724}]}, {"text": "We evaluate performance on individual segmentation points, which is standard for this task (Virpioja et al., 2011).", "labels": [], "entities": []}, {"text": "We compare predicted segmentations against the gold test data for each language and report overall Precision, Recall and F-1 scores calculated across http://research.ics.aalto.fi/events/morphochallenge/ all segmentation points in the data.", "labels": [], "entities": [{"text": "Precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9993104934692383}, {"text": "Recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.98310387134552}, {"text": "F-1", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9786739945411682}]}, {"text": "As is common in unsupervised segmentation (, we included the test words (without their segmentations) with the training words during parameter learning.", "labels": [], "entities": []}, {"text": "Baselines We compare our model with five other systems: Morfessor Baseline (Morf-Base), Morfessor CatMap (Morf-Cat), AGMorph, the Lee Segmenter and the system of.", "labels": [], "entities": [{"text": "AGMorph", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.8762079477310181}]}, {"text": "Morfessor has achieved excellent performance on the MorphoChallenge dataset, and is widely used for performing unsupervised morphological analysis on various languages, even in fairly recent work (.", "labels": [], "entities": [{"text": "MorphoChallenge dataset", "start_pos": 52, "end_pos": 75, "type": "DATASET", "confidence": 0.9419471621513367}]}, {"text": "In our experiments, we employ two variants of the system because their relative performance varies across languages.", "labels": [], "entities": []}, {"text": "We use publicly available implementations of these variants).", "labels": [], "entities": []}, {"text": "We perform several runs with various parameters, and choose the run with the best performance on each language.", "labels": [], "entities": []}, {"text": "We evaluate AGMorph by directly obtaining the posterior grammars from the authors.", "labels": [], "entities": [{"text": "AGMorph", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.780377209186554}]}, {"text": "We show results for the Compounding grammar, which we find has the best average performance over the languages.", "labels": [], "entities": []}, {"text": "The Lee Segmenter (), improved upon by using Maximum Marginal decoding in, has achieved excellent performance on the Arabic (ATB) dataset.", "labels": [], "entities": [{"text": "Arabic (ATB) dataset", "start_pos": 117, "end_pos": 137, "type": "DATASET", "confidence": 0.7155929923057556}]}, {"text": "We perform comparison experiments with the model 2 (M2) of the segmenter, which employs latent POS tags, and does not require sentence context which is not available for other languages in the dataset.", "labels": [], "entities": []}, {"text": "We obtained the code for the system, and run it on our English and Turkish datasets.", "labels": [], "entities": [{"text": "Turkish datasets", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.7863553166389465}]}, {"text": "We do not have access to an implementation of Poon et al's system; hence, we directly report scores from their paper on the ATB dataset and test our model on the same data.", "labels": [], "entities": [{"text": "ATB dataset", "start_pos": 124, "end_pos": 135, "type": "DATASET", "confidence": 0.9888412356376648}]}, {"text": "details the performance of the various models on the segmentation task.", "labels": [], "entities": [{"text": "segmentation task", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.929706484079361}]}, {"text": "We can see that our method outperforms both variants of Morfessor, with an absolute gain of 8.5%, 5.1% and 5% in Fscore on English, Turkish and Arabic, respectively.", "labels": [], "entities": [{"text": "Morfessor", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.792748749256134}, {"text": "Fscore", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9959267377853394}]}, {"text": "On Arabic, we obtain a 2.2% absolute improvement over Poon et al.'s model.", "labels": [], "entities": []}, {"text": "AGMorph doesn't segment better than Morfessor on English and Arabic but does very well on Turkish (60.9% F1 compared to our model's 61.2%).", "labels": [], "entities": [{"text": "AGMorph", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9502299427986145}, {"text": "F1", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9987855553627014}]}, {"text": "This could be due to the fact that the Compounding grammar is well suited to the agglutinative morphology in Turkish and hence provides more gains than for English and Arabic.", "labels": [], "entities": []}, {"text": "The Lee Segmenter (M2) performs the best on Arabic (82% F1), but lags behind on English and Turkish.", "labels": [], "entities": [{"text": "Lee Segmenter (M2)", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.869260060787201}, {"text": "F1", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9945875406265259}]}, {"text": "This result is consistent with the fact that the system was optimized for Arabic.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cosine similarities between word vectors of  various segments of the word player and the vector  of player.", "labels": [], "entities": []}, {"text": " Table 2: Example of various types of features used in the model.  w and  p are the word vectors for the word  and parent, respectively.", "labels": [], "entities": []}, {"text": " Table 4: Data corpora and statistics. MC-10 = Mor- phoChallenge 2010 10 , MC-05:10 = MorphoChal- lenges 2005-10 (aggregated), BOUN = BOUN cor- pus (Sak et al., 2008), Gigaword = Arabic Gigaword  corpus (Parker et al., 2011), ATB = Arabic Tree- bank (Maamouri et al., 2003)", "labels": [], "entities": [{"text": "BOUN", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9981181621551514}, {"text": "BOUN", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9685560464859009}, {"text": "Arabic Tree- bank (Maamouri et al., 2003)", "start_pos": 232, "end_pos": 273, "type": "DATASET", "confidence": 0.8992339481006969}]}, {"text": " Table 6: Examples of correct and incorrect segmentations produced by our model on the three languages.  Correct segmentations are taken directly from gold MorphoChallenge data.", "labels": [], "entities": [{"text": "MorphoChallenge data", "start_pos": 156, "end_pos": 176, "type": "DATASET", "confidence": 0.877719521522522}]}, {"text": " Table 7: Types of errors in analysis of 50 randomly  sampled incorrect segmentations for each language.  The remaining errors are due to incorrect placement  of segmentation points.", "labels": [], "entities": []}]}