{"title": [{"text": "From Visual Attributes to Adjectives through Decompositional Distributional Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "As automated image analysis progresses, there is increasing interest in richer linguistic annotation of pictures, with attributes of objects (e.g., furry, brown.. .) attracting most attention.", "labels": [], "entities": []}, {"text": "By building on the recent \"zero-shot learning\" approach, and paying attention to the linguistic nature of attributes as noun modifiers, and specifically adjectives, we show that it is possible to tag images with attribute-denoting adjectives even when no training data containing the relevant annotation are available.", "labels": [], "entities": []}, {"text": "Our approach relies on two key observations.", "labels": [], "entities": []}, {"text": "First, objects can be seen as bundles of attributes, typically expressed as adjectival modifiers (a dog is something furry, brown, etc.), and thus a function trained to map visual representations of objects to nominal labels can implicitly learn to map attributes to adjectives.", "labels": [], "entities": []}, {"text": "Second, objects and attributes come together in pictures (the same thing is a dog and it is brown).", "labels": [], "entities": []}, {"text": "We can thus achieve better attribute (and object) label retrieval by treating images as \"vi-sual phrases\", and decomposing their linguistic representation into an attribute-denoting adjective and an object-denoting noun.", "labels": [], "entities": [{"text": "object) label retrieval", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7192776501178741}]}, {"text": "Our approach performs comparably to a method exploiting manual attribute annotation, it out-performs various competitive alternatives in both attribute and object annotation, and it automatically constructs attribute-centric representations that significantly improve performance in supervised object recognition.", "labels": [], "entities": [{"text": "supervised object recognition", "start_pos": 283, "end_pos": 312, "type": "TASK", "confidence": 0.6241019268830618}]}, {"text": "* Current affiliation: Thomas J.", "labels": [], "entities": []}], "introductionContent": [{"text": "As the quality of image analysis algorithms improves, there is increasing interest in annotating images with linguistic descriptions ranging from single words describing the depicted objects and their properties) to richer expressions such as full-fledged image captions.", "labels": [], "entities": [{"text": "image analysis", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7340012788772583}]}, {"text": "This trend has generated wide interest in linguistic annotations beyond concrete nouns, with the role of adjectives in image descriptions receiving, in particular, much attention.", "labels": [], "entities": []}, {"text": "Adjectives are of special interest because of their central role in so-called attribute-centric image representations.", "labels": [], "entities": []}, {"text": "This framework views objects as bundles of properties, or attributes, commonly expressed by adjectives (e.g., furry, brown), and uses the latter as features to learn higher-level, semantically richer representations of objects.", "labels": [], "entities": []}, {"text": "Attribute-based methods achieve better generalization of object classifiers with less training data), while at the same time producing semantic representations of visual concepts that more accurately model human se- In this paper, we assume that, just like nouns are the linguistic counterpart of visual objects, visual attributes are expressed by adjectives.", "labels": [], "entities": []}, {"text": "An informal survey of the relevant literature suggests that, when attributes have linguistic labels, they are indeed mostly expressed by adjectives.", "labels": [], "entities": []}, {"text": "There are some attributes, such as parts, that are more naturally expressed by prepositional phrases (PPs: with a tail).", "labels": [], "entities": []}, {"text": "Interestingly, showed that the decomposition function we will adopt here can derive both adjective-noun and noun-PP phrases, suggesting that our approach could be seamlessly extended to visual attributes expressed by noun-modifying PPs.", "labels": [], "entities": []}, {"text": "Moreover, automated attribute annotation can facilitate finergrained image retrieval (e.g., searching fora rocky beach rather than a sandy beach) and provide the basis for more accurate image search (for example in cases of visual sense disambiguation (), where a user disambiguates their query by searching for images of wooden cabinet as furniture and not just cabinet, which can also mean council).", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7596405446529388}, {"text": "visual sense disambiguation", "start_pos": 224, "end_pos": 251, "type": "TASK", "confidence": 0.6642119487126669}]}, {"text": "Classic attribute-centric image analysis requires, however, extensive manual and often domainspecific annotation of attributes (, or, at best, complex unsupervised imageand-text-mining procedures to learn them (.", "labels": [], "entities": [{"text": "attribute-centric image analysis", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.6936450401941935}]}, {"text": "At the same time, resources with highquality per-image attribute annotations are limited; to the best of our knowledge, coverage of all publicly available datasets containing non-class specific attributes does not exceed 100 attributes, 2 orders of magnitude smaller than the equivalent objectannotated datasets.", "labels": [], "entities": []}, {"text": "Moreover, many visual attributes currently available (e.g., 2D-boxy, furniture leg), albeit visually meaningful, do not have straightforward linguistic equivalents, rendering them inappropriate for applications requiring natural linguistic expressions, such as the search scenarios considered above.", "labels": [], "entities": []}, {"text": "A promising way to limit manual attribute annotation effort is to extend recently proposed zero-shot learning methods, until now applied to object recognition, to the task of labeling images with attributedenoting adjectives.", "labels": [], "entities": [{"text": "attribute annotation", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.6889946311712265}, {"text": "object recognition", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.7534465491771698}]}, {"text": "The zero-shot approach relies on the possibility to extract, through distributional methods, semantically effective vector-based word representations from text corpora, on a large scale and without supervision.", "labels": [], "entities": []}, {"text": "In zero-shot learning, training images labeled with object names are also represented as vectors (of features extracted with standard image-analysis techniques), which are paired with the vectors representing the corresponding object names in languagebased distributional semantic space.", "labels": [], "entities": []}, {"text": "Given such The attribute datasets we are aware of are the ones of, and, containing annotations for 64, 7 and 25 attributes, respectively.", "labels": [], "entities": []}, {"text": "(This count excludes the SUN Attributes Database (), whose attributes characterize scenes rather than concrete objects.) paired training data, various algorithms can be used to induce a cross-modal projection of images onto linguistic space.", "labels": [], "entities": []}, {"text": "This projection is then applied to map previously unseen objects to the corresponding linguistic labels.", "labels": [], "entities": []}, {"text": "The method takes advantage of the similarities in the vector space topologies of the two modalities, allowing information propagation from the limited number of objects seen in training to virtually any object with a vector-based linguistic representation.", "labels": [], "entities": [{"text": "information propagation", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.7459606826305389}]}, {"text": "To adapt zero-shot learning to attributes, we rely on their nature as (salient) properties of objects, and on how this is reflected linguistically in modifier relations between adjectives and nouns.", "labels": [], "entities": []}, {"text": "We build on the observation that visual and linguistic attributeadjective vector spaces exhibit similar structures: The correlation \u03c1 between the pairwise similarities in visual and linguistic space of all attributesadjectives from our experiments is 0.14 (significant at p < 0.05).", "labels": [], "entities": []}, {"text": "3 While the correlation is smaller than for object-noun data (0.23), we conjecture it is sufficient for zero-shot learning of attributes.", "labels": [], "entities": [{"text": "correlation", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9791765213012695}]}, {"text": "We will confirm this by testing a cross-modal projection function from attributes, such as colors and shapes, onto adjectives in linguistic semantic space, trained on pre-existing annotated datasets covering less than 100 attributes (Experiment 1).", "labels": [], "entities": []}, {"text": "We proceed to develop an approach achieving equally good attribute-labeling performance without manual attribute annotation.", "labels": [], "entities": []}, {"text": "Inspired by linguistic and cognitive theories that characterize objects as attribute bundles, we hypothesize that when we learn to project images of objects to the corresponding noun labels, we implicitly learn to Figure 2: Images tagged with orange and liqueur are mapped in linguistic space closer to the vector of the phrase orange liqueur than to the orange or liqueur vectors (t-SNE visualization) (the figure also shows the nearest neighbours of phrase, adjective and noun in linguistic space).", "labels": [], "entities": []}, {"text": "The mapping is trained using solely nounannotated images.", "labels": [], "entities": []}, {"text": "associate the visual properties/attributes of the objects to the corresponding adjectives.", "labels": [], "entities": []}, {"text": "As an example, (left) displays the nearest attributes of car, bird and puppy in the visual space and, interestingly, the relative distance between the noun denoting objects and the adjective denoting attributes is also preserved in the linguistic space (right).", "labels": [], "entities": []}, {"text": "We further observe that, as also highlighted by recent work in object recognition, any object in an image is, in a sense, a visual phrase (), i.e., the object and its attributes are mutually dependent.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7420734316110611}]}, {"text": "For example, we cannot visually isolate the object drum from attributes such as wooden and round.", "labels": [], "entities": []}, {"text": "Indeed, within our data, in 80% of the cases the projected image of an object is closer to the semantic representation of a phrase describing it than to either the objector attribute labels.", "labels": [], "entities": []}, {"text": "Motivated by this observation, we turn to recent work in distributional semantics defining a vector decomposition framework () which, given a vector encoding the meaning of a phrase, aims at decoupling its constituents, producing vectors that can then be matched to a sequence of words best capturing the semantics of the phrase.", "labels": [], "entities": []}, {"text": "We adopt this framework to decompose image representations projected onto linguistic space into an adjective-noun phrase.", "labels": [], "entities": []}, {"text": "We show that the method yields results comparable to those obtained when using attribute-labeled training data, while only requiring object-annotated data.", "labels": [], "entities": []}, {"text": "Interestingly, this decompositional approach also doubles the performance of object/noun annotation over the standard zeroshot approach (Experiment 2).", "labels": [], "entities": []}, {"text": "Given the positive results of our proposed method, we conclude with an extrinsic evaluation (Experiment 3); we show that attribute-centric representations of images created with the decompositional approach boost performance in an object classification task, supporting claims about its practical utility.", "labels": [], "entities": [{"text": "object classification task", "start_pos": 231, "end_pos": 257, "type": "TASK", "confidence": 0.7950011293093363}]}, {"text": "In addition to contributions to image annotation, our work suggests new test beds for distributional semantic representations of nouns and associated adjectives, and provides more in-depth evidence of the potential of the decompositional approach.", "labels": [], "entities": [{"text": "image annotation", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7226583659648895}, {"text": "distributional semantic representations of nouns and associated adjectives", "start_pos": 86, "end_pos": 160, "type": "TASK", "confidence": 0.7284363210201263}]}], "datasetContent": [{"text": "Our approach relies on cross-modal mapping from a visual semantic space V, populated with vector-based representations of images, onto a linguistic (distributional semantic) space W of word vectors.", "labels": [], "entities": []}, {"text": "The mapping is performed by first inducing a function f proj : Rd 1 \u2192 Rd 2 from data points (v i , w i ), where vi \u2208 Rd 1 is a vector representation of an image tagged with an objector an attribute (such as dog or metallic), and w i \u2208 Rd 2 is the linguistic vector representation of the corresponding word.", "labels": [], "entities": []}, {"text": "The mapping function can subsequently be applied to any given image vi \u2208 V to obtain its projection w i \u2208 W onto linguistic space: Specifically, we consider two mapping methods.", "labels": [], "entities": []}, {"text": "In the RIDGE regression approach, we learn a linear function F proj \u2208 Rd 2 \u00d7d 1 by solving the TikhonovPhillips regularization problem, which minimizes the following objective: , where W T rand VT rare obtained by stacking the word vectors w i and corresponding image vectors vi , from the training set.", "labels": [], "entities": [{"text": "RIDGE regression", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7455195784568787}]}, {"text": "Second, motivated by the success of Canonical Correlations Analysis (CCA)) in several vision-and-language tasks, such as image and caption retrieval (, we adapt normalized Canonical Correlations Analysis (NCCA) to our setup.", "labels": [], "entities": [{"text": "Canonical Correlations Analysis (CCA))", "start_pos": 36, "end_pos": 74, "type": "TASK", "confidence": 0.7339199086030325}, {"text": "image and caption retrieval", "start_pos": 121, "end_pos": 148, "type": "TASK", "confidence": 0.6494074016809464}]}, {"text": "Given two paired observation matrices X and Y , in our case W T rand VT r , CCA seeks two projection matrices A and B that maximize the correlation between A TX and B T Y . This can be solved efficiently by applying SVD t\u00f4 C stands for the covariance matrix.", "labels": [], "entities": []}, {"text": "Finally, the projection matrices are defined as propose a normalized variant of CCA, in which the projection matrices are further scaled by some power \u03bb of the singular values \u03a3 returned by the SVD solution.", "labels": [], "entities": []}, {"text": "In our experiments, we tune the choice of \u03bb on the training data.", "labels": [], "entities": []}, {"text": "Trivially, if \u03bb = 0, NCCA reduces to CCA.", "labels": [], "entities": []}, {"text": "Note that other mapping functions could also be used.", "labels": [], "entities": []}, {"text": "We leave a more extensive exploration of possible alternatives to further research, since the details of how the vision-to-text conversion is conducted are not crucial for the current study.", "labels": [], "entities": [{"text": "vision-to-text conversion", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.647855818271637}]}, {"text": "As increasingly more effective mapping methods are developed, we can easily plug them into our architecture.", "labels": [], "entities": []}, {"text": "Through the selected cross-modal mapping function, any image can be projected onto linguistic space, where the word (possibly of the appropriate part of speech) corresponding to the nearest vector is returned as a candidate label for the image (following standard practice in distributional semantics, we measure proximity by the cosine measure).", "labels": [], "entities": []}, {"text": "For evaluation purposes, we use the dataset consisting of images annotated with adjective-noun phrases introduced in, which pertains to 384 WordNet/ImageNet synsets with 25 images per synset.", "labels": [], "entities": [{"text": "WordNet/ImageNet synsets", "start_pos": 140, "end_pos": 164, "type": "DATASET", "confidence": 0.8491495251655579}]}, {"text": "The images were manually annotated with 25 attribute-denoting adjectives related to texture, color, pattern and shape, respecting the constraints that a color must cover a significant part of the target object, and all other attributes must pertain to the object as a whole (as opposed to parts).", "labels": [], "entities": []}, {"text": "In order to increase annotation quality, we only consider attributes with full annotator consensus, fora total of 8,449 annotated images, with 2.7 attributes per-image on average.", "labels": [], "entities": []}, {"text": "Furthermore, to make the linguistic annotation more natural and avoid sparsity problems, we renamed excessively specific objects with a noun denoting a more general category, following recent work on entry-level categories (Or-8 http://www.vlfeat.org/applications/ apps.html Although vegetation is a noun, we have kept it in the evaluation set, treating it as an adjective.", "labels": [], "entities": []}, {"text": "In Section 1, we showed that there is a significant correlation between pairwise similarities of adjectives in a language-based distributional semantic space and those of visual feature vectors extracted from images labeled with the corresponding attributes.", "labels": [], "entities": []}, {"text": "In the first experiment, we test whether this correspondence in attribute-adjective similarity structure across modalities suffices to successfully apply zero-shot labeling.", "labels": [], "entities": []}, {"text": "We learn a crossmodal function from an annotated dataset and use it to label images from an evaluation dataset with attributes outside the training set.", "labels": [], "entities": []}, {"text": "We will refer to this approach as DIR A , for Direct Retrieval using Attribute annotation.", "labels": [], "entities": [{"text": "DIR A", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.7576529085636139}]}, {"text": "Note that this is the first time that zero-shot techniques are used in the attribute domain.", "labels": [], "entities": []}, {"text": "In the present evaluation, we distinguish DIR A -RIDGE and DIR A -NCCA, according to the cross-modal function used to project from images to linguistic representations (see Section 2.1 above).", "labels": [], "entities": [{"text": "DIR A -RIDGE", "start_pos": 42, "end_pos": 54, "type": "METRIC", "confidence": 0.5885046645998955}]}, {"text": "To gather sufficient data to train a cross-modal mapping function for attributes/adjectives, we combine the publicly available datasets of and with attributes and associated images extracted from MIR-FLICKR (.", "labels": [], "entities": [{"text": "MIR-FLICKR", "start_pos": 196, "end_pos": 206, "type": "DATASET", "confidence": 0.688129723072052}]}, {"text": "The resulting dataset contains 72 distinct attributes and 2,300 images.", "labels": [], "entities": []}, {"text": "Each image-attribute pair represents a training data point (v, w adj ), where v is the vector representation of the image, and w adj is the linguistic vector of the attribute (corresponding to an adjective).", "labels": [], "entities": []}, {"text": "No information about the depicted object is needed.", "labels": [], "entities": []}, {"text": "To further maximize the amount of training data points, we conduct a leave-one-attribute-out evaluation, in which the cross-modal mapping function is repeatedly learned on all 72 attributes from the training set, as well as all but one attribute from the evaluation set (Section 2.4), and the associated images.", "labels": [], "entities": []}, {"text": "This results in 72 + (25 \u2212 1) = 96 training attributes in total.", "labels": [], "entities": []}, {"text": "On average, 45 images per attribute are used.", "labels": [], "entities": []}, {"text": "The performance is measured for the single attribute that was excluded from training.", "labels": [], "entities": []}, {"text": "A numerical summary of the experiment setup is presented in the first row of.", "labels": [], "entities": []}, {"text": "Having shown that reasonably accurate annotations of unseen attributes can be obtained with zero-shot learning when a small amount of manual annotation is available, we now proceed to test the intuition, preliminarily supported by the data in, that, since objects are bundles of attributes, attributes are implicitly learned together with objects.", "labels": [], "entities": []}, {"text": "We thus try to induce attribute-denoting adjective labels by exploiting only widely-available object-noun data.", "labels": [], "entities": []}, {"text": "At the same time, building on the observation illustrated in that pictures of objects are pictures of visual phrases, we experiment with a vector decomposition model which treats images as composite and derives adjective and noun annotations jointly.", "labels": [], "entities": []}, {"text": "We compare it with standard zeroshot learning using direct label retrieval as well as against a number of challenging alternatives that exploit gold-standard information about the depicted objects.", "labels": [], "entities": []}, {"text": "The second row of gives a numerical summary of the setup for this experiment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Summary of training and evaluation sets.", "labels": [], "entities": []}, {"text": " Table 4: Percentage hit@k attribute retrieval scores.", "labels": [], "entities": []}, {"text": " Table 5: Percentage recall@k attribute retrieval scores.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.7710983157157898}]}, {"text": " Table 6: Percentage hit@k noun retrieval scores.", "labels": [], "entities": []}]}