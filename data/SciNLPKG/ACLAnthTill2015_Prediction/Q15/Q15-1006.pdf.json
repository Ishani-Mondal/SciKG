{"title": [{"text": "Which Step Do I Take First? Troubleshooting with Bayesian Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Online discussion forums and community question-answering websites provide one of the primary avenues for online users to share information.", "labels": [], "entities": []}, {"text": "In this paper, we propose text mining techniques which aid users navigate troubleshooting-oriented data such as questions asked on forums and their suggested solutions.", "labels": [], "entities": [{"text": "text mining", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.8172134459018707}]}, {"text": "We introduce Bayesian generative models of the troubleshooting data and apply them to two interrelated tasks: (a) predicting the complexity of the solutions (e.g., plugging a keyboard in the computer is easier compared to installing a special driver) and (b) presenting them in a ranked order from least to most complex.", "labels": [], "entities": []}, {"text": "Experimental results show that our models are on par with human performance on these tasks, while outperforming baselines based on solution length or readability.", "labels": [], "entities": []}], "introductionContent": [{"text": "Online forums and discussion boards have created novel ways for discovering, sharing, and distributing information.", "labels": [], "entities": []}, {"text": "Users typically post their questions or problems and obtain possible solutions from other users.", "labels": [], "entities": []}, {"text": "Through this simple mechanism of community-based question answering, it is possible to find answers to personal, open-ended, or highly specialized questions.", "labels": [], "entities": [{"text": "question answering", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7088267654180527}]}, {"text": "However, navigating the information available in web-archived data can be challenging given the lack of appropriate search and browsing facilities.", "labels": [], "entities": []}, {"text": "shows examples typical of the problems and proposed solutions found in troubleshootingoriented online forums.", "labels": [], "entities": []}, {"text": "The first problem concerns a shaky monitor and has three solutions with increasing degrees of complexity.", "labels": [], "entities": []}, {"text": "Solution (1) is probably easiest to implement in terms of user time, effort, and expertise; solution (3) is most complex (i.e., the user should understand what signal timing is and The screen is shaking.", "labels": [], "entities": []}, {"text": "1. Move all objects that emit a magnetic field, such as a motor or transformer, away from the monitor.", "labels": [], "entities": []}, {"text": "2. Check if the specified voltage is applied.", "labels": [], "entities": []}, {"text": "3. Check if the signal timing of the computer system is within the specification of the monitor.", "labels": [], "entities": []}, {"text": "\"Illegal Operation has Occurred\" error message is displayed.", "labels": [], "entities": []}, {"text": "1. Software being used is not Microsoft-certified for your version of Windows.", "labels": [], "entities": []}, {"text": "Verify that the software is certified by Microsoft for your version of Windows (see program packaging for this information).", "labels": [], "entities": []}, {"text": "2. Configuration files are corrupt.", "labels": [], "entities": []}, {"text": "If possible, save all data, close all programs, and restart the computer.", "labels": [], "entities": []}, {"text": "then try to establish whether it is within the specification of the monitor), whereas solution (2) is somewhere in between.", "labels": [], "entities": []}, {"text": "In most cases, the solutions are not organized in any particular fashion, neither in terms of content nor complexity.", "labels": [], "entities": []}, {"text": "In this paper, we present models to automatically predict the complexity of troubleshooting solutions, which we argue could improve user experience, and potentially help solve the problem faster (e.g., by prioritizing easier solutions).", "labels": [], "entities": []}, {"text": "Automatically structuring solutions according to complexity could also facilitate search through large archives of solutions or serve as a summarization tool.", "labels": [], "entities": [{"text": "summarization", "start_pos": 139, "end_pos": 152, "type": "TASK", "confidence": 0.9813711047172546}]}, {"text": "From a linguistic perspective, learning how complexity is verbalized can be viewed as an instance of grounded language acquisition.", "labels": [], "entities": []}, {"text": "Solutions direct users to carryout certain actions (e.g., on their computers or devices) and complexity is an attribute of these actions.", "labels": [], "entities": []}, {"text": "Information access systems incorporating a notion of complexity would allow to take user intentions into account and how these translate into natural language.", "labels": [], "entities": []}, {"text": "Current summarization and information retrieval methods are agnostic of such types of text semantics.", "labels": [], "entities": [{"text": "summarization", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.9835978150367737}, {"text": "information retrieval", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7606413960456848}]}, {"text": "Moreover, the models presented here could be used for analyzing collaborative prob-lem solving and its social networks.", "labels": [], "entities": [{"text": "prob-lem solving", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.6811239421367645}]}, {"text": "Characterizing the content of discussion forums by their complexity can provide additional cues for identifying user authority and if there is a need for expert intervention.", "labels": [], "entities": []}, {"text": "We begin by validating that the task is indeed meaningful and that humans perceive varying degrees of complexity when reading troubleshooting solutions.", "labels": [], "entities": []}, {"text": "We also show experimentally that users agree in their intuitions about the relative complexity of different solutions to the same problem.", "labels": [], "entities": []}, {"text": "We define \"complexity\" as an aggregate notion of the time, expertise, and money required to implement a solution.", "labels": [], "entities": []}, {"text": "We next model the complexity prediction task, following a Bayesian approach.", "labels": [], "entities": [{"text": "complexity prediction task", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.8179249167442322}]}, {"text": "Specifically, we learn to assign complexity levels to solutions based on their linguistic makeup.", "labels": [], "entities": []}, {"text": "We leverage weak supervision in the form of lists of solutions (to different problems) approximately ordered from low to high complexity (see).", "labels": [], "entities": []}, {"text": "We assume that the data is generated from a fixed number of discrete complexity levels.", "labels": [], "entities": []}, {"text": "Each level has a probability distribution over the vocabulary and there is a canonical ordering between levels indicating their relative complexity.", "labels": [], "entities": []}, {"text": "During inference, we recover the vocabularies of the complexity levels and the ordering of levels that explains the solutions and their attested sequences in the training data.", "labels": [], "entities": []}, {"text": "We explore two Bayesian models differing in how they learn an ordering among complexity levels.", "labels": [], "entities": []}, {"text": "The first model is local, it assigns an expected position (in any list of solutions) to each complexity level and orders the levels based on this expected position value.", "labels": [], "entities": []}, {"text": "The second model is global, it defines probabilities over permutations of complexity levels and directly uncovers a consensus ordering from the training data.", "labels": [], "entities": []}, {"text": "We evaluate our models on a solution ordering task, where the goal is to rank solutions from least to most complex.", "labels": [], "entities": [{"text": "solution ordering task", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.8466147184371948}]}, {"text": "We show that a supervised ranking approach using features based on the predictions of our generative models is on par with human performance on this task while outperforming competitive baselines based on length and readability of the solution text.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the previous section, we showed how our models can assign an expected complexity value to a solution text or an entire problem.", "labels": [], "entities": []}, {"text": "Now, we present evaluations based on model ability to order solutions according to relative complexity.", "labels": [], "entities": []}, {"text": "We trained the ranking model on 240 problem-solution sets; 30 sets were reserved for development and 30 for testing (in each fold).", "labels": [], "entities": []}, {"text": "The most frequent 20 words in each training set were filtered as stopwords.", "labels": [], "entities": []}, {"text": "The development data was used to tune the parameters and hyperparameters of the models and the number of complexity levels.", "labels": [], "entities": []}, {"text": "We experimented with ranges and found that the best number of levels was 10 for the position model and 20 for the permutationbased model, respectively.", "labels": [], "entities": []}, {"text": "For the expected position model, positions were normalized before training.", "labels": [], "entities": []}, {"text": "Let solution x ir denote the r th solution in the solution set for problem P i , where 1 \u2264 r \u2264 NP i . We normalize r to a value between 0 and 1 using a min-max method: r = r\u22121 NP i \u22121 . Then the [0-1] range is divided into k bins.", "labels": [], "entities": []}, {"text": "The identity of the bin containing r is taken as the normalized position, r.", "labels": [], "entities": []}, {"text": "We tuned k experimentally during development and found that k = 3 performed best.", "labels": [], "entities": []}, {"text": "For our ordering experiments we used Joachims' (2006) SVMRank package for training and testing.", "labels": [], "entities": [{"text": "Joachims' (2006) SVMRank package", "start_pos": 37, "end_pos": 69, "type": "DATASET", "confidence": 0.5181446224451065}]}, {"text": "During training, the classifier learns to minimize the number of swapped pairs of solutions over the training data.", "labels": [], "entities": []}, {"text": "We used a linear kernel and the regularization parameter was tuned using grid search on the development data of each fold.", "labels": [], "entities": []}, {"text": "We evaluate how well the model's output agrees with gold-standard ordering using Kendall's \u03c4 . summarizes our results (average Kendall's \u03c4 across folds).", "labels": [], "entities": []}, {"text": "We present the results of the discriminative ranker when using a single feature class based on likelihood and expected complexity (Position, Permutation), length, and syntactico-semantic features (SynSem), and their combinations (denoted via +).", "labels": [], "entities": []}, {"text": "We also report the performance of a baseline which computes a random permutation for each solution set (Random; results are averaged over five runs).", "labels": [], "entities": [{"text": "Random", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9604986310005188}]}, {"text": "We show results for all solution sets (All) and broken down into different set sizes (e.g., 2-3, 4-5).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Correlation matrix for annotators (A-D) and  original FAQ order using Kendall's \u03c4 (values are aver- ages over 100 problem-solution sets).", "labels": [], "entities": []}, {"text": " Table 6: Kendall's \u03c4 values on the solution reorder- ing task using 10-fold cross-validation and SVM ranking  models with different features. The results are broken  down by solution set size (the number of sets per size is  shown within parentheses). Boldface indicates the best  performing model for each set size.", "labels": [], "entities": [{"text": "solution reorder- ing task", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.8994052886962891}]}, {"text": " Table 7: Model accuracy at predicting the easiest solution  correctly (Rank 1), the most difficult one (Rank n), or  both. Bold face indicates the best performing model for  each rank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9956455230712891}]}]}