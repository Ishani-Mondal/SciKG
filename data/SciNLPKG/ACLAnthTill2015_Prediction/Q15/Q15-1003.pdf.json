{"title": [{"text": "Efficient Inference and Structured Learning for Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.6647070944309235}]}], "abstractContent": [{"text": "We present a dynamic programming algorithm for efficient constrained inference in semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7159889936447144}]}, {"text": "The algorithm tractably captures a majority of the structural constraints examined by prior work in this area, which has resorted to either approximate methods or off-the-shelf integer linear programming solvers.", "labels": [], "entities": [{"text": "integer linear programming solvers", "start_pos": 177, "end_pos": 211, "type": "TASK", "confidence": 0.6329754516482353}]}, {"text": "In addition , it allows training a globally-normalized log-linear model with respect to constrained conditional likelihood.", "labels": [], "entities": []}, {"text": "We show that the dynamic program is several times faster than an off-the-shelf integer linear programming solver, while reaching the same solution.", "labels": [], "entities": [{"text": "integer linear programming solver", "start_pos": 79, "end_pos": 112, "type": "TASK", "confidence": 0.6750486120581627}]}, {"text": "Furthermore, we show that our structured model results in significant improvements over its local counterpart , achieving state-of-the-art results on both PropBank-and FrameNet-annotated corpora.", "labels": [], "entities": [{"text": "PropBank-and FrameNet-annotated corpora", "start_pos": 155, "end_pos": 194, "type": "DATASET", "confidence": 0.7992218931516012}]}], "introductionContent": [{"text": "Semantic role labeling (henceforth, SRL) is the task of identifying the semantic arguments of predicates in natural language text.", "labels": [], "entities": [{"text": "Semantic role labeling (henceforth, SRL)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7679498940706253}, {"text": "identifying the semantic arguments of predicates in natural language text", "start_pos": 56, "end_pos": 129, "type": "TASK", "confidence": 0.5038160353899002}]}, {"text": "Pioneered by, this task has been widely investigated by the NLP community.", "labels": [], "entities": []}, {"text": "There have been two shared tasks at CoNLL 2004 and 2005 focusing on this problem, using PropBank conventions to identify the phrasal arguments of verbal predicates).", "labels": [], "entities": []}, {"text": "Since then, there has been work on SRL for nominal predicates ( and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9874020218849182}]}, {"text": "Here, we present an inference method for SRL, addressing the problem of phrasal argument structure prediction (as opposed to semantic dependencies).", "labels": [], "entities": [{"text": "SRL", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9922193288803101}, {"text": "phrasal argument structure prediction", "start_pos": 72, "end_pos": 109, "type": "TASK", "confidence": 0.6381538957357407}]}, {"text": "In contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and, our framework first performs frame identification, the subtask of disambiguating the predicate frame; this makes our analysis more interpretable.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.6894009113311768}, {"text": "frame identification", "start_pos": 167, "end_pos": 187, "type": "TASK", "confidence": 0.7092812210321426}]}, {"text": "The focus of this paper, however, is the subtask of semantic role labeling, wherein we take a set of (potentially overlapping) candidate sentential phrases and identify and label them with the semantic roles associated with the predicted frame.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.6428795059521993}]}, {"text": "This treatment is commonly used in frame semantic parsing () and our two-stage framework is able to model both PropBank and FrameNet conventions.", "labels": [], "entities": [{"text": "frame semantic parsing", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.6961963673432668}, {"text": "PropBank", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.9210750460624695}]}, {"text": "Previous work focusing on semantic role labeling imposed several structural constraints warranted by the annotation conventions of the task and other linguistic considerations, such as avoiding overlapping arguments and repeated core roles in the final prediction.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.6622125605742136}]}, {"text": "Such global inference often leads to improved results and more meaningful predictions compared to local unconstrained methods (.", "labels": [], "entities": []}, {"text": "A popular framework for imposing these constraints has been integer linear programming (ILP), wherein the inference problem is specified declaratively).", "labels": [], "entities": []}, {"text": "However, ILP-based inference methods often rely on generic off-the-shelf solvers that fail to exploit problem-specific structure).", "labels": [], "entities": []}, {"text": "Instead, we present a dynamic program (DP) that exactly enforces most of the constraints examined by; remaining constraints are enforced by reverting to k-best inference if needed.", "labels": [], "entities": []}, {"text": "We show that this technique solves the inference problem more than four times faster than a state-of-the-art off-the-shelf ILP solver, while I want to hold your hand .", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 123, "end_pos": 133, "type": "TASK", "confidence": 0.8296405971050262}]}], "datasetContent": [{"text": "We measure experimental results on three datasets.", "labels": [], "entities": []}, {"text": "First, we use the CoNLL 2005 shared task data annotated according to PropBank conventions with the standard training, development and test splits).", "labels": [], "entities": [{"text": "CoNLL 2005 shared task data annotated", "start_pos": 18, "end_pos": 55, "type": "DATASET", "confidence": 0.949202428261439}, {"text": "PropBank", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.924184262752533}]}, {"text": "These were originally constructed from sections 02-21, section 24 and section 23 of the Wall Street Journal (WSJ) portion of the Penn Treebank ().", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) portion of the Penn Treebank", "start_pos": 88, "end_pos": 142, "type": "DATASET", "confidence": 0.9475649161772295}]}, {"text": "The PropBank I resource was used to construct the verb frame lexicon for the CoNLL 2005 experiments.", "labels": [], "entities": [{"text": "CoNLL 2005 experiments", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.9357765913009644}]}, {"text": "Second, we perform experiments on a substantially larger data set annotated according to PropBank conventions, using the recent OntoNotes 5.0 corpus, with the CoNLL 2012 training, development and test splits from.", "labels": [], "entities": [{"text": "OntoNotes 5.0 corpus", "start_pos": 128, "end_pos": 148, "type": "DATASET", "confidence": 0.8506590326627096}, {"text": "CoNLL 2012 training", "start_pos": 159, "end_pos": 178, "type": "DATASET", "confidence": 0.8791260917981466}]}, {"text": "The frame lexicon for these experiments is derived from the OntoNotes frame files.", "labels": [], "entities": [{"text": "OntoNotes frame files", "start_pos": 60, "end_pos": 81, "type": "DATASET", "confidence": 0.9602816104888916}]}, {"text": "This corpus consists of nominal predicate-argument structure annotations in addition to verbs.", "labels": [], "entities": []}, {"text": "Specifically, we use version 12 downloaded from http://cemantix.", "labels": [], "entities": []}, {"text": "org/data/ontonotes.html, for which some errors from the initial release used by have been corrected.", "labels": [], "entities": []}, {"text": "Finally, we present results on FrameNet-annotated data, where our setup mirrors that of, who used the full-text annotations of the FrameNet 1.5 release.", "labels": [], "entities": [{"text": "FrameNet-annotated data", "start_pos": 31, "end_pos": 54, "type": "DATASET", "confidence": 0.9389370381832123}, {"text": "FrameNet 1.5 release", "start_pos": 131, "end_pos": 151, "type": "DATASET", "confidence": 0.8997438748677572}]}, {"text": "We use the same training, development and test splits as Hermann et al., which consists of 39, 16 and 23 documents, respectively.", "labels": [], "entities": []}, {"text": "For evaluation on PropBank, we use the script from the CoNLL 2005 shared task that measures role labeling precision, recall and F1-score, as well as the full argument structure accuracy.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.9338483214378357}, {"text": "CoNLL 2005 shared task", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.9336272478103638}, {"text": "role labeling", "start_pos": 92, "end_pos": 105, "type": "TASK", "confidence": 0.6719235479831696}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.8080189824104309}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9997043013572693}, {"text": "F1-score", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9989864230155945}, {"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9384772777557373}]}, {"text": "In the FrameNet setting, we use a reimplementation of the SemEval 2007 shared task evaluation script that measures joint frame-argument precision, recall and F1-score (.", "labels": [], "entities": [{"text": "SemEval 2007 shared task evaluation", "start_pos": 58, "end_pos": 93, "type": "TASK", "confidence": 0.5574279963970185}, {"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.8541772961616516}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9994230270385742}, {"text": "F1-score", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9990850687026978}]}, {"text": "For consistency, we use a stricter measure of full structure accuracy than with PropBank that gives credit only when both the predicted frame and all of its arguments are correct.", "labels": [], "entities": [{"text": "consistency", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9350006580352783}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9530410170555115}, {"text": "PropBank", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.9496780633926392}]}, {"text": "The statistical significance of the observed differences between our different models is assessed with a paired bootstrap test, using 1000 bootstrap samples.", "labels": [], "entities": []}, {"text": "For brevity, we only provide the p-values for the difference between our best and second best models on the test set, as well as between our second and third best models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Semantic role labeling results on the CoNLL 2005 data set. The method labels are training/inference. For  example, Local/DP means training with the local model, but inference with the dynamic program. Bold font indicates  the best system using a single model and a single parse, while the best scores among all systems are underlined.  Statistical significance was assessed for F1 and Comp. on the WSJ and Brown test sets with p < 0.01  *  and p < 0.05  *  *  .", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7630680004755656}, {"text": "CoNLL 2005 data set", "start_pos": 48, "end_pos": 67, "type": "DATASET", "confidence": 0.979360818862915}, {"text": "Statistical significance", "start_pos": 346, "end_pos": 370, "type": "METRIC", "confidence": 0.8172442018985748}, {"text": "F1", "start_pos": 388, "end_pos": 390, "type": "METRIC", "confidence": 0.9974831938743591}, {"text": "Comp", "start_pos": 395, "end_pos": 399, "type": "METRIC", "confidence": 0.9038282632827759}, {"text": "WSJ and Brown test sets", "start_pos": 408, "end_pos": 431, "type": "DATASET", "confidence": 0.9271493196487427}]}, {"text": " Table 4: Semantic role labeling results on the OntoNotes  5.0 development and test sets from CoNLL 2012. \"Prad- han\" is the Overall results from Table 5 of Pradhan et al.  (2013). \"Pradhan (revised)\" are corrected results from per- sonal communication with Pradhan et al. (see footnote 14  for details). Statistical significance was assessed for F1  and Comp. on the test set with p < 0.01  *  .", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7401242852210999}, {"text": "OntoNotes  5.0 development and test sets from CoNLL 2012", "start_pos": 48, "end_pos": 104, "type": "DATASET", "confidence": 0.9075637923346626}, {"text": "Statistical significance", "start_pos": 305, "end_pos": 329, "type": "METRIC", "confidence": 0.9283964335918427}, {"text": "F1", "start_pos": 347, "end_pos": 349, "type": "METRIC", "confidence": 0.9976680874824524}, {"text": "Comp", "start_pos": 355, "end_pos": 359, "type": "METRIC", "confidence": 0.9648876786231995}]}, {"text": " Table 6. The heuristics of", "labels": [], "entities": []}]}