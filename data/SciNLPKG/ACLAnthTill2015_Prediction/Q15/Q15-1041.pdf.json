{"title": [{"text": "Semantic Parsing of Ambiguous Input through Paraphrasing and Verification", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose anew method for semantic parsing of ambiguous and ungrammatical input, such as search queries.", "labels": [], "entities": [{"text": "semantic parsing of ambiguous and ungrammatical input", "start_pos": 27, "end_pos": 80, "type": "TASK", "confidence": 0.8428977940763746}]}, {"text": "We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7828821241855621}]}, {"text": "We generalize this SCFG framework to allow not one, but multiple outputs.", "labels": [], "entities": []}, {"text": "Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less ambiguous than the original input.", "labels": [], "entities": []}, {"text": "This paraphrase can be used to disambiguate the meaning representation via verification using a language model that calculates the probability of each paraphrase.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing (SP) is the problem of parsing a given natural language (NL) sentence into a meaning representation (MR) conducive to further processing by applications.", "labels": [], "entities": [{"text": "Semantic parsing (SP)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9009694576263427}, {"text": "parsing a given natural language (NL) sentence into a meaning representation (MR)", "start_pos": 40, "end_pos": 121, "type": "TASK", "confidence": 0.8100272417068481}]}, {"text": "One of the major challenges in SP stems from the fact that NL is rife with ambiguities.", "labels": [], "entities": [{"text": "SP", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9933701753616333}, {"text": "NL", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9227510690689087}]}, {"text": "For example, even the simple sentence \"Where can we eat a steak in Kobe?\" contains syntactic ambiguities (\"eat in Kobe\" or \"steak in Kobe\"?), quantifier scope ambiguities (do we all eat one steak, or each eat one steak?), and word sense ambiguities (is Kobe a city in Japan; or an NBA basketball player?).", "labels": [], "entities": []}, {"text": "Previous works using statistical models along with formalisms such as combinatorial categorial grammars, synchronous context free grammars, and dependency based compositional semantics have shown notable success in resolving these ambiguities).", "labels": [], "entities": []}, {"text": "Much previous work on SP has focused on the case of answering natural language queries to a database of facts, where the queries generally take the form of full sentences such as \"What is the height of Kobe Bryant?\"", "labels": [], "entities": [{"text": "SP", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9810412526130676}, {"text": "answering natural language queries to a database of facts", "start_pos": 52, "end_pos": 109, "type": "TASK", "confidence": 0.8218441406885783}]}, {"text": "While answering these questions provides an excellent first step to natural language information access, in many cases the input is not a full sentence, but something more underspecified and ungrammatical.", "labels": [], "entities": [{"text": "natural language information access", "start_pos": 68, "end_pos": 103, "type": "TASK", "confidence": 0.6088915318250656}]}, {"text": "For example, this is the case for keyword-based search queries () or short dialogue utterances.", "labels": [], "entities": []}, {"text": "Specifically taking the example of search queries, users tend to omit some of the function words and grammatical constructs in the language to make a more concise query.", "labels": [], "entities": []}, {"text": "The first column of illustrates several search queries of the pattern \"Kobe X\" where X is another word.", "labels": [], "entities": []}, {"text": "From these queries and their MRs in column two, we can see that there are several kinds of ambiguity, including not only the distinction between Kobe as city or a basketball player as in the previous example, but also more pernicious problems unique to the more ambiguous input.", "labels": [], "entities": [{"text": "MRs", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9927384257316589}]}, {"text": "Focusing on the queries \"Kobe hotels\" and \"Kobe flight\" we can see that it is also necessary to estimate the latent relationship between words, such as \"location\" or \"destination.\"", "labels": [], "entities": []}, {"text": "However it should be noted that if we take the keyword query and re-express it as a more explicit paraphrase, we can reduce this ambiguity to the point where there is only one reasonable interpretation.", "labels": [], "entities": []}, {"text": "For example, in the second line, if we add the preposition \"to\" the user is likely asking for flights that arriving in Kobe, and if we add \"from\" the user is asking for departures.", "labels": [], "entities": []}, {"text": "In this paper, we focus on SP of ambiguous input and propose anew method for dealing with the problem of ambiguity.", "labels": [], "entities": [{"text": "SP", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9822902083396912}]}, {"text": "Here we propose a framework where an ambiguous input (Column 1 in) is simultaneously transformed into both its MR (Column 2) and a more explicit, less ambiguous paraphrase (Column 3).", "labels": [], "entities": [{"text": "MR", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.911960244178772}]}, {"text": "The advantage of this method is that it is then possible to verify that the paraphrase indeed expresses the intended meaning of the underspecified input.", "labels": [], "entities": []}, {"text": "This verification can be done either manually by the system user or automatically using a probabilistic model trained to judge the naturalness of the paraphrases.", "labels": [], "entities": []}, {"text": "As a concrete approach, building upon the formalism of synchronous context free grammars (SCFG).", "labels": [], "entities": []}, {"text": "Unlike traditional SCFGs, which usually only generate one target string (in semantic parsing, an MR), we introduce anew variety of SCFGs that generate multiple strings on the target side.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.7451636791229248}]}, {"text": "This allows us to not only generate the MR, but also jointly generate the more explicit paraphrase.", "labels": [], "entities": [{"text": "MR", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.6117558479309082}]}, {"text": "We then use a language model over the paraphrases generated by each derivation to help determine which derivations, and consequently which MRs, are more likely.", "labels": [], "entities": []}, {"text": "We perform an evaluation using the standard Geoquery benchmark of 880 query-logic pairs.", "labels": [], "entities": [{"text": "Geoquery benchmark", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.8698170185089111}]}, {"text": "First we note that baseline SCFG parser achieves reasonable accuracy on regular questions but when the same method is used with underspecified input, the system accuracy decreases significantly.", "labels": [], "entities": [{"text": "SCFG parser", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.8230380415916443}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.996436595916748}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9920979738235474}]}, {"text": "On the other hand, when incorporating the proposed tri-synchronous grammar to generate paraphrases and verify them with a language model, we find that it is possible to recover the loss of accuracy, resulting in a model that is able to parse the ambiguous input with significantly better accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9972773194313049}, {"text": "accuracy", "start_pos": 288, "end_pos": 296, "type": "METRIC", "confidence": 0.9900508522987366}]}], "datasetContent": [{"text": "We evaluate our system using the Geoquery corpus (, which contains 880 sentences representing natural language questions about U.S. Geography, and their corresponding MRs.", "labels": [], "entities": [{"text": "Geoquery corpus", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.9478451609611511}, {"text": "MRs", "start_pos": 167, "end_pos": 170, "type": "METRIC", "confidence": 0.9572533965110779}]}, {"text": "While all evaluation up to this point has used language models to disambiguate paraphrases, we can assume that human users will be even better at judging whether or not a paraphrase makes sense.", "labels": [], "entities": []}, {"text": "Thus, we perform an additional evaluation in which human annotators evaluate the paraphrases generated from the systems.", "labels": [], "entities": []}, {"text": "First, we took the 1-best parse and 7 random parses from the Tri+LM and Tri-LM: The result of experiment with/without neural network language model (NNLM) for the proposed 3-SCFG framework.", "labels": [], "entities": []}, {"text": "Question-LM +NNLM achieved the best accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9995668530464172}]}, {"text": "Bold indicates a significant gain over the baseline Direct Keyword (second row of) and dagger indicates a significant gain over the 3-SCFG baseline without language model (-NNLM column, first row).", "labels": [], "entities": [{"text": "dagger", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9874266386032104}]}, {"text": "The Full and -Empty column use NNLM as language model.", "labels": [], "entities": [{"text": "NNLM", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.9246870875358582}]}, {"text": "The first row of the -NNLM column is the experiment without any language model.", "labels": [], "entities": []}, {"text": "systems where both systems produced a non-empty n-best.", "labels": [], "entities": []}, {"text": "Then we show both the keyword queries and all the paraphrases to human evaluators to annotate: i) a fluency score of 0, 1, or 2 where 0 is completely unnatural English, 1 indicates minor grammatical errors, and 2 indicates flawless English, ii) a letter starting from \"A\", \"B\", etc.", "labels": [], "entities": []}, {"text": "for the paraphrase that matches their preferred interpretation of the search query.", "labels": [], "entities": []}, {"text": "If the input has multiple interpretations, then a different letter is assigned for each possible interpretation in the order that the annotator believes that the interpretation is the correct one, and only paraphrase paraphrase is chosen for each interpretation.", "labels": [], "entities": []}, {"text": "If the human annotator does not find the paraphrase that matched his/her pboth features set.igned and annotation starts from \"B.\"", "labels": [], "entities": [{"text": "B", "start_pos": 126, "end_pos": 127, "type": "METRIC", "confidence": 0.9767476916313171}]}, {"text": "3 annotators were asked to annotate 300 keyword queries and their paraphrases.", "labels": [], "entities": []}, {"text": "There area total of 866 keyword queries (out of 880) that produced a non-empty n-best list in both systems, so we chose random duplications of 34 inputs to make the sum 900.: System precision with additional human help.", "labels": [], "entities": [{"text": "precision", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9927936792373657}]}, {"text": "shows the improvement of the system with human help.", "labels": [], "entities": []}, {"text": "We take all the answers from the annotators that were annotated with \"A\" and replaced the answer of Tri+LM system.", "labels": [], "entities": []}, {"text": "Overall, there were 35 questions that changed between the 1-best and human choices, with 23 improving and 12 degrading accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.995342493057251}]}, {"text": "This experiment suggests that it is possible to show the generated paraphrases to human users to improve the accuracy of the semantic parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9983626008033752}]}, {"text": "Now we look at the relationship between the fluency of the paraphrase and the accuracy of the semantic parsers in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9992358684539795}]}, {"text": "The statistics are gathered 580 from the one best output for both systems.", "labels": [], "entities": []}, {"text": "Tri+LM had a significantly larger percentage of fluent paraphrases with score \"2\" (54% v.s.", "labels": [], "entities": [{"text": "Tri+LM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.815273106098175}]}, {"text": "41%) compared to the system without the language model.", "labels": [], "entities": []}, {"text": "Of the paraphrases that were assigned \"2\" score, 91% corresponded to correct MRs, indicating that the subjective fluency of the paraphrase is a good indicator of parsing accuracy.", "labels": [], "entities": [{"text": "2\" score", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.6082375446955363}, {"text": "MRs", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.6984690427780151}, {"text": "parsing", "start_pos": 162, "end_pos": 169, "type": "TASK", "confidence": 0.967279851436615}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.8017776012420654}]}], "tableCaptions": [{"text": " Table 3: Parsing accuracy, where Keyword Direct is the  baseline for semantic parsing on keyword queries, and the  Tri with the language model (LM) for verification is our  proposed method. Bold indicates a significant gain over  both Direct and Tri-LM for keyword input according to  bootstrap resampling (Koehn, 2004) (p < 0.05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9774615168571472}, {"text": "semantic parsing", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7604974210262299}]}, {"text": " Table 5: The result of experiment with/without neural network language model (NNLM) for the proposed 3-SCFG  framework. Question-LM +NNLM achieved the best accuracy. Bold indicates a significant gain over the baseline  Direct Keyword (second row of", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9985173344612122}]}, {"text": " Table 6: System precision with additional human help.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9933424592018127}]}, {"text": " Table 7: Fluency, Ratio, and Precision statistics for the  one-best of both systems.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9983291029930115}, {"text": "Precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.999201238155365}]}, {"text": " Table 8: A result for the letter accuracy from the human  evaluation. Note that counts do not sum up to total be- cause it is possible that both systems generate same para- phrases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9719939231872559}]}]}