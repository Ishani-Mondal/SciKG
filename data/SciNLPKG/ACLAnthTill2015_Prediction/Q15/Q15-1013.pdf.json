{"title": [{"text": "Modelling and Optimizing on Syntactic N-Grams for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.8227306405703226}]}], "abstractContent": [{"text": "The role of language models in SMT is to promote fluent translation output, but traditional n-gram language models are unable to capture fluency phenomena between distant words, such as some morphological agreement phenomena, subcategorisation, and syntactic collocations with string-level gaps.", "labels": [], "entities": [{"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9954839944839478}]}, {"text": "Syntactic language models have the potential to fill this modelling gap.", "labels": [], "entities": []}, {"text": "We propose a language model for dependency structures that is relational rather than configurational and thus particularly suited for languages with a (rel-atively) free word order.", "labels": [], "entities": []}, {"text": "It is trainable with Neural Networks, and not only improves over standard n-gram language models, but also outperforms related syntactic language models.", "labels": [], "entities": []}, {"text": "We empirically demonstrate its effectiveness in terms of perplexity and as a feature function in string-to-tree SMT from English to German and Russian.", "labels": [], "entities": [{"text": "SMT", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.8868628740310669}]}, {"text": "We also show that using a syntactic evaluation metric to tune the log-linear parameters of an SMT system further increases translation quality when coupled with a syntactic language model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9853416681289673}]}], "introductionContent": [{"text": "Many languages exhibit fluency phenomena that are discontinuous in the surface string, and are thus not modelled well by traditional n-gram language models.", "labels": [], "entities": []}, {"text": "Examples include morphological agreement, e.g. subject-verb agreement in languages that do not (exclusively) follow SVO word order, subcategorisation, and collocations involving distant, but syntactically linked words.", "labels": [], "entities": []}, {"text": "Syntactic language models try to overcome the limitation to a local n-gram context by using syntactically related words (and non-terminals) as context information.", "labels": [], "entities": []}, {"text": "Despite their theoretical attractiveness, it has proven difficult to improve SMT with parsers as language models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9967284202575684}]}, {"text": "This paper describes an effective method to model, train, decode with, and weight a syntactic language model for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9922974705696106}]}, {"text": "While all these aspects are important for successfully applying a syntactic language model, our primary contributions area novel dependency language model which improves over prior work by making relational modelling assumptions, which we argue are better suited for languages with a (relatively) free word order, and the use of a syntactic evaluation metric for optimizing the loglinear parameters of the SMT model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 406, "end_pos": 409, "type": "TASK", "confidence": 0.9839301705360413}]}, {"text": "While language models that operate on words linked through a dependency chain -called syntactic n-grams () -can improve translation, some of the improvement is invisible to an n-gram metric such as BLEU.", "labels": [], "entities": [{"text": "translation", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.9535346031188965}, {"text": "BLEU", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.9621458649635315}]}, {"text": "As a result, tuning to BLEU does not show the full value of a syntactic language model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.989429771900177}]}, {"text": "What does show its value is an optimization metric that operates on the same syntactic n-grams that are modelled by the dependency LM.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our relational dependency language model; section 3 describes our neural network training procedure, and the integration of the model into an SMT decoder.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 162, "end_pos": 173, "type": "TASK", "confidence": 0.8996221721172333}]}, {"text": "We describe the syntactic evaluation metric we use for tuning in Section 4.", "labels": [], "entities": []}, {"text": "The language models are evaluated on the basis of perplexity and SMT 169 performance in section 5.", "labels": [], "entities": [{"text": "SMT 169", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.7304983735084534}]}, {"text": "We discuss related work in section 6, and finish with concluding remarks in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform three evaluations of our dependency language models.", "labels": [], "entities": []}, {"text": "Our perplexity evaluation measures model perplexity on the 1-best output of a baseline SMT system and a human reference translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.9773381948471069}]}, {"text": "Our SMT evaluation integrates the model as a feature function in a string-to-tree SMT system and evaluates its impact on translation quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9900887608528137}]}, {"text": "Finally, we quantify the effect of different language models on grammaticality by measuring the number of agreement errors of our SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.9837397933006287}]}, {"text": "We refer to the unlabelled variant of our model (equation 2) as DLM, and to the labelled variant (equation 4) as RDLM, emphasizing that the latter is a relational dependency LM.", "labels": [], "entities": [{"text": "RDLM", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.900615930557251}]}], "tableCaptions": [{"text": " Table 1: Handling unavailable input words by replacing  them with null words.", "labels": [], "entities": []}, {"text": " Table 2: Perplexity of different Neural Network language  models (and baseline with Kneser-Ney smoothing) on  German reference translation (newstest2013) and base- line English\u2192German translation output. Our goal is a  language model that prefers the reference over the trans- lation hypothesis, indicated by a lower perplexity and a  positive entropy difference.", "labels": [], "entities": []}, {"text": " Table 3: Translation quality of English\u2192German string-to-tree SMT system with different language models, with k- best batch MIRA optimization on BLEU and BLEU+HWCM f . Average of 3 optimization runs. bold: no other system  in same block is significantly better (p < 0.05); *: significantly better than same model with other MIRA objective  (p < 0.05). Higher scores are better for BLEU, HWCM f and METEOR; lower scores are better for TER.", "labels": [], "entities": [{"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.8219599723815918}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9798206090927124}, {"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9627169370651245}, {"text": "BLEU", "start_pos": 382, "end_pos": 386, "type": "METRIC", "confidence": 0.98682701587677}, {"text": "METEOR", "start_pos": 399, "end_pos": 405, "type": "METRIC", "confidence": 0.9670397043228149}, {"text": "TER", "start_pos": 435, "end_pos": 438, "type": "METRIC", "confidence": 0.9929686188697815}]}, {"text": " Table 4: SMT output of baseline system and best system (RDLM tuned on BLEU+HWCM f ).", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9741296172142029}, {"text": "RDLM", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9569898843765259}, {"text": "BLEU+HWCM f", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.7656357958912849}]}, {"text": " Table 6: Number of English\u2192German translation hy- potheses with at least one agreement error according  to unification grammar (Williams and Koehn, 2011) on  newstest2013 (3000 sentences). Average of three MIRA  runs.", "labels": [], "entities": [{"text": "newstest2013", "start_pos": 159, "end_pos": 171, "type": "DATASET", "confidence": 0.9623147249221802}, {"text": "MIRA", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.8529366254806519}]}]}