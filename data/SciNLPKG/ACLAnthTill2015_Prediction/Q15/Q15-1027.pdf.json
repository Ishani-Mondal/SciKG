{"title": [{"text": "Deriving Boolean structures from distributional vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "Corpus-based distributional semantic models capture degrees of semantic relatedness among the words of very large vocabularies , but have problems with logical phenomena such as entailment, that are instead elegantly handled by model-theoretic approaches, which, in turn, do not scale up.", "labels": [], "entities": []}, {"text": "We combine the advantages of the two views by inducing a mapping from distributional vectors of words (or sentences) into a Boolean structure of the kind in which natural language terms are assumed to denote.", "labels": [], "entities": []}, {"text": "We evaluate this Boolean Distributional Semantic Model (BDSM) on recognizing entailment between words and sentences.", "labels": [], "entities": [{"text": "recognizing entailment between words and sentences", "start_pos": 65, "end_pos": 115, "type": "TASK", "confidence": 0.8512974580128988}]}, {"text": "The method achieves results comparable to a state-of-the-art SVM, degrades more gracefully when less training data are available and displays interesting qualitative properties.", "labels": [], "entities": [{"text": "SVM", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9550995826721191}]}], "introductionContent": [{"text": "Different aspects of natural language semantics have been studied from different perspectives.", "labels": [], "entities": [{"text": "natural language semantics", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.6566664377848307}]}, {"text": "Distributional semantic models induce large-scale vector-based lexical semantic representations from statistical patterns of word usage.", "labels": [], "entities": []}, {"text": "These models have proven successful in tasks relying on meaning relatedness, such as synonymy detection, word sense discrimination, or even measuring phrase plausibility).", "labels": [], "entities": [{"text": "synonymy detection", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.803603321313858}, {"text": "word sense discrimination", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.7220494151115417}]}, {"text": "On the other hand, logical relations and operations, such as entailment, contradiction, conjunction and negation, receive an elegant treatment informal semantic models.", "labels": [], "entities": []}, {"text": "The latter lack, however, general procedures to learn from data, and consequently have problems scaling up to real-life problems.", "labels": [], "entities": []}, {"text": "Formal semantics captures fundamental aspects of meaning in set-theoretic terms: Entailment, for example, is captured as the inclusion relation between the sets (of the relevant type) denoted by words or other linguistic expressions, e.g., sets of possible worlds that two propositions hold of.", "labels": [], "entities": []}, {"text": "In finite models, a mathematically convenient way to represent these denotations is to encode them in Boolean vectors, i.e., vectors of 0s and 1s).", "labels": [], "entities": []}, {"text": "Given all elements e i in the domain in which linguistic expressions of a certain type denote, the Boolean vector associated to a linguistic expression of that type has 1 in position i if e i \u2208 S for S the set denoted by the expression, 0 otherwise.", "labels": [], "entities": []}, {"text": "An expression a entailing b will have a Boolean vector including the one of b, in the sense that all positions occupied by 1s in the b vector are also set to 1 in the a vector.", "labels": [], "entities": []}, {"text": "Very general expressions (entailing nearly everything else) will have very dense vectors, whereas very specific expressions will have very sparse vectors.", "labels": [], "entities": []}, {"text": "The negation of an expression a will denote a \"flipped\" version of the a Boolean vector.", "labels": [], "entities": []}, {"text": "Vice versa, two expressions with at least partially compatible meanings will have some overlap of the 1s in their vectors; conjunction and disjunction are carried through with the obvious bit-wise operations, etc.", "labels": [], "entities": []}, {"text": "To narrow the gap between the large-scale inductive properties of distributional semantic models and the logical power of Boolean semantics, we create Boolean meaning representations that build on the wealth of information inherent in distributional vectors of words (and sentences).", "labels": [], "entities": []}, {"text": "More precisely, we use word (or sentence) pairs labeled as entailing or not entailing to train a mapping from their distributional representations to Boolean vectors, enforcing feature inclusion in Boolean space for the entailing pairs.", "labels": [], "entities": []}, {"text": "By focusing on inducing Boolean representations that respect the inclusion relation, our method is radically different from recent supervised approaches that learn an entailment classifier directly on distributional vectors, without enforcing inclusion or other representational constraints.", "labels": [], "entities": []}, {"text": "We show, experimentally, that the method is competitive against state-of-the-art techniques in lexical entailment, improving on them in sentential entailment, while learning more effectively from less training data.", "labels": [], "entities": []}, {"text": "This is crucial for practical applications that involve bigger and more diverse data than the focused test sets we used for testing.", "labels": [], "entities": []}, {"text": "Moreover, extensive qualitative analysis reveals several interesting properties of the Boolean vectors we induce, suggesting that they are representations of greater generality beyond entailment, that might be exploited in further work for other logic-related semantic tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our approach is agnostic to the kind of distributional representation used, since it doesn't modify the input vectors, but builds on top of them.", "labels": [], "entities": []}, {"text": "Still, it is interesting to test whether specific kinds of distributional vectors are better suited to act as input to BDSM.", "labels": [], "entities": []}, {"text": "For our experiments, we use both the count and predict distributional semantic vectors of . 3 These vectors were shown by their creators to reach the best average performance (among comparable alternatives) on a variety of semantic relatedness/similarity tasks, such as synonymy detection, concept categorization and analogy solving.", "labels": [], "entities": [{"text": "synonymy detection", "start_pos": 270, "end_pos": 288, "type": "TASK", "confidence": 0.84090456366539}, {"text": "concept categorization", "start_pos": 290, "end_pos": 312, "type": "TASK", "confidence": 0.7435248494148254}, {"text": "analogy solving", "start_pos": 317, "end_pos": 332, "type": "TASK", "confidence": 0.9226363003253937}]}, {"text": "If the same vectors turnout to also serve as good inputs for constructing Boolean representations, we are thus getting the best of both worlds: distributional vectors with proven high performance on relatedness/similarity tasks which can be mapped into a Boolean space to tackle logicrelated tasks.", "labels": [], "entities": []}, {"text": "We also experiment with the pretrained vectors from TypeDM (, which are built by exploiting syntactic information, and should have different qualitative properties from the window-based approaches.", "labels": [], "entities": []}, {"text": "The count vectors of Baroni and colleagues are built from a 2-word-window co-occurrence matrix of 300k lower-cased words extracted from a 2.8 billion tokens corpus.", "labels": [], "entities": []}, {"text": "The matrix is weighted using positive Pointwise Mutual Information).", "labels": [], "entities": []}, {"text": "We use the full 300k\u00d7300k positive PMI matrix to compute the asymmetric similarity measures discussed in the next section, since the latter are designed for non-negative, sparse, full-rank representations.", "labels": [], "entities": []}, {"text": "Due to efficiency constraints, for BDSM and SVM (also presented next), the matrix is reduced to 300 dimensions by Singular Value Decomposition.", "labels": [], "entities": []}, {"text": "The experiments of  with these very same vectors suggest that SVD is lowering performance somewhat.", "labels": [], "entities": [{"text": "SVD", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9553095102310181}]}, {"text": "So we are, if anything, giving an advantage to the simple asymmetric measures.", "labels": [], "entities": []}, {"text": "The predict vectors are built with the word2vec tool () on the same corpus and for the same vocabulary as the count vectors, using the CBOW method.", "labels": [], "entities": []}, {"text": "They are constructed by associating 400-dimensional vectors to each word in the vocabulary and optimizing a single-layer neural network that, while traversing the training corpus, tries to predict the word in the center of a 5-word window from the vectors of those surrounding it.", "labels": [], "entities": []}, {"text": "The word2vec subsampling parameter (that downweights the impact of frequent words) is set to 1e \u22125 . Finally, TypeDM vectors were induced from the same corpus by taking into account the dependency links of a word with its sentential collocates.", "labels": [], "entities": []}, {"text": "Composition methods For sentence entailment (Section 6), we need vectors for sentences, rather than words.", "labels": [], "entities": [{"text": "sentence entailment", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7235157042741776}]}, {"text": "We derive them from the count vectors compositionally in two different ways.", "labels": [], "entities": []}, {"text": "First, we use the additive model (add), under which we sum the vectors of the words they contain to obtain sentence representations (.", "labels": [], "entities": []}, {"text": "This approach, however, does not take into account word order, which is of obvious relevance to determining entailment between phrases.", "labels": [], "entities": []}, {"text": "For example, a dog chases a cat does not entail a cat chases a dog, whereas each sentence entails itself.", "labels": [], "entities": []}, {"text": "Therefore, we also used sentence vectors derived with the linguistically-motivated \"practical lexical function\" model (plf), that takes syntactic structure and word order into account ().", "labels": [], "entities": []}, {"text": "In short, words acting as argument-taking functions (such as verbs) are not only associated to vectors, but also to one matrix for each argument they take (e.g., each transitive verb comes with a subject and an object matrix).", "labels": [], "entities": []}, {"text": "Vector representations of arguments are recursively multiplied by function matrices, following the syntactic structure of a sentence.", "labels": [], "entities": []}, {"text": "The final sentence representation is obtained by summing all the resulting vectors.", "labels": [], "entities": []}, {"text": "We used pre-trained vector and matrix representations provided by Paperno and colleagues.", "labels": [], "entities": []}, {"text": "Their setup is very comparable to the one of our count vectors: same source corpus, similar window size (3-word-window), positive PMI, and SVD reduction to 300 dimensions.", "labels": [], "entities": [{"text": "PMI", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.8919366598129272}]}, {"text": "The only notable differences area vocabulary cut-off to the top 30K most frequent words in the corpus, and the use of content words only as windows.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Percentage accuracy (LEDS) and F1  (BLESS) on the lexical entailment benchmarks.", "labels": [], "entities": [{"text": "Percentage accuracy (LEDS)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.8505982995033264}, {"text": "F1  (BLESS)", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.8563901633024216}]}, {"text": " Table 4: SICK results (percentages).", "labels": [], "entities": [{"text": "SICK", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.5828229188919067}]}]}