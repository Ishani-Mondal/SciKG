{"title": [{"text": "From Paraphrase Database to Compositional Paraphrase Model and Back", "labels": [], "entities": []}], "abstractContent": [{"text": "The Paraphrase Database (PPDB; Ganitke-vitch et al., 2013) is an extensive semantic resource , consisting of a list of phrase pairs with (heuristic) confidence estimates.", "labels": [], "entities": []}, {"text": "However, it is still unclear how it can best be used, due to the heuristic nature of the confidences and its necessarily incomplete coverage.", "labels": [], "entities": []}, {"text": "We propose models to leverage the phrase pairs from the PPDB to build parametric paraphrase models that score paraphrase pairs more accurately than the PPDB's internal scores while simultaneously improving its coverage.", "labels": [], "entities": []}, {"text": "They allow for learning phrase embeddings as well as improved word embeddings.", "labels": [], "entities": []}, {"text": "Moreover, we introduce two new, manually annotated datasets to evaluate short-phrase paraphrasing models.", "labels": [], "entities": []}, {"text": "Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Paraphrase detection 2 is the task of analyzing two segments of text and determining if they have the same meaning despite differences in structure and wording.", "labels": [], "entities": [{"text": "Paraphrase detection 2", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9329820473988851}]}, {"text": "It is useful fora variety of NLP tasks like question answering (, semantic parsing, textual entailment (, and machine translation).", "labels": [], "entities": [{"text": "question answering", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.8707296848297119}, {"text": "semantic parsing", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.73629330098629}, {"text": "textual entailment", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.6922852247953415}, {"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7729731500148773}]}, {"text": "One component of many such systems is a paraphrase table containing pairs of text snippets, usually automatically generated, that have the same meaning.", "labels": [], "entities": []}, {"text": "The most recent work in this area is the Paraphrase Database (PPDB;, a collection of confidence-rated paraphrases created using the pivoting technique of overlarge parallel corpora.", "labels": [], "entities": []}, {"text": "The PPDB is a massive resource, containing 220 million paraphrase pairs.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9145992994308472}]}, {"text": "It captures many short paraphrases that would be difficult to obtain using any other resource.", "labels": [], "entities": []}, {"text": "For example, the pair {we must do our utmost, we must make every effort} has little lexical overlap but is present in PPDB.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.9345998167991638}]}, {"text": "The PPDB has recently been used for monolingual alignment (, for predicting sentence similarity (), and to improve the coverage of FrameNet ().", "labels": [], "entities": [{"text": "monolingual alignment", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7255634963512421}, {"text": "predicting sentence similarity", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.8680822650591532}, {"text": "FrameNet", "start_pos": 131, "end_pos": 139, "type": "DATASET", "confidence": 0.8410231471061707}]}, {"text": "Though already effective for multiple NLP tasks, we note some drawbacks of PPDB.", "labels": [], "entities": []}, {"text": "The first is lack of coverage: to use the PPDB to compare two phrases, both must be in the database.", "labels": [], "entities": [{"text": "coverage", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9919994473457336}]}, {"text": "The second is that PPDB is a nonparametric paraphrase model; the number of parameters (phrase pairs) grows with the size of the dataset used to build it.", "labels": [], "entities": []}, {"text": "In practice, it can become unwieldy to work with as the size of the database increases.", "labels": [], "entities": []}, {"text": "A third concern is that the confidence estimates in PPDB area heuristic combination of features, and their quality is unclear.", "labels": [], "entities": []}, {"text": "We address these issues in this work by introducing ways to use PPDB to construct parametric paraphrase models.", "labels": [], "entities": []}, {"text": "First we show that initial skip-gram word vectors () can be finetuned for the paraphrase task by training on word pairs from PPDB.", "labels": [], "entities": []}, {"text": "We call them PARAGRAM word vectors.", "labels": [], "entities": [{"text": "PARAGRAM word vectors", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.5802185932795206}]}, {"text": "We find additive composition of PARA-GRAM vectors to be a simple but effective way to embed phrases for short-phrase paraphrase tasks.", "labels": [], "entities": []}, {"text": "We find improved performance by training a recursive neural network (RNN;) directly on phrase pairs from PPDB.", "labels": [], "entities": []}, {"text": "We show that our resulting word and phrase representations are effective on a wide variety of tasks, including two new datasets that we introduce.", "labels": [], "entities": []}, {"text": "The first, Annotated-PPDB, contains pairs from PPDB that were scored by human annotators.", "labels": [], "entities": []}, {"text": "It can be used to evaluate paraphrase models for short phrases.", "labels": [], "entities": []}, {"text": "We use it to show that the phrase embeddings produced by our methods are significantly more indicative of paraphrasability than the original heuristic scoring used by.", "labels": [], "entities": []}, {"text": "Thus we use the power of PPDB to improve its contents.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.9022951126098633}]}, {"text": "Our second dataset, ML-Paraphrase, is a reannotation of the bigram similarity corpus from.", "labels": [], "entities": [{"text": "bigram similarity corpus", "start_pos": 60, "end_pos": 84, "type": "DATASET", "confidence": 0.6460480888684591}]}, {"text": "The task was originally developed to measure semantic similarity of bigrams, but some annotations are not congruent with the functional similarity central to paraphrase relationships.", "labels": [], "entities": []}, {"text": "Our re-annotation can be used to assess paraphrasing capability of bigram compositional models.", "labels": [], "entities": []}, {"text": "In summary, we make the following contributions: Provide new PARAGRAM word vectors, learned using PPDB, that achieve state-of-the-art performance on the SimLex-999 lexical similarity task () and lead to improved performance in sentiment analysis.", "labels": [], "entities": [{"text": "PARAGRAM", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.8655500411987305}, {"text": "SimLex-999 lexical similarity task", "start_pos": 153, "end_pos": 187, "type": "TASK", "confidence": 0.564764216542244}, {"text": "sentiment analysis", "start_pos": 227, "end_pos": 245, "type": "TASK", "confidence": 0.9686935842037201}]}, {"text": "Provide ways to use PPDB to embed phrases.", "labels": [], "entities": []}, {"text": "We compare additive and RNN composition of PARA-GRAM vectors.", "labels": [], "entities": []}, {"text": "Both can improve PPDB by reranking the paraphrases in PPDB to improve correlations with human judgments.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.7972391843795776}]}, {"text": "They can be used as concise parameterizations of PPDB, thereby vastly increasing its coverage.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.8194353580474854}]}, {"text": "We also perform a qualitative analysis of the differences between additive and RNN composition.", "labels": [], "entities": []}, {"text": "The first contains PPDB phrase pairs and evaluates how well models can measure the quality of short paraphrases.", "labels": [], "entities": [{"text": "PPDB phrase pairs", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.5812126398086548}]}, {"text": "The second is anew annotation of the bigram similarity task in that makes it suitable for evaluating bigram paraphrases.", "labels": [], "entities": []}, {"text": "We release the new datasets, complete with annotation instructions and raw annotations, as well as our code and the trained models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We created two novel datasets: (1) Annotated-PPDB, a subset of phrase pairs from PPDB which are annotated according to how strongly they represent a paraphrase relationship, and (2) MLParaphrase, a re-annotation of the bigram similarity dataset from, again annotated for strength of paraphrase relationship.", "labels": [], "entities": [{"text": "MLParaphrase", "start_pos": 182, "end_pos": 194, "type": "DATASET", "confidence": 0.5731748938560486}, {"text": "bigram similarity dataset", "start_pos": 219, "end_pos": 244, "type": "DATASET", "confidence": 0.579254537820816}]}, {"text": "We first present experiments on learning lexical paraphrasability.", "labels": [], "entities": []}, {"text": "We train on word pairs from PPDB and evaluate on the SimLex-999 dataset (, achieving the best results reported to date.", "labels": [], "entities": [{"text": "SimLex-999 dataset", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.9587108194828033}]}, {"text": "Hyperparameters were tuned using the wordsim-353 (WS353) dataset (), specifically its similarity (WS-S) and relatedness (WS-R) partitions ().", "labels": [], "entities": [{"text": "wordsim-353 (WS353) dataset", "start_pos": 37, "end_pos": 64, "type": "DATASET", "confidence": 0.7403037428855896}]}, {"text": "In particular, we tuned to maximize 2\u00d7WS-S correlation minus the WS-R correlation.", "labels": [], "entities": [{"text": "WS-S correlation", "start_pos": 38, "end_pos": 54, "type": "METRIC", "confidence": 0.6360046565532684}]}, {"text": "The idea was to reward vectors with high similarity and relatively low relatedness, in order to target the paraphrase relationship.: Results on the SimLex-999 (SL999) word similarity task obtained by performing hyperparameter tuning based on 2\u00d7WS-S \u2212WS-R and treating SL999 as a held-out test set.", "labels": [], "entities": [{"text": "SimLex-999 (SL999) word similarity task", "start_pos": 148, "end_pos": 187, "type": "TASK", "confidence": 0.6106752923556736}]}, {"text": "n is word vector dimensionality.", "labels": [], "entities": []}, {"text": "A * indicates statistical significance (p < 0.05) over the 1000-dimensional skip-gram vectors.", "labels": [], "entities": [{"text": "A", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9757453799247742}, {"text": "statistical significance", "start_pos": 14, "end_pos": 38, "type": "METRIC", "confidence": 0.7980139851570129}]}, {"text": "After tuning, we evaluated the best hyperparameters on the SimLex-999 (SL999) dataset ().", "labels": [], "entities": [{"text": "SimLex-999 (SL999) dataset", "start_pos": 59, "end_pos": 85, "type": "DATASET", "confidence": 0.903892207145691}]}, {"text": "We chose SL999 as our primary test set as it most closely evaluates the paraphrase relationship.", "labels": [], "entities": []}, {"text": "Even though WS-S is a close approximation to this relationship, it does not include pairs that are merely associated and assigned low scores, which SL999 does (see discussion in.", "labels": [], "entities": []}, {"text": "Note that for all experiments we used cosine similarity as our similarity metric and evaluated the statistical significance of dependent correlations using the one-tailed method of. shows results on SL999 when improving the initial word vectors by training on word pairs from PPDB, both with and without constraints.", "labels": [], "entities": []}, {"text": "The \"PARAGRAM WS \" rows show results when tuning to maximize 2\u00d7WS-S \u2212 WS-R.", "labels": [], "entities": [{"text": "PARAGRAM WS \" rows", "start_pos": 5, "end_pos": 23, "type": "METRIC", "confidence": 0.8667185455560684}]}, {"text": "We also show results for strong skip-gram baselines and the best results from the literature, including the state-of-the-art results from as well as the interannotator agreement from.", "labels": [], "entities": []}, {"text": "The table illustrates that, by training on PPDB, we can surpass the previous best correlations on SL999 by 4-6% absolute, achieving the best results reported to date.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8401663303375244}]}, {"text": "We also find that we can train low-dimensional word vectors that exceed the performance of much larger vectors.", "labels": [], "entities": []}, {"text": "This is very useful as using large vectors can increase both time and memory consumption in NLP applications.", "labels": [], "entities": []}, {"text": "In this section, we describe experiments on a variety of compositional phrase-based paraphrasing tasks.", "labels": [], "entities": []}, {"text": "We start with the simplest case of bigrams, and then proceed to short phrases.", "labels": [], "entities": []}, {"text": "For all tasks, we again train on appropriate data from PPDB and test on various evaluation datasets, including our two novel datasets (Annotated-PPDB and ML-Paraphrase).", "labels": [], "entities": [{"text": "Annotated-PPDB", "start_pos": 135, "end_pos": 149, "type": "METRIC", "confidence": 0.8746256232261658}]}, {"text": "For all experiments, we again used cosine similarity as our similarity metric and evaluated the statistical significance using the method of.", "labels": [], "entities": []}, {"text": "A baseline used in all compositional experiments is vector addition of skip-gram (or PARAGRAM) word vectors.", "labels": [], "entities": [{"text": "PARAGRAM", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9037848114967346}]}, {"text": "Unlike explicit word vectors, where point-wise multiplication acts as a conjunction of features and performs well on composition tasks, using addition with skip-gram vectors ( gives better performance than multiplication.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results on the SimLex-999 (SL999) word similarity  task obtained by performing hyperparameter tuning based on  2\u00d7WS-S \u2212WS-R and treating SL999 as a held-out test set. n  is word vector dimensionality. A  *  indicates statistical signifi- cance (p < 0.05) over the 1000-dimensional skip-gram vectors.", "labels": [], "entities": [{"text": "SimLex-999 (SL999) word similarity  task", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.630211957863399}]}, {"text": " Table 4: Test set accuracies when comparing embeddings in a  static CNN on the binary sentiment analysis task from Socher  et al. (2013).", "labels": [], "entities": [{"text": "binary sentiment analysis task", "start_pos": 80, "end_pos": 110, "type": "TASK", "confidence": 0.7172076627612114}]}, {"text": " Table 6: Spearman correlation on Annotated-PPDB. The *  indicates statistically significant (p < 0.05) over the skip- gram model, the  \u2020 indicates statistically significant over the  {PARAGRAM, +} model, and the  \u2021 indicates statistically sig- nificant over PPDB+SVR.", "labels": [], "entities": [{"text": "PARAGRAM", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.8709911108016968}]}, {"text": " Table 7: Average absolute error of addition and RNN models  on different ranges of gold scores.", "labels": [], "entities": [{"text": "Average absolute error", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.7804463505744934}, {"text": "RNN", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.6877928972244263}]}, {"text": " Table 8: Illustrative phrase pairs from Annotated-PPDB with gold similarity > 4. The last three columns show the gold similarity  score, the similarity score of the RNN model, and the similarity score of vector addition. We note that addition performs better  when the pairs have high length ratio (rows 1-2) or overlap ratio (rows 3-4) while the RNN does better when those values are low  (rows 5-6 and 7-8 respectively). Boldface indicates smaller error compared to gold scores.", "labels": [], "entities": [{"text": "gold similarity  score", "start_pos": 114, "end_pos": 136, "type": "METRIC", "confidence": 0.8234692613283793}, {"text": "similarity score", "start_pos": 142, "end_pos": 158, "type": "METRIC", "confidence": 0.9478758573532104}, {"text": "overlap ratio", "start_pos": 313, "end_pos": 326, "type": "METRIC", "confidence": 0.9424563646316528}]}]}