{"title": [{"text": "Parsing Algebraic Word Problems into Equations", "labels": [], "entities": [{"text": "Parsing Algebraic Word Problems", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8501301407814026}]}], "abstractContent": [{"text": "This paper formalizes the problem of solving multi-sentence algebraic word problems as that of generating and scoring equation trees.", "labels": [], "entities": []}, {"text": "We use integer linear programming to generate equation trees and score their likelihood by learning local and global discriminative models.", "labels": [], "entities": []}, {"text": "These models are trained on a small set of word problems and their answers, without any manual annotation, in order to choose the equation that best matches the problem text.", "labels": [], "entities": []}, {"text": "We refer to the overall system as ALGES.", "labels": [], "entities": [{"text": "ALGES", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.6880438923835754}]}, {"text": "We compare ALGES with previous work and show that it covers the full gamut of arithmetic operations whereas Hosseini et al.", "labels": [], "entities": []}, {"text": "(2014) only handle addition and subtraction.", "labels": [], "entities": []}, {"text": "In addition, ALGES overcomes the brittleness of the Kush-man et al.", "labels": [], "entities": [{"text": "ALGES", "start_pos": 13, "end_pos": 18, "type": "TASK", "confidence": 0.6703453063964844}]}, {"text": "(2014) approach on single-equation problems, yielding a 15% to 50% reduction in error.", "labels": [], "entities": [{"text": "error", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.8758823275566101}]}], "introductionContent": [{"text": "Grade-school algebra word problems are brief narratives (see.", "labels": [], "entities": [{"text": "Grade-school algebra word problems", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6233303919434547}]}, {"text": "A typical problem first describes a partial world state consisting of characters, entities, and quantities.", "labels": [], "entities": []}, {"text": "Next it updates the condition of an entity or explicates the relationship between entities.", "labels": [], "entities": []}, {"text": "Finally, it poses a question about a quantity in the narrative.", "labels": [], "entities": []}, {"text": "An ordinary child has to learn the required algebra, but will easily grasp the narrative utilizing extensive world knowledge, large vocabulary, wordsense disambiguation, coreference resolution, mastery of syntax, and the ability to combine individual  sentences into a coherent mental model.", "labels": [], "entities": [{"text": "wordsense disambiguation", "start_pos": 144, "end_pos": 168, "type": "TASK", "confidence": 0.7181289196014404}, {"text": "coreference resolution", "start_pos": 170, "end_pos": 192, "type": "TASK", "confidence": 0.874858021736145}]}, {"text": "In contrast, the challenge for an NLP system is to \"make sense\" of the narrative, which may refer to arbitrary activities like renting bikes, collecting coins, or eating cookies.", "labels": [], "entities": []}, {"text": "Previous work coped with the open-domain aspect of algebraic word problems by relying on deterministic state transitions based on verb categorization () or by learning templates that cover equations of particular forms).", "labels": [], "entities": []}, {"text": "We have discovered, however, that both approaches are brittle, particularly as training data is scarce in this domain, and the space of equations grows exponentially with the number of quantities mentioned in the math problem.", "labels": [], "entities": []}, {"text": "We introduce ALGES, 1 which maps an unseen multi-sentence algebraic word problem into a set of possible equation trees.", "labels": [], "entities": [{"text": "ALGES", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.8279671669006348}]}, {"text": "shows an equation tree alongside the word problem it represents.", "labels": [], "entities": []}, {"text": "ALGES generates the space of trees via Integer Linear Programming (ILP), which allows it to con-strain the space of trees to represent type-consistent algebraic equations satisfying as many desirable properties as possible.", "labels": [], "entities": []}, {"text": "ALGES learns to map spans of text to arithmetic operators, to combine them given the global context of the problem, and to choose the \"best\" tree corresponding to the problem.", "labels": [], "entities": []}, {"text": "The training set for ALGES consists of unannotated algebraic word problems and their solution.", "labels": [], "entities": [{"text": "ALGES", "start_pos": 21, "end_pos": 26, "type": "TASK", "confidence": 0.7678059339523315}]}, {"text": "Solving the equation represented by such a tree is trivial.", "labels": [], "entities": []}, {"text": "ALGES is described in detail in Section 4.", "labels": [], "entities": [{"text": "ALGES", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6559145450592041}]}, {"text": "ALGES is able to solve word problems with single-variable equations like the ones in.", "labels": [], "entities": [{"text": "ALGES", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8835728764533997}]}, {"text": "In contrast to, ALGES covers +, \u2212, * , and /.", "labels": [], "entities": []}, {"text": "The work of has broader scope but we show that it relies heavily on overlap between training and test data.", "labels": [], "entities": []}, {"text": "When that overlap is reduced, ALGES is 15% to 50% more accurate than this system.", "labels": [], "entities": [{"text": "ALGES", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.8193889260292053}, {"text": "accurate", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9869424104690552}]}, {"text": "Our contributions are as follows: We formalize the problem of solving multi-sentence algebraic word problems as that of generating and ranking equation trees; (2) We show how to score the likelihood of equation trees by learning discriminative models trained from a small number of word problems and their solutions -without any manual annotation; and (3) We demonstrate empirically that ALGES has broader scope than the system of, and overcomes the brittleness of the method of.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section reports on three experiments: a comparison of ALGES with's template-based method, a comparison of ALGES with's verb-categorization methods, and ablation studies.", "labels": [], "entities": []}, {"text": "The experiments are complicated by the fact that ALGES is limited to single equations, and the verb categorization method can only handle single-equations without multiplication or division.", "labels": [], "entities": []}, {"text": "Our main experimental result is to show an improvement over the template-based method on single-equation algebra word problems.", "labels": [], "entities": []}, {"text": "We further show that the template-based method depends on lexical and template overlap between its training and test sets.", "labels": [], "entities": []}, {"text": "When these overlaps are reduced, the method's accuracy drops sharply.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9996412992477417}]}, {"text": "In contrast, ALGES is quite robust to changes in lexical and template overlap (see).", "labels": [], "entities": []}, {"text": "We use the Stanford Dependency Parser in CoreNLP 3.4) to obtain syntactic information used for grounding and feature computation.", "labels": [], "entities": [{"text": "Stanford Dependency Parser in CoreNLP 3.4", "start_pos": 11, "end_pos": 52, "type": "DATASET", "confidence": 0.7902226150035858}]}, {"text": "For the ILP model, we use CPLEX 12.6.1 to generate the top M = 100 equation trees with a maximum stack depth of 10, aborting exploration upon hitting 10K feasible solutions or 30 seconds.", "labels": [], "entities": [{"text": "CPLEX 12.6.1", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.8133199512958527}]}, {"text": "We use Python's SymPy package for solving equations for the unknown.", "labels": [], "entities": []}, {"text": "For the local and global models, we use the LIBSVM package to train SVM classifiers (Chang and Lin, 2011) with RBF kernels that return likelihood estimates as the score.", "labels": [], "entities": []}, {"text": "This work deals with grade-school algebra word problems that map to single equations with varying length.", "labels": [], "entities": []}, {"text": "Every equation may involve multiple math operations including multiplication, division, subtraction, and addition over non-negative rational numbers and one variable.", "labels": [], "entities": []}, {"text": "The data is gathered from http://math-aids.com, http: //k5learning.com, and http://ixl.com websites and a subset of the data from that maps word problems to single equations.", "labels": [], "entities": []}, {"text": "We refer to this dataset as SINGLEEQ (see for example problems).", "labels": [], "entities": [{"text": "SINGLEEQ", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.7021437287330627}]}, {"text": "The SINGLEEQ dataset consists of 508 problems, 1,117 sentences, and 15,292 words.", "labels": [], "entities": [{"text": "SINGLEEQ dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.7094394713640213}]}, {"text": "We compare our method with the template-based method () and the verb-categorization method ().", "labels": [], "entities": []}, {"text": "For the template-based method, we use the fully supervised setting, providing equations for each training example.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Decreasing template overlap: Accuracy of  ALGES versus the template-based method on single- equation algebra word problems. The first column corre- sponds to the SINGLEEQ dataset, and the other columns  are for subsets with decreasing template overlap.", "labels": [], "entities": [{"text": "SINGLEEQ dataset", "start_pos": 172, "end_pos": 188, "type": "DATASET", "confidence": 0.7951956689357758}]}, {"text": " Table 5: Decreasing lexical overlap: Accuracy of ALGES  versus the template-based method on single-equation al- gebra word problems. The first column corresponds to  the SINGLEEQ dataset, and the other columns are for sub- sets with decreasing lexical overlap.", "labels": [], "entities": [{"text": "SINGLEEQ dataset", "start_pos": 171, "end_pos": 187, "type": "DATASET", "confidence": 0.7874461710453033}]}, {"text": " Table 7: Ablation study of each component of ALGES.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9821818470954895}, {"text": "ALGES", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.7241379618644714}]}, {"text": " Table 8: Accuracy of local classifier in predicting the cor- rect operator between two Qsets and ablating feature sets.", "labels": [], "entities": []}]}