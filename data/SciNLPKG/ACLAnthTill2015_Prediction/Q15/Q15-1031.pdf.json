{"title": [{"text": "Modeling Word Forms Using Latent Underlying Morphs and Phonology", "labels": [], "entities": [{"text": "Modeling Word Forms", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8012992143630981}]}], "abstractContent": [{"text": "The observed pronunciations or spellings of words are often explained as arising from the \"underlying forms\" of their morphemes.", "labels": [], "entities": []}, {"text": "These forms are latent strings that linguists try to reconstruct by hand.", "labels": [], "entities": []}, {"text": "We propose to reconstruct them automatically at scale, enabling generalization to new words.", "labels": [], "entities": []}, {"text": "Given some surface word types of a concatenative language along with the abstract morpheme sequences that they express , we show how to recover consistent underlying forms for these morphemes, together with the (stochastic) phonology that maps each concatenation of underlying forms to a surface form.", "labels": [], "entities": []}, {"text": "Our technique involves loopy belief propagation in a natural directed graphical model whose variables are unknown strings and whose conditional distributions are encoded as finite-state machines with trainable weights.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.6920918822288513}]}, {"text": "We define training and evaluation paradigms for the task of surface word prediction, and report results on subsets of 7 languages.", "labels": [], "entities": [{"text": "surface word prediction", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.615779290596644}]}], "introductionContent": [{"text": "How is plurality expressed in English?", "labels": [], "entities": []}, {"text": "Comparing cats), dogs (), and quizzes (), the plural morpheme evidently has at least three pronunciations (,,) and at least two spellings (-s and -es).", "labels": [], "entities": []}, {"text": "Also, considering singular quiz, perhaps the \"short exam\" morpheme has multiple spellings (quizz-, quiz-).", "labels": [], "entities": []}, {"text": "Fortunately, languages are systematic.", "labels": [], "entities": []}, {"text": "The realization of a morpheme may vary by context but is largely predictable from context, in away that generalizes across morphemes.", "labels": [], "entities": []}, {"text": "In fact, generative linguists traditionally posit that each morpheme of a language has a single representation shared across all contexts).", "labels": [], "entities": [{"text": "generative linguists", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.9262136220932007}]}, {"text": "However, this string is a latent variable that is never observed.", "labels": [], "entities": []}, {"text": "Variation appears when the phonology of the language maps these underlying representations (URs)-in context-to surface representations (SRs) that maybe easier to pronounce.", "labels": [], "entities": []}, {"text": "The phonology is usually described by a grammar that may consist of either rewrite rules or ranked constraints ().", "labels": [], "entities": []}, {"text": "We will review this framework in section 2.", "labels": [], "entities": []}, {"text": "The upshot is that the observed words in a language are supposed to be explainable in terms of a smaller underlying lexicon of morphemes, plus a phonology.", "labels": [], "entities": []}, {"text": "Our goal in this paper is to recover the lexicon and phonology (enabling generalization to new words).", "labels": [], "entities": []}, {"text": "This is difficult even when we are told which morphemes are expressed by each word, because the unknown underlying forms of the morphemes must cooperate properly with one another and with the unknown phonological rules to produce the observed results.", "labels": [], "entities": []}, {"text": "Because of these interactions, we must reconstruct everything jointly.", "labels": [], "entities": []}, {"text": "We regard this as a problem of inference in a directed graphical model, as sketched in.", "labels": [], "entities": []}, {"text": "This is a natural problem for computational linguistics.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7802109718322754}]}, {"text": "Phonology students are trained to puzzle out solutions for small datasets by hand.", "labels": [], "entities": []}, {"text": "Children apparently solve it at the scale of an entire language.", "labels": [], "entities": []}, {"text": "Phonologists would like to have grammars for many languages, not just to study each language but also to understand universal principles and differences among related languages.", "labels": [], "entities": []}, {"text": "Automatic procedures would recover such grammars.", "labels": [], "entities": []}, {"text": "They would also allow comprehensive evaluation and comparison of different phonological theories (i.e., what inductive biases are useful?), and would suggest models of human language learning.", "labels": [], "entities": []}, {"text": "Solving this problem is also practically important for NLP.", "labels": [], "entities": []}, {"text": "What we recover is a model that can generate and help analyze novel word forms, which abound in morphologically complex languages.", "labels": [], "entities": []}, {"text": "Our approach is designed to model surface pronunciations (as needed for text-to-speech and ASR).", "labels": [], "entities": [{"text": "ASR", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9840514063835144}]}, {"text": "It might also be applied in practice An analyzer would require a prior over possible analyses.", "labels": [], "entities": []}, {"text": "Our present model defines just the corresponding likelihoods, i.e., the probability of the observed word given each analysis.", "labels": [], "entities": []}, {"text": "to model surface spellings (as needed for MT on text).", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9946712851524353}]}, {"text": "Good morphological analysis has been used to improve NLP tasks such as machine translation, parsing, and NER (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.836566299200058}, {"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.8997694253921509}, {"text": "NER", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.9078199863433838}]}, {"text": "Using loopy belief propagation, this paper attacks larger-scale learning problems than prior work on this task (section 8).", "labels": [], "entities": [{"text": "loopy belief propagation", "start_pos": 6, "end_pos": 30, "type": "TASK", "confidence": 0.6922166148821512}]}, {"text": "We also develop anew evaluation paradigm that examines how well an inferred grammar predicts held-out SRs.", "labels": [], "entities": []}, {"text": "Unlike previous algorithms, we do not pre-restrict the possible URs for each morpheme to a small or structured finite set, but use weighted finite-state machines to reason about the infinite space of all strings.", "labels": [], "entities": []}, {"text": "Our graphical model captures the standard assumption that each morpheme has a single UR, unlike some probabilistic learners.", "labels": [], "entities": [{"text": "UR", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9257650375366211}]}, {"text": "However, we do not try to learn traditional ordered rules or constraint rankings like previous methods.", "labels": [], "entities": []}, {"text": "We just search directly fora probabilistic finite-state transducer that captures likely UR-to-SR mappings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We objectively evaluate our learner on its ability to predict held-out surface forms.", "labels": [], "entities": []}, {"text": "This blind testing differs from traditional practice by linguists, who evaluate a manual or automatic analysis (= URs + phonology) on whether it describes the full dataset in a \"natural\" way that captures \"appropriate\" generalizations.", "labels": [], "entities": []}, {"text": "We avoid such theory-internal evaluation by simply quantifying whether the learner's analysis does generalize.", "labels": [], "entities": []}, {"text": "To avoid tailoring to our training/test data, we developed our method, code, features, and hyperparameters using only two development languages, English and German.", "labels": [], "entities": []}, {"text": "Thus, our learner was not engineered to do well on the other 5 languages below: the graphs below show its first attempt to learn those languages.", "labels": [], "entities": []}, {"text": "We do also evaluate our learners on English and German, using separate training/test data.", "labels": [], "entities": []}, {"text": "We provide all our data (including citations, development data, training-test splits, and natural classes) at http://hubal.cs.jhu.edu/ tacl2015/, along with brief sketches of the phonological phenomena in the datasets, the \"gold\" stem URs we assumed for evaluation, and our learner's predictions and error patterns.", "labels": [], "entities": []}, {"text": "Given a probability distribution p over surface word types of a language, we sample a training set of N types without replacement.", "labels": [], "entities": []}, {"text": "This simulates reading text until we have seen N distinct types.", "labels": [], "entities": []}, {"text": "For each of these frequent words, we observe the SR sand the morpheme sequence a.", "labels": [], "entities": []}, {"text": "After training our model, we evaluate its beliefs b about the SRs son a disjoint set of test words whose a are observed.", "labels": [], "entities": []}, {"text": "To improve interpretability of the results, we limit the test words to those whose morphemes have all appeared at least once in the training set.", "labels": [], "entities": []}, {"text": "(Any method would presumably get other words badly wrong, just as it would tend to get the training words right; we exclude both.)", "labels": [], "entities": []}, {"text": "To evaluate our belief b about the SR of a test word ( a, s * ), we use three measures for which \"smaller is better.\"", "labels": [], "entities": [{"text": "SR", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9781332015991211}]}, {"text": "First, 0-1 loss asks whether s * = argmax s b(s).", "labels": [], "entities": [{"text": "argmax s b", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9266629417737325}]}, {"text": "This could be compared with non-probabilistic predictors.", "labels": [], "entities": []}, {"text": "Second, the surprisal \u2212 log 2 b(s * ) is low if the model finds it plausible that s * realizes a.", "labels": [], "entities": [{"text": "surprisal \u2212 log 2 b", "start_pos": 12, "end_pos": 31, "type": "METRIC", "confidence": 0.9438929438591004}]}, {"text": "If so, this holds out promise for future work on analyzing or learning from unannotated tokens of s * . Third, we evaluate the whole We take the average of each measure over test words, weighting those words according top.", "labels": [], "entities": []}, {"text": "This yields our three reported metrics: 1-best error rate, cross-entropy, and expected edit distance.", "labels": [], "entities": [{"text": "1-best error rate", "start_pos": 40, "end_pos": 57, "type": "METRIC", "confidence": 0.8916365106900533}]}, {"text": "Each metric is the expected value of some measure on a random test token.", "labels": [], "entities": []}, {"text": "These metrics are actually random variables, since they depend on the randomly sampled training set and the resulting test distribution.", "labels": [], "entities": []}, {"text": "We report the expectations of these random variables by running many training-test splits (see section 7.2).", "labels": [], "entities": []}, {"text": "To test discovery of interesting patterns from limited data, we ran our learner on 5 \"exercises\" drawn from phonology textbooks (102 English nouns, 68 Maori verbs, 72 Catalan adjectives, 55 Tangale nouns, 44 Indonesian nouns), exhibiting a range of phenomena.", "labels": [], "entities": []}, {"text": "In each case we took p to be the uniform distribution over the provided word types.", "labels": [], "entities": []}, {"text": "We took N to be one less than the number of provided types.", "labels": [], "entities": []}, {"text": "Soto report our expected metrics, we ran all N + 1 experiments where we trained jointly on N forms and tested on the 1 remaining form.", "labels": [], "entities": []}, {"text": "This is close to linguists' practice of fitting an analysis on the entire dataset, yet it is a fair test.", "labels": [], "entities": []}, {"text": "There is no sampling error in these reported results, hence no need for error bars.", "labels": [], "entities": []}, {"text": "To test on larger, naturally occurring datasets, we ran our learner on subsets of the CELEX database (, which provides surface phonological forms and token counts for German, Dutch, and English words.", "labels": [], "entities": [{"text": "CELEX database", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.9620045721530914}]}, {"text": "For each language, we constructed a coherent subcorpus of 1000 nouns and verbs, focusing on inflections with common phonological phenomena.", "labels": [], "entities": []}, {"text": "These turned out to involve mainly voicing: final obstruent devoicing (German 2nd-person present indicative verbs, German nominative singular nouns, Dutch infinitive verbs, Dutch singular nouns) and voicing assimilation (English past tense verbs, English plural nouns).", "labels": [], "entities": []}, {"text": "We were restricted to relatively simple phenomena because our current representations are simple segmental strings that lack prosodic and autosegmental structure.", "labels": [], "entities": []}, {"text": "In future we plan to consider stress, vowel harmony, and templatic morphology.", "labels": [], "entities": [{"text": "vowel harmony", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.6808840483427048}, {"text": "templatic morphology", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.7575078904628754}]}, {"text": "We constructed the distribution pin proportion to CELEX's token counts.", "labels": [], "entities": [{"text": "CELEX's token counts", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.9252811968326569}]}, {"text": "In each language, we trained on N = 200, 400, 600, or 800 forms sampled from p.", "labels": [], "entities": []}, {"text": "To estimate the expectation of each metric overall training sets of size N , we report the sample mean and bootstrap standard error over 10 random training sets of size N . Except in Indonesian, every word happens to consist of \u2264 2 morphemes (a stem plus a possibly empty suffix).", "labels": [], "entities": [{"text": "bootstrap standard error", "start_pos": 107, "end_pos": 131, "type": "METRIC", "confidence": 0.9445566932360331}]}, {"text": "In all cases, we take the phoneme inventories \u03a3 u and \u03a3 s to be given as the set of all surface phonemes that appear in the full dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Percent of training words, weighted by the distribu- tion p, whose 1-best recovered UR (including the boundary #)  exactly matches the manual \"gold\" analysis. Results are av- erages over all runs (with N = 800 for the CELEX datasets).", "labels": [], "entities": [{"text": "UR", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.6564487218856812}, {"text": "CELEX datasets", "start_pos": 228, "end_pos": 242, "type": "DATASET", "confidence": 0.9914068877696991}]}]}