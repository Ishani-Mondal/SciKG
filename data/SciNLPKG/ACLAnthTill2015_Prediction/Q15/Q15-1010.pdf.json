{"title": [{"text": "Unsupervised Declarative Knowledge Induction for Constraint-Based Learning of Information Structure in Scientific Documents", "labels": [], "entities": [{"text": "Unsupervised Declarative Knowledge Induction", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.5719117224216461}]}], "abstractContent": [{"text": "Inferring the information structure of scientific documents is useful for many NLP applications.", "labels": [], "entities": []}, {"text": "Existing approaches to this task require substantial human effort.", "labels": [], "entities": []}, {"text": "We propose a framework for constraint learning that reduces human involvement considerably.", "labels": [], "entities": []}, {"text": "Our model uses topic models to identify latent topics and their key linguistic features in input documents, induces constraints from this information and maps sentences to their dominant information structure categories through a constrained unsupervised model.", "labels": [], "entities": []}, {"text": "When the induced constraints are combined with a fully unsupervised model, the resulting model challenges existing lightly supervised feature-based models as well as unsupervised models that use manually constructed declarative knowledge.", "labels": [], "entities": []}, {"text": "Our results demonstrate that useful declarative knowledge can be learned from data with very limited human involvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic analysis of scientific text can help scientists find information from literature faster, saving valuable research time.", "labels": [], "entities": []}, {"text": "In this paper we focus on the analysis of the information structure (IS) of scientific articles where the aim is to assign each unit of an article (typically a sentence) into a category that represents the information type it conveys.", "labels": [], "entities": [{"text": "analysis of the information structure (IS) of scientific articles", "start_pos": 30, "end_pos": 95, "type": "TASK", "confidence": 0.7344191399487582}]}, {"text": "By information structure we refer to a particular type of discourse structure that focuses on the functional role of a unit in the discourse).", "labels": [], "entities": []}, {"text": "For instance, in the scientific literature, the functional role of a sentence could be the background or motivation of the research, the methods used, the experiments carried out, the observations on the results, or the author's conclusions.", "labels": [], "entities": []}, {"text": "Readers of scientific literature find information in IS-annotated articles much faster than in unannotated articles).", "labels": [], "entities": []}, {"text": "Argumentative Zoning (AZ) -an information structure scheme that has been applied successfully to many scientific domains () -has improved tasks such as summarization and information extraction and retrieval.", "labels": [], "entities": [{"text": "Argumentative Zoning (AZ)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.810125207901001}, {"text": "summarization", "start_pos": 152, "end_pos": 165, "type": "TASK", "confidence": 0.9907714128494263}, {"text": "information extraction and retrieval", "start_pos": 170, "end_pos": 206, "type": "TASK", "confidence": 0.6921180784702301}]}, {"text": "Existing approaches to information structure analysis require substantial human effort.", "labels": [], "entities": [{"text": "information structure analysis", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.8141901095708212}]}, {"text": "Most use feature-based machine learning, such as SVMs and CRFs (e.g. () which rely on thousands of manually annotated training sentences.", "labels": [], "entities": []}, {"text": "Also the performance of such methods is rather limited: reported perclass F-scores ranging from .53 to .76 in the biochemistry and chemistry domains and reported substantially lower numbers for the challenging Introduction and Discussion sections in biomedical domain.", "labels": [], "entities": [{"text": "perclass F-scores", "start_pos": 65, "end_pos": 82, "type": "METRIC", "confidence": 0.84324511885643}]}, {"text": "recently applied the Generalized Expectation (GE) criterion) to information structure analysis using expert knowledge in the form of discourse and lexical constraints.", "labels": [], "entities": [{"text": "Generalized Expectation (GE) criterion", "start_pos": 21, "end_pos": 59, "type": "METRIC", "confidence": 0.6416071554025015}, {"text": "information structure analysis", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.7875808080037435}]}, {"text": "Their model produces promising results, especially for sections and categories where feature-based models perform poorly.", "labels": [], "entities": []}, {"text": "Even the unsupervised version which uses constraints under a maximum-entropy criterion without any featurebased model, outperforms fully-supervised featurebased models in detecting challenging low frequency categories across sections.", "labels": [], "entities": []}, {"text": "However, this approach still requires substantial human effort in constraint generation.", "labels": [], "entities": [{"text": "constraint generation", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7194421887397766}]}, {"text": "Particularly, lexical constraints were constructed by creating a detailed word list for each information structure category.", "labels": [], "entities": []}, {"text": "For example, words such as \"assay\" were carefully selected and used as a strong indicator of the \"Method\" category: p(Method|assay) was constrained to be high (above 0.9).", "labels": [], "entities": []}, {"text": "Such a constraint (developed for the biomedical domain) may not be applicable to anew domain (e.g. computer science) with a different vocabulary and writing style.", "labels": [], "entities": []}, {"text": "In fact, most existing works on learning with declarative knowledge rely on manually constructed constraints.", "labels": [], "entities": []}, {"text": "Little work exists on automatic declarative knowledge induction.", "labels": [], "entities": [{"text": "automatic declarative knowledge induction", "start_pos": 22, "end_pos": 63, "type": "TASK", "confidence": 0.5750805139541626}]}, {"text": "A notable exception is () that proposed a constraint learning model for timeline extraction.", "labels": [], "entities": [{"text": "timeline extraction", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.8446400463581085}]}, {"text": "This approach, however, requires human supervision in several forms including task specific constraint templates (see Section 2).", "labels": [], "entities": []}, {"text": "We present a novel framework for learning declarative knowledge which requires very limited human involvement.", "labels": [], "entities": []}, {"text": "We apply it to information structure analysis, based on two key observations: 1) Each information structure category defines a distribution over a section-specific and an article-level set of linguistic features.", "labels": [], "entities": [{"text": "information structure analysis", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.7660496830940247}]}, {"text": "2) Each sentence in a scientific document, while having a dominant category, may consist of features mostly related to other categories.", "labels": [], "entities": []}, {"text": "This flexible view enables us to make use of topic models which have not proved useful in previous related works ().", "labels": [], "entities": []}, {"text": "We construct topic models at both the individual section and article level and apply these models to data, identifying latent topics and their key linguistic features.", "labels": [], "entities": []}, {"text": "This information is used to constrain or bias unsupervised models for the task in a straightforward way: we automatically generate constraints fora GE model and a bias term fora graph clustering objective, such that the resulting models assign each of the input sentences to one information Zone Definition Background (BKG) the background of the study Problem work consistent with the current work work inconsistent with the current work Future work (FUT) the potential future direction of the research: The AZ categorization scheme of this paper structure category.", "labels": [], "entities": [{"text": "Zone Definition Background (BKG)", "start_pos": 291, "end_pos": 323, "type": "METRIC", "confidence": 0.6599030693372091}, {"text": "AZ", "start_pos": 508, "end_pos": 510, "type": "METRIC", "confidence": 0.7863191962242126}]}, {"text": "Both models provide high quality sentence-based classification, demonstrating the generality of our approach.", "labels": [], "entities": [{"text": "sentence-based classification", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.6784981340169907}]}, {"text": "We experiment with the AZ scheme for the analysis of the logical structure, scientific argumentation and intellectual attribution of scientific papers), using an eight-category version of this scheme for biomedicine),).", "labels": [], "entities": [{"text": "AZ", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9722639918327332}]}, {"text": "In evaluation against gold standard annotations, our model rivals the model of which relies on manually constructed constraints, as well as a strong supervised feature-based model trained with up to 2000 sentences.", "labels": [], "entities": []}, {"text": "In task-based evaluation we measure the usefulness of the induced categories for customized summarization) from specific types of information in an article.", "labels": [], "entities": []}, {"text": "The AZ categories induced by our model prove more valuable than those of () and those in the gold standard.", "labels": [], "entities": [{"text": "AZ", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9384341835975647}]}, {"text": "Our work demonstrates the great potential of automatically induced declarative knowledge in both improving the performance of information structure analysis and reducing reliance of human supervision.", "labels": [], "entities": [{"text": "information structure analysis", "start_pos": 126, "end_pos": 156, "type": "TASK", "confidence": 0.6812454064687093}]}], "datasetContent": [{"text": "Data and Models We used the full paper corpus earlier employed in () which includes 8171 annotated sentences (with reported inter-annotator agreement: \u03ba = .83) from 50 biomedical journal articles from the cancer risk assessment domain.", "labels": [], "entities": []}, {"text": "One third of this corpus was saved fora development set on which our model was designed and its hyperparameters were tuned (see below).", "labels": [], "entities": []}, {"text": "The corpus is annotated according to the Argumentative Zoning (AZ) scheme () described in. shows the distribution of AZ categories and the total number of sentences in each individual section.", "labels": [], "entities": []}, {"text": "Since section names vary across articles, we grouped similar sections before calculating the statistics (e.g. Materials and Methods sections were grouped under Method).", "labels": [], "entities": []}, {"text": "The table demonstrates that although there is a dominant category in each section (e.g. BKG in Introduction), up to 36.5% of the sentences in each section fall into other categories.", "labels": [], "entities": [{"text": "BKG in Introduction", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.7437262137730917}]}, {"text": "Feature Extraction We used the C&C POS tagger and parser trained on biomedical literature) in the feature extraction process.", "labels": [], "entities": [{"text": "Feature Extraction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7039406597614288}, {"text": "feature extraction", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.8121070563793182}]}, {"text": "Lemmatization was done with Morpha ().", "labels": [], "entities": [{"text": "Lemmatization", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9764941334724426}]}, {"text": "Baselines We compared our models (TopicGC and TopicGE) against the following baselines: (a) an unconstrained unsupervised model -the unbiased version of the graph clustering we use for TopicGC (i.e. where g(\u00b7) is omitted, GC); (b) the unsupervised constrained GE method of ( where the constraints were created by experts; (c) supervised unconstrained Maximum Entropy models, each trained to predict categories in a particular section using 150 sentences from that section, as in the lightly supervised casein () (MaxEnt); and (d) a baseline that assigns all the sentences in a given section to the most frequent gold-standard category of that section.", "labels": [], "entities": []}, {"text": "This baseline emulates the use of section names for information structure classification.", "labels": [], "entities": [{"text": "information structure classification", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.8360247611999512}]}, {"text": "Our constraints, which we use in the TopicGE and TopicGC models, are based on topics that are learned on the test corpus.", "labels": [], "entities": []}, {"text": "While having access to the raw test text at training time is a standard assumption in many unsupervised NLP works (e.g. (), it is important to quantify the extent to which our method depends on its access to the test set.", "labels": [], "entities": []}, {"text": "We therefore constructed the TopicGE* model which is identical to TopicGE except that the topics are learned from another collection of 47 biomedical articles containing 9352 sentences.", "labels": [], "entities": []}, {"text": "Like our test set, these articles are from the cancer risk assessment domain -all of them were published in the Toxicol.", "labels": [], "entities": [{"text": "Toxicol", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9255156517028809}]}, {"text": "Sci. journal in the years 2009-2012 and were retrieved using the PubMed search engine with the key words \"cancer risk assessment\".", "labels": [], "entities": [{"text": "Sci. journal", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.859759658575058}, {"text": "PubMed search engine", "start_pos": 65, "end_pos": 85, "type": "DATASET", "confidence": 0.9685147404670715}]}, {"text": "There is no overlap between this new dataset and our test set ().", "labels": [], "entities": []}, {"text": "Models and Parameters For graph clustering, we used the Graclus software (. For GE and MaxEnt, we used the Mallet software).", "labels": [], "entities": [{"text": "graph clustering", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7311588525772095}, {"text": "GE", "start_pos": 80, "end_pos": 82, "type": "DATASET", "confidence": 0.9576173424720764}]}, {"text": "The \u03b3 parameter in the graph clustering was set to 10 using the development data.", "labels": [], "entities": []}, {"text": "Several values of this parameter in the range of yielded very similar performance.", "labels": [], "entities": []}, {"text": "The number of key features considered for each topic, N , was set to 40, 20 and 15 for the article, Introduction section, and Discussion section topic models, respectively.", "labels": [], "entities": []}, {"text": "This difference reflects the number of feature types) and the text volume of the respective models.", "labels": [], "entities": []}, {"text": "Evaluation We evaluated the overall accuracy as well as the category-level precision, recall and Fscore for each section.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9996291399002075}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9659522175788879}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9997592568397522}, {"text": "Fscore", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9995843768119812}]}, {"text": "TopicGC, TopicGE, TopicGE* and the baseline GC methods are unsuper- Table 4: Performance (class based F1-score and overall accuracy (Acc.)) of unbiased Graph Clustering (GC), Graph Clustering with declarative knowledge learned from topic modeling (TopicGC model, TGC column), Generalized Expectation using constraints learned from topic modeling (TopicGE, TGE) and the same model where constraints are learned using an external set of articles (TopicGE*, TGE*), GE with constraints created by experts (ExpertGE, EGEa replication of) and the most frequent gold standard category of the section vised and therefore induce unlabeled categories.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9475281834602356}, {"text": "accuracy (Acc.))", "start_pos": 123, "end_pos": 139, "type": "METRIC", "confidence": 0.8182906955480576}]}, {"text": "To evaluate their output against the gold standard AZ annotation we first apply a standard greedy manyto-one mapping (naming) scheme in which each induced category is mapped to the gold category that shares the highest number of elements (sentence) with it).", "labels": [], "entities": []}, {"text": "The total number of induced topics was 9 with each topic model inducing three topics.", "labels": [], "entities": []}, {"text": "For light supervision, a ten-fold cross-validation scheme was applied.", "labels": [], "entities": []}, {"text": "In addition, we compare the quality of the automatically induced and manually constructed declarative knowledge in the context of customized summarization () where summaries of specific types of information in an article are to be generated (we focused on the article's conclusions).", "labels": [], "entities": []}, {"text": "While an intuitive solution would be to summarize the Discussion section of a paper, only 63.5% of its sentences belong to the gold standard Conclusion category.", "labels": [], "entities": [{"text": "summarize the Discussion section of a paper", "start_pos": 40, "end_pos": 83, "type": "TASK", "confidence": 0.8359499744006565}]}, {"text": "For our experiment, we first generated five sets of sentences.", "labels": [], "entities": []}, {"text": "The first four sets consist of the article sentences annotated with the CON category according to: TopicGE or TopicGC or ExpertGE or the gold standard annotation.", "labels": [], "entities": []}, {"text": "The fifth set is the Discussion section.", "labels": [], "entities": []}, {"text": "We then used Microsoft AutoSummarize to select sentences from each of the five sets such that the number of words in each summary amounts for 10% of the words in the input.", "labels": [], "entities": [{"text": "Microsoft AutoSummarize", "start_pos": 13, "end_pos": 36, "type": "DATASET", "confidence": 0.8463925123214722}]}, {"text": "For evaluation, we asked an expert to summarize the conclusions of each article in the corpus.", "labels": [], "entities": []}, {"text": "We then evaluated the five summaries against the goldstandard summaries written by the expert in terms of various ROUGE scores).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.9976422190666199}]}], "tableCaptions": [{"text": " Table 3: Distribution of sentences (shown in percentages)  in articles and individual sections in the AZ-annotated  corpus. The total number of sentences in each section  appears in parentheses below the section name.", "labels": [], "entities": [{"text": "AZ-annotated  corpus", "start_pos": 103, "end_pos": 123, "type": "DATASET", "confidence": 0.9818527400493622}]}, {"text": " Table 5: Performance (class based Precision, Recall and F-score as well as overall accuracy (Acc.)) of the TopicGE  model and of an unconstrained MaxEnt model trained with Light supervision (total of 600 sentences -150 training  sentences for each section-level model). The same pattern of results holds when the MaxEnt is trained with up to 2000  sentences (500 sentences for each section-level model).", "labels": [], "entities": [{"text": "Precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9747576117515564}, {"text": "Recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9865949153900146}, {"text": "F-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9929950833320618}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9877762794494629}, {"text": "Acc.", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9881834387779236}]}, {"text": " Table 6: ROUGE scores of zone (TopicGE, TopicGC, ExpertGE or gold standard) and Discussion section based sum- maries. TopicGE provides the best summaries. TopicGC outperforms ExpertGE and the Discussion section systems  and in two measures the gold categorization based system as well. Result patterns with ROUGE(3,4,W-1.2, S* and  SU*) are very similar to those of the table. The differences between TopicGE and ExpertGE are statistically significant  using t-test with p < 0.05. The differences between TopicGE and gold, as well as between ExpertGE and gold are not  statistically significant.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9568900465965271}, {"text": "ROUGE", "start_pos": 308, "end_pos": 313, "type": "METRIC", "confidence": 0.9653279185295105}]}]}