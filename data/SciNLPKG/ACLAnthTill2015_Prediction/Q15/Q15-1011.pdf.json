{"title": [], "abstractContent": [{"text": "Entity disambiguation with Wikipedia relies on structured information from redirect pages, article text, inter-article links, and categories.", "labels": [], "entities": [{"text": "Entity disambiguation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8497719168663025}]}, {"text": "We explore whether web links can replace a curated encyclopaedia, obtaining entity prior, name, context, and coherence models from a corpus of web pages with links to Wiki-pedia.", "labels": [], "entities": []}, {"text": "Experiments compare web link models to Wikipedia models on well-known CoNLL and TAC data sets.", "labels": [], "entities": [{"text": "CoNLL and TAC data sets", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.829948890209198}]}, {"text": "Results show that using 34 million web links approaches Wikipedia performance.", "labels": [], "entities": []}, {"text": "Combining web link and Wikipedia models produces the best-known disambiguation accuracy of 88.7 on standard newswire test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.7854441404342651}, {"text": "newswire test data", "start_pos": 108, "end_pos": 126, "type": "DATASET", "confidence": 0.8024452726046244}]}], "introductionContent": [{"text": "Entity linking (EL) resolves mentions in text to their corresponding node in a knowledge base (KB), or NIL if the entity is not in the KB.", "labels": [], "entities": [{"text": "Entity linking (EL) resolves mentions in text", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8723868396547105}]}, {"text": "Wikipedia and related semantic resources -Freebase, DBpedia, Yago2-have emerged as general repositories of notable entities.", "labels": [], "entities": []}, {"text": "The availability of Wikipedia, in particular, has driven work on EL, knowledge base population (KBP), and semantic search.", "labels": [], "entities": [{"text": "EL", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.7793501615524292}, {"text": "semantic search", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.7871114611625671}]}, {"text": "This literature demonstrates that the rich structure of Wikipediaredirect pages, article text, inter-article links, categories -delivers disambiguation accuracy above 85% on newswire (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9660083651542664}]}, {"text": "But what disambiguation accuracy can we expect in the absence of Wikipedia's curated structure?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9834148287773132}]}, {"text": "Web links provide much of the same information as Wikipedia inter-article links: anchors are used to derive alternative names and conditional probabilities of entities given names; in-link counts are used to derive a simple entity popularity measure; the text surrounding a link is used to derive textual context models; and overlap of in-link sources is used to derive entity cooccurrence models.", "labels": [], "entities": []}, {"text": "On the other hand, web links lack analogues of additional Wikipedia structure commonly used for disambiguation, e.g., categories, encyclopaedic descriptions.", "labels": [], "entities": []}, {"text": "Moreover, Wikipedia's editors ensure a clean and correct knowledge source while web links area potentially noisier annotation source.", "labels": [], "entities": []}, {"text": "We explore linking with web links versus Wikipedia.", "labels": [], "entities": []}, {"text": "Contributions include: (1) anew benchmark linker that instantiates entity prior probabilities, entity given name probabilities, entity context models, and efficient entity coherence models from Wikipedia-derived data sets; (2) an alternative linker that derives the same model using only alternative names and web pages that link to Wikipedia; (3) detailed development experiments, including analysis and profiling of Web link data, and a comparison of link and Wikipedia-derived models.", "labels": [], "entities": []}, {"text": "Results suggest that web link accuracy is at least 93% of a Wikipedia linker and that web links are complementary to Wikipedia, with the best scores coming from a combination.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9933684468269348}]}, {"text": "We argue that these results motivate open publishing of enterprise authorities and suggest that accumulating incoming links should be prioritised at least as highly as adding richer internal structure to an authority.", "labels": [], "entities": []}, {"text": "describe a disambiguation approach that exploits news documents that have been curated by professional editors.", "labels": [], "entities": []}, {"text": "In addition to consistently edited text, these include document-level tags for entities mentioned in the story.", "labels": [], "entities": []}, {"text": "Tags are exploited to build textual mention context, assign weights to alternative names, and train a disambiguator.", "labels": [], "entities": []}, {"text": "This leads to an estimated F 1 score of 78.0 for end-to-end linking to a KB of 32,000 companies.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9903813401858012}]}, {"text": "Our work is similar, but we replace quality curated news text with web pages and explore a larger KB of more than four million entites.", "labels": [], "entities": []}, {"text": "In place of document-level entity tags, hyperlinks pointing to Wikipedia articles are used to build context, name and coherence models.", "labels": [], "entities": []}, {"text": "This is a cheap form of thirdparty entity annotation with the potential for generalisation to any type of web-connected KB.", "labels": [], "entities": []}, {"text": "However, it presents an additional challenge in coping with noise, including prose that lacks editorial oversight and links with anchor text that do not correspond to actual aliases.", "labels": [], "entities": []}, {"text": "explore a similar task setting for microblogs, where short mention contexts exacerbate sparsity problems for underdeveloped entities.", "labels": [], "entities": []}, {"text": "They address the problem by building a topic model based on Wikipedia mention link contexts.", "labels": [], "entities": []}, {"text": "A bootstrapping approach analogous to query expansion augments the model using web pages returned from the Google search API.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7229051887989044}]}, {"text": "Results suggest that the bootstrapping process is beneficial, improving performance from approximately 81% to 87% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9951947331428528}]}, {"text": "We demonstrate that adding link data leads to similar improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report final experiments on the held-out CoNLL and TAC 2010 test sets.", "labels": [], "entities": [{"text": "CoNLL and TAC 2010 test sets", "start_pos": 44, "end_pos": 72, "type": "DATASET", "confidence": 0.8744206130504608}]}, {"text": "As described in Section 3 above, we report p@1 for CoNLL following Hoffart et al.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.854198694229126}]}, {"text": "(2011) and A KB for TAC following.", "labels": [], "entities": [{"text": "A KB", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9419577121734619}, {"text": "TAC", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.6724007725715637}]}, {"text": "We use a reference implementation to compute evaluation measures and pairwise significance ( ).", "labels": [], "entities": []}, {"text": "We bold the superior configuration for each column only if the difference is significant (p < 0.05).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data sets for disambiguation tasks addressed here. Statistics are described in Section 3.", "labels": [], "entities": [{"text": "disambiguation tasks", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.9170781075954437}]}, {"text": " Table 2: p@1 results for individual components on the  CoNLL development data. The first two columns corre- spond to the Wikipedia models described in Section 4.3,  one derived from article text and the other from mention  contexts. The last column corresponds to the web link  models described in Section 5.", "labels": [], "entities": [{"text": "CoNLL development data", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.9258750875790914}]}, {"text": " Table 5: Coverage of textual context models for each  source over entities (E) and mentions (M).", "labels": [], "entities": []}, {"text": " Table 6: Mean in-vocab tokens per entity ( \u00af  t E ) and tokens  per mention ( \u00af  t M ) for each textual context model.", "labels": [], "entities": []}, {"text": " Table 7: Web link components vs. Wikipedia.", "labels": [], "entities": []}, {"text": " Table 8: Web link combinations vs. Wikipedia.", "labels": [], "entities": []}, {"text": " Table 9: Web links complement Wikipedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.9603260159492493}]}, {"text": " Table 10: Comparison to the disambiguation literature.", "labels": [], "entities": []}]}