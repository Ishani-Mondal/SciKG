{"title": [{"text": "Learning Structural Kernels for Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "Structural kernels area flexible learning paradigm that has been widely used in Natural Language Processing.", "labels": [], "entities": []}, {"text": "However, the problem of model selection in kernel-based methods is usually overlooked.", "labels": [], "entities": [{"text": "model selection", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7216236889362335}]}, {"text": "Previous approaches mostly rely on setting default values for kernel hyperparameters or using grid search, which is slow and coarse-grained.", "labels": [], "entities": []}, {"text": "In contrast , Bayesian methods allow efficient model selection by maximizing the evidence on the training data through gradient-based methods.", "labels": [], "entities": []}, {"text": "In this paper we show how to perform this in the context of structural kernels by using Gaussian Processes.", "labels": [], "entities": []}, {"text": "Experimental results on tree kernels show that this procedure results in better prediction performance compared to hyperparameter optimization via grid search.", "labels": [], "entities": []}, {"text": "The framework proposed in this paper can be adapted to other structures besides trees, e.g., strings and graphs, thereby extending the utility of kernel-based methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Kernel-based methods area staple machine learning approach in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.6657408128182093}]}, {"text": "Frequentist kernel methods like the Support Vector Machine (SVM) pushed the state of the art in many NLP tasks, especially classification and regression.", "labels": [], "entities": [{"text": "classification", "start_pos": 123, "end_pos": 137, "type": "TASK", "confidence": 0.9643964767456055}]}, {"text": "One interesting aspect of kernels is their ability to be defined directly on structured objects like strings, trees and graphs.", "labels": [], "entities": []}, {"text": "This approach has the potential to move the modelling effort from feature engineering to kernel engineering.", "labels": [], "entities": [{"text": "kernel engineering", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8624030649662018}]}, {"text": "This is useful when we do not have much prior knowledge about how the data behaves, as we can more readily define a similarity metric between inputs instead of trying to characterize which features are the best for the task at hand.", "labels": [], "entities": []}, {"text": "Kernels area very flexible framework: they can be combined and parameterized in many different ways.", "labels": [], "entities": []}, {"text": "Complex kernels, however, lead to the problem of model selection, where the aim is to obtain the best kernel configuration in terms of hyperparameter values.", "labels": [], "entities": [{"text": "model selection", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7580462992191315}]}, {"text": "The usual approach for model selection in frequentist methods is to employ grid search on some development data disjoint from the training data.", "labels": [], "entities": [{"text": "model selection", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7190116196870804}]}, {"text": "This approach can rapidly become impractical when using complex kernels which increase the number of model hyperparameters.", "labels": [], "entities": []}, {"text": "Grid search also requires the user to explicitly set the grid values, making it difficult to fine tune the hyperparameters.", "labels": [], "entities": [{"text": "Grid search", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.5783871412277222}]}, {"text": "Recent advances in model selection tackle some of these issues, but have several limitations (see \u00a76 for details).", "labels": [], "entities": [{"text": "model selection", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7796898186206818}]}, {"text": "Our proposed approach for model selection relies on Gaussian Processes (GPs)), a widely used Bayesian kernel machine.", "labels": [], "entities": [{"text": "model selection", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7647482454776764}]}, {"text": "GPs allow efficient and fine-grained model selection by maximizing the evidence on the training data using gradient-based methods, dropping the requirement for development data.", "labels": [], "entities": []}, {"text": "As a Bayesian procedure, GPs also naturally balance between model capacity and generalization.", "labels": [], "entities": []}, {"text": "GPs have been shown to achieve state of the art performance in various regression tasks).", "labels": [], "entities": []}, {"text": "Therefore, we base our approach on this framework.", "labels": [], "entities": []}, {"text": "While prediction performance is important to consider (as we show in our experiments), we are mainly interested in two other significant aspects that are enabled by our approach: \u2022 Gradient-based methods are more efficient than grid search for high dimensional spaces.", "labels": [], "entities": []}, {"text": "This allows us to easily propose new rich kernel extensions that rely on a large number of hyperparameters, which in turn can result in better modelling capacity.", "labels": [], "entities": []}, {"text": "\u2022 Since the model selection process is now finegrained, we can interpret the resulting hyperparameter values, depending on how the kernel is defined.", "labels": [], "entities": []}, {"text": "In this work we focus on tree kernels, which have been successfully used in a number of NLP tasks (see \u00a76).", "labels": [], "entities": []}, {"text": "In most cases, these kernels are used as an SVM component and model selection is not considered an important issue.", "labels": [], "entities": [{"text": "model selection", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.7021266520023346}]}, {"text": "Hyperparameters are usually set to default values, which work reasonably well in terms of prediction performance.", "labels": [], "entities": []}, {"text": "However, this is only possible due to the small number of hyperparameters these kernels contain.", "labels": [], "entities": []}, {"text": "We perform experiments comprising synthetic data ( \u00a74) and two real NLP regression tasks: Emotion Analysis ( \u00a75.1) and Translation Quality Estimation ( \u00a75.2).", "labels": [], "entities": [{"text": "Emotion Analysis", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.7517350614070892}, {"text": "Translation Quality Estimation", "start_pos": 119, "end_pos": 149, "type": "TASK", "confidence": 0.8190948963165283}]}, {"text": "Our findings show that our approach outperforms SVMs using the same kernels.", "labels": [], "entities": []}], "datasetContent": [{"text": "A natural question that arises in the proposed method is how much data is needed to accurately learn the kernel hyperparameters.", "labels": [], "entities": []}, {"text": "To answer this question, we run a set of experiments using synthetic data.", "labels": [], "entities": []}, {"text": "We generate this data by using a set of 1000 natural language syntactic trees, where we fix a random subset of 200 instances for testing and use the remaining 800 instances as training.", "labels": [], "entities": []}, {"text": "For each training set size we define a GP over the full dataset, sample a function from it and use the function output as the response variable for each tree.", "labels": [], "entities": []}, {"text": "We try two different GP priors, one using the SSTK and another one using the SASSTK.", "labels": [], "entities": [{"text": "SSTK", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.743699848651886}, {"text": "SASSTK", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.5350015163421631}]}, {"text": "The conditions above provide a controlled environment to check the modelling capacities of our approach since we know the exact distribution where the data comes from.", "labels": [], "entities": []}, {"text": "The reasoning behind these experiments is that to be able to provide benefits in real tasks, where the data distribution is not known, our models have to be learnable in this controlled setting as well using a reasonable amount of data.", "labels": [], "entities": []}, {"text": "Finally, we also provide an empirical evaluation comparing the speed performance between our approach and grid search.", "labels": [], "entities": []}, {"text": "To provide an overview of how efficient is the gradient-based method compared to grid search we also run a set of experiments measuring wall clock training time vs. RMSE on a test set.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.9104331135749817}]}, {"text": "For both GP and SVM models we employ the SSTK as the kernel and we use the same synthetic data from the previous experiments . We perform 20 runs, keeping the test set as the same 200 instances for all runs and randomly sampling 200 instances from the remaining instances as training data.", "labels": [], "entities": []}, {"text": "shows the curves for both GP and SVM models.", "labels": [], "entities": []}, {"text": "The GP curve is obtained by increasing the maximum number of iterations of the gradient-based method (in this case, L-BFGS) and the SVM curve is obtained by increasing the granularity of the grid size.", "labels": [], "entities": []}, {"text": "We can see that optimizing the GP model is consistently much faster than doing grid search on the SVM model (notice the logarithmic scale), even though it shows some variance when letting L-BFGS run fora larger number of iterations.", "labels": [], "entities": []}, {"text": "The GP model also is able to better predictions in general.", "labels": [], "entities": []}, {"text": "Even when taking the variances into account, grid search would still need around 10 times more computation time to achieve the same predictions obtained by the GP model.", "labels": [], "entities": [{"text": "grid search", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.9401730298995972}]}, {"text": "In real settings, SVMs predictions tend to be more on par with the ones provided by a GP (as shown in \u00a75) but nevertheless these figures show that the GP can be much more time efficient when optimizing hyperparameters of a tree kernel.", "labels": [], "entities": []}, {"text": "An important performance aspect to take into account is parallelization.", "labels": [], "entities": [{"text": "parallelization", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.9526382088661194}]}, {"text": "Grid search is embarassingly parallelizable since each grid point can run in a different core.", "labels": [], "entities": [{"text": "Grid search", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6648154854774475}]}, {"text": "However, the GP optimization can also benefit from multiple cores by running each kernel computation inside the Gram matrix in parallel.", "labels": [], "entities": [{"text": "GP optimization", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.8870676755905151}]}, {"text": "To keep the comparisons simpler, the results shown in this section use a single core but all experiments in \u00a75 employ parallelization in the Gram matrix computation level (for both SVM and GP models).", "labels": [], "entities": []}, {"text": "Our experiments with NLP data address two regression tasks: Emotion Analysis and Quality Estimation.", "labels": [], "entities": [{"text": "Emotion Analysis", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.8791261315345764}]}, {"text": "For both tasks, we use the Stanford parser () to obtain constituency trees for all sentences.", "labels": [], "entities": []}, {"text": "Also, rather than using data official splits, we perform 5-fold cross-validation in order to obtain more reliable results.", "labels": [], "entities": []}], "tableCaptions": []}