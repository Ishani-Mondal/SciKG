{"title": [{"text": "A Hierarchical Distance-dependent Bayesian Model for Event Coreference Resolution", "labels": [], "entities": [{"text": "Event Coreference Resolution", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.8540810545285543}]}], "abstractContent": [{"text": "We present a novel hierarchical distance-dependent Bayesian model for event coref-erence resolution.", "labels": [], "entities": [{"text": "event coref-erence resolution", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.5895415345827738}]}, {"text": "While existing generative models for event coreference resolution are completely unsupervised, our model allows for the incorporation of pairwise distances between event mentions-information that is widely used in supervised coreference models to guide the generative clustering processing for better event clustering both within and across documents.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.8083861470222473}, {"text": "generative clustering processing", "start_pos": 257, "end_pos": 289, "type": "TASK", "confidence": 0.9256985584894816}, {"text": "event clustering", "start_pos": 301, "end_pos": 317, "type": "TASK", "confidence": 0.726588636636734}]}, {"text": "We model the distances between event mentions using a feature-rich learnable distance function and encode them as Bayesian priors for nonparametric clustering.", "labels": [], "entities": []}, {"text": "Experiments on the ECB+ corpus show that our model outperforms state-of-the-art methods for both within-and cross-document event coreference resolution.", "labels": [], "entities": [{"text": "ECB+ corpus", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.9412952462832133}, {"text": "cross-document event coreference resolution", "start_pos": 108, "end_pos": 151, "type": "TASK", "confidence": 0.7038581445813179}]}], "introductionContent": [{"text": "The task of event coreference resolution consists of identifying text snippets that describe events, and then clustering them such that all event mentions in the same partition refer to the same unique event.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.8181504805882772}]}, {"text": "Event coreference resolution can be applied within a single document or across multiple documents and is crucial for many natural language processing tasks including topic detection and tracking, information extraction, question answering and textual entailment.", "labels": [], "entities": [{"text": "Event coreference resolution", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7970665494600931}, {"text": "topic detection and tracking", "start_pos": 166, "end_pos": 194, "type": "TASK", "confidence": 0.866911306977272}, {"text": "information extraction", "start_pos": 196, "end_pos": 218, "type": "TASK", "confidence": 0.825135201215744}, {"text": "question answering", "start_pos": 220, "end_pos": 238, "type": "TASK", "confidence": 0.8965882658958435}, {"text": "textual entailment", "start_pos": 243, "end_pos": 261, "type": "TASK", "confidence": 0.7238247990608215}]}, {"text": "More importantly, event coreference resolution is a necessary component in any reasonable, broadly applicable computational model of natural language understanding ().", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.8672102491060892}, {"text": "natural language understanding", "start_pos": 133, "end_pos": 163, "type": "TASK", "confidence": 0.6450754006703695}]}, {"text": "In comparison to entity coreference resolution, which deals with identifying and grouping noun phrases that refer to the same discourse entity, event coreference resolution has not been extensively studied.", "labels": [], "entities": [{"text": "entity coreference resolution", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.7061565717061361}, {"text": "identifying and grouping noun phrases that refer to the same discourse entity", "start_pos": 65, "end_pos": 142, "type": "TASK", "confidence": 0.7083431432644526}, {"text": "event coreference resolution", "start_pos": 144, "end_pos": 172, "type": "TASK", "confidence": 0.7994990348815918}]}, {"text": "This is, in part, because events typically exhibit a more complex structure than entities: a single event can be described via multiple event mentions, and a single event mention can be associated with multiple event arguments that characterize the participants in the event as well as spatio-temporal information.", "labels": [], "entities": []}, {"text": "Hence, the coreference decisions for event mentions usually require the interpretation of event mentions and their arguments in context.", "labels": [], "entities": []}, {"text": "See, for example,, in which five event mentions across two documents all refer to the same underlying event: Plane bombs Yida camp.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments using the ECB+ corpus (, the largest available dataset with annotations of both withindocument (WD) and cross-document (CD) event coreference resolution.", "labels": [], "entities": [{"text": "ECB+ corpus", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.9454759558041891}, {"text": "cross-document (CD) event coreference resolution", "start_pos": 127, "end_pos": 175, "type": "TASK", "confidence": 0.6769008253301892}]}, {"text": "It extends ECB 0.1 () and ECB by adding event argument and argument type annotations as well as adding more news documents.", "labels": [], "entities": [{"text": "ECB 0.1", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.8935272097587585}, {"text": "ECB", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.8902832865715027}]}, {"text": "The cross-document coreference annotations only exist in documents that describe the same seminal event (the event that triggers the topic of the document and has interconnections with the majority of events from its surrounding textual context).", "labels": [], "entities": []}, {"text": "We divide the dataset into a training set (topics 1-20), a development set (topics 21-23), and a test set (topics 24-43).", "labels": [], "entities": []}, {"text": "shows the statistics of the data.", "labels": [], "entities": []}, {"text": "We performed event coreference resolution on all possible event mentions that are expressed in the documents.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.8396680951118469}]}, {"text": "Using the event extraction method described in Section 4, we extracted 53,429 event mentions, 43,682 participant mentions, 5,791 time mentions and 3,836 location mentions in the test data, covering 93.5%, 89.0%, 95.0%, 72.8% of the annotated event mentions, participants, time and locations, respectively.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.688699260354042}]}, {"text": "We evaluate both within-and cross-document event coreference resolution.", "labels": [], "entities": [{"text": "cross-document event coreference resolution", "start_pos": 28, "end_pos": 71, "type": "TASK", "confidence": 0.668006420135498}]}, {"text": "As in previous work, we evaluate cross-document coreference resolution by merging all documents from the same seminal event into a meta-document and then evaluate the metadocument as in within-document coreference resolution.", "labels": [], "entities": [{"text": "cross-document coreference resolution", "start_pos": 33, "end_pos": 70, "type": "TASK", "confidence": 0.750175674756368}, {"text": "coreference resolution", "start_pos": 202, "end_pos": 224, "type": "TASK", "confidence": 0.7588172256946564}]}, {"text": "However, during inference time, we do not assume the knowledge of the mapping of documents to seminal events.", "labels": [], "entities": []}, {"text": "We consider three widely used coreference resolution metrics: (1) MUC (, which measures how many gold (predicted) cluster merging operations are needed to recover each predicted (gold) cluster; (2) B 3 (Bagga and Baldwin, 1998), which measures the proportion of overlap between the predicted and gold clusters for each mention and computes the average scores; and (3) CEAF () (CEAF e ), which measures the best alignment of the gold-standard and predicted clusters.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.9016829431056976}, {"text": "MUC", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9327244162559509}, {"text": "B 3", "start_pos": 198, "end_pos": 201, "type": "METRIC", "confidence": 0.9577964544296265}, {"text": "CEAF", "start_pos": 368, "end_pos": 372, "type": "METRIC", "confidence": 0.8124070167541504}]}, {"text": "We also consider the CoNLL F1, which is the average F1 of the above three measures.", "labels": [], "entities": [{"text": "CoNLL F1", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.6791335344314575}, {"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9911054968833923}]}, {"text": "All the scores are computed using the latest version (v8.01) of the official CoNLL scorer ().", "labels": [], "entities": [{"text": "CoNLL scorer", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.8068112432956696}]}], "tableCaptions": [{"text": " Table 2: Statistics of the ECB+ corpus", "labels": [], "entities": [{"text": "ECB+ corpus", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.9164614280064901}]}, {"text": " Table 3: Within-and cross-document coreference results on the ECB+ corpus", "labels": [], "entities": [{"text": "cross-document coreference", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.6577456891536713}, {"text": "ECB+ corpus", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.9641012946764628}]}, {"text": " Table 4: Learned weights for selected features", "labels": [], "entities": []}]}