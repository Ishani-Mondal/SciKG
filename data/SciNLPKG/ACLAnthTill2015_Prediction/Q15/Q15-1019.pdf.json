{"title": [{"text": "Learning a Compositional Semantics for Freebase with an Open Predicate Vocabulary", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an approach to learning a model-theoretic semantics for natural language tied to Freebase.", "labels": [], "entities": []}, {"text": "Crucially, our approach uses an open predicate vocabulary, enabling it to produce denotations for phrases such as \"Re-publican front-runner from Texas\" whose semantics cannot be represented using the Free-base schema.", "labels": [], "entities": []}, {"text": "Our approach directly converts a sentence's syntactic CCG parse into a logical form containing predicates derived from the words in the sentence, assigning each word a consistent semantics across sentences.", "labels": [], "entities": []}, {"text": "This logical form is evaluated against a learned probabilistic database that defines a distribution over denotations for each textual predicate.", "labels": [], "entities": []}, {"text": "A training phase produces this prob-abilistic database using a corpus of entity-linked text and probabilistic matrix factoriza-tion with a novel ranking objective function.", "labels": [], "entities": []}, {"text": "We evaluate our approach on a compositional question answering task where it outperforms several competitive baselines.", "labels": [], "entities": [{"text": "compositional question answering task", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.7483824491500854}]}, {"text": "We also compare our approach against manually annotated Freebase queries, finding that our open predicate vocabulary enables us to answer many questions that Freebase cannot.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditional knowledge representation assumes that world knowledge can be encoded using a closed vocabulary of formal predicates.", "labels": [], "entities": [{"text": "knowledge representation", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.7632422745227814}]}, {"text": "In recent years, semantic parsing has enabled us to build compositional models of natural language semantics using such a closed predicate vocabulary).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7416320145130157}]}, {"text": "These semantic parsers map natural language statements to database queries, enabling applications such as answering questions using a large knowledge base (.", "labels": [], "entities": []}, {"text": "Furthermore, the modeltheoretic semantics provided by such parsers have the potential to improve performance on other tasks, such as information extraction and coreference resolution.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.8366268575191498}, {"text": "coreference resolution", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.9476138353347778}]}, {"text": "However, a closed predicate vocabulary has inherent limitations.", "labels": [], "entities": []}, {"text": "First, its coverage will be limited, as such vocabularies are typically manually constructed.", "labels": [], "entities": []}, {"text": "Second, it may abstract away potentially relevant semantic differences.", "labels": [], "entities": []}, {"text": "For example, the semantics of \"Republican front-runner\" cannot be adequately encoded in the Freebase schema because it lacks the concept of a \"front-runner.\"", "labels": [], "entities": [{"text": "Freebase schema", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.970399796962738}]}, {"text": "We could choose to encode this concept as \"politician\" at the cost of abstracting away the distinction between the two.", "labels": [], "entities": []}, {"text": "As this example illustrates, these two problems are prevalent in even the largest knowledge bases.", "labels": [], "entities": []}, {"text": "An alternative paradigm is an open predicate vocabulary, where each natural language word or phrase is given its own formal predicate.", "labels": [], "entities": []}, {"text": "This paradigm is embodied in both open information extraction () and universal schema ( . Open predicate vocabularies have the potential to capture subtle semantic distinctions and achieve high coverage.", "labels": [], "entities": [{"text": "open information extraction", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6782007316748301}]}, {"text": "However, we have yet to develop compelling approaches to compositional semantics within this paradigm.", "labels": [], "entities": []}, {"text": "This paper takes a step toward compositional se- Figure 1: Overview of our approach.", "labels": [], "entities": [{"text": "compositional se", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.8935990035533905}]}, {"text": "Top left: the text is converted to logical form by CCG syntactic parsing and a collection of manually-defined rules.", "labels": [], "entities": [{"text": "CCG syntactic parsing", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.7957416772842407}]}, {"text": "Bottom: low-dimensional embeddings of each entity (entity pair) and category (relation) are learned from an entity-linked web corpus.", "labels": [], "entities": []}, {"text": "These embeddings are used to construct a probabilistic database.", "labels": [], "entities": []}, {"text": "The labels of these matrices are shortened for space reasons.", "labels": [], "entities": []}, {"text": "Top right: evaluating the logical form on the probabilistic database computes the marginal probability that each entity is an element of the text's denotation.", "labels": [], "entities": []}, {"text": "mantics with an open predicate vocabulary.", "labels": [], "entities": []}, {"text": "Our approach defines a distribution over denotations (sets of Freebase entities) given an input text.", "labels": [], "entities": []}, {"text": "The model has two components, shown in.", "labels": [], "entities": []}, {"text": "The first component is a rule-based semantic parser that uses a syntactic CCG parser and manually-defined rules to map entity-linked texts to logical forms containing predicates derived from the words in the text.", "labels": [], "entities": []}, {"text": "The second component is a probabilistic database with a possible worlds semantics that defines a distribution over denotations for each textually-derived predicate.", "labels": [], "entities": []}, {"text": "This database assigns independent probabilities to individual predicate instances, such as P (FRONT-RUNNER(/EN/GEORGE BUSH)) = 0.9.", "labels": [], "entities": [{"text": "FRONT-RUNNER(/EN/GEORGE BUSH))", "start_pos": 94, "end_pos": 124, "type": "METRIC", "confidence": 0.7960286566189357}]}, {"text": "Together, these components define an exponentiallylarge distribution over denotations for an input text; to simplify this output, we compute the marginal probability, overall possible worlds, that each entity is an element of the text's denotation.", "labels": [], "entities": []}, {"text": "The learning problem in our approach is to train the probabilistic database to predict a denotation for each predicate.", "labels": [], "entities": []}, {"text": "We pose this problem as probabilistic matrix factorization with a novel query/answer ranking objective.", "labels": [], "entities": []}, {"text": "This factorization learns a lowdimensional embedding of each entity (entity pair) and category (relation) such that the denotation of a predicate is likely to contain entities or entity pairs with nearby vectors.", "labels": [], "entities": []}, {"text": "To train the database, we first collect training data by analyzing entity-linked sentences in a large web corpus with the rule-based semantic parser.", "labels": [], "entities": []}, {"text": "This process generates a collection of logical form queries with observed entity answers.", "labels": [], "entities": []}, {"text": "The query/answer ranking objective, when optimized, trains the database to rank the observed answers for each query above unobserved answers.", "labels": [], "entities": []}, {"text": "We evaluate our approach on a question answering task, finding that our approach outperforms several baselines and that our new training objective improves performance over a previously-proposed objective.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.8111046353975931}]}, {"text": "We also evaluate the trade-offs between open and closed predicate vocabularies by comparing our approach to a manually-annotated Freebase query for each question.", "labels": [], "entities": []}, {"text": "This comparison reveals that, when Freebase contains predicates that cover the question, it achieves higher precision and recall than our approach.", "labels": [], "entities": [{"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9990992546081543}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9991520643234253}]}, {"text": "However, our approach can correctly answer many questions not covered by Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.9748371243476868}]}], "datasetContent": [{"text": "We evaluate our approach to compositional semantics on a question answering task.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.7946511308352152}]}, {"text": "Each test example is a (compositional) natural language question whose answer is a set of Freebase entities.", "labels": [], "entities": []}, {"text": "We compare our open domain approach to several baselines based on prior work, as well as a human-annotated Freebase query for each example.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the test data set.", "labels": [], "entities": [{"text": "test data set", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.8497194051742554}]}, {"text": " Table 2: Mean average precision for our question  answering task. The difference in MAP between  each pair of adjacent models is statistically signifi- cant (p < .05) via the sign test.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.5008332133293152}, {"text": "question  answering task", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.8708539009094238}, {"text": "MAP", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9901022911071777}]}, {"text": " Table 3: Statistics of the Freebase MQL queries an- notated for the test data set.", "labels": [], "entities": [{"text": "Freebase MQL queries", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.8547693093617758}]}]}