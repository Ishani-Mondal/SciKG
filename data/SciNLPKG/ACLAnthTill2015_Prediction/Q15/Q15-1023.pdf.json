{"title": [], "abstractContent": [{"text": "Recent research on entity linking (EL) has introduced a plethora of promising techniques, ranging from deep neural networks to joint inference.", "labels": [], "entities": [{"text": "entity linking (EL)", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.8735316872596741}]}, {"text": "But despite numerous papers there is surprisingly little understanding of the state of the art in EL.", "labels": [], "entities": [{"text": "EL", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.8310006856918335}]}, {"text": "We attack this confusion by analyzing differences between several versions of the EL problem and presenting a simple yet effective, modular, unsupervised system, called VINCULUM, for entity linking.", "labels": [], "entities": [{"text": "EL problem", "start_pos": 82, "end_pos": 92, "type": "TASK", "confidence": 0.7572363913059235}, {"text": "entity linking", "start_pos": 183, "end_pos": 197, "type": "TASK", "confidence": 0.7727876007556915}]}, {"text": "We conduct an extensive evaluation on nine data sets, comparing VINCULUM with two state-of-the-art systems, and elucidate key aspects of the system that include mention extraction, candidate generation, entity type prediction, entity coreference, and coherence.", "labels": [], "entities": [{"text": "mention extraction", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.7220714092254639}, {"text": "candidate generation", "start_pos": 181, "end_pos": 201, "type": "TASK", "confidence": 0.8345259428024292}, {"text": "entity type prediction", "start_pos": 203, "end_pos": 225, "type": "TASK", "confidence": 0.6807960271835327}, {"text": "entity coreference", "start_pos": 227, "end_pos": 245, "type": "TASK", "confidence": 0.6893879473209381}]}], "introductionContent": [{"text": "Entity Linking (EL) is a central task in information extraction -given a textual passage, identify entity mentions (substrings corresponding to world entities) and link them to the corresponding entry in a given Knowledge Base (KB, e.g. Wikipedia or Freebase).", "labels": [], "entities": [{"text": "Entity Linking (EL)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8366185307502747}, {"text": "information extraction", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7310690879821777}]}, {"text": "For example, JetBlue begins direct service between Barnstable Airport and JFK International.", "labels": [], "entities": [{"text": "JetBlue", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9135904908180237}, {"text": "JFK International", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.9081753194332123}]}, {"text": "Here, \"JetBlue\" should be linked to the entity KB:JetBlue, \"Barnstable Airport\" to KB:Barnstable Municipal Airport, and \"JFK International\" to KB:John F. Kennedy International Airport 1 . The links not only provide semantic annotations to human readers but also a machine-consumable representation of the most basic semantic knowledge in the text.", "labels": [], "entities": [{"text": "JetBlue", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.8985772132873535}, {"text": "JFK International\" to KB:John F. Kennedy International Airport 1", "start_pos": 121, "end_pos": 185, "type": "DATASET", "confidence": 0.7123643681406975}]}, {"text": "Many other NLP applications can benefit from such links, such as distantly-supervised relation extraction) that uses EL to create training data, and some coreference systems that use EL for disambiguation).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.751290500164032}]}, {"text": "Unfortunately, in spite of numerous papers on the topic and several published data sets, there is surprisingly little understanding about state-of-the-art performance.", "labels": [], "entities": []}, {"text": "We argue that there are three reasons for this confusion.", "labels": [], "entities": []}, {"text": "First, there is no standard definition of the problem.", "labels": [], "entities": []}, {"text": "A few variants have been studied in the literature, such as Wikification () which aims at linking noun phrases to Wikipedia entities and Named Entity Linking (aka Named Entity Disambiguation)) which targets only named entities.", "labels": [], "entities": [{"text": "Named Entity Linking (aka Named Entity Disambiguation))", "start_pos": 137, "end_pos": 192, "type": "TASK", "confidence": 0.790339900387658}]}, {"text": "Here we use the term Entity Linking as a unified name for both problems, and Named Entity Linking (NEL) for the subproblem of linking only named entities.", "labels": [], "entities": [{"text": "Named Entity Linking (NEL)", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.789481540520986}]}, {"text": "But names are just one part of the problem.", "labels": [], "entities": []}, {"text": "For many variants there are no annotation guidelines for scoring links.", "labels": [], "entities": []}, {"text": "What types of entities are valid targets?", "labels": [], "entities": []}, {"text": "When multiple entities are plausible for annotating a mention, which one should be chosen?", "labels": [], "entities": []}, {"text": "Without agreement on these issues, a fair comparison is elusive.", "labels": [], "entities": []}, {"text": "Secondly, it is almost impossible to assess approaches, because systems are rarely compared using the same data sets.", "labels": [], "entities": []}, {"text": "For instance, developed anew data set (AIDA) based on the CoNLL 2003 Named Entity Recognition data set but failed to evaluate their system on MSNBC previously created by;) compared to the authors' previous system) using the originally selected datasets but didn't evaluate using AIDA data.", "labels": [], "entities": [{"text": "CoNLL 2003 Named Entity Recognition data set", "start_pos": 58, "end_pos": 102, "type": "DATASET", "confidence": 0.9391052297183445}, {"text": "AIDA data", "start_pos": 279, "end_pos": 288, "type": "DATASET", "confidence": 0.8361236751079559}]}, {"text": "Finally, when two end-to-end systems are compared, it is rarely clear which aspect of a system makes one better than the other.", "labels": [], "entities": []}, {"text": "This is especially problematic when authors introduce complex mechanisms or nondeterministic methods that involve learning-based reranking or joint inference.", "labels": [], "entities": []}, {"text": "To address these problems, we analyze several significant inconsistencies among the data sets.", "labels": [], "entities": []}, {"text": "To have a better understanding of the importance of various techniques, we develop a simple and modular, unsupervised EL system, VINCULUM.", "labels": [], "entities": [{"text": "VINCULUM", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.48854199051856995}]}, {"text": "We compare VINCULUM to the two leading sophisticated EL systems on a comprehensive set of nine datasets.", "labels": [], "entities": [{"text": "VINCULUM", "start_pos": 11, "end_pos": 19, "type": "TASK", "confidence": 0.4564390182495117}]}, {"text": "While our system does not consistently outperform the best EL system, it does come remarkably close and serves as a simple and competitive baseline for future research.", "labels": [], "entities": []}, {"text": "Furthermore, we carryout an extensive ablation analysis, whose results illustrate 1) even a near-trivial model using CrossWikis) performs surprisingly well, and 2) incorporating a fine-grained set of entity types raises that level even higher.", "labels": [], "entities": []}, {"text": "In summary, we make the following contributions: \u2022 We analyze the differences among several versions of the entity linking problem, compare existing data sets and discuss annotation inconsistencies between them.", "labels": [], "entities": [{"text": "entity linking problem", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.7934455573558807}]}, {"text": "(Sections 2 & 3) \u2022 We present a simple yet effective, modular, unsupervised system, VINCULUM, for entity linking.", "labels": [], "entities": [{"text": "VINCULUM", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.8518511652946472}, {"text": "entity linking", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.7793483436107635}]}, {"text": "We make the implementation open source and publicly available for future research.", "labels": [], "entities": []}, {"text": "2 (Section 4) \u2022 We compare VINCULUM to 2 state-of-the-art systems on an extensive evaluation of 9 data sets.", "labels": [], "entities": [{"text": "VINCULUM", "start_pos": 27, "end_pos": 35, "type": "TASK", "confidence": 0.6310554146766663}]}, {"text": "We also investigate several key aspects of the system including mention extraction, candidate generation, entity type prediction, entity coreference, and coherence between entities.", "labels": [], "entities": [{"text": "mention extraction", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7436748743057251}, {"text": "candidate generation", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.7961108982563019}, {"text": "entity type prediction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.6648471852143606}, {"text": "entity coreference", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.691738098859787}]}], "datasetContent": [{"text": "While a variety of metrics have been used for evaluation, there is little agreement on which one to use.", "labels": [], "entities": []}, {"text": "However, this detail is quite important, since the choice of metric strongly biases the results.", "labels": [], "entities": []}, {"text": "We describe the most common metrics below.", "labels": [], "entities": []}, {"text": "Bag-of-Concept F1 (ACE, MSNBC): For each document, a gold bag of Wikipedia entities is evaluated against a bag of system output entities requiring exact segmentation match.", "labels": [], "entities": [{"text": "Bag-of-Concept F1 (ACE, MSNBC)", "start_pos": 0, "end_pos": 30, "type": "DATASET", "confidence": 0.5120891758373806}]}, {"text": "This metric may have its historical reason for comparison but is in fact flawed since it will obtain 100% F1 for an annotation in which every mention is linked to the wrong entity, but the bag of entities is the same as the gold bag.", "labels": [], "entities": [{"text": "F1", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.9996052384376526}]}, {"text": "Micro Accuracy (TAC09, TAC10, TAC10T): For a list of given mentions, the metric simply measures the percentage of correctly predicted links.", "labels": [], "entities": [{"text": "Micro Accuracy", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.6142843663692474}, {"text": "TAC10", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.5776574611663818}, {"text": "TAC10T)", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.8946541249752045}]}, {"text": "TAC-KBP B 3 + F1 (TAC11, TAC12): The mentions that are predicted as NIL entities are required to be clustered according to their identities (NIL clustering).", "labels": [], "entities": [{"text": "TAC-KBP B 3 + F1", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.8346636652946472}]}, {"text": "The overall data set is evaluated using a entity cluster-based B 3 + F1.", "labels": [], "entities": []}, {"text": "NER-style F1 (AIDA): Similar to official CoNLL NER F1 evaluation, a link is considered correct only if the mention matches the gold boundary and the linked entity is also correct.", "labels": [], "entities": [{"text": "NER-style F1 (AIDA)", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.7753715872764587}, {"text": "CoNLL NER F1 evaluation", "start_pos": 41, "end_pos": 64, "type": "DATASET", "confidence": 0.8470703810453415}]}, {"text": "A wrong link with the correct boundary penalizes both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9996169805526733}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9987218976020813}]}, {"text": "We note that Bag-of-Concept F1 is equivalent to the measure for Concept-to-Wikipedia task proposed in ( and NER-style F1 is the same as strong annotation match.", "labels": [], "entities": [{"text": "Bag-of-Concept F1", "start_pos": 13, "end_pos": 30, "type": "METRIC", "confidence": 0.5509513765573502}]}, {"text": "In the experiments, we use the official metrics for the TAC data sets and NER-style F1 for the rest.", "labels": [], "entities": [{"text": "TAC data sets", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9657977024714152}, {"text": "NER-style", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.5825973153114319}, {"text": "F1", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.6899151802062988}]}, {"text": "In this section, we present experiments to address the following questions: \u2022 Is NER sufficient to identify mentions?", "labels": [], "entities": [{"text": "NER", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.7974399924278259}]}, {"text": "(Sec. 5.1) \u2022 How much does candidate generation affect final EL performance?", "labels": [], "entities": []}, {"text": "(Sec. 5.2) \u2022 How much does entity type prediction help EL?", "labels": [], "entities": [{"text": "entity type prediction", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7331733902295431}, {"text": "EL", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9619830846786499}]}, {"text": "What typeset is most appropriate?", "labels": [], "entities": []}, {"text": "(Sec. 5.3) \u2022 How much does coherence improve the EL results?", "labels": [], "entities": [{"text": "EL", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.5154903531074524}]}, {"text": "(Sec. 5.4) \u2022 How well does VINCULUM perform compared to the state-of-the-art?", "labels": [], "entities": [{"text": "VINCULUM", "start_pos": 27, "end_pos": 35, "type": "TASK", "confidence": 0.8526110053062439}]}, {"text": "(Sec. 5.5) \u2022 Finally, which of VINCULUM's components contribute the most to its performance?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Characteristics of the nine NEL data sets. Entity types: The AIDA data sets include named entities in four NER classes,", "labels": [], "entities": [{"text": "NEL data sets", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.8695191144943237}, {"text": "AIDA data sets", "start_pos": 71, "end_pos": 85, "type": "DATASET", "confidence": 0.9144302010536194}]}, {"text": " Table 3: Performance(%, R: Recall; P: Precision) of  the correct mentions using different mention extraction  strategies. ACE and MSNBC only annotate a subset of all  the mentions and therefore the absolute values of precision  are largely underestimated.", "labels": [], "entities": [{"text": "Recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.6396939754486084}, {"text": "Precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.6306199431419373}, {"text": "ACE", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.9526847004890442}, {"text": "MSNBC", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.9178226590156555}, {"text": "precision", "start_pos": 218, "end_pos": 227, "type": "METRIC", "confidence": 0.9988614320755005}]}, {"text": " Table 4: Performance (%) after incorporating entity types, comparing two sets of entity types (NER and FIGER).  Using a set of fine-grained entity types (FIGER) generally achieves better results.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.9852220416069031}]}, {"text": " Table 5: Performance (%) after re-ranking candidates using coherence scores, comparing two coherence measures  (NGD and REL). \"no COH\": no coherence based re-ranking is used. \"+BOTH\": an average of two scores is used for  re-ranking. Coherence in general helps: a combination of both measures often achieves the best effect and NGD has a  slight advantage over REL.", "labels": [], "entities": [{"text": "REL", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9433814287185669}, {"text": "BOTH", "start_pos": 178, "end_pos": 182, "type": "METRIC", "confidence": 0.9966095685958862}, {"text": "REL", "start_pos": 362, "end_pos": 365, "type": "METRIC", "confidence": 0.9622880816459656}]}, {"text": " Table 6: End-to-end performance (%): We compare VINCULUM in different stages with two state-of-the-art systems,  AIDA and WIKIFIER. The column \"Overall\" lists the average performance of nine data sets for each approach.  CrossWikis appears to be a strong baseline. VINCULUM is 0.6% shy from WIKIFIER, each winning in four data sets;  AIDA tops both VINCULUM and WIKIFIER on AIDA-test.", "labels": [], "entities": []}, {"text": " Table 8: We divide linking errors into six error categories and provide an example for each class.", "labels": [], "entities": []}, {"text": " Table 9: Error analysis: We analyze a random sample of 250 of VINCULUM's errors, categorize the errors into six  classes, and display the frequencies of each type across the nine datasets.", "labels": [], "entities": [{"text": "Error analysis", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8316677212715149}]}]}