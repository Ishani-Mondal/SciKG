{"title": [{"text": "Implicitly Intersecting Weighted Automata using Dual Decomposition *", "labels": [], "entities": [{"text": "Implicitly Intersecting Weighted Automata", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8525252938270569}]}], "abstractContent": [{"text": "We propose an algorithm to find the best path through an intersection of arbitrarily many weighted automata, without actually performing the intersection.", "labels": [], "entities": []}, {"text": "The algorithm is based on dual decomposition: the automata attempt to agree on a string by communicating about features of the string.", "labels": [], "entities": []}, {"text": "We demonstrate the algorithm on the Steiner consensus string problem, both on synthetic data and on consensus decoding for speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7655256390571594}]}, {"text": "This involves implicitly intersecting up to 100 automata.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many tasks in natural language processing involve functions that assign scores-such as logprobabilities-to candidate strings or sequences.", "labels": [], "entities": []}, {"text": "Often such a function can be represented compactly as a weighted finite state automaton (WFSA).", "labels": [], "entities": []}, {"text": "Finding the best-scoring string according to a WFSA is straightforward using standard best-path algorithms.", "labels": [], "entities": [{"text": "WFSA", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.7619775533676147}]}, {"text": "It is common to construct a scoring WFSA by combining two or more simpler WFSAs, taking advantage of the closure properties of WFSAs.", "labels": [], "entities": []}, {"text": "For example, consider noisy channel approaches to speech recognition) or machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8193250894546509}, {"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8059134781360626}]}, {"text": "Given an input f , the score of a possible English transcription or translation e is the sum of its language model score log p(e) and its channel model score log p(f | e).", "labels": [], "entities": [{"text": "English transcription or translation e", "start_pos": 43, "end_pos": 81, "type": "TASK", "confidence": 0.709783959388733}]}, {"text": "If each of these functions of e is represented as a WFSA, then their sum is represented as the intersection of those two WFSAs.", "labels": [], "entities": []}, {"text": "WFSA intersection corresponds to constraint conjunction, and hence is often a mathematically natural way to specify a solution to a problem involving * The authors are grateful to Damianos Karakos for providing tools and data for the ASR experiments.", "labels": [], "entities": [{"text": "WFSA intersection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.616992399096489}, {"text": "ASR", "start_pos": 234, "end_pos": 237, "type": "TASK", "confidence": 0.9736607074737549}]}, {"text": "This work was supported in part by an NSF multiple soft constraints on a desired string.", "labels": [], "entities": []}, {"text": "Unfortunately, the intersection maybe computationally inefficient in practice.", "labels": [], "entities": []}, {"text": "The intersection of K WFSAs having n 1 , n 2 , . .", "labels": [], "entities": []}, {"text": ", n K states may haven 1 \u00b7n 2 \u00b7 \u00b7 \u00b7 n K states in the worst case.", "labels": [], "entities": []}, {"text": "In this paper, we propose a more efficient method for finding the best path in an intersection without actually computing the full intersection.", "labels": [], "entities": []}, {"text": "Our approach is based on dual decomposition, a combinatorial optimization technique that was recently introduced to the vision ( and language processing communities (.", "labels": [], "entities": [{"text": "dual decomposition", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8180170059204102}]}, {"text": "Our idea is to interrogate the several WFSAs separately, repeatedly visiting each WFSA to seek a high-scoring path in each WFSA that agrees with the current paths found in the other WSFAs.", "labels": [], "entities": []}, {"text": "This iterative negotiation is reminiscent of message-passing algorithms (, while the queries to the WFSAs are reminiscent of loss-augmented inference).", "labels": [], "entities": [{"text": "WFSAs", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.9032946825027466}]}, {"text": "We remark that a general solution whose asymptotic worst-case runtime beat that of naive intersection would have important implications for complexity theory (.", "labels": [], "entities": []}, {"text": "Our approach is not such a solution.", "labels": [], "entities": []}, {"text": "We have no worst-case bounds on how long dual decomposition will take to converge in our setting, and indeed it can fail to converge altogether.", "labels": [], "entities": []}, {"text": "However, when it does converge, we have a \"certificate\" that the solution is optimal.", "labels": [], "entities": []}, {"text": "Dual decomposition is usually regarded as a method for finding an optimal vector in Rd , subject to several constraints.", "labels": [], "entities": [{"text": "Dual decomposition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9004040956497192}]}, {"text": "However, it is not obvious how best to represent strings as vectors-they Most regular expression operators combine WFSA sizes additively.", "labels": [], "entities": []}, {"text": "It is primarily intersection and its close relative, composition, that do so multiplicatively, leading to inefficiency when two large WFSAs are combined, and to exponential blowup when many WFSAs are combined.", "labels": [], "entities": []}, {"text": "Yet these operations are crucially important in practice.", "labels": [], "entities": []}, {"text": "An example that oscillates can be constructed along lines similar to the one given by  have unbounded length, and furthermore the absolute position of a symbol is not usually significant in evaluating its contribution to the score.", "labels": [], "entities": []}, {"text": "One contribution of this work is that we propose a general, flexible scheme for converting strings to feature vectors on which the WFSAs must agree.", "labels": [], "entities": [{"text": "WFSAs", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.8268013000488281}]}, {"text": "In principle the number of features maybe infinite, but the set of \"active\" features is expanded only as needed until the algorithm converges.", "labels": [], "entities": []}, {"text": "Our experiments use a particular instantiation of our general scheme, based on n-gram features.", "labels": [], "entities": []}, {"text": "We apply our method to a particular task: finding the Steiner consensus string) that has low total edit distance to a number of given, unaligned strings.", "labels": [], "entities": []}, {"text": "As an illustration, we are pleased to report that \"alia\" and \"aian\" are the consensus popular names for girls and boys born in the U.S. in 2010.", "labels": [], "entities": []}, {"text": "We use this technique for consensus decoding from speech recognition lattices, and to reconstruct the common source of up to 100 strings corrupted by random noise.", "labels": [], "entities": [{"text": "consensus decoding from speech recognition lattices", "start_pos": 26, "end_pos": 77, "type": "TASK", "confidence": 0.8499561349550883}]}, {"text": "Explicit intersection would be astronomically expensive in these cases.", "labels": [], "entities": []}, {"text": "We demonstrate that our approach tends to converge rather quickly, and that it finds good solutions quickly in any case.", "labels": [], "entities": []}], "datasetContent": [{"text": "To best highlight the utility of our approach, we consider applications that must (implicitly) intersect a large number of WFSAs.", "labels": [], "entities": []}, {"text": "We will demonstrate that, in many cases, our algorithm converges to an exact solution on problems involving 10, 25, and even 100 machines, all of which would be hopeless to solve by taking the full intersection.", "labels": [], "entities": []}, {"text": "We focus on the problem of solving for the Steiner consensus string: given a set of K strings, find the string in \u03a3 * that has minimal total edit distance to all strings in the set.", "labels": [], "entities": []}, {"text": "This is an NP-hard problem that can be solved as an intersection of K machines, as we will describe in \u00a75.2.", "labels": [], "entities": []}, {"text": "The consensus string also gives an implicit multiple sequence alignment, as we discuss in \u00a76.", "labels": [], "entities": [{"text": "multiple sequence alignment", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.5794738332430521}]}, {"text": "We begin with the application of minimum Bayes risk decoding of speech lattices, which we show can reduce to the consensus string problem.", "labels": [], "entities": []}, {"text": "We then explore the consensus problem in depth by applying it to a variety of different inputs.", "labels": [], "entities": []}, {"text": "We initialize \u0398 to include both unigrams and bigrams, as we find that unigrams alone are not productive features in these experiments.", "labels": [], "entities": []}, {"text": "As we expand \u0398, we allow it to include n-grams up to length five.", "labels": [], "entities": []}, {"text": "We run our algorithm fora maximum of 1000 iterations, using a subgradient step size of \u03b1/(t + 500) at iteration t, which satisfies the general properties to guarantee asymptotic convergence.", "labels": [], "entities": []}, {"text": "We initialize \u03b1 to 1 and 10 in the two subsections, respectively.", "labels": [], "entities": []}, {"text": "We halve \u03b1 whenever we hit a negativeweight cycle and need to backtrack.", "labels": [], "entities": []}, {"text": "If we still get negative-weight cycles after \u03b1 \u2264 10 \u22124 then we reset \u03b1 and increase the minimum order of n which is encoded in G.", "labels": [], "entities": []}, {"text": "(If n is already at our maximum of five, then we simply end without converging.)", "labels": [], "entities": []}, {"text": "In the case of non-convergence after 1000 iterations, we select the best string (according to the objective) from the set of strings that were solutions to any subproblem at any point during optimization.", "labels": [], "entities": []}, {"text": "Our implementation uses OpenFST 1.2.8 (Allauzen et al., 2007).", "labels": [], "entities": []}, {"text": "We ran our algorithm on Broadcast News data, using 226 lattices produced by the IBM Attila decoder  85% of the problems converged within 1000 iterations, with an average of 147.4 iterations.", "labels": [], "entities": [{"text": "Broadcast News data", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.9537451267242432}, {"text": "Attila decoder", "start_pos": 84, "end_pos": 98, "type": "DATASET", "confidence": 0.824931800365448}]}, {"text": "We found that the true consensus was often the most likely string under \u03c0, but not always-this was true 70% of the time.", "labels": [], "entities": []}, {"text": "In the Bayes risk objective we are optimizing in equation-the expected loss-our approach averaged a score of 1.59, while always taking the top string gives only a slightly worse average of 1.66.", "labels": [], "entities": []}, {"text": "8% of the problems encountered negativeweight cycles, which were all resolved either by decreasing the step size or encoding larger n-grams.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A summary of results for various consensus  problems, as described in  \u00a75.3.", "labels": [], "entities": []}]}