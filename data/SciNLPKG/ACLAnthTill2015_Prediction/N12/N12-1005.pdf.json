{"title": [{"text": "Continuous Space Translation Models with Neural Networks", "labels": [], "entities": [{"text": "Continuous Space Translation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6662650307019552}]}], "abstractContent": [{"text": "The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.6886051893234253}]}, {"text": "For lack of sufficient training data, most models only consider a small amount of context.", "labels": [], "entities": []}, {"text": "As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations.", "labels": [], "entities": [{"text": "continuous space translation", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.7123153805732727}]}, {"text": "In order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture.", "labels": [], "entities": []}, {"text": "In small scale and large scale English to French experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering significant improvements in performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The phrase-based approach to statistical machine translation (SMT) is based on the following inference rule, which, given a source sentence s, selects the target sentence t and the underlying alignment a maximizing the following term: where K feature functions (f k ) are weighted by a set of coefficients (\u03bb k ), and Z is a normalizing factor.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.8196497311194738}]}, {"text": "The phrase-based approach differs from other approaches by the hidden variables of the translation process: the segmentation of a parallel sentence pair into phrase pairs and the associated phrase alignments.", "labels": [], "entities": []}, {"text": "This formulation was introduced in () as an extension of the word based models (, then later motivated within a discriminative framework ().", "labels": [], "entities": []}, {"text": "One motivation for integrating more feature functions was to improve the estimation of the translation model P (t|s), which was initially based on relative frequencies, thus yielding poor estimates.", "labels": [], "entities": []}, {"text": "This is because the units of phrase-based models are phrase pairs, made of a source and a target phrase; such units are viewed as the events of discrete random variables.", "labels": [], "entities": []}, {"text": "The resulting representations of phrases (or words) thus entirely ignore the morphological, syntactic and semantic relationships that exist among those units in both languages.", "labels": [], "entities": []}, {"text": "This lack of structure hinders the generalization power of the model and reduces its ability to adapt to other domains.", "labels": [], "entities": []}, {"text": "Another consequence is that phrase-based models usually consider a very restricted context . This is a general issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as, for instance, using smoothing techniques, or working with linguistically enriched, or more abstract, representations.", "labels": [], "entities": [{"text": "statistical Natural Language Processing (NLP)", "start_pos": 120, "end_pos": 165, "type": "TASK", "confidence": 0.6655525607722146}]}, {"text": "In statistical language modeling, another line of research considers numerical representations, trained automatically through the use of neural network (see eg. ().", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.8040846784909567}]}, {"text": "An influential proposal, in this respect, is the work of ( on continuous space language models.", "labels": [], "entities": []}, {"text": "In this approach, n-gram probabilities are estimated using a continuous representation of words in lieu of standard discrete representations.", "labels": [], "entities": []}, {"text": "Experimental results, reported for instance in show significant improvements in speech recognition applications.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.8577972054481506}]}, {"text": "Recently, this model has been extended in several promising ways).", "labels": [], "entities": []}, {"text": "In the context of SMT,  is the first attempt to estimate translation probabilities in a continuous space.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9936544895172119}]}, {"text": "However, because of the proposed neural architecture, the authors only consider a very restricted set of translation units, and therefore report only a slight impact on translation performance.", "labels": [], "entities": []}, {"text": "The recent proposal of () seems especially relevant, as it is able, through the use of class-based models, to handle arbitrarily large vocabularies and opens the way to enhanced neural translation models.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.7430135607719421}]}, {"text": "In this paper, we explore various neural architectures for translation models and consider three different ways to factor the joint probability P (s, t) differing by the units (respectively phrase pairs, phrases or words) that are projected in continuous spaces.", "labels": [], "entities": []}, {"text": "While these decompositions are theoretically straightforward, they were not considered in the past because of data sparsity issues and of the resulting weaknesses of conventional maximum likelihood estimates.", "labels": [], "entities": []}, {"text": "Our main contribution is then to show that such joint distributions can be efficiently computed by neural networks, even for very large context sizes; and that their use yields significant performance improvements.", "labels": [], "entities": []}, {"text": "These models are evaluated in a n-best rescoring step using the framework of n-gram based systems, within which they integrate easily.", "labels": [], "entities": []}, {"text": "Note, however that they could be used with any phrase-based system.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first recollect, in Section 2, the n-gram based approach, and discuss various implementations of this framework.", "labels": [], "entities": []}, {"text": "We then describe, in Section 3, the neural architecture developed and explain how it can be made to handle large vocabulary tasks as well as language models over bilingual units.", "labels": [], "entities": []}, {"text": "We finally report, in Section 4, experimental results obtained on a large-scale English to French translation task.", "labels": [], "entities": [{"text": "English to French translation task", "start_pos": 80, "end_pos": 114, "type": "TASK", "confidence": 0.6879585444927215}]}], "datasetContent": [{"text": "We now turn to an experimental comparison of the models introduced in Section 2.", "labels": [], "entities": []}, {"text": "We first describe the tasks and data that were used, before presenting our n-gram based system and baseline set-up.", "labels": [], "entities": []}, {"text": "Our results are finally presented and discussed.", "labels": [], "entities": []}, {"text": "Let us first emphasize that the design and integration of a SOUL model for large SMT tasks is far from easy, given the computational cost of computing n-gram probabilities, a task that is performed repeatedly during the search of the best translation.", "labels": [], "entities": [{"text": "SMT tasks", "start_pos": 81, "end_pos": 90, "type": "TASK", "confidence": 0.9369167685508728}]}, {"text": "Our solution was to resort to a two pass approach: the first pass uses a conventional back-off n-gram model to produce a k-best list (the k most likely translations); in the second pass, the probability of a m-gram SOUL model is computed for each hypothesis, added as anew feature and the k-best list is accordingly reordered . In all the following experiments, we used a fixed context size for SOUL of m = 10, and used k = 300.", "labels": [], "entities": []}, {"text": "The comparison with the condition where we only use a SOUL target LM is interesting as well.", "labels": [], "entities": []}, {"text": "Here, the use of the word factored TM still yields to a 0.6 BLEU improvement.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9994831085205078}]}, {"text": "This result shows that there is an actual benefit in smoothing the TM estimates, rather than only focus on the LM estimates.", "labels": [], "entities": []}, {"text": "reports a comparison among the different components and variations of the word The standard deviations are below 0.1 and thus omitted in the reported results. and finally their combination, which yields the joint probability of the sentence pair.", "labels": [], "entities": []}, {"text": "Here, we observe that the best improvement is obtained with the translation term, which is 0.7 BLEU point better than the latter term.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9995597004890442}]}, {"text": "Moreover, the use of just a translation term only yields a BLEU score equal to the one obtained with the SOUL target LM, and its combination with the distortion term is decisive to attain the additional gain of 0.6 BLEU point.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.999496340751648}, {"text": "BLEU", "start_pos": 215, "end_pos": 219, "type": "METRIC", "confidence": 0.9991538524627686}]}, {"text": "The lower part of the table provides the same comparison, but for the variation of the word factored TM.", "labels": [], "entities": []}, {"text": "Besides a similar trend, we observe that this variation delivers slightly lower results.", "labels": [], "entities": []}, {"text": "This can be explained by the restricted context used by the translation term which no longer includes the current source phrase or word.: Results for the large English to French translation task obtained by adding various SOUL translation and language models (see text for details).", "labels": [], "entities": [{"text": "English to French translation task", "start_pos": 160, "end_pos": 194, "type": "TASK", "confidence": 0.7357914209365845}, {"text": "SOUL translation and language", "start_pos": 222, "end_pos": 251, "type": "TASK", "confidence": 0.8894928246736526}]}, {"text": "For the large-scale setting, the training material increases drastically with the use of the additional outof-domain data for the baseline models.", "labels": [], "entities": []}, {"text": "The first observation is the large increase of BLEU (+2.4 points) for the baseline system over the small-scale baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9997885823249817}]}, {"text": "For this task, only the word factored TM is evaluated since it significantly outperforms the others on the small task (see section 4.3).", "labels": [], "entities": [{"text": "TM", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.4961503744125366}]}, {"text": "Ina first scenario, we use a word factored TM, trained only on the small in-domain corpus.", "labels": [], "entities": []}, {"text": "Even though the training corpus of the baseline TM is one hundred times larger than this small in-domain data, adding the SOUL TM still yields a BLEU increase of 1.2 point 8 . Ina second scenario, we increase the training corpus for the SOUL, and include parts of the out-of-domain data (the WMT part).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9996687173843384}]}, {"text": "The resulting BLEU score is here slightly worse than the one obtained with just the in-domain TM, yet delivering improved results with the respect to the baseline.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.979003518819809}]}, {"text": "Ina last attempt, we amended the training regime of the neural network.", "labels": [], "entities": []}, {"text": "Ina fist step, we trained conventionally a SOUL model using the same out-ofdomain parallel data as before.", "labels": [], "entities": []}, {"text": "We then adapted this model by running five additional epochs of the backpropagation algorithm using the in-domain data.", "labels": [], "entities": []}, {"text": "Using this adapted model yielded our best results to date with a BLEU improvement of 1.6 points over the baseline results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9998416900634766}]}, {"text": "Moreover, the gains obtained using this simple domain adaptation strategy are re-spectively of +0.4 and +0.8 BLEU, as compared with the small in-domain model and the large outof-domain model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9986423850059509}]}, {"text": "These results show that the SOUL TM can scale efficiently and that its structure is well suited for domain adaptation.", "labels": [], "entities": [{"text": "SOUL TM", "start_pos": 28, "end_pos": 35, "type": "TASK", "confidence": 0.7725996375083923}, {"text": "domain adaptation", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.710567831993103}]}], "tableCaptions": [{"text": " Table 2: Results for the small English to French task ob- tained with the baseline system and with various SOUL  translation (TM) or target language (LM) models.", "labels": [], "entities": [{"text": "SOUL  translation (TM) or target language (LM)", "start_pos": 108, "end_pos": 154, "type": "TASK", "confidence": 0.8583318211815574}]}, {"text": " Table 3: Comparison of the different components and  variations of the word factored translation model.", "labels": [], "entities": [{"text": "word factored translation", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.6228925685087839}]}, {"text": " Table 4: Results for the large English to French trans- lation task obtained by adding various SOUL translation  and language models (see text for details).", "labels": [], "entities": [{"text": "trans- lation", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.6752706368764242}, {"text": "SOUL translation", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.8395329117774963}]}]}