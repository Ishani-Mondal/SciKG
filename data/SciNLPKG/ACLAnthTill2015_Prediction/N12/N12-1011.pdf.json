{"title": [{"text": "Exploring Content Features for Automated Speech Scoring", "labels": [], "entities": [{"text": "Automated Speech Scoring", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.599640021721522}]}], "abstractContent": [{"text": "Most previous research on automated speech scoring has focused on restricted, predictable speech.", "labels": [], "entities": [{"text": "automated speech scoring", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.6200276414553324}]}, {"text": "For automated scoring of unrestricted spontaneous speech, speech proficiency has been evaluated primarily on aspects of pronunciation , fluency, vocabulary and language usage but not on aspects of content and topi-cality.", "labels": [], "entities": []}, {"text": "In this paper, we explore features representing the accuracy of the content of a spoken response.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9962878227233887}]}, {"text": "Content features are generated using three similarity measures, including a lexical matching method (Vector Space Model) and two semantic similarity measures (Latent Semantic Analysis and Pointwise Mutual Information).", "labels": [], "entities": []}, {"text": "All of the features exhibit moderately high correlations with human proficiency scores on human speech transcriptions.", "labels": [], "entities": []}, {"text": "The correlations decrease somewhat due to recognition errors when evaluated on the output of an automatic speech recognition system; however , the additional use of word confidence scores can achieve correlations at a similar level as for human transcriptions.", "labels": [], "entities": [{"text": "correlations", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9646955728530884}]}], "introductionContent": [{"text": "Automated assessment of a non-native speaker's proficiency in a given language is an attractive application of automatic speech recognition (ASR) and natural language processing (NLP) technology; the technology can be used by language learners for individual practice and by assessment providers to reduce the cost of human scoring.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 111, "end_pos": 145, "type": "TASK", "confidence": 0.7914397815863291}]}, {"text": "While much research has been done about the scoring of restricted speech, such as reading aloud or repeating sentences verbatim (;), much less has been done about the scoring of spontaneous speech.", "labels": [], "entities": []}, {"text": "For automated scoring of unrestricted, spontaneous speech, most automated systems have estimated the non-native speakers' speaking proficiency primarily based on lowlevel speaking-related features, such as pronunciation, intonation, rhythm, rate of speech, and fluency (), although a few recent studies have explored features based on vocabulary and grammatical complexity).", "labels": [], "entities": []}, {"text": "To date, little work has been conducted on automatically assessing the relatively higher-level aspects of spontaneous speech, such as the content and topicality, the structure, and the discourse information.", "labels": [], "entities": []}, {"text": "Automated assessment of these aspects of a non-native speaker's speech is very challenging fora number of reasons, such as the short length of typical responses (approximately 100 words fora typical 1 minute response, compared to over 300 words in atypical essay/written response), the spontaneous nature of the speech, and the presence of disfluencies and possible grammatical errors.", "labels": [], "entities": []}, {"text": "Moreover, the assessment system needs text transcripts of the speech to evaluate the high level aspects, and these are normally obtained from ASR systems.", "labels": [], "entities": []}, {"text": "The recognition accuracy of state-of-the-art ASR systems on non-native spontaneous speech is still relatively low, which will sequentially impact the re-liability and accuracy of automatic scoring systems using these noisy transcripts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9362298250198364}, {"text": "ASR", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9867016673088074}, {"text": "re-liability", "start_pos": 150, "end_pos": 162, "type": "METRIC", "confidence": 0.9811611771583557}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.998110294342041}]}, {"text": "However, despite these difficulties, it is necessary for an automated assessment system to address the high level information of a spoken response in order to fully coverall aspects that are considered by human raters.", "labels": [], "entities": []}, {"text": "Thus, in this paper we focus on exploring features to represent the high-level aspect of speech mainly on the accuracy of the content.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.998367965221405}]}, {"text": "As a starting point, we consider approaches that have been used for the automated assessment of content in essays.", "labels": [], "entities": [{"text": "assessment of content in essays", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.7313485741615295}]}, {"text": "However, due to the qualitative differences between written essays and spontaneous speech, the techniques developed for written texts may not perform as well on spoken responses.", "labels": [], "entities": []}, {"text": "Still, as a baseline, we will evaluate the content features used for essay scoring on spontaneous speech.", "labels": [], "entities": []}, {"text": "In addition to a straightforward lexical Vector Space Model (VSM), we investigate approaches using two other similarity measures, Latent Semantic Analysis (LSA) and Pointwise Mutual Information (PMI), in order to represent the semantic-level proficiency of a speaker.", "labels": [], "entities": []}, {"text": "All of the content features are analyzed using both human transcripts and speech recognizer output, so we can have a better understanding of the impact of ASR errors on the performance of the features.", "labels": [], "entities": [{"text": "ASR", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.9866993427276611}]}, {"text": "As expected, the results show that the performance on ASR output is lower than when human transcripts are used.", "labels": [], "entities": [{"text": "ASR", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.987958550453186}]}, {"text": "Therefore, we propose improved content features that take into account ASR confidence scores to emphasize responses whose estimated word accuracy is comparatively higher than others.", "labels": [], "entities": [{"text": "ASR confidence scores", "start_pos": 71, "end_pos": 92, "type": "METRIC", "confidence": 0.7904983361562093}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.5060308575630188}]}, {"text": "These improved features can obtain similar performance when compared to the results using human transcripts.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section we introduce previous research on automated assessment of content in essays and spoken responses.", "labels": [], "entities": [{"text": "assessment of content in essays and spoken responses", "start_pos": 64, "end_pos": 116, "type": "TASK", "confidence": 0.7014770731329918}]}, {"text": "The content features we generated and the model we used to build the final speaking scores are described in Sections 3 and Section 4, respectively.", "labels": [], "entities": []}, {"text": "In Section 5 we show the performance of all our proposed features.", "labels": [], "entities": []}, {"text": "Finally, we conclude our work and discuss potential future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "To measure the quality of the developed features, we employ a widely used metric, the Pearson correlation coefficient (r).", "labels": [], "entities": [{"text": "Pearson correlation coefficient (r)", "start_pos": 86, "end_pos": 121, "type": "METRIC", "confidence": 0.932915061712265}]}, {"text": "In our experiments, we use the value of the Pearson correlation between the feature values and the human proficiency scores for each spoken response.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 44, "end_pos": 63, "type": "METRIC", "confidence": 0.9813849627971649}]}], "tableCaptions": [{"text": " Table 1: Summary statistics of training, development and evaluation data set.", "labels": [], "entities": []}, {"text": " Table 2: Pearson correlations of the content features using human transcripts.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9279451370239258}]}, {"text": " Table 3: Pearson correlations of the content features using ASR output.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9079551696777344}, {"text": "ASR", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.6537414193153381}]}, {"text": " Table 4: Pearson correlations of the content features using ASR output with confidence scores.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9171808362007141}, {"text": "ASR", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.46252936124801636}]}]}