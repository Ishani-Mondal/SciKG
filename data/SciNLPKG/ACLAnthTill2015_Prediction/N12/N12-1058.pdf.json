{"title": [{"text": "Co-reference via Pointing and Haptics in Multi-Modal Dialogues", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our ongoing work on resolving third person pronouns and deictic words in a multi-modal corpus.", "labels": [], "entities": []}, {"text": "We show that about two thirds of these referring expressions have antecedents that are introduced by pointing gestures or by haptic-ostensive actions (actions that involve manipulating an object).", "labels": [], "entities": []}, {"text": "After describing our annotation scheme, we discuss the co-reference models we learn from multi-modal features.", "labels": [], "entities": []}, {"text": "The usage of haptic-ostensive actions in a co-reference model is a novel contribution of our work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Co-reference resolution has received a lot of attention.", "labels": [], "entities": [{"text": "Co-reference resolution", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7865366637706757}]}, {"text": "However, as noted, most research on co-reference resolution has focused on written text.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7882804274559021}]}, {"text": "This task is much more difficult in dialogue, especially in multi-modal dialogue contexts.", "labels": [], "entities": []}, {"text": "First, utterances are informal, ungrammatical and disfluent.", "labels": [], "entities": []}, {"text": "Second, people spontaneously use gestures and other body language.", "labels": [], "entities": []}, {"text": "As noticed by,, in a multi-modal corpus, the antecedents of referring expressions are often introduced via gestures.", "labels": [], "entities": []}, {"text": "Whereas the role played by pointing gestures in referring has been studied, the same is not true for other types of gestures.", "labels": [], "entities": []}, {"text": "In this paper, alongside pointing gestures, we will discuss the role played by Haptic-Ostensive (H-O) actions, i.e., referring to an object by manipulating it in the world ().", "labels": [], "entities": []}, {"text": "As far as we know, no computational models of coreference have been developed that include H-O actions:) focused on perceptual salience and () on generation rather than interpretation.", "labels": [], "entities": []}, {"text": "We should point out that at the time of writing we only focus on resolving third person pronouns and deictics.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe our multi-modal annotation scheme.", "labels": [], "entities": []}, {"text": "In Section 3 we present the pronoun/deictic resolution system.", "labels": [], "entities": [{"text": "pronoun/deictic resolution", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.6717062294483185}]}, {"text": "In Section 4, we discuss experiments and results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have experimented with 3 types of classification models: Maximum Entropy (MaxEnt), Decision Tree and Support Vector Machine (SVM), respectively implemented via the following three packages: MaxEnt, J48 from Weka (, and LibSVM ().", "labels": [], "entities": []}, {"text": "All of the results reported below are calculated using 10 fold cross validation.", "labels": [], "entities": []}, {"text": "We have run a series of experiments changing the history length from 0 to 10 seconds for generating co-reference pairs (history changes in increments of 1 second, hence, there are 11 sets of experiments).", "labels": [], "entities": []}, {"text": "For each history length, we build the 3 models mentioned above.", "labels": [], "entities": []}, {"text": "An additional baseline model treats a co-reference pair as \"True\" if speaker agreement is true for the pair, and the time distance is 0.", "labels": [], "entities": []}, {"text": "Beside the specified baseline, J48 can be seen as a more sophisticated baseline as well.", "labels": [], "entities": [{"text": "J48", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.9084669947624207}]}, {"text": "When we ran the 10 fold experiment with J48 algorithm, 5 out of 10 generated decision trees only used 3 attributes.", "labels": [], "entities": []}, {"text": "We use two metrics to measure the performance of the models.", "labels": [], "entities": []}, {"text": "One are the standard precision, recall and F-Score with respect to the generated coreference pairs; the other is the number of pronouns and deictics that are correctly resolved.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9994229078292847}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.999241828918457}, {"text": "F-Score", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.995541512966156}]}, {"text": "Given a pronoun/deictic pi , if the classifier returns more than one positive co-reference pair for pi , we use a heuristic resolver to choose the target.", "labels": [], "entities": []}, {"text": "We divide those positive pairs into two subsets, those where the speaker of pi is the same as the performer of the gesture (SAME), and those with the other speaker (OTHER).", "labels": [], "entities": [{"text": "SAME", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9891535639762878}, {"text": "OTHER", "start_pos": 165, "end_pos": 170, "type": "METRIC", "confidence": 0.9851629137992859}]}, {"text": "If SOME is not empty, we will choose SOME, otherwise OTHER.", "labels": [], "entities": [{"text": "OTHER", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.7095996737480164}]}, {"text": "If the chosen set contains more than one pair, we will choose the target  of the gesture/H-O action in the most recent pair.", "labels": [], "entities": []}, {"text": "Given the space limit, only shows the results for each model which resolved most pronouns/deictics, and the model which produced the best F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 138, "end_pos": 145, "type": "METRIC", "confidence": 0.988058865070343}]}, {"text": "In, with the change of History window setting, the gold standard of co-reference pairs change.", "labels": [], "entities": [{"text": "History window setting", "start_pos": 23, "end_pos": 45, "type": "DATASET", "confidence": 0.9395853877067566}]}, {"text": "When the history window is larger, there are more co-reference candidate pairs, which help resolve more pronouns and deictics.", "labels": [], "entities": []}, {"text": "Given we work on anew corpus, it is hard to compare our results to previous work, additionally our models currently do not deal with textual antecedents.", "labels": [], "entities": []}, {"text": "For example reports their best F-Measure as .4742, while ours is .747.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9891825318336487}]}, {"text": "As concerns accuracy, whereas 384/827 (46%) may appear low, note the task we are performing is harder since we are trying to solve all pronouns/deictics via gestures, not only the ones which have an antecedent introduced by a pointing or H-O action (see).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.999408483505249}]}, {"text": "Even if our feature set is limited, all the classification models perform better than baseline in all the experiments; the biggest improvement is 14.4% in F-score, and solving 25 more pronouns and deictics.", "labels": [], "entities": [{"text": "F-score", "start_pos": 155, "end_pos": 162, "type": "METRIC", "confidence": 0.9976621866226196}]}, {"text": "There are no significant differences in the performances of the 3 different classification models.", "labels": [], "entities": []}, {"text": "shows that the history length of the best models is less than or equal to 2 seconds, which is within the standard error range of annotations when we marked the time spans for events.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Gesture&Haptics Co-reference Model Results", "labels": [], "entities": []}]}