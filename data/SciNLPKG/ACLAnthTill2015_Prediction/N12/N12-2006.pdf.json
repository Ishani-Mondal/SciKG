{"title": [{"text": "Choosing an Evaluation Metric for Parser Design", "labels": [], "entities": [{"text": "Parser Design", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.7946785688400269}]}], "abstractContent": [{"text": "This paper seeks to quantitatively evaluate the degree to which a number of popular met-rics provide overlapping information to parser designers.", "labels": [], "entities": []}, {"text": "Two routine tasks are considered: optimizing a machine learning regularization parameter and selecting an optimal machine learning feature set.", "labels": [], "entities": []}, {"text": "The main result is that the choice of evaluation metric used to optimize these problems (with one exception among popular metrics) has little effect on the solution to the optimization.", "labels": [], "entities": []}], "introductionContent": [{"text": "The question of how best to evaluate the performance of a parser has received considerable attention.", "labels": [], "entities": []}, {"text": "Numerous metrics have been proposed, and their relative merits have been debated.", "labels": [], "entities": []}, {"text": "In this paper, we seek to quantitatively evaluate the degree to which a number of popular metrics provide overlapping information for two concrete subtasks of the parser design problem.", "labels": [], "entities": []}, {"text": "The motivation for this study was to confirm our suspicion that parsing models that performed well under one metric were likely to perform well under other metrics, thereby validating the widespread practice of using just a single metric when conducting research on improving parser performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.9773907661437988}]}, {"text": "Our results are cautiously optimistic on this front.", "labels": [], "entities": []}, {"text": "We use the problem of selecting the best performer from a large space of varied but related parse disambiguation models (\"parsers\" henceforth) as the setting for our study.", "labels": [], "entities": []}, {"text": "The parsers are all conditional log-linear disambiguators with quadratic regularization, coupled to the English Resource Grammar (ERG)), a broad-coverage HPSGbased hand-built grammar of English.", "labels": [], "entities": []}, {"text": "Analyses from the ERG consist of a syntax tree together with an underspecified logical formula called an MRS ().", "labels": [], "entities": [{"text": "MRS", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.8457415103912354}]}, {"text": "The parsers differ from each other along two dimensions: the feature templates employed, and the degree of regularization used.", "labels": [], "entities": []}, {"text": "There are 57 different sets of traditional and novel feature templates collecting a variety of syntactic and semantic data about candidate ERG analyses.", "labels": [], "entities": [{"text": "ERG analyses", "start_pos": 139, "end_pos": 151, "type": "TASK", "confidence": 0.9049018919467926}]}, {"text": "For each set of feature templates, parsers were trained with 41 different values for the quadratic regularization parameter, fora total of 2337 different parsers.", "labels": [], "entities": []}, {"text": "The WeScience Treebank of about 9100 sentences () was used both for training and testing the parsers, with 10-fold cross validation.", "labels": [], "entities": [{"text": "WeScience Treebank", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9367071390151978}]}, {"text": "We breakdown the problem of selecting the best parser into two tasks.", "labels": [], "entities": []}, {"text": "The first task is to identify the optimal value for the regularization parameter for each set of feature templates.", "labels": [], "entities": []}, {"text": "The second task is to compare the different sets of feature templates to each other, considering only the optimal value of the regularization parameter for each, and select the overall best.", "labels": [], "entities": []}, {"text": "We attack each task with each of 14 metrics, and discuss the results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}