{"title": [{"text": "Reference Scope Identification in Citing Sentences", "labels": [], "entities": [{"text": "Scope Identification", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8102025091648102}]}], "abstractContent": [{"text": "A citing sentence is one that appears in a scientific article and cites previous work.", "labels": [], "entities": []}, {"text": "Citing sentences have been studied and used in many applications.", "labels": [], "entities": []}, {"text": "For example, they have been used in scientific paper summarization, automatic survey generation, paraphrase identification , and citation function classification.", "labels": [], "entities": [{"text": "scientific paper summarization", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.5239144861698151}, {"text": "automatic survey generation", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.613342156012853}, {"text": "paraphrase identification", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.9276572167873383}, {"text": "citation function classification", "start_pos": 129, "end_pos": 161, "type": "TASK", "confidence": 0.7564743955930074}]}, {"text": "Citing sentences that cite multiple papers are common in scientific writing.", "labels": [], "entities": [{"text": "Citing sentences that cite multiple papers", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8476888636747996}]}, {"text": "This observation should betaken into consideration when using citing sentences in applications.", "labels": [], "entities": []}, {"text": "For instance , when a citing sentence is used in a summary of a scientific paper, only the fragments of the sentence that are relevant to the summarized paper should be included in the summary.", "labels": [], "entities": []}, {"text": "In this paper, we present and compare three different approaches for identifying the fragments of a citing sentence that are related to a given target reference.", "labels": [], "entities": []}, {"text": "Our methods are: word classification, sequence labeling, and segment classification.", "labels": [], "entities": [{"text": "word classification", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8268381357192993}, {"text": "sequence labeling", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7031185626983643}, {"text": "segment classification", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.799020528793335}]}, {"text": "Our experiments show that segment classification achieves the best results.", "labels": [], "entities": [{"text": "segment classification", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.9165315330028534}]}], "introductionContent": [{"text": "Citation plays an important role in science.", "labels": [], "entities": []}, {"text": "It makes the accumulation of knowledge possible.", "labels": [], "entities": []}, {"text": "When a reference appears in a scientific article, it is usually accompanied by a span of text that highlights the important contributions of the cited article.", "labels": [], "entities": []}, {"text": "We calla sentence that contains an explicit reference to previous work a citation sentence.", "labels": [], "entities": []}, {"text": "For example, sentence (1) below is a citing sentence that cites a paper by Philip Resnik and describes the problem Resnik addressed in his paper.", "labels": [], "entities": []}, {"text": "Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (; AbuJbara and Radev, 2011), automatic survey generation (), citation function classification (, and paraphrase recognition ().", "labels": [], "entities": [{"text": "scientific paper summarization", "start_pos": 87, "end_pos": 117, "type": "TASK", "confidence": 0.5693926215171814}, {"text": "automatic survey generation", "start_pos": 148, "end_pos": 175, "type": "TASK", "confidence": 0.6270106534163157}, {"text": "citation function classification", "start_pos": 180, "end_pos": 212, "type": "TASK", "confidence": 0.7622946699460348}, {"text": "paraphrase recognition", "start_pos": 220, "end_pos": 242, "type": "TASK", "confidence": 0.9506478905677795}]}, {"text": "Sentence (1) above contains one reference, and the whole sentence is talking about that reference.", "labels": [], "entities": []}, {"text": "This is not always the casein scientific writing.", "labels": [], "entities": []}, {"text": "Sentences that contain references to multiple papers are very common.", "labels": [], "entities": []}, {"text": "For example, sentence (2) below contains three references.", "labels": [], "entities": []}, {"text": "(2) and use the web to generate corpora for languages where electronic resources are scarce, while Resnik (1999) describes a method for mining the web for bilingual texts.", "labels": [], "entities": []}, {"text": "The first fragment describes the contribution of and.", "labels": [], "entities": []}, {"text": "The second fragment describes the contribution of.", "labels": [], "entities": []}, {"text": "This observation should betaken into consideration when using citing sentences in the aforementioned applications.", "labels": [], "entities": []}, {"text": "For example, in citation-based summarization of scientific papers, a subset of citing sentences that cite a given target paper is selected and used to form a summary of that paper.", "labels": [], "entities": [{"text": "summarization of scientific papers", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.8567822873592377}]}, {"text": "It is very likely that one or more of the selected sentences cite multiple papers besides the target.", "labels": [], "entities": []}, {"text": "This means that some of the text included in the summary might be irrelevant to the summarized paper.", "labels": [], "entities": []}, {"text": "Including irrelevant text in the summary introduces several problems.", "labels": [], "entities": []}, {"text": "First, the summarization task aims at summarizing the contributions of the target paper using minimal text.", "labels": [], "entities": [{"text": "summarization", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.9927343130111694}, {"text": "summarizing", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.9760320782661438}]}, {"text": "Extraneous text takes space in the summary while being irrelevant and less important.", "labels": [], "entities": []}, {"text": "Second, including irrelevant text in the summary breaks the context and confuses the reader.", "labels": [], "entities": []}, {"text": "Therefore, if sentence (2) above is to be added to a citation-based summary of Resnik\u00b4sResnik\u00b4s (1999) paper, only the underlined fragment should be added to the summary and the rest of the sentence should be excluded.", "labels": [], "entities": [{"text": "Resnik\u00b4sResnik\u00b4s (1999) paper", "start_pos": 79, "end_pos": 108, "type": "DATASET", "confidence": 0.8049593687057495}]}, {"text": "For another example, consider the task of citation function classification.", "labels": [], "entities": [{"text": "citation function classification", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.8928280274073283}]}, {"text": "The goal of this task is to determine the reason for citing paper B by paper A based on linguistic and structural features extracted from citing sentences that appear in A and cite B.", "labels": [], "entities": []}, {"text": "If a citing sentence in A cites multiple papers besides B, classification features should be extracted only from the fragments of the sentence that are relevant to B.", "labels": [], "entities": []}, {"text": "Sentence (3) below shows an examples of this case.", "labels": [], "entities": []}, {"text": "(3) Cohn and used the GHKM extraction method), which is limited to constituent phrases and thus produces a reasonably small set of syntactic rules.", "labels": [], "entities": [{"text": "GHKM extraction", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7050309628248215}]}, {"text": "If the target reference is, only the underlined segment should be used for feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7427333295345306}]}, {"text": "The limitation stated in the second segment of sentence is referring to.", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of identifying the fragments of a citing sentence that are related to a given target reference.", "labels": [], "entities": []}, {"text": "Henceforth, we use the term Reference Scope to refer to those fragments.", "labels": [], "entities": []}, {"text": "We present and compare three different approaches to this problem.", "labels": [], "entities": []}, {"text": "In the first approach, we define the problem as a word classification task.", "labels": [], "entities": [{"text": "word classification task", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.8027560114860535}]}, {"text": "We classify each word in the sentence as inside or outside the scope of the target reference.", "labels": [], "entities": []}, {"text": "In the second approach, we define the problem as a sequence labeling problem.", "labels": [], "entities": [{"text": "sequence labeling problem", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.7199320197105408}]}, {"text": "This is different from the first approach in that the label assigned to each word is dependent on the labels of nearby words.", "labels": [], "entities": []}, {"text": "In the third approach, instead of classifying individual words, we split the sentence into segments and classify each segment as inside or outside the scope of the target reference.", "labels": [], "entities": []}, {"text": "Applying any of the three approaches is preceded by a preprocessing stage.", "labels": [], "entities": []}, {"text": "In this stage, citing sentences are analyzed to tag references, identify groups of references, and distinguish between syntactic and non-syntactic references.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 examines the related work.", "labels": [], "entities": []}, {"text": "We define the problem in Section3.", "labels": [], "entities": [{"text": "Section3", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8849719166755676}]}, {"text": "Section 4 presents our approaches.", "labels": [], "entities": []}, {"text": "Experiments, results and analysis are presented in Section 5.", "labels": [], "entities": []}, {"text": "We conclude and provide directions to future work in Section 6", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) () for text tokenization, part-of-speech tagging, chunking, and noun phrase head identification.", "labels": [], "entities": [{"text": "Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT)", "start_pos": 11, "end_pos": 75, "type": "DATASET", "confidence": 0.9262721538543701}, {"text": "text tokenization", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.6506166160106659}, {"text": "part-of-speech tagging", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.7239271104335785}, {"text": "noun phrase head identification", "start_pos": 140, "end_pos": 171, "type": "TASK", "confidence": 0.8063945174217224}]}, {"text": "We use the Stanford parser () for syntactic and dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8056963682174683}]}, {"text": "We use Lib-SVM (Chang and Lin, 2011) for Support Vector Machines (SVM) classification.", "labels": [], "entities": [{"text": "Support Vector Machines (SVM) classification", "start_pos": 41, "end_pos": 85, "type": "TASK", "confidence": 0.6297076429639544}]}, {"text": "Our SVM model uses a linear kernel.", "labels": [], "entities": []}, {"text": "We use Weka () for logistic regression classification.", "labels": [], "entities": [{"text": "logistic regression classification", "start_pos": 19, "end_pos": 53, "type": "TASK", "confidence": 0.7145767013231913}]}, {"text": "We use the Machine Learning for Language Toolkit (MALLET)) for CRF-based sequence labeling.", "labels": [], "entities": [{"text": "CRF-based sequence labeling", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.8514472246170044}]}, {"text": "In all the scope identification experiments and results below, we use 10-fold cross validation for training/testing.", "labels": [], "entities": [{"text": "scope identification", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.9026894569396973}]}, {"text": "We ran our three rule-based preprocessing modules on the testing data set and compared the output to the human annotations.", "labels": [], "entities": [{"text": "testing data set", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.8133262197176615}]}, {"text": "The test set was not used in the tuning of the system but was done using the development data set as described above.", "labels": [], "entities": []}, {"text": "We report the results for each of the preprocessing modules.", "labels": [], "entities": []}, {"text": "Our reference tagging module achieved 98.3% precision and 93.1% recall.", "labels": [], "entities": [{"text": "reference tagging", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7368767857551575}, {"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9967610239982605}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9994103908538818}]}, {"text": "Most of the errors were due to issues with text extraction from PDF or due to bad references practices by some authors (i.e., not following scientific referencing standards).", "labels": [], "entities": [{"text": "text extraction from PDF", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.7920465022325516}]}, {"text": "Our reference grouping module achieved perfect accuracy for all the correctly tagged references.", "labels": [], "entities": [{"text": "reference grouping", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7608026266098022}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.99945467710495}]}, {"text": "This was expected since this is a straightforward task.", "labels": [], "entities": []}, {"text": "The non-syntactic reference removal module achieved 90.08% precision and 90.1% recall.", "labels": [], "entities": [{"text": "reference removal", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7591598927974701}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9993836879730225}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9992790818214417}]}, {"text": "Again, most of the errors were the result of bad referencing practices by the authors.", "labels": [], "entities": []}, {"text": "We conducted several experiments to compare the methods proposed in Section 4 and their variants.", "labels": [], "entities": []}, {"text": "We ran all the experiments on the training/testing set (the 3300 sentences) described in Section 5.1.: Results of scope identification using the different algorithms described in the paper The experiments that we ran are as follows: 1) word classification using a SVM classifier (WC-SVM); 2) word classification using a logistic regression classifier(WC-LR); 3) CRF-based sequence labeling (SL-CRF); 4) segment classification using segmentation method-1 and label aggregation rule-1 (SC-S1-R1); 5,6,7,8,9) same as (4) but using different combinations of segmentation methods 1 and 2, and label aggregation rules 1,2 and 3: SC-S1-R2, SC-S1-R3, SC-S2-R1, SC-S2-R2, SC-S2-R3 (where Sx refers to segmentation method x and Ry refers to label aggregation rule y all as explained in Section 4.2.3).", "labels": [], "entities": [{"text": "scope identification", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.8514585196971893}, {"text": "word classification", "start_pos": 236, "end_pos": 255, "type": "TASK", "confidence": 0.7829433381557465}, {"text": "word classification", "start_pos": 292, "end_pos": 311, "type": "TASK", "confidence": 0.7895765602588654}, {"text": "CRF-based sequence labeling", "start_pos": 362, "end_pos": 389, "type": "TASK", "confidence": 0.5732095638910929}, {"text": "segment classification", "start_pos": 403, "end_pos": 425, "type": "TASK", "confidence": 0.9168156981468201}]}, {"text": "Finally, 10) we compare our methods to the baseline method proposed by Abu-Jbara and Radev (2011) which was described in Section 4 (AR-2011).", "labels": [], "entities": []}, {"text": "To better understand which of the features listed in are more important for the task, we use method for feature selection using SVM to rank the features based on their importance.", "labels": [], "entities": []}, {"text": "The results of the experiments and the feature analysis are presented and discussed in the following subsection.", "labels": [], "entities": []}, {"text": "We ran the experiments described in the previous subsection on the testing data described in Sec- A wide range of contextual information, such as surrounding words (GREF ), dependency or case structure (GTREF ), and dependency path (GREF ), has been utilized for similarity calculation, and achieved considerable success.", "labels": [], "entities": [{"text": "dependency path (GREF )", "start_pos": 216, "end_pos": 239, "type": "METRIC", "confidence": 0.6656669914722443}, {"text": "similarity calculation", "start_pos": 263, "end_pos": 285, "type": "TASK", "confidence": 0.6575468182563782}]}, {"text": "Sequence Labeling (SL-CRF) A wide range of contextual information, such as surrounding words (GREF), dependency or case structure (GTREF), and dependency path (GREF ), has been utilized for similarity calculation, and achieved considerable success.", "labels": [], "entities": [{"text": "Sequence Labeling (SL-CRF)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8540528416633606}, {"text": "dependency path (GREF )", "start_pos": 143, "end_pos": 166, "type": "METRIC", "confidence": 0.6578303635120392}, {"text": "similarity calculation", "start_pos": 190, "end_pos": 212, "type": "TASK", "confidence": 0.7651303112506866}]}, {"text": "Segment Classification (SC-S2-R1) A wide range of contextual information, such as surrounding words (GREF ), dependency or case structure (GTREF ), and dependency path (GREF ), has been utilized for similarity calculation, and achieved considerable success.", "labels": [], "entities": [{"text": "Segment Classification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8986354768276215}, {"text": "dependency path (GREF )", "start_pos": 152, "end_pos": 175, "type": "METRIC", "confidence": 0.6620436549186707}, {"text": "similarity calculation", "start_pos": 199, "end_pos": 221, "type": "TASK", "confidence": 0.7389520704746246}]}, {"text": "Word Classification (WC-SVM) Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering (REF).", "labels": [], "entities": [{"text": "Word Classification (WC-SVM)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8016876339912414}, {"text": "EM-based clustering (REF)", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.6122898757457733}]}, {"text": "Sequence Labeling (SL-CRF) Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering (REF).", "labels": [], "entities": [{"text": "Sequence Labeling (SL-CRF)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8404914855957031}, {"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9485357999801636}, {"text": "EM-based clustering (REF)", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.6292825639247894}]}, {"text": "Segment Classification (SC-S2-R1) Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering (REF).: Two example outputs produced by the three methods tion 5.1.", "labels": [], "entities": [{"text": "Segment Classification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8771423101425171}, {"text": "WordNet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9513758420944214}]}, {"text": "compares the precision, recall, F1, and accuracy for the three methods described in Section 4 and their variations.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9997931122779846}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9996768236160278}, {"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9997294545173645}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9998170733451843}]}, {"text": "All the metrics were computed at the word level.", "labels": [], "entities": []}, {"text": "The results show that all our methods outperform the baseline method AR-2011 that was proposed by.", "labels": [], "entities": [{"text": "AR-2011", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9435520768165588}]}, {"text": "In the word classification method, we notice no significant difference between the performance of the SVM vs Logistic Regression classifier.", "labels": [], "entities": [{"text": "word classification", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7544395625591278}]}, {"text": "We also notice that the CRF-based sequence labeling method performs significantly better than the word classification method.", "labels": [], "entities": [{"text": "CRF-based sequence labeling", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6858168641726176}, {"text": "word classification", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7985945045948029}]}, {"text": "This result corroborates our intuition that the labels of neighboring words are dependent.", "labels": [], "entities": []}, {"text": "The results also show that segment labeling generally performs better than word labeling.", "labels": [], "entities": [{"text": "segment labeling", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7790118157863617}, {"text": "word labeling", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.7943660318851471}]}, {"text": "More specifically, the results indicate that segmentation based on chunking and the label aggregation based on plurality when used together (i.e., SC-S2-R1) achieve higher precision, accuracy, and F-measure than the punctuation-based segmentation and the other label aggregation rules.", "labels": [], "entities": [{"text": "precision", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9993699193000793}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9990154504776001}, {"text": "F-measure", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9988527297973633}]}, {"text": "shows the output of the three methods on two example sentences.", "labels": [], "entities": []}, {"text": "The underlined words are labeled by the system as scope words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of scope identification using the different  algorithms described in the paper", "labels": [], "entities": [{"text": "scope identification", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.9616217911243439}]}]}