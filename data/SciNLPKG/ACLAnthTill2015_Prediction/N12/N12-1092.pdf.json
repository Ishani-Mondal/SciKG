{"title": [{"text": "Mind the Gap: Learning to Choose Gaps for Question Generation", "labels": [], "entities": [{"text": "Question Generation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8198434710502625}]}], "abstractContent": [{"text": "Not all learning takes place in an educational setting: more and more self-motivated learners are turning to on-line text to learn about new topics.", "labels": [], "entities": []}, {"text": "Our goal is to provide such learners with the well-known benefits of testing by automatically generating quiz questions for on-line text.", "labels": [], "entities": []}, {"text": "Prior work on question generation has focused on the grammaticality of generated questions and generating effective multiple choice distractors for individual question targets, both key parts of this problem.", "labels": [], "entities": [{"text": "question generation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.8326261639595032}]}, {"text": "Our work focuses on the complementary aspect of determining what part of a sentence we should be asking about in the first place; we call this \"gap selection.\"", "labels": [], "entities": []}, {"text": "We address this problem by asking human judges about the quality of questions generated from a Wikipedia-based corpus, and then training a model to effectively replicate these judgments.", "labels": [], "entities": []}, {"text": "Our data shows that good gaps are of variable length and span all semantic roles, i.e., nouns as well as verbs, and that a majority of good questions do not focus on named entities.", "labels": [], "entities": []}, {"text": "Our resulting system can generate fill-in-the-blank (cloze) questions from generic source materials.", "labels": [], "entities": []}], "introductionContent": [{"text": "Assessment is a fundamental part of teaching, both to measure a student's mastery of the material and to identify areas where she may need reinforcement or additional instruction.", "labels": [], "entities": []}, {"text": "Assessment has also been shown an important part of learning, as testing assists retention and can be used to guide learning.", "labels": [], "entities": []}, {"text": "Thus, as learners move on from an educational setting to unstructured self-learning settings, they would still benefit from having the means for assessment available.", "labels": [], "entities": []}, {"text": "Even in traditional educational settings, there is a need for automated test generation, as teachers want multiple tests for topics to give to different students, and students want different tests with which to study and practice the material.", "labels": [], "entities": []}, {"text": "One possible solution to providing quizzes for new source material is the automatic generation of questions.", "labels": [], "entities": []}, {"text": "This is a task the NLP community has already embraced, and significant progress has been made in recent years with the introduction of a shared task (.", "labels": [], "entities": []}, {"text": "However, thus far the research community has focused on the problem of generating grammatical questions (as in) or generating effective distractors for multiple-choice questions (.", "labels": [], "entities": []}, {"text": "While both of these research threads are of critical importance, there is another key issue that must be addressed -which questions should we be asking in the first place?", "labels": [], "entities": []}, {"text": "We have highlighted this aspect of the problem in the past (see) and begin to address it in this work, postulating that we can both collect human judgments on what makes a good question and train a machine learning model that can replicate these judgments.", "labels": [], "entities": []}, {"text": "The resulting learned model can then be applied to new material for automated question generation.", "labels": [], "entities": [{"text": "question generation", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7205561697483063}]}, {"text": "We see this effort as complementary to the earlier progress.", "labels": [], "entities": []}, {"text": "In our approach, we factor the problem of generating good questions into two parts: first, the selection of sentences to ask about, and second, the identification of which part of the resulting sentences the question should address.", "labels": [], "entities": []}, {"text": "Because we want to focus on these aspects of the problem and not the surface form of the questions, we have chosen to generate simple gap-fill (cloze) questions, though our results can also be used to trigger Whquestions or multiple-choice questions by leveraging prior work.", "labels": [], "entities": []}, {"text": "For sentence selection, we turn to methods in summarization and use the simple but effective SumBasic () algorithm to prioritize and choose important sentences from the article.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8142072260379791}, {"text": "summarization", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.9786616563796997}]}, {"text": "We cast the second part, gap selection, as a learning problem.", "labels": [], "entities": [{"text": "gap selection", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.7390840351581573}]}, {"text": "To do this, we first select a corpus of sentences from a very general body of instructional material (a range of popular topics from Wikipedia).", "labels": [], "entities": []}, {"text": "We then generate a constrained subset of all possible gaps via NLP heuristics, and pair each gap with abroad variety of features pertaining to how it was generated.", "labels": [], "entities": []}, {"text": "We then solicit a large number of human judgments via crowdsourcing to help us rate the quality of various gaps.", "labels": [], "entities": []}, {"text": "With that data in hand, we train a machine learning model to replicate these judgments.", "labels": [], "entities": []}, {"text": "The results are promising, with one possible operating point producing a true positive rate of 83% with a corresponding false positive rate of 19% (83% of the possible Good gaps are kept, and 19% of the not-Good gaps are incorrectly marked); see for the full ROC curve and Section 4 for an explanation of the labels.", "labels": [], "entities": [{"text": "ROC curve", "start_pos": 259, "end_pos": 268, "type": "DATASET", "confidence": 0.726533442735672}]}, {"text": "As the final model has only minimal dependence on Wikipedia-specific features, we expect that it can be applied to an even wider variety of material (blogs, news articles, health sites, etc.).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Breakdown of features by category", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9669132828712463}]}, {"text": " Table 2: Example false positives (human judges rated  these as not-Good)", "labels": [], "entities": []}, {"text": " Table 3: Example false negatives (human judges rated  these Good)", "labels": [], "entities": []}]}