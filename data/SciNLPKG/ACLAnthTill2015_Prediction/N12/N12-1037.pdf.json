{"title": [{"text": "Correction Detection and Error Type Selection as an ESL Educational Aid", "labels": [], "entities": [{"text": "Correction Detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7100265026092529}]}], "abstractContent": [{"text": "We present a classifier that discriminates between types of corrections made by teachers of English in student essays.", "labels": [], "entities": []}, {"text": "We define a set of linguistically motivated feature templates fora log-linear classification model, train this classifier on sentence pairs extracted from the Cambridge Learner Corpus, and achieve 89% accuracy improving upon a 33% base-line.", "labels": [], "entities": [{"text": "Cambridge Learner Corpus", "start_pos": 159, "end_pos": 183, "type": "DATASET", "confidence": 0.976793646812439}, {"text": "accuracy", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9992706179618835}]}, {"text": "Furthermore, we incorporate our classi-fier into a novel application that takes as input a set of corrected essays that have been sentence aligned with their originals and outputs the individual corrections classified by error type.", "labels": [], "entities": []}, {"text": "We report the F-Score of our implementation on this task.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9989583492279053}]}], "introductionContent": [{"text": "Ina typical foreign language education classroom setting, teachers are presented with student essays that are often fraught with errors.", "labels": [], "entities": []}, {"text": "These errors can be grammatical, semantic, stylistic, simple spelling errors, etc.", "labels": [], "entities": []}, {"text": "One task of the teacher is to isolate these errors and provide feedback to the student with corrections.", "labels": [], "entities": []}, {"text": "In this body of work, we address the possibility of augmenting this process with NLP tools and techniques, in the spirit of Computer Assisted Language Learning (CALL).", "labels": [], "entities": [{"text": "Computer Assisted Language Learning (CALL)", "start_pos": 124, "end_pos": 166, "type": "TASK", "confidence": 0.6822768322059086}]}, {"text": "We propose a step-wise approach in which a teacher first corrects an essay and then a computer program aligns their output with the original text and separates and classifies independent edits.", "labels": [], "entities": []}, {"text": "With the program's analysis the teacher would be provided accurate information that could be used in effective lesson planning tailored to the students' strengths and weaknesses.", "labels": [], "entities": []}, {"text": "This suggests a novel NLP task with two components: The first isolates individual corrections made by the teacher, and the second classifies these corrections into error types that the teacher would find useful.", "labels": [], "entities": []}, {"text": "A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC)).", "labels": [], "entities": [{"text": "Cambridge Learner Corpus (CLC))", "start_pos": 53, "end_pos": 84, "type": "DATASET", "confidence": 0.9488162795702616}]}, {"text": "The CLC contains approximately 1200 essays with error corrections annotated in XML within sentences.", "labels": [], "entities": []}, {"text": "Furthermore, these corrections are tagged with linguistically motivated error type codes.", "labels": [], "entities": []}, {"text": "To the best of our knowledge our proposed task is unexplored in previous work.", "labels": [], "entities": []}, {"text": "However, there is a significant amount of related work in automated grammatical error correction ().", "labels": [], "entities": [{"text": "automated grammatical error correction", "start_pos": 58, "end_pos": 96, "type": "TASK", "confidence": 0.6835145503282547}]}, {"text": "The Helping Our Own (HOO) shared task ( also explores this issue, with as the best performing system to date.", "labels": [], "entities": [{"text": "Helping Our Own (HOO) shared task", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.666702140122652}]}, {"text": "While often addressing the problem of error type selection directly, previous work has dealt with the more obviously useful task of end to end error detection and correction.", "labels": [], "entities": [{"text": "error type selection", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.6196238795916239}, {"text": "end to end error detection and correction", "start_pos": 132, "end_pos": 173, "type": "TASK", "confidence": 0.6637514574187142}]}, {"text": "As such, their classification systems are crippled by poor recall of errors as well as the lack of information from the corrected sentence and yield very low accuracies for error detection and type selection, e.g. Gamon (2011).", "labels": [], "entities": [{"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9945263266563416}, {"text": "error detection", "start_pos": 173, "end_pos": 188, "type": "TASK", "confidence": 0.6505338251590729}, {"text": "type selection", "start_pos": 193, "end_pos": 207, "type": "TASK", "confidence": 0.9477945566177368}]}, {"text": "Our task is fundamentally different as we assume the presence of both the original and corrected text.", "labels": [], "entities": []}, {"text": "While the utility of such a system is not as obvious as full error correction, we note two possible applications of our technique.", "labels": [], "entities": [{"text": "error correction", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7402051091194153}]}, {"text": "The first, mentioned above, is as an analytical tool for language teachers.", "labels": [], "entities": []}, {"text": "The second is as a complementary tool for automated error correction systems themselves.", "labels": [], "entities": [{"text": "automated error correction", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.6082873344421387}]}, {"text": "Just as tools such as BLAST (Stymne, 2011) are useful in the development of machine translation systems, our system can produce accurate summaries of the corrections made by automated systems even if the systems themselves do not involve such fine grained error type analysis.", "labels": [], "entities": [{"text": "BLAST", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.8450693488121033}, {"text": "machine translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7783325612545013}]}, {"text": "In the following, we describe our experimental methodology (Section 2) and then discuss the feature set we employ for classification (Section 3) and its performance.", "labels": [], "entities": []}, {"text": "Next, we outline our application (Section 4), its heuristic correction detection strategy and empirical evaluation.", "labels": [], "entities": [{"text": "heuristic correction detection", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.6974564095338186}]}, {"text": "We finish by discussing the implications for real world systems (Section 5) and avenues for improvement.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform five-fold cross-validation and achieve a classification accuracy of 88.9% for the 15 class problem and 83.8% for the full 75 class problem.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9316924214363098}]}, {"text": "The accuracies of the most common class baselines are 33.3% and 7.8% respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9882389307022095}]}, {"text": "The most common confusion in the 15 class case is between D (Derivation), R (Replacement) and S (Spelling).", "labels": [], "entities": []}, {"text": "These are mainly due to context-sensitive spelling corrections falling into the Replace category or noise in the mark-up of derivation errors.", "labels": [], "entities": [{"text": "context-sensitive spelling corrections", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.5504876573880514}]}, {"text": "For the 75 class case the most common confusion is between agreement of noun (AGN) and form of noun.", "labels": [], "entities": [{"text": "agreement of noun (AGN)", "start_pos": 59, "end_pos": 82, "type": "METRIC", "confidence": 0.8329495290915171}]}, {"text": "This is unsurprising as we do not incorporate long distance features which would encode agreement.", "labels": [], "entities": []}, {"text": "To check against over-fitting we performed an experiment where we takeaway the strongly lexicalized features (such as \"word w is inserted\") and observed a reduction from 88.9% to 82.4% for 15 class classification accuracy.", "labels": [], "entities": [{"text": "15 class classification", "start_pos": 189, "end_pos": 212, "type": "TASK", "confidence": 0.6353394985198975}, {"text": "accuracy", "start_pos": 213, "end_pos": 221, "type": "METRIC", "confidence": 0.8562130928039551}]}, {"text": "The lack of a dramatic reduction demonstrates the generalization power of our feature templates.", "labels": [], "entities": []}], "tableCaptions": []}