{"title": [], "abstractContent": [{"text": "Standard entity clustering systems commonly rely on mention (string) matching, syntactic features, and linguistic resources like English WordNet.", "labels": [], "entities": [{"text": "mention (string) matching", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.6617152035236359}]}, {"text": "When co-referent text mentions appear in different languages, these techniques cannot be easily applied.", "labels": [], "entities": []}, {"text": "Consequently, we develop new methods for clustering text mentions across documents and languages simultaneously , producing cross-lingual entity clusters.", "labels": [], "entities": [{"text": "clustering text mentions across documents and languages", "start_pos": 41, "end_pos": 96, "type": "TASK", "confidence": 0.8458212443760463}]}, {"text": "Our approach extends standard clustering algorithms with cross-lingual mention and context similarity measures.", "labels": [], "entities": []}, {"text": "Crucially, we do not assume a pre-existing entity list (knowledge base), so entity characteristics are unknown.", "labels": [], "entities": []}, {"text": "On an Arabic-English corpus that contains seven different text genres, our best model yields a 24.3% F1 gain over the baseline.", "labels": [], "entities": [{"text": "F1", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9995643496513367}]}], "introductionContent": [{"text": "This paper introduces techniques for clustering coreferent text mentions across documents and languages.", "labels": [], "entities": [{"text": "clustering coreferent text mentions across documents and languages", "start_pos": 37, "end_pos": 103, "type": "TASK", "confidence": 0.8474492877721786}]}, {"text": "On the web today, a breaking news item may instantly result in mentions to a real-world entity in multiple text formats: news articles, blog posts, tweets, etc.", "labels": [], "entities": []}, {"text": "Much NLP work has focused on model adaptation to these diverse text genres.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.753448873758316}]}, {"text": "However, the diversity of languages in which the mentions appear is a more significant challenge.", "labels": [], "entities": []}, {"text": "This was particularly evident during the 2011 popular uprisings in the Arab world, in which electronic media played a prominent role.", "labels": [], "entities": []}, {"text": "A key issue for the outside world was the aggregation of information that appeared simultaneously in English, French, and various Arabic dialects.", "labels": [], "entities": []}, {"text": "To our knowledge, we are the first to consider clustering entity mentions across languages without a priori knowledge of the quantity or types of real-world entities (a knowledge base).", "labels": [], "entities": [{"text": "clustering entity mentions across languages", "start_pos": 47, "end_pos": 90, "type": "TASK", "confidence": 0.8796108841896058}]}, {"text": "The cross-lingual setting introduces several challenges.", "labels": [], "entities": []}, {"text": "First, we cannot assume a prototypical name format.", "labels": [], "entities": []}, {"text": "For example, the Anglo-centric first/middle/last prototype used in previous name modeling) does not apply to Arabic names like Abdullah ibn Abd Al-Aziz Al-Saud or Chinese names like Hu Jintao (referred to as Mr. Hu, not Mr. Jintao).", "labels": [], "entities": []}, {"text": "Second, organization names often require both transliteration and translation.", "labels": [], "entities": []}, {"text": "For example, the Arabic 'General Motors Corp' contains transliterations of 'General Motors', but a translation of 'Corporation'.", "labels": [], "entities": [{"text": "General Motors Corp'", "start_pos": 25, "end_pos": 45, "type": "DATASET", "confidence": 0.7849842831492424}]}, {"text": "Our models are organized as a pipeline.", "labels": [], "entities": []}, {"text": "First, for each document, we perform standard mention detection and coreference resolution.", "labels": [], "entities": [{"text": "standard mention detection", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.6251050333182017}, {"text": "coreference resolution", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.9647852778434753}]}, {"text": "Then, we use pairwise cross-lingual similarity models to measure both mention and context similarity.", "labels": [], "entities": []}, {"text": "Finally, we cluster the mentions based on similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9598519802093506}]}, {"text": "Our work makes the following contributions: (1) introduction of the task, (2) novel models for crosslingual entity clustering of person and organization entities, (3) cross-lingual annotation of the NIST Automatic Content Extraction (ACE) 2008 Arabic-English evaluation set, and (4) experimental results using both gold and automatic within-document processing.", "labels": [], "entities": [{"text": "crosslingual entity clustering of person and organization entities", "start_pos": 95, "end_pos": 161, "type": "TASK", "confidence": 0.8039410784840584}, {"text": "NIST Automatic Content Extraction (ACE) 2008 Arabic-English evaluation set", "start_pos": 199, "end_pos": 273, "type": "DATASET", "confidence": 0.654562538320368}]}, {"text": "We will release our software and annotations to support future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task, which appeared in ACE2008 (.", "labels": [], "entities": [{"text": "cross-document coreference resolution task", "start_pos": 69, "end_pos": 111, "type": "TASK", "confidence": 0.6839060410857201}, {"text": "ACE2008", "start_pos": 131, "end_pos": 138, "type": "DATASET", "confidence": 0.9402989149093628}]}, {"text": "We evaluate name (NAM) mentions for cross-lingual person (PER) and organization (ORG) entities.", "labels": [], "entities": []}, {"text": "Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base).", "labels": [], "entities": []}, {"text": "We report results for both gold and automatic within-document mention detection and coreference resolution.", "labels": [], "entities": [{"text": "within-document mention detection", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.6068834861119589}, {"text": "coreference resolution", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.9695327579975128}]}, {"text": "We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions.", "labels": [], "entities": []}, {"text": "For the gold setting, we report: \u2022 B 3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis and reference clusters.", "labels": [], "entities": [{"text": "Precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9883878827095032}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9984886646270752}]}, {"text": "\u2022 CEAF (): Precision and recall are computed from a maximum bipartite matching between hypothesis and reference clusters.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9582774043083191}, {"text": "Precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9683722853660583}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9984747767448425}]}, {"text": "\u2022 NVI (Reichart and Rappoport, 2009): Information-theoretic measure that utilizes the entropy of the clusters and their mutual information.", "labels": [], "entities": []}, {"text": "Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set.", "labels": [], "entities": [{"text": "normalized VI (NVI)", "start_pos": 63, "end_pos": 82, "type": "METRIC", "confidence": 0.7689918160438538}]}, {"text": "For the automatic setting, we must apply a different metric since the number of system chains may differ from the reference.", "labels": [], "entities": []}, {"text": "We use B 3 sys (, a variant of B 3 that was shown to penalize both twinless reference chains and spurious system chains more fairly.", "labels": [], "entities": []}, {"text": "The automatic evaluation of cross-lingual coreference systems requires annotated showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly.", "labels": [], "entities": []}, {"text": "We performed intrinsic evaluations for both mention and context similarity.", "labels": [], "entities": []}, {"text": "For context similarity, we analyzed mono-lingual entity clustering, which also facilitated comparison to prior work on the ACE2008   evaluation set.", "labels": [], "entities": [{"text": "mono-lingual entity clustering", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.6423619190851847}, {"text": "ACE2008   evaluation set", "start_pos": 123, "end_pos": 147, "type": "DATASET", "confidence": 0.9040109713872274}]}, {"text": "Our main results are for the new task: cross-lingual entity clustering.", "labels": [], "entities": [{"text": "cross-lingual entity clustering", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.6791741847991943}]}, {"text": "Cross-lingual Mention Matching We created a random 80/10/10 (train, development, test) split of the Maxent training corpus and evaluated binary classification accuracy (Tbl.", "labels": [], "entities": [{"text": "Cross-lingual Mention Matching", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6234049598375956}, {"text": "Maxent training corpus", "start_pos": 100, "end_pos": 122, "type": "DATASET", "confidence": 0.7690497835477194}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9369131922721863}]}, {"text": "4: Cross-lingual entity clustering (test set, gold within-document processing).", "labels": [], "entities": [{"text": "Cross-lingual entity clustering", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6695877810319265}]}, {"text": "B 3 target is the standard B 3 metric applied to the subset of target cross-lingual entities in the test set.", "labels": [], "entities": []}, {"text": "For CEAF and B 3 , SSS is the stronger baseline due to the high proportion of singleton entities in the corpus.", "labels": [], "entities": [{"text": "CEAF and B 3", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8202484250068665}]}, {"text": "Of course, cross-lingual entities have at least two chains, so N--is a better baseline for cross-lingual clustering.", "labels": [], "entities": [{"text": "cross-lingual clustering", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.7518134117126465}]}, {"text": "Mono-lingual Entity Clustering For comparison, we also evaluated our system on a standard monolingual cross-document coreference task (Arabic and English) (Tbl. 5).", "labels": [], "entities": [{"text": "Tbl. 5", "start_pos": 156, "end_pos": 162, "type": "DATASET", "confidence": 0.936195969581604}]}, {"text": "We configured the system with HAC clustering and Jaro-Winkler (within-language) mention similarity.", "labels": [], "entities": [{"text": "HAC clustering", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.4803917855024338}]}, {"text": "We built mono-lingual ELMs for context similarity.", "labels": [], "entities": [{"text": "context similarity", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.703411728143692}]}, {"text": "We used two baselines: \u2022 SSS: E = C, i.e., the cross-lingual clustering solution is just the set of mono-lingual coreference chains.", "labels": [], "entities": []}, {"text": "This is a common baseline for mono-lingual entity clustering ().", "labels": [], "entities": [{"text": "mono-lingual entity clustering", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.6583558221658071}]}, {"text": "\u2022 N--: We run HAC with \u03c1 = \u221e.", "labels": [], "entities": []}, {"text": "Therefore, E is the set of fully-connected components in C subject to the pairwise constraints.", "labels": [], "entities": []}, {"text": "For HAC, we manually tuned the stop threshold \u03b3, the Jaro-Winkler threshold \u03b7, and the ELM smoothing parameter \u03c1 on the development set.", "labels": [], "entities": [{"text": "HAC", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8883321285247803}, {"text": "stop threshold \u03b3", "start_pos": 31, "end_pos": 47, "type": "METRIC", "confidence": 0.9144670764605204}, {"text": "ELM smoothing parameter \u03c1", "start_pos": 87, "end_pos": 112, "type": "METRIC", "confidence": 0.8308869153261185}]}, {"text": "For the DPMM, no development tuning was necessary, and we evaluated a single sample of E taken after 3,000 iterations.", "labels": [], "entities": [{"text": "DPMM", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.8299185633659363}]}, {"text": "To our knowledge, reported the only previous results on the ACE2008 data set.", "labels": [], "entities": [{"text": "ACE2008 data set", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.9916733105977377}]}, {"text": "However, they only gave gold results for English, and clustered the entire evaluation corpus (test+development).", "labels": [], "entities": []}, {"text": "To control for the effect of within-document errors, we considered their gold input (mention detection and within-document coreference resolution) results.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.6783217787742615}, {"text": "within-document coreference resolution)", "start_pos": 107, "end_pos": 146, "type": "TASK", "confidence": 0.6897709220647812}]}, {"text": "They reported B 3 for the two entity types separately: ORG (91.5% F1) and PER (94.3% F1  the two systems are at least in the same range.", "labels": [], "entities": [{"text": "B 3", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9865340292453766}, {"text": "ORG", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9953509569168091}, {"text": "F1", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9261622428894043}, {"text": "PER", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9858696460723877}, {"text": "F1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9853222370147705}]}], "tableCaptions": [{"text": " Table 3: ACE2008 evaluation corpus PER and ORG entity  statistics. Singleton chains account for 51.4% of the Arabic  data and 46.2% of the English data. Just 216 entities appear  in both languages.", "labels": [], "entities": [{"text": "ACE2008 evaluation corpus PER", "start_pos": 10, "end_pos": 39, "type": "DATASET", "confidence": 0.7830122262239456}, {"text": "ORG", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9797148108482361}]}, {"text": " Table 4: Cross-lingual mention matching accuracy [%].  The training data contains names from three genres: broad- cast news (bn), newswire (nw), and weblog (wb). We used  the full training corpus (all) for the cross-lingual clustering  experiments, but the model achieved high accuracy with  significantly fewer training examples (e.g., bn).", "labels": [], "entities": [{"text": "Cross-lingual mention matching", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7698970238367716}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.8912389278411865}, {"text": "accuracy", "start_pos": 278, "end_pos": 286, "type": "METRIC", "confidence": 0.9974944591522217}]}, {"text": " Table 5: Mono-lingual entity clustering evaluation (test  set, gold within-document processing). Higher scores (\u2191)  are better for CEAF and B 3 , whereas lower (\u2193) is better  for NVI. #gold indicates the number of reference entities,  whereas #hyp is the size of E.", "labels": [], "entities": [{"text": "Mono-lingual entity clustering evaluation", "start_pos": 10, "end_pos": 51, "type": "TASK", "confidence": 0.7165339589118958}, {"text": "CEAF", "start_pos": 132, "end_pos": 136, "type": "DATASET", "confidence": 0.5940122604370117}]}, {"text": " Table 6: Cross-lingual entity clustering (test set, gold within-document processing). B 3  target is the standard B 3 metric  applied to the subset of target cross-lingual entities in the test set. For CEAF and B 3 , SSS is the stronger baseline  due to the high proportion of singleton entities in the corpus. Of course, cross-lingual entities have at least two chains,  so N--is a better baseline for cross-lingual clustering.", "labels": [], "entities": [{"text": "Cross-lingual entity clustering", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.734728217124939}, {"text": "CEAF", "start_pos": 203, "end_pos": 207, "type": "DATASET", "confidence": 0.935919463634491}, {"text": "cross-lingual clustering", "start_pos": 404, "end_pos": 428, "type": "TASK", "confidence": 0.7272980809211731}]}]}