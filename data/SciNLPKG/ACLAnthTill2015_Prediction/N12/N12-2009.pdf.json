{"title": [{"text": "Deep Unsupervised Feature Learning for Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "Statistical natural language processing (NLP) builds models of language based on statistical features extracted from the input text.", "labels": [], "entities": [{"text": "Statistical natural language processing (NLP)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7599832543304988}]}, {"text": "We investigate deep learning methods for unsupervised feature learning for NLP tasks.", "labels": [], "entities": []}, {"text": "Recent results indicate that features learned using deep learning methods are not a silver bullet and do not always lead to improved results.", "labels": [], "entities": []}, {"text": "In this work we hypothesise that this is the result of a disjoint training protocol which results in mismatched word representations and classifiers.", "labels": [], "entities": []}, {"text": "We also hypothesise that modelling long-range dependencies in the input and (separately) in the output layers would further improve performance.", "labels": [], "entities": []}, {"text": "We suggest methods for overcoming these limitations, which will form part of our final thesis work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing (NLP) can be seen as building models h : X \u2192 Y for mapping an input encoding x \u2208 X representing a natural language (NL) fragment, to an output encoding y \u2208 Y representing some constructor formalism used in the particular NLP task of interest, e.g. part-of-speech (POS) tags, begin-, inside-, outside (BIO) tags for information extraction, semantic role labels, etc.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8037435710430145}, {"text": "information extraction", "start_pos": 343, "end_pos": 365, "type": "TASK", "confidence": 0.7672238349914551}]}, {"text": "Since the 90s, the predominant approach has been statistical NLP, where one models the problem as learning a predictive function h for mapping from h : X \u2192 Y using machine learning techniques.", "labels": [], "entities": [{"text": "statistical NLP", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.6382813453674316}]}, {"text": "Machine learning consists of a hypothesis function which learns this mapping based on latent or explicit features extracted from the input data.", "labels": [], "entities": []}, {"text": "In this framework, h is usually trained in a supervised setting from labelled training pairs (x i , y i ) \u2208 X \u00d7 Y.", "labels": [], "entities": []}, {"text": "Additionally, the discriminant function h typically operates on a transformed representation of the data to a common feature space encoded as a feature vector \u03c6(x), and then learns a mapping from feature space to the output space, h : \u03c6(x) \u2192 y.", "labels": [], "entities": []}, {"text": "In supervised learning, the idea: Example NLP syntactic chunking task for the sentence \"the cat sits on the mat\".", "labels": [], "entities": [{"text": "NLP syntactic chunking", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.5640763938426971}]}, {"text": "X represents the words in the input space, Y represents labels in the output space.", "labels": [], "entities": []}, {"text": "\u03c6( x) is a feature representation for the input text x and the bottom row represents the output named entity tags in a more standard form. is generally that features represent strong discriminating characteristics of the problem gained through manual engineering and domain-specific insight.", "labels": [], "entities": []}, {"text": "As a concrete example, consider the task of syntactic chunking, also called \"shallow parsing\", (): Given an input string, e.g. \"the cat sits on the mat\", the chunking problem consists of labelling segments of a sentence with syntactic constituents such as noun or verb phrases (NPs or VPs).", "labels": [], "entities": [{"text": "syntactic chunking", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7416006028652191}]}, {"text": "Each word is assigned one unique tag often encoded using the BIO encoding . We represent the input text as a vector of words xi \u2208 x, and each word's corresponding label is represented by y i \u2208 y (see).", "labels": [], "entities": []}, {"text": "Given a feature generating function \u03c6(x i ) and a set of labelled training pairs (x i , y i ) \u2208 X \u00d7 Y, the task then reduces to learning a suitable mapping h : \u03c6(X ) \u2192 Y.", "labels": [], "entities": []}, {"text": "Most previous works have focused on manually engineered features and simpler, linear models, including \"shallow\" model architectures, like the perceptron, linear SVM () and linear-chain conditional random fields (CRFs)).", "labels": [], "entities": []}, {"text": "However, a shallow learning architecture is only as good as its input features.", "labels": [], "entities": []}, {"text": "Due to the complex nature of NL, deeper architectures maybe re-quired to learn data representations which contain the appropriate level of information for the task at hand.", "labels": [], "entities": []}, {"text": "Prior to 2006, it was computationally infeasible to perform inference in hierarchical (\"deep\"), non-linear models such as multi-layer perceptrons with more than one hidden layer.", "labels": [], "entities": []}, {"text": "However, proposed an efficient, layer-wise greedy method for learning the model parameters in these architectures, which spurred a renewed interest in deep learning research.", "labels": [], "entities": []}, {"text": "Still, creating annotated training data is labourintensive and costly, and manually designing and extracting discriminating features from the data to be used in the learning process is a costly procedure requiring significant levels of domain expertise.", "labels": [], "entities": []}, {"text": "Over the last two decades, the growth of available unlabeled data x \u2208 X and the ubiquity of scalable computing power has shifted research focus to unsupervised approaches for automatically learning appropriate feature representations \u03c6(x) from large collections of unlabeled text.", "labels": [], "entities": []}, {"text": "Several methods have been proposed for unsupervised feature learning, including simple k-means clustering, Brown clustering (, mutual information, principal components analysis (PCA), and independent component analysis (ICA)).", "labels": [], "entities": [{"text": "independent component analysis (ICA))", "start_pos": 188, "end_pos": 225, "type": "TASK", "confidence": 0.7527003784974416}]}, {"text": "However, natural language has complex mappings from text to meaning, arguably involving higher-order correlations between words which these simpler methods struggle to model adequately.", "labels": [], "entities": []}, {"text": "Advances in the \"deep learning\" community allow us to perform efficient unsupervised feature learning in highly complex and highdimensional input feature spaces, making it an attractive method for learning features in e.g. vision or language.", "labels": [], "entities": []}, {"text": "The standard deep learning approach is to learn lower-dimensional embeddings from the raw highdimensional 2 input space X to lower dimensional (e.g. 50-dimensional) feature spaces in an unsupervised manner, via repeated, layer-wise, non-linear transformation of the input features, e.g. where f (i) (x) is some non-linear function (typically tanh) for which the parameters are learned by back propagating error gradients.", "labels": [], "entities": []}, {"text": "This configuration is referred to as a \"deep\" architecture with k layers (see for an example).", "labels": [], "entities": []}, {"text": "For feature generation, we present a trained network with anew vector x representing the input data on its input layer.", "labels": [], "entities": [{"text": "feature generation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7190627157688141}]}, {"text": "After performing one iteration of forwardpropagation through the network, we can then view the activation values in the hidden layers as dense, so-called \"distributed representations\" (features) of the input data.", "labels": [], "entities": []}, {"text": "These features can in turn be passed to an output classifier layer to produce some tagging task of interest.", "labels": [], "entities": []}, {"text": "Recent work in deep learning show state-of-the-art results in part-of-speech parsing, chunking and named-entity tagging, however performance in more complex NLP tasks like entity and event disambiguation and semantic role labelling are still trailing behind.", "labels": [], "entities": [{"text": "part-of-speech parsing", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7080599814653397}, {"text": "named-entity tagging", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.7248563468456268}, {"text": "entity and event disambiguation", "start_pos": 172, "end_pos": 203, "type": "TASK", "confidence": 0.6236706003546715}, {"text": "semantic role labelling", "start_pos": 208, "end_pos": 231, "type": "TASK", "confidence": 0.6432544688383738}]}, {"text": "In this work we focus specifically on extending current state of the art deep neural models to improve their performance on these more difficult tasks.", "labels": [], "entities": []}, {"text": "In the following section we briefly review and discuss the merits and limitations of three of the current state of the art deep learning models for NLP.", "labels": [], "entities": []}, {"text": "We then identify our primary research questions and introduce our proposed future work roadmap.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Final NER F1 results reported by Turian (2010).", "labels": [], "entities": [{"text": "NER", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.6754603385925293}, {"text": "F1", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.5063599348068237}]}]}