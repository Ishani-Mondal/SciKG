{"title": [{"text": "A Comparative Investigation of Morphological Language Modeling for the Languages of the European Union", "labels": [], "entities": [{"text": "Morphological Language Modeling", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.7531309326489767}]}], "abstractContent": [{"text": "We investigate a language model that combines morphological and shape features with a Kneser-Ney model and test it in a large crosslingual study of European languages.", "labels": [], "entities": []}, {"text": "Even though the model is generic and we use the same architecture and features for all languages , the model achieves reductions in per-plexity for all 21 languages represented in the Europarl corpus, ranging from 3% to 11%.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 184, "end_pos": 199, "type": "DATASET", "confidence": 0.9890406429767609}]}, {"text": "We show that almost all of this perplexity reduction can be achieved by identifying suffixes by frequency.", "labels": [], "entities": [{"text": "perplexity reduction", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.696472704410553}]}], "introductionContent": [{"text": "Language models are fundamental to many natural language processing applications.", "labels": [], "entities": []}, {"text": "In the most common approach, language models estimate the probability of the next word based on one or more equivalence classes that the history of preceding words is a member of.", "labels": [], "entities": []}, {"text": "The inherent productivity of natural language poses a problem in this regard because the history maybe rare or unseen or have unusual properties that make assignment to a predictive equivalence class difficult.", "labels": [], "entities": []}, {"text": "In many languages, morphology is a key source of productivity that gives rise to rare and unseen histories.", "labels": [], "entities": []}, {"text": "For example, even if a model can learn that words like \"large\", \"dangerous\" and \"serious\" are likely to occur after the relatively frequent history \"potentially\", this knowledge cannot be transferred to the rare history \"hypothetically\" without some generalization mechanism like morphological analysis.", "labels": [], "entities": []}, {"text": "Our primary goal in this paper is not to develop optimized language models for individual languages.", "labels": [], "entities": []}, {"text": "Instead, we investigate whether a simple generic language model that uses shape and morphological features can be made to work well across a large number of languages.", "labels": [], "entities": []}, {"text": "We find that this is the case: we achieve considerable perplexity reductions for all 21 languages in the Europarl corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 105, "end_pos": 120, "type": "DATASET", "confidence": 0.9883845746517181}]}, {"text": "We see this as evidence that morphological language modeling should be considered as a standard part of any language model, even for languages like English that are often not viewed as a good application of morphological modeling due to their morphological simplicity.", "labels": [], "entities": [{"text": "morphological language modeling", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.646234929561615}]}, {"text": "To understand which factors are important for good performance of the morphological component of a language model, we perform an extensive crosslingual analysis of our experimental results.", "labels": [], "entities": []}, {"text": "We look at three parameters of the morphological model we propose: the frequency threshold \u03b8 that divides words subject to morphological clustering from those that are not; the number of suffixes used \u03c6; and three different morphological segmentation algorithms.", "labels": [], "entities": []}, {"text": "We also investigate the differential effect of morphological language modeling on different word shapes: alphabetical words, punctuation, numbers and other shapes.", "labels": [], "entities": []}, {"text": "Some prior work has used morphological models that require careful linguistic analysis and languagedependent adaptation.", "labels": [], "entities": []}, {"text": "In this paper we show that simple frequency analysis performs only slightly worse than more sophisticated morphological analysis.", "labels": [], "entities": []}, {"text": "This potentially removes a hurdle to using morphological models in cases where sufficient resources to do the extra work required for sophisticated morphological analysis are not available.", "labels": [], "entities": []}, {"text": "The motivation for using morphology in language modeling is similar to distributional clustering.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7026585638523102}, {"text": "distributional clustering", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.7190267443656921}]}, {"text": "In both cases, we form equivalence classes of words with similar distributional behavior.", "labels": [], "entities": []}, {"text": "Ina preliminary experiment, we find that morphological equivalence classes reduce perplexity as much as traditional distributional classes -a surprising result we intend to investigate in future work.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "We present a language model design and a set of morphological and shape features that achieve reductions in perplexity for all 21 languages represented in the Europarl corpus, ranging from 3% to 11%, compared to a Kneser-Ney model.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 159, "end_pos": 174, "type": "DATASET", "confidence": 0.9908822774887085}]}, {"text": "We show that identifying suffixes by frequency is sufficient forgetting almost all of this perplexity reduction.", "labels": [], "entities": []}, {"text": "More sophisticated morphological segmentation methods do not further increase perplexity or just slightly.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.6870379149913788}]}, {"text": "Finally, we show that there is one parameter that must be tuned for good performance for most languages: the frequency threshold \u03b8 above which a word is not subject to morphological generalization because it occurs frequently enough for standard word n-gram language models to use it effectively for prediction.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we discuss related work.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the morphological and shape features we use.", "labels": [], "entities": []}, {"text": "Section 4 introduces language model and experimental setup.", "labels": [], "entities": []}, {"text": "Section 5 discusses our results.", "labels": [], "entities": []}, {"text": "Section 6 summarizes the contributions of this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments are performed using srilm), in particular the Kneser-Ney (KN) and generic class model implementations.", "labels": [], "entities": []}, {"text": "Estimation of optimal interpolation parameters is based on ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics for the 21 languages. S = Slavic, G  = Germanic, E = Greek, R = Romance, U = Uralic, B  = Baltic. Type/token ratio (T/T) and # sentences for the  training set and OOV rate for the validation set. The  two smallest and largest values in each column are bold.", "labels": [], "entities": [{"text": "Type/token ratio", "start_pos": 119, "end_pos": 135, "type": "METRIC", "confidence": 0.8280817121267319}, {"text": "OOV rate", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.986647754907608}, {"text": "validation", "start_pos": 201, "end_pos": 211, "type": "TASK", "confidence": 0.9701588749885559}]}]}