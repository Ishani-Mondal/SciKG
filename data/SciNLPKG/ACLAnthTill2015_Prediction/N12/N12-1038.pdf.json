{"title": [], "abstractContent": [{"text": "We introduce anew segmentation evaluation measure, WinPR, which resolves some of the limitations of WindowDiff.", "labels": [], "entities": []}, {"text": "WinPR distinguishes between false positive and false negative errors; produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses.", "labels": [], "entities": [{"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9994127750396729}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9929341077804565}, {"text": "F-measure", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9986900687217712}]}], "introductionContent": [{"text": "WindowDiff () has become the most frequently used measure to evaluate segmentation.", "labels": [], "entities": []}, {"text": "Segmentation is the task of dividing a stream of data (text or other media) into coherent units.", "labels": [], "entities": [{"text": "Segmentation is the task of dividing a stream of data (text or other media) into coherent units", "start_pos": 0, "end_pos": 95, "type": "Description", "confidence": 0.7464085864393335}]}, {"text": "These units maybe motivated topically (), structurally, or visually (, depending on the domain and task.", "labels": [], "entities": []}, {"text": "Segmentation evaluation is difficult because exact comparison of boundaries is too strict; a partial reward is required for close boundaries.", "labels": [], "entities": [{"text": "Segmentation evaluation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9636047780513763}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: WindowDiff values for examples A to E", "labels": [], "entities": []}, {"text": " Table 2. We will calculate precision and  recall using the WinPR confusion matrix, shown un- der WinP and WinR respectively. You will note that  we can easily see whether an error is a false posi- tive or a false negative. As we would expect, false  positives affect precision, and false negatives affect  recall. Near misses manifest as equal parts false pos- itive and false negative. In example E, each error is  counted, unlike WindowDiff.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9994683861732483}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9990828037261963}, {"text": "precision", "start_pos": 268, "end_pos": 277, "type": "METRIC", "confidence": 0.9984972476959229}, {"text": "recall", "start_pos": 307, "end_pos": 313, "type": "METRIC", "confidence": 0.9963812232017517}, {"text": "WindowDiff", "start_pos": 433, "end_pos": 443, "type": "DATASET", "confidence": 0.9098639488220215}]}, {"text": " Table 2: WinPR values for examples A to E", "labels": [], "entities": [{"text": "WinPR", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.821249783039093}]}]}