{"title": [{"text": "Towards Effective Tutorial Feedback for Explanation Questions: A Dataset and Baselines", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose anew shared task on grading student answers with the goal of enabling well-targeted and flexible feedback in a tutorial dialogue setting.", "labels": [], "entities": []}, {"text": "We provide an annotated corpus designed for the purpose, a precise specification fora prediction task and an associated evaluation methodology.", "labels": [], "entities": []}, {"text": "The task is feasible but non-trivial, which is demonstrated by creating and comparing three alternative baseline systems.", "labels": [], "entities": []}, {"text": "We believe that this corpus will be of interest to the researchers working in tex-tual entailment and will stimulate new developments both in natural language processing in tutorial dialogue systems and textual entail-ment, contradiction detection and other techniques of interest fora variety of computational linguistics tasks.", "labels": [], "entities": [{"text": "tex-tual entailment", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.6700624525547028}, {"text": "contradiction detection", "start_pos": 224, "end_pos": 247, "type": "TASK", "confidence": 0.7431319952011108}]}], "introductionContent": [{"text": "In human-human tutoring, it is an effective strategy to ask students to explain instructional material in their own words.", "labels": [], "entities": []}, {"text": "Self-explanation ( and contentful talk focused on the domain are correlated with better learning outcomes (.", "labels": [], "entities": []}, {"text": "There has therefore been much interest in developing automated tutorial dialogue systems that ask students open-ended explanation questions ().", "labels": [], "entities": []}, {"text": "In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content.", "labels": [], "entities": []}, {"text": "Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback ().", "labels": [], "entities": []}, {"text": "In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to produce detailed semantic representations of student input).", "labels": [], "entities": []}, {"text": "The advantage of this approach is that it allows for flexible adaptation of feedback to a variety of factors such as student performance.", "labels": [], "entities": []}, {"text": "For example, it is easy for the system to know if the student made the same error before, and adjust its feedback to reflect it.", "labels": [], "entities": []}, {"text": "Moreover, this approach allows for easy addition of new exercises : as long as an exercise relies on the concepts covered by the domain model, the system can apply standard instructional strategies to each new question automatically.", "labels": [], "entities": []}, {"text": "However, this approach is significantly limited by the requirement that the domain be small enough to allow comprehensive knowledge engineering, and it is very labor-intensive even for small domains.", "labels": [], "entities": [{"text": "comprehensive knowledge engineering", "start_pos": 108, "end_pos": 143, "type": "TASK", "confidence": 0.6344574292500814}]}, {"text": "Alternatively, we can adopt a data-driven approach, asking human tutors to anticipate in advance a range of possible correct and incorrect answers, and associating each answer with an appropriate remediation (.", "labels": [], "entities": []}, {"text": "The advantage of this approach is that it allows more complex and interesting domains and provides a good framework for eliciting the necessary information from the human experts.", "labels": [], "entities": []}, {"text": "A weakness of this approach, which also arises in content-scoring applications such as ETS's c-rater (, is that human experts find it extremely difficult to predict with any certainty what the full range of student responses will be.", "labels": [], "entities": [{"text": "ETS's c-rater", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.893055260181427}]}, {"text": "This leads to alack of adaptivity and generality -if the system designers have failed to predict the full range of possibilities, students will often receive the default feedback.", "labels": [], "entities": []}, {"text": "It is frustrating and confusing for students to repeatedly receive the same feedback, regardless of their past performance or dialogue context.", "labels": [], "entities": []}, {"text": "Our goal is to address the weaknesses of the datadriven approach by creating a framework for supporting more flexible and systematic feedback.", "labels": [], "entities": []}, {"text": "Our approach identifies general classes of error, such as omissions, incorrect statements and off-topic statements, then aims to develop general remediation strategies for each error type.", "labels": [], "entities": []}, {"text": "This has the potential to free system designers from the need to pre-author separate remediations for each individual question.", "labels": [], "entities": []}, {"text": "A precondition for the success of this approach is that the system be able to identify error types based on the student response and the model answers.", "labels": [], "entities": []}, {"text": "A contribution of this paper is to provide anew dataset that will enable researchers to develop classifiers specifically for this purpose.", "labels": [], "entities": []}, {"text": "The hope is that with an appropriate dataset the data-driven approach will be flexible and responsive enough to maintain student engagement.", "labels": [], "entities": []}, {"text": "We provide a corpus that is labeled fora set of five student response types, develop a precise definition of the corresponding supervised classification task, and report results fora variety of simple baseline classifiers.", "labels": [], "entities": []}, {"text": "This will provide a basis for the development, comparison and evaluation of alternative approaches to the error classification task.", "labels": [], "entities": [{"text": "error classification task", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.760127047697703}]}, {"text": "We believe that the natural language capabilities needed for this task will be directly applicable to afar wider range of tasks in educational assessment, information extraction and computational semantics.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 155, "end_pos": 177, "type": "TASK", "confidence": 0.8022144138813019}]}, {"text": "This dataset is publicly available and will be used in a community-wide shared task.", "labels": [], "entities": []}], "datasetContent": [{"text": "We held back part of the data set for use as standard test data in the future challenge tasks.", "labels": [], "entities": []}, {"text": "For BEETLE, this consisted of all student answers to 9 out of 56 explanation questions asked by the system, plus approximately 15% of the student answers to the remaining 47 questions, sampling so that the distribution of labels in test data was similar to the training data.", "labels": [], "entities": [{"text": "BEETLE", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.8792182207107544}]}, {"text": "For SCIENTSBANK, we used a previous traintest split).", "labels": [], "entities": []}, {"text": "For both data sets, the data was split so that in the future we can test how well the different systems generalize: i.e., how well they perform on answers to questions for which they have some sample student answers vs. how well they perform on answers to questions that were not in the training data (e.g., newly created questions in a deployed system).", "labels": [], "entities": []}, {"text": "We discuss this in more detail in Section 5.", "labels": [], "entities": []}, {"text": "In this paper, we report baseline performance on the training set to demonstrate that the task is sufficiently challenging to be interesting and that systems can be compared using our evaluation metrics.", "labels": [], "entities": []}, {"text": "We preserve the true test data for use in the planned large-scale system comparisons in a community shared task.", "labels": [], "entities": []}, {"text": "For the lexical similarity baseline, we use 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "For the BEETLE II system baseline, the language understanding module was de-veloped based on eight transcripts, each taken from the interaction of a different student with an earlier version of the system.", "labels": [], "entities": [{"text": "BEETLE II system baseline", "start_pos": 8, "end_pos": 33, "type": "DATASET", "confidence": 0.777204617857933}, {"text": "language understanding", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.7229558080434799}]}, {"text": "These sessions were completed prior to the beginning of the experiment during which the BEETLE corpus was collected, and are not included in the corpus presented here.", "labels": [], "entities": [{"text": "BEETLE corpus", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.9162337183952332}]}, {"text": "Thus, the dataset used in the paper constitutes unseen data for the BEETLE II system.", "labels": [], "entities": [{"text": "BEETLE II system", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.732023557027181}]}, {"text": "We process the two corpora separately because the additional system baseline is available for beetle, and because the corpora maybe different enough that it will be helpful for shared task participants to devise processing strategies that are sensitive to the provenance of the data.", "labels": [], "entities": []}, {"text": "shows the distribution of codes in the annotated data.", "labels": [], "entities": []}, {"text": "The distribution is unbalanced, and therefore in our evaluation results we report per-class precision, recall and F 1 scores, plus the averaged scores using two different ways to average over per-class evaluation scores, micro-and macro-averaging.", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.948563277721405}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9983586668968201}, {"text": "F 1 scores", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.9853522380193075}]}, {"text": "For a set of classes C, each represented with N c instances in the test set, the macro-averaged recall is defined as and the micro-averaged recall as Micro-and macro-averaged precision and F 1 are defined similarly.", "labels": [], "entities": [{"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9072719216346741}, {"text": "micro-averaged recall", "start_pos": 125, "end_pos": 146, "type": "METRIC", "confidence": 0.6998562216758728}, {"text": "Micro-and macro-averaged precision", "start_pos": 150, "end_pos": 184, "type": "METRIC", "confidence": 0.744935135046641}, {"text": "F 1", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9787991344928741}]}, {"text": "Micro-averaging takes class sizes into account, so a system that performs well on the most common classes will have a high micro-average score.", "labels": [], "entities": []}, {"text": "This is the most commonly used classifier evaluation metric.", "labels": [], "entities": []}, {"text": "Note that, in particular, overall classification accuracy (defined as the number of correctly classified instances out of all instances) is mathematically equivalent to micro-averaged recall).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9144172668457031}, {"text": "recall", "start_pos": 184, "end_pos": 190, "type": "METRIC", "confidence": 0.9604120850563049}]}, {"text": "However, macro-averaging better reflects performance on small classes, and is commonly used for unbalanced classification problems (see, e.g., In addition, we report the system scores on the binary decision of whether or not the corrective feedback should be issued (denoted \"corrective feedback\" in the results table).", "labels": [], "entities": []}, {"text": "It assumes that a tutoring system using a classifier will give corrective feedback if the classifiers returns any label other than \"correct\".", "labels": [], "entities": []}, {"text": "Thus, every instance classified as \"partially correct incomplete\", \"contradictory\", \"irrelevant\" or \"non domain\" is counted as true positive if the hand-annotated label also belongs to this set (even if the classifier disagrees with the annotation); and as false positive if the hand-annotated label is \"correct\".", "labels": [], "entities": []}, {"text": "This reflects the idea that students are likely to be frustrated if the system gives corrective feedback when their answer is in fact a fully accurate paraphrase of a correct answer.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of annotated labels in the data", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results for BEETLE corpus", "labels": [], "entities": [{"text": "BEETLE", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9226331114768982}]}, {"text": " Table 3: Evaluation results for SCIENTSBANK baselines", "labels": [], "entities": []}]}