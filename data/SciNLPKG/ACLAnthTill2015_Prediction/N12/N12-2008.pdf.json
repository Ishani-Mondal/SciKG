{"title": [{"text": "Using Ontology-based Approaches to Representing Speech Transcripts for Automated Speech Scoring", "labels": [], "entities": [{"text": "Automated Speech Scoring", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.6653813123703003}]}], "abstractContent": [{"text": "This paper presents a thesis proposal on approaches to automatically scoring non-native speech from second language tests.", "labels": [], "entities": []}, {"text": "Current speech scoring systems assess speech by primarily using acoustic features such as fluency and pronunciation; however content features are barely involved.", "labels": [], "entities": []}, {"text": "Motivated by this limitation , the study aims to investigate the use of content features in speech scoring systems.", "labels": [], "entities": []}, {"text": "For content features, a central question is how speech content can be represented inappropriate means to facilitate automated speech scoring.", "labels": [], "entities": [{"text": "automated speech scoring", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.6240257223447164}]}, {"text": "The study proposes using ontology-based representation to perform concept level representation on speech transcripts, and furthermore the content features computed from ontology-based representation may facilitate speech scoring.", "labels": [], "entities": [{"text": "speech scoring", "start_pos": 214, "end_pos": 228, "type": "TASK", "confidence": 0.7860085666179657}]}, {"text": "One baseline and two ontolo-gy-based representations are compared in experiments.", "labels": [], "entities": []}, {"text": "Preliminary results show that ontology-based representation slightly improves performance of one content feature for automated scoring over the baseline system.", "labels": [], "entities": []}], "introductionContent": [{"text": "With increasing number of language learners taking second language tests, the resulting responses add a huge burden to testing agencies, and thus automated scoring has become a necessity for efficiency and objectivity.", "labels": [], "entities": []}, {"text": "Speaking, an important aspect for assessing second language speakers' proficiency, is selected as the context of the study.", "labels": [], "entities": []}, {"text": "The general goal is to investigate new approaches to automatic scoring of second language speech.", "labels": [], "entities": [{"text": "automatic scoring of second language speech", "start_pos": 53, "end_pos": 96, "type": "TASK", "confidence": 0.8190621336301168}]}, {"text": "When giving a speaking test in computermediated environment, test-takers' responses are typically recorded as speech files.", "labels": [], "entities": []}, {"text": "These files can be considered to contain two layers: sound and text.", "labels": [], "entities": []}, {"text": "The sound is about the acoustic side of speech, whose features have been used to assess speaking proficiency in existing automated speechscoring systems.", "labels": [], "entities": []}, {"text": "However, the text side, which is about the content of speech, is by far not well addressed in scoring systems, mainly due to the imperfect performance of automatic speech recognizer systems.", "labels": [], "entities": []}, {"text": "As content is an integral part of speech, adding content features to existing scoring systems may further enhance system performance, and thus this study aims to examine the use of content features in speech scoring systems.", "labels": [], "entities": []}, {"text": "In order to acquire speech content, speech files need to be transcribed to text files, by human or Automatic Speech Recognition (ASR).", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 99, "end_pos": 133, "type": "TASK", "confidence": 0.7490702271461487}]}, {"text": "The resulted text files, namely, speech transcripts, are to be processed to extract content features.", "labels": [], "entities": []}, {"text": "Moreover, representation of text content (e.g. in vectors) is important because it is the pre-requisite for computing content features and building speech scoring models.", "labels": [], "entities": []}, {"text": "Therefore this study focuses on representing content of speech transcripts to facilitate automatic scoring of speech.", "labels": [], "entities": []}, {"text": "Speech transcripts can be seen as a special type of text documents, and therefore document representation approaches shed light on representation of speech transcripts, such as,,,,,,.", "labels": [], "entities": []}, {"text": "On the other hand, written essays, the output of writing section of second language test, share great similarity with speech transcripts, and representation of essays also has implications on speech transcript representation, such as,, and.", "labels": [], "entities": [{"text": "speech transcript representation", "start_pos": 192, "end_pos": 224, "type": "TASK", "confidence": 0.6568499406178793}]}, {"text": "Existing document representation approaches are primarily statistical and corpus based, using words or latent variables mined from corpus as representation units in the vector.", "labels": [], "entities": []}, {"text": "These approaches exhibit two challenges: 1) meaningfulness of representation units.", "labels": [], "entities": []}, {"text": "For example, synonymous words represent similar meaning and thus should be grouped as one representation unit.", "labels": [], "entities": []}, {"text": "Since words or latent variables in the vector are from training corpus, if an unknown term occurs in the testing corpus then it is difficult to determine the importance of the term in the training corpus because there is no prior knowledge of it in the training corpus.", "labels": [], "entities": []}, {"text": "Ontology concepts, representation units at the concept level, have been less employed in content representation.", "labels": [], "entities": [{"text": "content representation", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.7449203431606293}]}, {"text": "claim that ontology concepts can help reveal concepts and semantics in documents, and thus we hypothesize ontology-based representation may facilitate obtaining better content features for speech scoring.", "labels": [], "entities": [{"text": "speech scoring", "start_pos": 189, "end_pos": 203, "type": "TASK", "confidence": 0.7244386672973633}]}, {"text": "Ontologies can also complement the abovementioned shortcomings of statistical and corpus based representations by providing meaningful representation units and reasoning power between concepts.", "labels": [], "entities": []}, {"text": "The study compares baseline (statistical and corpus based) and ontology-based approaches.", "labels": [], "entities": []}, {"text": "The criterion is representing the same speech transcripts using these approaches, computing content features based on the representations, and comparing performance of content features in predicting speaking proficiency.", "labels": [], "entities": []}], "datasetContent": [{"text": "Concepts from ontology are identified in speech transcripts and then used to generate concept-level vectors.", "labels": [], "entities": []}, {"text": "In practice, concept mapping in transcripts varies according to characteristics of ontologies.", "labels": [], "entities": [{"text": "concept mapping", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.726155087351799}]}, {"text": "The WordNet ontology, containing mostly single words, is used as one casein the study.", "labels": [], "entities": [{"text": "WordNet ontology", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9243063926696777}]}, {"text": "In the future, we plan to try the Wikipedia ontology, which contains more phrasesbased concepts, for ontology-based representation.", "labels": [], "entities": [{"text": "ontology-based representation", "start_pos": 101, "end_pos": 130, "type": "TASK", "confidence": 0.8065107464790344}]}, {"text": "Synsets, groups of synonyms, are concepts in WordNet and used as ontology concepts here.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.936394453048706}]}, {"text": "Document text is split by whitespace and punctuations to a set of words.", "labels": [], "entities": []}, {"text": "Then the words are matched to WordNet synsets.", "labels": [], "entities": [{"text": "WordNet synsets", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.9582608938217163}]}, {"text": "As a word may have multiple senses (synsets), it is necessary to decide which synset to use in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9478378891944885}]}, {"text": "Therefore we try two sense selection strategies as in study: 1) simply use the first sense in WordNet; and 2) do part-of-speech tagging on sentences and find the corresponding sense in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.9608746767044067}, {"text": "part-of-speech tagging", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.7151526212692261}, {"text": "WordNet", "start_pos": 185, "end_pos": 192, "type": "DATASET", "confidence": 0.9759431481361389}]}, {"text": "We find the 1 st strategy obtains better performance than the 2 nd one and thus decide to use the 1 stone.", "labels": [], "entities": []}, {"text": "When constructing ontology-based vector, we include both concepts and words in the vector.", "labels": [], "entities": []}, {"text": "This approach is also implemented by using WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9654033780097961}]}, {"text": "First, transcripts are represented by ontology concepts as in section 3.2.2.", "labels": [], "entities": []}, {"text": "Then given an unknown concept in test transcripts, we identify its semantically similar concepts (N=5) in the training transcripts and then reason the weight of the unknown concept based on the weights of these similar concepts.", "labels": [], "entities": []}, {"text": "The reasoning makes use of semantic similarity between WordNet synsets.", "labels": [], "entities": []}, {"text": "Concept similarity is computed using the edge-based path similarity).", "labels": [], "entities": []}, {"text": "We select N=5 concepts from the training transcripts that are most similar to the unknown concept, and compute the weight of the unknown concept in the training transcripts by averaging the weights of the 5 similar concepts.", "labels": [], "entities": []}, {"text": "Representation approaches are evaluated based on their performance in predicting speaking proficiency of test takers.", "labels": [], "entities": []}, {"text": "More specifically, a representation approach generates a vector representation using specific representation units (e.g. words, concepts); for each test transcript, two content features are computed based on the vector representation; Pearson correlation r is computed between each content feature and speaking proficiency to indicate the predictiveness of the content feature resulting from a specific representation.", "labels": [], "entities": [{"text": "Pearson correlation r", "start_pos": 235, "end_pos": 256, "type": "METRIC", "confidence": 0.9721441666285197}]}, {"text": "Higher correlation indicates higher predictiveness on speaking proficiency.", "labels": [], "entities": []}, {"text": "Lastly, we compare content feature correlations of different representation approaches.", "labels": [], "entities": []}, {"text": "We consider that the higher the correlation is, the better the representation approach is.", "labels": [], "entities": [{"text": "correlation", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9764275550842285}]}, {"text": "In the preliminary stage, the BOW (baseline), ONTO-WordNet and OntoReason-WordNet (experimental) approaches are implemented.", "labels": [], "entities": [{"text": "BOW", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9943313598632812}, {"text": "ONTO-WordNet", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.5858184695243835}]}, {"text": "Meanwhile parameters are optimized to acquire the best parameter setup for each approach.", "labels": [], "entities": []}, {"text": "Since the speech files are transcribed by both human and ASR, same experiments are run on both data sets to compare representation performance on different transcriptions.", "labels": [], "entities": [{"text": "ASR", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.8286923170089722}]}, {"text": "The correlations of the two content features to speaking proficiency are computed for each representation.", "labels": [], "entities": []}, {"text": "show correlations of the max.cos and cos.w4 features respectively: For the max.cos feature, the average correlation of the ONTO-WordNet approach outperforms the BOW baseline slightly but the correlation drops dramatically when using the OntoReason-WordNet approach, for both the human and ASR transcripts.", "labels": [], "entities": [{"text": "BOW", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.8777311444282532}]}, {"text": "For the cos.w4 feature, the average correlation of the ONTO-WordNet approach outperforms the BOW, and the OntoReason-WordNet further outperforms the ONTO-WordNet approach, for both the human and ASR transcripts.", "labels": [], "entities": [{"text": "BOW", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9983175992965698}]}, {"text": "It shows some evidence that ontology-based representation can improve performance of both content features; the ontology-based reasoning increases performance of the cos.w4 feature but decreases the max.cos feature correlation.", "labels": [], "entities": []}, {"text": "Comparing the performance on human vs. ASR transcripts, the features extracted from the human transcripts exhibit better average correlations than the corresponding features from the ASR transcripts.", "labels": [], "entities": [{"text": "ASR transcripts", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.855628103017807}]}, {"text": "The results also show that the correlation difference between human and ASR transcripts is moderate.", "labels": [], "entities": [{"text": "ASR transcripts", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.8967027068138123}]}, {"text": "It may indicate that the representation approaches can be employed on ASR transcripts to further automate the speech scoring process.", "labels": [], "entities": [{"text": "ASR transcripts", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.8945999145507812}, {"text": "speech scoring", "start_pos": 110, "end_pos": 124, "type": "TASK", "confidence": 0.6843767613172531}]}], "tableCaptions": [{"text": " Table 1. Size of data set and subsets. The numbers in  parentheses are the number of documents on score lev- els 1-4.", "labels": [], "entities": []}, {"text": " Table 2. Correlations between the max.cos feature and speaking proficiency (Hum=using human transcriptions;  ASR=using ASR hypotheses).", "labels": [], "entities": [{"text": "Hum", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9283621311187744}, {"text": "ASR", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9385404586791992}]}, {"text": " Table 3. Correlations between the cos.w4 feature and speaking proficiency (Hum=using human transcriptions;  ASR=using ASR hypotheses)", "labels": [], "entities": [{"text": "Hum", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.7938186526298523}, {"text": "ASR", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.8966768383979797}]}]}