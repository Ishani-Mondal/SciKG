{"title": [{"text": "Transliteration Mining Using Large Training and Test Sets", "labels": [], "entities": [{"text": "Transliteration Mining", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9316304922103882}]}], "abstractContent": [{"text": "Much previous work on Transliteration Mining (TM) was conducted on short parallel snippets using limited training data, and successful methods tended to favor recall.", "labels": [], "entities": [{"text": "Transliteration Mining (TM)", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.8909229397773742}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9953004121780396}]}, {"text": "For such methods, increasing training data may impact precision and application on large comparable texts may impact precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9989174604415894}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9994340538978577}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9986588954925537}]}, {"text": "We adapt a state-of-the-art TM technique with the best reported scores on the ACL 2010 NEWS workshop dataset, namely graph reinforcement, to work with large training sets.", "labels": [], "entities": [{"text": "ACL 2010 NEWS workshop dataset", "start_pos": 78, "end_pos": 108, "type": "DATASET", "confidence": 0.9039339303970337}]}, {"text": "The method models observed character mappings between language pairs as a bipartite graph and unseen mappings are induced using random walks.", "labels": [], "entities": []}, {"text": "Increasing training data yields more correct initial mappings but induced mappings become more error prone.", "labels": [], "entities": []}, {"text": "We introduce parameterized exponential penalty to the formulation of graph reinforcement and we estimate the proper parameters for training sets of varying sizes.", "labels": [], "entities": []}, {"text": "The new formulation led to sizable improvements in precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9989438652992249}]}, {"text": "Mining from large comparable texts leads to the presence of phonetically similar words in target and source texts that may not be transliterations or may adversely impact candidate ranking.", "labels": [], "entities": []}, {"text": "To overcome this, we extracted related segments that have high translation overlap, and then we performed TM on them.", "labels": [], "entities": [{"text": "TM", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.8769915103912354}]}, {"text": "Segment extraction produced significantly higher precision for three different TM methods.", "labels": [], "entities": [{"text": "Segment extraction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8725137710571289}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9993736147880554}, {"text": "TM", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9650384187698364}]}], "introductionContent": [{"text": "Transliteration Mining (TM) is the process of finding transliterations in parallel or comparable texts of different languages.", "labels": [], "entities": [{"text": "Transliteration Mining (TM)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9047450065612793}]}, {"text": "For example, given the Arabic-English word sequence pairs: ( \u202b\ufeeb\ufbac\ufbaa\u06be\ufe8e\ufedf\ufef2\u202c \u202b\ufe8d\u0627\ufedf\ufee4\ufee0\ufeda\u202c \u202b,\ufeb3\ufefc\ufeb3\ufef2\u202c Haile Selassie I of Ethiopia), successful TM would mine the transliterations: (\u202b,\ufeeb\ufbac\ufbaa\u06be\ufe8e\ufedf\ufef2\u202c Haile) and (\u202b,\ufeb3\ufefc\ufeb3\ufef2\u202c Selassie).", "labels": [], "entities": [{"text": "TM", "start_pos": 130, "end_pos": 132, "type": "TASK", "confidence": 0.9306858777999878}]}, {"text": "TM has been shown to be effective in several Information Retrieval (IR) and Natural Language Processing (NLP) applications.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.8308825254440307}]}, {"text": "For example, in cross language IR, TM was used to handle out-of-vocabulary query words by mining transliterations between words in queries and top n retrieved documents and then using transliterations to expand queries ().", "labels": [], "entities": [{"text": "cross language IR", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.5052355428536733}]}, {"text": "In Machine Translation (MT), TM can improve alignment at training time and help enrich phrase tables with named entities that may not appear in parallel training data.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.8489852905273437}]}, {"text": "More broadly, TM is a character mapping problem.", "labels": [], "entities": [{"text": "TM", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9745726585388184}, {"text": "character mapping problem", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.889765739440918}]}, {"text": "Having good character mapping models can be beneficial in a variety of applications such as learning stemming models, learning spelling transformations between similar languages, and finding variant spellings of names (.", "labels": [], "entities": [{"text": "character mapping", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7189514935016632}, {"text": "learning stemming", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7761804759502411}, {"text": "learning spelling transformations between similar languages", "start_pos": 118, "end_pos": 177, "type": "TASK", "confidence": 0.7905109028021494}, {"text": "finding variant spellings of names", "start_pos": 183, "end_pos": 217, "type": "TASK", "confidence": 0.777033019065857}]}, {"text": "TM has attracted interest in recent years with a dedicated evaluation in the ACL 2010 NEWS workshop.", "labels": [], "entities": [{"text": "TM", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7612572312355042}, {"text": "ACL 2010 NEWS workshop", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.8272498250007629}]}, {"text": "In that evaluation, TM was performed using limited training data, namely 1,000 parallel transliteration word-pairs, on short parallel text segments, namely cross-language Wikipedia titles which were typically a few words long.", "labels": [], "entities": [{"text": "TM", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.991293728351593}]}, {"text": "Since TM was performed on very short parallel segments, the chances that two phonetically similar words would appear within such a short text segment in one language were typically very low.", "labels": [], "entities": [{"text": "TM", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.9878054857254028}]}, {"text": "Also, since TM training datasets were small, many valid mappings were not observed in training.", "labels": [], "entities": []}, {"text": "For these two reasons, most of the successful techniques related to that evaluation have focused on improving recall, while hurting precision slightly.", "labels": [], "entities": [{"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9993822574615479}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9993659853935242}]}, {"text": "Some of these techniques involved the use of letter conflation based on a SOUNDEX like scheme) and character n-gram similarity.", "labels": [], "entities": []}, {"text": "The most successful technique on ACL-NEWS dataset, involved the use of graph reinforcement in which observed mappings between language pairs were modeled using a bipartite graph and unseen mappings were induced using random walks.", "labels": [], "entities": [{"text": "ACL-NEWS dataset", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.8800192177295685}]}, {"text": "In this paper, we focus on improving TM between Arabic and English in more realistic settings, compared to the NEWS workshop dataset.", "labels": [], "entities": [{"text": "TM", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9807222485542297}, {"text": "NEWS workshop dataset", "start_pos": 111, "end_pos": 132, "type": "DATASET", "confidence": 0.938315232594808}]}, {"text": "Specifically, we focus on the cases where: 1.", "labels": [], "entities": []}, {"text": "Relatively large TM training sets, which are typical of production systems, are available.", "labels": [], "entities": []}, {"text": "As we will show, using more training data in conjunction with recall-oriented techniques that perform well on small training sets can adversely hurt precision, leading to drops in F-measure.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 62, "end_pos": 77, "type": "METRIC", "confidence": 0.9851959943771362}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9992235898971558}, {"text": "F-measure", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9932396411895752}]}, {"text": "A more fundamental question is what constitutes \"large\" versus \"small\" training sets.", "labels": [], "entities": []}, {"text": "Ideally, we want a unified solution for training sets of varying sizes.", "labels": [], "entities": []}, {"text": "2. TM is performed on large comparable texts which are ubiquitously available from different sources such as cross language news and Wikipedia articles.", "labels": [], "entities": [{"text": "TM", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.6845242381095886}]}, {"text": "In this case, there are two phenomena that arise.", "labels": [], "entities": []}, {"text": "First, there is an increased probability (compared to short texts) that words in the target and source texts maybe phonetically similar, while not being transliterations of each other.", "labels": [], "entities": []}, {"text": "One such example is the Arabic word \"\u202b,\"\ufee3\ufee6\u202c which means \"in\" and is pronounced as \"min\" and the English word \"men\".", "labels": [], "entities": []}, {"text": "Such cases adversely affect precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9662544131278992}]}, {"text": "Second, given a source language word, there maybe multiple target language words that are phonetically similar and TM may rank a wrong word higher than the correct one.", "labels": [], "entities": [{"text": "TM", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.6747974753379822}]}, {"text": "For example, consider the Arabic word \"\u202b,\"\ufe9f\ufeee\u202c which is pronounced as \"joe\" but is in fact the rendition of the Chinese name \"Zhou\".", "labels": [], "entities": []}, {"text": "If the English text has words such as \"jaw\", \"joe\", \"jo\", \"joy\", etc., one of them may rank higher than \"Zhou\".", "labels": [], "entities": []}, {"text": "Since only the top choice is considered, this phenomenon would hurt precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9990278482437134}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9967911839485168}]}, {"text": "We address these two situations by making the following two contributions: 1.", "labels": [], "entities": []}, {"text": "Modifying the TM technique with the best reported results on the ACL 2010 NEWS workshop, namely graph reinforcement () to handle training sets of arbitrary sizes by introducing parameterized exponential penalty to the mapping induction process.", "labels": [], "entities": [{"text": "TM", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9583726525306702}, {"text": "ACL 2010 NEWS workshop", "start_pos": 65, "end_pos": 87, "type": "DATASET", "confidence": 0.9104527831077576}]}, {"text": "We show that we can effectively learn the parameters that tune the penalty for two different training sets of varying sizes.", "labels": [], "entities": []}, {"text": "In doing so, we achieve better results for graph reinforcement with larger training sets.", "labels": [], "entities": []}, {"text": "2. For large comparable texts, we use contextual clues, namely translations of neighboring words, to constrain TM and to preserve precision.", "labels": [], "entities": [{"text": "TM", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.917986273765564}, {"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9975594282150269}]}, {"text": "Specifically, we initially extract text segments that are \"related\" based on cross lingual lexical overlap, and then we perform TM on these segments.", "labels": [], "entities": [{"text": "TM", "start_pos": 128, "end_pos": 130, "type": "TASK", "confidence": 0.9152472615242004}]}, {"text": "Though there have been some papers on extracting sub-sentence alignments from comparable text (), extracting related (as opposed to parallel) text segments maybe preferable because: 1) transliterations may not occur in parallel contexts; 2) using simple lexical overlap is efficient; and as we will show 3) simultaneous use of phonetic and contextual evidences maybe sufficient to produce high TM precision.", "labels": [], "entities": [{"text": "extracting sub-sentence alignments", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.8129477500915527}, {"text": "precision", "start_pos": 397, "end_pos": 406, "type": "METRIC", "confidence": 0.8741930723190308}]}, {"text": "Alternate solutions focused on performing TM on extracted named entities only ().", "labels": [], "entities": [{"text": "TM", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9843778014183044}]}, {"text": "Some drawbacks of such an approach are: 1) named entity recognition (NER) may not be available for many languages; and 2) NER has inherently low recall for languages such as Arabic where no discriminating features such as capitalization exist.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.7111529260873795}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9971569776535034}]}, {"text": "The remainder of the paper is organized as follows: Section 2 provides background on TM; Section 3 describes the basic TM system that is used in the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper.", "labels": [], "entities": [{"text": "TM", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9674105048179626}, {"text": "graph reinforcements", "start_pos": 176, "end_pos": 196, "type": "TASK", "confidence": 0.6914122253656387}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Results for NEWS-1k and NEWS-10k", "labels": [], "entities": [{"text": "NEWS-1k", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.9518863558769226}, {"text": "NEWS-10k", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9198910593986511}]}, {"text": " Table 2. F-measure for baseline, reinforcement, and  exponential penalty at estimated and optimal \u03b1  NEWS-1k NEWS-10k  Baseline  0.757  0.787  Reinforcement (\u03b1=0)  0.941  0.802  Estimated \u03b1  0.3  6.0  @ Estimated \u03b1  0.935  0.963", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9614518880844116}, {"text": "NEWS-1k NEWS-10k  Baseline  0.757  0.787", "start_pos": 102, "end_pos": 142, "type": "DATASET", "confidence": 0.7357054829597474}, {"text": "Reinforcement (\u03b1=0)  0.941  0.802  Estimated \u03b1  0.3", "start_pos": 144, "end_pos": 195, "type": "METRIC", "confidence": 0.6957078982483257}]}, {"text": " Table 3. Results for training using 1k training set  P  R  F1  Baseline  0.975 0.619 0.757  Reinforcement (\u03b1=0)  0.975 0.912 0.941  @ estimated \u03b1  0.980 0.894 0.935", "labels": [], "entities": [{"text": "F1  Baseline  0.975 0.619 0.757", "start_pos": 60, "end_pos": 91, "type": "METRIC", "confidence": 0.6697717428207397}, {"text": "Reinforcement", "start_pos": 93, "end_pos": 106, "type": "METRIC", "confidence": 0.5041563510894775}]}, {"text": " Table 4. Results for training using 10k training set  P  R  F1  Baseline  0.917 0.759 0.787  Reinforcement (\u03b1=0)  0.689 0.960 0.802  @ estimated \u03b1  0.976 0.948 0.963", "labels": [], "entities": [{"text": "F1  Baseline  0.917 0.759 0.787", "start_pos": 61, "end_pos": 92, "type": "METRIC", "confidence": 0.778242313861847}, {"text": "Reinforcement", "start_pos": 94, "end_pos": 107, "type": "METRIC", "confidence": 0.510316014289856}]}, {"text": " Table 5. Results for TM on full Wikipedia articles  Baseline  SOUNDEX  Reinforcement  P  0.610  0.059  0.650  R  0.415  0.402  0.500  F  0.494  0.103  0.565", "labels": [], "entities": [{"text": "TM", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9216193556785583}, {"text": "SOUNDEX  Reinforcement  P  0.610  0.059  0.650  R  0.415  0.402  0.500  F  0.494  0.103  0.565", "start_pos": 63, "end_pos": 157, "type": "METRIC", "confidence": 0.5844269735472543}]}, {"text": " Table 6. Results for TM on extracted segments  Baseline  SOUNDEX  Reinforcement  P  0.962  0.524  0.946  R  0.322  0.418  0.417  F  0.482  0.465  0.579", "labels": [], "entities": [{"text": "TM", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.944372832775116}, {"text": "SOUNDEX  Reinforcement  P  0.962  0.524  0.946  R  0.322  0.418  0.417  F  0.482  0.465  0.579", "start_pos": 58, "end_pos": 152, "type": "METRIC", "confidence": 0.755227053804057}]}]}