{"title": [{"text": "Domain-Specific Semantic Relatedness From Wikipedia: Can A Course Be Transferred?", "labels": [], "entities": [{"text": "Domain-Specific Semantic Relatedness", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.560347984234492}]}], "abstractContent": [{"text": "Semantic relatedness, or its inverse, semantic distance, measures the degree of close-ness between two pieces of text determined by their meaning.", "labels": [], "entities": []}, {"text": "Related work typically measures semantics based on a sparse knowledge base such as WordNet 1 or CYC that requires intensive manual efforts to build and maintain.", "labels": [], "entities": [{"text": "WordNet 1", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.9118377566337585}, {"text": "CYC", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.5399196147918701}]}, {"text": "Other work is based on the Brown corpus , or more recently, Wikipedia.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.9708052277565002}, {"text": "Wikipedia", "start_pos": 60, "end_pos": 69, "type": "DATASET", "confidence": 0.9661675691604614}]}, {"text": "Wikipedia-based measures, however, typically do not take into account the rapid growth of that resource , which exponentially increases the time to prepare and query the knowledge base.", "labels": [], "entities": []}, {"text": "Furthermore , the generalized knowledge domain maybe difficult to adapt to a specific domain.", "labels": [], "entities": []}, {"text": "To address these problems, this paper proposes a domain-specific semantic relatedness measure based on part of Wikipedia that analyzes course descriptions to suggest whether a course can be transferred from one institution to another.", "labels": [], "entities": []}, {"text": "We show that our results perform well when compared to previous work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many NLP techniques have been adapted to the education field for building systems such as automated scoring, intelligent tutoring, and learner cognition.", "labels": [], "entities": []}, {"text": "Few, however, address the identification of transfer course equivalencies.", "labels": [], "entities": [{"text": "identification of transfer course equivalencies", "start_pos": 26, "end_pos": 73, "type": "TASK", "confidence": 0.8626200675964355}]}, {"text": "A recent study by the National Association for College Admission Counseling 2 reveals that 1/3 of US college students trans-fer to another institution.", "labels": [], "entities": [{"text": "College Admission Counseling", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.6946113109588623}]}, {"text": "Correspondingly, University of Massachusetts Lowell (UML) accepts hundreds of transfer students every year.", "labels": [], "entities": []}, {"text": "Each transfer course must be evaluated for credits by manually comparing its course description to courses offered at UML.", "labels": [], "entities": [{"text": "UML", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.94387286901474}]}, {"text": "This process is labor-intensive and highly inefficient.", "labels": [], "entities": []}, {"text": "There is a publicly available course transfer dictionary which lists course numbers from hundreds of institutions and their equivalent courses at UML, but the data set is sparse, non-uniform, and always out of date.", "labels": [], "entities": [{"text": "UML", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.9410189390182495}]}, {"text": "External institutions cancel courses, change course numbers, etc., and such information is virtually impossible to keep up to date in the transfer dictionary.", "labels": [], "entities": []}, {"text": "Furthermore, the transfer dictionary does not list course descriptions.", "labels": [], "entities": []}, {"text": "From our experience, course descriptions changeover the years even when course numbers do not, and this of course affect equivalencies.", "labels": [], "entities": []}, {"text": "This work proposes a domain-specific semantic relatedness measure using Wikipedia that automatically suggests whether two courses from different institutions are equivalent by analyzing their course descriptions.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.9423311352729797}]}, {"text": "The goal is to assist transfer coordinators by suggesting equivalent courses within a reasonable amount of time on a standard laptop system.", "labels": [], "entities": []}, {"text": "Our model is a mapping function: f : (C 1 , C 2 ) \u2192 n, n \u2208 [0, 1], where C 1 is a Computer Science (CS) course from an external institution, and C 2 is a CS course offered at UML.", "labels": [], "entities": [{"text": "UML", "start_pos": 175, "end_pos": 178, "type": "DATASET", "confidence": 0.9706947207450867}]}, {"text": "Output n is the semantic relatedness score, where a bigger value indicates C 1 and C 2 are more related.", "labels": [], "entities": [{"text": "Output", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9700708389282227}]}, {"text": "Each course description is a short text passage: Fragments of WordNet and Wikipedia Taxonomies.", "labels": [], "entities": []}, {"text": "Fragments of WordNet 3.0 (top) and English Wikipedia of 2011/7 (bottom) taxonomies.", "labels": [], "entities": [{"text": "English Wikipedia of 2011/7", "start_pos": 35, "end_pos": 62, "type": "DATASET", "confidence": 0.9567120373249054}]}, {"text": "The root/centroid node is shown in red.", "labels": [], "entities": []}, {"text": "We choose Wikipedia as the knowledge base due to its rich contents) and continuously coalescent growth.", "labels": [], "entities": []}, {"text": "Although Wikipedia was launched 10 years later, it grew much faster than WordNet over the last decade).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.96089768409729}]}, {"text": "The contributions of this paper are twofold.", "labels": [], "entities": []}, {"text": "First, we address the problem of domain-specific semantic relatedness using Wikipedia.", "labels": [], "entities": [{"text": "domain-specific semantic relatedness", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.6531501114368439}, {"text": "Wikipedia", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.949195146560669}]}, {"text": "We propose a method to suggest course equivalencies by computing semantic relatedness among Computer Science course descriptions.", "labels": [], "entities": []}, {"text": "Our approach can be easily modified for other majors and even other languages.", "labels": [], "entities": []}, {"text": "Second, we evaluate the correlation of our approach and a human judgment data set we built.", "labels": [], "entities": []}, {"text": "Both accuracy and correlation indicate that our approach outperforms previous work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9996488094329834}, {"text": "correlation", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9933080673217773}]}], "datasetContent": [{"text": "Wikipedia offers its content as database backup dumps (wikidumps) freely available to download.", "labels": [], "entities": []}, {"text": "Our application is based on the English wikidump 8 of July 2011.", "labels": [], "entities": [{"text": "English wikidump 8 of July 2011", "start_pos": 32, "end_pos": 63, "type": "DATASET", "confidence": 0.9488515853881836}]}, {"text": "We have extracted redirections, titles, categories, and links from the wikidump into separate tables in MySQL.", "labels": [], "entities": [{"text": "MySQL", "start_pos": 104, "end_pos": 109, "type": "DATASET", "confidence": 0.9723626971244812}]}, {"text": "Using the steps outlined in Section 3.1, we built a table for the hierarchy with \"Category:Applied sciences\" as the root.", "labels": [], "entities": []}, {"text": "The attributes of each table were indexed to speedup queries.", "labels": [], "entities": []}, {"text": "Our experiment used \u03b1 = 0.2, \u03b2 = 0.5, \u03b4 = 0.2, and \u03b3 = 0.6.", "labels": [], "entities": []}, {"text": "These values were found The computation of M AXIN and M AXOU T could be time-consuming.", "labels": [], "entities": [{"text": "AXIN", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.5847963094711304}, {"text": "M AXOU T", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.6245865722497305}]}, {"text": "They are therefore based on the entire Wikipedia instead of the constructed hierarchy to avoid the recalculation when the domain changes.", "labels": [], "entities": []}, {"text": "This also ensures the maximum linkage information is unbiased for every domain.", "labels": [], "entities": []}, {"text": "For the July 2011 wikidump, page \"Geographic coordinate system\" has the most in-links, a total of 575,277.", "labels": [], "entities": []}, {"text": "Page \"List of Italian communes\" has the most out-links, a total of 8,103.", "labels": [], "entities": []}, {"text": "8 http://dumps.wikimedia.org/enwiki/20110722/ empirically to perform well over randomly selected samples.", "labels": [], "entities": []}, {"text": "We randomly selected 25 CS courses from 19 universities that can be transferred to University of Massachusetts Lowell (UML) according to the transfer dictionary.", "labels": [], "entities": []}, {"text": "Each transfer course was compared to all 44 CS courses offered at UML, a total of 1,100 comparisons.", "labels": [], "entities": [{"text": "UML", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.8763734102249146}]}, {"text": "The result was considered correct for each course if the real equivalent course in UML appears among the top 3 in the list of highest scores.", "labels": [], "entities": [{"text": "UML", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.9025641083717346}]}, {"text": "We excluded all Wikipedia pages whose titles contained specific dates or were annotated as \"magazine\", \"journal\", or \"album.\"", "labels": [], "entities": []}, {"text": "We removed both general and domain stop words (such as \"course,\" \"book,\" and \"student\") from course descriptions.", "labels": [], "entities": []}, {"text": "If a course description contains the keywords \"not\" or \"no,\" e.g., \"This course requires no computer programming skills,\" the segment after such keyword is ignored.", "labels": [], "entities": []}, {"text": "We tested our approach against the work by and TF-IDF on the same data set of course descriptions.", "labels": [], "entities": [{"text": "TF-IDF", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.891076385974884}]}, {"text": "The accuracy of our proposed approach is 72%, compared to 52% using, and 32% using TF-IDF.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997984766960144}]}, {"text": "Since the transfer dictionary is always out of date, we found a few equivalent course pairs that were unintuitive.", "labels": [], "entities": []}, {"text": "To make a more meaningful evaluation, we setup a human judgment data set.", "labels": [], "entities": []}, {"text": "We gave 6 annotators (CS students and professors) a list of 32 pairs of courses, with only course titles and descriptions.", "labels": [], "entities": []}, {"text": "They independently evaluated whether each pair is equivalent on a scale from 1 to 5.", "labels": [], "entities": []}, {"text": "We averaged their evaluations for each pair and converted the scale from to.", "labels": [], "entities": []}, {"text": "Next, we ran our approach, the work by, and TF-IDF on the same 32 course pairs.", "labels": [], "entities": [{"text": "TF-IDF", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.5956107974052429}]}, {"text": "reports the Pearson's correlation coefficient of course relatedness scores with human judgment, and statistical significances.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 12, "end_pos": 45, "type": "METRIC", "confidence": 0.8006808012723923}]}, {"text": "Our approach has a higher correlation to the human judgment data set compared to previ-ous work.", "labels": [], "entities": []}, {"text": "Furthermore, a smaller p-value indicates our approach is more likely to correlate with human judgment.", "labels": [], "entities": []}, {"text": "During the experiment, we have found some misclassified categories in the wikidump.", "labels": [], "entities": []}, {"text": "For example, \"Category:Software\" has over 350 subcategories with names similar to \"Category:A-Class Britney Spears articles,\" or \"Category:FA-Class Coca-Cola articles.\"", "labels": [], "entities": []}, {"text": "None of these appears in the Wikipedia website or the Wikipedia API 10 as a subcategory of \"Category:Software.\"", "labels": [], "entities": [{"text": "Wikipedia API 10", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.8466345469156901}]}, {"text": "More study is required on how they are formed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Pearson's correlation of course relatedness  scores with human judgments.", "labels": [], "entities": []}]}