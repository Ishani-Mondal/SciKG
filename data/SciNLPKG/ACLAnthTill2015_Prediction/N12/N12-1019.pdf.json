{"title": [{"text": "Re-examining Machine Translation Metrics for Paraphrase Identification", "labels": [], "entities": [{"text": "Machine Translation Metrics", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.7768421570460001}, {"text": "Paraphrase Identification", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.8891384899616241}]}], "abstractContent": [{"text": "We propose to reexamine the hypothesis that automated metrics developed for MT evaluation can prove useful for paraphrase identification in light of the significant work on the development of new MT metrics over the last 4 years.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.9740791916847229}, {"text": "paraphrase identification", "start_pos": 111, "end_pos": 136, "type": "TASK", "confidence": 0.9901697039604187}, {"text": "MT metrics", "start_pos": 196, "end_pos": 206, "type": "TASK", "confidence": 0.881680428981781}]}, {"text": "We show that a meta-classifier trained using nothing but recent MT metrics outperforms all previous paraphrase identification approaches on the Microsoft Research Paraphrase corpus.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.8780417442321777}, {"text": "Microsoft Research Paraphrase corpus", "start_pos": 144, "end_pos": 180, "type": "DATASET", "confidence": 0.8816768079996109}]}, {"text": "In addition, we apply our system to a second corpus developed for the task of plagiarism detection and obtain extremely positive results.", "labels": [], "entities": [{"text": "plagiarism detection", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.7639797627925873}]}, {"text": "Finally, we conduct extensive error analysis and uncover the top systematic sources of error fora paraphrase identification approach relying solely on MT metrics.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 98, "end_pos": 123, "type": "TASK", "confidence": 0.7017648369073868}, {"text": "MT", "start_pos": 151, "end_pos": 153, "type": "TASK", "confidence": 0.8762324452400208}]}, {"text": "We release both the new dataset and the error analysis annotations for use by the community.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the most important reasons for the recent advances made in Statistical Machine Translation (SMT) has been the development of automated metrics for evaluation of translation quality.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 66, "end_pos": 103, "type": "TASK", "confidence": 0.8762601912021637}]}, {"text": "The goal of any such metric is to assess whether the translation hypothesis produced by a system is semantically equivalent to the source sentence that was translated.", "labels": [], "entities": []}, {"text": "However, cross-lingual semantic equivalence is even harder to assess than monolingual, therefore, most MT metrics instead try to measure whether the hypothesis is semantically equivalent to a human-authored reference translation of the same source sentence.", "labels": [], "entities": [{"text": "cross-lingual semantic equivalence", "start_pos": 9, "end_pos": 43, "type": "TASK", "confidence": 0.7039042512575785}, {"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.9823290705680847}]}, {"text": "Using such automated metrics as proxies for human judgments can provide a quick assessment of system performance and allow for short feature and system development cycles, which are important for evaluating research ideas.", "labels": [], "entities": []}, {"text": "In the last 5 years, several shared tasks and competitions have led to the development of increasingly sophisticated metrics that go beyond the computation of n-gram overlaps (BLEU, NIST) or edit distances (TER, WER, PER etc.).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9937412142753601}, {"text": "PER", "start_pos": 217, "end_pos": 220, "type": "METRIC", "confidence": 0.6572468876838684}]}, {"text": "Note that the task of an MT metric is essentially one of identifying whether the translation produced by a system is a paraphrase of the reference translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.97757488489151}]}, {"text": "Although the notion of using MT metrics for the task of paraphrase identification is not novel (), it merits a re-examination in the light of the development of these novel MT metrics for which we can ask \"How much better, if at all, do these newer metrics perform for the task of paraphrase identification?\"", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.9003971219062805}, {"text": "paraphrase identification", "start_pos": 281, "end_pos": 306, "type": "TASK", "confidence": 0.9437673687934875}]}, {"text": "This paper describes such a re-examination.", "labels": [], "entities": []}, {"text": "We employ 8 different MT metrics for identifying paraphrases across two different datasets -the well-known Microsoft Research paraphrase corpus (MSRP) () and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task).", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9767063856124878}, {"text": "Microsoft Research paraphrase corpus (MSRP)", "start_pos": 107, "end_pos": 150, "type": "DATASET", "confidence": 0.8016056418418884}, {"text": "Uncovering Plagiarism, Authorship and Social Software Misuse shared task", "start_pos": 210, "end_pos": 282, "type": "TASK", "confidence": 0.5920407742261886}]}, {"text": "We include both MSRP and PAN in our study because they represent two very different sources of paraphrased text.", "labels": [], "entities": []}, {"text": "The creation of MSRP relied on the massive redundancy of news articles on the web and extracted sentential paraphrases from different stories written about the same topic.", "labels": [], "entities": []}, {"text": "In the case of PAN, humans consciously paraphrased existing text to generate new, plagiarized text.", "labels": [], "entities": []}, {"text": "In the next section, we discuss previous work on paraphrase identification.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.9828059673309326}]}, {"text": "In \u00a73, we describe our approach to paraphrase identification using MT metrics as features.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.9727655649185181}]}, {"text": "Our approach yields impressive results -the current state of the art for MSRP and extremely positive for PAN.", "labels": [], "entities": [{"text": "MSRP", "start_pos": 73, "end_pos": 77, "type": "TASK", "confidence": 0.8987168669700623}, {"text": "PAN", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.850310742855072}]}, {"text": "In the same section, we examine whether each metric's purported strength is demonstrated in our datasets.", "labels": [], "entities": []}, {"text": "Next, in \u00a74 we conduct an analysis of our system's misclassifications for both datasets and outline a taxonomy of errors that our system makes.", "labels": [], "entities": []}, {"text": "We also look at annotation errors in the datasets themselves.", "labels": [], "entities": []}, {"text": "We discuss the findings of the error analysis in \u00a75 and conclude in \u00a76.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the two datasets that we used to evaluate our approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Classification results for MSRP and PAN with  individual metrics as features. Entries are sorted by accu- racies on MSRP.", "labels": [], "entities": [{"text": "accu- racies", "start_pos": 110, "end_pos": 122, "type": "METRIC", "confidence": 0.9269458651542664}]}, {"text": " Table 3: The top 3 performing MT metrics for both  MSRP and PAN datasets as identified by ablation stud- ies. BLEU(1-4), NIST(1-5) and TER were used as the 10  base features in the classifiers.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9908862113952637}, {"text": "PAN datasets", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.7452768236398697}, {"text": "ablation stud- ies", "start_pos": 91, "end_pos": 109, "type": "METRIC", "confidence": 0.9281312525272369}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9966315627098083}, {"text": "NIST", "start_pos": 122, "end_pos": 126, "type": "DATASET", "confidence": 0.8712425231933594}, {"text": "TER", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.9935498833656311}]}]}