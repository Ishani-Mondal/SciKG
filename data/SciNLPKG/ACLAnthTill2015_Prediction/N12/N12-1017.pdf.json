{"title": [{"text": "HyTER: Meaning-Equivalent Semantics for Translation Evaluation", "labels": [], "entities": [{"text": "Translation Evaluation", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.9604367315769196}]}], "abstractContent": [{"text": "It is common knowledge that translation is an ambiguous, 1-ton mapping process, but to date, our community has produced no empirical estimates of this ambiguity.", "labels": [], "entities": [{"text": "translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9809558987617493}]}, {"text": "We have developed an annotation tool that enables us to create representations that compactly encode an exponential number of correct translations fora sentence.", "labels": [], "entities": []}, {"text": "Our findings show that naturally occurring sentences have billions of translations.", "labels": [], "entities": []}, {"text": "Having access to such large sets of meaning-equivalent translations enables us to develop anew metric, HyTER, for translation accuracy.", "labels": [], "entities": [{"text": "HyTER", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9713177680969238}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.7621562480926514}]}, {"text": "We show that our metric provides better estimates of machine and human translation accuracy than alternative evaluation metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.7390303611755371}]}], "introductionContent": [], "datasetContent": [{"text": "In this section, we present a use case for the HyTER metric outside of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7367543280124664}]}, {"text": "We automatically assess the proficiency of candidates who take a translation exam.", "labels": [], "entities": [{"text": "translation exam", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.899848610162735}]}, {"text": "We treat this as a classification task where, for each translation of the three passages, we predict the three passage assessment labels as well as one overall ILR rating.", "labels": [], "entities": []}, {"text": "In support of our goal, we asked annotators to create an English HyTER network for each foreign sentence in the exams.", "labels": [], "entities": []}, {"text": "These HyTER networks then serve as English references for the candidate translations.", "labels": [], "entities": []}, {"text": "The median number of paths in these HyTER networks is 1.6 \u00d7 10 6 paths/network.", "labels": [], "entities": []}, {"text": "In training, we observe a set of submitted exam translations, each of which is annotated with three passage-level ratings and one overall ILR rating.", "labels": [], "entities": [{"text": "ILR", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.7919888496398926}]}, {"text": "We develop features (Section 5.3) that describe each passage translation in its relation to the HyTER networks for the passage.", "labels": [], "entities": []}, {"text": "We then train a classifier to predict passage-level ratings given the passage-level features that describe the candidate translation.", "labels": [], "entities": []}, {"text": "As classifier, we use a multi-class support-vector machine).", "labels": [], "entities": []}, {"text": "In decoding, we observe a set of exams without their ratings, derive the features and use the trained SVM to predict ratings of the passage translations.", "labels": [], "entities": []}, {"text": "We then derive an overall ILR rating based on the predicted passage-level ratings.", "labels": [], "entities": [{"text": "ILR", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.6688275933265686}]}, {"text": "Since our dataset is small we run 10-fold cross-validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Scores assigned to human versus machine translations, under various metrics. Each score is normalized to  range from 100 (worst) to 0 (perfect translation).", "labels": [], "entities": []}, {"text": " Table 5: Predicting final ILR ratings for candidate exams.", "labels": [], "entities": [{"text": "Predicting final ILR ratings", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8132456690073013}]}]}