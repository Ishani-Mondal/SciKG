{"title": [{"text": "Optimized Online Rank Learning for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7987251579761505}]}], "abstractContent": [{"text": "We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.8303123811880747}]}, {"text": "Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.9697653651237488}]}, {"text": "We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm.", "labels": [], "entities": [{"text": "SGD", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9263481497764587}]}, {"text": "Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs.", "labels": [], "entities": []}, {"text": "Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results.", "labels": [], "entities": [{"text": "NIST Chinese-to-English Open MT task", "start_pos": 19, "end_pos": 55, "type": "TASK", "confidence": 0.6894846796989441}]}], "introductionContent": [{"text": "The advancement of statistical machine translation (SMT) relies on efficient tuning of several or many parameters in a model.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 19, "end_pos": 56, "type": "TASK", "confidence": 0.8309654494126638}]}, {"text": "One of the standards for such tuning is minimum error rate training (MERT), which directly minimize the loss of translation evaluation measures, i.e. BLEU ().", "labels": [], "entities": [{"text": "minimum error rate training (MERT)", "start_pos": 40, "end_pos": 74, "type": "METRIC", "confidence": 0.8504198959895543}, {"text": "translation evaluation", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.8706279098987579}, {"text": "BLEU", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9971470236778259}]}, {"text": "MERT has been successfully used in practical applications, although, it is known to be unstable ).", "labels": [], "entities": [{"text": "MERT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.3995574116706848}]}, {"text": "To overcome this instability, it requires multiple runs from random starting points and directions, or a computationally expensive procedure by linear programming and combinatorial optimization).", "labels": [], "entities": []}, {"text": "Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (), maximum entropy (, Margin Infused Relaxed Algorithm (MIRA)), or pairwise rank optimization (PRO) ().", "labels": [], "entities": [{"text": "Margin Infused Relaxed Algorithm (MIRA))", "start_pos": 140, "end_pos": 180, "type": "METRIC", "confidence": 0.8423125318118504}, {"text": "pairwise rank optimization (PRO)", "start_pos": 185, "end_pos": 217, "type": "TASK", "confidence": 0.6856028586626053}]}, {"text": "They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (), conditional loglikelihood (or softmax loss), risk (;, or ranking ().", "labels": [], "entities": []}, {"text": "We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.72223761677742}]}, {"text": "Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion.", "labels": [], "entities": []}, {"text": "As proposed by, BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (, and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9969161748886108}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9955008625984192}]}, {"text": "Setting the larger batch size implies the more accurate corpus-BLEU, but at the cost of slower convergence of SGD.", "labels": [], "entities": []}, {"text": "Therefore, we propose an optimized update method inspired by the passive-aggressive algorithm), in which each parameter update is further rescaled considering the tradeoff between the amount of updates to the parameters and the ranking loss.", "labels": [], "entities": []}, {"text": "Learning is efficiently parallelized by splitting training data among shards and by merging parameters in each round).", "labels": [], "entities": []}, {"text": "Instead of simple averaging, we perform an additional line search step to find the optimal merging across parallel jobs.", "labels": [], "entities": []}, {"text": "Experiments were carried out on the NIST 2008 Chinese-to-English Open MT task.", "labels": [], "entities": [{"text": "NIST 2008 Chinese-to-English Open MT task", "start_pos": 36, "end_pos": 77, "type": "DATASET", "confidence": 0.8607469300429026}]}, {"text": "We found significant gains over traditional MERT and other tuning algorithms, such as MIRA and PRO.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.5030136108398438}]}], "datasetContent": [{"text": "Experiments were carried out on the NIST 2008 Chinese-to-English Open MT task.", "labels": [], "entities": [{"text": "NIST 2008 Chinese-to-English Open MT task", "start_pos": 36, "end_pos": 77, "type": "DATASET", "confidence": 0.8607469300429026}]}, {"text": "The training data consists of nearly 5.6 million bilingual sentences and additional monolingual data, English Gigaword, for 5-gram language model estimation.", "labels": [], "entities": []}, {"text": "MT02 and MT06 were used as our tuning and development testing, and MT08 as our final testing with all data consisting of four reference translations.", "labels": [], "entities": [{"text": "MT02", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.969369649887085}, {"text": "MT06", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.9083181619644165}, {"text": "MT08", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.8763167858123779}]}, {"text": "We use an in-house developed hypergraph-based toolkit for training and decoding with synchronousCFGs (SCFG) for hierarchical phrase-bassed SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.6797962784767151}]}, {"text": "The system employs 14 features, consisting of standard Hiero-style features, and a set of indicator features, such as the number of synchronous-rules in a derivation.", "labels": [], "entities": []}, {"text": "Two 5-gram language models are also included, one from the English-side of bitexts and the other from English Gigaword, with features counting the number of out-of-vocabulary words in each model ).", "labels": [], "entities": []}, {"text": "For faster experiments, we precomputed translation forests inspired by.", "labels": [], "entities": []}, {"text": "Instead of generating forests from bitexts in each iteration, we construct and save translation forests by intersecting the source side of SCFG with input sentences and by keeping the target side of the inter-sected rules.", "labels": [], "entities": []}, {"text": "n-bests are generated from the precomputed forests on the fly using the forest rescoring framework) with additional non-local features, such as 5-gram language models.", "labels": [], "entities": []}, {"text": "We compared four algorithms, MERT, PRO, MIRA and our proposed online settings, online rank optimization (ORO).", "labels": [], "entities": [{"text": "MERT", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.8836909532546997}, {"text": "MIRA", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9627017974853516}, {"text": "online rank optimization", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.5282756984233856}]}, {"text": "Note that ORO without our optimization methods in Section 4 is essentially the same as Pegasos, but differs in that we employ the algorithm for ranking structured outputs with varied objectives, hinge loss or softmax loss . MERT learns parameters from forests () with 4 restarts and 8 random directions in each iteration.", "labels": [], "entities": []}, {"text": "We experimented on a variant of PRO 4 , in which the objective in Eq.", "labels": [], "entities": [{"text": "PRO 4", "start_pos": 32, "end_pos": 37, "type": "TASK", "confidence": 0.6557170152664185}]}, {"text": "4 with the hinge loss of Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.872097909450531}]}, {"text": "5 was solved in each iteration inline 4 of Alg.", "labels": [], "entities": [{"text": "Alg.", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.9288337826728821}]}, {"text": "1 using an off-the-shelf solver . Our MIRA solves the problem in Equation 13 inline 7 of Alg.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.7689495086669922}, {"text": "Alg.", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.8993109464645386}]}, {"text": "2. For a systematic comparison, we used our exhaustive oracle translation selection method in Section 3 for PRO, MIRA and ORO.", "labels": [], "entities": [{"text": "oracle translation selection", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.6779608329137167}, {"text": "PRO", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.5170782804489136}, {"text": "MIRA", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.8275236487388611}, {"text": "ORO", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9742001891136169}]}, {"text": "For each learning algorithm, we ran 30 iterations and generated duplicate removed 1,000-best translations in each iteration.", "labels": [], "entities": []}, {"text": "The hyperparameter \u03bb for PRO and ORO was set to 10 \u22125 , selected from among {10 \u22123 , 10 \u22124 , 10 \u22125 }, and 10 2 for MIRA, chosen from {10, 10 2 , 10 3 } by preliminary testing on MT06.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.6502057909965515}, {"text": "MT06", "start_pos": 178, "end_pos": 182, "type": "DATASET", "confidence": 0.9661742448806763}]}, {"text": "Both decoding and learning are parallelized and run on 8 cores.", "labels": [], "entities": []}, {"text": "Each online learning took roughly 12 hours, and PRO took one day.", "labels": [], "entities": [{"text": "PRO", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9537713527679443}]}, {"text": "It took roughly 3 days for MERT with 20 iterations.", "labels": [], "entities": [{"text": "MERT", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.4739059805870056}]}, {"text": "Translation results are measured by case sensitive BLEU.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9434776306152344}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9726347327232361}]}, {"text": "Among the parameters from multiple iterations, we report the outputs that performed the best on MT06.", "labels": [], "entities": [{"text": "MT06", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.9053001403808594}]}, {"text": "With Moses (, we achieved 30.36 and 23.64 BLEU for MT06 and MT08, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9992018342018127}, {"text": "MT06", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.8699033856391907}, {"text": "MT08", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.8663555383682251}]}, {"text": "We denote the \"O-\" prefix for the optimized parameter updates discussed in Section 4.1, and the \"-L\" suffix The other major difference is the use of a simpler learning rate, 1 \u03bbk , which was very slow in our preliminary studies.", "labels": [], "entities": [{"text": "O-\" prefix", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9246923923492432}]}, {"text": "Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks.", "labels": [], "entities": []}, {"text": "We used liblinear: Translation results by BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9896397590637207}]}, {"text": "Results without significant differences from the MERT baseline are marked \u2020.", "labels": [], "entities": [{"text": "MERT baseline", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.6434898525476456}]}, {"text": "The numbers in boldface are significantly better than the MERT baseline (both measured by the bootstrap resampling) with p > 0.05).", "labels": [], "entities": [{"text": "MERT baseline", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.744944155216217}]}, {"text": "for parameter mixing byline search as described in Section 4.2.", "labels": [], "entities": [{"text": "parameter mixing byline search", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.6194921880960464}]}, {"text": "The batch size was set to 16 for MIRA and ORO.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.45886918902397156}, {"text": "ORO", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.8029431700706482}]}, {"text": "In general, our PRO and MIRA settings achieved the results very comparable to MERT.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.8668560981750488}, {"text": "MERT", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.4103415310382843}]}, {"text": "The hinge-loss and softmax objective OROs were lower than those of the three baselines.", "labels": [], "entities": [{"text": "softmax objective OROs", "start_pos": 19, "end_pos": 41, "type": "METRIC", "confidence": 0.8159682750701904}]}, {"text": "The softmax objective with the optimized update (O-ORO-L softmax ) performed better than the non-optimized version, but it was still lower than our baselines.", "labels": [], "entities": []}, {"text": "In the case of the hinge-loss objective with the optimized update (O-ORO-L hinge ), the gain in MT08 was significant, and achieved the best BLEU.", "labels": [], "entities": [{"text": "MT08", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.6751857995986938}, {"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9989519119262695}]}, {"text": "presents the learning curves for three algorithms MIRA-L, ORO-L hinge and O-ORO-L hinge , in which the performance is measured by BLEU  on the training data (MT02) and on the test data (MT08).", "labels": [], "entities": [{"text": "MIRA-L", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9750216007232666}, {"text": "ORO-L", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9479106068611145}, {"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9987057447433472}, {"text": "MT02", "start_pos": 158, "end_pos": 162, "type": "DATASET", "confidence": 0.8690124154090881}, {"text": "MT08", "start_pos": 186, "end_pos": 190, "type": "DATASET", "confidence": 0.950646698474884}]}, {"text": "MIRA-L quickly converges and is slightly unstable in the test set, while ORO-L hinge is very stable and slow to converge, but with low performance on the training and test data.", "labels": [], "entities": [{"text": "MIRA-L", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5938298106193542}, {"text": "ORO-L hinge", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.9319678246974945}]}, {"text": "The stable learning curve in ORO-L hinge is probably influenced by our learning rate parameter \u03b7 0 = 0.2, which will be investigated in future work.", "labels": [], "entities": [{"text": "ORO-L hinge", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.6970672905445099}, {"text": "learning rate parameter \u03b7 0", "start_pos": 71, "end_pos": 98, "type": "METRIC", "confidence": 0.8995928049087525}]}, {"text": "O-ORO-L hinge is less stable in several iterations, but steadily improves its BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9991644620895386}]}, {"text": "The behavior is justified by our optimized update procedure, in which the learning rate \u03b7 k is used as a tradeoff parameter.", "labels": [], "entities": [{"text": "learning rate \u03b7 k", "start_pos": 74, "end_pos": 91, "type": "METRIC", "confidence": 0.8714204728603363}]}, {"text": "Thus, it tries a very aggressive update at the early stage of training, but eventually becomes conservative in updating parameters.", "labels": [], "entities": []}, {"text": "Next, we compare the effect of line search for parameter mixing in.", "labels": [], "entities": []}, {"text": "Line search was very effective for MIRA and O-ORO hinge , but less effective for the others.", "labels": [], "entities": [{"text": "Line search", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7719078361988068}, {"text": "MIRA", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.4469471573829651}]}, {"text": "Since the line search procedure directly minimizes a task loss, not objectives, this may hurt the performance for the softmax objective, where the margins between the correct and incorrect translations are softly penalized.", "labels": [], "entities": [{"text": "line search", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7272843420505524}]}, {"text": "Finally, shows the effect of batch size selected from {1, 4, 8, 16}.", "labels": [], "entities": []}, {"text": "There seems to be no clear trends in MIRA, and we achieved BLEU score of 24.58 by setting the batch size to 8.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.8283015489578247}, {"text": "BLEU score", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9622247517108917}]}, {"text": "Clearly, setting smaller batch size is better for ORO, but it is the reverse for the optimized variants of both the hinge and softmax objectives.", "labels": [], "entities": [{"text": "ORO", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9155030250549316}]}, {"text": "ing of the learning rate parameter will be required fora larger batch size.", "labels": [], "entities": []}, {"text": "As discussed in Section 3, the smaller batch size means frequent updates to parameters and a faster convergence, but potentially leads to a poor performance since the corpus-BLEU is approximately computed in a local batch.", "labels": [], "entities": []}, {"text": "Our optimized update algorithms address the problem by adjusting the tradeoff between the amount of update to parameters and the loss, and perform better for larger batch sizes with a more accurate corpus-BLEU.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Translation results by BLEU. Results with- out significant differences from the MERT baseline  are marked  \u2020. The numbers in boldface are signif- icantly better than the MERT baseline (both mea- sured by the bootstrap resampling", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9654494524002075}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9930676221847534}, {"text": "MERT baseline", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.7882854640483856}, {"text": "MERT baseline", "start_pos": 180, "end_pos": 193, "type": "DATASET", "confidence": 0.8251364529132843}]}, {"text": " Table 2: Parameter mixing by line search.", "labels": [], "entities": [{"text": "Parameter mixing", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7763329446315765}]}, {"text": " Table 2. Line search was very  effective for MIRA and O-ORO hinge , but less effec- tive for the others. Since the line search procedure  directly minimizes a task loss, not objectives, this  may hurt the performance for the softmax objective,  where the margins between the correct and incorrect  translations are softly penalized.", "labels": [], "entities": [{"text": "Line search", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7746314406394958}, {"text": "MIRA", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.42750000953674316}]}, {"text": " Table 3: Translation results with varied batch size.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9817702174186707}]}]}