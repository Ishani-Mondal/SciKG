{"title": [{"text": "Why Not Grab a Free Lunch? Mining Large Corpora for Parallel Sentences to Improve Translation Modeling", "labels": [], "entities": [{"text": "Improve Translation Modeling", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.7654963135719299}]}], "abstractContent": [{"text": "It is well known that the output quality of statistical machine translation (SMT) systems increases with more training data.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.7960271934668223}]}, {"text": "To obtain more parallel text for translation mod-eling, researchers have turned to the web to mine parallel sentences, but most previous approaches have avoided the difficult problem of pairwise similarity on cross-lingual documents and instead rely on heuristics.", "labels": [], "entities": [{"text": "translation mod-eling", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.9431261718273163}]}, {"text": "In contrast , we confront this challenge head on using the MapReduce framework.", "labels": [], "entities": []}, {"text": "On a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and Ger-man Wikipedia.", "labels": [], "entities": []}, {"text": "Augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline (2.39 BLEU points in the best case).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9987781643867493}]}], "introductionContent": [{"text": "It has been repeatedly shown that \"throwing more data at the problem\" is effective in increasing SMT output quality, both for translation modeling) and for language modeling.", "labels": [], "entities": [{"text": "SMT output", "start_pos": 97, "end_pos": 107, "type": "TASK", "confidence": 0.8670582473278046}, {"text": "translation modeling", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.9724963903427124}, {"text": "language modeling", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.7766349911689758}]}, {"text": "In this paper, we bring together two related research threads to gather parallel sentences for improved translation modeling: cross-lingual pairwise similarity to mine comparable documents and classification to identify sentence pairs that are mutual translations.", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 104, "end_pos": 124, "type": "TASK", "confidence": 0.9692021012306213}]}, {"text": "Unlike most previous work, which sidesteps the computationally-intensive task of pairwise comparisons to mine comparable documents and instead relies on heuristics, we tackle the challenge head on.", "labels": [], "entities": []}], "datasetContent": [{"text": "In all experiments, our MT system learned asynchronous context-free grammar, using GIZA++ for word alignments, MIRA for parameter tuning), cdec for decoding ( ), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9477893710136414}, {"text": "word alignments", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.7200234681367874}, {"text": "MIRA", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9972817897796631}, {"text": "language modeling", "start_pos": 181, "end_pos": 198, "type": "TASK", "confidence": 0.705758810043335}, {"text": "BLEU", "start_pos": 221, "end_pos": 225, "type": "METRIC", "confidence": 0.9890468716621399}]}, {"text": "The baseline system was trained on the German-English WMT10 training data, consisting of 3.1m sentence pairs.", "labels": [], "entities": [{"text": "German-English WMT10 training data", "start_pos": 39, "end_pos": 73, "type": "DATASET", "confidence": 0.7826661914587021}]}, {"text": "For development and testing, we used the newswire datasets provided for WMT10, including 2525 sentences for tuning and 2489 sentences for testing.", "labels": [], "entities": [{"text": "newswire datasets", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.8367834687232971}, {"text": "WMT10", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.9473515152931213}]}, {"text": "Our baseline system includes all standard features, including phrase translation probabilities in both directions, word and arity penalties, and language model scores.", "labels": [], "entities": [{"text": "phrase translation probabilities", "start_pos": 62, "end_pos": 94, "type": "TASK", "confidence": 0.7794567346572876}]}, {"text": "It achieves a BLEU score of 21.37 on the test set, which would place it 5 th out of 9 systems that reported comparable results in WMT10 (only three systems achieved a BLEU score over 22).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9877482354640961}, {"text": "WMT10", "start_pos": 130, "end_pos": 135, "type": "DATASET", "confidence": 0.8683955669403076}, {"text": "BLEU", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9990975856781006}]}, {"text": "Many of these systems used techniques that exploited the specific aspects of the task, e.g., German-specific morphological analysis.", "labels": [], "entities": [{"text": "German-specific morphological analysis", "start_pos": 93, "end_pos": 131, "type": "TASK", "confidence": 0.5689528981844584}]}, {"text": "In contrast, we present a knowledge-impoverished, entirely data-driven approach, by simply looking for more data in large collections.", "labels": [], "entities": []}, {"text": "For both experimental conditions (one-step classification, S 1 , and two-step classification, S 2 ) we varied the decision threshold to generate new bitext collections of different sizes.", "labels": [], "entities": []}, {"text": "Each of these collections was added to the baseline training data to induce an entirely new translation model (note that GIZA additionally filtered out some of the pairs based on length).", "labels": [], "entities": []}, {"text": "The final dataset sizes, along with BLEU scores on the test data, are shown in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.999578058719635}]}, {"text": "In S 1 , we observe that increasing the amount of data (by lowering the decision threshold) initially leads to lower BLEU scores (due to increased noise), but there is a threshold after which the improvement coming from the added data supersedes the noise.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 117, "end_pos": 128, "type": "METRIC", "confidence": 0.9826951622962952}]}, {"text": "The S 2 condition increases the quality of bitext by reducing this noise: the best run, with 5.8m pairs added to the baseline (final dataset has 8.1m pairs), yields 23.76 BLEU (labeled P on figure), 2.39 points above the baseline (and higher than the best WMT10 result).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9985652565956116}, {"text": "WMT10", "start_pos": 256, "end_pos": 261, "type": "DATASET", "confidence": 0.7214605808258057}]}, {"text": "These results show that the two-step classification process, while slower, is worth the additional processing time.", "labels": [], "entities": [{"text": "classification", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.9185208082199097}]}, {"text": "Our approach yields solid improvements even with less data added: with only 382k pairs added to the baseline, the BLEU score increases by 1.84 points.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.9795533418655396}]}, {"text": "In order to better examine the effect of data size alone, we created partial datasets from P by randomly sampling sentence pairs, and then repeated experiments, also shown in.", "labels": [], "entities": []}, {"text": "We see an increasing trend of BLEU scores with respect to data size.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9988836646080017}]}, {"text": "By comparing the three plots, we see that S 2 and random sampling from P work better than S 1 . Also, random sampling is not always worse than S 2 , since some pairs that receive low classifier confidence turnout to be helpful.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of the simple and complex sentence  classifiers on Europarl data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.994220495223999}, {"text": "Europarl data", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.9965471923351288}]}]}