{"title": [{"text": "Batch Tuning Strategies for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.8639041582743326}]}], "abstractContent": [{"text": "There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach.", "labels": [], "entities": [{"text": "SMT tuning", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.9896486699581146}]}, {"text": "We analyze a number of these algorithms in terms of their sentence-level loss functions, which motivates several new approaches, including a Structured SVM.", "labels": [], "entities": [{"text": "Structured SVM", "start_pos": 141, "end_pos": 155, "type": "TASK", "confidence": 0.789668619632721}]}, {"text": "We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings.", "labels": [], "entities": [{"text": "MERT", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.5597629547119141}]}, {"text": "Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 74, "end_pos": 78, "type": "TASK", "confidence": 0.8204299211502075}]}], "introductionContent": [{"text": "The availability of linear models and discriminative tuning algorithms has been a huge boon to statistical machine translation (SMT), allowing the field to move beyond the constraints of generative noisy channels ().", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 95, "end_pos": 132, "type": "TASK", "confidence": 0.8192574431498846}]}, {"text": "The ability to optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9961555600166321}, {"text": "Minimum Error Rate Training", "start_pos": 142, "end_pos": 169, "type": "METRIC", "confidence": 0.6449516341090202}]}, {"text": "However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces.", "labels": [], "entities": [{"text": "MERT", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.6959093809127808}]}, {"text": "These methods fall into a number of broad categories.", "labels": [], "entities": []}, {"text": "Minimum risk approaches) have been quietly capable of handling many features for sometime, but have yet to see widespread adoption.", "labels": [], "entities": []}, {"text": "Online methods (;, are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization.", "labels": [], "entities": []}, {"text": "Pairwise ranking) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure.", "labels": [], "entities": []}, {"text": "The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approximation to the true search space.", "labels": [], "entities": [{"text": "MERT", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.6814566850662231}]}, {"text": "This optimization is wrapped in an outer loop that iterates between optimizing weights and re-decoding with those weights to enhance the approximation.", "labels": [], "entities": []}, {"text": "Our primary contribution is to empirically compare eight tuning algorithms and variants, focusing on methods that work within MERT's established outer loop.", "labels": [], "entities": []}, {"text": "This is the first comparison to include all three categories of optimizer.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce three tuners that have not been previously tested.", "labels": [], "entities": []}, {"text": "In particular, we test variants of hope-fear MIRA that use k-best or lattice-approximated search spaces, producing a Batch MIRA that outperforms a popular mechanism for parallelizing online learners.", "labels": [], "entities": []}, {"text": "We also investigate the direct optimization of hinge loss on k-best lists, through the use of a Structured SVM ().", "labels": [], "entities": []}, {"text": "We review and organize the existing tuning literature, providing sentence-level loss functions for minimum risk, online and pairwise training.", "labels": [], "entities": []}, {"text": "Finally, since randomization plays a different role in each tuner, we also suggest anew method for testing an optimizer's stability), which sub-samples the tuning set instead of varying a random seed.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the six tuning strategies described in this paper, along with two MERT baselines, on three language pairs French-English (Fr-En), English-French (En-Fr) and Chinese-English (ZhEn) , across three different feature-set sizes.", "labels": [], "entities": [{"text": "MERT", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.8653702735900879}]}, {"text": "Each setting was run five times over randomized variants to improve reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.9585176110267639}]}, {"text": "To cope with the resulting large number of configurations, we ran all experiments using an efficient phrase-based decoder similar to.", "labels": [], "entities": []}, {"text": "All tuning methods that use an approximate\u02dcEapproximate\u02dc approximate\u02dcE perform 15 iterations of the outer loop and return the weights that achieve the best development BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9860016107559204}]}, {"text": "When present, \u03bb was coarsely tuned (trying 3 values differing by magnitudes of 10) in our largefeature Chinese-English setting.", "labels": [], "entities": []}, {"text": "\u2022 kb-mert : k-best MERT with 20 random restarts.", "labels": [], "entities": [{"text": "MERT", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.8788150548934937}]}, {"text": "All k-best methods use k = 100.", "labels": [], "entities": []}, {"text": "\u2022 lb-mert : Lattice MERT ( using unpruned lattices and aggregating only those paths on the line search's upper envelope.", "labels": [], "entities": [{"text": "MERT", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9157636761665344}]}, {"text": "\u2022 mira : Online MIRA ( \u00a72.1).", "labels": [], "entities": [{"text": "Online", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.9285823702812195}, {"text": "MIRA", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.40172335505485535}]}, {"text": "All MIRA variants use a pseudo-corpus decay \u03b3 = 0.999 and C = 0.01.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.5017560720443726}]}, {"text": "Online parallelization follows, using 8 shards.", "labels": [], "entities": []}, {"text": "We tested 20, 15, 10, 8 and 5 shards during development.", "labels": [], "entities": []}, {"text": "\u2022 lb-mira : Batch Lattice MIRA ( \u00a73.1).", "labels": [], "entities": [{"text": "Batch Lattice", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.6161323487758636}, {"text": "MIRA", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.7540497183799744}]}, {"text": "\u2022 kb-mira : Batch k-best MIRA ( \u00a73.1).", "labels": [], "entities": [{"text": "Batch k-best", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.8269268274307251}, {"text": "MIRA", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.5219687223434448}]}, {"text": "\u2022 pro : PRO ( \u00a72.3) follows Hopkins and May (2011), footnote 6, implementing a logistic sigmoid sampler with both the initial and maximum sample size set to 100 pairs.", "labels": [], "entities": [{"text": "PRO", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.5275869369506836}]}, {"text": "For classification, we employ an in-house maximum entropy classifier with a regularization weight \u03bb = 1000.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9720742702484131}]}, {"text": "We tried many variants of the primary sampler described by, but we were unable to find settings for it that worked well for all language pairs.", "labels": [], "entities": []}, {"text": "\u2022 mr : MR as described in \u00a72.  2010).", "labels": [], "entities": [{"text": "MR", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.9960914254188538}]}, {"text": "Upon reaching a local optimum, we reshuffle our data, re-tune our learning rate, and re-start from the optimum, repeating this process 5 times.", "labels": [], "entities": []}, {"text": "We do not sharpen our distribution with a temperature or otherwise control for entropy; instead, we trust \u03bb = 50 to maintain a reasonable distribution.", "labels": [], "entities": []}, {"text": "\u2022 svm : Structured SVM ( \u00a73.2) with \u03bb = 1000.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hansard Corpus (English/French)", "labels": [], "entities": [{"text": "Hansard Corpus (English/French)", "start_pos": 10, "end_pos": 41, "type": "DATASET", "confidence": 0.9253971746989659}]}, {"text": " Table 4: French to English Translation (Fr-En)", "labels": [], "entities": [{"text": "French to English Translation", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.5155912339687347}, {"text": "Fr-En)", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.8468966782093048}]}, {"text": " Table 5: English to French Translation (En-Fr)", "labels": [], "entities": [{"text": "English to French Translation", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.5730099380016327}]}, {"text": " Table 6: Chinese to English Translation (Zh-En)", "labels": [], "entities": [{"text": "Chinese to English Translation", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.5756045579910278}]}]}