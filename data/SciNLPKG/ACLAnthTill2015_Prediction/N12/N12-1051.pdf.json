{"title": [{"text": "Taxonomy Induction Using Hierarchical Random Graphs", "labels": [], "entities": [{"text": "Taxonomy Induction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7939600050449371}]}], "abstractContent": [{"text": "This paper presents a novel approach for inducing lexical taxonomies automatically from text.", "labels": [], "entities": []}, {"text": "We recast the learning problem as that of inferring a hierarchy from a graph whose nodes represent taxonomic terms and edges their degree of relatedness.", "labels": [], "entities": []}, {"text": "Our model takes this graph representation as input and fits a taxonomy to it via combination of a maximum likelihood approach with a Monte Carlo Sampling algorithm.", "labels": [], "entities": []}, {"text": "Essentially, the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph.", "labels": [], "entities": []}, {"text": "We use our model to infer a taxonomy over 541 nouns and show that it outperforms popular flat and hierarchical clustering algorithms .", "labels": [], "entities": []}], "introductionContent": [{"text": "The semantic knowledge encoded in lexical resources such as WordNet) has been proven beneficial for several applications including question answering (, document classification), and textual entailment).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9431939125061035}, {"text": "question answering", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.8852720558643341}, {"text": "document classification", "start_pos": 153, "end_pos": 176, "type": "TASK", "confidence": 0.752017468214035}, {"text": "textual entailment", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.7002816647291183}]}, {"text": "As the effort involved in creating such resources manually is prohibitive (cost, consistency and coverage are often cited problems) and has to be repeated for new languages or domains, recent years have seen increased interest in automatic taxonomy induction.", "labels": [], "entities": [{"text": "consistency", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.9793434143066406}, {"text": "taxonomy induction", "start_pos": 240, "end_pos": 258, "type": "TASK", "confidence": 0.8201473355293274}]}, {"text": "The task has assumed several guises, such as term extraction -finding the concepts of the taxonomy (), term relation discovery -learning whether any two terms stand in an semantic relation such as IS-A, or PART-OF), and taxonomy construction --creating the taxonomy proper by organizing its terms hierarchically ().", "labels": [], "entities": [{"text": "term extraction -finding the concepts", "start_pos": 45, "end_pos": 82, "type": "TASK", "confidence": 0.8163681825002035}, {"text": "term relation discovery", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.6659167110919952}, {"text": "taxonomy construction", "start_pos": 220, "end_pos": 241, "type": "TASK", "confidence": 0.8398776352405548}]}, {"text": "Previous work has also focused on the complementary task of augmenting an existing taxonomy with missing information (.", "labels": [], "entities": []}, {"text": "In this paper we propose an unsupervised approach to taxonomy induction.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.9452670812606812}]}, {"text": "Given a corpus and a set of terms, our algorithm jointly induces their relations and their taxonomic organization.", "labels": [], "entities": []}, {"text": "We view taxonomy learning as an instance of the problem of inferring a hierarchy from a network or graph.", "labels": [], "entities": [{"text": "taxonomy learning", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.8398712873458862}]}, {"text": "We create this graph from unstructured text simply by drawing an edge between distributionally similar terms.", "labels": [], "entities": []}, {"text": "Next, we fit a Hierarchical Random Graph model (HRG;) to the observed graph data based on maximum likelihood methods and Markov chain Monte Carlo sampling.", "labels": [], "entities": []}, {"text": "The model essentially works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph.", "labels": [], "entities": []}, {"text": "This is advantageous as it allows us to consider the ensemble of random graphs that are statistically similar to the original graph, and through this to derive a consensus hierarchical structure from the ensemble of sampled models.", "labels": [], "entities": []}, {"text": "The approach differs crucially from hierarchical clustering in that it explicitly acknowledges that most real-world networks have many plausible hierarchical representations of roughly equal likelihood and does not seek a single hierarchical representation fora given network.", "labels": [], "entities": []}, {"text": "This feature also bodes well with the nature of lexical taxonomies: there is no uniquely correct taxonomy fora set of terms, rather different taxonomies are likely to be appropriate for different tasks and different taxonomization criteria.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are three-fold: we adapt the HRG model to the taxonomy induction task and show that its performance is superior to alternative methods based on either flat or hierarchical clustering; we analyze the requirements of the algorithm with respect to the input graph and the semantic representation of its nodes; and introduce new ways of evaluating the fit of an automatically induced taxonomy against a gold-standard.", "labels": [], "entities": [{"text": "taxonomy induction task", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.8568849960962931}]}, {"text": "In the following section we provide an overview of related work.", "labels": [], "entities": []}, {"text": "Next, we describe our HRG model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4).", "labels": [], "entities": []}, {"text": "We conclude the paper by presenting and discussing our results (Sections 4.1-4.4).", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We evaluated our taxonomy induction algorithm using dataset which consists of for 541 basic level nouns (e.g., and TABLE).", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.787138432264328}, {"text": "TABLE", "start_pos": 120, "end_pos": 125, "type": "METRIC", "confidence": 0.8919671177864075}]}, {"text": "Each noun is associated with features (e.g., has-legs, is-flat, and made-of-wood for TABLE) collected from human participants in multiple studies over several years.", "labels": [], "entities": [{"text": "TABLE)", "start_pos": 85, "end_pos": 91, "type": "TASK", "confidence": 0.5621002465486526}]}, {"text": "The original norming study does not include class labels for these nouns, however, we were able to exploit a clustering provided by, in which a set of online participants annotated each of the McRae et al. nouns with basic category labels.", "labels": [], "entities": []}, {"text": "The nouns and their class labels were further taxonomized using WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9834303855895996}]}, {"text": "Specifically, we first identified the full hypernym path in WordNet for each noun in dataset, e.g., APPLE > PLANT STRUCTURE > NAT-URAL OBJECT > PHYSICAL OBJECT > ENTITY (a total of 493 concepts appear in both).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9604533314704895}, {"text": "APPLE > PLANT STRUCTURE", "start_pos": 100, "end_pos": 123, "type": "METRIC", "confidence": 0.4838179424405098}, {"text": "ENTITY", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9257757663726807}]}, {"text": "These hypernym paths were then combined to yield a full taxonomy over McRae et al.'s nouns; internal nodes having only a single child were recursively removed to produce a final, compact taxonomy 1 containing 186 semantic classes (e.g., ANIMALS, WEAPONS, FRUITS) organized into varying levels of granularity (e.g., SONGBIRDS > BIRDS >ANIMALS).", "labels": [], "entities": [{"text": "FRUITS", "start_pos": 255, "end_pos": 261, "type": "METRIC", "confidence": 0.9515489339828491}, {"text": "BIRDS", "start_pos": 327, "end_pos": 332, "type": "METRIC", "confidence": 0.9721431732177734}]}, {"text": "Evaluation measures Evaluation of taxonomically organized information is notoriously hard (see for an extensive discussion on this topic).", "labels": [], "entities": [{"text": "Evaluation of taxonomically organized information", "start_pos": 20, "end_pos": 69, "type": "TASK", "confidence": 0.8582568645477295}]}, {"text": "This is due to the nature of the task which is inherently subjective and application specific (e.g., a dolphin can be a Mammal to a biologist, but a Fish to a fisherman or someone visiting an aquarium).", "labels": [], "entities": []}, {"text": "Nevertheless, we assessed the taxonomies produced by the HRG against the WordNet-like taxonomy described above using two measures, one that simply evaluates the grouping of the nouns into classes without taking account of their position in the taxonomy and one which evaluates the taxonomy directly.", "labels": [], "entities": [{"text": "HRG", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.8846991062164307}]}, {"text": "To evaluate a flat clustering into classes we use the F-score measure introduced in the; it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively.", "labels": [], "entities": [{"text": "F-score measure", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.9805269539356232}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9959583878517151}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.997521698474884}]}, {"text": "Although informative, evaluation based solely on F-score puts the HRG model at a comparative disadvantage as the task of taxonomy induction is significantly more difficult than simple clustering.", "labels": [], "entities": [{"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9863249659538269}, {"text": "taxonomy induction", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.9284555912017822}]}, {"text": "To overcome this disadvantage we propose an automatic method of evaluating taxonomies directly by first computing the walk distance between pairs of terms that share a gold-standard category label within a gold-standard and a candidate taxonomy, and then computing the pairwise correlation between distances in each tree.", "labels": [], "entities": []}, {"text": "This captures the intuition that a 'good' hierarchy is one in which items appearing near one another in the gold taxonomy also appear near one another in the induced one.", "labels": [], "entities": []}, {"text": "It is also conceptually similar to the task-based IS-A evaluation () which has been traditionally used to evaluate taxonomies.", "labels": [], "entities": []}, {"text": "Formally, let G = {g 0,1 , g 0,2 . .", "labels": [], "entities": []}, {"text": "g n,n\u22121 }, where g a,b indicates the walk distance between terms a and b The taxonomy and flat cluster labels are available from http://homepages.inf.ed.ac.uk/s0897549/data.", "labels": [], "entities": []}, {"text": "in the gold standard hierarchy.", "labels": [], "entities": []}, {"text": "Similarly, let C = {c 0,1 , c 0,2 . .", "labels": [], "entities": []}, {"text": "c n,n\u22121 }, where c a,b is the distance between a and bin the candidate hierarchy.", "labels": [], "entities": []}, {"text": "The treeheight correlation between G and C is then given by Spearman's \u03c1 correlation coefficient between the two sets.", "labels": [], "entities": [{"text": "\u03c1 correlation coefficient", "start_pos": 71, "end_pos": 96, "type": "METRIC", "confidence": 0.8806935946146647}]}, {"text": "All tree-height correlations reported in our experiments were computed using the WordNetbased gold-standard taxonomy over nouns.", "labels": [], "entities": [{"text": "WordNetbased gold-standard taxonomy", "start_pos": 81, "end_pos": 116, "type": "DATASET", "confidence": 0.9600170850753784}]}, {"text": "Baselines We compared the HRG output against three baselines.", "labels": [], "entities": []}, {"text": "The first is Chinese Whispers (CW;), a randomized graph-clustering algorithm which like the HRG also takes as input a graph with weighted edges.", "labels": [], "entities": [{"text": "HRG", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.9026472568511963}]}, {"text": "It produces a hard (flat) clustering over the nodes in the graph, where the number of clusters is determined automatically.", "labels": [], "entities": []}, {"text": "Our second baseline is agglomerative clustering algorithm that induces a mapping from word types to classes.", "labels": [], "entities": []}, {"text": "It starts with K classes for the K most frequent word types and then proceeds by alternately adding the next most frequent word to the class set and merging the two classes which result in the least decrease in the mutual information between class bigrams.", "labels": [], "entities": []}, {"text": "The result is a class hierarchy with word types at the leaves.", "labels": [], "entities": []}, {"text": "Additionally, we compare against standard agglomerative clustering) which produces a binary dendrogram in a bottom-up fashion by recursively identifying concepts or clusters with the highest pairwise similarity.", "labels": [], "entities": []}, {"text": "In the following, we present our taxonomy induction experiments (Sections 4.1-4.3).", "labels": [], "entities": []}, {"text": "Since HRGs provide a means of inducing a hierarchy over a graph-based representation, which maybe constructed in an arbitrary fashion, our experiments were designed to investigate how the topology and quality of the input graph influences the algorithm's performance.", "labels": [], "entities": []}, {"text": "We thus report results when the semantic network is created from data sources of varying quality and granularity.", "labels": [], "entities": []}, {"text": "Method We first considered the case where the input graph is of high semantic quality and constructed a semantic network from the feature norms collected by.", "labels": [], "entities": []}, {"text": "Each noun was represented as a vector with dimensions corresponding to the possible features generated by participants of the norming study; the value of a term along a dimen- sion was taken to be the frequency with which participants generated the corresponding feature when given the term.", "labels": [], "entities": []}, {"text": "For each pair of terms an edge was added to the semantic network if the cosine similarity between their vector representations exceeded a fixed threshold T (set to 0.15).", "labels": [], "entities": []}, {"text": "The resulting network was then provided as input to the HRG, which was resampled until convergence.", "labels": [], "entities": [{"text": "HRG", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8250936269760132}]}, {"text": "The binary tree at convergence was collapsed into a hierarchy over clusters using the procedure described in Section 3.4; this hierarchy was evaluated by computing the cluster F-score between its constituent clusters and those of a gold-standard (human-produced) clustering.", "labels": [], "entities": [{"text": "F-score", "start_pos": 176, "end_pos": 183, "type": "METRIC", "confidence": 0.900337278842926}]}, {"text": "The resulting consensus hierarchy was evaluated by computing the treeheight correlation between it and the gold-standard (WordNet-derived) hierarchy.", "labels": [], "entities": []}, {"text": "Method The results of Experiment 1 can be considered as an upper bound of what can be achieved by the HRG when the input graph is constructed from highly accurate semantic information.", "labels": [], "entities": []}, {"text": "Feature norms provide detailed knowledge about meaning which would be very difficult if not close to impossible to obtain from a corpus.", "labels": [], "entities": []}, {"text": "Nevertheless, it is interesting to explore how well we can induce taxonomies using a lower quality semantic network.", "labels": [], "entities": []}, {"text": "We therefore constructed a network based on cooccurrence statistics computed from the British National Corpus and provided the resulting semantic network as input to the HRG, CW, and Agglo models; additionally, we employed the algorithm of to induce a hierarchy over the target terms directly from the corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 86, "end_pos": 109, "type": "DATASET", "confidence": 0.9557144641876221}, {"text": "HRG", "start_pos": 170, "end_pos": 173, "type": "DATASET", "confidence": 0.9553974866867065}]}, {"text": "Unfortunately, this algorithm requires the number of desired output clusters to be specified in advance; in all trials this parameter was set to the number of clusters in the gold-standard clustering, thus providing the Brown-induced clusterings with a slight oracle advantage.", "labels": [], "entities": []}, {"text": "Again, nouns were represented as vectors in semantic space.", "labels": [], "entities": []}, {"text": "We used a context window of five words on either side of the target word and 5,000 vector components corresponding to the most frequent non-stopwords in the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 157, "end_pos": 160, "type": "DATASET", "confidence": 0.9420226216316223}]}, {"text": "Raw frequency counts were transformed using pointwise mutual information (PMI).", "labels": [], "entities": []}, {"text": "An edge was added to the semantic network between a pair of nouns if their similarity exceeded a predefined threshold (the same as in Experiment 1).", "labels": [], "entities": []}, {"text": "The similarity of two nouns was defined as the cosine distance between their corresponding vectors.", "labels": [], "entities": []}, {"text": "The HRG algorithm was used to produce a taxonomy from this network and was also compared against.", "labels": [], "entities": []}, {"text": "The latter induces a hierarchy from a corpus directly, without the intermediate graph representation.", "labels": [], "entities": []}, {"text": "All resulting taxonomies were evaluated against gold standard flat and hierarchical clusterings, again as in Experiment 1.", "labels": [], "entities": []}, {"text": "Results Results are shown in.", "labels": [], "entities": []}, {"text": "With regard to flat clustering (the F-score column in the table), the HRG has a slight advantage against CW, and algorithm (Brown).", "labels": [], "entities": []}, {"text": "However, differences in performance are not statistically significant.", "labels": [], "entities": []}, {"text": "Agglomerative clustering is the worst performing method leading to a decrease in F-score of approximately 1.5.", "labels": [], "entities": [{"text": "F-score", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9995618462562561}]}, {"text": "With regard to tree correlation, the output of the HGRG is comparable to Brown (the difference between the two is not statistically significant).", "labels": [], "entities": []}, {"text": "Both algorithms are significantly better (p < 0.01) than Agglo.", "labels": [], "entities": []}, {"text": "Performance of the HRG is better when the semantic network is based on feature norms (compare, both in terms of tree-height correlation and F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 140, "end_pos": 147, "type": "METRIC", "confidence": 0.9829220771789551}]}, {"text": "This suggests that the algorithm is highly dependent on the quality of the semantic network used as input.", "labels": [], "entities": []}, {"text": "In particular, HRGs are known to be more appropriate for so-called small-world networks, graphs composed of densely-connected subgraphs with relatively sparse connections between (.", "labels": [], "entities": []}, {"text": "Indeed, inspection of the semantic network produced from the BNC (see shows that our corpus-derived graph is emphatically not a small-world graph, yet the HRG is able to recover some taxonomic information from such a densely-connected network.", "labels": [], "entities": []}, {"text": "In the following experiments we first assess the difficulty of the taxonomy induction task to get a feel of how well the algorithms are performing in comparison to humans and then investigate ways of rendering the BNC-based graph more similar to a small-world network.", "labels": [], "entities": [{"text": "taxonomy induction task", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.8892697691917419}]}, {"text": "Method The previous experiments evaluated the performance of the HRG against a gold-standard hierarchy derived from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.9644604325294495}]}, {"text": "For any set of concepts there will exist multiple valid taxonomies, each representing an accurate if differing organization of identical concepts using different criteria; for the set of concepts used in Experiments 1-2 the WordNet hierarchy represents merely one of many valid hierarchies.", "labels": [], "entities": []}, {"text": "Noting this, it is interesting to explore how well the hierarchies output by the model fit within the set of possible, valid taxonomies over a given set of concepts.", "labels": [], "entities": []}, {"text": "We thus conducted an experiment in which human participants were asked to organize words into arbitrary hierarchies.", "labels": [], "entities": []}, {"text": "To render the task feasible, they were given a small subset of 12 words rather than the full set of 541 nouns over which the HRG operates.", "labels": [], "entities": []}, {"text": "We first selected a sub-hierarchy of the WordNet tree ('living things') along with its subtrees (e.g., 'animals', 'plants'), and chose target concepts from within these trees in order to produce a taxonomy in which some items were differentiated at a high level (e.g., 'python' vs. 'dog') and others at a fine-grained level (e.g., 'lion' vs 'tiger').", "labels": [], "entities": [{"text": "WordNet tree", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.9381105303764343}]}, {"text": "The experiment was conducted using Amazon Mechanical Turk 2 , and involved 41 participants, all self-reported native English speakers.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk 2", "start_pos": 35, "end_pos": 59, "type": "DATASET", "confidence": 0.9596003890037537}]}, {"text": "No guidelines as to what features participants were to use when organizing these concepts were provided.", "labels": [], "entities": []}, {"text": "Participants were presented with a web-based, graphical, mouse-driven interface for constructing a taxonomy over the cho-: Model performance on a subset of the target words used in Experiments 1-2, applied to a subset of the semantic network used in Experiment 2.", "labels": [], "entities": []}, {"text": "Instead of a WordNet-derived hierarchy, models were evaluated against hierarchies manually produced by participants in an online study.", "labels": [], "entities": []}, {"text": "Tree correlation values are means; we also report the minimum (Min), maximum (Max), and standard deviation (Std) of the mean.", "labels": [], "entities": [{"text": "standard deviation (Std)", "start_pos": 88, "end_pos": 112, "type": "METRIC", "confidence": 0.9732576131820678}]}, {"text": "To evaluate the HRG, along with the baselines from Experiment 2, against the resulting hierarchies we constructed a semantic network over the subset of concepts using similarities derived from the BNC; this network was a subgraph of that used in Experiment 2.", "labels": [], "entities": []}, {"text": "We compute inter-annotator agreement as the mean pairwise tree-height correlation between the hierarchies our participants produced.", "labels": [], "entities": []}, {"text": "We also report for each model the mean tree-height correlation between the hierarchy it produced and those created by human annotators.", "labels": [], "entities": []}, {"text": "Results As shown in, participants achieve a mean pairwise tree correlation of 0.511.", "labels": [], "entities": [{"text": "pairwise tree correlation", "start_pos": 49, "end_pos": 74, "type": "METRIC", "confidence": 0.5009380479653677}]}, {"text": "This indicates that there is a fair amount of agreement with respect to the taxonomic organization of the words in question.", "labels": [], "entities": []}, {"text": "The HRG comes close achieving a mean tree correlation of 0.412, followed by Agglo, and Brown.", "labels": [], "entities": [{"text": "HRG", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.65399569272995}, {"text": "mean tree correlation", "start_pos": 32, "end_pos": 53, "type": "METRIC", "confidence": 0.687302827835083}]}, {"text": "In general, we observe that the HRG manages to produce hierarchies that resemble those generated by humans to a larger extent than competing algorithms.", "labels": [], "entities": []}, {"text": "The results in also hint at the fact that the taxonomy induction task is relatively hard as participants do not achieve perfect agreement despite the fact that they are asked to taxonomize only 12 words.", "labels": [], "entities": [{"text": "taxonomy induction task", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.8852580587069193}]}, {"text": "Small-world Network Method In Experiment 2 we hypothesized that a small-world input graph would be more advantageous for the HRG.", "labels": [], "entities": []}, {"text": "In order to explore this further, we imposed something of a small-world structure on  the BNC semantic network, using a combination of the baseline clustering methods evaluated in Experiment 2.", "labels": [], "entities": [{"text": "BNC semantic network", "start_pos": 90, "end_pos": 110, "type": "DATASET", "confidence": 0.784597376982371}]}, {"text": "Specifically, we first obtain a (flat) clustering using either CW or Brown, which we then use to re-weight the BNC graph given as input to the HRG.", "labels": [], "entities": [{"text": "HRG", "start_pos": 143, "end_pos": 146, "type": "DATASET", "confidence": 0.898608922958374}]}, {"text": "Note that, as the clustering algorithms used are unsupervised this procedure does not introduce any outside supervision into the overall taxonomy induction task.", "labels": [], "entities": [{"text": "taxonomy induction task", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.8766998251279196}]}, {"text": "The modified weight W A,B between a pair of terms A, B was computed according to Equation, where s indicates the proportion of edge weight drawn from the clustering, W A,B is the edge weight in the original (BNC) semantic network, and C A,B is a binary value indicating that A and B belong to the same cluster (i.e., C A,B = 1 if A and B share a cluster; C A,B = 0 otherwise).", "labels": [], "entities": [{"text": "Equation", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9920640587806702}]}, {"text": "The value of the s parameter was tuned empirically on held-out development data and set to s = 0.4 for both CW and Brown algorithms.", "labels": [], "entities": []}, {"text": "Each re-weighted network was then used as input to an HRG, and the resulting taxonomies were evaluated in the same manner as in Experiments 1 and 2.", "labels": [], "entities": []}, {"text": "Results shows results for cluster F-score and tree-height correlation for the HRG when using a graph derived from the BNC without any modifications, and two re-weighted versions using the CW and Brown clustering algorithms, respectively.", "labels": [], "entities": [{"text": "cluster F-score", "start_pos": 26, "end_pos": 41, "type": "METRIC", "confidence": 0.6916148662567139}, {"text": "HRG", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.8162543177604675}]}, {"text": "As can been seen, re-weighting improves tree-height correlation substantially: HRG with CW and Brown is significantly better than HRG on its own (p < 0.05).", "labels": [], "entities": []}, {"text": "In the case of CW, cluster F-score also yields a slight improvement.", "labels": [], "entities": [{"text": "cluster", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9589589834213257}, {"text": "F-score", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.6715781092643738}]}, {"text": "Interestingly, the tree-height correlations obtained with CW and Brown are comparable to those attained by the HRG when using the human-produced feature norms (differences in correlations are not statistically significant).", "labels": [], "entities": []}, {"text": "An excerpt of a HRG-induced taxonomy is shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cluster F-score and tree-height correla- tion evaluation; a semantic network constructed over  McRae et al.'s (2005) nouns and features is given as  input to the algorithms.", "labels": [], "entities": []}, {"text": " Table 2: Cluster F-score and tree-height correla- tion evaluation for taxonomies inferred over McRae  et al.'s (2005) nouns; all algorithms are run on the  BNC.", "labels": [], "entities": [{"text": "Cluster F-score", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7271201312541962}, {"text": "BNC", "start_pos": 157, "end_pos": 160, "type": "DATASET", "confidence": 0.9736189246177673}]}, {"text": " Table 3: Model performance on a subset of the  target words used in Experiments 1-2, applied to  a subset of the semantic network used in Experi- ment 2. Instead of a WordNet-derived hierarchy,  models were evaluated against hierarchies manually  produced by participants in an online study. Tree  correlation values are means; we also report the min- imum (Min), maximum (Max), and standard devia- tion (Std) of the mean.", "labels": [], "entities": [{"text": "devia- tion (Std)", "start_pos": 393, "end_pos": 410, "type": "METRIC", "confidence": 0.7895765602588654}]}, {"text": " Table 4: Cluster F-score and tree-height correlation  evaluation for taxonomies inferred by the HRG us- ing semantic network derived from the BNC and re- weighted using CW and Brown.", "labels": [], "entities": [{"text": "F-score", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.5025761127471924}, {"text": "HRG us- ing semantic network", "start_pos": 97, "end_pos": 125, "type": "DATASET", "confidence": 0.742697944243749}, {"text": "BNC", "start_pos": 143, "end_pos": 146, "type": "DATASET", "confidence": 0.8846248984336853}]}]}