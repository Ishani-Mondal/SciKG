{"title": [{"text": "Behavioral Factors in Interactive Training of Text Classifiers", "labels": [], "entities": [{"text": "Interactive Training of Text Classifiers", "start_pos": 22, "end_pos": 62, "type": "TASK", "confidence": 0.5843070149421692}]}], "abstractContent": [{"text": "This paper describes a user study where humans interactively train automatic text clas-sifiers.", "labels": [], "entities": []}, {"text": "We attempt to replicate previous results using multiple \"average\" Internet users instead of a few domain experts as annotators.", "labels": [], "entities": []}, {"text": "We also analyze user annotation behaviors to find that certain labeling actions have an impact on classifier accuracy, drawing attention to the important role these behavioral factors play in interactive learning systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9481968879699707}]}], "introductionContent": [{"text": "There is growing interest in methods that incorporate human domain knowledge in machine learning algorithms, either as priors on model parameters or as constraints in an objective function.", "labels": [], "entities": []}, {"text": "Such approaches lend themselves well to natural language tasks, where input features are often discrete variables that carry semantic meaning (e.g., words).", "labels": [], "entities": []}, {"text": "A feature label is a simple but expressive form of domain knowledge that has received considerable attention recently.", "labels": [], "entities": []}, {"text": "For example, a single feature (word) can be used to indicate a particular label or set of labels, such as \"excellent\" \u21d2 positive or \"terrible\" \u21d2 negative, which might be useful word-label rules fora sentiment analysis task.", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 199, "end_pos": 222, "type": "TASK", "confidence": 0.9318464795748392}]}, {"text": "Contemporary work has also focused on making such learning algorithms active, by enabling them to pose \"queries\" in the form of feature-based rules to be labeled by annotators in addition toand sometimes lieu of -data instances such as documents ().", "labels": [], "entities": []}, {"text": "These concepts were recently implemented in a practical system for interactive training of text classifiers called DUALIST . reports that, in user experiments with real annotators, humans were able to train near state of the art classifiers with only a few minutes of effort.", "labels": [], "entities": []}, {"text": "However, there were only five subjects, who were all computer science researchers.", "labels": [], "entities": []}, {"text": "It is possible that these positive results can be attributed to the subjects' implicit familiarity with machine learning and natural language processing algorithms.", "labels": [], "entities": []}, {"text": "This short paper sheds more light on previous experiments by replicating them with many more human subjects, and of a different type: non-experts recruited through the Amazon Mechanical Turk service 2 . We also analyze the impact of annotator behavior on the resulting classifiers, and suggest relationships to recent work in curriculum learning.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk service", "start_pos": 168, "end_pos": 198, "type": "DATASET", "confidence": 0.9159300327301025}]}, {"text": "shows a screenshot of DUALIST, an interactive machine learning system for quickly building text classifiers.", "labels": [], "entities": []}, {"text": "The annotator is allowed to take three kinds of actions: \u008c label query documents (instances) by clicking class-label buttons in the left panel, \u008d label query words (features) by selecting them from the class-label columns in the right panel, or \u008e \"volunteer\" domain knowledge by typing labeled words into a text box at the top of each class column.", "labels": [], "entities": []}, {"text": "The underlying classifier is a na\u00a8\u0131vena\u00a8\u0131ve Bayes variant combining informative priors, maximum likelihood estimation, and the EM algorithm for fast semi-supervised training.", "labels": [], "entities": []}, {"text": "When a user performs action \u008c or \u008d, she labels queries that should help minimize the classifier's uncertainty on unlabeled documents (according to active learning heuristics).", "labels": [], "entities": []}, {"text": "For action \u008e, the user is free to volunteer any relevant word, whether or not it appears in a document or word column.", "labels": [], "entities": []}, {"text": "For example, the user might volunteer the labeled word \"oscar\" \u21d2 positive in a sentiment analysis task for movie reviews (leveraging her knowledge of domain), even if the word \"oscar\" does not appear anywhere in the interface.", "labels": [], "entities": [{"text": "sentiment analysis task for movie reviews", "start_pos": 79, "end_pos": 120, "type": "TASK", "confidence": 0.8970905641714731}]}, {"text": "This flexibility goes beyond traditional active learning, which restricts the user to feedback on items queried by the learner (i.e., actions \u008c and \u008d).", "labels": [], "entities": []}, {"text": "After a few labeling actions, the user submits her feedback and receives the next set of queries in real time.", "labels": [], "entities": []}, {"text": "For more details, see Settles (2011).", "labels": [], "entities": [{"text": "Settles (2011)", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.6867337822914124}]}], "datasetContent": [{"text": "We recruited annotators through the crowdsourcing marketplace Mechanical Turk.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.8849654495716095}]}, {"text": "Subjects were shown a tutorial page with a brief description of the classification task, as well as a cartoon of the interface similar to explaining the various annotation options.", "labels": [], "entities": []}, {"text": "When they decided they were ready, users followed a link to a web server running a customized version of DUALIST, which is an open source webbased application.", "labels": [], "entities": []}, {"text": "At the end of each trial, subjects were given a confirmation code to receive payment.", "labels": [], "entities": []}, {"text": "We conducted experiments using two corpora from the original DUALIST study: Science (a subset of the 20 Newsgroups benchmark: cryptography, electronics, medicine, and space) and Movie Reviews (a sentiment analysis collection).", "labels": [], "entities": [{"text": "DUALIST study", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.876991331577301}]}, {"text": "These are not specialized domains, i.e., we could expect average Internet users to be knowledgable enough to perform the annotations.", "labels": [], "entities": []}, {"text": "While both are generally accessible, these corpora represent different types of tasks and vary both in number of categories (four vs. two) and difficulty (Movie Reviews is known to be harder for learning algorithms).", "labels": [], "entities": [{"text": "difficulty", "start_pos": 143, "end_pos": 153, "type": "METRIC", "confidence": 0.9672653675079346}]}, {"text": "We replicated the same experimental conditions as previous work: DUALIST (the full interface in), active-doc (the left-hand \u008c document panel only), and passivedoc (the \u008c document panel only, but with texts selected at random and not queried by active learning).", "labels": [], "entities": [{"text": "DUALIST", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.8525967597961426}]}, {"text": "For each condition, we recruited 25 users for the Science corpus (75 total) and 35 users for Movie Reviews (105 total).", "labels": [], "entities": [{"text": "Science corpus", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9202647507190704}]}, {"text": "We were careful to publish tasks on MTurk in away that no one user annotated more than one condition.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.8722700476646423}]}, {"text": "Some users experienced technical difficulties that nullified their work, and four appeared to be spammers 3 . After removing these subjects from the analysis, we were left with 23 users for the Science DUALIST condition, 25 each for the two document-only conditions (73 total), 32 users for the Movie Reviews DUALIST condition, and 33 each for the document-only conditions (98 total).", "labels": [], "entities": []}, {"text": "DUALIST automatically logged data about user actions and model accuracies as training progressed, although users could not see these statistics.", "labels": [], "entities": []}, {"text": "Trials lasted 6 minutes for the Science corpus and 10 minutes for Movie Reviews.", "labels": [], "entities": [{"text": "Science corpus", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.932049036026001}]}, {"text": "We did advertise a \"bonus\" for the user who trained the best classifier to encourage correctness, but otherwise offered no guidance on how subjects should prioritize their time.", "labels": [], "entities": []}, {"text": "standard passive learning, which is consistent with previous work.", "labels": [], "entities": []}, {"text": "However, for Movie Reviews (bottom), there is little difference among the three settings, and in fact models trained with DUALIST appear to lag behind active learning with documents.(b) shows the distribution of final classifier accuracies in each condition.", "labels": [], "entities": [{"text": "DUALIST", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.7611688375473022}]}, {"text": "For Science, the DUALIST users are significantly better than either of the baselines (two-sided KS test, p < 0.005).", "labels": [], "entities": [{"text": "DUALIST", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.8522859811782837}, {"text": "KS test", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.7420312762260437}]}, {"text": "While the differences in DUALIST accuracies are not significantly different, we can see that the top quartile does much better than the two baselines.", "labels": [], "entities": [{"text": "DUALIST accuracies", "start_pos": 25, "end_pos": 43, "type": "METRIC", "confidence": 0.8238171339035034}]}, {"text": "Clearly some DUALIST users are making better use of the interface and training better classifiers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The range of volunteered words and number of  users in each behavioral subgroup of DUALIST subjects.", "labels": [], "entities": []}, {"text": " Table 2: Linear regressions estimating the accuracy of a  classifier as a function of annotator actions and behaviors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9955239295959473}]}]}