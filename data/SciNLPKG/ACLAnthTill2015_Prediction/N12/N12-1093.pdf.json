{"title": [], "abstractContent": [{"text": "Concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input.", "labels": [], "entities": [{"text": "Concept-to-text generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7027543485164642}]}, {"text": "We present a joint model that captures content selection (\"what to say\") and surface realization (\"how to say\") in an unsupervised domain-independent fashion.", "labels": [], "entities": []}, {"text": "Rather than breaking up the generation process into a sequence of local decisions, we define a probabilistic context-free grammar that globally describes the inherent structure of the input (a corpus of database records and text describing some of them).", "labels": [], "entities": []}, {"text": "We represent our grammar compactly as a weighted hypergraph and recast generation as the task of finding the best derivation tree fora given input.", "labels": [], "entities": []}, {"text": "Experimental evaluation on several domains achieves competitive results with state-of-the-art systems that use domain specific constraints, explicit feature engineering or labeled data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Concept-to-text generation broadly refers to the task of automatically producing textual output from nonlinguistic input).", "labels": [], "entities": [{"text": "Concept-to-text generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6932921558618546}]}, {"text": "Depending on the application and the domain at hand, the input may assume various representations including databases of records, expert system knowledge bases, simulations of physical systems and soon.", "labels": [], "entities": []}, {"text": "shows input examples and their corresponding text for three domains, air travel, sportscasting and weather forecast generation.", "labels": [], "entities": [{"text": "weather forecast generation", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.6739151179790497}]}, {"text": "A typical concept-to-text generation system implements a pipeline architecture consisting of three core stages, namely text planning (determining the content and structure of the target text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (rendering the specification chosen by the sentence planner into a surface string).", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 193, "end_pos": 210, "type": "TASK", "confidence": 0.7356330752372742}, {"text": "surface realization", "start_pos": 288, "end_pos": 307, "type": "TASK", "confidence": 0.7310207784175873}]}, {"text": "Traditionally, these components are hand-engineered in order to generate high quality text, however at the expense of portability and scalability.", "labels": [], "entities": []}, {"text": "It is thus no surprise that recent years have witnessed a growing interest in automatic methods for creating trainable generation components.", "labels": [], "entities": []}, {"text": "Examples include learning which database records should be present in a text (; and how these should be verbalized (.", "labels": [], "entities": []}, {"text": "Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end.", "labels": [], "entities": []}, {"text": "Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component.", "labels": [], "entities": []}, {"text": "A common modeling strategy is to breakup the generation process into a sequence of local decisions, each learned separately (.", "labels": [], "entities": []}, {"text": "In this paper we describe an end-to-end generation model that performs content selection and surface realization jointly.", "labels": [], "entities": [{"text": "content selection", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7462097704410553}, {"text": "surface realization", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7589066028594971}]}, {"text": "Given a corpus of database records and textual descriptions (for some of them), we define a probabilistic context-free grammar (PCFG) that captures the structure of the database and how it can be rendered into natural Cloud Sky Cover Time Percent (%) 06:00-09: 25-50 09:00-12:00 50-75", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We used our system to generate soccer commentaries, weather forecasts, and spontaneous utterances relevant to the air travel domain (examples are given in).", "labels": [], "entities": []}, {"text": "For the first domain we used the dataset of, which consists of 1,539 scenarios from the 2001-2004 Robocup game finals.", "labels": [], "entities": []}, {"text": "Each scenario contains on average |d| = 2.4 records, each paired with a short sentence (5.7 words).", "labels": [], "entities": []}, {"text": "This domain has a small vocabulary (214 words) and simple syntax (e.g., a transitive verb with its subject and object).", "labels": [], "entities": []}, {"text": "Records in this dataset (henceforth ROBOCUP) were aligned manually to their corresponding sentences.", "labels": [], "entities": []}, {"text": "Given the relatively small size of this dataset, we performed cross-validation following previous work.", "labels": [], "entities": []}, {"text": "We trained our system on three ROBOCUP games and tested on the fourth, averaging over the four train/test splits.", "labels": [], "entities": []}, {"text": "For weather forecast generation, we used the dataset of, which consists of 29,528 weather scenarios for 3,753 major US cities (collected over four days).", "labels": [], "entities": [{"text": "weather forecast generation", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.752499391635259}]}, {"text": "The vocabulary in this domain (henceforth WEATHERGOV) is comparable to ROBOCUP (345 words), however, the texts are longer (|w| = 29.3) and more varied.", "labels": [], "entities": [{"text": "ROBOCUP", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9007298350334167}]}, {"text": "On average, each forecast has 4 sentences and the content selection problem is more challenging; only 5.8 out of the 36 records per scenario are mentioned in the text which roughly corresponds to 1.4 records per sentence.", "labels": [], "entities": [{"text": "content selection", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.732130616903305}]}, {"text": "We used 25,000 scenarios from WEATHER-GOV for training, 1,000 scenarios for development and 3,528 scenarios for testing.", "labels": [], "entities": [{"text": "WEATHER-GOV", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.9288575053215027}]}, {"text": "This is the same partition used in.", "labels": [], "entities": []}, {"text": "For the air travel domain we used the ATIS dataset (, consisting of 5,426 scenarios.", "labels": [], "entities": [{"text": "ATIS dataset", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.968750387430191}]}, {"text": "These are transcriptions of spontaneous utterances of users interacting with a hypothetical on-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: BLEU scores on ROBOCUP (fixed content se- lection), WEATHERGOV, and ATIS.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998994767665863}, {"text": "ROBOCUP", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9299333691596985}, {"text": "WEATHERGOV", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.7028789520263672}, {"text": "ATIS", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.7078172564506531}]}, {"text": " Table 4: Mean ratings for fluency (F) and semantic cor- rectness (SC) on system output elicited by humans on  ROBOCUP, WEATHERGOV, and ATIS (  *  : sig. diff. from  HUMAN;  \u2020 : sig. diff. from k-BEST.)", "labels": [], "entities": [{"text": "fluency (F)", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.6583236083388329}, {"text": "semantic cor- rectness (SC)", "start_pos": 43, "end_pos": 70, "type": "METRIC", "confidence": 0.7989965975284576}, {"text": "ATIS", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.4984823167324066}]}]}