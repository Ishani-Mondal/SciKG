{"title": [{"text": "Multi Event Extraction Guided by Global Constraints", "labels": [], "entities": [{"text": "Multi Event Extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7101319233576456}]}], "abstractContent": [{"text": "This paper addresses the extraction of event records from documents that describe multiple events.", "labels": [], "entities": []}, {"text": "Specifically, we aim to identify the fields of information contained in a document and aggregate together those fields that describe the same event.", "labels": [], "entities": []}, {"text": "To exploit the inherent connections between field extraction and event identification, we propose to model them jointly.", "labels": [], "entities": [{"text": "field extraction", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7035238593816757}, {"text": "event identification", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.7538307011127472}]}, {"text": "Our model is novel in that it integrates information from separate sequential models, using global potentials that encourage the extracted event records to have desired properties.", "labels": [], "entities": []}, {"text": "While the model contains high-order potentials, efficient approximate inference can be performed with dual-decomposition.", "labels": [], "entities": []}, {"text": "We experiment with two data sets that consist of newspaper articles describing multiple terrorism events, and show that our model substantially outperforms traditional pipeline models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Today, most efforts in information extraction have focused on the field extraction task, commonly formulated as a sequence tagging problem.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.8987417221069336}, {"text": "field extraction task", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7660117546717325}, {"text": "sequence tagging problem", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.7712682882944742}]}, {"text": "When a document describes a single event, the list of extracted fields provides a useful abstraction of the input document.", "labels": [], "entities": []}, {"text": "In practice, however, atypical newspaper document describes multiple events, and a flat list of field values may not contain the sufficient structure required for many NLP applications.", "labels": [], "entities": []}, {"text": "Our goal is therefore to extract event templates which aggregate field values for individual events.", "labels": [], "entities": []}, {"text": "Consider, for instance, the New York Times article excerpt in that describes three related terrorist events.", "labels": [], "entities": [{"text": "New York Times article excerpt", "start_pos": 28, "end_pos": 58, "type": "DATASET", "confidence": 0.6357227802276612}]}, {"text": "As this example illustrates, in order to populate the corresponding event templates, the model needs to identify segments that describe individual events.", "labels": [], "entities": []}, {"text": "Such segmentation is challenging, as event boundaries are not explicitly demarcated in the text.", "labels": [], "entities": []}, {"text": "Moreover, descriptions of different events are often intermingled, as in the above example, further complicating boundary recovery.", "labels": [], "entities": [{"text": "boundary recovery", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7514725029468536}]}, {"text": "In this paper, we consider a model that jointly performs event segmentation and field extraction.", "labels": [], "entities": [{"text": "event segmentation", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7421489655971527}, {"text": "field extraction", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.6792447715997696}]}, {"text": "This model capitalizes on the inherent connection between the two tasks in order to reduce the ambiguity of template-based extraction.", "labels": [], "entities": [{"text": "template-based extraction", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.7080741822719574}]}, {"text": "For example, the distribution of field values in the text provides strong clues about event segmentation, such as the presence of multiple new fields strongly signaling a segment boundary.", "labels": [], "entities": [{"text": "event segmentation", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7443223893642426}]}, {"text": "Likewise, knowledge of the boundaries enables the model to rule out mutually inconsistent predictions, such as extracting two distinct locations for the same event.", "labels": [], "entities": []}, {"text": "We formulate our approach as a joint model that marks each word with field and event labels simultaneously.", "labels": [], "entities": []}, {"text": "At the sentence level, segmentation and field extraction taggers are implemented using separate sequence models operating over local features.", "labels": [], "entities": [{"text": "field extraction taggers", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.7917955418427786}]}, {"text": "At the document level, the model encourages global consistency via potentials that link the extracted event records and their fields.", "labels": [], "entities": []}, {"text": "Some of these potentials are limited to fields of an individual event such as the \"single city per event\" constraint.", "labels": [], "entities": []}, {"text": "Others encode discourse-level properties of the whole document and thus involve records of multiple events, A powerful car bomb exploded today in Baghdad inside the holiest Shiite shrine . As many as 95 people were killed in the event, according to sources in Washington.", "labels": [], "entities": []}, {"text": "The blast came only two days after another car bomb exploded in a crowded street in Mosul in the northern part of Iraq, killing 13 pedestrians, in an attack carried out by Al Qaeda.", "labels": [], "entities": []}, {"text": "Together with the previous attack by Al Qaeda, the shooting in Najaf three weeks ago that killed 15 American soldiers, violence seemed to spike to its highest level.", "labels": [], "entities": []}, {"text": "The bombing today, happened around 9am, when the roads are crowded with people.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data This work focuses on multi-event extraction.", "labels": [], "entities": [{"text": "multi-event extraction", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.85941943526268}]}, {"text": "While some of the articles in the MUC test corpus do have multiple events, the majority contain only one (77.5%) or two (12%).", "labels": [], "entities": [{"text": "MUC test corpus", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9684465726216634}]}, {"text": "We therefore created two corpora for our experiments.", "labels": [], "entities": []}, {"text": "The first is anew corpus of 70 articles from New York Times (NYT) LDC corpus, each describing one or more terrorist events from various parts of the world.", "labels": [], "entities": [{"text": "New York Times (NYT) LDC corpus", "start_pos": 45, "end_pos": 76, "type": "DATASET", "confidence": 0.6744279749691486}]}, {"text": "The second, also of 70 articles, consists of a subset of the MUC articles that describe more than one event.", "labels": [], "entities": [{"text": "MUC articles", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.8265019059181213}]}, {"text": "We stripped this corpus from the MUC annotation and annotated it according to our scheme.", "labels": [], "entities": [{"text": "MUC annotation", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.9473856091499329}]}, {"text": "Annotations were provided by two annotators with graduate school educations.", "labels": [], "entities": []}, {"text": "Every word was tagged with afield and an event id.", "labels": [], "entities": []}, {"text": "The 8 fields we use are: Terrorist Organization, Target, Tactic, Weapon, Fatalities, Injuries, Country and City.", "labels": [], "entities": [{"text": "Fatalities", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9515806436538696}]}, {"text": "We compared the agreement between annotators on 10 articles by computing the percentage of words for which the annotators gave the same labeling.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement was 90.9% (kappa = 0.9) when fields and events are evaluated together (i.e., the annotators are considered to agree only when they assign the same field and event id to the word), 97.8% (kappa = 0.97) for events only, and 92% (kappa = 0.91) for fields only.", "labels": [], "entities": []}, {"text": "The two corpora differ from each other with respect to several important properties.", "labels": [], "entities": []}, {"text": "The New-York Times articles are longer (40.3 compared to 12.4 sentences per article) and describe a larger number of events (4.4 compared to 3.1 events per article on average).", "labels": [], "entities": []}, {"text": "In addition, while our hypothesis about the predominance of the main (first) event coverage holds for both corpora, it better characterizes the New-York Times corpus, as is demonstrated by the following two statistics.", "labels": [], "entities": [{"text": "New-York Times corpus", "start_pos": 144, "end_pos": 165, "type": "DATASET", "confidence": 0.870553215344747}]}, {"text": "First, in the NYT corpus the average number of sentences containing field fillers for the main event is 14.7, while for any other event the average number is 3.2.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 14, "end_pos": 24, "type": "DATASET", "confidence": 0.9510784447193146}]}, {"text": "In the MUC corpus the corresponding numbers are 5.3 and 2.0.", "labels": [], "entities": [{"text": "MUC corpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9710459113121033}]}, {"text": "Second, in the NYT corpus the number of times an article goes back to a previously described event is 182 (average of 2.6 times per article), of which 154 (84.6%) are transitions to the main event.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 15, "end_pos": 25, "type": "DATASET", "confidence": 0.9554069340229034}]}, {"text": "In the MUC corpus the number of times an article goes back to a previously described event is only 38 (average of 0.54 times per article), but, similarly to the NYT, in as much as 32 (84.2%) of these cases the transitions are to the main event.", "labels": [], "entities": [{"text": "MUC corpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9300400018692017}, {"text": "NYT", "start_pos": 161, "end_pos": 164, "type": "DATASET", "confidence": 0.9047948122024536}]}, {"text": "Experimental Setup For both corpora, we used 30 articles for training (1218 sentences in NYT, 423 in MUC), 7 articles for development (358 sentences in NYT, 79 in MUC) and 33 articles for test (1244 sentences in NYT, 367 in MUC).", "labels": [], "entities": [{"text": "NYT", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.8597323298454285}, {"text": "NYT", "start_pos": 212, "end_pos": 215, "type": "DATASET", "confidence": 0.8702327013015747}]}, {"text": "The sentences were POS tagged with the MXPOST tagger and parsed with the Charniak parser.", "labels": [], "entities": [{"text": "MXPOST", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.7935312986373901}]}, {"text": "We trained our model with a two steps procedure.", "labels": [], "entities": []}, {"text": "First, the local CRFs were separately trained on the training articles.", "labels": [], "entities": []}, {"text": "Then, we trained the parameters of the global potentials using the structured perceptron algorithm) on the development data.", "labels": [], "entities": []}, {"text": "We perform joint inference over the local CRFs as well as the global potentials with dual decomposition.", "labels": [], "entities": []}, {"text": "This algorithm is guaranteed to give the MAP assignment if it converges to a solution in which all the potentials agree on the label assignment for the variables in their scope.", "labels": [], "entities": []}, {"text": "To deal with disagreements, we ran the algorithm for 200 iterations past the point of fluctuations around the dual minimum.", "labels": [], "entities": []}, {"text": "The final label assignment is determined by a majority vote between the potentials in the 10 iterations with the highest total inter potential agreement ( . Baselines We compare our algorithm to two baseline models.", "labels": [], "entities": []}, {"text": "The first baseline is related to previous techniques that decompose the task into field extraction and event segmentation sub-tasks.", "labels": [], "entities": [{"text": "field extraction", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.7132761776447296}, {"text": "event segmentation", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.6945898979902267}]}, {"text": "For this PIPELINE baseline, we run the CRF models described in Section 3.1, first the field CRF and then the event CRF.", "labels": [], "entities": [{"text": "PIPELINE baseline", "start_pos": 9, "end_pos": 26, "type": "DATASET", "confidence": 0.7126662880182266}]}, {"text": "The fieldbased features of the event CRF are extracted from the output of the field CRF.", "labels": [], "entities": []}, {"text": "Our model incorporates global dependencies into a document level model.", "labels": [], "entities": []}, {"text": "An alternative approach is to encode this information as local features that reflect global dependencies (.", "labels": [], "entities": []}, {"text": "We therefore constructed a second baseline, the bidirectional pipeline model (BI-PIPELINE), that considers global features which encode similar properties to those encouraged by our global potentials.", "labels": [], "entities": [{"text": "BI-PIPELINE", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.9950403571128845}]}, {"text": "We implement this by incorporating event-based features into the feature set of the field labeling CRF, while kipping the event segmentation CRF fixed.", "labels": [], "entities": [{"text": "field labeling CRF", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.6799643834431967}]}, {"text": "As in the pipeline model, each CRF is trained separately on the training data.", "labels": [], "entities": []}, {"text": "The BI-PIPELINE model, however, emulates our joint inference procedure by iteratively running afield labeling and an event segmentation CRFs.", "labels": [], "entities": [{"text": "BI-PIPELINE", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9627854228019714}, {"text": "event segmentation CRFs", "start_pos": 117, "end_pos": 140, "type": "TASK", "confidence": 0.7808600266774496}]}, {"text": "The number of iterations for this model was estimated on development data.", "labels": [], "entities": []}, {"text": "Evaluation Measures We follow the MUC-4 scoring guidelines.", "labels": [], "entities": [{"text": "Evaluation Measures", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6292969286441803}, {"text": "MUC-4", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.7524626851081848}]}, {"text": "To compare between a learned and a gold standard event, we compute the word-level F-score between each of their fields and average the results.", "labels": [], "entities": [{"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.7298804521560669}]}, {"text": "If afield is empty in both event records, it is not counted in the mutual event score, while if it is empty in only one of the event records, its F-score is 0.", "labels": [], "entities": [{"text": "F-score", "start_pos": 146, "end_pos": 153, "type": "METRIC", "confidence": 0.9990122318267822}]}, {"text": "Ideally, the measure should be able to capture paraphrases.", "labels": [], "entities": []}, {"text": "For example, if the Tactic field in a gold event record contains the words \"bombing\" and \"blast\", the measure is expected to give a perfect score to a learned record that contains one of these words.", "labels": [], "entities": []}, {"text": "Therefore, as in the MUC-4 guidelines, we count pre-specified synonyms and morphological derivations of the same word only once.", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.6130566596984863}]}, {"text": "For every document, we then map the learned events to the gold events in a greedy 1-1 manner using the Kuhn-Munkres algorithm.", "labels": [], "entities": []}, {"text": "Once we have an event mapping, we can report an average recall, precision and F-score across the test set for all fields, events and documents (where the document F-score is the average F-score of its events).", "labels": [], "entities": [{"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9984473586082458}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9987918734550476}, {"text": "F-score", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9980663657188416}, {"text": "F-score", "start_pos": 186, "end_pos": 193, "type": "METRIC", "confidence": 0.9686509370803833}]}, {"text": "We use the sign test to measure the statistical significance for our results.", "labels": [], "entities": []}, {"text": "Since the number of events described in a document is not given to the models as input, we also report the average ratio between the number of induced and gold events.: Performance of the joint and the pipeline models on the labeling tasks of assigning words to fields (left) and to events (right).", "labels": [], "entities": []}, {"text": "Field values are computed for words tagged with the non-NULL field.", "labels": [], "entities": []}, {"text": "Events values are computed for words that are assigned to a non-NULL field by the gold standard (GF) or by the model (LF).", "labels": [], "entities": []}, {"text": "When the joint model is superior, results for fields are statistically significant with p < 0.01 and for events with p < 0.05.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of the joint model and the pipeline models on the event record extraction task. Top table is for  the New-York Times data. Bottom table is for the MUC data. All results are statistically significant with p < 0.05.", "labels": [], "entities": [{"text": "event record extraction task", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.6909031495451927}, {"text": "New-York Times data", "start_pos": 124, "end_pos": 143, "type": "DATASET", "confidence": 0.9246567090352377}, {"text": "MUC data", "start_pos": 169, "end_pos": 177, "type": "DATASET", "confidence": 0.9750165343284607}]}, {"text": " Table 3: Comparison between the joint model and the pipeline models for the different fields. When the joint model is  superior results are statistically significance with p < 0.05.", "labels": [], "entities": []}, {"text": " Table 4: Performance of the joint and the pipeline models on the labeling tasks of assigning words to fields (left) and  to events (right). Field values are computed for words tagged with the non-NULL field. Events values are computed  for words that are assigned to a non-NULL field by the gold standard (GF) or by the model (LF). When the joint model  is superior, results for fields are statistically significant with p < 0.01 and for events with p < 0.05.", "labels": [], "entities": []}, {"text": " Table 5: Performance of the joint model and the pipeline models when the gold standard for one of the labeling tasks  is given at test time. Results are statistically significant with p < 0.05.", "labels": [], "entities": []}, {"text": " Table 6: The effect of the record coherence potentials and  of the discourse potentials on the performance of the joint  model. Results are presented for F-scores, each line is for  the full model when potentials of one type are excluded.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9782319664955139}]}]}