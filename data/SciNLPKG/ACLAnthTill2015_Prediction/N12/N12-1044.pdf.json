{"title": [{"text": "Leveraging supplemental representations for sequential transduction", "labels": [], "entities": []}], "abstractContent": [{"text": "Sequential transduction tasks, such as grapheme-to-phoneme conversion and machine transliteration, are usually addressed by inducing models from sets of input-output pairs.", "labels": [], "entities": [{"text": "Sequential transduction", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.686179518699646}, {"text": "grapheme-to-phoneme conversion", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.7243349701166153}, {"text": "machine transliteration", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.680963546037674}]}, {"text": "Supplemental representations offer valuable additional information, but incorporating that information is not straightforward.", "labels": [], "entities": []}, {"text": "We apply a unified reranking approach to both grapheme-to-phoneme conversion and machine transliteration demonstrating substantial accuracy improvements by utilizing heterogeneous transliterations and transcriptions of the input word.", "labels": [], "entities": [{"text": "grapheme-to-phoneme conversion", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.7224859744310379}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9950674772262573}]}, {"text": "We describe several experiments that involve a variety of supplemental data and two state-of-the-art transduction systems, yielding error rate reductions ranging from 12% to 43%.", "labels": [], "entities": [{"text": "error rate reductions", "start_pos": 132, "end_pos": 153, "type": "METRIC", "confidence": 0.9612766901652018}]}, {"text": "We further apply our approach to system combination, with error rate reductions between 4% and 9%.", "labels": [], "entities": [{"text": "error rate reductions", "start_pos": 58, "end_pos": 79, "type": "METRIC", "confidence": 0.9739029407501221}]}], "introductionContent": [{"text": "Words exist independently of writing, as abstract entities shared among the speakers of a language.", "labels": [], "entities": []}, {"text": "Those abstract entities have various representations, which in turn may have different realizations.", "labels": [], "entities": []}, {"text": "Orthographic forms, phonetic transcriptions, alternative transliterations, and even sound-wave spectrograms are all related by referring to the same abstract word and they all convey information about its pronunciation.", "labels": [], "entities": []}, {"text": "Several well-known NLP tasks involve matching, alignment, and conversion between different word representations.", "labels": [], "entities": []}, {"text": "Grapheme-to-phoneme conversion (G2P) aims at generating a transcription of a word from its orthographic representation.", "labels": [], "entities": [{"text": "Grapheme-to-phoneme conversion (G2P)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8145702064037323}]}, {"text": "The reverse task is phoneme-to-grapheme conversion (P2G).", "labels": [], "entities": [{"text": "phoneme-to-grapheme conversion", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.7155766189098358}]}, {"text": "Machine transliteration (MTL) deals with conversion between orthographic representations in different writing scripts; forward transliteration proceeds from the primary representation to secondary representations, while the reverse task is called back-transliteration (BTL).", "labels": [], "entities": [{"text": "Machine transliteration (MTL)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8594627916812897}]}, {"text": "The conversion between a sound and an orthography is the goal of text-to-speech synthesis (TTS) and speech recognition (SR), where transcriptions are often used as intermediate forms.", "labels": [], "entities": [{"text": "text-to-speech synthesis (TTS)", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.824178159236908}, {"text": "speech recognition (SR)", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.8721290349960327}]}, {"text": "Although both MTL and G2P take orthographic representations as input, the two tasks are rarely considered in conjunction.", "labels": [], "entities": []}, {"text": "Traditionally, G2P has been investigated in the context of text-to-speech systems, while MTL is of interest to the information retrieval and machine translation communities.", "labels": [], "entities": [{"text": "information retrieval and machine translation", "start_pos": 115, "end_pos": 160, "type": "TASK", "confidence": 0.6567569255828858}]}, {"text": "In addition, unlike phonetic transcription schemes, which are often specific to a particular pronunciation lexicon, writing systems are well-standardized, with plenty of transliteration examples available in text corpora and on the Web ().", "labels": [], "entities": []}, {"text": "While the goal of G2P is producing a maximally faithful representation of the pronunciation, transliterations are often influenced by other factors, such as the phonological constraints of the target language, the orthographic form in the source language, the morphological adaptations related to the translation process, and even the semantic connotations of the output in the case of logographic scripts.", "labels": [], "entities": []}, {"text": "In spite of those differences, both transcriptions and transliterations contain valuable information about the pronunciation of the word.", "labels": [], "entities": []}, {"text": "In this paper, we show that it is possible to improve the accuracy of both G2P and MTL by incorporating supplemental representations of the word pronunciation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9993124008178711}]}, {"text": "Our method is based on SVM reranking of the n-best output lists of abase transduction system, with features including similarity scores between representations and n-grams derived from accurate alignments.", "labels": [], "entities": []}, {"text": "We describe a series of experiments in both G2P and MTL contexts, demonstrating substantial reductions in error rate for these base tasks when augmented with various supplemental representations.", "labels": [], "entities": [{"text": "error rate", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9551725387573242}]}, {"text": "We then further test the effectiveness of the same approach for combining the results of two independent base systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments aim at comprehensive evaluation of the reranking approach on both MTL and G2P tasks, employing various supplemental representations.", "labels": [], "entities": []}, {"text": "Relevant code and scripts associated with our experimental results are available online 1 .  When faced with the task of transliterating a word from the original script to a secondary script, we would like to leverage the information encoded in transliterations of the same word that are available in other scripts.", "labels": [], "entities": []}, {"text": "For example, consider the problem of automatically generating a Wikipedia stub article 3 in Hindi about guitarist John Petrucci.", "labels": [], "entities": [{"text": "automatically generating a Wikipedia stub article 3 in Hindi about guitarist John Petrucci", "start_pos": 37, "end_pos": 127, "type": "TASK", "confidence": 0.503089923125047}]}, {"text": "We assume that we have access to an MTL system trained on the English-Hindi transliterations, but we also want to take advantage of the existing transliterations of the name that are easy to extract from the corresponding articles on the topic in Japanese and other languages.", "labels": [], "entities": []}, {"text": "In this case, the orthography of the last name reflects its Italian origins, but the pronunciation depends on its degree of assimilation to English phonology.", "labels": [], "entities": []}, {"text": "This type of information is often difficult to determine even for humans, and we posit that it maybe inferred from other transliterations.", "labels": [], "entities": []}, {"text": "Similarly, phonetic transcriptions more directly encode the pronunciation and thus present an important resource for exploitation.", "labels": [], "entities": []}, {"text": "In fact, some transliteration systems use a phonetic transcription as an intermediate representation, although these methods do not generally fare as well as those that perform the transliteration process directly).", "labels": [], "entities": []}, {"text": "Transcriptions are often available; larger pronunciation dictionaries contain tens of thousands of entries, including some proper names (for which machine transliteration is most relevant), and many names in Wikipedia are accompanied by an IPA transcription.", "labels": [], "entities": []}, {"text": "Our first experiment aims at improving the transliteration accuracy from English to Japanese Katakana.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9423102736473083}]}, {"text": "The English-Japanese corpus has one of the largest overlaps (number of entries with a common input)  with the other transliteration and transcription corpora, the former of which is shown in.", "labels": [], "entities": []}, {"text": "In total, there are 18,505 entries for which at least one transliteration from a non-Japanese language is available and 6,288 for which at least one transcription is available.", "labels": [], "entities": []}, {"text": "The reranker is trained on an intersection of the English-Japanese training set and the supplemental data; similarly, the reranking test set is an intersection of the English-Japanese test set and the supplemental data.", "labels": [], "entities": []}, {"text": "Note that we compute word accuracy on these intersected sets, so the results of the base systems that we report here may not represent their performance on the full data set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.8391019701957703}]}, {"text": "shows the results 4 on the test set of 1,891 entries, including the performance of an oracle (perfect) reranker for comparison.", "labels": [], "entities": []}, {"text": "This same approach applied to the English-to-Hindi transliteration task yields an error rate reduction of 9% over the base performance of DIRECTL+ (Bhargava et al., 2011) 5 , which confirms that our reranking method's applicability is not limited to a particular language.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 82, "end_pos": 102, "type": "METRIC", "confidence": 0.9754354953765869}]}, {"text": "In the second experiment, instead of supplemental transliterations, we use supplemental transcriptions from the RP and GA Combilex corpora as well as CELEX.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 150, "end_pos": 155, "type": "DATASET", "confidence": 0.8034564852714539}]}, {"text": "The number of common elements with the English-Japanese transliteration corpus was 6,288 for Combilex and 2,351 for CELEX; in total, there were 6,384 transliteration entries for which at least Unless otherwise noted, all improvements reported in this paper are statistically significant with p < 0.01 using the McNemar test.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 116, "end_pos": 121, "type": "DATASET", "confidence": 0.9475958347320557}]}, {"text": "Note that this result is computed over the full English-Hindi data set, so is in fact slightly diluted compared to the results we present here.", "labels": [], "entities": [{"text": "English-Hindi data set", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.823697050412496}]}, {"text": "shows the results, giving a similar error rate reduction as for using supplemental transliterations.", "labels": [], "entities": [{"text": "error rate", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9736908078193665}]}, {"text": "Surprisingly, if we proceed to the next logical step and use both transcriptions and transliterations as supplemental representations simultaneously, the error rate reduction is slightly lower than in the above two experiments.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 154, "end_pos": 174, "type": "METRIC", "confidence": 0.981593906879425}]}, {"text": "This difference is so small as to be statistically insignificant.", "labels": [], "entities": []}, {"text": "We have no convincing explanation for this phenomenon, although we note that, in general, significant heterogeneity in data can increase the difficulty of a given task.", "labels": [], "entities": []}, {"text": "Consider the example of an automatic speech synthesis system tasked with generating an audio version of a news article that contains foreign names.", "labels": [], "entities": []}, {"text": "Often, foreign versions of the same news article already exist; in these, the name will have been transliterated.", "labels": [], "entities": []}, {"text": "These transliterations could then be leveraged to guide the system's pronunciation of the name.", "labels": [], "entities": []}, {"text": "The same is conceivable of other types of words, although transliterations are generally mostly available for names only.", "labels": [], "entities": []}, {"text": "On the other hand, transcription schemes are not consistent across different pronunciation lexica.", "labels": [], "entities": []}, {"text": "Their phonemic inventories often differ, and it is not always possible to construct a consistent mapping between them.", "labels": [], "entities": []}, {"text": "In addition, because of pronunciation variation and dialectal differences, a substantial fraction of transcriptions fail to match across dictionaries.", "labels": [], "entities": []}, {"text": "Nevertheless, if a phonetic transcription is already available, even in an alternative format, it could facilitate the task of generating anew pronunciation.", "labels": [], "entities": [{"text": "generating anew pronunciation", "start_pos": 127, "end_pos": 156, "type": "TASK", "confidence": 0.6836339036623637}]}, {"text": "The first G2P experiment concerns the application of supplemental transcriptions.", "labels": [], "entities": []}, {"text": "The goal is to quantify the improvements achieved using our reranking approach, and to compare reranking to two other methods of utilizing supplemental transcriptions, to which we refer as MERGE and P2P, respectively.", "labels": [], "entities": []}, {"text": "MERGE implements the most intuitive approach of merging different lexica into a single training set.", "labels": [], "entities": [{"text": "MERGE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7974187135696411}]}, {"text": "In order to make this work, we first need to make sure that all data is converted to a single transcription scheme.", "labels": [], "entities": []}, {"text": "Combilex and CELEX make different distinctions among phonemes, making it unclear how some phonemes might be mapped from CELEX into Combilex; fortuitously, the opposite conversion is more agreeable.", "labels": [], "entities": []}, {"text": "This allows us to convert Combilex to CELEX's format via a simple rule-based script and then merge the two corpora together.", "labels": [], "entities": []}, {"text": "This provides an alternative method against which we can compare our reranking-based approach which would treat Combilex as a source of supplemental representations.", "labels": [], "entities": []}, {"text": "P2P is a phoneme-to-phoneme conversion approach inspired by the work of.", "labels": [], "entities": [{"text": "phoneme-to-phoneme conversion", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.7214266508817673}]}, {"text": "In that approach, a phoneme-to-phoneme model is derived from a training set of phonetic transcription pairs representing two different pronunciation lexicons.", "labels": [], "entities": []}, {"text": "We use such model to convert the Combilex transcriptions to the scheme used by CELEX for the words that are missing from CELEX.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.9562435150146484}, {"text": "CELEX", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.9640320539474487}]}, {"text": "Where use decision trees for both the base system and the corpus converter, we use the much higher-performing state-of-the-art SEQUITUR and DIRECTL+ systems.", "labels": [], "entities": []}, {"text": "The two transcription corpora have 15,028 entries in common.", "labels": [], "entities": []}, {"text": "As with the MTL experiments, the reranker is trained on an intersection of the Combilex G2P data and the supplemental data.", "labels": [], "entities": [{"text": "MTL", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.8909973502159119}, {"text": "Combilex G2P data", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.7862117091814677}]}, {"text": "The results on the intersected set of 1,498 words are shown in.", "labels": [], "entities": []}, {"text": "We can see that merging the corpora provides a clear detriment in performance for data where an alternative transcription is available from another corpus.", "labels": [], "entities": []}, {"text": "Even if we look at the full CELEX test set (as opposed to the intersected subset used in), DIRECTL+ trained only on CELEX achieves 93.0% word accuracy on the CELEX test set where DIRECTL+ trained on CELEX merged with Combilex achieves 87.3%.", "labels": [], "entities": [{"text": "CELEX test set", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9506873289744059}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9238406419754028}, {"text": "CELEX test set", "start_pos": 158, "end_pos": 172, "type": "DATASET", "confidence": 0.9377724726994833}]}, {"text": "Evidently, the dis-   parate conventions of the two corpora \"confuse\" the base G2P systems.", "labels": [], "entities": []}, {"text": "In contrast, our reranker performs well, yielding spectacular error reductions of 32% and 42%.", "labels": [], "entities": [{"text": "error reductions", "start_pos": 62, "end_pos": 78, "type": "METRIC", "confidence": 0.9782685339450836}]}, {"text": "The differences between the two corpora account for the inadequate performance of the P2P approach.", "labels": [], "entities": []}, {"text": "Inducing a full transduction model requires much more training data that simply reranking the existing outputs, but in this case models for these two approaches (P2P and reranking) are trained on the same amount of data.", "labels": [], "entities": []}, {"text": "Furthermore, when the supplemental transcription is radically different from the n-best outputs, the alignment simply fails, and the reranking approach gracefully falls back to the original G2P model.", "labels": [], "entities": []}, {"text": "In contrast, the P2P approach has no such option.", "labels": [], "entities": []}, {"text": "It maybe interesting to note what happens when the P2P model is replaced with our rule-based Combilex-to-CELEX converter.", "labels": [], "entities": []}, {"text": "Such an approach has the advantage of being fast and not dependent on the training of any base system.", "labels": [], "entities": []}, {"text": "However, it achieves only 64.8% word accuracy, which is lower than any of the results in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9601222276687622}]}, {"text": "Clearly, a simple mapping script fails to capture the differences between the corpora.", "labels": [], "entities": []}, {"text": "Turning to supplemental transliterations, Bhargava and Kondrak (2011) have already shown that supplemental transliterations can improve G2P accuracy on names.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9358721971511841}]}, {"text": "It is interesting to verify whether this conclusion also applies to other types of words that occur in the G2P data set.", "labels": [], "entities": [{"text": "G2P data set", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.9117808739344279}]}, {"text": "Performing this test with DI-RECTL+ as the base system shows good error rate reduction on names (about 12%) as reported, but a much smaller statistically insignificant error rate re-duction on core vocabulary words (around 2%).", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 66, "end_pos": 86, "type": "METRIC", "confidence": 0.9731172720591227}]}, {"text": "In other words, the supplemental transliterations are able to help only for names.", "labels": [], "entities": []}, {"text": "This discrepancy is attributable to the fact that names (and, more generally, named entities) are the raison d'\u02c6 etre of transliterations.", "labels": [], "entities": []}, {"text": "Because the process of transliteration occurs primarily for names that must be \"translated\" phonetically, we expect transliterations' utility as supplemental representations to apply mostly for names.", "labels": [], "entities": []}, {"text": "The smaller number of transliterations for core vocabulary words also makes it difficult for any system to learn how to apply transliterations of such words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of unique entries in each translit- eration corpus, and the number of common single-word  entries (overlap) with the Japanese corpus.", "labels": [], "entities": []}, {"text": " Table 2: Word accuracies and error rate reductions (ERR)  in percentages for English-to-Japanese MTL augmented  by corresponding transliterations from other languages.  BASE is the base system while RERANKED represents the  same system with its output reranked using supplemental  transliterations. ORACLE represents an oracle reranker.", "labels": [], "entities": [{"text": "Word accuracies and error rate reductions (ERR)", "start_pos": 10, "end_pos": 57, "type": "METRIC", "confidence": 0.7750548422336578}, {"text": "MTL", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.8790567517280579}, {"text": "BASE", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9654152393341064}, {"text": "RERANKED", "start_pos": 200, "end_pos": 208, "type": "METRIC", "confidence": 0.9633183479309082}, {"text": "ORACLE", "start_pos": 300, "end_pos": 306, "type": "METRIC", "confidence": 0.9642265439033508}]}, {"text": " Table 3: Word accuracies and error rate reductions (ERR)  in percentages for English-to-Japanese MTL augmented  by corresponding transcriptions.", "labels": [], "entities": [{"text": "Word accuracies and error rate reductions (ERR)", "start_pos": 10, "end_pos": 57, "type": "METRIC", "confidence": 0.7906365858183967}, {"text": "MTL", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.8961571455001831}]}, {"text": " Table 4: Word accuracies and error rate reductions (ERR)  in percentages for CELEX G2P augmented by Combilex  transcriptions.", "labels": [], "entities": [{"text": "Word accuracies and error rate reductions (ERR)", "start_pos": 10, "end_pos": 57, "type": "METRIC", "confidence": 0.7876210941208733}]}, {"text": " Table 5: Word accuracies and error rate reductions (ERR)  in percentages for English-to-Japanese MTL augmented  by predicted transliterations from the other base system.", "labels": [], "entities": [{"text": "Word accuracies and error rate reductions (ERR)", "start_pos": 10, "end_pos": 57, "type": "METRIC", "confidence": 0.8002707031038072}, {"text": "MTL", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.858477771282196}]}]}