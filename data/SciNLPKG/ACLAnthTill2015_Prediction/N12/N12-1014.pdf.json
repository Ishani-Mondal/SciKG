{"title": [{"text": "Unsupervised Learning on an Approximate Corpus *", "labels": [], "entities": []}], "abstractContent": [{"text": "Unsupervised learning techniques can take advantage of large amounts of unannotated text, but the largest text corpus (the Web) is not easy to use in its full form.", "labels": [], "entities": []}, {"text": "Instead, we have statistics about this corpus in the form of n-gram counts (Brants and Franz, 2006).", "labels": [], "entities": []}, {"text": "While n-gram counts do not directly provide sentences, a distribution over sentences can be estimated from them in the same way that n-gram language models are estimated.", "labels": [], "entities": []}, {"text": "We treat this distribution over sentences as an approximate corpus and show how unsupervised learning can be performed on such a corpus using variational inference.", "labels": [], "entities": []}, {"text": "We compare hidden Markov model (HMM) training on exact and approximate corpora of various sizes, measuring speed and accuracy on unsu-pervised part-of-speech tagging.", "labels": [], "entities": [{"text": "speed", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.9765148162841797}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9982269406318665}, {"text": "part-of-speech tagging", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.6934020817279816}]}], "introductionContent": [{"text": "We consider the problem of training generative models on very large datasets in sublinear time.", "labels": [], "entities": []}, {"text": "It is well known how to train an HMM to maximize the likelihood of a corpus of sentences.", "labels": [], "entities": []}, {"text": "Here we show how to train faster on a distribution over sentences that compactly approximates the corpus.", "labels": [], "entities": []}, {"text": "The distribution is given by an 5-gram backoff language model that has been estimated from statistics of the corpus.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate our approach on a traditional testbed for new structured-prediction learning algorithms, namely HMMs.", "labels": [], "entities": []}, {"text": "We focus on unsupervised learning.", "labels": [], "entities": []}, {"text": "This serves to elucidate the structure of our variational training approach, which stitches overlapping n-grams together rather than treating them in isolation.", "labels": [], "entities": []}, {"text": "It also confirms that at least in this case, accuracy is not harmed by the key approximations made by our method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9994639754295349}]}, {"text": "In future, we hope to scale up to the Google n-gram corpus () and learn a more detailed, explanatory joint model of tags, syntactic dependencies, and topics.", "labels": [], "entities": [{"text": "Google n-gram corpus", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.8496556679407755}]}, {"text": "Our intuition here is that web-scale data maybe needed to learn the large number of lexically and contextually specific parameters.", "labels": [], "entities": []}, {"text": "* Work was supported in part by NSF grant No. 0347822.", "labels": [], "entities": [{"text": "NSF grant No. 0347822", "start_pos": 32, "end_pos": 53, "type": "DATASET", "confidence": 0.9078306406736374}]}], "datasetContent": [], "tableCaptions": []}