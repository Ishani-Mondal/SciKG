{"title": [{"text": "Automatic Parallel Fragment Extraction from Noisy Data", "labels": [], "entities": [{"text": "Parallel Fragment Extraction from Noisy", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.702277535200119}]}], "abstractContent": [{"text": "We present a novel method to detect parallel fragments within noisy parallel corpora.", "labels": [], "entities": []}, {"text": "Isolating these parallel fragments from the noisy data in which they are contained frees us from noisy alignments and stray links that can severely constrain translation-rule extraction.", "labels": [], "entities": [{"text": "translation-rule extraction", "start_pos": 158, "end_pos": 185, "type": "TASK", "confidence": 0.8837979435920715}]}, {"text": "We do this with existing machinery, making use of an existing word alignment model for this task.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.7085671424865723}]}, {"text": "We evaluate the quality and utility of the extracted data on large-scale Chinese-English and Arabic-English translation tasks and show significant improvements over a state-of-the-art baseline.", "labels": [], "entities": [{"text": "Chinese-English and Arabic-English translation tasks", "start_pos": 73, "end_pos": 125, "type": "TASK", "confidence": 0.714329195022583}]}], "introductionContent": [{"text": "A decade ago, showed that scaling to very large corpora is game-changing fora variety of tasks.", "labels": [], "entities": []}, {"text": "Methods that work well in a smalldata setting often lose their luster when moving to large data.", "labels": [], "entities": []}, {"text": "Conversely, other methods that seem to perform poorly in that same small-data setting, may perform markedly differently when trained on large data.", "labels": [], "entities": []}, {"text": "Perhaps most importantly, Banko and Brill showed that there was no significant variation in performance among a variety of methods trained atscale with large training data.", "labels": [], "entities": []}, {"text": "If you desire to scale to large datasets, use a simple solution for your task, and throw in as much data as possible.", "labels": [], "entities": []}, {"text": "The community at large has taken this message to heart, and inmost cases it has been an effective way to increase performance.", "labels": [], "entities": []}, {"text": "Today, for machine translation, more data than what we already have is getting harder and harder to come by; we require large parallel corpora to: Example of a word alignment resulting from noisy parallel data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.8089293539524078}, {"text": "word alignment", "start_pos": 160, "end_pos": 174, "type": "TASK", "confidence": 0.7267013490200043}]}, {"text": "The structure of the resulting alignment makes it difficult to find and extract parallel fragments via the standard heuristics or simply by inspection.", "labels": [], "entities": []}, {"text": "How can we discover automatically those parallel fragments hidden within such data?", "labels": [], "entities": []}, {"text": "train state-of-the-art statistical, data-driven models.", "labels": [], "entities": []}, {"text": "Groups that depend on clearinghouses like LDC for their data increasingly find that there is less of a mandate to gather parallel corpora on the scale of what was produced in the last 5-10 years.", "labels": [], "entities": []}, {"text": "Others, who directly exploit the entire web to gather such data will necessarily run up against a wall after all that data has been collected.", "labels": [], "entities": []}, {"text": "We need to learn how to do more with the data we already have.", "labels": [], "entities": []}, {"text": "Previous work has focused on detecting parallel documents and sentences on the web, e.g. ()., and later, extend the state-of-the-art for this task to parallel fragments.", "labels": [], "entities": [{"text": "detecting parallel documents and sentences", "start_pos": 29, "end_pos": 71, "type": "TASK", "confidence": 0.8247401475906372}]}, {"text": "In this paper, we present a novel method for detecting parallel fragments in large, existing and potentially noisy parallel corpora using existing ma-chinery and show significant improvements to two state-of-the-art MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 216, "end_pos": 218, "type": "TASK", "confidence": 0.9895461201667786}]}, {"text": "We also depart from previous work in that we only consider parallel corpora that have previously been cleaned, sanitized, and thought to be non-noisy, e.g. parallel corpora available from LDC.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our parallel fragment extraction in a large-scale Chinese-English and Arabic-English MT setting.", "labels": [], "entities": [{"text": "parallel fragment extraction", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.6072887380917867}, {"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9442917108535767}]}, {"text": "In our experiments we use a tree-to-string syntax-based MT system (), and evaluate on a standard test set, NIST08.", "labels": [], "entities": [{"text": "NIST08", "start_pos": 107, "end_pos": 113, "type": "DATASET", "confidence": 0.9791377782821655}]}, {"text": "We parse the English side of our parallel corpus with the Berkeley parser (), and tune parameters of the MT system with MIRA ().", "labels": [], "entities": [{"text": "MIRA", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.8550817966461182}]}, {"text": "We decode with an integrated language model trained on about 4 billion words of English.", "labels": [], "entities": []}, {"text": "Chinese-English We align a parallel corpus of 8.4M parallel segments, with 210M words of English and 193M words of Chinese.", "labels": [], "entities": []}, {"text": "From this we extract 868,870 parallel fragments according to the process described in Section 2, and append these fragments to the end of the parallel corpus.", "labels": [], "entities": []}, {"text": "In doing so, we have created a larger parallel corpus of 9.2M parallel segments, consisting of 217M and 198M words of English and Chinese, respectively.", "labels": [], "entities": []}, {"text": "Arabic-English We align a parallel corpus of 9.0M parallel segments, with 223M words of English and 194M words of Arabic.", "labels": [], "entities": []}, {"text": "From this we extract 996,538 parallel fragments, and append these fragments to the end of the parallel corpus.", "labels": [], "entities": []}, {"text": "The resulting corpus has 10M parallel segments, consisting of 233M and 202M words of English and Arabic, respectively.", "labels": [], "entities": []}, {"text": "Using our parallel fragment extraction, we learn 68M additional unique Arabic-English rules that are not in the baseline system; likewise, we learn 38M new unique ChineseEnglish rules not in the baseline system for that language pair.", "labels": [], "entities": [{"text": "parallel fragment extraction", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.6374141176541647}]}, {"text": "Note that we are not simply duplicating portions of the parallel data.", "labels": [], "entities": []}, {"text": "While each sequence fragment of source and target words we extract will be found elsewhere in the larger parallel corpus, these fragments will largely not make it into fruitful translation rules to be used in the downstream MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 224, "end_pos": 226, "type": "TASK", "confidence": 0.9701908826828003}]}, {"text": "We learning new and useful translation rules we previously were not in our grammars.", "labels": [], "entities": []}, {"text": "These results are significant with p < 0.05 for Arabic-English and p < 0.01 for Chinese-English.", "labels": [], "entities": []}], "tableCaptions": []}