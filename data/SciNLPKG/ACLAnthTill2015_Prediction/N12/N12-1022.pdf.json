{"title": [{"text": "Topical Segmentation: a Study of Human Performance and a New Measure of Quality", "labels": [], "entities": [{"text": "Topical Segmentation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.871245801448822}]}], "abstractContent": [{"text": "Ina large-scale study of how people find topical shifts in written text, 27 annotators were asked to mark topically continuous segments in 20 chapters of a novel.", "labels": [], "entities": []}, {"text": "We analyze the resulting corpus for inter-annotator agreement and examine disagreement patterns.", "labels": [], "entities": []}, {"text": "The results suggest that, while the overall agreement is relatively low, the annotators show high agreement on a subset of topical breaks-places where most prominent topic shifts occur.", "labels": [], "entities": [{"text": "agreement", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.988010585308075}, {"text": "agreement", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9739130139350891}]}, {"text": "We recommend taking into account the prominence of topical shifts when evaluating topical segmentation, effectively penalizing more severely the errors on more important breaks.", "labels": [], "entities": [{"text": "topical segmentation", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.7164696753025055}]}, {"text": "We propose to account for this in a simple modification of the windowDiff metric.", "labels": [], "entities": []}, {"text": "We discuss the experimental results of evaluating several topical segmenters with and without considering the importance of the individual breaks, and emphasize the more insightful nature of the latter analysis.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topical segmentation is a useful intermediate step in many high-level NLP applications such as information retrieval, automatic summarization and question answering.", "labels": [], "entities": [{"text": "Topical segmentation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8438343405723572}, {"text": "information retrieval", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.835992842912674}, {"text": "summarization", "start_pos": 128, "end_pos": 141, "type": "TASK", "confidence": 0.682601809501648}, {"text": "question answering", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.8996846973896027}]}, {"text": "It is often necessary to split along document into topically continuous segments.", "labels": [], "entities": []}, {"text": "Segmentation maybe particularly beneficial when working with documents without overt structure: speech transcripts (), newswire) or novels ().", "labels": [], "entities": []}, {"text": "The customary approach is to cast text segmentation as a binary problem: is there a shift of topic between any two adjacent textual units (e.g., sentences or paragraphs)?", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7341708242893219}]}, {"text": "While necessary, this simplification is quite crude.", "labels": [], "entities": []}, {"text": "Topic in discourse usually changes continually; some shifts are subtle, others -more prominent.", "labels": [], "entities": []}, {"text": "The evaluation of text segmentation remains an open research problem.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7286451160907745}]}, {"text": "It is a tradition to compile a gold-standard segmentation reference using one or more annotations created by humans.", "labels": [], "entities": []}, {"text": "If an automatic segmenter agrees with the reference, it is rewarded, otherwise it is penalized (see Section 4 for details).", "labels": [], "entities": []}, {"text": "The nature of the task, however, is such that creating and applying a reference segmentation is far from trivial.", "labels": [], "entities": []}, {"text": "The identification of topical shifts requires discretization of a continuous concept -how much the topic changes between two adjacent units.", "labels": [], "entities": [{"text": "identification of topical shifts", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.8699793666601181}]}, {"text": "That is why annotators often operate at different levels of granularity.", "labels": [], "entities": []}, {"text": "Some people mark only the most prominent topic fluctuations, while others also include finer changes.", "labels": [], "entities": []}, {"text": "The task is also necessarily under-defined.", "labels": [], "entities": []}, {"text": "In addition to topic changes per se, annotators effectively must classify some rhetorical and pragmatic phenomena -exactly how much it is depends on the document genre.", "labels": [], "entities": []}, {"text": "For simplicity we do not directly address the latter problem here; we concentrate on the former.", "labels": [], "entities": []}, {"text": "To study how people identify topical shifts in written text, we asked 27 annotators to segment into episodes 20 chapters of the novel The Moonstone by Wilkie Collins.", "labels": [], "entities": [{"text": "The Moonstone by Wilkie Collins", "start_pos": 134, "end_pos": 165, "type": "DATASET", "confidence": 0.8240708112716675}]}, {"text": "Each chapter was annotated by 4-6 people.", "labels": [], "entities": []}, {"text": "An episode roughly corresponds to a topically continuous segment -the term is defined in Section 3.", "labels": [], "entities": []}, {"text": "The analysis of the resulting corpus reveals that while the overall inter-annotator agreement is quite low and is not uniform throughout each chapter.", "labels": [], "entities": []}, {"text": "Some topical shifts are marked by most or all annotators, others -by one or by a minority.", "labels": [], "entities": [{"text": "topical shifts", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.7919006645679474}]}, {"text": "In fact, only about 50% of all annotated topical shifts are supported by at least 50% of annotators (including near-hits), while the other half is only marked by a minority.", "labels": [], "entities": []}, {"text": "In this work we take the agreement about a certain topical shift as a measure of its prominence, and show how this measure can be simply utilized for the purpose of evaluation.", "labels": [], "entities": []}, {"text": "The main claim of this paper is perhaps the following: when evaluating the performance of automatic segmenters, it is important to consider not only the overall similarity between human and machine segmentations, but also to examine the regions of disagreement.", "labels": [], "entities": []}, {"text": "When a program misses or misplaces a prominent topic shift -a segment boundary marked by all annotators -it should be penalized more than if it was mistaken about a boundary marked by one person.", "labels": [], "entities": []}, {"text": "Similarly, a false positive in the region where none of the annotators found a change in topic is worse than a boundary inserted in a place whereat least one person perceived a topic change.", "labels": [], "entities": []}, {"text": "We suggest that it is important to use all available reference segmentations instead of compiling them into a single gold standard.", "labels": [], "entities": []}, {"text": "We show how a small modification to the popular windowDiff) metric can allow considering multiple annotations at once.", "labels": [], "entities": []}, {"text": "To demonstrate the increased interpretive power of such evaluation we run and evaluate several stateof-the art segmenters on the corpus described in this work.", "labels": [], "entities": []}, {"text": "We evaluate their performance first in a conventional manner -by combining all available references into one -and then by using the proposed modification.", "labels": [], "entities": []}, {"text": "Comparing the results suggests that the information provided by this method differs from what existing methods provide.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief background on text segmentation.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.783629983663559}]}, {"text": "Section 3 describes the corpus and how it was collected.", "labels": [], "entities": []}, {"text": "Section 4 contain quantitative and qualitative analysis of the corpus and its interpretations.", "labels": [], "entities": []}, {"text": "Section 5 proposes a modified version of windowDiff and motivates it.", "labels": [], "entities": []}, {"text": "Section 6 compares evaluation of three segmenters in several different ways.", "labels": [], "entities": []}, {"text": "Section 7 contains the conclusions and outlines directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to illustrate why using a single goldstandard reference segmentation can be problematic, we evaluate three publicly available segmenters,), and APS (), using several different gold standards and then using all available annotations.", "labels": [], "entities": [{"text": "APS", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.8830258846282959}]}, {"text": "The corpus used for evaluation is The Moonstone corpus described in Sections 3-4.", "labels": [], "entities": [{"text": "Moonstone corpus", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.8942873775959015}]}, {"text": "We withheld the first four chapters for development and used the remaining 16 for testing.", "labels": [], "entities": []}, {"text": "We also compared the segmenters to a random baseline which consisted of randomly selecting a number of boundaries equal to the average number of segments across all available annotations.", "labels": [], "entities": []}, {"text": "None of the segmenters requires training in the conventional sense, but APS and MinCutSeg segmeters come with scripts allowing to fine-tune several parameters.", "labels": [], "entities": [{"text": "APS", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.7062553763389587}]}, {"text": "We selected the best parameters for these two segmenters using the first four chapters of the corpus.", "labels": [], "entities": []}, {"text": "BayesSeg segmeter, a probabilistic segmenter, does not require setting any parameters.", "labels": [], "entities": []}, {"text": "Each row corresponds to one reference segmentation and metricregular windowDiff in the first six rows.", "labels": [], "entities": []}, {"text": "We compiled several flavours of consensus reference segmentations: 1) all boundaries marked by \u2265 50% of the annotators (windowDiff \u2265 50%), 2) all boundaries marked by \u2265 30% of the annotators (windowDiff \u2265 30%), 3) all boundaries marked by at least one annotator (windowDiff union).", "labels": [], "entities": [{"text": "consensus reference segmentations", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.6217800478140513}]}, {"text": "To illustrate why comparing against a single annotation is unreliable, we report comparisons against three single-person annotations (windowDiff annotator 1, 4, 2).", "labels": [], "entities": []}, {"text": "multWinDiff is the proposed multi-annotator version from Equation 10.", "labels": [], "entities": []}, {"text": "The best-case bound for multWinDiff is 0.21 and the worst-case bound is 1.0.", "labels": [], "entities": []}, {"text": "Each segmenter produced just one segmentation, so the numbers in the  The three segmenters and a random baseline compared using different references for computing windowDiff.", "labels": [], "entities": []}, {"text": "windowDiff \u226550%: the gold standard consists of all boundaries specified by at least 50% of the annotators; windowDiff \u226530%: all boundaries specified by at least 30% of the annotators; windowDiff union: all boundaries specified by at least one person; windowDiff annotator a: comparisons against individual annotators.", "labels": [], "entities": []}, {"text": "multWinDiff is multi-annotator windowDiff from equation (10).", "labels": [], "entities": []}, {"text": "line remains the worst inmost cases.", "labels": [], "entities": [{"text": "line", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9366109371185303}]}, {"text": "The APS and BayesSeg segmenters tend to appear better than the MinCutSeg but it is not always the case and the rankings among the three are not consistent.", "labels": [], "entities": [{"text": "APS", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9121813178062439}, {"text": "MinCutSeg", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.9527498483657837}]}, {"text": "The last row reports multi-annotator windowDiff which takes into account all available references and also the best-case and the worst-case bounds.", "labels": [], "entities": []}, {"text": "In principle, there is noway to prove that the metric is better than using windowDiff and a single reference annotation.", "labels": [], "entities": []}, {"text": "It does, however, take into account all available information and provides a different, if not unambiguously more true, picture of the comparative performance of automatic segmenters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of inter-annotator agreement.", "labels": [], "entities": []}]}