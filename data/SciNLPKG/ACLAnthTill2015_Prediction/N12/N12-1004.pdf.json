{"title": [{"text": "Fast Inference in Phrase Extraction Models with Belief Propagation", "labels": [], "entities": [{"text": "Phrase Extraction", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.8674647808074951}]}], "abstractContent": [{"text": "Modeling overlapping phrases in an alignment model can improve alignment quality but comes with a high inference cost.", "labels": [], "entities": []}, {"text": "For example, the model of DeNero and Klein (2010) uses an ITG constraint and beam-based Viterbi decoding for tractability, but is still slow.", "labels": [], "entities": []}, {"text": "We first show that their model can be approximated using structured belief propagation , with again in alignment quality stemming from the use of marginals in decoding.", "labels": [], "entities": [{"text": "structured belief propagation", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.7061931093533834}]}, {"text": "We then consider a more flexible, non-ITG matching constraint which is less efficient for exact inference but more efficient for BP.", "labels": [], "entities": [{"text": "BP", "start_pos": 129, "end_pos": 131, "type": "TASK", "confidence": 0.5331610441207886}]}, {"text": "With this new constraint, we achieve a relative error reduction of 40% in F 5 and a 5.5x speed-up.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 48, "end_pos": 63, "type": "METRIC", "confidence": 0.9107867777347565}, {"text": "F 5", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9800132811069489}]}], "introductionContent": [{"text": "Modern statistical machine translation (MT) systems most commonly infer their transfer rules from word-level alignments (), typically using a deterministic heuristic to convert these to phrase alignments (.", "labels": [], "entities": [{"text": "statistical machine translation (MT)", "start_pos": 7, "end_pos": 43, "type": "TASK", "confidence": 0.7832051912943522}]}, {"text": "There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (.", "labels": [], "entities": [{"text": "phrase alignment problem", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.867364505926768}]}, {"text": "However, most of these have met with limited success compared to the simpler heuristic method.", "labels": [], "entities": []}, {"text": "One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases) and reducing coverage, which in turn reduces translation quality).", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7842956483364105}]}, {"text": "On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9746217131614685}]}, {"text": "In response to these effects, the recent phrase alignment work of models extraction sets: collections of overlapping phrase pairs that are consistent with an underlying word alignment.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.8007246553897858}]}, {"text": "Their extraction set model is empirically very accurate.", "labels": [], "entities": []}, {"text": "However, the ability to model overlapping -and therefore non-local -features comes at a high computational cost.", "labels": [], "entities": []}, {"text": "handle this in part by imposing a structural ITG constraint (Wu, 1997) on the underlying word alignments.", "labels": [], "entities": []}, {"text": "This permits a polynomial-time algorithm, but it is still O(n 6 ), with a large constant factor once the state space is appropriately enriched to capture overlap.", "labels": [], "entities": []}, {"text": "Therefore, they use a heavily beamed Viterbi search procedure to find a reasonable alignment within an acceptable time frame.", "labels": [], "entities": []}, {"text": "In this paper, we show how to use belief propagation (BP) to improve on the model's ITG-based structural formulation, resulting in anew model that is simultaneously faster and more accurate.", "labels": [], "entities": [{"text": "belief propagation (BP)", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7954502642154694}]}, {"text": "First, given the model of, we decompose it into factors that admit an efficient BP approximation.", "labels": [], "entities": [{"text": "BP", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.967791736125946}]}, {"text": "BP is an inference technique that can be used to efficiently approximate posterior marginals on variables in a graphical model; here the marginals of interest are the phrase pair posteriors.", "labels": [], "entities": [{"text": "BP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.6347359418869019}]}, {"text": "BP has only recently come into use in the NLP community, but it has been shown to be effective in other complex structured classification tasks, such as dependency parsing.", "labels": [], "entities": [{"text": "BP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9340440034866333}, {"text": "dependency parsing", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.871619701385498}]}, {"text": "There has also been some prior success in using BP for both discriminative and generative Crom\u00ec eres and) word alignment models.", "labels": [], "entities": [{"text": "BP", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.8917792439460754}, {"text": "word alignment", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7145663499832153}]}, {"text": "By aligning all phrase pairs whose posterior under BP exceeds some fixed threshold, our BP approximation of the model of can achieve a comparable phrase pair F 1 . Furthermore, because we have posterior marginals rather than a single Viterbi derivation, we can explicitly force the aligner to choose denser extraction sets simply by lowering the marginal threshold.", "labels": [], "entities": [{"text": "BP", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.910232424736023}, {"text": "BP", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9413856267929077}]}, {"text": "Therefore, we also show substantial improvements over in recall-heavy objectives, such as F 5 . More importantly, we also show how the BP factorization allows us to relax the ITG constraint, replacing it with anew set of constraints that permit a wider family of alignments.", "labels": [], "entities": [{"text": "recall-heavy", "start_pos": 57, "end_pos": 69, "type": "METRIC", "confidence": 0.8760499358177185}, {"text": "F 5", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.7840868234634399}, {"text": "BP", "start_pos": 135, "end_pos": 137, "type": "METRIC", "confidence": 0.9620156288146973}]}, {"text": "Compared to ITG, the resulting model is less efficient for exact inference (where it is exponential), but more efficient for our BP approximation (where it is only quadratic).", "labels": [], "entities": [{"text": "ITG", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.8588036894798279}, {"text": "BP", "start_pos": 129, "end_pos": 131, "type": "METRIC", "confidence": 0.7873196005821228}]}, {"text": "Our new model performs even better than the ITG-constrained model on phrase alignment metrics while being faster by a factor of 5.5x.", "labels": [], "entities": [{"text": "phrase alignment metrics", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.8797395626703898}]}, {"text": "shows part of an aligned sentence pair, including the word-to-word alignments, and the extracted phrase pairs licensed by those alignments.", "labels": [], "entities": []}, {"text": "Formally, given a sentence pair (e, f), a word-level alignment a is a collection of links between target words e i and source words f j . Following past work, we further divide word links into two categories: sure and possible, shown in as solid and hatched grey squares, respectively.", "labels": [], "entities": []}, {"text": "We represent a as a grid of ternary word link variables a ij , each of which can take the value sure to represent a sure link between e i and f j , poss to represent a possible link, or off to represent no link.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are performed on Chinese-toEnglish alignment.", "labels": [], "entities": [{"text": "Chinese-toEnglish alignment", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.5944475382566452}]}, {"text": "We trained and evaluated all models on the NIST MT02 test set, which consists of 150 training and 191 test sentences and has been used previously in alignment experiments.", "labels": [], "entities": [{"text": "NIST MT02 test set", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.9257783591747284}]}, {"text": "The unsupervised HMM word aligner used to generate features for the model was trained on 11.3 million words of FBIS newswire data.", "labels": [], "entities": [{"text": "FBIS newswire data", "start_pos": 111, "end_pos": 129, "type": "DATASET", "confidence": 0.9406293630599976}]}, {"text": "We test three models: the Viterbi ITG model of, our BP ITG model that uses the ITG factor, and our BP Relaxed model that replaces the ITG factor with the ONESPAN factors.", "labels": [], "entities": [{"text": "ONESPAN", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.5509421825408936}]}, {"text": "In all of our experiments, the phrase length d was set to 3.", "labels": [], "entities": [{"text": "phrase length d", "start_pos": 31, "end_pos": 46, "type": "METRIC", "confidence": 0.8016872604688009}]}], "tableCaptions": [{"text": " Table 2: Machine translation results.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8273089230060577}]}]}