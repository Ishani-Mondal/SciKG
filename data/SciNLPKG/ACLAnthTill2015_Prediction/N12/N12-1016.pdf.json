{"title": [], "abstractContent": [{"text": "We propose anew segmentation evaluation metric, called segmentation similarity (S), that quantifies the similarity between two segmen-tations as the proportion of boundaries that are not transformed when comparing them using edit distance, essentially using edit distance as a penalty function and scaling penalties by segmentation size.", "labels": [], "entities": [{"text": "segmentation similarity (S)", "start_pos": 55, "end_pos": 82, "type": "METRIC", "confidence": 0.7040700614452362}]}, {"text": "We propose several adapted inter-annotator agreement coefficients which use S that are suitable for segmenta-tion.", "labels": [], "entities": []}, {"text": "We show that S is configurable enough to suit a wide variety of segmentation evaluations , and is an improvement upon the state of the art.", "labels": [], "entities": [{"text": "segmentation evaluations", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.9021080136299133}]}, {"text": "We also propose using inter-annotator agreement coefficients to evaluate automatic segmenters in terms of human performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Segmentation is the task of splitting up an item, such as a document, into a sequence of segments by placing boundaries within.", "labels": [], "entities": [{"text": "Segmentation is the task of splitting up an item, such as a document, into a sequence of segments", "start_pos": 0, "end_pos": 97, "type": "Description", "confidence": 0.799566000699997}]}, {"text": "The purpose of segmenting can vary greatly, but one common objective is to denote shifts in the topic of a text, where multiple boundary types can also be present (e.g., major versus minor topic shifts).", "labels": [], "entities": []}, {"text": "Human-competitive automatic segmentation methods can help a wide range of computational linguistic tasks which depend upon the identification of segment boundaries in text.", "labels": [], "entities": [{"text": "Human-competitive automatic segmentation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.5460746785004934}]}, {"text": "To evaluate automatic segmentation methods, a method of comparing an automatic segmenter's performance against the segmentations produced by human judges (coders) is required.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.8488759994506836}]}, {"text": "Current methods of performing this comparison designate only one coder's segmentation as a reference to compare against.", "labels": [], "entities": []}, {"text": "A single \"true\" reference segmentation from a coder should not be trusted, given that interannotator agreement is often reported to be rather poor.", "labels": [], "entities": []}, {"text": "Additionally, to ensure that an automatic segmenter does not over-fit to the preference and bias of one particular coder, an automatic segmenter should be compared directly against multiple coders.", "labels": [], "entities": []}, {"text": "The state of the art segmentation evaluation metrics (P k and WindowDiff) slide a window across a designated reference and hypothesis segmentation, and count the number of windows where the number of boundaries differ.", "labels": [], "entities": []}, {"text": "Window-based methods suffer from a variety of problems, including: i) unequal penalization of error types; ii) an arbitrarily defined window size parameter (whose choice greatly affects outcomes); iii) lack of clear intuition; iv) inapplicability to multiply-coded corpora; and v) reliance upon a \"true\" reference segmentation.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew method of comparing two segmentations, called segmentation similarity 1 (S), that: i) equally penalizes all error types (unless explicitly configured otherwise); ii) appropriately responds to scenarios tested; iii) defines no arbitrary parameters; iv) is intuitive; and v) is adapted for use in a variety of popular interannotator agreement coefficients to handle multiplycoded corpora; and vi) does not rely upon a \"true\" reference segmentation (it is symmetric).", "labels": [], "entities": []}, {"text": "Capitalizing on the adapted inter-annotator agreement coefficients, the relative difficulty that human segmenters have with various segmentation tasks can now be quantified.", "labels": [], "entities": []}, {"text": "We also propose that these coefficients can be used to evaluate and compare automatic segmentation methods in terms of human agreement.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review segmentation evaluation and interannotator agreement.", "labels": [], "entities": [{"text": "segmentation evaluation", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.9544571042060852}]}, {"text": "In Section 3, we present Sand inter-annotator agreement coefficient adaptations.", "labels": [], "entities": []}, {"text": "In Section 4, we evaluate Sand WindowDiff in various scenarios and simulations, and upon a multiplycoded corpus.", "labels": [], "entities": [{"text": "Sand WindowDiff", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.8289719223976135}]}], "datasetContent": [{"text": "Precision, recall, and their mean (F \u03b2 -measure) have been previously applied to segmentation evaluation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9763157963752747}, {"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9988405108451843}, {"text": "F \u03b2 -measure)", "start_pos": 35, "end_pos": 48, "type": "METRIC", "confidence": 0.9370085954666137}, {"text": "segmentation evaluation", "start_pos": 81, "end_pos": 104, "type": "TASK", "confidence": 0.9599050879478455}]}, {"text": "Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis 3).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9913457632064819}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9995018243789673}]}, {"text": "For segmentation, these metrics are unsuitable because they penalize near-misses of boundaries as full-misses, causing them to drastically overestimate the error.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.987181544303894}]}, {"text": "Near-misses are prevalent in segmentation and can account fora large proportion of the errors produced by a coder, and as inter-annotator agreement often shows, they do not reflect coder error, but the difficulty of the task.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.9760892987251282}]}, {"text": "P k () 2 is a window-based metric which attempts to solve the harsh near-miss penalization of precision, recall, and F \u03b2 -measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9994992017745972}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9987890124320984}, {"text": "F \u03b2 -measure", "start_pos": 117, "end_pos": 129, "type": "METRIC", "confidence": 0.9848912954330444}]}, {"text": "In P k , a window of size k, where k is defined as half of the mean reference segment size, is slid across the text to compute penalties.", "labels": [], "entities": []}, {"text": "A penalty of 1 is assigned for each window whose boundaries are detected to be in different segments of the reference and hypothesis segmentations, and this count is normalized by the number of windows.) highlighted a number of issues with P k , specifically that: i) False negatives (FNs) are penalized more than false positives (FPs); ii) It does not penalize FPs that fall within k units of a reference boundary; iii) Its sensitivity to variations in segment size can cause it to linearly decrease the penalty for FPs if the size of any segments fall below k; and iv) Near-miss errors are too harshly penalized.", "labels": [], "entities": []}, {"text": "To attempt to mitigate the shortcomings of P k , Pevzner and Hearst proposed a modified metric which changed how penalties were 2 P k is a modification of P\u00b5 ().", "labels": [], "entities": []}, {"text": "Other modifications such as TDT Cseg) have been proposed, but P k has seen greater usage.", "labels": [], "entities": []}, {"text": "counted, named WindowDiff (W D).", "labels": [], "entities": []}, {"text": "A window of size k is still slid across the text, but now penalties are attributed to windows where the number of boundaries in each segmentation differs (see Equation 1, where b(R ij ) and b(H ij ) represents the number of boundaries within the segments in a window of size k from position i to j, and N the number of sentences plus one), with the same normalization.", "labels": [], "entities": []}, {"text": "WindowDiff is able to reduce, but not eliminate, sensitivity to segment size, gives more equal weights to both FPs and FNs (FNs are, in effect, penalized less 3 ), and is able to catch mistakes in both small and large segments.", "labels": [], "entities": [{"text": "WindowDiff", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8376299738883972}]}, {"text": "It is not without issues though; demonstrated that WindowDiff penalizes errors less at the beginning and end of a segmentation (this is corrected by padding the segmentation at each end by size k).", "labels": [], "entities": []}, {"text": "Additionally, variations in the window size k lead to difficulties in interpreting and comparing WindowDiff's values, and the intuition of the method remains vague.", "labels": [], "entities": []}, {"text": "proposed measuring performance in terms of the number of words that are FNs and FPs, normalized by the number of word positions present (see Equation 2).", "labels": [], "entities": []}, {"text": "RF N and RF P have the advantage that they take into account the severity of an error in terms of segment size, allowing them to reflect the effects of erroneously missing, or added, words in a segment better than window based metrics.", "labels": [], "entities": []}, {"text": "Unfortunately, RF N and RF P suffer from the same flaw as precision, recall, and F \u03b2 -measure in that they do not account for near misses.", "labels": [], "entities": [{"text": "RF P", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.8886139094829559}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9997202754020691}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9987418055534363}, {"text": "F \u03b2 -measure", "start_pos": 81, "end_pos": 93, "type": "METRIC", "confidence": 0.9870105981826782}]}, {"text": "To demonstrate the advantages of using S, as opposed to WindowDiff (W D), we compare both metrics using a variety of contrived scenarios, and then compare our adapted agreement coefficients against pairwise mean W D 9 for the segmentations collected by.", "labels": [], "entities": []}, {"text": "In this section, because W Dis a penalty-based metric, it is reported as 1\u2212W D so that it is easier to compare against S values.", "labels": [], "entities": []}, {"text": "When reported in this way, 1\u2212W D and S both range from, where 1 represents no errors and 0 represents maximal error.", "labels": [], "entities": [{"text": "maximal error", "start_pos": 102, "end_pos": 115, "type": "METRIC", "confidence": 0.9543710350990295}]}], "tableCaptions": [{"text": " Table 1: Stability of mean (with standard deviation) values of W D and S in three different scenarios, each defining  the: probability of a false positive (FP), false negative (FN), or both. Each scenario varies the range of internal  segment sizes (e.g.,", "labels": [], "entities": [{"text": "probability of a false positive (FP), false negative (FN)", "start_pos": 124, "end_pos": 181, "type": "METRIC", "confidence": 0.7500317245721817}]}]}