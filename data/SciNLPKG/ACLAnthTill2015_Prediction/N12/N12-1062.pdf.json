{"title": [], "abstractContent": [{"text": "We propose a tuning method for statistical machine translation, based on the pairwise ranking approach.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.7475638488928477}]}, {"text": "Hopkins and May (2011) presented a method that uses a binary classifier.", "labels": [], "entities": []}, {"text": "In this work, we use linear regression and show that our approach is as effective as using a binary classifier and converges faster.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since its introduction, the minimum error rate training (MERT) method has been the most popular method used for parameter tuning in machine translation.", "labels": [], "entities": [{"text": "minimum error rate training (MERT", "start_pos": 28, "end_pos": 61, "type": "METRIC", "confidence": 0.8196932474772135}, {"text": "parameter tuning", "start_pos": 112, "end_pos": 128, "type": "TASK", "confidence": 0.7436334788799286}, {"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7340264022350311}]}, {"text": "Although MERT has nice properties such as simplicity, effectiveness and speed, it is known to not scale well for systems with large numbers of features.", "labels": [], "entities": [{"text": "MERT", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.3919059932231903}]}, {"text": "One alternative that has been used for large numbers of features is the Margin Infused Relaxed Algorithm (MIRA) (.", "labels": [], "entities": [{"text": "Margin Infused Relaxed Algorithm (MIRA)", "start_pos": 72, "end_pos": 111, "type": "METRIC", "confidence": 0.6798356345721653}]}, {"text": "MIRA works well with a large number of features, but the optimization problem is much more complicated than MERT.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.4371989965438843}, {"text": "MERT", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.6786439418792725}]}, {"text": "MIRA also involves some modifications to the decoder itself to produce hypotheses with high scores against gold translations.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.806638777256012}]}, {"text": "Hopkins and May (2011) introduced the method of pairwise ranking optimization (PRO), which casts the problem of tuning as a ranking problem between pairs of translation candidates.", "labels": [], "entities": [{"text": "pairwise ranking optimization (PRO)", "start_pos": 48, "end_pos": 83, "type": "TASK", "confidence": 0.7970757285753886}]}, {"text": "The problem is solved by doing a binary classification between \"correctly ordered\" and \"incorrectly ordered\" pairs.", "labels": [], "entities": []}, {"text": "use the maximum entropy classifier MegaM) to do the binary classification.", "labels": [], "entities": []}, {"text": "Their method compares well to the results of MERT, scales better for high dimensional feature spaces, and is simpler than MIRA.", "labels": [], "entities": [{"text": "MERT", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.5122529864311218}, {"text": "MIRA", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.7342005968093872}]}, {"text": "In this paper, we use the same idea for tuning, but, instead of using a classifier, we use linear regression.", "labels": [], "entities": []}, {"text": "Linear regression is simpler than maximum entropy based methods.", "labels": [], "entities": [{"text": "Linear regression", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.769728034734726}]}, {"text": "The most complex computation that it needs is a matrix inversion, whereas maximum entropy based classifiers use iterative numerical optimization methods.", "labels": [], "entities": []}, {"text": "We implemented a parameter tuning program with linear regression and compared the results to PRO's results.", "labels": [], "entities": [{"text": "PRO", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.722649097442627}]}, {"text": "The results of our experiments are comparable to PRO, and in many cases (also on average) we get a better maximum BLEU score.", "labels": [], "entities": [{"text": "PRO", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.665856122970581}, {"text": "BLEU score", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.9830436706542969}]}, {"text": "We also observed that on average, our method reaches the maximum BLEU score in a smaller number of iterations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9861223101615906}]}, {"text": "The contributions of this paper include: First, we show that linear regression tuning is an effective method for tuning, and it is comparable to tuning with a binary maximum entropy classifier.", "labels": [], "entities": [{"text": "linear regression tuning", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6328268746534983}]}, {"text": "Second, we show linear regression is faster in terms of the number of iterations it needs to reach the best results.", "labels": [], "entities": [{"text": "linear regression", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.5887331068515778}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Average of maximum BLEU scores of the ex- periments and the maximum BLEU score from the ex- periments. Numbers in the parentheses indicate standard  of deviations of maximum BLEU scores.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9927710890769958}, {"text": "BLEU scores", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.954323410987854}, {"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.983850359916687}, {"text": "BLEU", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.9860033392906189}]}]}