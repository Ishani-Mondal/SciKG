{"title": [{"text": "Graph-Based Lexicon Expansion with Sparsity-Inducing Penalties", "labels": [], "entities": [{"text": "Sparsity-Inducing Penalties", "start_pos": 35, "end_pos": 62, "type": "METRIC", "confidence": 0.7746720016002655}]}], "abstractContent": [{"text": "We present novel methods to construct compact natural language lexicons within a graph-based semi-supervised learning framework, an attractive platform suited for propagating soft labels onto new natural language types from seed data.", "labels": [], "entities": []}, {"text": "To achieve compactness, we induce sparse measures at graph vertices by incorporating sparsity-inducing penalties in Gaussian and entropic pairwise Markov networks constructed from labeled and unla-beled data.", "labels": [], "entities": []}, {"text": "Sparse measures are desirable for high-dimensional multi-class learning problems such as the induction of labels on natural language types, which typically associate with only a few labels.", "labels": [], "entities": []}, {"text": "Compared to standard graph-based learning methods, for two lexicon expansion problems, our approach produces significantly smaller lexicons and obtains better predictive performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semi-supervised learning (SSL) is attractive for the learning of complex phenomena, for example, linguistic structure, where data annotation is expensive.", "labels": [], "entities": [{"text": "Semi-supervised learning (SSL)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7665338039398193}]}, {"text": "Natural language processing applications have benefited from various SSL techniques, such as distributional word representations), self-training (), and entropy regularization (.", "labels": [], "entities": []}, {"text": "In this paper, we focus on semi-supervised learning that uses a graph constructed from labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "This framework, graph-based SSL-see and for introductory material on this topic-has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (, ch. 21).", "labels": [], "entities": []}, {"text": "The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled.", "labels": [], "entities": []}, {"text": "Pairs of vertices are connected by weighted edges denoting the similarity between the pair.", "labels": [], "entities": []}, {"text": "Traditionally, Markov random walks or optimization of a loss function based on smoothness properties of the graph (, inter alia) are performed to propagate labels from the labeled vertices to the unlabeled ones.", "labels": [], "entities": []}, {"text": "In this work, we are interested in multi-class generalizations of graph-propagation algorithms suitable for NLP applications, where each graph vertex can assume one or more out of many possible labels ().", "labels": [], "entities": []}, {"text": "For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric.", "labels": [], "entities": []}, {"text": "Recently, this setup has been used to learn soft labels on natural language types (say, word n-grams or syntactically disambiguated predicates) from seed data, resulting in large but noisy lexicons, which are used to constrain structured prediction models.", "labels": [], "entities": []}, {"text": "Applications have ranged from domain adaptation of part-of-speech (POS) taggers (, unsupervised learning of POS taggers by using bilingual graph-based projections (, and shallow semantic parsing for unknown predicates ().", "labels": [], "entities": [{"text": "domain adaptation of part-of-speech (POS) taggers", "start_pos": 30, "end_pos": 79, "type": "TASK", "confidence": 0.7874944880604744}, {"text": "shallow semantic parsing", "start_pos": 170, "end_pos": 194, "type": "TASK", "confidence": 0.6881271799405416}]}, {"text": "However, none of the above captured the empirical fact that only a few categories typically associate with a given type (vertex).", "labels": [], "entities": []}, {"text": "Take the case of POS tagging: construct a graph over trigram types as vertices, with 45 possible tags for the middle word of a trigram as the label set for each vertex.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.8073729872703552}]}, {"text": "It is empirically observed that contextualized word types can assume very few (most often, one) POS tags.", "labels": [], "entities": []}, {"text": "However, along with graph smoothness terms, they apply a penalty that encourages distributions to be close to uniform, the premise being that it would maximize the entropy of the distribution fora vertex that is faraway or disconnected from a labeled vertex.", "labels": [], "entities": []}, {"text": "To prefer maximum entropy solutions in low confidence regions of graphs, a similar entropic penalty is applied by.", "labels": [], "entities": []}, {"text": "In this paper, we make two major algorithmic contributions.", "labels": [], "entities": []}, {"text": "First, we relax the assumption made by most previous work () that the 1 norm of the masses assigned to the labels fora given vertex must be 1.", "labels": [], "entities": []}, {"text": "In other words, in our framework, the label distribution at each vertex is unnormalized-the only constraint we put on the vertices' vectors is that they must be nonnegative.", "labels": [], "entities": []}, {"text": "This relaxation simplifies optimization: since only a nonnegativity constraint for each label's mass at each vertex needs to be imposed, we can apply a generic quasi-Newton method ().", "labels": [], "entities": []}, {"text": "Second, we replace the penalties that prefer maximum entropy, used in prior work, with penalties that aim to identify sparse unnormalized measures at each graph vertex.", "labels": [], "entities": []}, {"text": "We achieve this by penalizing the graph propagation objective with the 1 norm or the mixed 1,2 norm ( of the measures at each vertex, aiming for global and vertex-level sparsity, respectively.", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7913447022438049}]}, {"text": "Importantly, the proposed graph objective functions are convex, so we avoid degenerate solutions and local minima.", "labels": [], "entities": []}, {"text": "We present experiments on two natural language lexicon expansion problems in a semi-supervised setting: (i) inducing distributions of POS tags over n-gram types in the Wall Street Journal section of the Penn Treebank corpus and (ii) inducing distributions of semantic frames over predicates unseen in anno-1 Moreover, we also assume the edge weights in a given graph are unconstrained, consistent with prior work on graphbased SSL ().", "labels": [], "entities": [{"text": "natural language lexicon expansion", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.699911542236805}, {"text": "Wall Street Journal section of the Penn Treebank corpus", "start_pos": 168, "end_pos": 223, "type": "DATASET", "confidence": 0.8990054196781583}]}, {"text": "Our methods produce sparse measures at graph vertices resulting in compact lexicons, and also result in better performance with respect to label propagation using Gaussian penalties () and entropic measure propagation (), two state-ofthe-art graph propagation algorithms.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.724224403500557}]}], "datasetContent": [{"text": "In this section, we compare the six graph objective functions in with the two baseline objectives on two lexicon expansion tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Area under the precision recall curve for the two  baseline objectives and our methods for POS tag lexicon  induction. This is a measure of how well the type lexicon  (for some types unlabeled during training) is recovered  by each method. The test set contains 588,705 types.", "labels": [], "entities": [{"text": "precision recall curve", "start_pos": 25, "end_pos": 47, "type": "METRIC", "confidence": 0.8597855369249979}, {"text": "POS tag lexicon  induction", "start_pos": 101, "end_pos": 127, "type": "TASK", "confidence": 0.7898378074169159}]}]}