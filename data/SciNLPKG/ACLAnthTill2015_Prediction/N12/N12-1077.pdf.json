{"title": [{"text": "Measuring Word Relatedness Using Heterogeneous Vector Space Models", "labels": [], "entities": [{"text": "Word Relatedness", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7206802219152451}]}], "abstractContent": [{"text": "Noticing that different information sources often provide complementary coverage of word sense and meaning, we propose a simple and yet effective strategy for measuring lexical semantics.", "labels": [], "entities": []}, {"text": "Our model consists of a committee of vector space models built on a text corpus , Web search results and thesauruses, and measures the semantic word relatedness using the averaged cosine similarity scores.", "labels": [], "entities": []}, {"text": "Despite its simplicity, our system correlates with human judgements better or similarly compared to existing methods on several benchmark datasets, including WordSim353.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 158, "end_pos": 168, "type": "DATASET", "confidence": 0.9796684384346008}]}], "introductionContent": [{"text": "Measuring the semantic relatedness of words is a fundamental problem in natural language processing and has many useful applications, including textual entailment, word sense disambiguation, information retrieval and automatic thesaurus discovery.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.7011383324861526}, {"text": "word sense disambiguation", "start_pos": 164, "end_pos": 189, "type": "TASK", "confidence": 0.6900918781757355}, {"text": "information retrieval", "start_pos": 191, "end_pos": 212, "type": "TASK", "confidence": 0.8126563131809235}, {"text": "automatic thesaurus discovery", "start_pos": 217, "end_pos": 246, "type": "TASK", "confidence": 0.634064773718516}]}, {"text": "Existing approaches can be roughly categorized into two kinds: knowledge-based and corpusbased, where the former includes graph-based algorithms and similarity measures operating on a lexical database such as WordNet () and the latter consists of various kinds of vector space models (VSMs) constructed with the help of a large collection of text.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 209, "end_pos": 216, "type": "DATASET", "confidence": 0.9410857558250427}]}, {"text": "In this paper, we present a conceptually simple model for solving this problem.", "labels": [], "entities": []}, {"text": "Observing that various kinds of information sources, such as * Work conducted while interning at Microsoft Research.", "labels": [], "entities": []}, {"text": "general text corpora, Web search results and thesauruses, have different word and sense coverage, we first build individual vector space models from each of them separately.", "labels": [], "entities": []}, {"text": "Given two words, each VSM measures the semantic relatedness by the cosine similarity of the corresponding vectors in its space.", "labels": [], "entities": []}, {"text": "The final prediction is simply the averaged cosine scores derived from these VSMs.", "labels": [], "entities": []}, {"text": "Despite its simplicity, our system surprisingly yields very strong empirical performance.", "labels": [], "entities": []}, {"text": "When comparing the predictions with the human annotations on four different datasets, our system achieves higher correlation than existing methods on two datasets and provides very competitive results on the others.", "labels": [], "entities": [{"text": "correlation", "start_pos": 113, "end_pos": 124, "type": "METRIC", "confidence": 0.9695284366607666}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly reviews the related work.", "labels": [], "entities": []}, {"text": "Section 3 details how we construct each individual vector space model, followed by the experimental evaluation in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the quality of the VSMs constructed using methods described in Section 3 on different benchmark datasets, as well as the performance when combining them.", "labels": [], "entities": []}, {"text": "We follow the standard evaluation method, which directly tests the correlation of the word relatedness measures with human judgements on a set of word pairs, using the Spearman's rank correlation coefficient.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 168, "end_pos": 207, "type": "METRIC", "confidence": 0.56075878739357}]}, {"text": "Our study was conducted using four different datasets, including WS-353, RG-65, MC-30 and MTurk-287.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.9351645112037659}, {"text": "RG-65", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.6534999012947083}, {"text": "MTurk-287", "start_pos": 90, "end_pos": 99, "type": "DATASET", "confidence": 0.949340283870697}]}, {"text": "The WordSim353 dataset (WS-353) is the largest among them and has been used extensively in recent work.", "labels": [], "entities": [{"text": "WordSim353 dataset (WS-353)", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.9040988922119141}]}, {"text": "Originally collected by, the dataset consists of 353 word pairs.", "labels": [], "entities": []}, {"text": "The degree of relatedness of each pair is assessed on a 0-10 scale by 13-16 human judges, where the mean is used as the final score.", "labels": [], "entities": []}, {"text": "Examining the relations between the words in each pair, further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations.", "labels": [], "entities": []}, {"text": "Collected by, RG-65 contains 65 pairs of words that are either synonyms or unrelated, assessed on a 0-4 scale by 51 human subjects.", "labels": [], "entities": []}, {"text": "Taking 30 pairs from them, created the (MC-30) dataset by reassessing these word pairs using 38 subjects.", "labels": [], "entities": [{"text": "MC-30) dataset", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.7002580165863037}]}, {"text": "These 30 pairs of words are also a subset of WS-353.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.944175660610199}]}, {"text": "Although these three datasets contain overlapping word pairs, their scores are different because of the degree of relatedness were given by different human subjects.", "labels": [], "entities": []}, {"text": "In addition to these datasets, we also evaluate our VSMs on the Mturk-287 dataset that consists of 287 word pairs collected by) using Amazon MTurk.", "labels": [], "entities": [{"text": "Mturk-287 dataset", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.9885126948356628}, {"text": "Amazon MTurk", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.8789905607700348}]}, {"text": "summarizes the results of various methods, where the top part lists the performance of state-ofthe-art systems and the bottom shows the results of individual vector space models, as well as combining these models using the averaged cosine scores.", "labels": [], "entities": []}, {"text": "We make several observations here.", "labels": [], "entities": []}, {"text": "First, while none of the four VSMs we tested outperforms the best existing systems on the benchmark datasets, surprisingly, using the averaged cosine scores of these models, the performance is improved substantially.", "labels": [], "entities": []}, {"text": "It achieves higher Spearman's rank coefficient on WS-353 and MTurk-287 than any other systems 2 and are close to the state-of-the-art on MC-30 and RG-65.", "labels": [], "entities": [{"text": "Spearman's rank coefficient", "start_pos": 19, "end_pos": 46, "type": "METRIC", "confidence": 0.8775878548622131}, {"text": "WS-353", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.9003072381019592}, {"text": "MTurk-287", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.9189497232437134}, {"text": "MC-30", "start_pos": 137, "end_pos": 142, "type": "DATASET", "confidence": 0.962821900844574}, {"text": "RG-65", "start_pos": 147, "end_pos": 152, "type": "DATASET", "confidence": 0.8593499660491943}]}, {"text": "Unlike some approach like, which performs well on some datasets but poorly on others, combing the VSMs from heterogeneous sources is more robust.", "labels": [], "entities": []}, {"text": "Individually, we notice that Wikipedia context VSM provides consistently strong results, while thesaurusbased models work only reasonable on MC-30 and RG-65, potentially because other datasets contain more out-of-vocabulary words or proper nouns.", "labels": [], "entities": [{"text": "Wikipedia context VSM", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.5443617304166158}]}, {"text": "Due to the inherent ambiguity of the task, there is a high variance among judgements from different annotators.", "labels": [], "entities": []}, {"text": "Therefore, it is unrealistic to assume any of the methods can correlate perfectly to the mean human judgement scores.", "labels": [], "entities": []}, {"text": "In fact, the inter-agreement study done on the WS-353 dataset indicates that the result of our approach of combining heterogeneous VSMs is close to the averaged human performance.", "labels": [], "entities": [{"text": "WS-353 dataset", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9785199761390686}]}], "tableCaptions": [{"text": " Table 1: The performance of the state-of-the-art methods and different vector space models on measuring semantic  word relatedness using the cosine similarity.", "labels": [], "entities": []}]}