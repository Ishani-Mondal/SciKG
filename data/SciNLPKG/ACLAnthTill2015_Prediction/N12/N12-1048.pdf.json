{"title": [{"text": "Real-time Incremental Speech-to-Speech Translation of Dialogs", "labels": [], "entities": [{"text": "Incremental Speech-to-Speech Translation of Dialogs", "start_pos": 10, "end_pos": 61, "type": "TASK", "confidence": 0.8286579728126526}]}], "abstractContent": [{"text": "Ina conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally.", "labels": [], "entities": []}, {"text": "In this work, we address the problem of incre-mental speech-to-speech translation (S2S) that enables cross-lingual communication between two remote participants over a telephone.", "labels": [], "entities": [{"text": "incre-mental speech-to-speech translation (S2S)", "start_pos": 40, "end_pos": 87, "type": "TASK", "confidence": 0.7806071043014526}]}, {"text": "We investigate the problem in a novel real-time Session Initiation Protocol (SIP) based S2S framework.", "labels": [], "entities": []}, {"text": "The speech translation is performed incrementally based on generation of partial hypotheses from speech recognition.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7459686100482941}]}, {"text": "We describe the statistical models comprising the S2S system and the SIP architecture for enabling real-time two-way cross-lingual dialog.", "labels": [], "entities": []}, {"text": "We present dialog experiments performed in this framework and study the tradeoff inaccuracy versus latency in incremental speech translation.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7527237832546234}]}, {"text": "Experimental results demonstrate that high quality translations can be generated with the incremental approach with approximately half the latency associated with non-incremental approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, speech-to-speech translation (S2S) technology has played an increasingly important role in narrowing the language barrier in crosslingual interpersonal communication.", "labels": [], "entities": [{"text": "speech-to-speech translation (S2S)", "start_pos": 17, "end_pos": 51, "type": "TASK", "confidence": 0.8480161190032959}]}, {"text": "The improvements in automatic speech recognition (ASR), statistical machine translation (MT), and, text-to-speech synthesis (TTS) technology has facilitated the serial binding of these individual components to achieve S2S translation of acceptable quality.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 20, "end_pos": 54, "type": "TASK", "confidence": 0.8037164211273193}, {"text": "statistical machine translation (MT)", "start_pos": 56, "end_pos": 92, "type": "TASK", "confidence": 0.8005143801371256}, {"text": "text-to-speech synthesis (TTS)", "start_pos": 99, "end_pos": 129, "type": "TASK", "confidence": 0.8094632446765899}, {"text": "S2S translation", "start_pos": 218, "end_pos": 233, "type": "TASK", "confidence": 0.8833387494087219}]}, {"text": "Prior work on S2S translation has primarily focused on providing either one-way or two-way translation on a single device (.", "labels": [], "entities": [{"text": "S2S translation", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.8666008710861206}]}, {"text": "Typically, the user interface requires the participant(s) to choose the source and target language apriori.", "labels": [], "entities": []}, {"text": "The nature of communication, either single user talking or turn taking between two users can result in a one-way or cross-lingual dialog interaction.", "labels": [], "entities": []}, {"text": "In most systems, the necessity to choose the directionality of translation for each turn does takeaway from a natural dialog flow.", "labels": [], "entities": []}, {"text": "Furthermore, single interface based S2S translation (embedded or cloudbased) is not suitable for cross-lingual communication when participants are geographically distant, a scenario more likely in a global setting.", "labels": [], "entities": [{"text": "S2S translation", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.813346803188324}]}, {"text": "In such a scenario, it is imperative to provide real-time and low latency communication.", "labels": [], "entities": []}, {"text": "Ina conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally.", "labels": [], "entities": []}, {"text": "Similarly, cross-lingual dialog between two remote participants will greatly benefit through incremental translation.", "labels": [], "entities": []}, {"text": "While incremental decoding for text translation has been addressed previously in, we address the problem in a speech-to-speech translation setting for enabling real-time cross-lingual dialog.", "labels": [], "entities": [{"text": "text translation", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7397014647722244}, {"text": "speech-to-speech translation", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.7230907678604126}]}, {"text": "We address the problem of incrementality in a novel session initiation protocol (SIP) based S2S translation system that enables two people to interact and engage in crosslingual dialog over a telephone (mobile phone or landline).", "labels": [], "entities": [{"text": "S2S translation", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.7688937187194824}]}, {"text": "Our system performs incremental speech recognition and translation, allowing for low latency interaction that provides an ideal setting for remote dialog aimed at accomplishing a task.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7526994943618774}]}, {"text": "We present previous work in this area in Section 2 and introduce the problem of incremental translation in Section 3.", "labels": [], "entities": [{"text": "incremental translation", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.5617733597755432}]}, {"text": "We describe the statistical models used in the S2S translation framework in Section 4 followed by a description of the SIP communication framework for real-time translation in Section 5.", "labels": [], "entities": [{"text": "S2S translation", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.8747946619987488}]}, {"text": "In Section 6, we describe the basic call flow of our system following which we present dialog experiments performed using our framework in Section 8.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 9 along with directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe speech translation experiments performed on the dialog corpus collected through our system.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.723136693239212}]}, {"text": "We present baseline results followed by results of incremental translation.", "labels": [], "entities": []}, {"text": "The models described in Section 4 were used to establish baseline results on the dialog corpus.", "labels": [], "entities": []}, {"text": "No A: Hello, I am calling from room four twenty one the T.V. is not working.", "labels": [], "entities": []}, {"text": "Do you think you can send someone to fix it please?", "labels": [], "entities": []}, {"text": "B: Si, Se\u00f1or enseguida enviamos a alguien para que la arregle.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9659281969070435}]}, {"text": "Si no le cambiaremos de habitaci\u00f3n.", "labels": [], "entities": []}, {"text": "A: Thank you very much.", "labels": [], "entities": []}, {"text": "B: Estamos aqu para servirle.", "labels": [], "entities": []}, {"text": "Ll\u00e1menos si necesita algo m\u00e1s.: Example of a sample dialog scenario.", "labels": [], "entities": []}, {"text": "contextual information was used in these experiments, i.e., the audio utterances were decoded independently.", "labels": [], "entities": []}, {"text": "The ASR WER for English and Spanish sides of the dialogs is shown in.", "labels": [], "entities": [{"text": "ASR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9924682974815369}, {"text": "WER", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.85354083776474}]}, {"text": "The average WER for English and Spanish side of the conversations is 27.73% and 22.83%, respectively.", "labels": [], "entities": [{"text": "WER", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9995335340499878}]}, {"text": "The recognized utterances were subsequently translated using the MT system described above.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.8002617955207825}]}, {"text": "The MT performance in terms of Translation Edit Rate (TER)) and BLEU () is shown in.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9728877544403076}, {"text": "Translation Edit Rate (TER))", "start_pos": 31, "end_pos": 59, "type": "METRIC", "confidence": 0.8375053654114405}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9993031024932861}]}, {"text": "The MT performance is shown across all the turns for both reference transcriptions and ASR output.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.839302122592926}, {"text": "ASR", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.8863656520843506}]}, {"text": "The results show that the performance of the Spanish-English MT model is better in comparison to the EnglishSpanish model on the dialog corpus.", "labels": [], "entities": []}, {"text": "The performance on ASR input drops by about 18% compared to translation on reference text.", "labels": [], "entities": [{"text": "ASR input", "start_pos": 19, "end_pos": 28, "type": "TASK", "confidence": 0.919945478439331}]}, {"text": "Turn taking in a dialog typically involves the subjects speaking one or more utterances in a turn.", "labels": [], "entities": [{"text": "Turn taking in a dialog", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8274920582771301}]}, {"text": "Since, machine translation systems are trained on chunked parallel texts (40 words or less), it is beneficial to segment the ASR hypotheses before translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7291016578674316}]}, {"text": "Previous studies have shown significant improvements in translation performance through the segmentation of ASR hypotheses (.", "labels": [], "entities": [{"text": "translation", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.9618344902992249}, {"text": "ASR", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.7841243147850037}]}, {"text": "We experimented with the notion of segmentation defined by silence frames in the ASR output.", "labels": [], "entities": []}, {"text": "A threshold of 8-10 frames (100 ms) was found to be suitable for segmenting the ASR output into sentence chunks.", "labels": [], "entities": [{"text": "ASR output into sentence chunks", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.7650359034538269}]}, {"text": "We did not use any lexical features for segmenting the turns.", "labels": [], "entities": []}, {"text": "The BLEU scores for different silence thresholds used in segmentation is shown in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9993840456008911}, {"text": "segmentation", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.9673082232475281}]}, {"text": "The BLEU scores improvement for Spanish-English is 1.6 BLEU points higher than the baseline model using no segmentation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9993597865104675}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9992759823799133}]}, {"text": "The im-: BLEU score (Spanish-English) for incremental speech translation across varying timeout periods in the speech recognizer partial-translation parameter in Moses (.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.8941285908222198}]}, {"text": "The partial hypotheses are generated as a function of speech recognizer timeouts.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.6487160623073578}]}, {"text": "Timeout is defined as the time interval with which the speech recognizer generates partial hypotheses.", "labels": [], "entities": [{"text": "Timeout", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8553012013435364}, {"text": "speech recognizer generates partial hypotheses", "start_pos": 55, "end_pos": 101, "type": "TASK", "confidence": 0.8146258950233459}]}, {"text": "For each timeout interval, the speech recognizer mayor may not generate a partial result based on the search path at that instant in time.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.6868772059679031}]}, {"text": "As the timeout interval increases, the performance of incremental translation approaches that of non-incremental translation.", "labels": [], "entities": []}, {"text": "The key is to choose an operating point such that the user perception of latency is minimal with acceptable BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.999308705329895}]}, {"text": "It is interesting that very good performance can be attained at a timeout of 500 ms in comparison with non-incremental speech translation, i.e., the latency can be reduced in half with acceptable translation quality.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7663183212280273}]}, {"text": "The continue-partial option in Moses performs slightly better than the partial case as it conditions the decision on prior source input as well as translation.", "labels": [], "entities": []}, {"text": "In, we present the latency measurements of the various components in our framework.", "labels": [], "entities": []}, {"text": "We do not have a row for ASR since it is not possible to get the start time for each recognition run as the RTP packets are continuously flowing in the SIP framework.", "labels": [], "entities": [{"text": "ASR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.975697934627533}]}, {"text": "The latency between various system components is very low (5-30 ms).", "labels": [], "entities": [{"text": "latency", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9701517224311829}]}, {"text": "While the average time taken for translation (incremental) is \u2248 100 ms, the TTS takes the longest time as it is non-incremental in the current work.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9761247038841248}]}, {"text": "It can also been seen that the average time taken for generating incremental MT output is half that of TTS that is non-incremental.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9617432951927185}]}, {"text": "The overall results show that the communication in our SIP-based framework has low latency.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. About 30% of  the training data was obtained from the Web (Ran- garajan Sridhar et al., 2011). The development set  (identical to the one used in ASR) was used in  MERT training as well as perplexity based optimiza- tion of the interpolated language model. The lan- guage model for MT and ASR was constructed from  identical data.", "labels": [], "entities": [{"text": "ASR", "start_pos": 156, "end_pos": 159, "type": "TASK", "confidence": 0.9755203723907471}, {"text": "MERT training", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.6902034282684326}, {"text": "MT", "start_pos": 292, "end_pos": 294, "type": "TASK", "confidence": 0.9568379521369934}]}, {"text": " Table 3: Latency measurements (in ms) for the S2S  components in the real-time SIP framework.", "labels": [], "entities": [{"text": "Latency", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9877907037734985}]}]}