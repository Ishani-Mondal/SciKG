{"title": [], "abstractContent": [{"text": "Most existing theory of structured prediction assumes exact inference, which is often intractable in many practical problems.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7098407745361328}]}, {"text": "This leads to the routine use of approximate inference such as beam search but there is not much theory behind it.", "labels": [], "entities": [{"text": "beam search", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.8698086142539978}]}, {"text": "Based on the structured perceptron, we propose a general framework of \"violation-fixing\" perceptrons for inexact search with a theoretical guarantee for convergence under new separability conditions.", "labels": [], "entities": []}, {"text": "This framework subsumes and justifies the popular heuristic \"early-update\" for perceptron with beam search (Collins and Roark, 2004).", "labels": [], "entities": []}, {"text": "We also propose several new update methods within this framework, among which the \"max-violation\" method dramatically reduces training time (by 3 fold as compared to early-update) on state-of-the-art part-of-speech tagging and incremental parsing systems.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 200, "end_pos": 222, "type": "TASK", "confidence": 0.6849715411663055}]}], "introductionContent": [{"text": "Discriminative structured prediction algorithms such as conditional random fields (), structured perceptron), maxmargin markov networks (, and structural SVMs () lead to state-of-the-art performance on many structured prediction problems such as part-of-speech tagging, sequence labeling, and parsing.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 246, "end_pos": 268, "type": "TASK", "confidence": 0.7365724742412567}, {"text": "sequence labeling", "start_pos": 270, "end_pos": 287, "type": "TASK", "confidence": 0.6596011519432068}]}, {"text": "But despite their success, there remains a major problem: these learning algorithms all assume exact inference (over an exponentially-large search space), which is needed to ensure their theoretical properties such as convergence.", "labels": [], "entities": []}, {"text": "This exactness assumption, however, rarely holds in practice since exact inference is often intractable in many important problems such as machine translation (), incremental parsing (, and bottom-up parsing.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.7932699620723724}, {"text": "incremental parsing", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.5123108923435211}]}, {"text": "This leads to routine use of approximate inference such as beam search as evidenced in the above-cited papers, but the inexactness unfortunately abandons existing theoretical guarantees of the learning algorithms, and besides notable exceptions discussed below and in Section 7, little is known for theoretical properties of structured prediction under inexact search.", "labels": [], "entities": []}, {"text": "Among these notable exceptions, many examine how and which approximations break theoretical guarantees of existing learning algorithms (), but we ask a deeper and practically more useful question: can we modify existing learning algorithms to accommodate the inexactness in inference, so that the theoretical properties are still maintained?", "labels": [], "entities": []}, {"text": "For the structured perceptron, provides a partial answer: they suggest variant called \"early update\" for beam search, which updates on partial hypotheses when the correct solution falls out of the beam.", "labels": [], "entities": [{"text": "early update", "start_pos": 87, "end_pos": 99, "type": "METRIC", "confidence": 0.946722537279129}, {"text": "beam search", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.8374652862548828}]}, {"text": "This method works significantly better than standard perceptron, and is followed by later incremental parsers, for instance in (.", "labels": [], "entities": []}, {"text": "However, two problems remain: first, up till now there has been no theoretical justification for early update; and secondly, it makes learning extremely slow as witnessed by the above-cited papers because it only learns on partial examples and often requires 15-40 iterations to converge while normal perceptron converges in 5-10 iterations).", "labels": [], "entities": []}, {"text": "We develop a theoretical framework of \"violationfixing\" perceptron that addresses these challenges.", "labels": [], "entities": []}, {"text": "In particular, we make the following contributions: \u2022 We show that, somewhat surprisingly, exact search is not required by perceptron convergence.", "labels": [], "entities": [{"text": "perceptron convergence", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.7036706954240799}]}, {"text": "All we need is that each update involves a \"violation\", i.e., the 1-best sequence has a higher model score than the correct sequence.", "labels": [], "entities": []}, {"text": "Such an update is considered a \"valid update\", and any perceptron variant that maintains this is bound to converge.", "labels": [], "entities": []}, {"text": "We call these variants \"violation-fixing perceptrons\" (Section 3.1).", "labels": [], "entities": []}, {"text": "\u2022 This theory explains why standard perceptron update may fail to work with inexact search, because violation is no longer guaranteed: the correct structure might indeed be preferred by the model, but was pruned during the search process (Sec. 3.2).", "labels": [], "entities": [{"text": "violation", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9660336375236511}]}, {"text": "Such an update is thus considered invalid, and experiments show that invalid updates lead to bad learning (Sec. 6.2).", "labels": [], "entities": []}, {"text": "\u2022 We show that the early update is always valid and is thus a special casein our framework; this is the first theoretical justification for early update (Section 4).", "labels": [], "entities": []}, {"text": "We also show that (a variant of)) is another special case (Section 7).", "labels": [], "entities": []}, {"text": "\u2022 We then propose several other update methods within this framework (Section 5).", "labels": [], "entities": []}, {"text": "Experiments in Section 6 confirm that among them, the max-violation method can learn equal or better models with dramatically reduced learning times (by 3 fold as compared to early update) on state-of-the-art part-of-speech tagging) 1 and incremental parsing systems.", "labels": [], "entities": [{"text": "part-of-speech tagging)", "start_pos": 209, "end_pos": 232, "type": "TASK", "confidence": 0.7651879191398621}]}, {"text": "We also found strong correlation between search error and invalid updates, suggesting that the advantage of valid update methods is more pronounced with harder inference problems.", "labels": [], "entities": []}, {"text": "Our techniques are widely applicable to other strcutured prediction problems which require inexact search like machine translation and protein folding.", "labels": [], "entities": [{"text": "strcutured prediction", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.747765839099884}, {"text": "machine translation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7456064224243164}, {"text": "protein folding", "start_pos": 135, "end_pos": 150, "type": "TASK", "confidence": 0.71869757771492}]}], "datasetContent": [{"text": "We now present early update for beam search as a Local Violation Fixing Perceptron in Algorithm 8.", "labels": [], "entities": [{"text": "beam search", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.9222213625907898}]}, {"text": "Theorem 6 (convergence of beam search with early update).", "labels": [], "entities": [{"text": "beam search", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.7882799506187439}]}, {"text": "For a separable training scenario S = D, \u03a6, Cb (D), the early update perceptron by plugging Algorithm 8 into Algorithm 2 will make finite number of updates (before convergence): Proof.", "labels": [], "entities": []}, {"text": "By Lemma 3 and Theorem 2.", "labels": [], "entities": [{"text": "Lemma", "start_pos": 3, "end_pos": 8, "type": "METRIC", "confidence": 0.9640956521034241}]}, {"text": "We conduct experiments on two typical structured learning tasks: part-of-speech tagging with a trigram model where exact search is possible, and incremental dependency parsing with arbitrary non-local features where exact search is intractable.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7470666468143463}]}, {"text": "We run both experiments on state-of-the-art implementations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Convergence rate of part-of-speech tagging. In  general, max-violation converges faster and better than  early and standard updates, esp. in smallest beams.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7279726266860962}]}, {"text": " Table 2: Final test results on POS tagging. *:baseline.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.727592796087265}]}, {"text": " Table 3: Final results on incremental parsing. *: baseline.", "labels": [], "entities": []}]}