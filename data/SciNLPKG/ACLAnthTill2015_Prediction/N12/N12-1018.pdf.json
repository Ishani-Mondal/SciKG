{"title": [{"text": "Apples to Oranges: Evaluating Image Annotations from Natural Language Processing Systems", "labels": [], "entities": [{"text": "Evaluating Image Annotations from Natural Language Processing", "start_pos": 19, "end_pos": 80, "type": "TASK", "confidence": 0.7523433991840908}]}], "abstractContent": [{"text": "We examine evaluation methods for systems that automatically annotate images using co-occurring text.", "labels": [], "entities": []}, {"text": "We compare previous datasets for this task using a series of baseline measures inspired by those used in information retrieval , computer vision, and extractive sum-marization.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.7771259844303131}]}, {"text": "Some of our baselines match or exceed the best published scores for those datasets.", "labels": [], "entities": []}, {"text": "These results illuminate incorrect assumptions and improper practices regarding preprocessing, evaluation metrics, and the collection of gold image annotations.", "labels": [], "entities": []}, {"text": "We conclude with a list of recommended practices for future research combining language and vision processing techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic image annotation is an important area with many applications such as tagging, generating captions, and indexing and retrieval on the web.", "labels": [], "entities": [{"text": "Automatic image annotation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7047249476114908}, {"text": "tagging", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.9629176259040833}]}, {"text": "Given an input image, the goal is to generate relevant descriptive keywords that describe the visual content of the image.", "labels": [], "entities": []}, {"text": "The Computer Vision (CV) literature contains countless approaches to this task, using a wide range of learning techniques and visual features to identify aspects such as objects, people, scenes, and events.", "labels": [], "entities": [{"text": "Computer Vision (CV)", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.680419921875}]}, {"text": "Text processing is computationally less expensive than image processing and easily provides information that is difficult to learn visually.", "labels": [], "entities": [{"text": "Text processing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8160302042961121}, {"text": "image processing", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.8304856717586517}]}, {"text": "For this reason, most commerical image search websites identify the semantic content of images using co-occurring text exclusively.", "labels": [], "entities": []}, {"text": "But co-occurring text is also a noisy source for candidate annotations, since not all of the text is visually relevant.", "labels": [], "entities": []}, {"text": "Techniques from Natural Language Processing help align descriptive words and images.", "labels": [], "entities": []}, {"text": "Some examples of previous research use named-entity recognition to identify people in images; term association to estimate the \"visualness\" of candidate annotations (; and topic models to annotate images given both visual and textual features.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.7348383367061615}]}, {"text": "Image annotation using NLP is still an emerging area with many different tasks, datasets, and evaluation methods, making it impossible to compare many recent systems to each other.", "labels": [], "entities": [{"text": "Image annotation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8392859697341919}]}, {"text": "Although there is some effort being made towards establishing shared tasks 1 , it is not yet clear which kinds of tasks and datasets will provide interesting research questions and practical applications in the long term.", "labels": [], "entities": []}, {"text": "Until then, establishing general \"best practices\" for NLP image annotation will help advance and legitimitize this work.", "labels": [], "entities": [{"text": "NLP image annotation", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.8038908044497172}]}, {"text": "In this paper, we propose some good practices and demonstrate why they are important.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first review related work in image annotation evaluation in computer vision, specific challenges, and proposed solutions.", "labels": [], "entities": [{"text": "image annotation evaluation", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.8232549428939819}]}, {"text": "We then relate these challenges to the NLP image annotation task and some of the specific problems we propose to address.", "labels": [], "entities": [{"text": "NLP image annotation task", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.8056071549654007}]}, {"text": "In this paper, we examine two established image annotation datasets: the BBC News Dataset of (henceforth referred to as BBC), and the general web dataset of (henceforth referred to as UNT).", "labels": [], "entities": [{"text": "BBC News Dataset", "start_pos": 73, "end_pos": 89, "type": "DATASET", "confidence": 0.9554166793823242}, {"text": "BBC", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.9648709297180176}, {"text": "UNT", "start_pos": 184, "end_pos": 187, "type": "DATASET", "confidence": 0.9071763753890991}]}, {"text": "These datasets were both built to evaluate image annotation systems that use longer co-occurring text such as a news article or a webpage, but they use data from differ- ent domains, different sources of gold image annotations, different preprocessing procedures, and different evaluation measures.", "labels": [], "entities": []}, {"text": "provides an overview of the datasets; while this section covers the source of the datsets and their gold annotations in more detail.", "labels": [], "entities": []}, {"text": "Evaluation on UNT uses a framework originally developed for the SemEval lexical substitution task.", "labels": [], "entities": [{"text": "SemEval lexical substitution task", "start_pos": 64, "end_pos": 97, "type": "TASK", "confidence": 0.9271711856126785}]}, {"text": "This framework accounts for disagreement between annotators by weighting each generated keyword by the number of human annotators who also selected that keyword.", "labels": [], "entities": []}, {"text": "The scoring framework consists of four evaluation measures: best normal, best mode, oot (out-of-ten) normal, and oot mode.", "labels": [], "entities": []}, {"text": "The two best evaluations find the accuracy of a single \"best\" keyword generated by the system 12 . And as we stated earlier, the relative performance of term frequency vs tf*idf is different from dataset to dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9991518259048462}]}, {"text": "11 Both the original framework and its adaptation by give precision and recall for each of the evaluation measures.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9994818568229675}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9994094371795654}]}, {"text": "However, precision and recall are identical for all baselines and systems, and only slightly different on the upper bound (human) scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9996602535247803}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9996042847633362}]}, {"text": "To preserve space, we only present the metric and scores for precision.", "labels": [], "entities": [{"text": "metric", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9633640646934509}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9985703229904175}]}, {"text": "In contrast to the original SemEval task, where systems can Best normal measures the accuracy for each system annotation a j as the number of times a j appears in the R j , the multi-set union of human tags, and averages overall the test images.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.8737660348415375}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9993048906326294}]}], "tableCaptions": [{"text": " Table 2: Image annotation results for previous systems and our proposed baselines on the BBC Dataset.", "labels": [], "entities": [{"text": "BBC Dataset", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.9939120709896088}]}, {"text": " Table 4: Examples of gold annotations from the UNT Dataset.", "labels": [], "entities": [{"text": "UNT Dataset", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.9795397222042084}]}, {"text": " Table 5: Image annotation results for our proposed baselines, the text mining systems from (", "labels": [], "entities": [{"text": "text mining", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.7120656073093414}]}]}