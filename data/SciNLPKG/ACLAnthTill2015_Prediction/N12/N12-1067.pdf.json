{"title": [{"text": "Better Evaluation for Grammatical Error Correction", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.9647186994552612}, {"text": "Grammatical Error Correction", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.5691993435223898}]}], "abstractContent": [{"text": "We present a novel method for evaluating grammatical error correction.", "labels": [], "entities": [{"text": "evaluating grammatical error correction", "start_pos": 30, "end_pos": 69, "type": "TASK", "confidence": 0.6351761445403099}]}, {"text": "The core of our method, which we call MaxMatch (M 2), is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the gold-standard annotation.", "labels": [], "entities": [{"text": "overlap", "start_pos": 207, "end_pos": 214, "type": "METRIC", "confidence": 0.9527571201324463}]}, {"text": "This optimal edit sequence is subsequently scored using F 1 measure.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 56, "end_pos": 67, "type": "METRIC", "confidence": 0.9861134092013041}]}, {"text": "We test our M 2 scorer on the Helping Our Own (HOO) shared task data and show that our method results in more accurate evaluation for grammatical error correction.", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.7968607346216837}, {"text": "grammatical error correction", "start_pos": 134, "end_pos": 162, "type": "TASK", "confidence": 0.5971838732560476}]}], "introductionContent": [{"text": "Progress in natural language processing (NLP) research is driven and measured by automatic evaluation methods.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.7717230021953583}]}, {"text": "Automatic evaluation allows fast and inexpensive feedback during development, and objective and reproducible evaluation during testing time.", "labels": [], "entities": []}, {"text": "Grammatical error correction is an important NLP task with useful applications for second language learning.", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8239245414733887}, {"text": "second language learning", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.6953487197558085}]}, {"text": "Evaluation for error correction is typically done by computing F 1 measure between a set of proposed system edits and a set of humanannotated gold-standard edits (.", "labels": [], "entities": [{"text": "error correction", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.6632594764232635}, {"text": "F 1 measure", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9732502301534017}]}, {"text": "Unfortunately, evaluation is complicated by the fact that the set of edit operations fora given system hypothesis is ambiguous.", "labels": [], "entities": []}, {"text": "This is due to two reasons.", "labels": [], "entities": []}, {"text": "First, the set of edits that transforms one string into another is not necessarily unique, even at the token level.", "labels": [], "entities": []}, {"text": "Second, edits can consist of longer phrases which introduce additional ambiguity.", "labels": [], "entities": []}, {"text": "To see how this can affect evaluation, consider the following source sentence and system hypothesis from the recent Helping Our Own (HOO) shared task () on grammatical error correction: Source : Our baseline system feeds word into PB-SMT pipeline.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 156, "end_pos": 184, "type": "TASK", "confidence": 0.6686803797880808}]}, {"text": ": Our baseline system feeds a word into PB-SMT pipeline.", "labels": [], "entities": []}, {"text": "The HOO evaluation script extracts the system edit ( \u2192 a), i.e., inserting the article a.", "labels": [], "entities": []}, {"text": "Unfortunately, the gold-standard annotation instead contains the edits (word \u2192 {a word, words}).", "labels": [], "entities": []}, {"text": "Although the extracted system edit results in the same corrected sentence as the first gold-standard edit option, the system hypothesis was considered to be invalid.", "labels": [], "entities": []}, {"text": "In this work, we propose a method, called MaxMatch (M 2 ), to overcome this problem.", "labels": [], "entities": []}, {"text": "The key idea is that if there are multiple possible ways to arrive at the same correction, the system should be evaluated according to the set of edits that matches the gold-standard as often as possible.", "labels": [], "entities": []}, {"text": "To this end, we propose an algorithm for efficiently computing the set of phrase-level edits with the maximum overlap with the gold standard.", "labels": [], "entities": []}, {"text": "The edits are subsequently scored using F 1 measure.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.9876857995986938}]}, {"text": "We test our method in the context of the HOO shared task and show that our method results in a more accurate evaluation for error correction.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 describes the proposed method; Section 3 presents experimental results; Section 4 discusses some details of grammar correction evaluation; and Section 5 concludes the paper.", "labels": [], "entities": [{"text": "grammar correction evaluation", "start_pos": 171, "end_pos": 200, "type": "TASK", "confidence": 0.8518437147140503}]}], "datasetContent": [{"text": "We experimentally test our M 2 method in the context of the HOO shared task.", "labels": [], "entities": [{"text": "HOO shared task", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.49618135889371234}]}, {"text": "The HOO test data 2 consists of text fragments from NLP papers together with manually-created gold-standard corrections (see () for details).", "labels": [], "entities": [{"text": "HOO test data 2", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.780915193259716}]}, {"text": "We test our method by re-scoring the best runs of the participating teams 3 in the HOO shared task with our M 2 scorer and comparing the scores with the official HOO scorer, which simply uses GNU wdiff 4 to extract system edits.", "labels": [], "entities": [{"text": "HOO shared task", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.8222118616104126}, {"text": "HOO scorer", "start_pos": 162, "end_pos": 172, "type": "DATASET", "confidence": 0.8306015133857727}]}, {"text": "We obtain each system's output and segment it at the sentence level according to the gold standard sentence segmentation.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7735027372837067}]}, {"text": "The M 2 scorer . .", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7858158151308695}]}, {"text": "should basic translational unit be (word \u2192 a word) . .", "labels": [], "entities": []}, {"text": "should basic translational unit be *( \u2192 a) word . .", "labels": [], "entities": []}, {"text": "development set similar (with \u2192 to) ( \u2192 the) test set . .", "labels": [], "entities": []}, {"text": "development set similar *(with \u2192 to the) test set . .", "labels": [], "entities": []}, {"text": "M 2 scorer ( \u2192 The) *(Xinhua portion of \u2192 xinhua portion of) the English Gigaword3 . .", "labels": [], "entities": []}, {"text": "HOO scorer *(Xinhua \u2192 The xinhua) portion of the English Gigaword3 . .", "labels": [], "entities": [{"text": "HOO scorer", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.7310247421264648}]}, {"text": ".: Results for participants in the HOO shared task.", "labels": [], "entities": [{"text": "HOO shared task", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7448951403299967}]}, {"text": "The run of the system is shown in parentheses.", "labels": [], "entities": [{"text": "run", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9866260290145874}]}, {"text": "source sentences, system hypotheses, and corrections are tokenized using the Penn Treebank standard ().", "labels": [], "entities": [{"text": "Penn Treebank standard", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.9946494301160177}]}, {"text": "The character edit offsets are automatically converted to token offsets.", "labels": [], "entities": []}, {"text": "We set the parameter u to 2, allowing up to two unchanged words per edit.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Note that the M 2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed.", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.7272869149843851}, {"text": "HOO scorer", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9256042540073395}]}, {"text": "We can see that the M 2 scorer results in higher scores than the official scorer for all systems, showing that the official scorer missed some valid edits.", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.8357091347376505}]}, {"text": "For example, the M 2 scorer finds 155 valid edits for the UI system compared to 141 found by the official scorer, and 83 valid edits for the NU system, compared to 78 by the official scorer.", "labels": [], "entities": [{"text": "UI system", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.8425173163414001}, {"text": "NU system", "start_pos": 141, "end_pos": 150, "type": "DATASET", "confidence": 0.9428712129592896}]}, {"text": "We manually inspect the output of the scorers and find that the M 2 scorer indeed extracts the correct edits matching the gold standard where possible.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples of different edits extracted by the M 2 scorer and the official HOO scorer. Edits that do not match  the gold-standard annotation are marked with an asterisk (*).", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.6800528963406881}, {"text": "HOO scorer", "start_pos": 83, "end_pos": 93, "type": "DATASET", "confidence": 0.768425315618515}]}, {"text": " Table 1: Results for participants in the HOO shared task.  The run of the system is shown in parentheses.", "labels": [], "entities": [{"text": "HOO shared task", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.5893044471740723}, {"text": "run", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.984005331993103}]}]}