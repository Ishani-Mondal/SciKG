{"title": [{"text": "Are You Sure? Confidence in Prediction of Dependency Tree Edges", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe and evaluate several methods for estimating the confidence in the per-edge cor-rectness of a predicted dependency parse.", "labels": [], "entities": []}, {"text": "We show empirically that the confidence is associated with the probability that an edge is selected correctly and that it can be used to detect incorrect edges very efficiently.", "labels": [], "entities": []}, {"text": "We evaluate our methods on parsing text in 14 languages .", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency parsers construct directed edges between words of a given sentence to their arguments according to syntactic or semantic rules.", "labels": [], "entities": [{"text": "Dependency parsers construct directed edges between words of a given sentence", "start_pos": 0, "end_pos": 77, "type": "TASK", "confidence": 0.8797989108345725}]}, {"text": "We use MSTParser of and focus on non-projective dependency parse trees with nontyped (unlabeled) edges.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 7, "end_pos": 16, "type": "DATASET", "confidence": 0.8497553467750549}]}, {"text": "MSTParser produces a parse tree fora sentence by constructing a full, directed and weighted graph over the words of the sentence, and then outputting the maximal spanning tree (MST) of the graph.", "labels": [], "entities": []}, {"text": "A linear model is employed for computing the weights of the edges using features depending on the two words the edge connects.", "labels": [], "entities": []}, {"text": "Example features are the distance between the two words, words identity and words part-of-speech.", "labels": [], "entities": []}, {"text": "MSTParser is training a model using online learning and specifically the MIRA algorithm).", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9279273748397827}, {"text": "MIRA", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.6668983697891235}]}, {"text": "The output of MSTParser is the highest scoring parse tree, it is not accompanied by any additional information about its quality.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 14, "end_pos": 23, "type": "DATASET", "confidence": 0.9054425954818726}]}, {"text": "In this work we evaluate few methods for estimating the confidence in the correctness of the prediction of a parser.", "labels": [], "entities": []}, {"text": "This information can be used in several ways.", "labels": [], "entities": []}, {"text": "For example, when using parse trees as input to another system such as machine translation, the confidence information can be used to correct inputs with low confidence.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7698935866355896}]}, {"text": "Another example is to guide manual validation to outputs which are more likely to be erroneous, saving human labor.", "labels": [], "entities": []}, {"text": "We adapt methods proposed by in order to produce per-edge confidence estimations in the prediction.", "labels": [], "entities": []}, {"text": "Specifically, one approach is based on sampling, and another on a generalization of the concept of margin.", "labels": [], "entities": [{"text": "margin", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9722352623939514}]}, {"text": "Additionally, we propose anew method based on combining both approaches, and show that is outperforms both.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the algorithms using 13 languages used in CoNLL 2006 shared task 1 , and the English Penn Treebank.", "labels": [], "entities": [{"text": "CoNLL 2006 shared task 1", "start_pos": 55, "end_pos": 79, "type": "DATASET", "confidence": 0.9047885179519654}, {"text": "English Penn Treebank", "start_pos": 90, "end_pos": 111, "type": "DATASET", "confidence": 0.8979169329007467}]}, {"text": "The number of training sentences is between 1.5-72K, with an average of 20K sentences and 50K-1M words.", "labels": [], "entities": []}, {"text": "The test sets contain \u223c 400 sentences and \u223c 6K words for all datasets, except English with 2.3K sentences and 55K words.", "labels": [], "entities": []}, {"text": "Parameter tuning was performed on development sets with 200 sentences per dataset.", "labels": [], "entities": [{"text": "Parameter tuning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7591325342655182}]}, {"text": "We trained a model per dataset and used it to parse the test set.", "labels": [], "entities": []}, {"text": "Predicted edge accuracy of the parser ranges from 77% on Turkish to 93% on Japanese, with an average of 85%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.7886533737182617}]}, {"text": "We then assigned each predicted edge a confidence score using the various confidence estimation methods.", "labels": [], "entities": []}, {"text": "Absolute Confidence: We first evaluate the accuracy of the actual confidence values assigned by all methods.", "labels": [], "entities": [{"text": "Absolute", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9849505424499512}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9991298317909241}]}, {"text": "Similar to we grouped edges according to the value of their confidence.", "labels": [], "entities": []}, {"text": "We used 20 bins dividing the confidence range into intervals of size 0.05.", "labels": [], "entities": []}, {"text": "Bin indexed j contains edges with confidence value in the range [ j\u22121 20 , j 20 ] , j = 1..20.", "labels": [], "entities": []}, {"text": "Let b j be the center value of bin j and let c j be the fraction of edges predicted correctly from the edges assigned to bin j.", "labels": [], "entities": []}, {"text": "For a good confidence estimator we expect b j \u2248 c j . Results for 4 datasets are presented in.", "labels": [], "entities": []}, {"text": "Plots show the measured fraction of correctly predicted edges c j vs. the value of the center of bin b j . Best performance is obtained when a line corresponding to a method is close to the line y = x.", "labels": [], "entities": []}, {"text": "Results are shown for KD-Fix and WKB; Delta is omitted as it produces confidence scores out of  and too pessimistic in another range.", "labels": [], "entities": [{"text": "WKB", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.8220600485801697}, {"text": "Delta", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9959282279014587}, {"text": "confidence", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9552999138832092}]}, {"text": "We computed the root mean square-error (RMSE) in predicting the bin center value given by Incorrect Edges Detection: The goal of this task is to efficiently detect incorrect predicted-edges.", "labels": [], "entities": [{"text": "root mean square-error (RMSE)", "start_pos": 16, "end_pos": 45, "type": "METRIC", "confidence": 0.8052022705475489}, {"text": "Incorrect Edges Detection", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.5995910366376241}]}, {"text": "We ranked all predicted edges of the test-set (per dataset) according to their confidence score, ordering from low to high.", "labels": [], "entities": []}, {"text": "Ideally, erroneous edges by the parser are ranked at the top.", "labels": [], "entities": []}, {"text": "A summary of the average precision, computed at all ranks of erroneous edges, (averaged overall datasets, due to lack of space), for all confidence estimation methods is summarized in the first row of.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9073683619499207}]}, {"text": "The average precision achieved by random ordering is about equal to the error rate for each dataset.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9990468621253967}, {"text": "error rate", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9606007039546967}]}, {"text": "The Delta method improves significantly over both the random ordering and WKB.", "labels": [], "entities": [{"text": "WKB", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.8944510221481323}]}, {"text": "KD-Fix achieves the best performance in 12 of 14 datasets and the best averageperformance.", "labels": [], "entities": [{"text": "KD-Fix", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7546690106391907}]}, {"text": "These results are consistent with the results obtained for sequence labeling by.", "labels": [], "entities": []}, {"text": "Average precision summarizes the detection of all incorrect edges into a single number.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.8605935573577881}]}, {"text": "More refined analysis is encapsulated in Precision-Recall (PR) plots, showing the precision as more incorrect edges are detected.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9993639588356018}]}, {"text": "PR plots for three datasets are shown in.", "labels": [], "entities": []}, {"text": "From these plots (applied also to other datasets, omitted due to lack of space) we observe that inmost cases KD-Fix performs significantly better than Delta in the early detection stage (first 10-20% of the incorrect edges), while Delta performs better in late detection stages (last 10-20% of the incorrect edges).", "labels": [], "entities": []}, {"text": "The second and third rows of summarize the precision after detecting only 10% incorrect edges and after detecting 90% of the incorrect edges, averaged overall datasets.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9994557499885559}]}, {"text": "KD-Fix assigns at most K distinct confidence values to each edge -the number of models that agreed on that particular edge.", "labels": [], "entities": []}, {"text": "Thus, when edges are ranked according to the confidence, all edges that are assigned the same value are ordered randomly.", "labels": [], "entities": []}, {"text": "Furthermore, large fraction of the edges, \u223c 70 \u2212 80%, are assigned one of the top-three scores (i.e. K-2, K-1, K).", "labels": [], "entities": []}, {"text": "As a results, the precision performance of KD-Fix drops sharply for recall values of 80% and above.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9996242523193359}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9993909597396851}]}, {"text": "On the other hand, we hypothesize that the lower precision of Delta at low recall values (diamond in) is because by definition Delta takes into account only two parses, ignoring additional possible parses with score close to the highest score.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9984797835350037}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9959771037101746}]}, {"text": "This makes Delta method more sensitive to small differences in score values compared to KD-Fix.", "labels": [], "entities": [{"text": "KD-Fix", "start_pos": 88, "end_pos": 94, "type": "DATASET", "confidence": 0.8228575587272644}]}, {"text": "Based on this observation, we propose combining both KD-Fix and Delta.", "labels": [], "entities": []}, {"text": "Our new method sets the confidence score of an edge to be a weighted mean of the score values of KD-Fix and Delta, with weights a and 1-a, respectively.", "labels": [], "entities": []}, {"text": "We use a value To illustrate the effectiveness of the incorrect edges detection process, presents the number of incorrect edges detected vs. number of edges inspected for the English dataset.", "labels": [], "entities": [{"text": "incorrect edges detection", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.6663934389750162}, {"text": "English dataset", "start_pos": 175, "end_pos": 190, "type": "DATASET", "confidence": 0.7981980443000793}]}, {"text": "The test set for this task includes 55K words and the parser made mistake on 6, 209 edges, that is, accuracy of 88.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9995588660240173}]}, {"text": "We see that using the ranking induced by KD-Fix+Delta method, inspection of 550, 2750 and 5500 edges (1, 5, 10% of all edges), allows detection of 6.6 \u2212 46% of all incorrect edges, over 4.5 times more effective than random validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Row 1: Average precision in ranking all edges ac-", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.954243540763855}]}, {"text": " Table 2: Number of incorrect edges detected, and the corre-", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9689775109291077}, {"text": "corre-", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9414694607257843}]}]}