{"title": [{"text": "Getting More from Morphology in Multilingual Dependency Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.7837190628051758}]}], "abstractContent": [{"text": "We propose a linguistically motivated set of features to capture morphological agreement and add them to the MSTParser dependency parser.", "labels": [], "entities": [{"text": "MSTParser dependency parser", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.7000666459401449}]}, {"text": "Compared to the built-in morphological feature set, ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks.", "labels": [], "entities": [{"text": "accurate", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.962011456489563}]}, {"text": "We find increases inaccuracy of up to 5.3% absolute.", "labels": [], "entities": [{"text": "absolute", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9595020413398743}]}, {"text": "While some of this results from the feature set capturing information unrelated to morphology , there is still significant improvement, up to 4.6% absolute, due to the agreement model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most data-driven dependency parsers are meant to be language-independent.", "labels": [], "entities": []}, {"text": "They do not use any information that is specific to the language being parsed, and they often rely heavily on n-grams, or sequences of words and POS tags, to make parsing decisions.", "labels": [], "entities": []}, {"text": "However, designing a parser without incorporating any specific linguistic details does not guarantee its language-independence; even linguistically na\u00a8\u0131vena\u00a8\u0131ve systems can involve design decisions which in fact bias the system towards languages with certain properties.", "labels": [], "entities": []}, {"text": "It is often taken for granted that using linguistic information necessarily makes a system languagedependent.", "labels": [], "entities": []}, {"text": "But it is possible to design a linguistically intelligent parser without tuning it to a specific language, by modeling at a high level phenomena which appear cross-linguistically.", "labels": [], "entities": []}, {"text": "Such a system is still language-independent; it does not require any knowledge or modeling of specific languages, but it does use linguistic knowledge to make the most of the available data.", "labels": [], "entities": []}, {"text": "We present modifications to an existing system, MSTParser), to incorporate a very simple model of morphological agreement.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.9137688279151917}]}, {"text": "These modifications improve parsing performance across a variety of languages by making better use of morphological annotations.", "labels": [], "entities": []}], "datasetContent": [{"text": "All experiments were performed using 5-fold crossvalidation.", "labels": [], "entities": []}, {"text": "Reported accuracies, run times, and feature counts are averages overall five folds.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.8206132054328918}, {"text": "feature counts", "start_pos": 36, "end_pos": 50, "type": "METRIC", "confidence": 0.9138203561306}]}, {"text": "We ran experiments on multiple cross-validation dataset sizes in order to assess the performance of our model when trained on different amounts of data.", "labels": [], "entities": []}, {"text": "For each treebank, we report results on a \"reference size\": 9,000 sentences or the largest size available (for treebanks of less than 9,000 sentences).", "labels": [], "entities": []}, {"text": "For evaluation, we used the module built into MSTParser.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.9690853357315063}]}, {"text": "We focused on the unlabeled accuracy score (percentage of tokens with correctly assigned heads, ignoring labels).", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.9805794358253479}]}, {"text": "We also looked at labeled accuracies, but found they displayed trends very similar, if not identical, to the unlabeled scores.", "labels": [], "entities": []}, {"text": "Figures 2 presents unlabeled accuracy when parsing Czech with the orig and agr configurations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9985669255256653}, {"text": "parsing Czech", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.8335843682289124}]}, {"text": "Improvement with agr is roughly uniform across all dataset sizes; this was the general trend for all treebanks.", "labels": [], "entities": []}, {"text": "This is somewhat unexpected; we had predicted that the agreement features would be more helpful at smaller dataset sizes.: Unlabeled accuracy, run time in seconds, and number of features for all treebanks and feature configurations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9681946635246277}]}, {"text": "Run time and number of features for orig, agr, and agr+orig are given as percent change relative to no-morph", "labels": [], "entities": [{"text": "Run time", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9520525336265564}]}], "tableCaptions": [{"text": " Table 4: Sample sentence (Haji\u010d", "labels": [], "entities": [{"text": "Sample", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.7328228950500488}]}, {"text": " Table 5: Language, ISO 639-2 code, treebank name, total number of sentences, reference size, average number of  morphological attributes per token, and reference for each treebank used, ordered by average number of attributes.", "labels": [], "entities": []}, {"text": " Table 6: Unlabeled accuracy, run time in seconds, and number of features for all treebanks and feature configurations.  Run time and number of features for orig, agr, and agr+orig are given as percent change relative to no-morph", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9561412930488586}]}, {"text": " Table 7: Unlabeled accuracy on Hebrew dataset, with  gold and automatic POS and morphological annotations", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9920719265937805}, {"text": "Hebrew dataset", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.731552928686142}, {"text": "POS", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9135576486587524}]}, {"text": " Table 8: Unlabeled accuracy, run time in seconds, and number of features with PPL feature included. Run time and  number of features for orig, agr, and agr+orig are given as percent change relative to no-morph.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.959472119808197}]}]}