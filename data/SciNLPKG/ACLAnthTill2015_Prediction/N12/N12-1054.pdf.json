{"title": [{"text": "Vine Pruning for Efficient Multi-Pass Dependency Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.5392685532569885}]}], "abstractContent": [{"text": "Coarse-to-fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9950851798057556}]}, {"text": "We propose a multi-pass coarse-to-fine architecture for dependency parsing using linear-time vine pruning and structured prediction cascades.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.8950324356555939}]}, {"text": "Our first-, second-, and third-order models achieve accuracies comparable to those of their un-pruned counterparts, while exploring only a fraction of the search space.", "labels": [], "entities": []}, {"text": "We observe speed-ups of up to two orders of magnitude compared to exhaustive search.", "labels": [], "entities": [{"text": "speed-ups", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9728690385818481}]}, {"text": "Our pruned third-order model is twice as fast as an un-pruned first-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Coarse-to-fine inference has been extensively used to speedup structured prediction models.", "labels": [], "entities": []}, {"text": "The general idea is simple: use a coarse model where inference is cheap to prune the search space for more complex models.", "labels": [], "entities": []}, {"text": "In this work, we present a multipass coarse-to-fine architecture for graph-based dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7302951216697693}]}, {"text": "We start with a linear-time vine pruning pass and buildup to higher-order models, achieving speed-ups of two orders of magnitude while maintaining state-of-the-art accuracies.", "labels": [], "entities": []}, {"text": "In constituency parsing, exhaustive inference for all but the simplest grammars tends to be prohibitively slow.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.8767610490322113}]}, {"text": "Consequently, most high-accuracy constituency parsers routinely employ a coarse grammar to prune dynamic programming chart cells * Research conducted at Google. of the final grammar of interest (.", "labels": [], "entities": []}, {"text": "While there are no strong theoretical guarantees for these approaches, in practice one can obtain significant speed improvements with minimal loss inaccuracy.", "labels": [], "entities": []}, {"text": "This benefit comes primarily from reducing the large grammar constant |G| that can dominate the runtime of the cubic-time CKY inference algorithm.", "labels": [], "entities": []}, {"text": "Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference.", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7108262628316879}]}, {"text": "However, the increased model complexity of a third-order parser forced  to prune with a first-order model in order to make inference practical.", "labels": [], "entities": []}, {"text": "While fairly effective, all these approaches are limited by the fact that inference in the coarse model remains cubic in the sentence length.", "labels": [], "entities": []}, {"text": "The desire to parse vast amounts of text necessitates more efficient dependency parsing algorithms.", "labels": [], "entities": [{"text": "parse vast amounts of text", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.8190351009368897}, {"text": "dependency parsing", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7977204024791718}]}, {"text": "We thus propose a multi-pass coarse-to-fine approach where the initial pass is a linear-time sweep, which tries to resolve local ambiguities, but leaves arcs beyond a fixed length b unspecified (Section 3).", "labels": [], "entities": []}, {"text": "The dynamic program is a form of vine parsing), which we use to compute parse max-marginals, rather than for finding the 1-best parse tree.", "labels": [], "entities": [{"text": "vine parsing", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.7460477352142334}]}, {"text": "To reduce pruning errors, the parameters of the vine parser (and all subsequent pruning models) are trained using the structured prediction cascades of to optimize for pruning efficiency, and not for 1-best prediction (Section 4).", "labels": [], "entities": []}, {"text": "Despite a limited scope of b = 3, the vine pruning pass is able to preserve >98% of the correct arcs, while ruling out \u223c86% of all possible arcs.", "labels": [], "entities": [{"text": "correct arcs", "start_pos": 88, "end_pos": 100, "type": "METRIC", "confidence": 0.6896512508392334}]}, {"text": "Subsequent i-th order passes introduce larger scope features, while further constraining the search space.", "labels": [], "entities": []}, {"text": "In Section 5 we present experiments in multiple languages.", "labels": [], "entities": []}, {"text": "Our coarse-to-fine first-, second-, and third-order parsers preserve the accuracy of the unpruned models, but are faster by up to two orders of magnitude.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9995830655097961}]}, {"text": "Our pruned third-order model is faster than an unpruned first-order model, and compares favorably in speed to the state-of-the-art transitionbased parser of.", "labels": [], "entities": [{"text": "speed", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9740132093429565}]}, {"text": "It is worth noting the relationship to greedy transition-based dependency parsers that are also linear-time () or quadratic-time).", "labels": [], "entities": []}, {"text": "It is their success that motivates building explicitly trained, linear-time pruning models.", "labels": [], "entities": []}, {"text": "However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, recently showed that computing exact solutions or (max-)marginals has time complexity O(n 4 ), making these models inappropriate for coarse-to-fine style pruning.", "labels": [], "entities": []}, {"text": "As an alternative, and present approaches where individual classifiers are used to prune chart cells.", "labels": [], "entities": []}, {"text": "Such approaches have the drawback that pruning decisions are made locally and therefore can rule out all valid structures, despite explicitly evaluating O(n 2 ) chart cells.", "labels": [], "entities": []}, {"text": "In contrast, we make pruning decisions based on global parse max-marginals using a vine pruning pass, which is linear in the sentence length, but nonetheless guarantees to preserve a valid parse structure.", "labels": [], "entities": []}], "datasetContent": [{"text": "To empirically demonstrate the effectiveness of our approach, we compare our vine pruning cascade with a wide range of common pruning methods on the Penn WSJ Treebank (PTB) (.", "labels": [], "entities": [{"text": "Penn WSJ Treebank (PTB)", "start_pos": 149, "end_pos": 172, "type": "DATASET", "confidence": 0.9693710307280222}]}, {"text": "We then also show that vine pruning is effective across a variety of different languages.", "labels": [], "entities": []}, {"text": "For English, we convert the PTB constituency trees to dependencies using the Stanford dependency framework).", "labels": [], "entities": [{"text": "PTB constituency trees", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.8871171673138937}]}, {"text": "We then train on the standard PTB split with sections 2-21 as training, section 22 as validation, and section 23 as test.", "labels": [], "entities": [{"text": "PTB split", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.9362498819828033}]}, {"text": "Results are similar using the conversion.", "labels": [], "entities": []}, {"text": "We additionally selected six languages from the CoNLL-X shared task) that cover a number of different language families: Bulgarian, Chinese, Japanese, German, Portuguese, and Swedish.", "labels": [], "entities": []}, {"text": "We use the standard CoNLL-X training/test split and tune parameters with cross-validation.", "labels": [], "entities": []}, {"text": "All experiments use unlabeled dependencies for training and test.", "labels": [], "entities": []}, {"text": "Accuracy is reported as unlabeled attachment score (UAS), the percentage of tokens with the correct headword.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.995417594909668}, {"text": "unlabeled attachment score (UAS)", "start_pos": 24, "end_pos": 56, "type": "METRIC", "confidence": 0.8477078974246979}]}, {"text": "For English, UAS ignores punctuation tokens and the test set uses predicted POS tags.", "labels": [], "entities": []}, {"text": "For the other languages we follow the CoNLL-X setup and include punctuation in UAS and use gold POS tags on the set set.", "labels": [], "entities": []}, {"text": "Speedups are given in terms of time relative to a highly optimized C++ implementation.", "labels": [], "entities": []}, {"text": "Our unpruned firstorder baseline can process roughly two thousand tokens a second and is comparable in speed to the greedy shift-reduce parser of.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results comparing pruning methods on PTB Section 22. Oracle is the max achievable UAS after pruning.  Pruning efficiency (PE) is the percentage of non-gold first-order dependency arcs pruned. Speed is parsing time relative  to the unpruned first-order model (around 2000 tokens/sec). UAS is the unlabeled attachment score of the final parses.", "labels": [], "entities": [{"text": "PTB Section 22", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9651559789975485}, {"text": "max achievable UAS", "start_pos": 77, "end_pos": 95, "type": "METRIC", "confidence": 0.6500603159268697}, {"text": "Pruning efficiency (PE)", "start_pos": 112, "end_pos": 135, "type": "METRIC", "confidence": 0.9016849756240845}, {"text": "Speed", "start_pos": 202, "end_pos": 207, "type": "METRIC", "confidence": 0.9629859328269958}, {"text": "parsing", "start_pos": 211, "end_pos": 218, "type": "TASK", "confidence": 0.9626973867416382}, {"text": "UAS", "start_pos": 294, "end_pos": 297, "type": "METRIC", "confidence": 0.9941827654838562}]}, {"text": " Table 2: Relative speed of pruning models in a multi-pass  cascade. Note that the 1-best models use richer features  than the corresponding pruning models.", "labels": [], "entities": []}, {"text": " Table 3: Speed and accuracy results for the vine prun- ing cascade across various languages. B is the un- pruned baseline model, and V is the vine pruning cas- cade. The first section of the table gives results for  the CoNLL-X test datasets for Bulgarian (BG), German  (DE), Japanese (JA), Portuguese (PT), Swedish (SW),  and Chinese (ZH). The second section gives the result  for the English (EN) test set, PTB Section 23.", "labels": [], "entities": [{"text": "Speed", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9911893606185913}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9993664622306824}, {"text": "vine prun- ing cascade", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.669920003414154}, {"text": "CoNLL-X test datasets", "start_pos": 221, "end_pos": 242, "type": "DATASET", "confidence": 0.9025943477948507}, {"text": "English (EN) test set", "start_pos": 387, "end_pos": 408, "type": "DATASET", "confidence": 0.726531465848287}, {"text": "PTB Section 23", "start_pos": 410, "end_pos": 424, "type": "DATASET", "confidence": 0.9251508514086405}]}]}