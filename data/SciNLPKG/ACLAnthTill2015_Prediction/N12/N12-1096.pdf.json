{"title": [], "abstractContent": [{"text": "With a few exceptions, extensions to latent Dirichlet allocation (LDA) have focused on the distribution over topics for each document.", "labels": [], "entities": [{"text": "latent Dirichlet allocation (LDA)", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.7074792683124542}]}, {"text": "Much less attention has been given to the underlying structure of the topics themselves.", "labels": [], "entities": []}, {"text": "As a result, most topic models generate topics independently from a single underlying distribution and require millions of parameters, in the form of multinomial distributions over the vocabulary.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the Shared Components Topic Model (SCTM), in which each topic is a normalized product of a smaller number of underlying component distributions.", "labels": [], "entities": []}, {"text": "Our model learns these component distributions and the structure of how to combine subsets of them into topics.", "labels": [], "entities": []}, {"text": "The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic models are probabilistic graphical models meant to capture the semantic associations underlying corpora.", "labels": [], "entities": []}, {"text": "Since the introduction of latent Dirichlet allocation (LDA) (, these models have been extended to account for more complex distributions over topics, such as adding supervision, non-parametric priors (), topic correlations () and sparsity ().", "labels": [], "entities": [{"text": "latent Dirichlet allocation (LDA)", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.7154260724782944}]}, {"text": "While much research has focused on modeling distributions over topics, less focus has been given to the makeup of the topics themselves.", "labels": [], "entities": []}, {"text": "This emphasis leads us to find two problems with LDA and its variants mentioned above: (1) independently generated topics and (2) overparameterized models.", "labels": [], "entities": []}, {"text": "Independent Topics In the models above, the topics are modeled as independent draws from a single underlying distribution, typically a Dirichlet.", "labels": [], "entities": []}, {"text": "This violates the topic modeling community's intuition that these distributions over words are often related.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7821331918239594}]}, {"text": "As an example, consider a corpus that supports two related topics, baseball and hockey.", "labels": [], "entities": []}, {"text": "These topics likely overlap in their allocation of mass to high probability words (e.g. team, season, game, players), even though the two topics are unlikely to appear in the same documents.", "labels": [], "entities": []}, {"text": "When topics are generated independently, the model does not provide away to capture this sharing between related topics.", "labels": [], "entities": []}, {"text": "Many extensions to LDA have addressed a related issue, LDA's inability to model topic correlation, 1 by changing the distributions over topics ;).", "labels": [], "entities": []}, {"text": "Yet, none of these change the underlying structure of the topic's distributions over words.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the SCTM with LDA in terms of overall model performance (held-out perplexity) as well as parameter usage (varying numbers of components and topics).", "labels": [], "entities": []}, {"text": "We select LDA as our baseline since our model differs only in how it forms topics, which focuses evaluation on the benefit of this model change.", "labels": [], "entities": []}, {"text": "We consider two popular data sets for comparison: NIPS: A collection of 1,617 NIPS abstracts from 1987 to 1999 7 , with 77,952 tokens and 1,632 types.", "labels": [], "entities": []}, {"text": "20NEWS: 1,000 randomly selected articles from the 20 Newsgroups dataset, 8 with 70,011 tokens and 1,722 types.", "labels": [], "entities": [{"text": "20NEWS", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9565408825874329}, {"text": "20 Newsgroups dataset", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.714244931936264}]}, {"text": "Both data sets excluded stop words and words occurring in fewer than 10 documents.", "labels": [], "entities": []}, {"text": "For 20NEWS, we used the standard by-date train/test split.", "labels": [], "entities": [{"text": "20NEWS", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.9422377347946167}]}, {"text": "For NIPS, we randomly partitioned the data by document into 75% train and 25% test.", "labels": [], "entities": []}, {"text": "We compare the SCTM to LDA by evaluating the average perplexity-per-word of the held-out test", "labels": [], "entities": []}], "tableCaptions": []}