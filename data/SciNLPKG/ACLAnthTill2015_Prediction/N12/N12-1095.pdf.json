{"title": [], "abstractContent": [{"text": "We propose an unsupervised method for clustering the translations of a word, such that the translations in each cluster share a common semantic sense.", "labels": [], "entities": []}, {"text": "Words are assigned to clusters based on their usage distribution in large monolingual and parallel corpora using the soft K-Means algorithm.", "labels": [], "entities": []}, {"text": "In addition to describing our approach, we formalize the task of translation sense clustering and describe a procedure that leverages WordNet for evaluation.", "labels": [], "entities": [{"text": "translation sense clustering", "start_pos": 65, "end_pos": 93, "type": "TASK", "confidence": 0.9077098766962687}, {"text": "WordNet", "start_pos": 134, "end_pos": 141, "type": "DATASET", "confidence": 0.9312382340431213}]}, {"text": "By comparing our induced clusters to reference clusters generated from WordNet, we demonstrate that our method effectively identifies sense-based translation clusters and benefits from both monolingual and parallel corpora.", "labels": [], "entities": []}, {"text": "Finally, we describe a method for annotating clusters with usage examples.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ability to learn a bilingual lexicon from a parallel corpus was an early and influential area of success for statistical modeling techniques in natural language processing.", "labels": [], "entities": []}, {"text": "Probabilistic word alignment models can induce bilexical distributions over target-language translations of source-language words (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7338982075452805}]}, {"text": "However, word-to-word correspondences do not capture the full structure of a bilingual lexicon.", "labels": [], "entities": []}, {"text": "Consider the example bilingual dictionary entry in; in addition to enumerating the translations of a word, the dictionary author has grouped those translations into three sense clusters.", "labels": [], "entities": []}, {"text": "Inducing such a clustering would prove useful in generating bilingual dictionaries automatically or building tools to assist bilingual lexicographers.", "labels": [], "entities": []}, {"text": "This excerpt from a bilingual dictionary groups English translations of the polysemous Spanish word colocar into three clusters that correspond to different word senses (Vel\u00e1zquez de la.", "labels": [], "entities": []}, {"text": "This paper formalizes the task of clustering a set of translations by sense, as might appear in a published bilingual dictionary, and proposes an unsupervised method for inducing such clusters.", "labels": [], "entities": [{"text": "clustering a set of translations", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.8325024724006653}]}, {"text": "We also show how to add usage examples for the translation sense clusters, hence providing complete structure to a bilingual dictionary.", "labels": [], "entities": []}, {"text": "The input to this task is a set of source words and a set of target translations for each source word.", "labels": [], "entities": []}, {"text": "Our proposed method clusters these translations in two steps.", "labels": [], "entities": []}, {"text": "First, we induce a global clustering of the entire target vocabulary using the soft K-Means algorithm, which identifies groups of words that appear in similar contexts (in a monolingual corpus) and are translated in similar ways (in a parallel corpus).", "labels": [], "entities": []}, {"text": "Second, we derive clusters over the translations of each source word by projecting the global clusters.", "labels": [], "entities": []}, {"text": "We evaluate these clusters by comparing them to reference clusters with the overlapping BCubed metric ().", "labels": [], "entities": []}, {"text": "We propose a clustering criterion that allows us to derive reference clusters from the synonym groups of WordNet R.", "labels": [], "entities": [{"text": "WordNet R", "start_pos": 105, "end_pos": 114, "type": "DATASET", "confidence": 0.9272093176841736}]}, {"text": "Our experiments using Spanish-English and Japanese-English datasets demonstrate that the automatically generated clusters produced by our method are substantially more similar to the Sense cluster WordNet sense description Usage example collocate group or chunk together in a certain order or place side by side colocar juntas todas los libros collocate all the books invest, place, put make an investment capitales para colocar capital to invest locate, place assign a location to colocar el n\u00famero de serie locate the serial number place, position, put put into a certain place or abstract location colocar en un lugar put in a place: Correct sense clusters for the translations of Spanish verb s = colocar, assuming that it has translation set Ts = {collocate, invest, locate, place, position, put}.", "labels": [], "entities": []}, {"text": "Only the sense clusters are outputs of the translation sense clustering task; the additional columns are presented for clarity.", "labels": [], "entities": [{"text": "translation sense clustering task", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.8557540774345398}]}, {"text": "WordNet-based reference clusters than naive baselines.", "labels": [], "entities": [{"text": "WordNet-based", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8897395133972168}]}, {"text": "Moreover, we show that bilingual features collected from parallel corpora improve clustering accuracy over monolingual distributional similarity features alone.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9468294978141785}]}, {"text": "Finally, we present a method for annotating clusters with usage examples, which enrich our automatically generated bilingual dictionary entries.", "labels": [], "entities": []}], "datasetContent": [{"text": "The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method can effectively recover the reference sense clusters derived from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 166, "end_pos": 173, "type": "DATASET", "confidence": 0.9515902996063232}]}, {"text": "We conduct experiments using two bilingual datasets: Spanish-to-English (S\u2192E) and Japaneseto-English (J\u2192E).: Sizes of the Spanish-to-English (S\u2192E) and Japaneseto-English (J\u2192E) datasets. are limited in size because we solicited human annotators to filter the set of translations for each source word.", "labels": [], "entities": []}, {"text": "The S\u2192E dataset has 52 source-words with a part-of-speech-tag distribution of 38 nouns, 10 verbs and 4 adverbs.", "labels": [], "entities": []}, {"text": "The J\u2192E dataset has 369 sourcewords with 319 nouns, 38 verbs and 12 adverbs.", "labels": [], "entities": [{"text": "J\u2192E dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.545996829867363}]}, {"text": "We included only these parts of speech because WordNet version 2.1 has adequate coverage for them.", "labels": [], "entities": [{"text": "WordNet version 2.1", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.9011112252871195}]}, {"text": "Most source words have 3 to 5 translations each.", "labels": [], "entities": []}, {"text": "Monolingual features for K-Means clustering were computed from an English corpus of Web documents with 700 billion tokens of text.", "labels": [], "entities": [{"text": "K-Means clustering", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7619287967681885}]}, {"text": "Bilingual features were computed from 0.78 (S\u2192E) and 1.04 (J\u2192E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (.", "labels": [], "entities": []}, {"text": "Word alignments were induced from the HMMbased alignment model (, initialized with the bilexical parameters of IBM Model 1 ().", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6700700521469116}]}, {"text": "Both models were trained using 2 iterations of the expectation maximization algorithm.", "labels": [], "entities": []}, {"text": "Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation ().", "labels": [], "entities": [{"text": "phrasebased machine translation", "start_pos": 83, "end_pos": 114, "type": "TASK", "confidence": 0.6826334198315939}]}, {"text": "The quality of text clustering algorithms can be evaluated using a wide set of metrics.", "labels": [], "entities": [{"text": "text clustering", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.7485952377319336}]}, {"text": "For evaluation by set matching, the popular measures are Purity () and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen).", "labels": [], "entities": [{"text": "Purity", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9920163154602051}, {"text": "Inverse Purity", "start_pos": 71, "end_pos": 85, "type": "METRIC", "confidence": 0.9282810688018799}, {"text": "F measure", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9672702848911285}]}, {"text": "For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (.", "labels": [], "entities": []}, {"text": "Metrics based on entropy include Cluster Entropy (), Class Entropy (), VI-measure, Q 0 (Dom, 2001), V-measure ( and Mutual Information (.", "labels": [], "entities": []}, {"text": "Lastly, there exist the BCubed metrics (, a family of metrics that decompose the clus-tering evaluation by estimating precision and recall for each item in the distribution.", "labels": [], "entities": [{"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.990412175655365}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9973400235176086}]}, {"text": "compares the various clustering metrics mentioned above and their properties.", "labels": [], "entities": []}, {"text": "They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families.", "labels": [], "entities": []}, {"text": "Their analysis shows that of the wide range of metrics, only BCubed satisfies those constraints.", "labels": [], "entities": []}, {"text": "After defining each constraint below, we briefly describe its relevance to the translation sense clustering task.", "labels": [], "entities": [{"text": "translation sense clustering task", "start_pos": 79, "end_pos": 112, "type": "TASK", "confidence": 0.9102203100919724}]}, {"text": "Homogeneity: Ina cluster, we should not mix items belonging to different categories.", "labels": [], "entities": []}, {"text": "Relevance: All words in a proposed cluster should share some common WordNet sense.", "labels": [], "entities": [{"text": "Relevance", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.938248872756958}]}, {"text": "Completeness: Items belonging to the same category should be grouped in the same cluster.", "labels": [], "entities": []}, {"text": "Relevance: All words that share some common WordNet sense should appear in the same cluster.", "labels": [], "entities": [{"text": "Relevance", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9725518822669983}]}, {"text": "Rag Bag: Introducing disorder into a disordered cluster is less harmful than introducing disorder into a clean cluster.", "labels": [], "entities": [{"text": "Rag Bag", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.7577079236507416}]}, {"text": "Relevance: We prefer to maximize the number of error-free clusters, because these are most easily interpreted and therefore most useful.", "labels": [], "entities": []}, {"text": "Cluster Size vs. Quantity: A small error in a big cluster is preferable to a large number of small errors in small clusters.", "labels": [], "entities": []}, {"text": "Relevance: We prefer to minimize the total number of erroneous clusters in a dictionary.", "labels": [], "entities": []}, {"text": "also show that BCubed extends cleanly to settings with overlapping clusters, where an element can simultaneously belong to more than one cluster.", "labels": [], "entities": []}, {"text": "For these reasons, we focus on BCubed for cluster similarity evaluation.", "labels": [], "entities": [{"text": "BCubed", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.7732449173927307}, {"text": "cluster similarity evaluation", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.6682319442431132}]}, {"text": "The BCubed metric for scoring overlapping clusters is computed from the pair-wise precision and recall between pairs of items: where e and e are two items, L(e) is the set of reference clusters fore and C(e) is the set of predicted clusters fore (i.e., clusters to which e belongs).", "labels": [], "entities": [{"text": "BCubed", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.7394945025444031}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9648073315620422}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.998761773109436}]}, {"text": "Note that P(e, e ) is defined only when e and e share some predicted cluster, and R(e, e ) when e and e share some reference cluster.", "labels": [], "entities": [{"text": "R", "start_pos": 82, "end_pos": 83, "type": "METRIC", "confidence": 0.9763761758804321}]}, {"text": "The BCubed precision associated to one item is its averaged pair-wise precision over other items sharing some of its predicted clusters, and likewise for recall ; and the overall BCubed precision (or recall) is the averaged precision (or recall) of all items: 6.3 Results shows the F \u03b2 -score for various \u03b2 values: This graph gives us a trade-off between precision and recall (\u03b2 = 0 is exact precision and \u03b2 \u2192 \u221e tends to exact recall).", "labels": [], "entities": [{"text": "BCubed precision", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.8369350135326385}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9974466562271118}, {"text": "BCubed precision", "start_pos": 179, "end_pos": 195, "type": "METRIC", "confidence": 0.7801509499549866}, {"text": "recall)", "start_pos": 200, "end_pos": 207, "type": "METRIC", "confidence": 0.9179118573665619}, {"text": "recall", "start_pos": 238, "end_pos": 244, "type": "METRIC", "confidence": 0.8766031265258789}, {"text": "F \u03b2 -score", "start_pos": 282, "end_pos": 292, "type": "METRIC", "confidence": 0.9779994338750839}, {"text": "precision", "start_pos": 355, "end_pos": 364, "type": "METRIC", "confidence": 0.9972115159034729}, {"text": "recall", "start_pos": 369, "end_pos": 375, "type": "METRIC", "confidence": 0.9947962164878845}, {"text": "exact precision", "start_pos": 386, "end_pos": 401, "type": "METRIC", "confidence": 0.8813540637493134}, {"text": "exact recall", "start_pos": 421, "end_pos": 433, "type": "METRIC", "confidence": 0.6954689919948578}]}, {"text": "Each curve in represents a particular clustering method.", "labels": [], "entities": []}, {"text": "We include three naive baselines: ewnc: Each word in its own cluster aw1c: All words in one cluster Random: Each target word is assigned M random cluster id's in the range 1 to K, then translation sets are clustered with the CP algorithm.", "labels": [], "entities": []}, {"text": "The curves for K-Means clustering include one condition with monolingual features alone and two curves that include bilingual features as well.", "labels": [], "entities": [{"text": "K-Means clustering", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7514156103134155}]}, {"text": "The bilingual curves correspond to two different feature sets: the first includes only unigram features (t1), while the second includes both unigram and bigram features (t1t2).", "labels": [], "entities": []}, {"text": "Each point on an F \u03b2 curve in (including the baseline curves) represents a maximum over two   parameters: K, the number of clusters created in the whole corpus and M , the number of clusters allowed per word (in M -best soft K-Means).", "labels": [], "entities": []}, {"text": "As both the random baseline and proposed clustering methods can be tuned to favor precision or recall, we show the best result from each technique across this spectrum of F \u03b2 metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9991123080253601}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9972477555274963}]}, {"text": "We vary \u03b2 to highlight different potential objectives of translation sense clustering.", "labels": [], "entities": [{"text": "translation sense clustering", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.8373237053553263}]}, {"text": "An application that focuses on synonym discovery would favor recall, while an application portraying highly granular sense distinctions would favor precision.", "labels": [], "entities": [{"text": "synonym discovery", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.9599489271640778}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9937710165977478}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9965724945068359}]}, {"text": "Clustering accuracy improves over the baselines with monolingual features alone, and it improves further with the addition of bilingual features, fora wide range of \u03b2 values.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9838516116142273}]}, {"text": "Our unsupervised approach with bilingual features achieves up to 6-8% absolute improvement over the random baseline, and is particularly effective for recall-weighted metrics.", "labels": [], "entities": [{"text": "recall-weighted", "start_pos": 151, "end_pos": 166, "type": "METRIC", "confidence": 0.991801917552948}]}, {"text": "As an example, in a S\u2192E experiment with a K-Means setting of K = 4096 : M = 3, the overall F 1.5 score increases from 80.58% to 86.12% upon adding bilingual features.", "labels": [], "entities": [{"text": "F 1.5 score", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.9874585866928101}]}, {"text": "shows two examples from that experiment for which bilingual features improve the output clusters.", "labels": [], "entities": []}, {"text": "The parameter values we use in our experiments are K \u2208 {2 3 , 2 4 , . .", "labels": [], "entities": []}, {"text": ", 2 12 } and M \u2208 {1, 2, 3, 4, 5}.", "labels": [], "entities": []}, {"text": "To provide additional detail, shows the BCubed precision and recall for each induced clustering, as the values of K and M vary, for JapaneseEnglish.", "labels": [], "entities": [{"text": "BCubed", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9297895431518555}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.6592835187911987}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9995703101158142}, {"text": "JapaneseEnglish", "start_pos": 132, "end_pos": 147, "type": "DATASET", "confidence": 0.9552112817764282}]}, {"text": "Each point in this scatter plot represents a clustering methodology and a particular value for K and M . Soft K-Means with bilingual features provides the strongest performance across abroad range of cluster parameters.", "labels": [], "entities": []}, {"text": "Certain special cases needed to be addressed in order to complete this evaluation.", "labels": [], "entities": []}, {"text": "Target words not in WordNet: Words that did not have any synset in WordNet were each assigned to a singleton reference cluster.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.9744632244110107}, {"text": "WordNet", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9737987518310547}]}, {"text": "The S\u2192E dataset has only 2 out of 225 target types missing in WordNet and the J\u2192E dataset has only 55 out of 1351 target types missing.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9824748635292053}, {"text": "J\u2192E dataset", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.7164526730775833}]}, {"text": "Target words not clustered by K-Means: The KMeans algorithm applies various thresholds during different parts of the process.", "labels": [], "entities": []}, {"text": "As a result, there are some target word types that are not assigned any cluster at the end of the algorithm.", "labels": [], "entities": []}, {"text": "For example, in the J\u2192E experiment with K = 4096 and with bilingual (t1 only) features, only 49 out of 1351 target-types are not assigned any cluster by K-Means.", "labels": [], "entities": []}, {"text": "These unclustered words were each assigned to a singleton cluster in post-processing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples showing improvements in clustering when we move from K-Means clustering with only monolingual features  to clustering with additional bilingual features.", "labels": [], "entities": []}]}