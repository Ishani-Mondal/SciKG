{"title": [{"text": "Structured Ramp Loss Minimization for Machine Translation", "labels": [], "entities": [{"text": "Structured Ramp Loss Minimization", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7775448113679886}, {"text": "Machine Translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7874139547348022}]}], "abstractContent": [{"text": "This paper seeks to close the gap between training algorithms used in statistical machine translation and machine learning, specifically the framework of empirical risk minimization.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6984132826328278}, {"text": "empirical risk minimization", "start_pos": 154, "end_pos": 181, "type": "TASK", "confidence": 0.6523441473642985}]}, {"text": "We review well-known algorithms, arguing that they do not optimize the loss functions they are assumed to optimize when applied to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7138034105300903}]}, {"text": "Instead, most have implicit connections to particular forms of ramp loss.", "labels": [], "entities": [{"text": "ramp loss", "start_pos": 63, "end_pos": 72, "type": "TASK", "confidence": 0.7591286897659302}]}, {"text": "We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others.", "labels": [], "entities": []}, {"text": "Most notably, our structured ramp loss minimization algorithm, RAMPION, is less sensitive to initialization and random seeds than standard approaches.", "labels": [], "entities": [{"text": "RAMPION", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9022904634475708}, {"text": "initialization", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.9616107940673828}]}], "introductionContent": [{"text": "Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text.", "labels": [], "entities": [{"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9282912611961365}]}, {"text": "Well-known examples include MERT, MIRA (, and PRO ().", "labels": [], "entities": [{"text": "MERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9272620677947998}, {"text": "MIRA", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9553267955780029}, {"text": "PRO", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.4474901854991913}]}, {"text": "While such procedures can be analyzed as machine learning algorithms-e.g., in the general framework of empirical risk minimization-their procedural specifications have made this difficult.", "labels": [], "entities": [{"text": "empirical risk minimization-their", "start_pos": 103, "end_pos": 136, "type": "TASK", "confidence": 0.7116173307100931}]}, {"text": "From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters.", "labels": [], "entities": []}, {"text": "In this paper, we consider training algorithms that are first specified declaratively, as loss functions to be minimized.", "labels": [], "entities": []}, {"text": "We relate well-known training algorithms for MT to particular loss functions.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9959954023361206}]}, {"text": "We show that a family of structured ramp loss functions () is useful for this analysis.", "labels": [], "entities": []}, {"text": "For example, recently suggested that, while) described their algorithm as \"MIRA\"), in fact it targets a kind of ramp loss.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9037320613861084}]}, {"text": "We note here other examples: described their algorithm as a variant of the perceptron), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by. sought to optimize log loss (likelihood in a probabilistic model;) but actually optimized aversion of the soft ramp loss.", "labels": [], "entities": []}, {"text": "Why isn't the application of ML to MT more straightforward?", "labels": [], "entities": [{"text": "ML", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.8455202579498291}, {"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.7536541819572449}]}, {"text": "We note two key reasons: (i) ML generally assumes that the correct output can always be scored by a model, but in MT the reference translation is often unreachable, due to a model's limited expressive power or search error, requiring the use of \"surrogate\" references; (ii) MT models nearly always include latent derivation variables, leading to non-convex losses that have generally received little attention in ML.", "labels": [], "entities": [{"text": "MT", "start_pos": 114, "end_pos": 116, "type": "TASK", "confidence": 0.9241935014724731}, {"text": "ML", "start_pos": 413, "end_pos": 415, "type": "TASK", "confidence": 0.9362676739692688}]}, {"text": "In this paper, we discuss how these two have caused a disconnect between the loss function minimized by an algorithm in ML and the loss minimized when it is adapted for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 169, "end_pos": 171, "type": "TASK", "confidence": 0.9050072431564331}]}, {"text": "From a practical perspective, our framework leads to a simple training algorithm for structured ramp loss based on general optimization techniques.", "labels": [], "entities": []}, {"text": "Our algorithm is simple to implement and, being a batch algorithm like MERT and PRO, can easily be inte-grated with any decoder.", "labels": [], "entities": [{"text": "MERT", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.5504781603813171}]}, {"text": "Our experiments show that our algorithm, which we call RAMPION, performs comparably to MERT and PRO, is less sensitive to randomization and initialization conditions, and is robust in large-feature scenarios.", "labels": [], "entities": [{"text": "RAMPION", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.7656659483909607}, {"text": "MERT", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.932361364364624}, {"text": "PRO", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.561806321144104}]}], "datasetContent": [{"text": "The goal of our experiments is to compare RAM-PION (Alg.", "labels": [], "entities": [{"text": "RAM-PION", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.8324697017669678}]}, {"text": "1) to state-of-the-art methods for training MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9517179131507874}]}, {"text": "RAMPION minimizes loss ramp 3 , which we found in preliminary experiments to work better than other loss functions tested.", "labels": [], "entities": [{"text": "RAMPION", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7490953803062439}, {"text": "loss ramp 3", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.8876739740371704}]}, {"text": "System and Datasets We use the Moses phrasebased MT system ( ) and consider Urdu\u2192English (UR\u2192EN), Chinese\u2192English (ZH\u2192EN) translation, and Arabic\u2192English (AR\u2192EN) translation.", "labels": [], "entities": []}, {"text": "We trained a Moses system using default settings and features, except for setting the distortion limit to 10.", "labels": [], "entities": [{"text": "distortion limit", "start_pos": 86, "end_pos": 102, "type": "METRIC", "confidence": 0.9687888622283936}]}, {"text": "Word alignment was performed using GIZA++ in both directions, the grow-diag-final-and heuristic was used to symmetrize the alignments, and a max phrase length of 7 was used for phrase extraction.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6862250715494156}, {"text": "phrase extraction", "start_pos": 177, "end_pos": 194, "type": "TASK", "confidence": 0.8179740607738495}]}, {"text": "We estimated 5-gram language models using the SRI toolkit) with modified Kneser-Ney smoothing).", "labels": [], "entities": [{"text": "SRI toolkit", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.8600217998027802}]}, {"text": "For each language pair, we used the English side of the parallel text and 600M words of randomlyselected sentences from the Gigaword v4 corpus (excluding NYT and LAT).", "labels": [], "entities": [{"text": "Gigaword v4 corpus", "start_pos": 124, "end_pos": 142, "type": "DATASET", "confidence": 0.8636954625447592}, {"text": "NYT and LAT", "start_pos": 154, "end_pos": 165, "type": "DATASET", "confidence": 0.7245055039723715}]}, {"text": "For UR\u2192EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words.", "labels": [], "entities": [{"text": "NIST MT08 evaluation", "start_pos": 42, "end_pos": 62, "type": "DATASET", "confidence": 0.9024184544881185}]}, {"text": "We used half of the documents (882 sentences) from the MT08 test set for tuning.", "labels": [], "entities": [{"text": "MT08 test set", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9687219858169556}]}, {"text": "We used the remaining half for one test set (\"MT08 * \") and MT09 as our other test set.", "labels": [], "entities": [{"text": "MT08", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.850573718547821}, {"text": "MT09", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.937124490737915}]}, {"text": "For ZH\u2192EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14).", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.9703060984611511}]}, {"text": "We segmented the Chinese data using the Stanford Chinese segmenter) in \"CTB\" mode, giving us 7.9M Chinese words and 9.4M English words.", "labels": [], "entities": []}, {"text": "We used MT03 for tuning and used MT02 and MT05 for testing.", "labels": [], "entities": [{"text": "MT03", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.9503703117370605}, {"text": "MT02", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.9609555006027222}, {"text": "MT05", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9421057105064392}]}, {"text": "For AR\u2192EN, we used data provided by the LDC for the NIST evaluations, including 3.29M sentence pairs of UN data and 982k sentence pairs of non-UN data.", "labels": [], "entities": [{"text": "NIST", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.8985766172409058}]}, {"text": "The Arabic data was preprocessed using an HMM segmenter that splits off attached prepositional phrases, personal pronouns, and the future marker ().", "labels": [], "entities": []}, {"text": "The common stylistic sentence-initial wa# (and ...) was removed from the training and test data.", "labels": [], "entities": []}, {"text": "The resulting corpus contained 130M Arabic tokens and 130M English tokens.", "labels": [], "entities": []}, {"text": "We used MT06 for tuning and three test sets: MT05, the MT08 newswire test set (\"MT08 NW\"), and the MT08 weblog test set (\"MT08 WB\").", "labels": [], "entities": [{"text": "MT06", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.9647164344787598}, {"text": "MT05", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.948259711265564}, {"text": "MT08 newswire test set", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.9448406845331192}, {"text": "MT08 NW", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.7106482684612274}, {"text": "MT08 weblog test set", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.9277400821447372}, {"text": "MT08 WB", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.7545455098152161}]}, {"text": "For all languages we evaluated translation output using case-insensitive IBM BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.8476731181144714}]}, {"text": "Training Algorithms Our baselines are MERT and PRO as implemented in the Moses toolkit.", "labels": [], "entities": [{"text": "MERT", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9792431592941284}, {"text": "PRO", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9192571043968201}]}, {"text": "10 PRO uses the hyperparameter settings from, including k-best lists of size 1500 and 25 training iterations.", "labels": [], "entities": [{"text": "PRO", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.48892372846603394}]}, {"text": "11 MERT uses k-best lists of size 100 and was run to convergence.", "labels": [], "entities": [{"text": "11", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7678876519203186}, {"text": "MERT", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.34880995750427246}]}, {"text": "For both MERT and PRO, previous iterations' k-best lists were merged in.", "labels": [], "entities": [{"text": "MERT", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.5897873640060425}, {"text": "PRO", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.6743220686912537}]}, {"text": "For RAMPION, we used T = 20, T = 10, T = 5, k = 500, \u03b7 = 0.0001, and C = 1.", "labels": [], "entities": [{"text": "RAMPION", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.6820856332778931}, {"text": "T", "start_pos": 21, "end_pos": 22, "type": "METRIC", "confidence": 0.9771044850349426}, {"text": "T", "start_pos": 29, "end_pos": 30, "type": "METRIC", "confidence": 0.9591127038002014}, {"text": "T", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.974721372127533}]}, {"text": "Our cost function is \u03b1(1 \u2212 BLEU +1 (y, y )) where BLEU +1 (y, y ) returns the BLEU +1 score () for reference y and hypothesis y . We used \u03b1 = 10.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9970360994338989}, {"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.99782395362854}, {"text": "BLEU +1 score", "start_pos": 78, "end_pos": 91, "type": "METRIC", "confidence": 0.9677308946847916}]}, {"text": "We used these same hyperparameter values for all experiments reported here and found them to perform well across other language pairs and systems.", "labels": [], "entities": []}, {"text": "MERT and PRO were run 3 times with differing random seeds and averages The PRO algorithm samples pairs of translations from kbest lists on each iteration and trains a binary classifier to rank pairs according to the cost function.", "labels": [], "entities": [{"text": "MERT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6930623650550842}]}, {"text": "The loss function underlying PRO depends on the choice of binary classifier and also on the sampling strategy.", "labels": [], "entities": [{"text": "PRO", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8137734532356262}]}, {"text": "We leave an analysis of PRO's loss function to future work.", "labels": [], "entities": [{"text": "PRO", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9470421671867371}]}, {"text": "Hopkins and May used 30 iterations, but showed that training had converged by 25.", "labels": [], "entities": []}, {"text": "We found performance to be better when using a smaller value of T ; we suspect that using small T guards against overfitting to any particular set of k-best lists.", "labels": [], "entities": []}, {"text": "We also found the value of \u03b1 to affect performance, although \u03b1 \u2208 {1, 5, 10} all worked well.", "labels": [], "entities": []}, {"text": "Performance was generally insensitive to C.", "labels": [], "entities": [{"text": "C", "start_pos": 41, "end_pos": 42, "type": "TASK", "confidence": 0.9341036081314087}]}, {"text": "We fixed \u03b7 = 0.0001 early on and did little tuning to it. and standard deviations are shown.", "labels": [], "entities": []}, {"text": "The three algorithms perform very similarly on the whole, with certain algorithms performing better on certain languages.", "labels": [], "entities": []}, {"text": "MERT shows larger variation across random seeds, as reported by many others in the community.", "labels": [], "entities": [{"text": "MERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5429032444953918}]}, {"text": "On average across all language pairs and test sets, RAMPION leads to slightly higher BLEU scores.", "labels": [], "entities": [{"text": "RAMPION", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9825336337089539}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9992270469665527}]}], "tableCaptions": [{"text": " Table 1: %BLEU on several test sets for UR\u2192EN, ZH\u2192EN, and AR\u2192EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9957833290100098}, {"text": "BLEU", "start_pos": 378, "end_pos": 382, "type": "METRIC", "confidence": 0.9975881576538086}, {"text": "RAMPION", "start_pos": 473, "end_pos": 480, "type": "METRIC", "confidence": 0.6741942763328552}]}, {"text": " Table 2: %BLEU with large feature sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.998195469379425}]}]}