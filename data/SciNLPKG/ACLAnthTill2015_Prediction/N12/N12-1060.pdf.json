{"title": [{"text": "Improved Reordering for Shallow-n Grammar based Hierarchical Phrase-based Translation", "labels": [], "entities": [{"text": "Hierarchical Phrase-based Translation", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.6184039811293284}]}], "abstractContent": [{"text": "Shallow-n grammars (de Gispert et al., 2010) were introduced to reduce over-generation in the Hiero translation model (Chiang, 2005) resulting in much faster decoding and restricting reordering to a desired level for specific language pairs.", "labels": [], "entities": [{"text": "Hiero translation", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.667742520570755}]}, {"text": "However, Shallow-n grammars require parameters which cannot be directly optimized using minimum error-rate tuning by the decoder.", "labels": [], "entities": []}, {"text": "This paper introduces some novel improvements to the translation model for Shallow-n grammars.", "labels": [], "entities": [{"text": "translation", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.9652148485183716}]}, {"text": "We introduce two rules: a BITG-style reordering glue rule and a simpler monotonic concatenation rule.", "labels": [], "entities": [{"text": "BITG-style", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.8487745523452759}]}, {"text": "We use separate features for the new rules in our log-linear model allowing the decoder to directly optimize the feature weights.", "labels": [], "entities": []}, {"text": "We show this formulation of Shallow-n hierarchical phrase-based translation is comparable in translation quality to full Hiero-style decoding (without shallow rules) while at the same time being considerably faster.", "labels": [], "entities": [{"text": "Shallow-n hierarchical phrase-based translation", "start_pos": 28, "end_pos": 75, "type": "TASK", "confidence": 0.5208761543035507}]}], "introductionContent": [{"text": "Hierarchical phrase-based translation extends the highly lexicalized models from phrase-based translation systems in order to model lexicalized reordering and discontiguous phrases.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.7036279588937759}]}, {"text": "However, a major drawback in this approach, when compared to phrase-based systems, is the total number of rules that are learnt are several orders of magnitude larger than standard phrase tables, which leads to over-generation and search errors and contribute to much longer decoding times.", "labels": [], "entities": []}, {"text": "Several approaches have been proposed to address these issues: from filtering the extracted synchronous grammar () to alternative Bayesian approaches for learning minimal grammars ().", "labels": [], "entities": []}, {"text": "The idea of Shallow-n grammars (de) takes an orthogonal direction for controlling the over-generation and search space in Hiero decoder by restricting the degree of nesting allowed for Hierarchical rules.", "labels": [], "entities": [{"text": "Hiero decoder", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.9019254744052887}]}, {"text": "We propose an novel statistical model for Shallow-n grammars which does not require additional non-terminals for monotonic re-ordering and also eliminates hand-tuned parameters and instead introduces an automatically tunable alternative.", "labels": [], "entities": []}, {"text": "We introduce a BITG-style () reordering glue rule ( \u00a7 3) and a monotonic X-glue rule ( \u00a7 4).", "labels": [], "entities": [{"text": "BITG-style", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.8305557370185852}]}, {"text": "Our experiments show the resulting Shallow-n decoding is comparable in translation quality to full Hiero-style decoding while at the same time being considerably faster.", "labels": [], "entities": []}, {"text": "All the experiments in this paper were done using) hierarchical phrasebased system which also supports decoding with Shallow-n grammars.", "labels": [], "entities": []}, {"text": "We extended Kriya to additionally support reordering glue rules as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present results for Chinese-English translation as it often requires heavy reordering.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.5328836739063263}]}, {"text": "We use the HK parallel text and GALE phase-1 corpus consisting of \u223c2.3M sentence pairs for training.", "labels": [], "entities": [{"text": "GALE phase-1 corpus", "start_pos": 32, "end_pos": 51, "type": "DATASET", "confidence": 0.853189488252004}]}, {"text": "For tuning and testing, we use the MTC parts 1 and 3 (1928 sentences) and MTC part 4 (919 sentences) respectively.", "labels": [], "entities": [{"text": "MTC parts 1", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.8779547810554504}, {"text": "MTC part", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.8963539898395538}]}, {"text": "We used the usual pre-processing pipeline and an additional segmentation step for the Chinese side of the bitext using the LDC segmenter 2 . Our log-linear model uses the standard features conditional (p(e|f ) and p(f |e)) and lexical (p l (e|f ) and pl (f |e)) probabilities, phrase (p p ) and word (w p ) penalties, language model and regular glue penalty (m g ) apart from two additional features for R\u2212glue (r g ) and X\u2212glue (x g ).", "labels": [], "entities": []}, {"text": "shows the BLEU scores and decoding time for the MTC test-set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9977060556411743}, {"text": "MTC test-set", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.8630093932151794}]}, {"text": "We provide the IBM BLEU () scores for the Shallown grammars for order: n = 1, 2, 3 and compare it to the full-Hiero baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.87471604347229}, {"text": "Shallown grammars", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.8563978970050812}]}, {"text": "Finally, we experiment with two variants of the S glue rules, i) a restricted version where the glue rules combine only X at level N , (column 'Glue: X N ' in table), ii) more free variant where they are allowed to use any X freely (column 'Glue: X' in table).", "labels": [], "entities": []}, {"text": "As it can be seen, the unrestricted glue rules variant (column 'Glue: X') consistently outperforms the glue rules restricted to the top-level non-terminal X N , achieving a maximum BLEU score of 26.24, which is about 1.4 BLEU points higher than the latter and is also marginally higher than full Hiero.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 181, "end_pos": 191, "type": "METRIC", "confidence": 0.9858401715755463}, {"text": "BLEU", "start_pos": 221, "end_pos": 225, "type": "METRIC", "confidence": 0.9987050294876099}, {"text": "Hiero", "start_pos": 296, "end_pos": 301, "type": "DATASET", "confidence": 0.9223002195358276}]}, {"text": "The decoding speeds for free-Glue and restricted-Glue variants were mostly identical and so we only provide the decoding time for the latter.", "labels": [], "entities": []}, {"text": "Shallow-2 and   shallow-3 free glue variants achieve BLEU scores comparable to full-Hiero and at the same time being 12 \u2212 20% faster.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9998061060905457}]}, {"text": "R-glue (r g ) appears to contribute more than the X-glue (x g ) as can be seen in shallow-2 and shallow-3 cases.", "labels": [], "entities": []}, {"text": "Interestingly, x g is more helpful for the shallow-1 case specifically when the glue rules are restricted.", "labels": [], "entities": []}, {"text": "As the glue rules are restricted, the X-glue rules concatenates other lower-order rules before being folded into the glue rules.", "labels": [], "entities": []}, {"text": "Both r g and x g improve the BLEU scores by 0.58 over the plain shallow case for shallow orders 1 and 2 and performs comparably for shallow-3 case.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9990886449813843}]}, {"text": "We have also conducted experiments for Arabic-English and we notice that X-glue is more effective and that Rglue is helpful for higher shallow orders.: Results for Arabic-English.", "labels": [], "entities": [{"text": "Rglue", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.9308615922927856}]}, {"text": "The decoding time is in secs/word on the Test set.", "labels": [], "entities": [{"text": "Test set", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.9556657373905182}]}], "tableCaptions": [{"text": " Table 1: An example for the level of reordering in Chinese-English translation", "labels": [], "entities": []}, {"text": " Table 3: Results for Arabic-English. The decoding time  is in secs/word on the Test set.", "labels": [], "entities": []}]}