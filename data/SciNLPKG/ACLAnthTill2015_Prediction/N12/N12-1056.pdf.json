{"title": [{"text": "Space Efficiencies in Discourse Modeling via Conditional Random Sampling", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent exploratory efforts in discourse-level language modeling have relied heavily on calculating Pointwise Mutual Information (PMI), which involves significant computation when done overlarge collections.", "labels": [], "entities": [{"text": "discourse-level language modeling", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.6156531870365143}]}, {"text": "Prior work has required aggressive pruning or independence assumptions to compute scores on large collections.", "labels": [], "entities": []}, {"text": "We show the method of Conditional Random Sampling, thus far an underuti-lized technique, to be a space-efficient means of representing the sufficient statistics in discourse that underly recent PMI-based work.", "labels": [], "entities": [{"text": "Conditional Random Sampling", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.753964920838674}]}, {"text": "This is demonstrated in the context of inducing Shankian script-like structures over news articles.", "labels": [], "entities": []}], "introductionContent": [{"text": "It has become common to model the distributional affinity between some word or phrase pair, (w i , w j ), as a function of co-occurance within some context boundary.", "labels": [], "entities": []}, {"text": "suggested pointwise mutual information: PMI(w i , w j ) = log Pr(w i ,w j ) Pr(w i ) Pr(w j ) , showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora.", "labels": [], "entities": []}, {"text": "Later work such as by continued this tradition.", "labels": [], "entities": []}, {"text": "Here we consider document, or discourse-level contexts, such as explored by or, and more recently by those such as Chambers and Jurafsky or Van Durme and Lall.", "labels": [], "entities": []}, {"text": "In the spirit of recent work in randomized algorithms for large-scale HLT (such as by,,,,Van Durme and Lall (2009a),,,, Van Durme and Lall, or), we propose the method of Conditional Random Sampling (CRS) by as an efficient way to store approximations of the statistics used to calculate PMI for applications in inducing rudimentary script-like structures.", "labels": [], "entities": [{"text": "Conditional Random Sampling (CRS)", "start_pos": 170, "end_pos": 203, "type": "TASK", "confidence": 0.77266925573349}, {"text": "PMI", "start_pos": 287, "end_pos": 290, "type": "TASK", "confidence": 0.6759713292121887}]}, {"text": "Efficiently storing such structures is an important step in integrating document-level statistics into downstream tasks, such as characterizing complex scenarios (Chambers and Jurafsky, 2011), or story understanding).", "labels": [], "entities": [{"text": "characterizing complex scenarios", "start_pos": 129, "end_pos": 161, "type": "TASK", "confidence": 0.8763646880785624}, {"text": "story understanding", "start_pos": 196, "end_pos": 215, "type": "TASK", "confidence": 0.8407211601734161}]}], "datasetContent": [{"text": "Setup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger () and the Morpha lemmatizer ().", "labels": [], "entities": [{"text": "New York Times section of the Gigaword Corpus", "start_pos": 93, "end_pos": 138, "type": "DATASET", "confidence": 0.8300008103251457}]}, {"text": "After filtering various POS tagger errors and setting a minimum document frequency (df) of 50, we went from a vocabulary of 94,803 words to 8,051.", "labels": [], "entities": [{"text": "minimum document frequency (df)", "start_pos": 56, "end_pos": 87, "type": "METRIC", "confidence": 0.7699544727802277}]}, {"text": "For various values of k we built sketches over 1,655,193 documents, for each resulting word type.", "labels": [], "entities": []}, {"text": "We use a generalized definition of PMI for three or more items as the logarithm of the joint probability divided by the product of the marginals.", "labels": [], "entities": [{"text": "PMI", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.7880699038505554}]}, {"text": "Subjective Quality We first consider the lemmatized version of the motivating example by: [plead, admit, convict], breaking it into 1-, 2-, and 3-element seeds.", "labels": [], "entities": []}, {"text": "They reported the top 6 elements that maximize average pairwise PMI as: sentence, parole, fire, indict, fine, deny.", "labels": [], "entities": [{"text": "PMI", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.7359761595726013}]}, {"text": "We see similar results in, while noting again the distinction in underlying statistics: we did not restrict ourselves to cooccurrence based on shared coreferring arguments.", "labels": [], "entities": []}, {"text": "These results show intuitive discourse-level relationships with a sketch size as small ask = 100 for the unary seed.", "labels": [], "entities": []}, {"text": "In addition, when examining the true PMI rank of each of these terms (reflected as subscripts), we see that highly ranked items in the approximate lists come from the set of items highly ranked in the non-approximate version.", "labels": [], "entities": []}, {"text": "A major benefit of the approach is that it allows for approximate scoring of larger sets of elements jointly, without the traditionally assumed storage penalty.", "labels": [], "entities": []}, {"text": "Accuracy 1 We measured the trade-off between PMI approximation accuracy and sketch size.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9954934120178223}, {"text": "PMI approximation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7066659033298492}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.704328179359436}]}, {"text": "Triples of verb tokens were sampled at random from the narrative cloze test set of.", "labels": [], "entities": []}, {"text": "Seed terms were limited to verbs with df between 1,000 and 100,000 to extract lists of the top-25 candidate verbs by joint, approximate PMI.", "labels": [], "entities": [{"text": "PMI", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.9909819960594177}]}, {"text": "For a given rank r, we measured the overlap of the true top-3 PMI and the approximate list, rank r or higher (see).", "labels": [], "entities": [{"text": "overlap", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9854192733764648}]}, {"text": "If query size is 2, k = 10, 000, the true top-3 true PMI items tend to rank well in the approximate PMI list.", "labels": [], "entities": []}, {"text": "We observe that these randomly assembled queries tax the sketch-based approximation, motivating the next experiment on non-uniformly sampled queries.", "labels": [], "entities": []}, {"text": "Accuracy 2 Ina more realistic scenario, we might have more discretion in selecting terms of interest.", "labels": [], "entities": []}, {"text": "Here we chose the first word of each seed uniformly at random from each document, and selected subsequent seed words to maximize the true PMI with the established words in the seed.", "labels": [], "entities": [{"text": "PMI", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.48582911491394043}]}, {"text": "We constrained the seed terms to have df between 1,000 and 100,000.", "labels": [], "entities": []}, {"text": "Then, for each seed of length 1, 2, and 3 words, we found the 25-best list of terms using approximate PMI, considering only terms that occur in more than 50 documents.", "labels": [], "entities": [{"text": "PMI", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.5502490401268005}]}, {"text": "shows the results of this PMI approximation tradeoff.", "labels": [], "entities": []}, {"text": "With a sketch size of 10,000, a rank of 5 is enough to contain two out of the top three items, and the number gradually continues to grow as rank size increases.", "labels": [], "entities": []}, {"text": "Memory Analysis Accuracy in a CRS is a function of the aggressiveness in space savings: ask approaches the true length of the posting list for w i , the resulting approximations are closer to truth, at the cost of increased storage.", "labels": [], "entities": [{"text": "Memory Analysis", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6209372282028198}, {"text": "Accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.8592219352722168}]}, {"text": "When k = \u221e, CRS is the same as using an inverted index: shows the percent memory required for our data, compared to a standard index, as the sketch size increases.", "labels": [], "entities": [{"text": "CRS", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.975663423538208}]}, {"text": "For our data, a full index involves storing 95 million document numbers.", "labels": [], "entities": []}, {"text": "For the k = 10, 000 results, we see that 23% of a full index was needed.", "labels": [], "entities": []}, {"text": "top 3 true PMI items to appear in the top 10.", "labels": [], "entities": [{"text": "PMI", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.8942988514900208}]}, {"text": "Over 40% memory is needed fora 4-word query.", "labels": [], "entities": [{"text": "memory", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9879735708236694}]}, {"text": "2.5 of the top 3 true PMI items appear in the top 50 when the memory is about 35%.", "labels": [], "entities": [{"text": "PMI", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9372627139091492}, {"text": "memory", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9720783829689026}]}, {"text": "This suggests that CRS allows us to use a fraction of the memory of storing a full inverted index, but that memory requirements grow with query size.", "labels": [], "entities": [{"text": "CRS", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.890247106552124}]}, {"text": "Discussion Storing exact PMIs of three or four words would be expensive to store in memory for any moderately sized vocabulary, because it would involve storing on the order of V m count statistics.", "labels": [], "entities": [{"text": "Discussion Storing exact PMIs", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7967393100261688}]}, {"text": "If we are approximating this with a CRS, we store sketches of length k or less for every word in the vocabulary, which is O(kV ). and show that the two-word queries start to get good performance when k is near 10,000.", "labels": [], "entities": [{"text": "O", "start_pos": 122, "end_pos": 123, "type": "METRIC", "confidence": 0.9830498099327087}]}, {"text": "This requires 22.7% of the memory of a complete inverted index, or 21.5 million postings.", "labels": [], "entities": [{"text": "memory", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9680432677268982}]}, {"text": "The three and four word queries get good performance near k = 100, 000.", "labels": [], "entities": []}, {"text": "With this sketch size, 60.5 million postings are stored.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top-n by approximate PMI, for varying k. Subscripts denote rank under true PMI, when less than 50.  plead  plead, admit  plead, admit, convict  1  sentence 4  sentence 4  sentence 4  abuse \u2212  sentence 5  owe \u2212  sentence 2  2  commit \u2212  defraud 5  misbrand 2  convict 22  prosecute 15  admitt 11  prosecute 3  3  indict 10  indict 10  defraud 5  owe \u2212  testify 20  engage \u2212  arrest 8  4 prosecute 33  arraign 6  arraign 6  investigate \u2212  indict 10  investigate 28  testify 5  5  abuse \u2212  conspire 11 manslaughter 1 understand \u2212  defraud 7  prey \u2212  acquit 1  6  convict 24  convict 24  bilk 8  defraud 7  convict 22  defraud \u2212  indict 4  k =  100  1,000  10,000  1,000  10,000  1,000  10,000", "labels": [], "entities": [{"text": "PMI", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.8389323353767395}]}]}