{"title": [], "abstractContent": [{"text": "It has long been observed that monolingual text exhibits a tendency toward \"one sense per discourse ,\" and it has been argued that a related \"one translation per discourse\" constraint is operative in bilingual contexts as well.", "labels": [], "entities": []}, {"text": "In this paper , we introduce a novel method using forced decoding to confirm the validity of this constraint , and we demonstrate that it can be exploited in order to improve machine translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.7874648571014404}]}, {"text": "Three ways of incorporating such a preference into a hierarchical phrase-based MT model are proposed, and the approach where all three are combined yields the greatest improvements for both Arabic-English and Chinese-English translation experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.8888893723487854}]}], "introductionContent": [{"text": "In statistical Machine Translation (MT), the state-ofthe-art approach is to translate phrases in the context of a sentence and to re-order those phrases appropriately.", "labels": [], "entities": [{"text": "statistical Machine Translation (MT)", "start_pos": 3, "end_pos": 39, "type": "TASK", "confidence": 0.805202712615331}]}, {"text": "Intuitively, it seems as if it should also be possible to draw on information outside of a single sentence to further improve translation quality.", "labels": [], "entities": []}, {"text": "In this paper, we challenge the conventional approach of translating each sentence independently, and argue that it can indeed also be beneficial to consider document-scale context when translating text.", "labels": [], "entities": []}, {"text": "Motivated by the success of a \"one sense per discourse\" heuristic in Word Sense Disambiguation (WSD), we explore the potential benefit of leveraging a \"one translation per discourse\" heuristic in MT.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.7391330053408941}, {"text": "MT", "start_pos": 196, "end_pos": 198, "type": "TASK", "confidence": 0.979546070098877}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We begin with related work in Section 2.", "labels": [], "entities": []}, {"text": "Next, we provide new confirmation that the hypothesized one-translationper-discourse condition does indeed often hold, based on a novel analysis using forced decoding (Section 3).", "labels": [], "entities": []}, {"text": "We incorporate this idea into a hierarchical MT framework by adding three new documentscale features to the translation model (Section 4).", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9393450617790222}]}, {"text": "We then present experimental results demonstrating solid improvements in translation quality obtained by leveraging these features, both for ArabicEnglish (Ar-En) and Chinese-English (Zh-En) translation (Section 5).", "labels": [], "entities": []}, {"text": "Conclusions and future work are presented in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have evaluated the one-translation-per-discourse feature using the cdec MT system (.", "labels": [], "entities": []}, {"text": "We started by building a baseline system using standard features in cdec: lexical and phrase translation probabilities in both directions, word and arity penalty features, and a 5-gram language model.", "labels": [], "entities": [{"text": "phrase translation probabilities", "start_pos": 86, "end_pos": 118, "type": "TASK", "confidence": 0.7708020309607188}]}, {"text": "We then added each of the three consistency feature variants, along with all two-way and the one three-way combinations of them, thus yielding a total of eight systems for comparison, including the baseline.", "labels": [], "entities": []}, {"text": "For training the Ar-En system, we used the dataset from the DARPA GALE evaluation, which consists of NIST and LDC releases.", "labels": [], "entities": [{"text": "DARPA GALE evaluation", "start_pos": 60, "end_pos": 81, "type": "DATASET", "confidence": 0.7981975475947062}, {"text": "NIST and LDC releases", "start_pos": 101, "end_pos": 122, "type": "DATASET", "confidence": 0.7749492675065994}]}, {"text": "The corpus was filtered to remove sentence pairs with anomalous length ratios and subsampled to yield a training set containing 3.4 million parallel sentence pairs.", "labels": [], "entities": []}, {"text": "The Arabic text was preprocessed to produce two different segmentations (simple punctuation tokenization with orthographic normalization, and LDC's ATBv3 representation (), represented together using cdec's lattice input format.", "labels": [], "entities": []}, {"text": "The Zh-En system was trained on parallel training text consisting of the non-UN portions and non-HK Hansards portions of the NIST training corpora.", "labels": [], "entities": [{"text": "Hansards", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.9419021606445312}, {"text": "NIST training corpora", "start_pos": 125, "end_pos": 146, "type": "DATASET", "confidence": 0.9168895681699117}]}, {"text": "Chinese was automatically segmented by the Stanford segmenter (), and traditional characters were simplified.", "labels": [], "entities": []}, {"text": "After subsampling and filtering, we obtain a training corpus of 1.6 million parallel sentences.", "labels": [], "entities": []}, {"text": "Both training sets were word-aligned with GIZA++ (), using 5 Model 1 and 5 HMM iterations.", "labels": [], "entities": [{"text": "GIZA++", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.722434937953949}]}, {"text": "A SCFG was then extracted from these alignments using a suffix array extractor.", "labels": [], "entities": []}, {"text": "Evaluation was done with multi-reference BLEU () on test sets with four references for each language pair, and MIRA was used for tuning).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9913954138755798}, {"text": "MIRA", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.995837926864624}]}, {"text": "In our experiments, we run the first decoding phase using feature weights that are guessed heuristically based on weights from previously tuned systems.", "labels": [], "entities": []}, {"text": "All feature weights, including the discourse feature, were then tuned together, based on the output of the second decoding phase.", "labels": [], "entities": []}, {"text": "For Ar-En parameter tuning, we used the MT06 newswire dataset, which contains 104 documents and a total of 1,797 sentences.", "labels": [], "entities": [{"text": "Ar-En parameter tuning", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6769144733746847}, {"text": "MT06 newswire dataset", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.9505531191825867}]}, {"text": "For testing, we used the MT08 dataset described above (74 documents, 813 sentences).", "labels": [], "entities": [{"text": "MT08 dataset", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9668939709663391}]}, {"text": "For Zh-En experiments, the MT02 newswire dataset (100 documents, 878 sentences) was used for tuning, and evaluation was done on the MT06 test set (79 documents, 1,664 sentences).", "labels": [], "entities": [{"text": "MT02 newswire dataset", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.9601135651270548}, {"text": "MT06 test set", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.9657405018806458}]}, {"text": "For both language pairs, DF values were computed from the tuning set for both tuning and evaluation experiments.", "labels": [], "entities": [{"text": "DF", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9620655179023743}]}, {"text": "When we used NIST's official metric (BLEU-4) to compare our results to the official NIST evaluation, our baseline system achieved 54.70 for Ar-En and 31.69 for ZhEn.", "labels": [], "entities": [{"text": "NIST", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.942395031452179}, {"text": "BLEU-4", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.8668431043624878}, {"text": "NIST evaluation", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.9227806329727173}, {"text": "Ar-En", "start_pos": 140, "end_pos": 145, "type": "METRIC", "confidence": 0.9948101043701172}]}, {"text": "Based on reported NIST results, our baseline would have ranked 4 thin the Zh-En MT06 evaluation, and would have outperformed all Ar-En MT08 systems.", "labels": [], "entities": [{"text": "NIST", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.8823266625404358}, {"text": "Zh-En MT06 evaluation", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.7149635155995687}]}, {"text": "We used a slightly different IBM-BLEU metric for the rest of our evaluation.", "labels": [], "entities": [{"text": "IBM-BLEU metric", "start_pos": 29, "end_pos": 44, "type": "METRIC", "confidence": 0.6095154583454132}]}, {"text": "In this case, the baseline system achieved 53.07 BLEU points for Ar-En and 30.43 points for Zh-En.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9995125532150269}, {"text": "Ar-En", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9956886172294617}]}, {"text": "Among more recent papers, the best reported results were 56.87 for Ar-En MT08 () and 35.87 for Zh-En MT06 (), although many papers report BLEU scores below 53 points for Arabic) and 32 points for Chinese.", "labels": [], "entities": [{"text": "Ar-En", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.8874231576919556}, {"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9993202686309814}]}, {"text": "The systems that outperformed our baseline applied novel techniques, and used larger language models, as well as many nonstandard features.", "labels": [], "entities": []}, {"text": "We argue that these novelties are complementary to our approach, and therefore do not damage the credibility of our baseline.", "labels": [], "entities": []}, {"text": "Among the single-feature runs, C 3 had the best performance in Ar-En experiments, with 53.84 BLEU points, whereas C 2 yielded the best results for Zh-En with a BLEU score of 30.96.", "labels": [], "entities": [{"text": "Ar-En", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9301579594612122}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.998479425907135}, {"text": "BLEU score", "start_pos": 160, "end_pos": 170, "type": "METRIC", "confidence": 0.9816316664218903}]}, {"text": "In any case, all three variants outperformed the baseline (see).", "labels": [], "entities": []}, {"text": "When multiple features were combined, we generally observed an increase in BLEU, suggesting that our features have usefully different error char-   acteristics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.999219536781311}]}, {"text": "The combination of all three variants, C 123 , yielded the best results, nearly 1.0 BLEU point higher than the baseline for both language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9988763928413391}]}, {"text": "Evaluation results are summarized in.", "labels": [], "entities": []}, {"text": "Given our focus on documents, it is natural to ask what fraction of the documents were helped or harmed by consistency features.", "labels": [], "entities": []}, {"text": "Documentlevel BLEU scores for Arabic-to-English translations show that C 3 outperformed the baseline on a larger number of documents than any other single feature (42/74=57%), compared with 37/74 (50%) for both C 1 and C 2 . C 123 did better by this measure as well, with BLEU increasing for 43 of the documents.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9910842180252075}, {"text": "BLEU", "start_pos": 272, "end_pos": 276, "type": "METRIC", "confidence": 0.9994468092918396}]}, {"text": "There were no documents where the BLEU score was exactly the same, therefore the BLEU score declined for the remaining documents.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9868459105491638}, {"text": "BLEU score", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9885410368442535}]}, {"text": "As indicates, document-level BLEU for the Zh-En experiments shows similar results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9434019923210144}]}, {"text": "We can also look at our results in a more finegrained way, focusing on differences in how each system translated the same source-language phrase.", "labels": [], "entities": []}, {"text": "For this analysis, we defined English phrases e and e \u2032 to be different if edit distance(e, e \u2032 ) >  max(length(e), length(e \u2032 ))/2.", "labels": [], "entities": []}, {"text": "By this way of counting, there are 197 unique (Arabic phrase, document) pairs for which at least one single-feature system produced translations differently from the baseline system.", "labels": [], "entities": []}, {"text": "Together, these cases affect 553 sentences (68%) in 67 of the 74 documents, with as many as 12 differences observed in a single document.", "labels": [], "entities": []}, {"text": "The number of such differences is even higher for Chinese-to-English translation, probably due to lower confidence from the translation model and longer documents.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.6287795752286911}]}, {"text": "shows the number of changes by each system, and the percentage of the test set affected by these changes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation results: BLEU scores with four ref- erences for Ar-En and Zh-En experiments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9989550113677979}, {"text": "ref- erences", "start_pos": 52, "end_pos": 64, "type": "METRIC", "confidence": 0.7803473075230917}, {"text": "Ar-En", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9783669114112854}]}, {"text": " Table 3: Doc-level analysis: Number of documents where  each variant outperforms baseline.", "labels": [], "entities": [{"text": "Doc-level analysis", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7248236835002899}]}, {"text": " Table 4: Effect of applying variants of the consistency  feature (Any=C 1 or C 2 or C 3 ).", "labels": [], "entities": []}]}