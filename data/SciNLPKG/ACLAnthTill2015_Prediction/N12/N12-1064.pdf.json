{"title": [{"text": "How Text Segmentation Algorithms Gain from Topic Models", "labels": [], "entities": [{"text": "Text Segmentation Algorithms", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.8257702787717184}]}], "abstractContent": [{"text": "This paper introduces a general method to incorporate the LDA Topic Model into text seg-mentation algorithms.", "labels": [], "entities": [{"text": "LDA Topic Model", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.7911215821901957}]}, {"text": "We show that semantic information added by Topic Models significantly improves the performance of two word-based algorithms, namely TextTiling and C99.", "labels": [], "entities": []}, {"text": "Additionally, we introduce the new TopicTil-ing algorithm that is designed to take better advantage of topic information.", "labels": [], "entities": []}, {"text": "We show consistent improvements over word-based methods and achieve state-of-the art performance on a standard dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Texts are often structured into segments to ease understanding and readability of texts.", "labels": [], "entities": []}, {"text": "Knowing about sentence boundaries is advantageous for natural language processing (NLP) tasks such as summarization or indexing.", "labels": [], "entities": [{"text": "summarization", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.9793673157691956}]}, {"text": "While many genres such as encyclopedia entries or scientific articles follow rather formal conventions of breaking up a text into meaningful units, there are plenty of electronically available texts without defined segments, e.g. web documents.", "labels": [], "entities": []}, {"text": "Text segmentation is the task of automatically segmenting texts into parts.", "labels": [], "entities": [{"text": "Text segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7399364709854126}]}, {"text": "Viewing a wellwritten text as sequence of subtopics and assuming that subtopics correspond to segments, a segmentation algorithm needs to find changes of subtopics to identify the natural division of an unstructured text.", "labels": [], "entities": []}, {"text": "In this work, we utilize semantic information from Topic Models (TMs) to inform text segmentation algorithms.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7616588473320007}]}, {"text": "For this, we compare two early word-based algorithms with their topic-based variants, and construct our own algorithm called TopicTiling.", "labels": [], "entities": []}, {"text": "We show that using topics estimated by Latent Dirichlet Allocation (LDA) in lieu of words substantially improves earlier segmentation algorithms.", "labels": [], "entities": []}, {"text": "In comparison to TextTiling (TT), neither smoothing nor a blocksize or window size is needed.", "labels": [], "entities": []}, {"text": "TT using TMs and our own algorithm improve on the state-ofthe-art fora standard dataset, while being conceptually simpler and computationally more efficient than other topic-based segmentation algorithms.", "labels": [], "entities": [{"text": "TT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.7732975482940674}]}], "datasetContent": [{"text": "As laid out in Section 3, a LDA Model is estimated on a training dataset and used for inference on the test set.", "labels": [], "entities": []}, {"text": "To ensure that we do no use information from the test set, we perform a 10-fold Cross Validation (CV) for all reported results.", "labels": [], "entities": [{"text": "Cross Validation (CV)", "start_pos": 80, "end_pos": 101, "type": "METRIC", "confidence": 0.9223195075988769}]}, {"text": "To reduce the variance of the shown results, derived by the random nature of sampling and inference, the results for each fold are calculated 30 times using different LDA models.", "labels": [], "entities": []}, {"text": "The LDA model is trained with N=100 topics, 500 sampling iterations and symmetric hyperparameters as recommended by(\u03b1=50/N and \u03b2=0.01), using JGibbsLda.", "labels": [], "entities": [{"text": "JGibbsLda", "start_pos": 142, "end_pos": 151, "type": "DATASET", "confidence": 0.9581165909767151}]}, {"text": "For the annotation of unseen data with topic information, we use LDA inference, sampling 100 iterations.", "labels": [], "entities": []}, {"text": "Inference is executed sentence-wise, since sentences form the minimal unit of our segmentation algorithms and we cannot use document information in the test setting.", "labels": [], "entities": []}, {"text": "The performance of the algorithms is measured using P k and WindowDiff (WD) metrics).", "labels": [], "entities": []}, {"text": "The C99 algorithm is initialized with a 11\u00d711 ranking mask, as recommended in.", "labels": [], "entities": []}, {"text": "TT is configured according to with sequence length w=20 and block size k=6.", "labels": [], "entities": [{"text": "TT", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.5397405624389648}]}], "tableCaptions": [{"text": " Table 1: Results by segment length for TT with  words and topics (TTLDA), C99 with words and topics  (C99LDA) and TopicTiling using all sentences and using  only sentences with more then 5 word tokens (filtered).", "labels": [], "entities": [{"text": "TT with  words and topics", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.7760468244552612}]}, {"text": " Table 2: List of lowest P k values for the Choi data set for  different algorithms in the literature.", "labels": [], "entities": [{"text": "Choi data set", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9471052487691244}]}]}