{"title": [{"text": "A language\u2212independent shallow\u2212parser Compiler", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a rule\u2212based shallow\u2212 parser compiler, which allows to generate a robust shallow\u2212parser for any language, even in the absence of training data, by resorting to a very limited number of rules which aim at identifying constituent boundaries.", "labels": [], "entities": []}, {"text": "We contrast our approach to other approaches used for shallow\u2212parsing (i.e. finite\u2212state and probabilistic methods).", "labels": [], "entities": []}, {"text": "We present an evaluation of our tool for English (Penn Treebank) and for French (newspaper corpus \"LeMonde\") for several tasks (NP\u2212chunking & \"deeper\" parsing) .", "labels": [], "entities": [{"text": "Penn Treebank)", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9797323743502299}, {"text": "newspaper corpus \"LeMonde\")", "start_pos": 81, "end_pos": 108, "type": "DATASET", "confidence": 0.8880507111549377}]}], "introductionContent": [{"text": "Full syntactic parsers of unrestricted text are costly to develop, costly to run and often yield errors, because of lack of robustness of wide\u2212 coverage grammars and problems of attachment.", "labels": [], "entities": []}, {"text": "This has led, as early as 1958, to the development of shallow\u2212parsers, which aim at identifying as quickly and accurately as possible, main constituents (and possibly syntactic functions) in an input, without dealing with the most difficult problems encountered with \"full\u2212parsing\".", "labels": [], "entities": []}, {"text": "Hence, shallow\u2212parsers are very practical tools.", "labels": [], "entities": []}, {"text": "There are two main techniques used to develop shallow\u2212parsers: 1\u2212 Probabilistic techniques (e.g. Magerman 94, Ratnaparkhi 97,) 2\u2212 Finite\u2212state techniques (e.g. Grefenstette 96) Probabilistic techniques require large amounts of syntactically\u2212annotated training data 1 , which makes them very unsuitable for languages for We are leaving aside unsupervised learning techniques here, since to our knowledge they have not proved a successful for developing practical shallow\u2212parsers.", "labels": [], "entities": []}, {"text": "which no such data is available (i.e. most languages except English) and also, they are not domain\u2212independent nor \"style\u2212independent\" (e.g. they do not allow to successfully shallow\u2212 parse speech, if no annotated data is available for that \"style\").", "labels": [], "entities": []}, {"text": "Finally, a shallow\u2212parser developed using these techniques will have to mirror the information contained in the training data.", "labels": [], "entities": []}, {"text": "For instance, if one trains such a tool on data were only non recursive NP chunks are marked 2 , then one will not be able to obtain richer information such as chunks of other categories, embeddings, syntactic functions...", "labels": [], "entities": []}, {"text": "On the other hand, finite\u2212state techniques rely on the development of a large set of rules (often based on regular expressions) to capture all the ways a constituent can expend.", "labels": [], "entities": []}, {"text": "So for example for detecting English NPs, one could write the following rules : NP \u2192 Det adj* noun adj* NP \u2192 Det adj (for noun ellipsis) NP \u2192 ProperNoun etc ....", "labels": [], "entities": []}, {"text": "But this is time consuming and difficult since one needs to foresee all possible rewriting cases, and if some rule is forgotten, or if too many POS errors are left, robustness and/or accuracy will suffer.", "labels": [], "entities": [{"text": "robustness", "start_pos": 165, "end_pos": 175, "type": "METRIC", "confidence": 0.9561532735824585}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9991012811660767}]}, {"text": "Then these regular expressions have to be manipulated i.e. transformed into automata, which will be determinized and minimized (both being costly operations).", "labels": [], "entities": []}, {"text": "And even though determinization and minimization must be done only once (in theory) fora given set of rules, it is still costly to port such tools to anew set of rules (e.g. fora new language, anew domain) or to change some existing rules.", "labels": [], "entities": []}, {"text": "In this paper, we argue that in order to accomplish the same task, it is unnecessary to develop full sets of regular expression : instead of specifying all the ways a constituent can be rewritten, it is sufficient to express how it begins and/or ends.", "labels": [], "entities": []}, {"text": "This allows to achieve similar results but with far fewer rules, and without a need for determinization or minimization because rules which are written that way are de\u2212facto deterministic.", "labels": [], "entities": []}, {"text": "So in a sense, our approach bears some similarities with the constraint\u2212based formalism because we resort to \"local rules\"), but we focus on identifying constituent boundaries (and not syntactic functions), and allow any level of embedding thanks to the use of a stack.", "labels": [], "entities": []}, {"text": "In the first part of this paper, we present our tool: a shallow\u2212parser compiler.", "labels": [], "entities": []}, {"text": "Ina second part, we present output samples as well as several evaluations for French and for English, where the tool has been used to develop both an NP\u2212chunker and a richer shallow\u2212parser.", "labels": [], "entities": []}, {"text": "We also explain why our approach is more tolerant to POS\u2212tagging errors.", "labels": [], "entities": [{"text": "POS\u2212tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.5743333299954733}]}, {"text": "Finally, we discuss some other practical uses which are made of this shallow\u2212parser compiler.", "labels": [], "entities": []}], "datasetContent": [{"text": "When we began our task, we had at our disposal a 1 million word POS tagged and hand\u2212 corrected corpus (Abeill\u00e9 & Cl\u00e9ment 98).", "labels": [], "entities": [{"text": "Abeill\u00e9 & Cl\u00e9ment 98)", "start_pos": 103, "end_pos": 124, "type": "DATASET", "confidence": 0.8400943398475647}]}, {"text": "The corpus was meant to be syntactically annotated for constituency.", "labels": [], "entities": []}, {"text": "To achieve this, precise annotation guidelines for constituency had been written and a portion of the corpus had been hand\u2212annotated (independently of the development of the shallow\u2212parser) to test the guidelines (approx. 25 000 words) . To evaluate the shallow parser, we performed as described at the beginning of section 3 : We parsed the 1 million words.", "labels": [], "entities": []}, {"text": "We set aside 500 sentences (approx. 15 000 words) for quickly tuning our rules.", "labels": [], "entities": []}, {"text": "We also set aside the 25 000 words that had been independently annotated in order to compare the output of the parser to a portion of the final Treebank.", "labels": [], "entities": []}, {"text": "In addition, an annotator hand\u2212corrected the output of the shallow\u2212parser on 1000 new randomly chosen sentences (approx. 30 000 words).", "labels": [], "entities": []}, {"text": "Contrary to the 25 000 words which constituted the beginning of the Treebank, for these 30 000 words verb arguments, PPs and modifiers were not attached.", "labels": [], "entities": []}, {"text": "Finally, we extracted bare non\u2212 recursive NPs from the 25 000 words, in order to evaluate how the parser did on this particular task.", "labels": [], "entities": []}, {"text": "When compared to the hand\u2212corrected parser's output, for opening brackets we obtain a recall of 94.3 % and a precision of 95.2%.", "labels": [], "entities": [{"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9995855689048767}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9995132684707642}]}, {"text": "For closing brackets, we obtain a precision of 92.2 % and a recall of 91.4 %.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.999724805355072}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9997225403785706}]}, {"text": "Moreover, 95.6 % of the correctly placed brackets are labeled correctly, the remaining 4.4% are not strictly speaking labeled incorrectly, since they are labeled INC (i.e. unknown) These unknown constituents, rather then errors, constitute a mechanism of underspecification (the idea being to assign as little wrong information as possible) . When compared to the 25 000 words of the Treebank, For opening brackets, the recall is 92.9% and the precision is 94%.", "labels": [], "entities": [{"text": "INC", "start_pos": 162, "end_pos": 165, "type": "METRIC", "confidence": 0.9810810685157776}, {"text": "recall", "start_pos": 420, "end_pos": 426, "type": "METRIC", "confidence": 0.9997137188911438}, {"text": "precision", "start_pos": 444, "end_pos": 453, "type": "METRIC", "confidence": 0.9995344877243042}]}, {"text": "For closing brackets, the recall is 62,8% and the precision is 65%.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9997995495796204}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.99985671043396}]}, {"text": "These lower results are normal, since the Treebank contains attachments that the parser is not supposed to make.", "labels": [], "entities": []}, {"text": "Finally, on the specific task of identifying non\u2212 recursive NP\u2212chunks, we obtain a recall of 96.6 % and a precision of 95.8 %.", "labels": [], "entities": [{"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9996676445007324}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9994600415229797}]}, {"text": "for opening These underspecified label can be removed at a deeper parsing stage, or one can add a guesser .  To give an idea about the coverage of the parser, sentences are on average 30 words long and comprise 20.6 opening brackets (and thus as many closing brackets).", "labels": [], "entities": []}, {"text": "Errors difficult to correct with access to a limited context involve mainly \"missing\" brackets (e.g. \"comptez vous * ne pas le traiter\" (do you expect not to treat him) appears as single constituent, while there should be 2) , while \"spurious\" brackets can often be eliminated by adding more rules (e.g. for multiple prepositions : \"de chez\").", "labels": [], "entities": []}, {"text": "Most errors for closing brackets are due to clause boundaries(i.e. SUB, COORD and REL).", "labels": [], "entities": [{"text": "COORD", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9019593596458435}, {"text": "REL", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9559464454650879}]}, {"text": "To obtain these results, we had to write only 48 rules.", "labels": [], "entities": []}, {"text": "Concerning speed, as argued in, we found that rule\u2212based systems are not necessarily slow, since the 1 million words are parsed in 3mn 8 seconds.", "labels": [], "entities": [{"text": "speed", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9886200428009033}]}, {"text": "One can compare this to (Ait\u2212Moktar & Chanod 97), who, in order to shallow\u2212parse French resort to 14 networks and parse 150words /sec (Which amounts to approx. 111 minutes for one million words) . It is difficult to compare our result to other results, since most Shallow\u2212parsers pursue different tasks, and use different evaluation metrics.", "labels": [], "entities": []}, {"text": "However to give an idea, standard techniques typically produce an output for one million words in 20 mn and report a precision and a recall ranging from 70% to 95% depending on the language, kind of text and task.", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9996042847633362}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9873138070106506}]}, {"text": "Again, we are not saying that our technique obtains best results, but simply that it is fast and easy to use for unrestricted text for any language.", "labels": [], "entities": []}, {"text": "To give a better idea to the reader, we provide an output of the Shallow\u2212parser for French on.", "labels": [], "entities": []}, {"text": "In order to improve our tool and our rules, a demo is available online on the author's homepage.", "labels": [], "entities": []}], "tableCaptions": []}