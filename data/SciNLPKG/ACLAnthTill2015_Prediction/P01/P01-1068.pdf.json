{"title": [{"text": "Multi-Class Composite N-gram Language Model for Spoken Language Processing Using Multiple Word Clusters", "labels": [], "entities": [{"text": "Spoken Language Processing", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.851023276646932}]}], "abstractContent": [{"text": "In this paper, anew language model, the Multi-Class Composite N-gram, is proposed to avoid a data sparseness problem for spoken language in that it is difficult to collect training data.", "labels": [], "entities": []}, {"text": "The Multi-Class Composite N-gram maintains an accurate word prediction capability and reliability for sparse data with a compact model size based on multiple word clusters, called Multi-Classes.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7625332772731781}]}, {"text": "In the Multi-Class, the statistical connectivity at each position of the N-grams is regarded as word attributes, and one word cluster each is created to represent the positional attributes.", "labels": [], "entities": []}, {"text": "Furthermore , by introducing higher order word N-grams through the grouping of frequent word successions, Multi-Class N-grams are extended to Multi-Class Composite N-grams.", "labels": [], "entities": []}, {"text": "In experiments, the Multi-Class Composite N-grams result in 9.5% lower perplexity and a 16% lower word error rate in speech recognition with a 40% smaller parameter size than conventional word 3-grams.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 98, "end_pos": 113, "type": "METRIC", "confidence": 0.7615924080212911}, {"text": "speech recognition", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7018573731184006}]}], "introductionContent": [{"text": "Word N-grams have been widely used as a statistical language model for language processing.", "labels": [], "entities": [{"text": "language processing", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.6878421604633331}]}, {"text": "Word N-grams are models that give the transition probability of the next word from the previous AE \u00bd word sequence based on a statistical analysis of the huge text corpus.", "labels": [], "entities": []}, {"text": "Though word N-grams are more effective and flexible than rule-based grammatical constraints in many cases, their performance strongly depends on the size of training data, since they are statistical models.", "labels": [], "entities": []}, {"text": "In word N-grams, the accuracy of the word prediction capability will increase according to the number of the order N, but also the number of word transition combinations will exponentially increase.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9996060729026794}, {"text": "word prediction", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.752806156873703}]}, {"text": "Moreover, the size of training data for reliable transition probability values will also dramatically increase.", "labels": [], "entities": []}, {"text": "This is a critical problem for spoken language in that it is difficult to collect training data sufficient enough fora reliable model.", "labels": [], "entities": []}, {"text": "As a solution to this problem, class Ngrams are proposed.", "labels": [], "entities": []}, {"text": "In class N-grams, multiple words are mapped to one word class, and the transition probabilities from word to word are approximated to the probabilities from word class to word class.", "labels": [], "entities": []}, {"text": "The performance and model size of class N-grams strongly depend on the definition of word classes.", "labels": [], "entities": []}, {"text": "In fact, the performance of class N-grams based on the part-of-speech (POS) word class is usually quite a bit lower than that of word N-grams.", "labels": [], "entities": []}, {"text": "Based on this fact, effective word class definitions are required for high performance in class N-grams.", "labels": [], "entities": []}, {"text": "In this paper, the Multi-Class assignment is proposed for effective word class definitions.", "labels": [], "entities": [{"text": "word class definitions", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.6296887497107188}]}, {"text": "The word class is used to represent word connectivity, i.e. which words will appear in a neighboring position with what probability.", "labels": [], "entities": []}, {"text": "In MultiClass assignment, the word connectivity in each position of the N-grams is regarded as a different attribute, and multiple classes corresponding to each attribute are assigned to each word.", "labels": [], "entities": [{"text": "MultiClass assignment", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.8782145082950592}]}, {"text": "For the word clustering of each Multi-Class for each word, a method is used in which word classes are formed automatically and statistically from a corpus, not using a priori knowledge as POS information.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7033687829971313}]}, {"text": "Furthermore, by introducing higher order word N-grams through the grouping of frequent word successions, Multi-Class N-grams are extended to Multi-Class Composite N-grams.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have evaluated Multi-Class N-grams in perplexity as the next equations.", "labels": [], "entities": []}, {"text": "The Good-Turing discount is used for smoothing.", "labels": [], "entities": [{"text": "smoothing", "start_pos": 37, "end_pos": 46, "type": "TASK", "confidence": 0.9676727652549744}]}, {"text": "The perplexity is compared with those of word 2-grams and word 3-grams.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9882854223251343}]}, {"text": "The evaluation data set is the ATR Spoken Language Database ().", "labels": [], "entities": [{"text": "ATR Spoken Language Database", "start_pos": 31, "end_pos": 59, "type": "DATASET", "confidence": 0.8848536610603333}]}, {"text": "The total number of words in the training set is 1,387,300, the vocabulary size is 16,531, and 5,880 words in 42 conversations which are not included in the training set are used for the evaluation.", "labels": [], "entities": []}, {"text": "Figure1 shows the perplexity of Multi-Class 2-grams for each number of classes.", "labels": [], "entities": []}, {"text": "In the MultiClass, the numbers of following and preceding classes are fixed to the same value just for comparison.", "labels": [], "entities": []}, {"text": "As shown in the figure, the Multi-Class 2-gram with 1,200 classes gives the lowest perplexity of 22.70, and it is smaller than the 23.93 in the conventional word 2-gram.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9791485071182251}]}, {"text": "shows the perplexity of Multi-Class 3-grams for each number of classes.", "labels": [], "entities": []}, {"text": "The number of following and preceding classes is 1,200 (which gives the lowest perplexity in Multi-Class 2-grams).", "labels": [], "entities": []}, {"text": "The number of pre-preceding classes is changed from 100 to 1,500.", "labels": [], "entities": []}, {"text": "As shown in this figure, Multi-Class 3-grams result in lower perplexity than the conventional word 3-gram, indicating the reasonability of word clustering based on the distance-2 2-gram.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 139, "end_pos": 154, "type": "TASK", "confidence": 0.7107861042022705}]}, {"text": "We have also evaluated Multi-Class Composite N-grams in perplexity under the same conditions as the Multi-Class N-grams stated in the previous section.", "labels": [], "entities": []}, {"text": "The Multi-Class 2-gram is used for the initial condition of the Multi-Class Composite 2-gram.", "labels": [], "entities": []}, {"text": "The threshold of frequency for introducing word successions is set to 10 based on a preliminary experiment.", "labels": [], "entities": [{"text": "introducing word successions", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.7270229359467825}]}, {"text": "The same word succession set as that of the Multi-Class Composite 2-gram is used for the Multi-Class Composite 3-gram.", "labels": [], "entities": [{"text": "word succession set", "start_pos": 9, "end_pos": 28, "type": "METRIC", "confidence": 0.7481016318003336}]}, {"text": "The evaluation results are shown in. shows that the Multi-Class Composite 3-gram results in 9.5% lower perplexity with a 40% smaller parameter size than the conventional word 3-gram, and that it is in fact a compact and high-performance model.", "labels": [], "entities": []}, {"text": "Though perplexity is a good measure for the performance of language models, it does not always have a direct bearing on performance in language processing.", "labels": [], "entities": []}, {"text": "We have evaluated the proposed model in continuous speech recognition.", "labels": [], "entities": [{"text": "continuous speech recognition", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.6143167515595754}]}, {"text": "As in the perplexity results, the Multi-Class Composite 3-gram shows the highest performance of all models, and its error reduction from the conventional word 3-gram is 16%.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 116, "end_pos": 131, "type": "METRIC", "confidence": 0.9864258766174316}]}], "tableCaptions": [{"text": " Table 1: Evaluation of Multi-Class Composite N- grams in Perplexity", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.4274952709674835}]}, {"text": " Table 2: Evaluation of Multi-Class Composite N- grams in Continuous Speech Recognition", "labels": [], "entities": [{"text": "Continuous Speech Recognition", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.6354230244954427}]}]}