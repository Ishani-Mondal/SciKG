{"title": [{"text": "A machine learning approach to the automatic evaluation of machine translation", "labels": [], "entities": [{"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.6632296442985535}]}], "abstractContent": [{"text": "We present a machine learning approach to evaluating the well-formedness of output of a machine translation system, using classifiers that learn to distinguish human reference translations from machine translations.", "labels": [], "entities": []}, {"text": "This approach can be used to evaluate an MT system, tracking improvements overtime; to aid in the kind of failure analysis that can help guide system development; and to select among alternative output strings.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9783762693405151}]}, {"text": "The method presented is fully automated and independent of source language, target language and domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human evaluation of machine translation (MT) output is an expensive process, often prohibitively so when evaluations must be performed quickly and frequently in order to measure progress.", "labels": [], "entities": [{"text": "evaluation of machine translation (MT) output", "start_pos": 6, "end_pos": 51, "type": "TASK", "confidence": 0.7911818735301495}]}, {"text": "This paper describes an approach to automated evaluation designed to facilitate the identification of areas for investigation and improvement.", "labels": [], "entities": []}, {"text": "It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer.", "labels": [], "entities": [{"text": "evaluating content transfer", "start_pos": 85, "end_pos": 112, "type": "TASK", "confidence": 0.6060488621393839}]}, {"text": "Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output., for example, employ n-gram metrics to select among candidate outputs in natural language generation, while use ngram perplexity to compare the output of MT systems., and employ string edit distance between reference and output sentences to gauge output quality for MT and generation.", "labels": [], "entities": [{"text": "MT and natural language generation", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.7265162169933319}, {"text": "natural language generation", "start_pos": 243, "end_pos": 270, "type": "TASK", "confidence": 0.6953936417897543}, {"text": "MT", "start_pos": 436, "end_pos": 438, "type": "TASK", "confidence": 0.9833049774169922}]}, {"text": "To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required.", "labels": [], "entities": []}, {"text": "for useful discussion of this issue.)", "labels": [], "entities": []}, {"text": "The better the MT system, the more its output will resemble human-generated text.", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9685249924659729}]}, {"text": "Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9953872561454773}]}, {"text": "We have observed that in general humans can easily and reliably categorize a sentence as either machine-or human-generated.", "labels": [], "entities": []}, {"text": "Moreover, they can usually justify their decision.", "labels": [], "entities": []}, {"text": "This observation suggests that evaluation of the wellformedness of output sentences can be treated as a classification problem: given a sentence, how accurately can we predict whether it has been translated by machine?", "labels": [], "entities": []}, {"text": "In this paper we cast the problem of MT evaluation as a machine learning classification task that targets both linguistic features and more abstract features such as ngram perplexity.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9900952279567719}, {"text": "machine learning classification task", "start_pos": 56, "end_pos": 92, "type": "TASK", "confidence": 0.7356964945793152}]}, {"text": "Our corpus consists of 350,000 aligned SpanishEnglish sentence pairs taken from published computer software manuals and online help documents.", "labels": [], "entities": []}, {"text": "We extracted 200,000 English sentences for building language models to evaluate per-sentence perplexity.", "labels": [], "entities": []}, {"text": "From the remainder of the corpus, we extracted 100,000 aligned sentence pairs.", "labels": [], "entities": []}, {"text": "The Spanish sentences in this latter sample were then translated by the Microsoft machine translation system, which was trained on documents from this domain ().", "labels": [], "entities": [{"text": "Microsoft machine translation", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.5564649701118469}]}, {"text": "This yielded a set of 200,000 English sentences, one half of which were English reference sentences, and the other half of which were MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 134, "end_pos": 136, "type": "TASK", "confidence": 0.7105169296264648}]}, {"text": "(The Spanish sentences were not used in building or evaluating the classifiers).", "labels": [], "entities": []}, {"text": "We split the 200,000 English sentences 90/10, to yield 180,000 sentences for training classifiers and 20,000 sentences that we used as held-out test data.", "labels": [], "entities": []}, {"text": "Training and test data were evenly divided between reference English sentences and Spanish-to-English translations.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 Accuracy of the decision trees", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.999103307723999}]}]}