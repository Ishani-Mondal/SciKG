{"title": [{"text": "Using a Randomised Controlled Clinical Trial to Evaluate an NLG System", "labels": [], "entities": []}], "abstractContent": [{"text": "The STOP system, which generates personalised smoking-cessation letters, was evaluated by a randomised controlled clinical trial.", "labels": [], "entities": [{"text": "STOP", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.49514856934547424}]}, {"text": "We believe this is the largest and perhaps most rigorous task effectiveness evaluation ever performed on an NLG system.", "labels": [], "entities": []}, {"text": "The detailed results of the clinical trial have been presented elsewhere, in the medical literature.", "labels": [], "entities": []}, {"text": "In this paper we discuss the clinical trial itself: its structure and cost, what we did and did not learn from it (especially considering that the trial showed that STOP was not effective), and how it compares to other NLG evaluation techniques.", "labels": [], "entities": [{"text": "STOP", "start_pos": 165, "end_pos": 169, "type": "TASK", "confidence": 0.7616899609565735}]}], "introductionContent": [{"text": "There is increasing interest in techniques for evaluating Natural Language Generation (NLG) systems.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.808424154917399}]}, {"text": "However, we are not aware of any previously reported evaluations of NLG systems which have rigorously compared the task effectiveness of an NLG system to a non-NLG alternative.", "labels": [], "entities": []}, {"text": "In this paper we discuss such an evaluation, a large scale (2553 subjects) randomised controlled clinical trial which evaluated the effectiveness of personalised smoking-cessation letters generated by the STOP system ().", "labels": [], "entities": [{"text": "STOP", "start_pos": 205, "end_pos": 209, "type": "TASK", "confidence": 0.8101064562797546}]}, {"text": "We believe that this is the largest, most expensive, and perhaps most rigorous evaluation ever done of an NLG system; it was also a disappointing evaluation, as it showed that STOP letters in general were no more effective than control letters.", "labels": [], "entities": [{"text": "STOP letters", "start_pos": 176, "end_pos": 188, "type": "TASK", "confidence": 0.813143789768219}]}, {"text": "The detailed results of the STOP evaluation have been presented elsewhere, in the medical literature ().", "labels": [], "entities": [{"text": "STOP", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.9770417809486389}]}, {"text": "The purpose of this paper is to discuss the clinical trial from an NLG evaluation perspective, in order to help future researchers decide when a clinical trial (or similar large-scale task effectiveness evaluation) would bean appropriate way to evaluate their systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation is becoming increasingly important in NLG, as in other areas of NLP; see fora summary of NLG evaluation.", "labels": [], "entities": []}, {"text": "As Mellish and Dale point out, we can evaluate the effectiveness of underlying theories, general properties of NLG systems and texts (such as computational speed, or text understandability), or the effectiveness of the generated texts in an actual task or application context.", "labels": [], "entities": [{"text": "text understandability", "start_pos": 166, "end_pos": 188, "type": "TASK", "confidence": 0.6479182839393616}]}, {"text": "Theory evaluations are typically done by comparing predictions of a theory to what is observed in a humanauthored corpus (for example,).", "labels": [], "entities": []}, {"text": "Evaluations of text properties are typically done by asking human judges to rate the quality of generated texts (for example,); sometimes human-authored texts are included in the rated set (without judges knowing which texts are human-authored) to provide a baseline.", "labels": [], "entities": []}, {"text": "Task evaluations (for example,)) are typically done by showing human subjects different texts, and measuring differences in an outcome variable, such as success in performing a task.", "labels": [], "entities": []}, {"text": "However, despite the above work, we are not aware of any previous evaluation which has compared the effectiveness of NLG texts at meeting a communicative goal against the effectiveness of non-NLG control texts.", "labels": [], "entities": []}, {"text": "Young's task evaluation, which maybe the most rigorous previous task evaluation of an NLG system, compared the effectiveness of texts generated by different NLG algorithms, while the IDAS task evaluation ( did not include a control text of any kind. and have compared NLG texts to humanwritten and (in Coch's case) mail-merge texts, but the comparisons were judgements by human domain experts, they did not measure the actual impact of the texts on users.", "labels": [], "entities": []}, {"text": "probably came closest to a controlled evaluation of NLG vs non-NLG alternatives, because they compared the impact of NLG argumentative texts to a no-text control (where users had access to the underlying data but were not given any texts arguing fora particular choice).", "labels": [], "entities": []}, {"text": "Task evaluations that compare the effectiveness of texts from NLG systems to the effectiveness of non-NLG alternatives (mail-merge texts, humanwritten texts, or fixed texts) are expensive and difficult to organise, but we believe they are essential to the progress of NLG, both scientifically and technologically.", "labels": [], "entities": []}, {"text": "In this paper we describe such an evaluation which we performed on the STOP system.", "labels": [], "entities": [{"text": "STOP system", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.6743577718734741}]}, {"text": "The evaluation was indeed expensive and time-consuming, and ultimately was disappointing in that it suggested STOP texts were no more effective than control texts, but we believe that this kind of evaluation was essential to the project.", "labels": [], "entities": [{"text": "STOP texts", "start_pos": 110, "end_pos": 120, "type": "TASK", "confidence": 0.6940525770187378}]}, {"text": "We hope that our description of the STOP clinical trial and what we learned from it will encourage other researchers to consider performing effectiveness evaluations of NLG systems against non-NLG alternatives.", "labels": [], "entities": [{"text": "STOP clinical", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.8589229881763458}]}, {"text": "The clinical trial was by far the biggest evaluation exercise in STOP, but we also performed some smaller evaluations in order to test our algorithms and knowledge acquisition methodology).", "labels": [], "entities": [{"text": "STOP", "start_pos": 65, "end_pos": 69, "type": "TASK", "confidence": 0.8556671142578125}, {"text": "knowledge acquisition", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.7361215651035309}]}, {"text": "Asking smokers or domain experts to read two letters, and state which one they thought was superior; 2.", "labels": [], "entities": []}, {"text": "Statistical analyses of characteristics of smokers; and 3.", "labels": [], "entities": []}, {"text": "Comparing the effectiveness of different algorithms at filling up but not exceeding 4 A5 pages.", "labels": [], "entities": []}, {"text": "These evaluations were much smaller, simpler, and cheaper than the clinical trial, and often gave easier to interpret results.", "labels": [], "entities": []}, {"text": "For example, the letter-comparison experiments suggested (although they did not prove) that older people preferred a more formal writing style than younger people; the statistical analysis suggested (although again did not prove) that the tailoring rules should have been more influenced by level of addiction; and the algorithmic analysis showed that a revision architecture outperformed a conventional pipeline architecture.", "labels": [], "entities": []}, {"text": "So, these experiments produced clearer results at a fraction of the cost of the clinical trial.", "labels": [], "entities": []}, {"text": "But the cheapness of (1) and (2) were partially due to the fact that they were too small to produce statistically solid findings, and the cheapness of and (3) were partially due to the fact that they exploited data sets and resources that were built as part of the clinical trial.", "labels": [], "entities": []}, {"text": "Overall, we believe that these small-scale experiments were worth doing, but as a supplement to, not a replacement for, the clinical trial.", "labels": [], "entities": []}], "tableCaptions": []}