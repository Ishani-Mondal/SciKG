{"title": [], "abstractContent": [{"text": "We present a syntax-based statistical translation model.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.5749427825212479}]}, {"text": "Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node.", "labels": [], "entities": []}, {"text": "These operations capture linguistic differences such as word order and case marking.", "labels": [], "entities": [{"text": "case marking", "start_pos": 71, "end_pos": 83, "type": "TASK", "confidence": 0.6985552906990051}]}, {"text": "Model parameters are estimated in polynomial time using an EM algorithm.", "labels": [], "entities": []}, {"text": "The model produces word alignments that are better than those produced by IBM Model 5.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.759475827217102}]}], "introductionContent": [{"text": "A statistical translation model (TM) is a mathematical model in which the process of humanlanguage translation is statistically modeled.", "labels": [], "entities": [{"text": "statistical translation model (TM)", "start_pos": 2, "end_pos": 36, "type": "TASK", "confidence": 0.8346848885218302}, {"text": "humanlanguage translation", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.7250325381755829}]}, {"text": "Model parameters are automatically estimated using a corpus of translation pairs.", "labels": [], "entities": []}, {"text": "TMs have been used for statistical machine translation, word alignment of a translation corpus), multilingual document retrieval (), automatic dictionary construction, and data preparation for word sense disambiguation programs (.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.7144083281358083}, {"text": "word alignment of a translation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.8520202040672302}, {"text": "multilingual document retrieval", "start_pos": 97, "end_pos": 128, "type": "TASK", "confidence": 0.624834289153417}, {"text": "automatic dictionary construction", "start_pos": 133, "end_pos": 166, "type": "TASK", "confidence": 0.5912376046180725}, {"text": "data preparation", "start_pos": 172, "end_pos": 188, "type": "TASK", "confidence": 0.6845695823431015}, {"text": "word sense disambiguation", "start_pos": 193, "end_pos": 218, "type": "TASK", "confidence": 0.6942627926667532}]}, {"text": "Developing a better TM is a fundamental issue for those applications.", "labels": [], "entities": [{"text": "TM", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9134511351585388}]}, {"text": "Researchers at IBM first described such a statistical TM in.", "labels": [], "entities": [{"text": "TM", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.8835819959640503}]}, {"text": "Their models are based on a string-to-string noisy channel model.", "labels": [], "entities": []}, {"text": "The channel converts a sequence of words in one language (such as English) into another (such as French).", "labels": [], "entities": []}, {"text": "The channel operations are movements, duplications, and translations, applied to each word independently.", "labels": [], "entities": []}, {"text": "The movement is conditioned only on word classes and positions in the string, and the duplication and translation are conditioned only on the word identity.", "labels": [], "entities": []}, {"text": "Mathematical details are fully described in.", "labels": [], "entities": []}, {"text": "One criticism of the IBM-style TM is that it does not model structural or syntactic aspects of the language.", "labels": [], "entities": []}, {"text": "The TM was only demonstrated fora structurally similar language pair.", "labels": [], "entities": [{"text": "TM", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8384777903556824}]}, {"text": "It has been suspected that a language pair with very different word order such as English and Japanese would not be modeled well by these TMs.", "labels": [], "entities": []}, {"text": "To incorporate structural aspects of the language, our channel model accepts a parse tree as an input, i.e., the input sentence is preprocessed by a syntactic parser.", "labels": [], "entities": []}, {"text": "The channel performs operations on each node of the parse tree.", "labels": [], "entities": []}, {"text": "The operations are reordering child nodes, inserting extra words at each node, and translating leaf words.", "labels": [], "entities": [{"text": "translating leaf words", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.8641812403996786}]}, {"text": "shows the overview of the operations of our model.", "labels": [], "entities": []}, {"text": "Note that the output of our model is a string, not a parse tree.", "labels": [], "entities": []}, {"text": "Therefore, parsing is only needed on the channel input side.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.985675036907196}]}, {"text": "The reorder operation is intended to model translation between languages with different word orders, such as SVO-languages (English or Chinese) and SOV-languages (Japanese or Turkish).", "labels": [], "entities": []}, {"text": "The word-insertion operation is intended to capture linguistic differences in specifying syntactic cases.", "labels": [], "entities": []}, {"text": "E.g., English and French use structural position to specify case, while Japanese and Korean use case-marker particles.", "labels": [], "entities": []}, {"text": "enhanced the IBM models by introducing phrases, and and showed statistical models based on syntactic structure.", "labels": [], "entities": []}, {"text": "The way we handle syntactic parse trees is inspired by their work, although their approach is not to model the translation process, but to formalize a model that generates two languages at the same time.", "labels": [], "entities": []}, {"text": "Our channel operations are also similar to the mechanism in Twisted Pair Grammar) used in their knowledge-based system.", "labels": [], "entities": []}, {"text": "Following and the other literature in TM, this paper only focuses the details of TM.", "labels": [], "entities": [{"text": "TM", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9455191493034363}, {"text": "TM", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9126145839691162}]}, {"text": "Applications of our TM, such as machine translation or dictionary construction, will be described in a separate paper.", "labels": [], "entities": [{"text": "TM", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9296512603759766}, {"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.825141191482544}, {"text": "dictionary construction", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.8292757272720337}]}, {"text": "Section 2 describes our model in detail.", "labels": [], "entities": []}, {"text": "Section 3 shows experimental results.", "labels": [], "entities": []}, {"text": "We conclude with Section 4, followed by an Appendix describing the training algorithm in more detail.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9652741551399231}]}], "datasetContent": [{"text": "To experiment, we trained our model on a small English-Japanese corpus.", "labels": [], "entities": []}, {"text": "To evaluate performance, we examined alignments produced by the learned model.", "labels": [], "entities": []}, {"text": "For comparison, we also trained IBM Model 5 on the same corpus.", "labels": [], "entities": []}, {"text": "The training procedure resulted in the tables of estimated model parameters.", "labels": [], "entities": []}, {"text": "in Section 2.1 shows part of those parameters obtained by the training above.", "labels": [], "entities": []}, {"text": "To evaluate performance, we let the models generate the most probable alignment of the training corpus (called the Viterbi alignment).", "labels": [], "entities": []}, {"text": "The alignment shows how the learned model induces the internal structure of the training data.", "labels": [], "entities": []}, {"text": "shows alignments produced by our model and IBM Model 5.", "labels": [], "entities": [{"text": "IBM Model 5", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9081307450930277}]}, {"text": "Darker lines indicates that the particular alignment link was judged correct by humans.", "labels": [], "entities": []}, {"text": "Three humans were asked to rate each alignment as okay (1.0 point), not sure (0.5 point), or wrong (0 point).", "labels": [], "entities": []}, {"text": "The darkness of the lines in the figure reflects the human score.", "labels": [], "entities": []}, {"text": "We obtained the average score of the first 50 sentence pairs in the corpus.", "labels": [], "entities": []}, {"text": "We also counted the number of perfectly aligned sentence pairs in the 50 pairs.", "labels": [], "entities": []}, {"text": "Perfect means that all alignments in a sentence pair were judged okay by all the human judges.", "labels": [], "entities": []}, {"text": "Our model got a better result compared to IBM Model 5.", "labels": [], "entities": []}, {"text": "Note that there were no perfect alignments from the IBM Model.", "labels": [], "entities": [{"text": "IBM Model", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.9666388034820557}]}, {"text": "Errors by the IBM Model were spread out over the whole set, while our errors were localized to some sentences.", "labels": [], "entities": [{"text": "Errors", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9599618315696716}, {"text": "IBM Model", "start_pos": 14, "end_pos": 23, "type": "DATASET", "confidence": 0.9408622980117798}]}, {"text": "We expect that our model will therefore be easier to improve.", "labels": [], "entities": []}, {"text": "Also, localized errors are good if the TM is used for corpus preparation or filtering.", "labels": [], "entities": [{"text": "corpus preparation or filtering", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6865456476807594}]}, {"text": "We also measured training perplexity of the models.", "labels": [], "entities": []}, {"text": "The perplexity of our model was 15.79, and that of IBM Model 5 was 9.84.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9682254195213318}, {"text": "IBM Model 5", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.9128379623095194}]}, {"text": "For reference, the perplexity after 5 iterations of Model 1 was 24.01.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9712240695953369}]}, {"text": "Perplexity values roughly indicate the predictive power of the model.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.980863094329834}]}, {"text": "Generally, lower perplexity means a better model, but it might cause over-fitting to a training data.", "labels": [], "entities": []}, {"text": "Since the IBM Model usually requires millions of training sentences, the lower perplexity value for the IBM Model is likely due to over-fitting.", "labels": [], "entities": [{"text": "IBM Model", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.914728969335556}, {"text": "IBM Model", "start_pos": 104, "end_pos": 113, "type": "DATASET", "confidence": 0.9192657470703125}]}], "tableCaptions": []}