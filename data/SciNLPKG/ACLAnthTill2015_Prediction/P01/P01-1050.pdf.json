{"title": [{"text": "Towards a Unified Approach to Memory-and Statistical-Based Machine Translation", "labels": [], "entities": [{"text": "Statistical-Based Machine Translation", "start_pos": 41, "end_pos": 78, "type": "TASK", "confidence": 0.6119794150193533}]}], "abstractContent": [{"text": "We present a set of algorithms that enable us to translate natural language sentences by exploiting both a translation memory and a statistical-based translation model.", "labels": [], "entities": [{"text": "translate natural language sentences", "start_pos": 49, "end_pos": 85, "type": "TASK", "confidence": 0.8205623030662537}]}, {"text": "Our results show that an automatically derived translation memory can be used within a statistical framework to often find translations of higher probability than those found using solely a statistical model.", "labels": [], "entities": []}, {"text": "The translations produced using both the translation memory and the statistical model are significantly better than translations produced by two commercial systems: our hybrid system translated perfectly 58% of the 505 sentences in a test collection, while the commercial systems translated perfectly only 40-42% of them.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the last decade, much progress has been made in the fields of example-based (EBMT) and statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 92, "end_pos": 129, "type": "TASK", "confidence": 0.83665398756663}]}, {"text": "EBMT systems work by modifying existing, human produced translation instances, which are stored in a translation memory (TMEM).", "labels": [], "entities": []}, {"text": "Many methods have been proposed for storing translation pairs in a TMEM, finding translation examples that are relevant for translating unseen sentences, and modifying and integrating translation fragments to produce correct outputs., for example, stores complete parse trees in the TMEM and selects and generates new translations by performing similarity matchings on these trees.", "labels": [], "entities": []}, {"text": "store complete sentences; new translations are generated by modifying the TMEM translation that is most similar to the input sentence.", "labels": [], "entities": []}, {"text": "Others store phrases; new translations are produced by optimally partitioning the input into phrases that match examples from the TMEM (, or by finding all partial matches and then choosing the best possible translation using a multi-engine translation system.", "labels": [], "entities": []}, {"text": "With a few exceptions (), most SMT systems are couched in the noisy channel framework (see).", "labels": [], "entities": [{"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9943525791168213}]}, {"text": "In this framework, the source language, let's say English, is assumed to be generated by a noisy probabilistic source.", "labels": [], "entities": []}, {"text": "Most of the current statistical MT systems treat this source as a sequence of words (.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.8516443371772766}]}, {"text": "(Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases) or a syntactic tree .) In the noisy-channel framework, a monolingual corpus is used to derive a statistical language model that assigns a probability to a sequence of words or phrases, thus enabling one to distinguish between sequences of words that are grammatically correct and sequences that are not.", "labels": [], "entities": []}, {"text": "A sentence-aligned parallel corpus is then used in order to build a probabilistic translation model that explains how the source can be turned into the target and that assigns a probability to every way in which a source e can be mapped into a target f.", "labels": [], "entities": []}, {"text": "Once the parameters of the language and translation models are estimated using traditional maximum likelihood and EM techniques, one can take as input any string in the target language f, and find the source e of highest probability that could have generated the target, a process called decoding (see).", "labels": [], "entities": []}, {"text": "It is clear that EBMT and SMT systems have different strengths and weaknesses.", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9645349979400635}]}, {"text": "If a sentence to be translated or a very similar one can be found in the TMEM, an EBMT system has a good chance of producing a good translation.", "labels": [], "entities": []}, {"text": "However, if the sentence to be translated has no close matches in the TMEM, then an EBMT system is less likely to succeed.", "labels": [], "entities": []}, {"text": "In contrast, an SMT system maybe able to produce perfect translations even when the sentence given as input does not resemble any sentence from the training corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9924778938293457}]}, {"text": "However, such a system maybe unable to generate translations that use idioms and phrases that reflect long-distance dependencies and contexts, which are usually not captured by current translation models.", "labels": [], "entities": []}, {"text": "This paper advances the state-of-the-art in two respects.", "labels": [], "entities": []}, {"text": "First, we show how one can use an existing statistical translation model () in order to automatically derive a statistical TMEM.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.6320525705814362}]}, {"text": "Second, we adapt a decoding algorithm so that it can exploit information specific both to the statistical TMEM and the translation model.", "labels": [], "entities": []}, {"text": "Our experiments show that the automatically derived translation memory can be used within the statistical framework to often find translations of higher probability than those found using solely the statistical model.", "labels": [], "entities": []}, {"text": "The translations produced using both the translation memory and the statistical model are significantly better than translations produced by two commercial systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extracted from the test corpus a collection of 505 French sentences, uniformly distributed across the lengths 6, 7, 8, 9, and 10.", "labels": [], "entities": []}, {"text": "For each French sentence, we had access to the humangenerated English translation in the test corpus, and to translations generated by two commercial systems.", "labels": [], "entities": []}, {"text": "We produced translations using three versions of the greedy decoder: one used only the statistical translation model, one used the translation model and the FTMEM, and one used the translation model and the PTMEM.", "labels": [], "entities": [{"text": "FTMEM", "start_pos": 157, "end_pos": 162, "type": "METRIC", "confidence": 0.8536957502365112}, {"text": "PTMEM", "start_pos": 207, "end_pos": 212, "type": "DATASET", "confidence": 0.8930404186248779}]}, {"text": "We initially assessed how often the translations obtained from TMEM seeds had higher probaSent.", "labels": [], "entities": [{"text": "TMEM seeds", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.8002488613128662}]}], "tableCaptions": [{"text": " Table 1: Examples of automatically constructed statistical translation memory entries.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy of automatically constructed  TMEMs.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9932138919830322}]}, {"text": " Table 4: The utility of the FTMEM.", "labels": [], "entities": [{"text": "FTMEM", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.8315468430519104}]}, {"text": " Table 5: The utility of the PTMEM.", "labels": [], "entities": [{"text": "PTMEM", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.8150466084480286}]}]}