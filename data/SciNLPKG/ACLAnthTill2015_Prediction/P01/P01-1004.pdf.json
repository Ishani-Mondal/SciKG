{"title": [{"text": "Low-cost, High-performance Translation Retrieval: Dumber is Better", "labels": [], "entities": [{"text": "Translation Retrieval", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.9690856039524078}]}], "abstractContent": [{"text": "In this paper, we compare the relative effects of segment order, segmen-tation and segment contiguity on the retrieval performance of a translation memory system.", "labels": [], "entities": []}, {"text": "We take a selection of both bag-of-words and segment order-sensitive string comparison methods , and run each over both character-and word-segmented data, in combination with a range of local segment con-tiguity models (in the form of N-grams).", "labels": [], "entities": []}, {"text": "Over two distinct datasets, we find that indexing according to simple character bigrams produces a retrieval accuracy superior to any of the tested word N-gram models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9212684631347656}]}, {"text": "Further, in their optimum configuration, bag-of-words methods are shown to be equivalent to segment order-sensitive methods in terms of retrieval accuracy, but much faster.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.964958906173706}]}, {"text": "We also provide evidence that our findings are scal-able.", "labels": [], "entities": []}], "introductionContent": [{"text": "Translation memories (TMs) area list of translation records (source language strings paired with a unique target language translation), which the TM system accesses in suggesting a list of target language (L2) translation candidates fora given source language (L1) input.", "labels": [], "entities": []}, {"text": "Translation retrieval (TR) is a description of this process of selecting from the TM a set of translation records (TRecs) of maximum L1 similarity to a given input.", "labels": [], "entities": [{"text": "Translation retrieval (TR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9352838039398194}]}, {"text": "Typically in example-based machine translation, either a single TRec is retrieved from the TM based on a match with the overall L1 input, or the input is partitioned into coherent segments, and individual translations retrieved for each; this is the first step toward generating a customised translation for the input.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.699185311794281}]}, {"text": "With stand-alone TM systems, on the other hand, the system selects an arbitrary number of translation candidates falling within a certain empirical corridor of similarity with the overall input string, and simply outputs these for manual manipulation by the user in fashioning the final translation.", "labels": [], "entities": []}, {"text": "A key assumption surrounding the bulk of past TR research has been that the greater the match stringency/linguistic awareness of the retrieval mechanism, the greater the final retrieval accuracy will become.", "labels": [], "entities": [{"text": "TR", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9820143580436707}, {"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9038755893707275}]}, {"text": "Naturally, any appreciation in retrieval complexity comes at a price in terms of computational overhead.", "labels": [], "entities": []}, {"text": "We thus follow the lead of in asking the question: what is the empirical effect on retrieval performance of different match approaches?", "labels": [], "entities": []}, {"text": "Here, retrieval performance is defined as the combination of retrieval speed and accuracy, with the ideal method offering fast response times at high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9994947910308838}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9754930138587952}]}, {"text": "In this paper, we choose to focus on retrieval performance within a Japanese-English TR context.", "labels": [], "entities": []}, {"text": "One key area of interest with Japanese is the effect that segmentation has on retrieval performance.", "labels": [], "entities": []}, {"text": "As Japanese is a non-segmenting language (does not explicitly delimit words orthographically), we can take the brute-force approach in treating each string as a sequence of characters (character-based indexing), or alternatively call upon segmentation technology in partitioning each string into words (word-based indexing).", "labels": [], "entities": []}, {"text": "Orthogonal to this is the question of sensitivity to segment order.", "labels": [], "entities": []}, {"text": "That is, should our match mechanism treat each string as an unorganised multiset of terms (the bag-of-words approach), or attempt to find the match that best preserves the original segment order in the input (the segment order-sensitive approach)?", "labels": [], "entities": []}, {"text": "We tackle this issue by implementing a sample of representative bag-of-words and segment ordersensitive methods and testing the retrieval performance of each.", "labels": [], "entities": []}, {"text": "As a third orthogonal parameter, we consider the effects of segment contiguity.", "labels": [], "entities": []}, {"text": "That is, do matches over contiguous segments provide closer overall translation correspondence than matches over displaced segments?", "labels": [], "entities": []}, {"text": "Segment contiguity is either explicitly modelled within the string match mechanism, or provided as an add-in in the form of segment N-grams.", "labels": [], "entities": [{"text": "Segment contiguity", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8501704037189484}]}, {"text": "To preempt the major findings of this paper, over a series of experiments we find that character-based indexing is consistently superior to word-based indexing.", "labels": [], "entities": []}, {"text": "Furthermore, the bagof-words methods we test are equivalent in retrieval accuracy to the more expensive segment order-sensitive methods, but superior in retrieval speed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.978559136390686}]}, {"text": "Finally, segment contiguity models provide benefits in terms of both retrieval accuracy and retrieval speed, particularly when coupled with character-based indexing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9594985246658325}]}, {"text": "We thus provide clear evidence that high-performance TR is achievable with naive methods, and moreso that such methods outperform more intricate, expensive methods.", "labels": [], "entities": [{"text": "TR", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9603310823440552}]}, {"text": "That is, the dumber the retrieval mechanism, the better.", "labels": [], "entities": []}, {"text": "Below, we review the orthogonal parameters of segmentation, segment order and segment contiguity ( \u00a7 2).", "labels": [], "entities": []}, {"text": "We then present a range of both bagof-words and segment order-sensitive string comparison methods ( \u00a7 3) and detail the evaluation methodology ( \u00a7 4).", "labels": [], "entities": []}, {"text": "Finally, we evaluate the different methods in a Japanese-English TR context ( \u00a7 5), before concluding the paper ( \u00a7 6).", "labels": [], "entities": [{"text": "TR context", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.7338507771492004}]}], "datasetContent": [{"text": "As our main dataset, we used 3033 unique Japanese-English TRecs extracted from construction machinery field reports for the purposes of this research.", "labels": [], "entities": []}, {"text": "Most TRecs comprise a single sentence, with an average Japanese character length of 27.7 and English word length of 13.3.", "labels": [], "entities": [{"text": "English word length", "start_pos": 93, "end_pos": 112, "type": "METRIC", "confidence": 0.6760836939016978}]}, {"text": "Importantly, our dataset constitutes a controlled language, that is, a given word will tend to be translated identically across all usages, and only a limited range of syntactic constructions are employed.", "labels": [], "entities": []}, {"text": "In secondary evaluation of retrieval performance over differing data sizes, we extracted 61,236 Japanese-English TRecs from the JEIDA parallel corpus, which is made up of government white papers.", "labels": [], "entities": [{"text": "JEIDA parallel corpus", "start_pos": 128, "end_pos": 149, "type": "DATASET", "confidence": 0.8962751229604086}]}, {"text": "The alignment granularity of this second corpus is much coarser than for the first corpus, with a single TRec often extending over multiple sentences.", "labels": [], "entities": []}, {"text": "The average Japanese character length of each TRec is 76.3, and the average English word length is 35.7.", "labels": [], "entities": []}, {"text": "The language used in the JEIDA corpus is highly constrained, although not as controlled as that in the first corpus.", "labels": [], "entities": [{"text": "JEIDA corpus", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9404422640800476}]}, {"text": "The construction of TRecs from both corpora was based on existing alignment data, and no further effort was made to subdivide partitions.", "labels": [], "entities": []}, {"text": "For Japanese word-based indexing, segmentation was carried out primarily with ChaSen v2.0 (), and where specifically mentioned, JUMAN v3.5 ( and ALTJAWS 3 were also used.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9664789438247681}]}, {"text": "Evaluation of retrieval accuracy is carried out according to a modified version of the method proposed by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9710030555725098}]}, {"text": "The first step in this process is to determine the set of \"optimal\" translations byway of the same basic TR procedure as described above, except that we use the held-out translation for each input to search through the L2 component of the TM.", "labels": [], "entities": []}, {"text": "As for L1 TR, a threshold on translation utility is then applied to ascertain whether the optimal translations are similar enough to the model translation to be of use, and in the case that this threshold is not achieved, the empty string is returned as the sole optimal translation.", "labels": [], "entities": []}, {"text": "Next, we proceed to ascertain whether the actual system output coincides with one of the optimal translations, and rate the accuracy of each method according to the proportion of optimal outputs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9992533326148987}]}, {"text": "If multiple outputs are produced, we select from among them randomly.", "labels": [], "entities": []}, {"text": "This guarantees a unique translation output and differs from the methodology of, who judged the system output to be \"correct\" if the potentially multiple set of top-ranking outputs contained an optimal translation, placing methods with greater fan-out of outputs at an advantage.", "labels": [], "entities": []}, {"text": "So as to filter out any bias towards a given string comparison method in TR, we determine translation optimality based on both 3-operation edit distance (operating over English word bigrams) and also weighted sequential correspondence (operating over English word unigrams).", "labels": [], "entities": []}, {"text": "We then derive the final translation accuracy as the average of the accuracies from the respective evaluation sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.8139336705207825}]}, {"text": "Here again, our approach differs from that of, who based determination of translation optimality exclusively on 3-operation edit distance (operating over word unigrams), a method which we found to produce a strong bias toward 3-operation edit distance in L1 TR.", "labels": [], "entities": [{"text": "translation optimality", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.8627735376358032}]}, {"text": "In determining translation optimality, all punctuation and stop words were first filtered out of each L2 (English) string, and all remaining segments scored at a wt of 1.", "labels": [], "entities": []}, {"text": "Stop words are defined as those contained within the SMART) stop word list.", "labels": [], "entities": [{"text": "SMART) stop word list", "start_pos": 53, "end_pos": 74, "type": "DATASET", "confidence": 0.6874179363250732}]}, {"text": "Perhaps the main drawback of our approach to evaluation is that we assume a unique model translation for each input, wherein fact, multiple translations of equivalent quality could reasonably be expected to exist.", "labels": [], "entities": []}, {"text": "In our case, however, both corpora represent relatively controlled languages and language use is hence highly predictable.", "labels": [], "entities": []}, {"text": "The proposed evaluation methodology is thus justified.", "labels": [], "entities": []}, {"text": "In this section, we test our five string comparison methods over the construction machinery corpus, under both character-and word-based indexing, and with each of unigrams, bigrams and mixed unigrams/bigrams.", "labels": [], "entities": []}, {"text": "The retrieval accuracies and times for the different string comparison methods are presented in  Above, we established that character-based indexing is superior to word-based indexing for distinct datasets and a range of segmentation modules, even when segmentation is coupled with lexical normalisation.", "labels": [], "entities": []}, {"text": "Additionally, we provided evidence to the effect that bag-of-words methods offer superior translation retrieval performance to segment order-sensitive methods.", "labels": [], "entities": [{"text": "translation retrieval", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.9601537883281708}]}, {"text": "We are still no closer, however, to determining why this should be the case.", "labels": [], "entities": []}, {"text": "Here, we seek to provide an explanation for these intriguing results.", "labels": [], "entities": []}, {"text": "First comparing character-and word-based indexing, we found that the disparity in retrieval accuracy was largely related to the scoring of katakana words, which are significantly longer in character length than native Japanese words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9483652114868164}]}, {"text": "For the construction machinery dataset as analysed with ChaSen, for example, the average character length of katakana words is 3.62, as compared to 2.05 overall.", "labels": [], "entities": []}, {"text": "Under word-based indexing, all words are treated equally and character length does not enter into calculations.", "labels": [], "entities": [{"text": "word-based indexing", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.675200343132019}]}, {"text": "Thus a katakana word is treated identically to any other word type.", "labels": [], "entities": []}, {"text": "Under character-based indexing, on the other hand, the longer the word, the more segments it generates, and a single matching katakana sequence thus tends to contribute more heavily to the final score than other words.", "labels": [], "entities": []}, {"text": "Effectively, therefore, katakana sequences receive a higher score than kanji and other sequences, producing a preference for TRecs which incorporate the same katakana sequences as the input.", "labels": [], "entities": []}, {"text": "As noted above, katakana sequences generally represent key technical terms, and such weighting thus tends to be beneficial to retrieval accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9665515422821045}]}, {"text": "We next examine the reason for the high correlation in retrieval accuracy between bag-of-words and segment order-sensitive methods in their optimum configurations (i.e. when coupled with character bigrams).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.987677276134491}]}, {"text": "Essentially, the probability of a given segment set permuting in different string contexts diminishes as the number of co-occurring segments decreases.", "labels": [], "entities": []}, {"text": "That is, fora given string pair, the greater the segment overlap between them (relative to the overall string lengths), the lower the probability that those segments are going to occur in different orderings.", "labels": [], "entities": []}, {"text": "This is particularly the case when local segment contiguity is modelled within the segment description, as occurs for the character bigram and mixed word uni/bigram models.", "labels": [], "entities": []}, {"text": "For high-scoring matches, therefore, segment order sensitivity becomes largely superfluous, and the slight edge in retrieval accuracy for segment order-sensitive methods tends to come for mid-scoring matches, in the vicinity of the translation utility threshold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.8575297594070435}]}], "tableCaptions": []}