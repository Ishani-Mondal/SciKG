{"title": [{"text": "The Role of Lexico-Semantic Feedback in Open-Domain Textual Question-Answering", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents an open-domain textual Question-Answering system that uses several feedback loops to enhance its performance.", "labels": [], "entities": []}, {"text": "These feedback loops combine in anew way statistical results with syntactic, semantic or pragmatic information derived from texts and lexical databases.", "labels": [], "entities": []}, {"text": "The paper presents the contribution of each feedback loop to the overall performance of 76% human-assessed precise answers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open-domain textual Question-Answering (Q&A), as defined by the TREC competitions 1 , is the task of identifying in large collections of documents a text snippet where the answer to a natural language question lies.", "labels": [], "entities": [{"text": "Open-domain textual Question-Answering (Q&A)", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6598476245999336}, {"text": "TREC competitions 1", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.8557934959729513}, {"text": "identifying in large collections of documents a text snippet where the answer to a natural language question", "start_pos": 101, "end_pos": 209, "type": "TASK", "confidence": 0.5486353057272294}]}, {"text": "The answer is constrained to be found either in a short (50 bytes) or along (250 bytes) text span.", "labels": [], "entities": []}, {"text": "Frequently, keywords extracted from the natural language question are either within the text span or in its immediate vicinity, forming a text paragraph.", "labels": [], "entities": []}, {"text": "Since such paragraphs must be identified throughout voluminous collections, automatic and autonomous Q&A systems incorporate an index of the collection as well as a paragraph retrieval mechanism.", "labels": [], "entities": []}, {"text": "Recent results from the TREC evaluations (() () show that Information Retrieval (IR) techniques alone are not sufficient for finding answers with high precision.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.80887211561203}, {"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.977460503578186}]}, {"text": "In fact, more and more systems adopt architectures in which the semantics of the questions are captured prior to paragraph retrieval (e.g. () ( ) and used later in extracting the answer (cf. ().", "labels": [], "entities": []}, {"text": "When processing a natural language question two goals must be achieved.", "labels": [], "entities": []}, {"text": "First we need to know what is the expected answer type; in other words, we need to know what we are looking for.", "labels": [], "entities": []}, {"text": "Second, we need to know whereto look for the answer, e.g. we must identify the question keywords to be used in the paragraph retrieval.", "labels": [], "entities": []}, {"text": "The expected answer type is determined based on the question stem, e.g. who, where or how much and eventually one of the question concepts, when the stem is ambiguous (for example what), as described in ( ) ()).", "labels": [], "entities": []}, {"text": "However finding question keywords that retrieve all candidate answers cannot be achieved only by deriving some of the words used in the question.", "labels": [], "entities": []}, {"text": "Frequently, question reformulations use different words, but imply the same answer.", "labels": [], "entities": [{"text": "question reformulations", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.7207244038581848}]}, {"text": "Moreover, many equivalent answers are phrased differently.", "labels": [], "entities": []}, {"text": "In this paper we argue that the answer to complex natural language questions cannot be extracted with significant precision from large collections of texts unless several lexico-semantic feedback loops are allowed.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9726458191871643}]}, {"text": "In Section 2 we survey the related work whereas in Section 3 we describe the feedback loops that refine the search for correct answers.", "labels": [], "entities": []}, {"text": "Section 4 presents the approach of devising keyword alternations whereas Section 5 details the recognition of question reformulations.", "labels": [], "entities": [{"text": "recognition of question reformulations", "start_pos": 95, "end_pos": 133, "type": "TASK", "confidence": 0.7411894649267197}]}, {"text": "Section 6 evaluates the results of the Q&A system and Section 7 summarizes the conclusions.", "labels": [], "entities": [{"text": "Q&A", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8864721655845642}]}], "datasetContent": [{"text": "To evaluate the role of lexico-semantic feedback loops in an open-domain textual Q&A system we have relied on the 890 questions employed in the TREC-8 and TREC-9 Q&A evaluations.", "labels": [], "entities": [{"text": "TREC-9 Q&A evaluations", "start_pos": 155, "end_pos": 177, "type": "TASK", "confidence": 0.6449182450771331}]}, {"text": "In TREC, for each question the performance was computed by the reciprocal value of the rank (RAR) of the highest-ranked correct answer given by the system.", "labels": [], "entities": [{"text": "TREC", "start_pos": 3, "end_pos": 7, "type": "TASK", "confidence": 0.5386946201324463}, {"text": "rank (RAR)", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.7331451028585434}]}, {"text": "Given that only the first five answers were considered in the TREC evaluations, if the RAR is defined as its value is 1 if the first answer is correct; 0.5 if the second answer was correct, but not the first one; 0.33 when the correct answer was on the third position; 0.25 if the fourth answer was correct; 0.2 when the fifth answer was correct and 0 if none of the first five answers were correct.", "labels": [], "entities": [{"text": "TREC evaluations", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.6587702929973602}, {"text": "RAR", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9622535705566406}]}, {"text": "The Mean Reciprocal Answer Rank (MRAR) is used to compute the overall performance of the systems participating in the TREC evaluation In addition, TREC-9 imposed the constraint that an answer is considered correct only when the textual context from the document that contains it can account for it.", "labels": [], "entities": [{"text": "Mean Reciprocal Answer Rank (MRAR)", "start_pos": 4, "end_pos": 38, "type": "METRIC", "confidence": 0.9436419946806771}]}, {"text": "When the human assessors were convinced this constraint was satisfied, they considered the RAR to be strict, otherwise, the RAR was considered lenient.: NIST-evaluated performance NIST for the system on which we evaluated the role of lexico-semantic feedbacks.", "labels": [], "entities": [{"text": "RAR", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.9783447980880737}]}, {"text": "lists the quantitative analysis of the feedback loops.", "labels": [], "entities": []}, {"text": "Loop 1 was generated more often than any other loop.", "labels": [], "entities": []}, {"text": "However, the small overall average number of feedback loops that have been carried out indicate that the fact they port little overhead to the Q&A system.", "labels": [], "entities": [{"text": "Q&A", "start_pos": 143, "end_pos": 146, "type": "TASK", "confidence": 0.7929531733194987}]}, {"text": "Average Maximal number number Loop 1 1.384 7 Loop 2 1.15 3 Loop 3 1.07 5 More interesting is the qualitative analysis of the effect of the feedback loops on the Q&A evaluation.", "labels": [], "entities": [{"text": "Maximal number number Loop 1 1.384", "start_pos": 8, "end_pos": 42, "type": "METRIC", "confidence": 0.9271961450576782}, {"text": "Q&A", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.8807399074236552}]}, {"text": "Overall, the precision increases substantially when all loops were enabled, as illustrated in  Individually, the effect of Loop 1 has an accuracy increase of over 40%, the effect of Loop 2 had an enhancement of more than 52% while Loop 3 produced an enhancement of only 8%.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996119141578674}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9994725584983826}]}, {"text": "Table 4 lists also the combined effect of the feedbacks, showing that when all feedbacks are enabled, for short answers we obtained an MRAR of 0.568, i.e. 76% increase over Q&A without feedbacks.", "labels": [], "entities": [{"text": "MRAR", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9983789920806885}]}, {"text": "The MRAR for long answers had a similar increase of 91%.", "labels": [], "entities": [{"text": "MRAR", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9770888686180115}]}, {"text": "Because we also used the answer caching technique, we gained more than 1% for short answers and almost 3% for long answers, obtaining the result listed in.", "labels": [], "entities": []}, {"text": "In our experiments, from the total of 890 TREC questions, lexical alternations were used for 129 questions and the semantic alternations were needed only for 175 questions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. To classify questions in reformulation  groups, we used the algorithm:", "labels": [], "entities": [{"text": "classify questions in reformulation  groups", "start_pos": 13, "end_pos": 56, "type": "TASK", "confidence": 0.6849529027938843}]}, {"text": " Table 1: Two classes of TREC-9 question refor- mulations.", "labels": [], "entities": []}, {"text": " Table 4: Effect of feedbacks on accuracy.  L1=Loop 1; L2=Loop 2; L3=Loop 3.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9994452595710754}]}]}