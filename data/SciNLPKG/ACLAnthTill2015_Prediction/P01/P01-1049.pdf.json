{"title": [{"text": "Building Semantic Perceptron Net for Topic Spotting", "labels": [], "entities": [{"text": "Topic Spotting", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.8299193382263184}]}], "abstractContent": [{"text": "This paper presents an approach to automatically build a semantic perceptron net (SPN) for topic spotting.", "labels": [], "entities": [{"text": "topic spotting", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.8344442248344421}]}, {"text": "It uses context at the lower layer to select the exact meaning of key words, and employs a combination of context, co-occurrence statistics and thesaurus to group the distributed but semantically related words within a topic to form basic semantic nodes.", "labels": [], "entities": []}, {"text": "The semantic nodes are then used to infer the topic within an input document.", "labels": [], "entities": []}, {"text": "Experiments on Reuters 21578 data set demonstrate that SPN is able to capture the semantics of topics, and it performs well on topic spotting task.", "labels": [], "entities": [{"text": "Reuters 21578 data set", "start_pos": 15, "end_pos": 37, "type": "DATASET", "confidence": 0.984115332365036}, {"text": "SPN", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9044294953346252}, {"text": "topic spotting task", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.8194338083267212}]}], "introductionContent": [{"text": "Topic spotting is the problem of identifying the presence of a predefined topic in a text document.", "labels": [], "entities": [{"text": "Topic spotting", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7757862210273743}, {"text": "identifying the presence of a predefined topic in a text document", "start_pos": 33, "end_pos": 98, "type": "TASK", "confidence": 0.6501560536297885}]}, {"text": "More formally, given a set of n topics together with a collection of documents, the task is to determine for each document the probability that one or more topics is present in the document.", "labels": [], "entities": []}, {"text": "Topic spotting maybe used to automatically assign subject codes to newswire stories, filter electronic emails and online news, and pre-screen document in information retrieval and information extraction applications.", "labels": [], "entities": [{"text": "Topic spotting", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7849939465522766}, {"text": "information retrieval and information extraction", "start_pos": 154, "end_pos": 202, "type": "TASK", "confidence": 0.6577959775924682}]}, {"text": "Topic spotting, and its related problem of text categorization, has been a hot area of research for over a decade.", "labels": [], "entities": [{"text": "Topic spotting", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8720838129520416}, {"text": "text categorization", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7856113612651825}]}, {"text": "A large number of techniques have been proposed to tackle the problem, including: regression model, nearest neighbor classification, Bayesian probabilistic model, decision tree, inductive rule learning, neural network, on-line learning, and, support vector machine).", "labels": [], "entities": [{"text": "nearest neighbor classification", "start_pos": 100, "end_pos": 131, "type": "TASK", "confidence": 0.680110494295756}]}, {"text": "Most of these methods are word-based and consider only the relationships between the features and topics, but not the relationships among features.", "labels": [], "entities": []}, {"text": "It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy.", "labels": [], "entities": []}, {"text": "A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus, to word-sense disambiguation) and context).", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 154, "end_pos": 179, "type": "TASK", "confidence": 0.7253203690052032}]}, {"text": "The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 127, "end_pos": 154, "type": "TASK", "confidence": 0.6747818986574808}, {"text": "information retrieval", "start_pos": 156, "end_pos": 177, "type": "TASK", "confidence": 0.8115905225276947}, {"text": "image understanding", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.7655306756496429}]}, {"text": "Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting.", "labels": [], "entities": [{"text": "topic spotting", "start_pos": 191, "end_pos": 205, "type": "TASK", "confidence": 0.780186265707016}]}, {"text": "However, there have been few attempts in this direction.", "labels": [], "entities": []}, {"text": "This is mainly because of difficulties in automatically constructing the semantic networks for the topics.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approach to automatically build a semantic perceptron net (SPN) for topic spotting.", "labels": [], "entities": [{"text": "topic spotting", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.8164391815662384}]}, {"text": "The SPN is a connectionist model with hierarchical structure.", "labels": [], "entities": []}, {"text": "It uses a combination of context, co-occurrence statistics and thesaurus to group the distributed but semantically related words to form basic semantic nodes.", "labels": [], "entities": []}, {"text": "The semantic nodes are then used to identify the topic.", "labels": [], "entities": []}, {"text": "This paper discusses the design, implementation and testing of an SPN for topic spotting.", "labels": [], "entities": [{"text": "SPN", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.964089035987854}, {"text": "topic spotting", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.7050842940807343}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses the topic representation, which is the prototype structure for SPN.", "labels": [], "entities": [{"text": "SPN", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.926173985004425}]}, {"text": "respectively discuss our approach to extract the semantic correlations between words, and build semantic groups and topic tree.", "labels": [], "entities": []}, {"text": "Section 5 describes the building and training of SPN, while Section 6 presents the experiment results.", "labels": [], "entities": [{"text": "SPN", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9670428037643433}]}, {"text": "Finally, Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We employ the ModApte Split version of Reuters-21578 corpus to test our method.", "labels": [], "entities": [{"text": "ModApte Split version of Reuters-21578 corpus", "start_pos": 14, "end_pos": 59, "type": "DATASET", "confidence": 0.868476798137029}]}, {"text": "In order to ensure that the training is meaningful, we select only those classes that have at least one document in each of the training and test sets.", "labels": [], "entities": []}, {"text": "This results in 90 classes in both the training and test sets.", "labels": [], "entities": []}, {"text": "After eliminating documents that do not belong to any of these 90 classes, we obtain a training set of 7,770 documents and a test set of 3,019 documents.", "labels": [], "entities": []}, {"text": "From the set of training documents, we derive the set of semantic nodes for each topic using the procedures outlined in Section 4.", "labels": [], "entities": []}, {"text": "From the training set, we found that the average number of semantic nodes for each topic is 132, and the average number of terms in each node is 2.4.", "labels": [], "entities": []}, {"text": "For illustration,: Examples of semantic nodes a) Under the topic \"wheat \", we list four semantic nodes.", "labels": [], "entities": []}, {"text": "Node 1 contains the common attribute set of the topic.", "labels": [], "entities": []}, {"text": "Node 2 is related to the \"buying and selling of wheat\".", "labels": [], "entities": []}, {"text": "Node 3 is related to \"wheat production\"; and node 4 is related to \"the effects of insect on wheat production\".", "labels": [], "entities": []}, {"text": "The results show that the automatically extracted basic semantic nodes are meaningful and are able to capture most semantics of a topic.", "labels": [], "entities": []}, {"text": "b) Node 1 originally contains two terms \"wheat\" and \"corn\" that belong to the same synset found by looking up WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.9675740003585815}]}, {"text": "However, in the training stage, the weight of the word \" corn\" was found to be very small in topic \"wheat\", and hence it was removed from the semantic group.", "labels": [], "entities": []}, {"text": "This is similar to the discourse based word sense disambiguation.", "labels": [], "entities": [{"text": "discourse based word sense disambiguation", "start_pos": 23, "end_pos": 64, "type": "TASK", "confidence": 0.6629956364631653}]}, {"text": "c) The granularity of information expressed by the semantic nodes may not be the same as what human expert produces.", "labels": [], "entities": []}, {"text": "For example, it is possible that a human expert may divide node 2 into two nodes {import} and {export, output}.", "labels": [], "entities": []}, {"text": "d) Node 5 contains four words and is formed by analyzing context.", "labels": [], "entities": []}, {"text": "Each context vector of the four words has the same two components: \"price\" and \" digital number\".", "labels": [], "entities": []}, {"text": "Meanwhile, \"rise\" and \"fall\" can also be grouped together by \"antonym\" relation.", "labels": [], "entities": []}, {"text": "\"fell\" is actually the past tense of \" fall\".", "labels": [], "entities": [{"text": "fell", "start_pos": 1, "end_pos": 5, "type": "METRIC", "confidence": 0.9123693108558655}]}, {"text": "This means that by comparing context, it is possible to group together those words with grammatical variations without performing grammatical analysis.", "labels": [], "entities": []}, {"text": "summarizes the results of SPN in terms of macro and micro F 1 values (see for definitions of the macro and micro F 1 values).", "labels": [], "entities": [{"text": "SPN", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9041564464569092}]}, {"text": "For comparison purpose, the lists the results of other TC methods as reported in.", "labels": [], "entities": [{"text": "TC", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.976326584815979}]}, {"text": "From the table, it can be seen that the SPN method achieves the best macF1 value.", "labels": [], "entities": []}, {"text": "This indicates that the method performs well on classes with a small number of training samples.", "labels": [], "entities": []}, {"text": "In terms of the micro F 1 measures, SPN outperforms NB, NNet, LSF and KNN, while posting a slightly lower performance than that of SVM.", "labels": [], "entities": []}, {"text": "The results are encouraging as they are rather preliminary.", "labels": [], "entities": []}, {"text": "We expect the results to improve further by tuning the system ranging from the initial values of various parameters, to the choice of error functions, context, grouping algorithm, and the structures of topic tree and SPN.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of semantic nodes  a) Under the topic \"wheat \", we list four semantic  nodes. Node 1 contains the common attribute  set of the topic. Node 2 is related to the \"buying  and selling of wheat\". Node 3 is related to  \"wheat production\"; and node 4 is related to  \"the effects of insect on wheat production\". The  results show that the automatically extracted  basic semantic nodes are meaningful and are  able to capture most semantics of a topic.  b) Node 1 originally contains two terms \"wheat\"  and \"corn\" that belong to the same synset found  by looking up WordNet. However, in the  training stage, the weight of the word \" corn\"  was found to be very small in topic \"wheat\",  and hence it was removed from the semantic  group. This is similar to the discourse based  word sense disambiguation.  c) The granularity of information expressed by the  semantic nodes may not be the same as what  human expert produces. For example, it is  possible that a human expert may divide node 2  into two nodes {import} and {export, output}.  d) Node 5 contains four words and is formed by  analyzing context. Each context vector of the  four words has the s ame two components:  \"price\" and \" digital number\". Meanwhile,  \"rise\" and \"fall\" can also be grouped together  by \"antonym\" relation. \"fell\" is actually the past  tense of \" fall\". This means that by comparing  context, it is possible to group together those  words with grammatical variations without  performing grammatical analysis.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 576, "end_pos": 583, "type": "DATASET", "confidence": 0.9410359263420105}]}, {"text": " Table 2. The performance comparison", "labels": [], "entities": []}]}