{"title": [{"text": "Fast Decoding and Optimal Decoding for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7785347104072571}]}], "abstractContent": [{"text": "A good decoding algorithm is critical to the success of any statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.6351842681566874}]}, {"text": "The decoder's job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them).", "labels": [], "entities": []}, {"text": "Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions.", "labels": [], "entities": []}, {"text": "In this paper , we compare the speed and output quality of a traditional stack-based decoding algorithm with two new de-coders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.", "labels": [], "entities": []}], "introductionContent": [{"text": "A statistical MT system that translates (say) French sentences into English, is divided into three parts: (1) a language model (LM) that assigns a probability P(e) to any English string, (2) a translation model (TM) that assigns a probability P(f\u00a4 e) to any pair of English and French strings, and (3) a decoder.", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9456627368927002}]}, {"text": "The decoder takes a previously unseen sentence \u00a5 and tries to find the \u00a6 that maximizes P(e \u00a4 f), or equivalently maximizes P(e) \u00a7 P (f\u00a4 e).", "labels": [], "entities": []}, {"text": "introduced a series of TMs based on word-for-word substitution and reordering, but did not include a decoding algorithm.", "labels": [], "entities": [{"text": "word-for-word substitution", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.6936059296131134}]}, {"text": "If the source and target languages are constrained to have the same word order (by choice or through suitable pre-processing), then the linear Viterbi algorithm can be applied ().", "labels": [], "entities": []}, {"text": "If re-ordering is limited to rotations around nodes in a binary tree, then optimal decoding can be carried out by a high-polynomial algorithm.", "labels": [], "entities": []}, {"text": "For arbitrary word-reordering, the decoding problem is NP-complete.", "labels": [], "entities": []}, {"text": "A sensible strategy) is to examine a large subset of likely decodings and choose just from that.", "labels": [], "entities": []}, {"text": "Of course, it is possible to miss a good translation this way.", "labels": [], "entities": [{"text": "translation", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.9662972688674927}]}, {"text": "If the decoder returns e\u00a8bute\u00a8but there exists some e for which P(e\u00a4 f) \u00a9 P(e\u00a8\u00a4 e\u00a8\u00a4 f), this is called a search error.", "labels": [], "entities": []}, {"text": "As remark, it is hard to know whether a search error has occurred-the only way to show that a decoding is sub-optimal is to actually produce a higherscoring one.", "labels": [], "entities": []}, {"text": "Thus, while decoding is a clear-cut optimization task in which every problem instance has aright answer, it is hard to come up with good answers quickly.", "labels": [], "entities": []}, {"text": "This paper reports on measurements of speed, search errors, and translation quality in the context of a traditional stack decoder) and two new decoders.", "labels": [], "entities": [{"text": "speed", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9767274260520935}]}, {"text": "The first is a fast greedy decoder, and the second is a slow optimal decoder based on generic mathematical programming techniques.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we used a test collection of 505 sentences, uniformly distributed across the lengths 6, 8, 10, 15, and 20.", "labels": [], "entities": []}, {"text": "We evaluated all decoders with respect to (1) speed, (2) search optimality, and (3) translation accuracy.", "labels": [], "entities": [{"text": "speed", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9847232103347778}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9008772969245911}]}, {"text": "The last two factors may not always coincide, as Model 4 is an imperfect model of the translation process-i.e., there is no guarantee that a numerically optimal decoding is actually a good translation.", "labels": [], "entities": []}, {"text": "Suppose , and neither is a perfect translation.", "labels": [], "entities": []}, {"text": "Here, \"perfect\" refers to a human-judged translation that transmits all of the meaning of the source sentence using flawless target-language syntax.", "labels": [], "entities": []}, {"text": "We have found it very useful to have several decoders on hand.", "labels": [], "entities": []}, {"text": "It is only through IP decoder output, for example, that we can know the stack decoder is returning optimal solutions for so many sentences (see).", "labels": [], "entities": []}, {"text": "The IP and stack decoders enabled us to quickly locate bugs in the greedy decoder, and to implement extensions to the basic greedy search that can find better solutions.", "labels": [], "entities": []}, {"text": "(We came up with the greedy operations discussed in Section 5 by carefully analyzing error logs of the kind shown in).", "labels": [], "entities": []}, {"text": "The results in also enable us to prioritize the items on our research agenda.", "labels": [], "entities": []}, {"text": "Since the majority of the translation errors can be attributed to the language and translation models we use (see column PME in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of decoders on sets of 101 test sentences. All experiments in this table use a  bigram language model.", "labels": [], "entities": []}, {"text": " Table 2: Comparison between decoders using a  trigram language model. Greedy\u00bf and greedy", "labels": [], "entities": []}]}