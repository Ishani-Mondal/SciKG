{"title": [{"text": "Scaling to Very Very Large Corpora for Natural Language Disambiguation", "labels": [], "entities": [{"text": "Natural Language Disambiguation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.7176975607872009}]}], "abstractContent": [{"text": "The amount of readily available on-line text has reached hundreds of billions of words and continues to grow.", "labels": [], "entities": []}, {"text": "Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less.", "labels": [], "entities": []}, {"text": "In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used.", "labels": [], "entities": [{"text": "confusion set disambiguation", "start_pos": 129, "end_pos": 157, "type": "TASK", "confidence": 0.5730587840080261}]}, {"text": "We are fortunate that for this particular application, correctly labeled training data is free.", "labels": [], "entities": []}, {"text": "Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine learning techniques, which automatically learn linguistic information from online text corpora, have been applied to a number of natural language problems throughout the last decade.", "labels": [], "entities": []}, {"text": "A large percentage of papers published in this area involve comparisons of different learning approaches trained and tested with commonly used corpora.", "labels": [], "entities": []}, {"text": "While the amount of available online text has been increasing at a dramatic rate, the size of training corpora typically used for learning has not.", "labels": [], "entities": []}, {"text": "In part, this is due to the standardization of data sets used within the field, as well as the potentially large cost of annotating data for those learning methods that rely on labeled text.", "labels": [], "entities": []}, {"text": "The empirical NLP community has put substantial effort into evaluating performance of a large number of machine learning methods over fixed, and relatively small, data sets.", "labels": [], "entities": []}, {"text": "Yet since we now have access to significantly more data, one has to wonder what conclusions that have been drawn on small data sets may carryover when these learning methods are trained using much larger corpora.", "labels": [], "entities": []}, {"text": "In this paper, we present a study of the effects of data size on machine learning for natural language disambiguation.", "labels": [], "entities": [{"text": "natural language disambiguation", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.6380752623081207}]}, {"text": "In particular, we study the problem of selection among confusable words, using orders of magnitude more training data than has ever been applied to this problem.", "labels": [], "entities": []}, {"text": "First we show learning curves for four different machine learning algorithms.", "labels": [], "entities": []}, {"text": "Next, we consider the efficacy of voting, sample selection and partially unsupervised learning with large training corpora, in hopes of being able to obtain the benefits that come from significantly larger training corpora without incurring too large a cost.", "labels": [], "entities": [{"text": "sample selection", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7351424396038055}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Committee Agreement vs. Accuracy", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9719796180725098}]}, {"text": " Table 3. Committee-Based Unsupervised Learning", "labels": [], "entities": []}, {"text": " Table 4. Choosing only the labeled  instances most likely to be correct as judged by  a committee of classifiers results in higher  accuracy than using all instances classified by a  model trained with the labeled seed corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9981813430786133}]}, {"text": " Table 4. Comparison of Unsupervised Learning  Methods", "labels": [], "entities": []}]}