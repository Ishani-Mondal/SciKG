{"title": [{"text": "From Chunks to Function-Argument Structure: A Similarity-Based Approach", "labels": [], "entities": []}], "abstractContent": [{"text": "Chunk parsing has focused on the recognition of partial constituent structures at the level of individual chunks.", "labels": [], "entities": [{"text": "Chunk parsing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6462682634592056}]}, {"text": "Little attention has been paid to the question of how such partial analyses can be combined into larger structures for complete utterances.", "labels": [], "entities": []}, {"text": "Such larger structures are not only desirable fora deeper syntactic analysis.", "labels": [], "entities": []}, {"text": "They also constitute a necessary prerequisite for assigning function-argument structure.", "labels": [], "entities": []}, {"text": "The present paper offers a similarity-based algorithm for assigning functional labels such as subject, object, head, complement, etc.", "labels": [], "entities": []}, {"text": "to complete syntactic structures on the basis of pre-chunked input.", "labels": [], "entities": []}, {"text": "The evaluation of the algorithm has concentrated on measuring the quality of functional labels.", "labels": [], "entities": []}, {"text": "It was performed on a German and an English treebank using two different annotation schemes at the level of function-argument structure.", "labels": [], "entities": []}, {"text": "The results of 89.73 % correct functional labels for German and 90.40 % for English validate the general approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current research on natural language parsing tends to gravitate toward one of two extremes: robust, partial parsing with the goal of broad data coverage versus more traditional parsers that aim at complete analysis fora narrowly defined set of data.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6499782701333364}]}, {"text": "Chunk parsing) offers a particularly promising and by now widely used example of the former kind.", "labels": [], "entities": [{"text": "Chunk parsing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.5450889766216278}]}, {"text": "The main insight that underlies the chunk parsing strategy is to isolate the (finite-state) analysis of non-recursive syntactic structure, i.e. chunks, from larger, recursive structures.", "labels": [], "entities": [{"text": "chunk parsing", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.7896817326545715}]}, {"text": "This results in a highly-efficient parsing architecture that is realized as a cascade of finite-state transducers and that pursues a leftmost longest-match patternmatching strategy at each level of analysis.", "labels": [], "entities": []}, {"text": "Despite the popularity of the chunk parsing approach, there seems to be a gap in current research: Chunk parsing research has focused on the recognition of partial constituent structures at the level of individual chunks.", "labels": [], "entities": [{"text": "chunk parsing", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.8411338329315186}, {"text": "Chunk parsing", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.7107502669095993}]}, {"text": "By comparison, little or no attention has been paid to the question of how such partial analyses can be combined into larger structures for complete utterances.", "labels": [], "entities": []}, {"text": "Such larger structures are not only desirable fora deeper syntactic analysis; they also constitute a necessary prerequisite for assigning function-argument structure.", "labels": [], "entities": []}, {"text": "Automatic assignment of function-argument structure has long been recognized as a desideratum beyond pure syntactic labeling . The present paper offers a similarity-1 With the exception of dependency-grammar-based parsers), where functional labels are treated as first-class citizens as relations between words, and recent work on a semi-automatic method for treebank construction (), little has been reported on based algorithm for assigning functional labels such as subject, object, head, complement, etc.", "labels": [], "entities": [{"text": "Automatic assignment of function-argument structure", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.7231100022792816}]}, {"text": "to complete syntactic structures on the basis of pre-chunked input.", "labels": [], "entities": []}, {"text": "The evaluation of the algorithm has concentrated on measuring the quality of these functional labels.", "labels": [], "entities": []}], "datasetContent": [{"text": "Quantitive evaluations of robust parsers typically focus on the three PARSEVAL measures: labeled precision, labeled recall and crossing accuracy.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9172868132591248}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.8966620564460754}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9479154348373413}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.7354742288589478}]}, {"text": "It has frequently been pointed out that these evaluation parameters provide little or no information as to whether a parser assigns the correct semantic structure to a given input, if the set of category labels comprises only syntactic categories in the narrow sense, i.e. includes only names of lexical and phrasal categories.", "labels": [], "entities": []}, {"text": "This justified criticism observes that a measure of semantic accuracy can only be obtained if the gold standard includes annotations of syntactic-semantic dependencies between bracketed constituents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9768556356430054}]}, {"text": "It is to answer this criticism that the evaluation of the T\u00fcSBL system presented here focuses on the correct assignment of functional labels.", "labels": [], "entities": []}, {"text": "For an in-depth evaluation that focuses on syntactic categories, we refer the interested reader to).", "labels": [], "entities": []}, {"text": "The quantitative evaluation of T\u00fcSBL has been conducted on the treebanks of German and English described in section 3.", "labels": [], "entities": [{"text": "T\u00fcSBL", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.5178573131561279}]}, {"text": "Each treebank uses a different annotation scheme at the level of function-argument structure.", "labels": [], "entities": []}, {"text": "As shown in, the English treebank uses a total of 13 functional labels, while the German treebank has a richer set of 36 function labels.", "labels": [], "entities": [{"text": "English treebank", "start_pos": 17, "end_pos": 33, "type": "DATASET", "confidence": 0.9136609435081482}, {"text": "German treebank", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.8694689273834229}]}, {"text": "The evaluation consisted of a ten-fold crossvalidation test, where the training data provide an instance base of already seen cases for T\u00fcSBL's tree construction module.", "labels": [], "entities": [{"text": "T\u00fcSBL's tree construction", "start_pos": 136, "end_pos": 161, "type": "TASK", "confidence": 0.7043519169092178}]}, {"text": "The evaluation was performed for both the German and English data.", "labels": [], "entities": []}, {"text": "For each language, the following parameters were measured: 1.", "labels": [], "entities": []}, {"text": "labeled precision for syntactic catconstruct tree(chunk list, treebank): while (chunk list is not empty) do remove first chunk from chunk list process chunk(chunk, treebank) Figure 5: Pseudo-code for tree construction, main routine.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9826410412788391}]}, {"text": "process chunk(chunk, treebank): words := string yield(chunk) tree := complete match(words, treebank) if (tree is not empty) direct hit, then output(tree) i.e. complete chunk found in treebank else tree := partial match(words, treebank) if (tree is not empty) then if (tree = postfix of chunk) then tree1 := attach next chunk(tree, treebank) if (tree is not empty) then tree := tree1 if ((chunk -tree) is not empty) if attach next chunk succeeded then tree := extend tree(chunk -tree, tree, treebank) chunk might consist of both chunks output(tree) if ((chunk -tree) is not empty) chunk might consist of both chunks (s.a.) then process chunk i.e. process remaining chunk else back off to POS sequence pos := pos yield(chunk) tree := complete match(pos, treebank) if (tree is not empty) then output(tree) else back off to subchunks while (chunk is not empty) do remove first subchunk c1 from chunk process chunk(c1, treebank) Figure 6: Pseudo-code for tree construction, subroutine process chunk.", "labels": [], "entities": []}, {"text": "attach next chunk(tree, treebank): attempts to attach the next chunk to the tree take first chunk chunk2 from chunk list words2 := string yield(tree, chunk2) tree2 := complete match(words2, treebank) if (tree2 is not empty) then remove chunk2 from chunk list return tree2 else return empty    ing approach like ours.", "labels": [], "entities": []}, {"text": "We have, therefore divided the incorrectly matched nodes into three categories: the genuine false positives where a tree structure is found that matches the gold standard, but is assigned the wrong label; nodes which, relative to the gold standard, remain unattached in the output tree; and nodes contained in the gold standard for which no match could be found in the parser output.", "labels": [], "entities": []}, {"text": "Our approach follows a strategy of positing and attaching nodes only if sufficient evidence can be found in the instance base.", "labels": [], "entities": []}, {"text": "Therefore the latter two categories cannot really be considered errors in the strict sense.", "labels": [], "entities": []}, {"text": "Nevertheless, in future research we will attempt to significantly reduce the proportion of unattached and unmatched nodes by exploring matching algorithms that permit a higher level of generalization when matching the input against the instance base.", "labels": [], "entities": []}, {"text": "What is encouraging about the recall results reported in is that the parser produces genuine false positives for an average of only 3.03 % for German and 3.25 % for English.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9986950755119324}]}, {"text": "For German, labeled precision for syntactic categories yielded 81.56 % correctness.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9217204451560974}, {"text": "correctness", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.9716749787330627}]}, {"text": "While these results do not reach the performance reported for other parsers), it is important to note that the two treebanks consist of transliterated spontaneous speech data.", "labels": [], "entities": []}, {"text": "The fragmentary and partially illformed nature of such spoken data makes them harder to analyze than written data such as the Penn treebank typically used as gold standard.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 126, "end_pos": 139, "type": "DATASET", "confidence": 0.9892673790454865}]}, {"text": "It should also be kept in mind that the basic PARSEVAL measures were developed for parsers that have as their main goal a complete analysis that spans the entire input.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.6794711947441101}]}, {"text": "This runs counter to the basic philosophy underlying an amended chunk parser such as T\u00fcSBL, which has as its main goal robustness of partially analyzed structures.", "labels": [], "entities": [{"text": "T\u00fcSBL", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.8351486325263977}]}, {"text": "Labeled precision of functional labels for the German data resulted in a score of 89.73 % correctness.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9835155606269836}, {"text": "German data", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.9237483739852905}, {"text": "correctness", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.9962537288665771}]}, {"text": "For English, precision of functional labels was 90.40 %.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996236562728882}]}, {"text": "The slightly lower correctness rate for German is a reflection of the larger set of function labels used by the grammar.", "labels": [], "entities": [{"text": "correctness rate", "start_pos": 19, "end_pos": 35, "type": "METRIC", "confidence": 0.9834615588188171}]}, {"text": "This raises interesting more general issues about trade-offs inaccuracy and granularity of functional annotations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The functional label set for the German and the English treebanks.", "labels": [], "entities": [{"text": "English treebanks", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.8489319384098053}]}]}