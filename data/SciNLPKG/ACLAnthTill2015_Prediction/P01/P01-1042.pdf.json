{"title": [{"text": "Joint and conditional estimation of tagging and parsing models *", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7445889910062155}]}], "abstractContent": [{"text": "This paper compares two different ways of estimating statistical language models.", "labels": [], "entities": [{"text": "estimating statistical language models", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.8289864808320999}]}, {"text": "Many statistical NLP tagging and parsing models are estimated by maximizing the (joint) likelihood of the fully-observed training data.", "labels": [], "entities": [{"text": "NLP tagging and parsing", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.7474968582391739}]}, {"text": "However , since these applications only require the conditional probability distributions , these distributions can in principle be learnt by maximizing the conditional likelihood of the training data.", "labels": [], "entities": []}, {"text": "Perhaps somewhat surprisingly, models estimated by maximizing the joint were superior to models estimated by maximizing the conditional, even though some of the latter models intuitively had access to \"more information\".", "labels": [], "entities": []}], "introductionContent": [{"text": "Many statistical NLP applications, such as tagging and parsing, involve finding the value of some hidden variable Y (e.g., a tag or a parse tree) which maximizes a conditional probability distribution P \u03b8 (Y |X), where X is a given word string.", "labels": [], "entities": [{"text": "parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.8694883584976196}]}, {"text": "The model parameters \u03b8 are typically estimated by maximum likelihood: i.e., maximizing the likelihood of the training * I would like to thank Eugene Charniak and the other members of BLLIP for their comments and suggestions.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 183, "end_pos": 188, "type": "DATASET", "confidence": 0.641553521156311}]}, {"text": "Fernando Pereira was especially generous with comments and suggestions, as were the ACL reviewers; I apologize for not being able to followup all of your good suggestions.", "labels": [], "entities": [{"text": "ACL reviewers", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.8720596134662628}]}, {"text": "This research was supported by NSF awards 9720368 and 9721276 and NIH award R01 MH60922-01A2. data.", "labels": [], "entities": [{"text": "NIH award R01 MH60922-01A2", "start_pos": 66, "end_pos": 92, "type": "METRIC", "confidence": 0.5628402605652809}]}, {"text": "Given a (fully observed) training corpus D = ((y 1 , x 1 ), . .", "labels": [], "entities": []}, {"text": ", (y n , x n )), the maximum (joint) likelihood estimate (MLE) of \u03b8 is: However, it turns out there is another maximum likelihood estimation method which maximizes the conditional likelihood or \"pseudo-likelihood\" of the training data.", "labels": [], "entities": [{"text": "joint) likelihood estimate (MLE)", "start_pos": 30, "end_pos": 62, "type": "METRIC", "confidence": 0.8338154171194349}]}, {"text": "Maximum conditional likelihood is consistent for the conditional distribution.", "labels": [], "entities": []}, {"text": "Given a training corpus D, the maximum conditional likelihood estimate (MCLE) of the model parameters \u03b8 is: Figure 1 graphically depicts the difference between the MLE and MCLE.", "labels": [], "entities": [{"text": "maximum conditional likelihood estimate (MCLE)", "start_pos": 31, "end_pos": 77, "type": "METRIC", "confidence": 0.8184206017426082}]}, {"text": "Let \u2126 be the universe of all possible pairs (y, x) of hidden and visible values.", "labels": [], "entities": []}, {"text": "Informally, the MLE selects the model parameter \u03b8 which make the training data pairs (y i , xi ) as likely as possible relative to all other pairs (y \ud97b\udf59 , x \ud97b\udf59 ) in \u2126.", "labels": [], "entities": []}, {"text": "The MCLE, on the other hand, selects the model parameter \u03b8 in order to make the training data pair (y i , xi ) more likely than other pairs (y \ud97b\udf59 , xi ) in \u2126, i.e., pairs with the same visible value xi as the training datum.", "labels": [], "entities": [{"text": "MCLE", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8652588129043579}]}, {"text": "In statistical computational linguistics, maximum conditional likelihood estimators have mostly been used with general exponential or \"maximum entropy\" models because standard maximum likelihood estimation is usually computationally intractable.", "labels": [], "entities": [{"text": "statistical computational linguistics", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.6943821509679159}, {"text": "maximum conditional likelihood estimators", "start_pos": 42, "end_pos": 83, "type": "TASK", "confidence": 0.5964883491396904}]}, {"text": "Wellknown computational linguistic models such as Maximum-Entropy Markov Models () and Stochastic Unification-based) are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of the estimated model.", "labels": [], "entities": []}, {"text": "It should be noted that in practice, the MCLE of a model with a large number of features with complex dependencies may yield far better performance than the MLE of the much smaller model that could be estimated with the same computational effort.", "labels": [], "entities": [{"text": "MLE", "start_pos": 157, "end_pos": 160, "type": "METRIC", "confidence": 0.9803005456924438}]}, {"text": "Nevertheless, as this paper shows, conditional estimators can be used with other kinds of models besides MaxEnt models, and in any event it is interesting to ask whether the MLE differs from the MCLE in actual applications, and if so, how.", "labels": [], "entities": []}, {"text": "Because the MLE is consistent for the joint distribution P(Y, X) (e.g., in a tagging application, the distribution of word-tag sequences), it is also consistent for the conditional distribution P(Y |X) (e.g., the distribution of tag sequences given word sequences) and the marginal distribution P(X) (e.g., the distribution of word strings).", "labels": [], "entities": []}, {"text": "On the other hand, the MCLE is consistent for the conditional distribution P(Y |X) alone, and provides no information about either the joint or the marginal distributions.", "labels": [], "entities": [{"text": "MCLE", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.4894137382507324}]}, {"text": "Applications such as language modelling for speech recognition and EM procedures for estimating from hidden data either explicitly or implicitly require marginal distributions over the visible data (i.e., word strings), so it is not statistically sound to use MCLEs for such applications.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7244028747081757}, {"text": "speech recognition", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.686946377158165}]}, {"text": "On the other hand, applications which involve predicting the value of the hidden variable from the visible variable (such as tagging or parsing) usually only involve the conditional distribution, which the MCLE estimates directly.", "labels": [], "entities": [{"text": "tagging or parsing", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.6535224914550781}]}, {"text": "Since both the MLE and MCLE are consistent for the conditional distribution, both converge in the limit to the \"true\" distribution if the true distribution is in the model class.", "labels": [], "entities": [{"text": "MLE", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.7987068295478821}, {"text": "MCLE", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.6620525121688843}]}, {"text": "However, given that we often have insufficient data in computational linguistics, and there are good reasons to believe that the true distribution of sentences or parses cannot be described by our models, there is no reason to expect these asymptotic results to hold in practice, and in the experiments reported below the MLE and MCLE behave differently experimentally.", "labels": [], "entities": [{"text": "MLE", "start_pos": 322, "end_pos": 325, "type": "METRIC", "confidence": 0.623305082321167}]}, {"text": "A priori, one can advance plausible arguments in favour of both the MLE and the MCLE.", "labels": [], "entities": [{"text": "MLE", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.5924810767173767}, {"text": "MCLE", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.8729536533355713}]}, {"text": "Informally, the MLE and the MCLE differ in the following way.", "labels": [], "entities": [{"text": "MLE", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.6514394283294678}, {"text": "MCLE", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.6302647590637207}]}, {"text": "Since the MLE is obtained by maximizing , the MLE exploits information about the distribution of word strings xi in the training data that the MCLE does not.", "labels": [], "entities": []}, {"text": "Thus one might expect the MLE to converge faster than the MCLE in situations where training data is not over-abundant, which is often the casein computational linguistics.", "labels": [], "entities": []}, {"text": "On the other hand, since the intended application requires a conditional distribution, it seems reasonable to directly estimate this conditional distribution from the training data as the MCLE does.", "labels": [], "entities": [{"text": "MCLE", "start_pos": 188, "end_pos": 192, "type": "DATASET", "confidence": 0.8989561796188354}]}, {"text": "Furthermore, suppose that the model class is wrong (as is surely true of all our current language models), i.e., the \"true\" model P(Y, X) \ud97b\udf59 = P \u03b8 (Y, X) for all \u03b8, and that our best models are particularly poor approximations to the true distribution of word strings P(X).", "labels": [], "entities": []}, {"text": "Then ignoring the distribution of word strings in the training data as the MCLE does might indeed be a reasonable thing to do.", "labels": [], "entities": [{"text": "MCLE", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.8844338059425354}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "The next section formulates the MCLEs for HMMs and PCFGs as constrained optimization problems and describes an iterative dynamicprogramming method for solving them.", "labels": [], "entities": []}, {"text": "Because of the computational complexity of these problems, the method is only applied to a simple PCFG based on the ATIS corpus.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 116, "end_pos": 127, "type": "DATASET", "confidence": 0.9795577526092529}]}, {"text": "For this example, the MCLE PCFG does perhaps produce slightly better parsing results than the standard MLE (relative-frequency) PCFG, although the result does not reach statistical significance.", "labels": [], "entities": [{"text": "MCLE PCFG", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.8283424973487854}, {"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9651899933815002}]}, {"text": "It seems to be difficult to find model classes for which the MLE and MCLE are both easy to compute.", "labels": [], "entities": [{"text": "MLE", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.457186222076416}, {"text": "MCLE", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.45750701427459717}]}, {"text": "However, often it is possible to find two closely related model classes, one of which has an easily computed MLE and the other which has an easily computed MCLE.", "labels": [], "entities": [{"text": "MLE", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9110051393508911}]}, {"text": "Typically, the model classes which have an easily computed MLE define joint probability distributions over both the hidden and the visible data (e.g., over wordtag pair sequences for tagging), while the model classes which have an easily computed MCLE define conditional probability distributions over the hidden data given the visible data (e.g., over tag sequences given word sequences).", "labels": [], "entities": []}, {"text": "Section 3 investigates closely related joint and conditional tagging models (the latter can be regarded as a simplification of the Maximum Entropy Markov Models of), and shows that MLEs outperform the MCLEs in this application.", "labels": [], "entities": []}, {"text": "The final empirical section investigates two different kinds of stochastic shift-reduce parsers, and shows that the model estimated by the MLE outperforms the model estimated by the MCLE.", "labels": [], "entities": [{"text": "MLE", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.602281928062439}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The likelihood P(\ud97b\udf59 y) and conditional likelihood", "labels": [], "entities": [{"text": "likelihood P(\ud97b\udf59 y)", "start_pos": 14, "end_pos": 31, "type": "METRIC", "confidence": 0.8906544208526611}]}, {"text": " Table 2: Labelled precision and recall results for joint and", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9802976846694946}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9988483190536499}]}]}