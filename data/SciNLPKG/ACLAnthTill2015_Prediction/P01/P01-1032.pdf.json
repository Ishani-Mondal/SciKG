{"title": [{"text": "Mapping Lexical Entries in a Verbs Database to WordNet Senses", "labels": [], "entities": [{"text": "Mapping Lexical Entries", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8453947305679321}]}], "abstractContent": [{"text": "This paper describes automatic techniques for mapping 9611 entries in a database of English verbs to Word-Net senses.", "labels": [], "entities": []}, {"text": "The verbs were initially grouped into 491 classes based on syntactic features.", "labels": [], "entities": []}, {"text": "Mapping these verbs into WordNet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and cross-language information retrieval.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.8121175467967987}, {"text": "cross-language information retrieval", "start_pos": 150, "end_pos": 186, "type": "TASK", "confidence": 0.7108337084452311}]}, {"text": "Our techniques make use of (1) a training set of 1791 disambiguated entries, representing 1442 verb entries from 167 classes; (2) word sense probabilities, from frequency counts in a tagged corpus; (3) semantic similarity of WordNet senses for verbs within the same class; (4) probabilistic correlations between WordNet data and attributes of the verb classes.", "labels": [], "entities": [{"text": "WordNet data", "start_pos": 312, "end_pos": 324, "type": "DATASET", "confidence": 0.952743649482727}]}, {"text": "The best results achieved 72% precision and 58% recall, versus a lower bound of 62% precision and 38% recall for assigning the most frequently occurring WordNet sense, and an upper bound of 87% precision and 75% recall for human judgment.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9995065927505493}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9994070529937744}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9978278279304504}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.996207594871521}, {"text": "assigning the most frequently occurring WordNet sense", "start_pos": 113, "end_pos": 166, "type": "TASK", "confidence": 0.612871242421014}, {"text": "precision", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.9983653426170349}, {"text": "recall", "start_pos": 212, "end_pos": 218, "type": "METRIC", "confidence": 0.9983342289924622}]}], "introductionContent": [{"text": "Our goal is to map entries in a lexical database of 4076 English verbs automatically to WordNet senses,) to support such applications as machine translation and cross-language information retrieval.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.8166508078575134}, {"text": "cross-language information retrieval", "start_pos": 161, "end_pos": 197, "type": "TASK", "confidence": 0.7678645849227905}]}, {"text": "For example, the verb drop is multiply ambiguous, with many potential translations in Spanish: bajar, caerse, dejar caer, derribar, disminuir, echar, hundir, soltar, etc.", "labels": [], "entities": []}, {"text": "The database specifies a set of interpretations for drop, depending on its context in the source-language (SL).", "labels": [], "entities": []}, {"text": "Inclusion of WordNet senses in the database enables the selection of an appropriate verb in the target language (TL).", "labels": [], "entities": []}, {"text": "Final selection is based on a frequency count of WordNet senses across all classes to which the verb belongs-e.g., disminuir is selected when the WordNet sense corresponds to the meaning of drop in Prices dropped.", "labels": [], "entities": []}, {"text": "Our task differs from standard word sense disambiguation (WSD) in several ways.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.7768787940343221}]}, {"text": "First, the words to be disambiguated are entries in a lexical database, not tokens in a text corpus.", "labels": [], "entities": []}, {"text": "Second, we take an \"all-words\" rather than a \"lexical-sample\" approach): All words in the lexical database \"text\" are disambiguated, not just a small number for which detailed knowledge is available.", "labels": [], "entities": []}, {"text": "Third, we replace the contextual data typically used for WSD with information about verb senses encoded in terms of thematic grids and lexical-semantic representations from ( ).", "labels": [], "entities": [{"text": "WSD", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9133907556533813}]}, {"text": "Fourth, whereas a single word sense for each token in a text corpus is often assumed, the absence of sentential context leads to a situation where several WordNet senses maybe equally appropriate fora database entry.", "labels": [], "entities": []}, {"text": "Indeed, as distinctions between WordNet senses can be fine-grained), it maybe unclear, even in context, which sense is meant.", "labels": [], "entities": []}, {"text": "The verb database contains mostly syntactic information about its entries, much of which applies at the class level within the database.", "labels": [], "entities": []}, {"text": "WordNet, on the other hand, is a significant source for information about semantic relationships, much of which applies at the \"synset\" level (\"synsets\" are WordNet's groupings of synonymous word senses).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9737105369567871}]}, {"text": "Mapping entries in the database to their corresponding WordNet senses greatly extends the semantic potential of the database.", "labels": [], "entities": []}], "datasetContent": [{"text": "Subsequent to the culling of the training set, several processes were undertaken that resulted in full mapping of entries in the lexical database to WordNet senses.", "labels": [], "entities": []}, {"text": "Much, but not all, of this mapping was accomplished manually.", "labels": [], "entities": []}, {"text": "Each entry whose WordNet senses were assigned manually was considered by at least two coders, one coder who was involved in the entire manual assignment process and the other drawn from a handful of coders working independently on different subsets of the verb lexicon.", "labels": [], "entities": []}, {"text": "In the manual tagging, if a WordNet sense was considered appropriate fora lexical entry by anyone of the coders, it was assigned.", "labels": [], "entities": []}, {"text": "Overall, 13452 WordNet sense assignments were made.", "labels": [], "entities": [{"text": "WordNet sense assignments", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.6967288057009379}]}, {"text": "Of these, 51% were agreed upon by multiple coders.", "labels": [], "entities": []}, {"text": "The kappa coefficient (| ) of intercoder agreement was .47 fora first round of manual tagging and (only) .24 fora second round of more problematic cases.", "labels": [], "entities": [{"text": "kappa coefficient", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.943035900592804}]}, {"text": "While the full tagging of the lexical database may make the automatic tagging task appear superfluous, the low rate of agreement between coders and the automatic nature of some of the tagging suggest there is still room for adjustment of WordNet sense assignments in the verb database.", "labels": [], "entities": []}, {"text": "On the one hand, even the higher of the kappa coefficients mentioned above is significantly lower than the standard suggested for good reliability (| ~ } \u0080 \u007f \u0082 \u0081 ) or even the level where tentative conclusions maybe drawn ( ),.", "labels": [], "entities": [{"text": "reliability", "start_pos": 135, "end_pos": 146, "type": "METRIC", "confidence": 0.9651724100112915}]}, {"text": "On the other hand, if the automatic assignments agree with human coding at levels comparable to the degree of agreement among humans, it maybe used to identify current assignments that need review The kappa statistic measures the degree to which pairwise agreement of coders on a classification task surpasses what would be expected by chance; the standard definition of this coefficient is: , where is the actual percentage of agreement and is the expected percentage of agreement, averaged overall pairs of assignments.", "labels": [], "entities": []}, {"text": "Several adjustments in the computation of the kappa coefficient were made necessary by the possible assignment of multiple senses for each verb in a Levin+ class, since without prior knowledge of how many senses are to be assigned, there is no basis on which to compute and to suggest new assignments for consideration.", "labels": [], "entities": []}, {"text": "In addition, consistency checking is done more easily by machine than by hand.", "labels": [], "entities": [{"text": "consistency checking", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.748952329158783}]}, {"text": "For example, the same-synset assumption is more easily enforced automatically than manually.", "labels": [], "entities": []}, {"text": "When this assumption is implemented for the 2756 senses in the training set, another 967 sense assignments are generated, only 131 of which were actually assigned manually.", "labels": [], "entities": []}, {"text": "Similarly, when this premise is enforced on the entirety of the lexical database of 13452 assignments, another 5059 sense assignments are generated.", "labels": [], "entities": []}, {"text": "If the same-synset assumption is valid and if the senses assigned in the database are accurate, then the human tagging has a recall of no more than 73%.", "labels": [], "entities": [{"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.999352753162384}]}, {"text": "Because a word sense was assigned even if only one coder judged it to apply, human coding has been treated as having a precision of 100%.", "labels": [], "entities": [{"text": "human coding", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.7094157040119171}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9988126754760742}]}, {"text": "However, some of the solo judgments are likely to have been in error.", "labels": [], "entities": []}, {"text": "To determine what proportion of such judgments were in reality precision failures, a random sample of 50 WordNet senses selected by only one of the two original coders was investigated further by a team of three judges.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9854971766471863}]}, {"text": "In this round, judges rated WordNet senses assigned to verb entries as falling into one of three categories: definitely correct, definitely incorrect, and arguable whether correct.", "labels": [], "entities": []}, {"text": "As it turned out, if anyone of the judges rated a sense definitely correct, another judge independently judged it definitely correct; this accounts for 31 instances.", "labels": [], "entities": []}, {"text": "In 13 instances the assignments were judged definitely incorrect by at least two of the judges.", "labels": [], "entities": []}, {"text": "No consensus was reached on the remaining 6 instances.", "labels": [], "entities": []}, {"text": "Extrapolating from this sample to the full set of solo judgments in the database leads to an estimate that approximately 1725 (26% of 6636 solo judgments) of those senses are incorrect.", "labels": [], "entities": []}, {"text": "This suggests that the precision of the human coding is approximately 87%.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9995900988578796}]}, {"text": "The upper bound for this task, asset by human performance, is thus 73% recall and 87% precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9993065595626831}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9972636699676514}]}, {"text": "The lower bound, based on assigning the WordNet sense with the greatest prior probability, is 38% recall and 62% precision.", "labels": [], "entities": [{"text": "WordNet sense", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.8997567892074585}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9996193647384644}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9987707734107971}]}], "tableCaptions": [{"text": " Table 2: Recall (R) and Precision (P) for Majority  Voting Scheme, Before (W/O) and After (W/) En- forcement of the Same-Synset (SS) Assumption", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9345355480909348}, {"text": "Precision (P)", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.9437245726585388}, {"text": "After", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.9726682305335999}, {"text": "Same-Synset (SS) Assumption", "start_pos": 117, "end_pos": 144, "type": "TASK", "confidence": 0.5592812240123749}]}, {"text": " Table 3: Recall (R) and Precision (P) for Thresh- old Voting Scheme", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9422753006219864}, {"text": "Precision (P)", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.9619123786687851}, {"text": "Thresh- old Voting", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.6500891596078873}]}]}