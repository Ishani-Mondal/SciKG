{"title": [{"text": "XML-Based Data Preparation for Robust Deep Parsing", "labels": [], "entities": [{"text": "XML-Based Data Preparation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5732817550500234}, {"text": "Robust Deep Parsing", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7537668943405151}]}], "abstractContent": [{"text": "We describe the use of XML tokenisa-tion, tagging and markup tools to prepare a corpus for parsing.", "labels": [], "entities": []}, {"text": "Our techniques are generally applicable but here we focus on parsing Medline abstracts with the ANLT wide-coverage grammar.", "labels": [], "entities": [{"text": "parsing Medline abstracts", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.6552939017613729}, {"text": "ANLT wide-coverage grammar", "start_pos": 96, "end_pos": 122, "type": "TASK", "confidence": 0.5212553938229879}]}, {"text": "Hand-crafted grammars inevitably lack coverage but many coverage failures are due to inadequacies of their lexicons.", "labels": [], "entities": [{"text": "coverage", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9676263332366943}]}, {"text": "We describe a method of gaining a degree of robustness by interfacing POS tag information with the existing lexicon.", "labels": [], "entities": []}, {"text": "We also show that XML tools provide a sophisticated approach to pre-processing, helping to ameliorate the 'messiness' in real language data and improve parse performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The field of parsing technology currently has two distinct strands of research with few points of contact between them.", "labels": [], "entities": [{"text": "parsing technology", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.9363599121570587}]}, {"text": "On the one hand, there is thriving research on shallow parsing, chunking and induction of statistical syntactic analysers from treebanks; and on the other hand, there are systems which use hand-crafted grammars which provide both syntactic and semantic coverage.", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.6184786260128021}]}, {"text": "'Shallow' approaches have good coverage on corpus data, but extensions to semantic analysis are still in a relative infancy.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.8767619431018829}]}, {"text": "The 'deep' strand of research has two main problems: inadequate coverage, and alack of reliable techniques to select the correct parse.", "labels": [], "entities": [{"text": "coverage", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9566107988357544}]}, {"text": "In this paper we describe ongoing research which uses hybrid technologies to address the problem of inadequate coverage of a 'deep' parsing system.", "labels": [], "entities": []}, {"text": "In Section 2 we describe how we have modified an existing hand-crafted grammar's look-up procedure to utilise part-ofspeech (POS) tag information, thereby ameliorating the lexical information shortfall.", "labels": [], "entities": []}, {"text": "In Section 3 we describe how we combine a variety of existing NLP tools to pre-process real data up to the point where a hand-crafted grammar can start to be useful.", "labels": [], "entities": []}, {"text": "The work described in both sections is enabled by the use of an XML processing paradigm whereby the corpus is converted to XML with analysis results encoded as XML annotations.", "labels": [], "entities": []}, {"text": "In Section 4 we report on an experiment with a random sample of 200 sentences which gives an approximate measure of the increase in performance we have gained.", "labels": [], "entities": []}, {"text": "The work we describe here is part of a project which aims to combine statistical and symbolic processing techniques to compute lexical semantic relationships, e.g. the semantic relations between nouns in complex nominals.", "labels": [], "entities": []}, {"text": "We have chosen the medical domain because the field of medical informatics provides a relative abundance of pre-existing knowledge bases and ontologies.", "labels": [], "entities": []}, {"text": "Our efforts so far have focused on the OHSUMED corpus ( which is a collection of Medline abstracts of medical journal papers.", "labels": [], "entities": [{"text": "OHSUMED corpus", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.9167932868003845}, {"text": "Medline abstracts of medical journal papers", "start_pos": 81, "end_pos": 124, "type": "DATASET", "confidence": 0.7735060652097067}]}, {"text": "While the focus of the project is on semantic issues, a prerequisite is a large, reliably annotated corpus and a level of syntactic process-ing that supports the computation of semantics.", "labels": [], "entities": []}, {"text": "The computation of 'grammatical relations' from shallow parsers or chunkers is still at an early stage ( and there are few other robust semantic processors, and none in the medical domain.", "labels": [], "entities": [{"text": "computation of 'grammatical relations' from shallow parsers or chunkers", "start_pos": 4, "end_pos": 75, "type": "TASK", "confidence": 0.6971454441547393}]}, {"text": "We have therefore chosen to re-use an existing handcrafted grammar which produces compositionally derived underspecified logical forms, namely the wide-coverage grammar, morphological analyser and lexicon provided by the Alvey Natural Language Tools (ANLT) system.", "labels": [], "entities": [{"text": "Alvey Natural Language Tools (ANLT) system", "start_pos": 221, "end_pos": 263, "type": "DATASET", "confidence": 0.6293597109615803}]}, {"text": "Our immediate aim is to increase coverage up to a reasonable level and thereafter to experiment with ranking the parses, e.g. using  probabilistic extension of the ANLT software.", "labels": [], "entities": [{"text": "coverage", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9789793491363525}, {"text": "ANLT software", "start_pos": 164, "end_pos": 177, "type": "DATASET", "confidence": 0.6878208667039871}]}, {"text": "We use XML as the preprocessing mark-up technology, specifically the LT TTT and LT XML tools ().", "labels": [], "entities": [{"text": "LT TTT", "start_pos": 69, "end_pos": 75, "type": "DATASET", "confidence": 0.9169971942901611}]}, {"text": "In the initial stages of the project we converted the OHSUMED corpus into XML annotated format with mark-up that encodes word tokens, POS tags, lemmatisation information etc.", "labels": [], "entities": [{"text": "OHSUMED corpus", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.9384814202785492}]}, {"text": "The research reported here builds on that mark-up in a further stage of pre-processing prior to parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 96, "end_pos": 103, "type": "TASK", "confidence": 0.971833348274231}]}, {"text": "The XML paradigm has proved invaluable throughout.", "labels": [], "entities": []}], "datasetContent": [{"text": "With a corpus such as OHSUMED where there is no gold-standard tagged or hand-parsed subpart, it is hard to reliably evaluate our system.", "labels": [], "entities": [{"text": "OHSUMED", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.9111699461936951}]}, {"text": "However, we did an experiment on 200 sentences taken at random from the corpus (average sentence length: 21 words).", "labels": [], "entities": []}, {"text": "We ran three versions of our pre-processor over the 200 sentences to produce three different input files for the parser and for each input we counted the sentences which were assigned at least one parse.", "labels": [], "entities": []}, {"text": "All three versions started from the same basic XML annotated data, where words were tagged by both taggers and parenthesised material was removed.", "labels": [], "entities": []}, {"text": "Version 1 converted from this format to ANLT input simply by discarding the mark-up and separating off punctuation.", "labels": [], "entities": [{"text": "ANLT input", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.6022617220878601}]}, {"text": "Version 2 was the same except that content word POS tags were retained.", "labels": [], "entities": []}, {"text": "Version 3 was put through our full pipeline which recognises formulae, numbers etc. and which corrects some tagging errors.", "labels": [], "entities": []}, {"text": "The following table shows numbers of sentences successfully parsed with each of the three different inputs:", "labels": [], "entities": []}], "tableCaptions": []}