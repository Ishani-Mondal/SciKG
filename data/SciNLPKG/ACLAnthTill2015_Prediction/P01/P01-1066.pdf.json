{"title": [{"text": "Quantitative and Qualitative Evaluation of Darpa Communicator Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Darpa Communicator Spoken Dialogue Systems", "start_pos": 43, "end_pos": 85, "type": "DATASET", "confidence": 0.8927495121955872}]}], "abstractContent": [{"text": "This paper describes the application of the PARADISE evaluation framework to the corpus of 662 human-computer dialogues collected in the June 2000 Darpa Communicator data collection.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.6983377933502197}, {"text": "Darpa Communicator data collection", "start_pos": 147, "end_pos": 181, "type": "DATASET", "confidence": 0.9125357270240784}]}, {"text": "We describe results based on the standard logfile metrics as well as results based on additional qualitative metrics derived using the DATE dialogue act tagging scheme.", "labels": [], "entities": [{"text": "DATE dialogue act tagging", "start_pos": 135, "end_pos": 160, "type": "TASK", "confidence": 0.6220377087593079}]}, {"text": "We show that performance models derived via using the standard metrics can account for 37% of the variance in user satisfaction, and that the addition of DATE metrics improved the models by an absolute 5%.", "labels": [], "entities": [{"text": "DATE", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.7536541223526001}]}], "introductionContent": [{"text": "The objective of the DARPA COMMUNICATOR program is to support research on multi-modal speech-enabled dialogue systems with advanced conversational capabilities.", "labels": [], "entities": []}, {"text": "In order to make this a reality, it is important to understand the contribution of various techniques to users' willingness and ability to use a spoken dialogue system.", "labels": [], "entities": []}, {"text": "In June of 2000, we conducted an exploratory data collection experiment with nine participating communicator systems.", "labels": [], "entities": [{"text": "exploratory data collection", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.6546013752619425}]}, {"text": "All systems supported travel planning and utilized some form of mixedinitiative interaction.", "labels": [], "entities": [{"text": "travel planning", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7464635968208313}]}, {"text": "However the systems varied in several critical dimensions: (1) They targeted different back-end databases for travel information; (2) System modules such as ASR, NLU, TTS and dialogue management were typically different across systems.", "labels": [], "entities": [{"text": "ASR", "start_pos": 157, "end_pos": 160, "type": "TASK", "confidence": 0.7384878396987915}, {"text": "dialogue management", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.8385356068611145}]}, {"text": "The Evaluation Committee chaired by Walker (, with representatives from the nine COMMUNICATOR sites and from NIST, developed the experimental design.", "labels": [], "entities": [{"text": "NIST", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.9529968500137329}]}, {"text": "A logfile standard was developed by MITRE along with a set of tools for processing the logfiles; the standard and tools were used by all sites to collect a set of core metrics for making cross system comparisons.", "labels": [], "entities": [{"text": "MITRE", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.9389291405677795}]}, {"text": "The core metrics were developed during a workshop of the Evaluation Committee and included all metrics that anyone in the committee suggested, that could be implemented consistently across systems.", "labels": [], "entities": []}, {"text": "NIST's contribution was to recruit the human subjects and to implement the experimental design specified by the Evaluation Committee.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.968871533870697}]}, {"text": "The experiment was designed to make it possible to apply the PARADISE evaluation framework (), which integrates and unifies previous approaches to evaluation).", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.7295041084289551}]}, {"text": "The framework posits that user satisfaction is the overall objective to be maximized and that task success and various interaction costs can be used as predictors of user satisfaction.", "labels": [], "entities": []}, {"text": "Our results from applying PARADISE include that user satisfaction differed considerably across the nine systems.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.8770657777786255}]}, {"text": "Subsequent modeling of user satisfaction gave us some insight into why each system was more or less satisfactory; four variables accounted for 37% of the variance in user-satisfaction: task completion, task duration, recognition accuracy, and mean system turn duration.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 229, "end_pos": 237, "type": "METRIC", "confidence": 0.8725207448005676}]}, {"text": "However, when doing our analysis we were struck by the extent to which different aspects of the systems' dialogue behavior weren't captured by the core metrics.", "labels": [], "entities": []}, {"text": "For example, the core metrics logged the number and duration of system turns, but didn't distinguish between turns used to request or present information, to give instructions, or to indicate errors.", "labels": [], "entities": []}, {"text": "Recent research on dialogue has been based on the assumption that dialogue acts provide a useful way of characterizing dialogue behaviors.", "labels": [], "entities": []}, {"text": "Several research efforts have explored the use of dialogue act tagging schemes for tasks such as improving recognition performance), identifying important parts of a dialogue (, and as a constraint on nominal expression generation).", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6351775328318278}, {"text": "nominal expression generation", "start_pos": 201, "end_pos": 230, "type": "TASK", "confidence": 0.6354334453741709}]}, {"text": "Thus we decided to explore the application of a dialogue act tagging scheme to the task of evaluating and comparing dialogue systems.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.6475082735220591}]}, {"text": "Section 2 describes the corpus.", "labels": [], "entities": []}, {"text": "Section 3 describes the dialogue act tagging scheme we developed and applied to the evaluation of COM-MUNICATOR dialogues.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.6947054465611776}]}, {"text": "Section 4 first describes our results utilizing the standard logged metrics, and then describes results using the DATE metrics.", "labels": [], "entities": [{"text": "DATE", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.4540262818336487}]}, {"text": "Section 5 discusses future plans.", "labels": [], "entities": []}], "datasetContent": [{"text": "The hypothesis underlying the application of dialogue act tagging to system evaluation is that a system's dialogue behaviors have a strong effect on the usability of a spoken dialogue system.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.6394534508387247}]}, {"text": "However, each COMMUNICATOR system has a unique dialogue strategy and a unique way of achieving particular communicative goals.", "labels": [], "entities": []}, {"text": "Thus, in order to explore this hypothesis, we needed away of characterizing system dialogue behaviors that could be applied uniformly across the nine different communicator travel planning systems.", "labels": [], "entities": []}, {"text": "We developed a dialogue act tagging scheme for this purpose which we call DATE (Dialogue Act Tagging for Evaluation).", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.6175928711891174}, {"text": "Dialogue Act Tagging for Evaluation)", "start_pos": 80, "end_pos": 116, "type": "TASK", "confidence": 0.7254437307516733}]}, {"text": "In developing DATE, we believed that it was important to allow for multiple views of each dialogue act.", "labels": [], "entities": [{"text": "DATE", "start_pos": 14, "end_pos": 18, "type": "TASK", "confidence": 0.8616618514060974}]}, {"text": "This would allow us, for example, to investigate what part of the task an utterance contributes to separately from what speech act function it serves.", "labels": [], "entities": []}, {"text": "Thus, a central aspect of DATE is that it makes distinctions within three orthogonal dimensions of utterance classification: (1) a SPEECH-ACT dimension; (2) a TASK-SUBTASK dimension; and (3) a CONVERSATIONAL-DOMAIN dimension.", "labels": [], "entities": [{"text": "DATE", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.9638069868087769}, {"text": "utterance classification", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.7904181182384491}, {"text": "SPEECH-ACT dimension", "start_pos": 131, "end_pos": 151, "type": "METRIC", "confidence": 0.9532552659511566}, {"text": "TASK-SUBTASK dimension", "start_pos": 159, "end_pos": 181, "type": "METRIC", "confidence": 0.9691581428050995}]}, {"text": "We believe that these distinctions are important for using such a scheme for evaluation.", "labels": [], "entities": []}, {"text": "shows a COMMUNICATOR dialogue with each system utterance classified on these three dimensions.", "labels": [], "entities": []}, {"text": "The tagset for each dimension are briefly described in the remainder of this section.", "labels": [], "entities": []}, {"text": "See) for more detail.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Predictive power and significance of  Core Metrics", "labels": [], "entities": [{"text": "Predictive", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9467484951019287}]}, {"text": " Table 2: Predictive power and significance of Di- alogue Act Metrics", "labels": [], "entities": [{"text": "Predictive", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9594439268112183}, {"text": "Di- alogue Act Metrics", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.6979441583156586}]}]}