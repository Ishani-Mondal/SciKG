{"title": [{"text": "Evaluating a Trainable Sentence Planner fora Spoken Dialogue System", "labels": [], "entities": []}], "abstractContent": [{"text": "Techniques for automatically training modules of a natural language generator have recently been proposed, but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches.", "labels": [], "entities": []}, {"text": "In this paper We experimentally evaluate a trainable sentence planner fora spoken dialogue system by eliciting subjective human judgments.", "labels": [], "entities": []}, {"text": "In order to perform an exhaustive comparison, we also evaluate a hand-crafted template-based generation component, two rule-based sentence planners, and two baseline sentence planners.", "labels": [], "entities": []}, {"text": "We show that the train-able sentence planner performs better than the rule-based systems and the baselines, and as well as the hand-crafted system.", "labels": [], "entities": [{"text": "sentence planner", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.6614408940076828}]}], "introductionContent": [{"text": "The past several years have seen a large increase in commercial dialog systems.", "labels": [], "entities": []}, {"text": "These systems typically use system-initiative dialog strategies, with system utterances highly scripted for style and register and recorded by voice talent.", "labels": [], "entities": []}, {"text": "However several factors argue against the continued use of these simple techniques for producing the system side of the conversation.", "labels": [], "entities": []}, {"text": "First, text-tospeech has improved to the point of being a viable alternative to pre-recorded prompts.", "labels": [], "entities": []}, {"text": "Second, there is a perceived need for spoken dialog systems to be more flexible and support user initiative, but this requires greater flexibility in utterance generation.", "labels": [], "entities": [{"text": "utterance generation", "start_pos": 150, "end_pos": 170, "type": "TASK", "confidence": 0.8519855737686157}]}, {"text": "Finally, systems to support complex planning are being developed, which will require more sophisticated output.", "labels": [], "entities": []}, {"text": "As we move away from systems with prerecorded prompts, there are two possible approaches to producing system utterances.", "labels": [], "entities": []}, {"text": "The first is template-based generation, where utterances are produced from hand-crafted string templates.", "labels": [], "entities": [{"text": "template-based generation", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.7094819098711014}]}, {"text": "Most current research systems use template-based generation because it is conceptually straightforward.", "labels": [], "entities": []}, {"text": "However, while little or no linguistic training is needed to write templates, it is a tedious and time-consuming task: one or more templates must be written for each combination of goals and discourse contexts, and linguistic issues such as subject-verb agreement and determiner-noun agreement must be repeatedly encoded for each template.", "labels": [], "entities": []}, {"text": "Furthermore, maintenance of the collection of templates becomes a software engineering problem as the complexity of the dialog system increases.", "labels": [], "entities": []}, {"text": "The second approach is natural language generation (NLG), which customarily divides the generation process into three modules: (1) Text Planning, (2) Sentence Planning, and (3) Surface Realization.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.816521406173706}, {"text": "Text Planning", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.7793324291706085}, {"text": "Sentence Planning", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.8495687246322632}, {"text": "Surface Realization", "start_pos": 177, "end_pos": 196, "type": "TASK", "confidence": 0.7246623635292053}]}, {"text": "In this paper, we discuss only sentence planning; the role of the sentence planner is to choose abstract lexico-structural resources fora text plan, where a text plan encodes the communicative goals for an utterance (and, sometimes, their rhetorical structure).", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7508664429187775}]}, {"text": "In general, NLG promises portability across application domains and dialog situations by focusing on the development of rules for each generation module that are general and domain-independent.", "labels": [], "entities": []}, {"text": "However, the quality of the output fora particular domain, or a particular situation in a dialog, maybe inferior to that of a templatebased system without considerable investment in domain-specific rules or domain-tuning of general rules.", "labels": [], "entities": []}, {"text": "Furthermore, since rule-based systems use sophisticated linguistic representations, this handcrafting requires linguistic knowledge.", "labels": [], "entities": []}, {"text": "Recently, several approaches for automatically training modules of an NLG system have been proposed).", "labels": [], "entities": []}, {"text": "These hold the promise that the complex step of customizing NLG systems by hand can be automated, while avoiding the need for tedious hand-crafting of templates.", "labels": [], "entities": []}, {"text": "While the engineering benefits of trainable approaches appear obvious, it is unclear whether the utterance quality is high enough.", "labels": [], "entities": []}, {"text": "In ( we propose anew model of sentence planning called SPOT.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7140506953001022}]}, {"text": "In SPOT, the sentence planner is automatically trained, using feedback from two human judges, to choose the best from among different options for realizing a set of communicative goals.", "labels": [], "entities": [{"text": "SPOT", "start_pos": 3, "end_pos": 7, "type": "TASK", "confidence": 0.9338387250900269}, {"text": "sentence planner", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.712918683886528}]}, {"text": "In (), we evaluate the performance of the learning component of SPOT, and show that SPOT learns to select sentence plans that are highly rated by the two human judges.", "labels": [], "entities": []}, {"text": "While this evaluation shows that SPOT has indeed learned from the human judges, it does not show that using only two human judgments is sufficient to produce more broadly acceptable results, nor does it show that SPOT performs as well as optimized hand-crafted template or rule-based systems.", "labels": [], "entities": [{"text": "SPOT", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.923258364200592}]}, {"text": "In this paper we address these questions.", "labels": [], "entities": []}, {"text": "Because SPOT is trained on data from a working system, we can directly compare SPOT to the hand-crafted, template-based generation component of the current system.", "labels": [], "entities": []}, {"text": "In order to perform an exhaustive comparison, we also implemented two rule-based and two baseline sentence-planners.", "labels": [], "entities": []}, {"text": "One baseline simply produces a single sentence for each communicative goal.", "labels": [], "entities": []}, {"text": "Another baseline randomly makes decisions about how to combine communicative goals into sentences.", "labels": [], "entities": []}, {"text": "We directly compare these different approaches in an evaluation experiment in which 60 human subjects rate each system's output on a scale of 1 to 5.", "labels": [], "entities": []}, {"text": "The experimental design is described in section System1: Welcome....", "labels": [], "entities": [{"text": "Welcome", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.867542564868927}]}, {"text": "What airport would you like to fly out of?", "labels": [], "entities": []}, {"text": "User2: I need to go to Dallas.", "labels": [], "entities": []}, {"text": "System3: Flying to Dallas.", "labels": [], "entities": []}, {"text": "What departure airport was that?", "labels": [], "entities": []}, {"text": "User4: from Newark on September the 1st.", "labels": [], "entities": [{"text": "Newark on September the 1st", "start_pos": 12, "end_pos": 39, "type": "DATASET", "confidence": 0.9319363236427307}]}, {"text": "System5: Flying from Newark to Dallas, Leaving on the 1st of September, And what time did you want to leave?", "labels": [], "entities": []}, {"text": "Figure 1: A dialog with AMELIA 2.", "labels": [], "entities": [{"text": "AMELIA", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.5422931909561157}]}, {"text": "The sentence planners used in the evaluation are described in section 3.", "labels": [], "entities": []}, {"text": "In section 4, we present our results.", "labels": [], "entities": []}, {"text": "We show that the trainable sentence planner performs better than both rule-based systems and as well as the hand-crafted templatebased system.", "labels": [], "entities": []}, {"text": "These four systems outperform the baseline sentence planners.", "labels": [], "entities": []}, {"text": "Section 5 summarizes our results and discusses related and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our research concerns developing and evaluating a portable generation component fora mixedinitiative travel planning system, AMELIA, developed at AT&T Labs as part of DARPA Communicator.", "labels": [], "entities": [{"text": "AMELIA", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.7823801040649414}]}, {"text": "Consider the required generation capabilities of AMELIA, as illustrated in.", "labels": [], "entities": [{"text": "AMELIA", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.704043984413147}]}, {"text": "Utterance System1 requests information about the caller's departure airport, but in User2, the caller takes the initiative to provide information about her destination.", "labels": [], "entities": [{"text": "User2", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.9467266798019409}]}, {"text": "In System3, the system's goal is to implicitly confirm the destination (because of the possibility of error in the speech recognition component), and request information (for the second time) of the caller's departure airport.", "labels": [], "entities": []}, {"text": "This combination of communicative goals arises dynamically in the dialog because the system supports user initiative, and requires different capabilities for generation than if the system could only understand the direct answer to the question that it asked in System1.", "labels": [], "entities": []}, {"text": "In User4, the caller provides this information but takes the initiative to provide the month and day of travel.", "labels": [], "entities": [{"text": "User4", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9265070557594299}]}, {"text": "Given the system's dialog strategy, the communicative goals for its next turn are to implicitly confirm all the information that the user has provided so far, i.e. the departure and destination cities and the month and day information, as well as to request information about the time of travel.", "labels": [], "entities": []}, {"text": "The system's representation of its communicative goals for System5 is in.", "labels": [], "entities": []}, {"text": "As before, this combination of communicative goals arises in response to the user's initiative.", "labels": [], "entities": []}, {"text": "implicit-confirm(orig-city:NEWARK) implicit-confirm(dest-city:DALLAS) implicit-confirm(month:9) implicit-confirm(day-number:1) request(depart-time:whatever): The text plan (communicative goals) for System5 in Like most working research spoken dialog systems, AMELIA uses hand-crafted, templatebased generation.", "labels": [], "entities": []}, {"text": "Its output is created by choosing string templates for each elementary speech act, using a large choice function which depends on the type of speech act and various context conditions.", "labels": [], "entities": []}, {"text": "Values of template variables (such as origin and destination cities) are instantiated by the dialog manager.", "labels": [], "entities": []}, {"text": "The string templates for all the speech acts of a turn are heuristically ordered and then appended to produce the output.", "labels": [], "entities": []}, {"text": "In order to produce output that is not highly redundant, string templates must be written for every possible combination of speech acts in a text plan.", "labels": [], "entities": []}, {"text": "We refer to the output generated by AMELIA using this approach as the   We perform an evaluation using human subjects who judged the TEMPLATE output of AMELIA against five NLG-based approaches: SPOT, two rule-based approaches, and two baselines.", "labels": [], "entities": []}, {"text": "We describe them in Section 3.", "labels": [], "entities": []}, {"text": "An example output for the text plan in for each system is in.", "labels": [], "entities": []}, {"text": "The experiment required human subjects to read 5 dialogs of real interactions with AMELIA.", "labels": [], "entities": [{"text": "AMELIA", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.8579889535903931}]}, {"text": "At 20 points over the 5 dialogs, AMELIA's actual utterance (TEMPLATE) is augmented with a set of variants; each set of variants included a representative generated by SPOT, and representatives of the four comparison sentence planners.", "labels": [], "entities": [{"text": "AMELIA's actual utterance", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.5567639544606209}, {"text": "TEMPLATE", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.7706444263458252}]}, {"text": "At times two or more of these variants coincided, in which case sentences were not repeated and fewer than six sentences were presented to the subjects.", "labels": [], "entities": []}, {"text": "The subjects rated each variation on a 5-point Likert scale, by stating the degree to which they agreed with the statement The system's utterance is easy to understand, well-formed, and appropriate to the dialog context.", "labels": [], "entities": []}, {"text": "Sixty colleagues not involved in this research completed the experiment.", "labels": [], "entities": []}, {"text": "All 60 subjects completed the experiment in a half hour or less.", "labels": [], "entities": []}, {"text": "The experiment resulted in a total of 1200 judgements for each of the systems being compared, since each subject judged 20 utterances by each system.", "labels": [], "entities": []}, {"text": "We first discuss overall differences among the different systems and then make comparisons among the four different types of systems: (1) TEMPLATE, (2) SPOT, (3) two rule-based systems, and (4) two baseline systems.", "labels": [], "entities": [{"text": "TEMPLATE", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9275123476982117}, {"text": "SPOT", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.7437825798988342}]}, {"text": "All statistically significant results discussed here had p values of less than .01.", "labels": [], "entities": []}, {"text": "We first examined whether differences inhuman ratings (score) were predictable from the Figure 7: Summary of Overall Results for all Systems Evaluated type of system that produced the utterance being rated.", "labels": [], "entities": []}, {"text": "A one-way ANOVA with system as the independent variable and score as the dependent variable showed that there were significant differences in score as a function of system.", "labels": [], "entities": []}, {"text": "The overall differences are summarized in.", "labels": [], "entities": []}, {"text": "As indicates, some system outputs received more consistent scores than others, e.g. the standard deviation for TEMPLATE was much smaller than RANDOM.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 88, "end_pos": 106, "type": "METRIC", "confidence": 0.9176706075668335}, {"text": "TEMPLATE", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.8988068103790283}, {"text": "RANDOM", "start_pos": 142, "end_pos": 148, "type": "DATASET", "confidence": 0.6410041451454163}]}, {"text": "The ranking of the systems by average score is TEMPLATE, SPOT, ICF, RBS, NOAGG, and RANDOM.", "labels": [], "entities": [{"text": "TEMPLATE", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.996717631816864}, {"text": "SPOT", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9576050639152527}, {"text": "ICF", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9522247910499573}, {"text": "RBS", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9827769994735718}, {"text": "NOAGG", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.970895528793335}, {"text": "RANDOM", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.5972311496734619}]}, {"text": "Posthoc comparisons of the scores of individual pairs of systems using the adjusted Bonferroni statistic revealed several different groupings.", "labels": [], "entities": []}, {"text": "The highest ranking systems were TEMPLATE and SPOT, whose ratings were not statistically significantly different from one another.", "labels": [], "entities": [{"text": "TEMPLATE", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.8648746013641357}, {"text": "SPOT", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.6572757363319397}]}, {"text": "This shows that it is possible to match the quality of a hand-crafted system with a trainable one, which should be more portable, more general and require less overall engineering effort.", "labels": [], "entities": []}, {"text": "The next group of systems were the two rulebased systems, ICF and RBS, which were not statistically different from one another.", "labels": [], "entities": [{"text": "ICF", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.4946042001247406}, {"text": "RBS", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.7209943532943726}]}, {"text": "However SPOT was statistically better than both of these systems (p .01).", "labels": [], "entities": [{"text": "SPOT", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.5276166200637817}]}, {"text": "shows that SPOT got more high rankings than either of the rulebased systems.", "labels": [], "entities": [{"text": "SPOT", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.441368967294693}]}, {"text": "Ina sense this may not be that surprising, because as point out, it is difficult to construct a rule-based sentence planner that handles all the rule interactions in a reasonable way.", "labels": [], "entities": []}, {"text": "Features that SPoT's SPR uses allow SPOT to be sensitive to particular discourse configurations or lexical collocations.", "labels": [], "entities": []}, {"text": "In order to encode these in a rule-based sentence", "labels": [], "entities": []}], "tableCaptions": []}