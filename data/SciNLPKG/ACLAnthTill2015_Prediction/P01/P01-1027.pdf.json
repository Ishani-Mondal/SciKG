{"title": [{"text": "Refined Lexicon Models for Statistical Machine Translation using a Maximum Entropy Approach", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.8414672613143921}]}], "abstractContent": [{"text": "Typically, the lexicon models used in statistical machine translation systems do not include any kind of linguistic or contextual information, which often leads to problems in performing a correct word sense disambiguation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.6306993961334229}, {"text": "word sense disambiguation", "start_pos": 197, "end_pos": 222, "type": "TASK", "confidence": 0.6524467170238495}]}, {"text": "One way to deal with this problem within the statistical framework is to use maximum entropy methods.", "labels": [], "entities": []}, {"text": "In this paper, we present how to use this type of information within a statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.6233338812987009}]}, {"text": "We show that it is possible to significantly decrease training and test corpus perplexity of the translation models.", "labels": [], "entities": [{"text": "training", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9877326488494873}]}, {"text": "In addition, we perform a rescoring of \u00a2-Best lists using our maximum entropy model and thereby yield an improvement in translation quality.", "labels": [], "entities": []}, {"text": "Experimental results are presented on the so-called \"Verbmobil Task\".", "labels": [], "entities": []}], "introductionContent": [{"text": "Typically, the lexicon models used in statistical machine translation systems are only single-word based, that is one word in the source language corresponds to only one word in the target language.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.6307124594847361}]}, {"text": "Those lexicon models lack from context information that can be extracted from the same parallel corpus.", "labels": [], "entities": []}, {"text": "This additional information could be: from WordNet), current/previous speech or dialog act.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.955879271030426}]}, {"text": "To include this additional information within the statistical framework we use the maximum entropy approach.", "labels": [], "entities": []}, {"text": "This approach has been applied in natural language processing to a variety of tasks.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6671792467435201}]}, {"text": "applies this approach to the so-called IBM Candide system to build context dependent models, compute automatic sentence splitting and to improve word reordering in translation.", "labels": [], "entities": [{"text": "IBM Candide system", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.9188792904218038}, {"text": "sentence splitting", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7289456576108932}]}, {"text": "Similar techniques are used in) for socalled direct translation models instead of those proposed in (.) describes two methods for incorporating information about the relative position of bilingual word pairs into a maximum entropy translation model.", "labels": [], "entities": [{"text": "socalled direct translation", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.6448210179805756}]}, {"text": "Other authors have applied this approach to language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8130272626876831}]}, {"text": "A short review of the maximum entropy approach is outlined in Section 3.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: The 10 most important features and their  respective category and", "labels": [], "entities": []}, {"text": " Table 4: Number of features used according to  different cut-off threshold. In the second column  of the table are shown the number of features used  when only the English context is considered. The  third column correspond to English, German and  Word-Classes contexts.", "labels": [], "entities": []}, {"text": " Table 5: Corpus characteristics for translation  task.", "labels": [], "entities": [{"text": "translation  task", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.9196222722530365}]}, {"text": " Table 6: Corpus characteristics for perplexity  quality experiments.", "labels": [], "entities": []}, {"text": " Table 7: Training and Test perplexities us- ing different contextual information and different  thresholds", "labels": [], "entities": []}, {"text": " Table 8: Preliminary translation results for the  Verbmobil Test-147 for different contextual infor- mation and different thresholds using the top-10  translations. The baseline translation results for  model 4 are WER=54.80 and PER=43.07.", "labels": [], "entities": [{"text": "Verbmobil Test-147", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.8862859606742859}, {"text": "WER", "start_pos": 216, "end_pos": 219, "type": "METRIC", "confidence": 0.9976716637611389}, {"text": "PER", "start_pos": 230, "end_pos": 233, "type": "METRIC", "confidence": 0.9985681772232056}]}]}