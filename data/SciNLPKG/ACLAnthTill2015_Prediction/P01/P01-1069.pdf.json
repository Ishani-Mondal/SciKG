{"title": [], "abstractContent": [{"text": "Many machine learning methods have recently been applied to natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.7025888562202454}]}, {"text": "Among them, the Winnow algorithm has been argued to be particularly suitable for NLP problems, due to its robustness to irrelevant features.", "labels": [], "entities": []}, {"text": "However in theory, Winnow may not converge for non-separable data.", "labels": [], "entities": []}, {"text": "To remedy this problem , a modification called regularized Winnow has been proposed.", "labels": [], "entities": []}, {"text": "In this paper , we apply this new method to text chunking.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.8187489807605743}]}, {"text": "We show that this method achieves state of the art performance with significantly less computation than previous approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently there has been considerable interest in applying machine learning techniques to problems in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 101, "end_pos": 128, "type": "TASK", "confidence": 0.6378094951311747}]}, {"text": "One method that has been quite successful in many applications is the SNoW architecture ().", "labels": [], "entities": []}, {"text": "This architecture is based on the Winnow algorithm, which in theory is suitable for problems with many irrelevant attributes.", "labels": [], "entities": []}, {"text": "In natural language processing, one often encounters a very high dimensional feature space, although most of the features are irrelevant.", "labels": [], "entities": []}, {"text": "Therefore the robustness of Winnow to high dimensional feature space is considered an important reason why it is suitable for NLP tasks.", "labels": [], "entities": []}, {"text": "However, the convergence of the Winnow algorithm is only guaranteed for linearly separable data.", "labels": [], "entities": []}, {"text": "In practical NLP applications, data are often linearly non-separable.", "labels": [], "entities": []}, {"text": "Consequently, a direct application of Winnow may lead to numerical instability.", "labels": [], "entities": []}, {"text": "A remedy for this, called regularized Winnow, has been recently proposed in.", "labels": [], "entities": [{"text": "regularized Winnow", "start_pos": 26, "end_pos": 44, "type": "METRIC", "confidence": 0.5663719773292542}]}, {"text": "This method modifies the original Winnow algorithm so that it solves a regularized optimization problem.", "labels": [], "entities": []}, {"text": "It converges both in the linearly separable case and in the linearly nonseparable case.", "labels": [], "entities": []}, {"text": "Its numerical stability implies that the new method can be more suitable for practical NLP problems that may not be linearly separable.", "labels": [], "entities": []}, {"text": "In this paper, we compare regularized Winnow and Winnow algorithms on text chunking.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.7661592960357666}]}, {"text": "In order for us to rigorously compare our system with others, we use the CoNLL-2000 shared task dataset), which is publicly available from http://lcgwww.uia.ac.be/conll2000/chunking.", "labels": [], "entities": [{"text": "CoNLL-2000 shared task dataset", "start_pos": 73, "end_pos": 103, "type": "DATASET", "confidence": 0.8497763276100159}]}, {"text": "An advantage of using this dataset is that a large number of state of the art statistical natural language processing methods have already been applied to the data.", "labels": [], "entities": []}, {"text": "Therefore we can readily compare our results with other reported results.", "labels": [], "entities": []}, {"text": "For simplicity, we shall assume 3 % 5 4 in this paper.", "labels": [], "entities": []}, {"text": "The restriction does not cause problems in practice since one can always append a constant feature to the input data , which offsets the effect of . Given a training set of labeled data , a number of approaches to finding linear discriminant functions have been advanced over the years.", "labels": [], "entities": []}, {"text": "We are especially interested in the Winnow multiplicative update algorithm.", "labels": [], "entities": [{"text": "Winnow multiplicative update", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.5815114478270212}]}, {"text": "This algorithm updates the weight vector by going through the training data repeatedly.", "labels": [], "entities": []}, {"text": "It is mistake driven in the sense that the weight vector is updated only when the algorithm is notable to correctly classify an example.", "labels": [], "entities": []}, {"text": "The , where c is a prior which is typically chosen to be uniform.", "labels": [], "entities": []}, {"text": "There can be several variants of the Winnow algorithm.", "labels": [], "entities": []}, {"text": "One is called balanced Winnow, which is equivalent to an embedding of the input space into a higher dimensional space as: to have the effect of both positive and negative weights for the original input . One problem of the Winnow online update algorithm is that it may not converge when the data are not linearly separable.", "labels": [], "entities": []}, {"text": "One may partially remedy this problem by decreasing the learning rate parameter Y during the updates.", "labels": [], "entities": [{"text": "learning rate parameter Y", "start_pos": 56, "end_pos": 81, "type": "METRIC", "confidence": 0.8650998622179031}]}, {"text": "However, this is rather ad hoc since it is unclear what is the best way to do so.", "labels": [], "entities": []}, {"text": "Therefore in practice, it can be quite difficult to implement this idea properly.", "labels": [], "entities": []}, {"text": "In order to obtain a systematic solution to this problem, we shall first examine a derivation of the Winnow algorithm in, which motivates a more general solution to be presented later.", "labels": [], "entities": []}, {"text": "Following . The Winnow update (1) can be regarded as an approximate solution to.", "labels": [], "entities": []}, {"text": "Although the above derivation does not solve the non-convergence problem of the original Winnow method when the data are not linearly separable, it does provide valuable insights which can lead to a more systematic solution of the problem.", "labels": [], "entities": []}, {"text": "The basic idea was given in, where the original Winnow algorithm was converted into a numerical optimization problem that can handle linearly non-separable data.", "labels": [], "entities": []}, {"text": "The resulting formulation is closely related to (2).", "labels": [], "entities": []}, {"text": "However, instead of looking atone example at a time as in an online formulation, we incorporate all examples at the same time.", "labels": [], "entities": []}, {"text": "In addition, we add a margin condition into the \"hinge loss\".).", "labels": [], "entities": [{"text": "margin", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9786280989646912}]}, {"text": "These results imply that the new method, while it can properly handle non-separable data, shares similar theoretical advantages of Winnow in that it is also robust to irrelevant features.", "labels": [], "entities": []}, {"text": "This theoretical insight implies that the algorithm is suitable for NLP tasks with large feature spaces.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Our chunk prediction results: with basic  features", "labels": [], "entities": [{"text": "chunk prediction", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.9617434144020081}]}, {"text": " Table 3: Chunk prediction results using original  Winnow (with basic features)", "labels": [], "entities": [{"text": "Chunk prediction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7857146263122559}, {"text": "Winnow", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9144598245620728}]}]}