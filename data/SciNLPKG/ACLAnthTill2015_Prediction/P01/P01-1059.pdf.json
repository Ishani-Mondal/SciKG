{"title": [{"text": "Producing Biographical Summaries: Combining Linguistic Knowledge with Corpus Statistics 1", "labels": [], "entities": [{"text": "Producing Biographical Summaries", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6831210752328237}]}], "abstractContent": [{"text": "We describe a biographical multi-document summarizer that summarizes information about people described in the news.", "labels": [], "entities": [{"text": "summarizes information about people described in the news", "start_pos": 58, "end_pos": 115, "type": "TASK", "confidence": 0.8237007036805153}]}, {"text": "The summarizer uses corpus statistics along with linguistic knowledge to select and merge descriptions of people from a document collection, removing redundant descriptions.", "labels": [], "entities": []}, {"text": "The summarization components have been extensively evaluated for coherence, accuracy, and non-redundancy of the descriptions produced.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9991852641105652}]}], "introductionContent": [{"text": "The appositive phrases usually provide descriptions of attributes of a person.", "labels": [], "entities": []}, {"text": "However, the preprocessing component described in Section 2.1 does produce errors in appositive extraction, which are filtered out by syntactic and semantic tests.", "labels": [], "entities": [{"text": "appositive extraction", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.6916503608226776}]}, {"text": "The system also filters out redundant descriptions, both duplicate descriptions as well as similar ones.", "labels": [], "entities": []}, {"text": "These filtering methods are discussed next.", "labels": [], "entities": []}], "datasetContent": [{"text": "The component evaluation tests how accurately the tagger can identify whether ahead noun in a description is appropriate as a person description The evaluation uses the WordNet 1.6 SEMCOR semantic concordance, which has files from the Brown corpus whose words have semantic tags (created by WordNet' s creators) indicating WordNet sense numbers.", "labels": [], "entities": [{"text": "WordNet 1.6 SEMCOR semantic concordance", "start_pos": 169, "end_pos": 208, "type": "DATASET", "confidence": 0.9022498250007629}]}, {"text": "Evaluation on 6,000 sentences with almost 42,000 nouns compares people tags generated by the program with SEMCOR tags, and provided the following results: right = 41,555, wrong = 1,298, missing = 0, yielding Precision, Recall, and F-Measure of 0.97.", "labels": [], "entities": [{"text": "Precision", "start_pos": 208, "end_pos": 217, "type": "METRIC", "confidence": 0.998946487903595}, {"text": "Recall", "start_pos": 219, "end_pos": 225, "type": "METRIC", "confidence": 0.9832540154457092}, {"text": "F-Measure", "start_pos": 231, "end_pos": 240, "type": "METRIC", "confidence": 0.9991219639778137}]}, {"text": "This component evaluation tests the wellformedness of the extracted relative clauses.", "labels": [], "entities": []}, {"text": "For this evaluation, we used the Clinton corpus.", "labels": [], "entities": [{"text": "Clinton corpus", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.9472275972366333}]}, {"text": "The relative clause is judged correct if it has the right extent, and the correct coreference index indicating which person the relative clause description pertains to.", "labels": [], "entities": []}, {"text": "The judgments are based on 36 instances of relative clauses from 22 documents.", "labels": [], "entities": []}, {"text": "The results show 28 correct relative clauses found, plus 4 spurious finds, yielding Precision of 0.87, Recall of 0.78, and F-measure of .82.", "labels": [], "entities": [{"text": "Precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9997538924217224}, {"text": "Recall of 0.78", "start_pos": 103, "end_pos": 117, "type": "METRIC", "confidence": 0.9718130628267924}, {"text": "F-measure", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9990178346633911}]}, {"text": "Although the sample is small, the results are very promising.", "labels": [], "entities": []}, {"text": "This component evaluation tests the system's ability to accurately merge appositive descriptions.", "labels": [], "entities": []}, {"text": "The score is based on an automatic comparison of the system's merge of systemgenerated appositive descriptions against a human merge of them.", "labels": [], "entities": []}, {"text": "We took all the names that were identified in the Clinton corpus and ran the system on each document in the corpus.", "labels": [], "entities": [{"text": "Clinton corpus", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9342247545719147}]}, {"text": "We took the raw descriptions that the system produced before merging, and wrote a brief description by hand for each person who had two or more raw descriptions.", "labels": [], "entities": []}, {"text": "The hand-written descriptions were not done with any reference to the automatically merged descriptions nor with any reference to the underlying source material.", "labels": [], "entities": []}, {"text": "The hand-written descriptions were then compared with the final output of the system (i.e., the result after merging).", "labels": [], "entities": []}, {"text": "The comparison was automatic, measuring similarity among vectors of content words (i.e., stop words such as articles and prepositions were removed).", "labels": [], "entities": []}, {"text": "Here is an example to further clarify the strict standard of the automatic evaluation (words scored correct are underlined): System: E. Lawrence Barcella is a Washington lawyer, Washington white-collar defense lawyer, former federal prosecutor System Merge: Washington white-collar defense lawyer Human Merge: a Washington lawyer and former federal prosecutor Automatic Score: Correct=2; Extra-Words=2; Missed-Words=3 Thus, although 'lawyer' and 'prosecutor' are synonymous in WordNet, the automatic scorer doesn't know that, and so 'prosecutor' is penalized as an extra word.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 477, "end_pos": 484, "type": "DATASET", "confidence": 0.9343287348747253}]}, {"text": "The evaluation was carried out over the entire Clinton corpus, with descriptions compared for 226 people who had more than one description.", "labels": [], "entities": [{"text": "Clinton corpus", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9117911159992218}]}, {"text": "65 out of the 226 descriptions were Correct (28%), with a further 32 cases being semantically correct 'obviously similar' substitutions which the automatic scorer missed (giving an adjusted accuracy of 42%).", "labels": [], "entities": [{"text": "Correct", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9987924098968506}, {"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9948258996009827}]}, {"text": "As a baseline, a merging program which performed just a string match scored 21% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9991324543952942}]}, {"text": "The major problem areas were errors in coreference (e.g., Clinton family members being put in the same coreference class), lack of good descriptions for famous people (news articles tend not to introduce such people), and parsing limitations (e.g., \"Senator Clinton\" being parsed erroneously as an NP in \"The Senator Clinton disappointed\u2026\").", "labels": [], "entities": []}, {"text": "Ultimately, of course, domain-independent systems like ours are limited semantically in merging by the lack of world knowledge, e.g., knowing that Starr' s chief lieutenant can be a prosecutor.", "labels": [], "entities": []}, {"text": "To assess the coherence and informativeness of the relative clause descriptions 3 , we asked 4 subjects who were unaware of our research to judge descriptions generated by our system from the Clinton corpus.", "labels": [], "entities": [{"text": "Clinton corpus", "start_pos": 192, "end_pos": 206, "type": "DATASET", "confidence": 0.9088815152645111}]}, {"text": "For each relative clause description, the subject was given the description, a person name to whom that description pertained, and a capsule description consisting of merged appositives created by the system.", "labels": [], "entities": []}, {"text": "The subject was asked to assess (a) the coherence of the relative clause description in terms of its succinctness (was it a good length?) and its comprehensibility (was it and understandable by itself or in conjunction with the capsule?), and (b) its informativeness in terms of whether it was an accurate description (does it conflict with the capsule or with what you know?) and whether it was non-redundant (is it distinct or does it repeat what is in the capsule?).", "labels": [], "entities": []}, {"text": "The subjects marked 87% of the descriptions as accurate, 96% as non-redundant, and 65% as coherent.", "labels": [], "entities": []}, {"text": "A separate 3-subject inter-annotator agreement study, where all subjects judged the same 46 decisions, showed that all three subjects agreed on 82% of the accuracy decisions, 85% of the non-redundancy decisions and 82% of the coherence decisions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9985949397087097}]}], "tableCaptions": [{"text": " Table 2. Accuracy of Different Description  Learners on Clinton corpus", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9933639764785767}, {"text": "Clinton corpus", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9507711231708527}]}]}