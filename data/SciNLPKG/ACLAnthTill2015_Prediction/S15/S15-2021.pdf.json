{"title": [{"text": "ECNU: Using Traditional Similarity Measurements and Word Embedding for Semantic Textual Similarity Estimation", "labels": [], "entities": [{"text": "ECNU", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.845873236656189}, {"text": "Semantic Textual Similarity Estimation", "start_pos": 71, "end_pos": 109, "type": "TASK", "confidence": 0.7126841843128204}]}], "abstractContent": [{"text": "This paper reports our submissions to semantic textual similarity task, i.e., task 2 in Semantic Evaluation 2015.", "labels": [], "entities": [{"text": "semantic textual similarity task", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.6664933413267136}, {"text": "Semantic Evaluation 2015", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.9207993149757385}]}, {"text": "We built our systems using various traditional features, such as string-based, corpus-based and syntactic similarity metrics, as well as novel similarity measures based on distributed word representations , which were trained using deep learning paradigms.", "labels": [], "entities": []}, {"text": "Since the training and test datasets consist of instances collected from various domains , three different strategies of the usage of training datasets were explored: (1) use all available training datasets and build a unified supervised model for all test datasets; (2) select the most similar training dataset and separately construct a individual model for each test set; (3) adopt multi-task learning framework to make full use of available training sets.", "labels": [], "entities": []}, {"text": "Results on the test datasets show that using all datasets as training set achieves the best averaged performance and our best system ranks 15 out of 73.", "labels": [], "entities": []}], "introductionContent": [{"text": "Estimating the degree of semantic similarity between two sentences is the building block of many natural language processing (NLP) applications, such as textual entailment (), text summarization, question answering (, etc.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.7509617507457733}, {"text": "question answering", "start_pos": 196, "end_pos": 214, "type": "TASK", "confidence": 0.8892376124858856}]}, {"text": "Therefore, semantic textual similarity (STS) has been received an increasing amount of attention in recent years, e.g., the Semantic Textual Similarity competitions in Semantic Evaluation Exercises have been held from 2012 to 2014.", "labels": [], "entities": [{"text": "semantic textual similarity (STS)", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.7659425089756647}, {"text": "Semantic Textual Similarity", "start_pos": 124, "end_pos": 151, "type": "TASK", "confidence": 0.7807473341623942}]}, {"text": "This year the participants in the STS task in SemEval 2015 () are required to rate the similar degree of a pair of sentences by a value from 0 (no relation) to 5 (semantic equivalence) with an optional confidence score.", "labels": [], "entities": [{"text": "STS task in SemEval 2015", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.6185122072696686}]}, {"text": "To identify semantic textual similarity of text pairs, most existing works adopt at least one of the following feature types: (1) string based similarity) which employs common functions to calculate similarities over string sequences extracted from original strings, e.g., lemma, stem, or n-gram sequences; (2) corpus based similarity) where distributional models such as Latent Semantic Analysis (LSA), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; (3) knowledge based method) which estimates the similarities with the aid of external resources, such as WordNet . Among them, lots of researchers () leverage different word alignment strategies to bring word-level similarity to sentence-level similarity.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 703, "end_pos": 710, "type": "DATASET", "confidence": 0.9758728742599487}]}, {"text": "In this work, we first borrow aforementioned effective types of similarity measurements including string-based, corpus-based, syntactic features and soon, to capture the semantic similarity between two sentences.", "labels": [], "entities": []}, {"text": "Beside, we also present a novel feature type based on word embeddings that are induced using neural language models over a large raw cor-pus (.", "labels": [], "entities": []}, {"text": "Then these features are served as input of a regression model.", "labels": [], "entities": []}, {"text": "Notice that, the organizers provide us seventeen training datasets and five test datasets, which are drawn from different but related domains.", "labels": [], "entities": []}, {"text": "Accordingly, we build three different systems in terms of the usage of training datasets: (1) exploit all the training datasets and train a single model for all test datasets; (2) choose one domain-dependent training dataset for each test dataset using cosine distance selection criterion and train models individually for each test dataset; (3) to overcome overuse or underuse of training datasets, we adopt multi-task learning (MTL) framework to make full use of available training datasets, that is, for each test set the main task is built upon designated training datasets and the rest training datasets are used in the auxiliary tasks.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes various similarity measurements used in our systems.", "labels": [], "entities": []}, {"text": "System setups and experimental results on training and test datasets are presented in Section 3.", "labels": [], "entities": []}, {"text": "Finally, conclusions and future work are given in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Participants built their systems on seventeen datasets in development period and evaluated their systems on five test datasets in test period.", "labels": [], "entities": []}, {"text": "Each dataset consists of a number of sentence pairs and each pair has a human-assigned similarity score in the range  We built three different systems according to the usage of training datasets as follows.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 87, "end_pos": 103, "type": "METRIC", "confidence": 0.8708462417125702}]}, {"text": "allData: We used all the training datasets and built a single global regression model regardless of domain information of different test datasets.", "labels": [], "entities": []}, {"text": "DesignatedData: For each test dataset, we calculated the cosine distance with every candidate training dataset.", "labels": [], "entities": []}, {"text": "Then the training dataset with the lowest distance score was chose as the training dataset to fit a regression model for specific test dataset.", "labels": [], "entities": []}, {"text": "On one hand, taking all the training datasets into consideration may hurt the performance since training and test datasets are from different domains.", "labels": [], "entities": []}, {"text": "On the other hand, using the most related datasets leads to insufficient usage of available datasets.", "labels": [], "entities": []}, {"text": "Therefore, we considered to adopt multi-task learning framework to take full advantage of available training sets.", "labels": [], "entities": []}, {"text": "Under multi-task learning framework, a main task learns together with other related auxiliary tasks at the same time, using a shared representation.", "labels": [], "entities": []}, {"text": "This often leads to a better model for the main task, because it allows the learner to use the commonality among the tasks.", "labels": [], "entities": []}, {"text": "Hence, for each test dataset we selected the datasets whose cosine distances are less than 0.1 (at least one training set) as training set to construct the main task, and then used the remaining training sets to construct auxiliary tasks.", "labels": [], "entities": []}, {"text": "In this work, we adopted the robust multitask feature learning (rMTFL) (, which assumes that the model W can be decomposed into two components: a shared feature structure P that captures task relatedness and a groupsparse structure Q that detects outlier tasks.", "labels": [], "entities": []}, {"text": "Specifically, it solves following formulation: subject to : W = P + Q where X i denotes the input matrix of the i-th task, Y i denotes its corresponding label, W i is the model for task i, the regularization parameter \u03c1 1 controls the joint feature learning, and the regularization parameter \u03c1 2 controls the columnwise group sparsity on Q that detects outliers.", "labels": [], "entities": []}, {"text": "In our preliminary experiments, several regression algorithms were examined, including Support Vector Regression (SVR, linear), Random Forest (RF) and Gradient Boosting (GB) implemented in the scikit-learn toolkit (Pedregosa et al., 2011).", "labels": [], "entities": [{"text": "Support Vector Regression", "start_pos": 87, "end_pos": 112, "type": "METRIC", "confidence": 0.8618794679641724}, {"text": "Random Forest (RF)", "start_pos": 128, "end_pos": 146, "type": "METRIC", "confidence": 0.8093551993370056}, {"text": "Gradient Boosting (GB)", "start_pos": 151, "end_pos": 173, "type": "METRIC", "confidence": 0.9011242389678955}]}, {"text": "The system performance is evaluated using Pearson correlation (r).", "labels": [], "entities": [{"text": "Pearson correlation (r)", "start_pos": 42, "end_pos": 65, "type": "METRIC", "confidence": 0.951980471611023}]}], "tableCaptions": [{"text": " Table 2: Pearson of allData,DesignatedData using different algorithms and MTL on STS 2014 datasets.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9653782248497009}, {"text": "STS 2014 datasets", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.8435510198275248}]}, {"text": " Table 3: Results of our three runs on STS 2015 test datasets, as well as top rank runs.", "labels": [], "entities": [{"text": "STS 2015 test datasets", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.9037384390830994}]}]}