{"title": [], "abstractContent": [{"text": "Explicit Semantic Analysis (ESA) utilizes the Wikipedia knowledge base to represent the semantics of a word by a vector where every dimension refers to an explicitly defined concept like a Wikipedia article.", "labels": [], "entities": [{"text": "Explicit Semantic Analysis (ESA)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7906885147094727}, {"text": "Wikipedia knowledge base", "start_pos": 46, "end_pos": 70, "type": "DATASET", "confidence": 0.8615290919939677}]}, {"text": "ESA inherently assumes that Wikipedia concepts are orthogonal to each other, therefore, it considers that two words are related only if they co-occur in the same articles.", "labels": [], "entities": [{"text": "ESA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8211883902549744}]}, {"text": "However, two words can be related to each other even if they appear separately in related articles rather than co-occurring in the same articles.", "labels": [], "entities": []}, {"text": "This leads to a need for extending the ESA model to consider the relatedness between the explicit concepts (i.e. Wikipedia articles in Wikipedia based implementation) for computing textual relatedness.", "labels": [], "entities": []}, {"text": "In this paper, we present Non-Orthogonal ESA (NESA) which represents more fine grained semantics of a word as a vector of explicit concept dimensions, where every such concept dimension further constitutes a semantic vector builtin another vector space.", "labels": [], "entities": []}, {"text": "Thus, NESA considers the concept correlations in computing the relatedness between two words.", "labels": [], "entities": [{"text": "NESA", "start_pos": 6, "end_pos": 10, "type": "TASK", "confidence": 0.7422279119491577}]}, {"text": "We explore different approaches to compute the concept correlation weights, and compare these approaches with other existing methods.", "labels": [], "entities": []}, {"text": "Furthermore, we evaluate our model NESA on several word related-ness benchmarks showing that it outperforms the state of the art methods.", "labels": [], "entities": [{"text": "NESA", "start_pos": 35, "end_pos": 39, "type": "TASK", "confidence": 0.9283233284950256}]}], "introductionContent": [{"text": "Significance of quantifying relatedness between two natural language texts has been shown in various tasks which deal with information retrieval (IR), natural language processing (NLP), or other related fields.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.8500059843063354}, {"text": "natural language processing (NLP)", "start_pos": 151, "end_pos": 184, "type": "TASK", "confidence": 0.7901783684889475}]}, {"text": "The semantics of a word can be obtained from existing lexical resources like WordNet and FrameNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.9710832238197327}, {"text": "FrameNet", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.8796870708465576}]}, {"text": "However, such lexical resources require domain expertise for defining the hierarchical structure, which makes their creation very expensive.", "labels": [], "entities": []}, {"text": "Therefore, distributional semantic models (DSMs) have achieved much attention as they utilize available document collections like Wikipedia, and do not depend upon human expertise.", "labels": [], "entities": [{"text": "distributional semantic models (DSMs)", "start_pos": 11, "end_pos": 48, "type": "TASK", "confidence": 0.6737064470847448}]}, {"text": "DSMs represent the semantics of a word by transforming it to a high dimensional distributional vector in a predefined concept space.", "labels": [], "entities": []}, {"text": "Many models have been proposed that derive this concept space by using explicit concepts or implicit concepts.", "labels": [], "entities": []}, {"text": "Explicit Semantic Analysis (ESA) ( utilizes the concepts which are explicitly derived under human cognition like Wikipedia concepts (articles).", "labels": [], "entities": [{"text": "Explicit Semantic Analysis (ESA)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7592396289110184}]}, {"text": "However, Latent Semantic Analysis (LSA) derives a latent concept space by performing dimensionality reduction ().", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.7088806450366973}]}, {"text": "introduced ESA model in which Wikipedia and Open Directory Project were used to obtain the explicit concepts, however, Wikipedia has been a popular choice in further ESA implementations).", "labels": [], "entities": []}, {"text": "ESA represents the semantics of a word with a high dimensional vector over the Wikipedia concepts.", "labels": [], "entities": [{"text": "ESA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8629857301712036}]}, {"text": "The tf-idf weight of the word with the textual content under a Wikipedia concept reflects the magnitude Association football United Soccer Leagues of the corresponding vector dimension.", "labels": [], "entities": []}, {"text": "To obtain the semantic relatedness between two words, it computes the vector dot product between their vectors.", "labels": [], "entities": []}, {"text": "ESA considers the dimensions as orthogonal to each other.", "labels": [], "entities": [{"text": "ESA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9664926528930664}]}, {"text": "For instance, the synonyms like \"soccer\" and \"football\" are highly related, however, they may not co-occur together in many Wikipedia articles.", "labels": [], "entities": []}, {"text": "shows that the top 5 Wikipedia concepts retrieved for \"football\" and \"soccer\" do not share any concept, however, the concepts may exhibit relatedness to each other.", "labels": [], "entities": []}, {"text": "Consequently, ESA model assumes that words can be related only if they co-occur in the same articles.", "labels": [], "entities": []}, {"text": "However, two words can also be related even if they do not share the same articles at all, but appear in the related ones.", "labels": [], "entities": []}, {"text": "LSA resolves the orthogonality issue to some extent by building latent concept space in an unsupervised way).", "labels": [], "entities": []}, {"text": "However, the resulting latent concepts are not as clearly interpretable as the human-labeled concepts in the ESA model.", "labels": [], "entities": []}, {"text": "Previous studies ( show that ESA performs better than LSA for computing text relatedness.", "labels": [], "entities": [{"text": "computing text relatedness", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.7071073452631632}]}, {"text": "Therefore, it is important to consider the relatedness between dimensions in the ESA model, rather than considering them orthogonal, and also without losing the explicit property of ESA model at the same time.", "labels": [], "entities": [{"text": "ESA", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.8276867866516113}]}, {"text": "In this paper, we present Non-Orthogonal ESA (NESA) model, an extension to ESA, which also uses relatedness between the explicit concepts for computing semantic relatedness between texts.", "labels": [], "entities": []}, {"text": "The concepts in ESA model are clearly interpretable and they refer to the title of Wikipedia articles.", "labels": [], "entities": []}, {"text": "This characteristic provides an opportunity to investigate different concept relatedness measures, such as relatedness between articles' content (document relatedness) or relatedness between corresponding Wikipedia titles.", "labels": [], "entities": []}, {"text": "In order to investigate the performance of these concept relatedness measures, we evaluate them on an entity relatedness benchmark called KORE) as Wikipedia article title generally refers to an entity.", "labels": [], "entities": [{"text": "KORE", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.6988865733146667}]}, {"text": "We then apply the different approaches for computing concept relatedness in our model NESA to compute text relatedness.", "labels": [], "entities": [{"text": "text relatedness", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.7254309058189392}]}, {"text": "We evaluate NESA on several word relatedness benchmarks to verify whether considering non-orthogonality in ESA model improves its performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the different approaches defined for computing concept relatedness measures in the previous section.", "labels": [], "entities": []}, {"text": "For our evaluation, we use the snapshot of English Wikipedia from 1 st October, 2013.", "labels": [], "entities": [{"text": "snapshot of English Wikipedia from 1 st October, 2013", "start_pos": 31, "end_pos": 84, "type": "DATASET", "confidence": 0.9170184016227723}]}, {"text": "This snapshot consists of 13,872,614 articles, in which 5,934,022 are Wikipedia redirects.", "labels": [], "entities": []}, {"text": "We filtered out all the namespace 5 pages by using the articles' titles as they have specific namespace patterns.", "labels": [], "entities": []}, {"text": "There are 3,571,206 namespace pages in this snapshot.", "labels": [], "entities": []}, {"text": "We remove all those articles which contain less than 100 unique words or less than 5 hyperlinks; such articles are too specific and may generate some noise.", "labels": [], "entities": []}, {"text": "We perform further filtering by removing all the articles if their titles are numbers like \"19\", dates like \"June 1\", or if the title starts with \"list\".", "labels": [], "entities": []}, {"text": "We finally obtain a total of 3,635,833 Wikipedia articles for our experiment.", "labels": [], "entities": []}, {"text": "We implement all the concept relatedness measures by using these obtained Wikipedia articles.", "labels": [], "entities": []}, {"text": "VSM-Text represents the semantics of a concept with a column vector of mx1, where m is the total number of unique words appear in Wikipedia.", "labels": [], "entities": [{"text": "VSM-Text", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8316031694412231}]}, {"text": "Wikipedia contains more than 2.5 billion unique words, therefore, to reduce the matrix size, we use only 5 million most frequent words.", "labels": [], "entities": []}, {"text": "ESA-WikiTitle represents the semantics of a concept with a column vector of mx1, where m is 3,635,833 in our implementation.", "labels": [], "entities": [{"text": "ESA-WikiTitle", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9408575892448425}]}, {"text": "In order to obtain the hyperlinks for VSM-Hyperlink and DiSER, we retain only those text segments which have manually defined links provided by Wikipedia volunteers.", "labels": [], "entities": [{"text": "VSM-Hyperlink", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.8798417448997498}]}, {"text": "However, the volunteers may not create the link for every surface form appearing in the article content.", "labels": [], "entities": []}, {"text": "For instance, \"Apple\" occurs 213 times in \"Steve Jobs\" Wikipedia page in our corpus, but only 7 out of these 213 are linked to the \"Apple Inc.\"", "labels": [], "entities": []}, {"text": "The term frequency of \"Apple\" is calculated without considering the partial string matches, for example, we do not count if \"apple\" appears as a substring of any annotated text segment like \"Apple Store\" or \"Apple Lisa\".", "labels": [], "entities": [{"text": "Apple Store\" or \"Apple Lisa\"", "start_pos": 191, "end_pos": 219, "type": "DATASET", "confidence": 0.8104473426938057}]}, {"text": "To obtain the actual frequency of every hyperlink for computing the magnitude of the dimension, we apply \"one sense per discourse\" heuristic (, which assumes that a term tends to have the same meaning in the same discourse.", "labels": [], "entities": []}, {"text": "We link every additional un-linked occurrence of the text segment with the same hyperlink appearing most of the times for the same segment in the article.", "labels": [], "entities": []}, {"text": "The total number of hyperlinks possible in our corpus would be 5 http://en.wikipedia.org/wiki/Wikipedia:Namespace equal to the total number of Wikipedia articles i.e. 3,635,833.", "labels": [], "entities": []}, {"text": "We use 6 different word relatedness benchmarks to evaluate NESA.", "labels": [], "entities": [{"text": "NESA", "start_pos": 59, "end_pos": 63, "type": "TASK", "confidence": 0.7199610471725464}]}, {"text": "WN353 consists of 353 word pairs annotated by 13-15 human experts on a scale of 0-10.", "labels": [], "entities": [{"text": "WN353", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9603894352912903}]}, {"text": "0 refers to un-related and 10 stands for highly related or identical.", "labels": [], "entities": []}, {"text": "This dataset mainly contains generic words like \"money\", \"drink\", \"movie\", etc..", "labels": [], "entities": []}, {"text": "It also contains named entities such as \"Jerusalem\", \"Palestinian\" and \"Israel\", which makes this dataset more challenging for approaches that use only the lexical resources.", "labels": [], "entities": []}, {"text": "WN353Rel and WN353Sim datasets are the subsets of WN353.", "labels": [], "entities": [{"text": "WN353Rel", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9762316346168518}, {"text": "WN353Sim datasets", "start_pos": 13, "end_pos": 30, "type": "DATASET", "confidence": 0.9734641313552856}, {"text": "WN353", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.96269690990448}]}, {"text": "As WN353 contains similar and related word pairs, Agirre at el. refine the WN353 gold standard by splitting it in two parts: related word pairs and similar word pairs.", "labels": [], "entities": [{"text": "WN353", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9164117574691772}]}, {"text": "The notion of similarity and relatedness are defined as follow: two words are similar if they are connected through the taxonomic relations like synonym or hyponym in lexical resources, while two words can be considered related if they are connected through other relations such as meronym and holonym.", "labels": [], "entities": []}, {"text": "For instance, \"football\" and \"soccer\" are two similar words while \"computer\" and \"software\" can be considered as related.", "labels": [], "entities": []}, {"text": "Finally, WN353Rel and WN353Sim contain 252 and 203 word pairs respectively.", "labels": [], "entities": [{"text": "WN353Rel", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.9631240367889404}, {"text": "WN353Sim", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.9244678020477295}]}, {"text": "MC30 is the dataset build by that contains the selected word pairs of WN353.", "labels": [], "entities": [{"text": "MC30", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.948225200176239}, {"text": "WN353", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.9643842577934265}]}, {"text": "The relatedness scores of these words are 97 RG65 is a collection of 65 non-technical word pairs.", "labels": [], "entities": [{"text": "RG65", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.546915590763092}]}, {"text": "These word pairs are annotated by 51 human experts (see for more detail).", "labels": [], "entities": []}, {"text": "MT287 is a relatively newer dataset that contains 287 word pairs.", "labels": [], "entities": [{"text": "MT287", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9690751433372498}]}, {"text": "This dataset is prepared mainly to study the effect of temporal distribution) of a word over several years.", "labels": [], "entities": []}, {"text": "The relatedness scores of the word pairs are obtained from 15-20 mechanical turkers.", "labels": [], "entities": []}, {"text": "We shows the results of the NESA model with different concept relatedness approaches and other state of the art methods of calculating word relatedness.", "labels": [], "entities": [{"text": "calculating word relatedness", "start_pos": 123, "end_pos": 151, "type": "TASK", "confidence": 0.5934941172599792}]}, {"text": "The knowledge-based methods that use lexical resources like WordNet or Roget thesaurus (), achieve higher accuracy if the words in benchmark datasets are available in the knowledge bases.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9622595310211182}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9976896047592163}]}, {"text": "For instance, WordNet-based measures), L&C (,), W&P (, Resnik (Resnik, 1995) J&C (Jiang and Conrath, 1997), Lin (Lin, 1998)) and Roget thesaurus-based measure () achieved higher accuracy on MC30 and RG65 datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9991917014122009}, {"text": "MC30 and RG65 datasets", "start_pos": 190, "end_pos": 212, "type": "DATASET", "confidence": 0.7560825869441032}]}, {"text": "However, these approaches may not fit well for the datasets that contain non-dictionary words, therefore, the accuracy of knowledge-based measures decrease significantly on other datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9987087249755859}]}, {"text": "Corpus-based measures ESA and SSA achieved higher scores than knowledge-based methods on WN353, WN353Rel, WN353Sim and MT287 datasets.", "labels": [], "entities": [{"text": "ESA", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.8186378479003906}, {"text": "WN353", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.9802286028862}, {"text": "WN353Rel", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.8863286375999451}, {"text": "WN353Sim", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.9089237451553345}, {"text": "MT287 datasets", "start_pos": 119, "end_pos": 133, "type": "DATASET", "confidence": 0.9507672786712646}]}, {"text": "Moreover, corpus-based methods performed comparable to knowledge-based methods on MC30 and RG65.", "labels": [], "entities": [{"text": "MC30", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.9477119445800781}, {"text": "RG65", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.7545926570892334}]}, {"text": "Most of the knowledge-based measures use the taxonomic relations for computing word relatedness.", "labels": [], "entities": []}, {"text": "Therefore, these measures 98 obtained poor results on WN353Rel in contrast to WN353Sim dataset.", "labels": [], "entities": [{"text": "WN353Rel", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.9616792798042297}, {"text": "WN353Sim dataset", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.981524646282196}]}, {"text": "However, corpus-based measures performed well for both type of relations i.e. similarity and relatedness.", "labels": [], "entities": []}, {"text": "In this section, we evaluate NESA for word relatedness.", "labels": [], "entities": [{"text": "NESA", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.6978875398635864}, {"text": "word relatedness", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.6765613108873367}]}, {"text": "We experiment by using different concept relatedness measures as explained in section 4 for building the C n,n in NESA model as shown in equations 1 and 2.", "labels": [], "entities": [{"text": "NESA model", "start_pos": 114, "end_pos": 124, "type": "DATASET", "confidence": 0.8666256964206696}]}, {"text": "We use the same filtered Wikipedia articles as used for evaluating the concept relatedness measures in the previous section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Spearman rank correlation of concept related- ness measures with gold standard", "labels": [], "entities": [{"text": "Spearman rank correlation", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.7052541176478068}]}, {"text": " Table 3: Spearman rank correlation of relatedness measures with gold standard datasets", "labels": [], "entities": [{"text": "Spearman", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.8669159412384033}, {"text": "gold standard datasets", "start_pos": 65, "end_pos": 87, "type": "DATASET", "confidence": 0.7190260887145996}]}]}