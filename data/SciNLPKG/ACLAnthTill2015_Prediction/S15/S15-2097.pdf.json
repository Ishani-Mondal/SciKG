{"title": [{"text": "Webis: An Ensemble for Twitter Sentiment Detection", "labels": [], "entities": [{"text": "Twitter Sentiment Detection", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.7271489898363749}]}], "abstractContent": [{"text": "We reproduce four Twitter sentiment classification approaches that participated in previous SemEval editions with diverse feature sets.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.6529980897903442}]}, {"text": "The reproduced approaches are combined in an ensemble, averaging the individual classifiers' confidence scores for the three classes (positive, neutral, negative) and deciding sentiment polarity based on these averages.", "labels": [], "entities": []}, {"text": "The experimental evaluation on Sem-Eval data shows our re-implementations to slightly outperform their respective originals.", "labels": [], "entities": [{"text": "Sem-Eval data", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.7110408991575241}]}, {"text": "Moreover, not too surprisingly, the ensemble of the reproduced approaches serves as a strong baseline in the current edition where it is top-ranked on the 2015 test set.", "labels": [], "entities": [{"text": "2015 test set", "start_pos": 155, "end_pos": 168, "type": "DATASET", "confidence": 0.7812309463818868}]}], "introductionContent": [{"text": "We reproduce four state-of-the-art approaches to classifying the sentiment expressed in a given tweet, and combine the four approaches to an ensemble based on the individual classifiers' confidence scores.", "labels": [], "entities": [{"text": "classifying the sentiment expressed in a given tweet", "start_pos": 49, "end_pos": 101, "type": "TASK", "confidence": 0.8791424334049225}]}, {"text": "In particular, we focus on subtask B of SemEval 2015's task 10 \"Sentiment Analysis in Twitter,\" where the goal is to classify the whole tweet as either positive, neutral, or negative.", "labels": [], "entities": [{"text": "SemEval 2015's task 10 \"Sentiment Analysis in Twitter", "start_pos": 40, "end_pos": 93, "type": "TASK", "confidence": 0.7027760505676269}]}, {"text": "Since the notebook descriptions accompanying submissions to shared tasks are understandably very terse, it is often a challenge to reproduce the results reported.", "labels": [], "entities": []}, {"text": "Therefore, we attempt to reproduce the state-of-the-art Twitter sentiment detection algorithms that have been submitted to the aforementioned task in its previous two editions.", "labels": [], "entities": [{"text": "Twitter sentiment detection", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.6464930872122446}]}, {"text": "Furthermore, we combine the reproduced classifiers in an ensemble.", "labels": [], "entities": []}, {"text": "Since the individual approaches employ diverse feature sets, the goal of the ensemble is to combine their individual strengths.", "labels": [], "entities": []}, {"text": "The paper at hand is a slight extension of the approach from our ECIR 2015 reproducibility track paper such that also text passages are reused.", "labels": [], "entities": [{"text": "ECIR 2015 reproducibility track paper", "start_pos": 65, "end_pos": 102, "type": "DATASET", "confidence": 0.9353319048881531}]}, {"text": "In our ECIR paper, we showed that three selected approaches participating in the SemEval 2013 Twitter sentiment task 2 could be reproduced from the papers accompanying the individual approaches.", "labels": [], "entities": [{"text": "ECIR paper", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9316921830177307}, {"text": "SemEval 2013 Twitter sentiment task 2", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.9150707523028055}]}, {"text": "Adding the best participant of the respective SemEval 2014 task 9 is shown to form a very strong baseline that was not outperformed by the SemEval 2015 participants on the 2015 test data and that also places in the top-10 in the progress test.", "labels": [], "entities": [{"text": "SemEval 2014 task 9", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7507987320423126}]}, {"text": "In Section 2 we briefly describe some related work while in Section 3 we provide more details on the four individual approaches as well as our ensemble scheme.", "labels": [], "entities": []}, {"text": "Some concluding remarks and an outlook on future work close the paper in Section 4.", "labels": [], "entities": []}, {"text": "An experimental evaluation of our approach and an in-depth comparison to the other participants is not included in this paper since it can be found in the task overview ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: F1-scores of the original and reimplemented  classifiers on the SemEval 2013 and 2014 test data and  performance of the final system on the 2015 test data.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9988563060760498}, {"text": "SemEval 2013 and 2014 test data", "start_pos": 74, "end_pos": 105, "type": "DATASET", "confidence": 0.8217190702756246}]}]}