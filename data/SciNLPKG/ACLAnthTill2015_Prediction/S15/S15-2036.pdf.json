{"title": [{"text": "QCRI: Answer Selection for Community Question Answering - Experiments for Arabic and English", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.8890356421470642}, {"text": "Community Question Answering", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.6012775500615438}]}], "abstractContent": [{"text": "This paper describes QCRI's participation in SemEval-2015 Task 3 \"Answer Selection in Community Question Answering\", which targeted real-life Web forums, and was offered in both Arabic and English.", "labels": [], "entities": [{"text": "SemEval-2015 Task 3", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8343177835146586}, {"text": "Answer Selection in Community Question Answering\"", "start_pos": 66, "end_pos": 115, "type": "TASK", "confidence": 0.8034376161439079}]}, {"text": "We apply a supervised machine learning approach considering a manifold of features including among others word n-grams, text similarity, sentiment analysis , the presence of specific words, and the context of a comment.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.9376035928726196}]}, {"text": "Our approach was the best performing one in the Arabic subtask and the third best in the two English subtasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "SemEval-2015 Task 3 \"Answer Selection in Community Question Answering\" challenged the participants to automatically predict the appropriateness of the answers in a community question answering setting.", "labels": [], "entities": [{"text": "Answer Selection in Community Question Answering", "start_pos": 21, "end_pos": 69, "type": "TASK", "confidence": 0.7206828743219376}, {"text": "community question answering setting", "start_pos": 164, "end_pos": 200, "type": "TASK", "confidence": 0.7592057585716248}]}, {"text": "Given a question q \u2208 Q asked by user u q and a set of comments C, the main task was to determine whether a comment c \u2208 C offered a suitable answer to q or not.", "labels": [], "entities": []}, {"text": "In the case of Arabic, the questions were extracted from Fatwa, a community question answering website about Islam.", "labels": [], "entities": []}, {"text": "Each question includes five comments, provided by scholars on the topic, each of which has to be automatically labeled as (i) DIRECT : a direct answer to the question; (ii) RELATED : not a direct answer to the question but with information related to the topic; and (iii) IRRELEVANT : an answer to another question, not related to the topic.", "labels": [], "entities": [{"text": "DIRECT", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9859550595283508}, {"text": "RELATED", "start_pos": 173, "end_pos": 180, "type": "METRIC", "confidence": 0.9931946992874146}, {"text": "IRRELEVANT", "start_pos": 272, "end_pos": 282, "type": "METRIC", "confidence": 0.9648351669311523}]}, {"text": "This is subtask A, Arabic.", "labels": [], "entities": []}, {"text": "In the case of English, the dataset was extracted from Qatar Living, a forum for people to pose questions on multiple aspects of daily life in Qatar.", "labels": [], "entities": [{"text": "Qatar Living", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.9483608305454254}]}, {"text": "Unlike Fatwa, the questions and comments in this dataset come from regular users, making them significantly more varied, informal, open, and noisy.", "labels": [], "entities": [{"text": "Fatwa", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.9181942343711853}]}, {"text": "In this case, the input to the system consists of a question and a variable number of comments, each of which is to be labeled as (i) GOOD : the comment is definitively relevant; (ii) POTENTIAL : the comment is potentially useful; and (iii) BAD : the comment is irrelevant (e.g., it is part of a dialogue, unrelated to the topic, or it is written in a language other than English).", "labels": [], "entities": [{"text": "GOOD", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9960278272628784}, {"text": "POTENTIAL", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9906851649284363}, {"text": "BAD", "start_pos": 241, "end_pos": 244, "type": "METRIC", "confidence": 0.980342447757721}]}, {"text": "This is subtask A, English.", "labels": [], "entities": []}, {"text": "Additionally, a subset of the questions required a YES /NO answer, and there was another subtask for them, which asked to determine whether the overall answer to the question, according to the evidence provided by the comments, is (i) YES , (ii) NO , or (iii) UNSURE . This is subtask B, English.", "labels": [], "entities": [{"text": "YES /NO answer", "start_pos": 51, "end_pos": 65, "type": "METRIC", "confidence": 0.7697120606899261}, {"text": "YES", "start_pos": 235, "end_pos": 238, "type": "METRIC", "confidence": 0.9918438792228699}, {"text": "NO", "start_pos": 246, "end_pos": 248, "type": "METRIC", "confidence": 0.976875901222229}, {"text": "UNSURE", "start_pos": 260, "end_pos": 266, "type": "METRIC", "confidence": 0.9525904655456543}]}, {"text": "Details about the subtasks and the experimental settings can be found in.", "labels": [], "entities": []}, {"text": "Below we describe the supervised learning approach of QCRI, which considers different kinds of features: lexical, syntactic and semantic similarities; the context in which a comment appears; n-grams occurrence; and some heuristics.", "labels": [], "entities": []}, {"text": "We ranked first in the Arabic, and third in the two English subtasks.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 describes the features used, Section 3 discusses our models and our official results, and Section 4 presents post-competition experiments and offers some final remarks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Per-class and macro-averaged F 1 scores for our  official primary and contrastive submissions to SemEval- 2015 Task 3 for Arabic (ar) and English (en), subtasks A  and B.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9538112878799438}, {"text": "SemEval- 2015 Task", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.714128851890564}]}, {"text": " Table 2: Post-submission results for Arabic (ar) and En- glish (en), for subtasks A and B. The lines marked with  only show results using a particular type of features only,  while those marked as without show results when using  all features but those of a particular type. The best results  for each subtask are marked in bold; the results for our of- ficial primary submissions are included for comparison.", "labels": [], "entities": []}]}