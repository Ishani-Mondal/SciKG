{"title": [{"text": "RoseMerry: A Baseline Message-level Sentiment Classification System", "labels": [], "entities": [{"text": "RoseMerry", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9001544713973999}, {"text": "Sentiment Classification", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.8491709530353546}]}], "abstractContent": [{"text": "In this paper, we propose a baseline message-level sentiment classification method, as developed for SemEval-2015 Task 10, Subtask B. This system leverages both hand-crafted features and message-level embedding features , and uses an SVM classifier for message-level sentiment classification.", "labels": [], "entities": [{"text": "message-level sentiment classification", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.7683284878730774}, {"text": "SemEval-2015 Task 10", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.7094994982083639}, {"text": "message-level sentiment classification", "start_pos": 253, "end_pos": 291, "type": "TASK", "confidence": 0.7697994907697042}]}, {"text": "In pre-training the embedding features, we use one million randomly-selected tweets.", "labels": [], "entities": []}, {"text": "We present results over SemEval-2015 Task 10, Subtask B, as well as the Stanford Sentiment Treebank.", "labels": [], "entities": [{"text": "SemEval-2015 Task 10", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7167177200317383}, {"text": "Stanford Sentiment Treebank", "start_pos": 72, "end_pos": 99, "type": "DATASET", "confidence": 0.9300027887026469}]}, {"text": "Our experiments show the effectiveness of our method over both datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "The rise of social media such as blogs and microblogs (e.g., Twitter) has fueled interest in sentiment analysis (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.961155503988266}]}, {"text": "One of the most popular settings for carrying out sentiment analysis is at the sentence level or over individual micro-blog posts, using the simple threelabel class set of POSITIVE, NEGATIVE and NEU-TRAL ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9600900709629059}, {"text": "POSITIVE", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.8554078340530396}]}, {"text": "Sentiment classification has been shown to have utility in various business intelligence applications, including product marketing, identifying new business opportunities, and managing a company's reputation.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9524546563625336}]}, {"text": "Learning effective features plays an important role in building sentiment classification systems (.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.9339397251605988}]}, {"text": "For example, the winning system in the SemEval-2013 message polarity classification task () was based on a rich set of hand-tuned features such as word-sentiment association lexicon features, word n-grams, punctuation, and emoticons, which were combined using a simple SVM-based classifier (.", "labels": [], "entities": [{"text": "SemEval-2013 message polarity classification task", "start_pos": 39, "end_pos": 88, "type": "TASK", "confidence": 0.8587501764297485}]}, {"text": "Recently, there has been a surge of interest in representation learning -automatically learning word and document representations, often in the form of continuous-valued vectors or \"embeddings\" -using auto-encoders or neural network language models ().", "labels": [], "entities": [{"text": "representation learning", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.8757995665073395}]}, {"text": "Of particular relevance to message-level sentiment analysis, proposed a deep learning approach to learn sentiment-specific word representation features, and proposed a neural network auto-encoder to learn message-level vectors.", "labels": [], "entities": [{"text": "message-level sentiment analysis", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.8227115074793497}]}, {"text": "In this paper, we detail RoseMerry, a (strong) baseline sentiment analysis method that combines hand-crafted features with message-level 1 embeddings generated by doc2vec (, using a linear-kernel SVM.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.8686480224132538}]}, {"text": "Our interest in sentiment analysis stems from a desire to use it as part of a commercial text analytics system.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9696413576602936}]}, {"text": "As such, there is an overarching constraint associated with the system and all third-party components must be licensed in a manner which is compatible with commercial use.", "labels": [], "entities": []}, {"text": "In our description below, we point out places where we were unable to use notable resources because of this constraint.", "labels": [], "entities": []}, {"text": "The message-level embeddings are pre-trained using doc2vec over the combination of the training data and a random sample of 1M English tweets, as detailed in Section 2.1.", "labels": [], "entities": []}, {"text": "The hand-crafted features are based heavily off the work of, and are detailed in Section 2.2.", "labels": [], "entities": []}, {"text": "Finally, the d-dimensional message-level embedding is concatenated with the N -dimensional hand-crafted features to form ad + N -dimensional combined feature vector.", "labels": [], "entities": []}, {"text": "We experiment with each of the two feature subsets, in addition to the combined feature set.", "labels": [], "entities": []}, {"text": "One significant divergence from is that we do not use many of the sentiment lexicons, due to non-commercial licensing.", "labels": [], "entities": []}, {"text": "Given that one of the key findings in that work was that lexicons are one of the most reliable features, we expect that this will have a large impact on our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we will detail the experimental setup and the results of our experiments.", "labels": [], "entities": []}, {"text": "We evaluate our method over two labelled datasets, and also two unlabelled datasets to pre-train doc2vec, as detailed below.", "labels": [], "entities": []}, {"text": "Stanford Sentiment Treebank Dataset: a collection of movie review documents from www.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank Dataset", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.9437507092952728}]}, {"text": "rottentomatoes.com, which have been sentence tokenised and annotated for sentiment at the sentence level) and prepartitioned into training and test data, as detailed in. additionally annotated the data at the phrase and lexical levels, but we use only the sentence-level annotations in this paper.", "labels": [], "entities": [{"text": "rottentomatoes.com", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.921978235244751}]}, {"text": "Twitter Dataset: a random sample of 10M English tweets from a 5.3TB Twitter dataset crawled from 18 June to 4 Dec, 2014 using the Twitter Trending API.", "labels": [], "entities": [{"text": "Twitter Dataset", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9335305094718933}]}, {"text": "This is used as additional data to pre-train the message-level embeddings for the SemEval-2015 Dataset.", "labels": [], "entities": [{"text": "SemEval-2015 Dataset", "start_pos": 82, "end_pos": 102, "type": "DATASET", "confidence": 0.8308951556682587}]}, {"text": "IMDB Dataset: a 100K sentence movie review dataset from www.imdb.com, collected by.", "labels": [], "entities": [{"text": "IMDB Dataset", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9857405126094818}]}, {"text": "This is used as additional data to pretrain the message-level embeddings for the Stanford Sentiment Treebank dataset.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank dataset", "start_pos": 81, "end_pos": 116, "type": "DATASET", "confidence": 0.9348829835653305}]}, {"text": "To evaluate the effectiveness of the different feature sets, we report on results as follows: \u2022 RM-manual: only hand-crafted features \u2022 RM-doc2vec: only message-level embeddings \u2022 RM-all: both hand-crafted features and message-level embeddings As our primary evaluation metric, we use F1 PN , which is the average F1 PN for the POSITIVE (i.e., F1 pos ) and NEGATIVE classes (i.e., F1 neg ): We also report the overall classification accuracy (Acc) across the three classes, and the F1 PN score of each class (i.e., F1 pos , F1 neg and F1 neu ).", "labels": [], "entities": [{"text": "F1", "start_pos": 285, "end_pos": 287, "type": "METRIC", "confidence": 0.9969555139541626}, {"text": "F1", "start_pos": 314, "end_pos": 316, "type": "METRIC", "confidence": 0.9829666614532471}, {"text": "F1", "start_pos": 381, "end_pos": 383, "type": "METRIC", "confidence": 0.9463683366775513}, {"text": "classification accuracy (Acc)", "start_pos": 418, "end_pos": 447, "type": "METRIC", "confidence": 0.7696992039680481}, {"text": "F1 PN score", "start_pos": 482, "end_pos": 493, "type": "METRIC", "confidence": 0.9722923437754313}, {"text": "F1", "start_pos": 535, "end_pos": 537, "type": "METRIC", "confidence": 0.8794998526573181}]}, {"text": "For the message-level embeddings, we used d = 100 and a context window size of 10.", "labels": [], "entities": []}, {"text": "We used LibSVM with a linear-kernel and default parameter settings.", "labels": [], "entities": []}, {"text": "In this section, we present the results first over the SemEval-2015 datasets, and then over the Stanford Sentiment Treebank.", "labels": [], "entities": [{"text": "SemEval-2015 datasets", "start_pos": 55, "end_pos": 76, "type": "DATASET", "confidence": 0.9171154797077179}, {"text": "Stanford Sentiment Treebank", "start_pos": 96, "end_pos": 123, "type": "DATASET", "confidence": 0.9346776008605957}]}], "tableCaptions": [{"text": " Table 1: The number of POSITIVE, NEGATIVE, NEUTRAL  documents in the SemEval-2015 dataset", "labels": [], "entities": [{"text": "SemEval-2015 dataset", "start_pos": 70, "end_pos": 90, "type": "DATASET", "confidence": 0.7127063125371933}]}, {"text": " Table 2: The number of POSITIVE, NEGATIVE and  NEUTRAL sentences in the Stanford Sentiment Treebank  dataset", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank  dataset", "start_pos": 73, "end_pos": 109, "type": "DATASET", "confidence": 0.8972543627023697}]}, {"text": " Table 3: The official evaluation results for the SemEval-2015 Test and Progress Test set (F1 PN )", "labels": [], "entities": [{"text": "SemEval-2015 Test and Progress Test set (F1 PN )", "start_pos": 50, "end_pos": 98, "type": "DATASET", "confidence": 0.7270541310310363}]}]}