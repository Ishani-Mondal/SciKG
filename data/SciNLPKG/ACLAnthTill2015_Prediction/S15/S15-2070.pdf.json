{"title": [{"text": "ULisboa: Recognition and Normalization of Medical Concepts", "labels": [], "entities": [{"text": "ULisboa", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.860136091709137}, {"text": "Recognition and Normalization of Medical Concepts", "start_pos": 9, "end_pos": 58, "type": "TASK", "confidence": 0.8541233837604523}]}], "abstractContent": [{"text": "This paper describes a system developed for the disorder identification subtask within task 14 of SemEval 2015.", "labels": [], "entities": [{"text": "disorder identification subtask", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.7638953030109406}, {"text": "SemEval 2015", "start_pos": 98, "end_pos": 110, "type": "TASK", "confidence": 0.7187910974025726}]}, {"text": "The developed system is based on a chain of two modules, one for recognition and another for normaliza-tion.", "labels": [], "entities": []}, {"text": "The recognition module is based on an adapted version of the Stanford NER system to train CRF models in order to recognize disorder mentions.", "labels": [], "entities": []}, {"text": "CRF models were build based on a novel encoding of entity spans as token classifications to also consider non-continuous entities, along with a rich set of features based on (i) domain lexicons and (ii) Brown clusters inferred from a large collection of clinical texts.", "labels": [], "entities": []}, {"text": "For disorder normalization, we (i) generated anon ambiguous dictionary of abbreviations from the labelled files, using it together with (ii) an heuristic method based on similarity search and (iii) a comparison method based on the information content of each disorder.", "labels": [], "entities": [{"text": "disorder normalization", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7062099277973175}]}, {"text": "The system achieved an F-measure of 0.740 (the second best), with a precision of 0.779, a recall of 0.705.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9996472597122192}, {"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.998848557472229}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9993100166320801}]}], "introductionContent": [{"text": "Clinical notes are an important source of information recorded by medical professionals.", "labels": [], "entities": []}, {"text": "However, this information, when available, is not easily accessible within automated procedures.", "labels": [], "entities": []}, {"text": "Clinical notes are inherently complex, due to their lack of structure (i.e., narrative language) and due to the need for contextual interpretation.", "labels": [], "entities": []}, {"text": "To address this complexity, text mining approaches represent an effective solution to assist the users in retrieving and extracting the required information.", "labels": [], "entities": [{"text": "text mining", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.8342360556125641}]}, {"text": "This paper presents a text mining system for processing clinical text, that we developed for SemEval based on a pipeline with two modules, one for entity recognition and another for normalization.", "labels": [], "entities": [{"text": "text mining", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.7056791037321091}, {"text": "entity recognition", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.7339692115783691}]}, {"text": "The entity recognition module is based on the Stanford NER tool (), and it uses CRF models trained on annotated biomedical notes.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8333801627159119}, {"text": "Stanford NER tool", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.8178857366243998}]}, {"text": "The module tags the text according to an SBIEON encoding of entities as token classes, supporting the recognition of non-continuous entities ().", "labels": [], "entities": []}, {"text": "We relied on features based on Brown clusters and domain specific lexicons.", "labels": [], "entities": []}, {"text": "Thus, this approach combines both supervised (Stanford NER) and unsupervised methods (Brown Clusters).", "labels": [], "entities": [{"text": "Stanford NER)", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.854767640431722}, {"text": "Brown Clusters", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.9360540211200714}]}, {"text": "For practical applications, entity recognition is incomplete without performing normalization, i.e. without mapping each entity to an identifier (CUI) in a controlled vocabulary like SNOMED CT, that defines its semantic meaning.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7983834743499756}]}, {"text": "One of the main challenges in this task consists in resolving the ambiguous cases, where the same entity can have distinct semantic meanings (i.e., mapped to distinct CUIs) depending on the context.", "labels": [], "entities": []}, {"text": "Our normalization module relies on the following components: (i) a procedure for the automatic generation of auxiliary dictionaries from the labelled training data (e.g, abbreviations) and from SNOMED CT, to be used as mapping dictionaries, (ii) an heuristic for similarity search, and (iii) an information content measure for each concept.", "labels": [], "entities": [{"text": "SNOMED CT", "start_pos": 194, "end_pos": 203, "type": "DATASET", "confidence": 0.7076962888240814}, {"text": "similarity search", "start_pos": 263, "end_pos": 280, "type": "TASK", "confidence": 0.678181067109108}]}, {"text": "Our system is an extension of the one used in the 2014 edition of SemEval ().", "labels": [], "entities": []}, {"text": "Both systems used the same approach for entity recognition but, in terms of the normalization component, the system from 2014 was entirely based on a lexical similarity approach using NGram, Levenstein and JaroWinkler distances.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.8592735528945923}, {"text": "NGram", "start_pos": 184, "end_pos": 189, "type": "DATASET", "confidence": 0.9440178871154785}]}, {"text": "The current system is instead based on a pipeline were the information content was also incorporated.", "labels": [], "entities": []}, {"text": "Besides SNOMED CT, the current system also integrated dictionaries automatically generated from the training data.", "labels": [], "entities": [{"text": "SNOMED CT", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.6104719340801239}]}], "datasetContent": [{"text": "Similarly to the last edition of the competition (), two sets of labelled data were given to the participants, which were separated into two categories (training and development).", "labels": [], "entities": []}, {"text": "They were used for training and testing of our system, respectively.", "labels": [], "entities": []}, {"text": "Unlabelled clinical notes from the MIMIC corpus were also provided.", "labels": [], "entities": [{"text": "MIMIC corpus", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.8612526059150696}]}, {"text": "Later, an unlabelled test set was released to evaluate the final system.", "labels": [], "entities": []}, {"text": "Unlabelled clinical notes consisted on plain text without any additional information, while labelled clinical notes consist on plain text together with a list of disorder mentions contained on them.", "labels": [], "entities": []}, {"text": "--: Statistical characterization of the datasets.", "labels": [], "entities": [{"text": "Statistical characterization", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.8910258114337921}]}, {"text": "Three runs were submitted to the SemEval 2015 competition: Run 1: A 2nd-order CRF model was trained using the SBIEON encoding, and a rich set of features that includes the domain lexicons and 100 Brown clusters.", "labels": [], "entities": [{"text": "SemEval 2015 competition", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.8527823289235433}]}, {"text": "For training, we only used notes from the training set.", "labels": [], "entities": []}, {"text": "For assigning a UMLS identifier to each entity, we used the framework that was previously described.", "labels": [], "entities": []}, {"text": "Run 2: This run is identical to Run 1 with the exception of the domain lexicon features that were not included.", "labels": [], "entities": []}, {"text": "Normalization followed the same strategy as in Run 1.", "labels": [], "entities": []}, {"text": "Run 3: Identical to Run 1, with the exception that both train and devel documents were used as training data, resulting in the addition of 133 notes to the training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistical characterization of the datasets.", "labels": [], "entities": [{"text": "Statistical characterization", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.9100472629070282}]}, {"text": " Table 2: The official results for Task 1 of the SemEval 2015 challenge on clinical NLP.", "labels": [], "entities": [{"text": "SemEval 2015 challenge on clinical NLP", "start_pos": 49, "end_pos": 87, "type": "TASK", "confidence": 0.7845829427242279}]}]}