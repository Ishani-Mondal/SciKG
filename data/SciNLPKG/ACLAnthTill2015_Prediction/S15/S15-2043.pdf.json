{"title": [{"text": "Voltron: A Hybrid System For Answer Validation Based On Lexical And Distance Features", "labels": [], "entities": [{"text": "Answer Validation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.9833028316497803}]}], "abstractContent": [{"text": "The purpose of this paper is to describe our submission to the SemEval-2015 Task 3 on Answer Selection in Community Question Answering.", "labels": [], "entities": [{"text": "SemEval-2015 Task 3", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.729711671670278}, {"text": "Answer Selection in Community Question Answering", "start_pos": 86, "end_pos": 134, "type": "TASK", "confidence": 0.8037627538045248}]}, {"text": "We participated in subtask A, where the systems had to classify community answers fora given question as definitely relevant , potentially useful, or irrelevant.", "labels": [], "entities": []}, {"text": "For every question-answer pair in the training data we extract a vector with a variety of features.", "labels": [], "entities": []}, {"text": "These vectors are then fed to a MaxEnt classi-fier for training.", "labels": [], "entities": [{"text": "MaxEnt", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.9033480286598206}]}, {"text": "Given a question and an answer the trained classifier outputs class probabilities for each of the three desired categories.", "labels": [], "entities": []}, {"text": "The one with the highest probability is chosen.", "labels": [], "entities": []}, {"text": "Our system scores better than the average score in subtask A of Task 3.", "labels": [], "entities": []}], "introductionContent": [{"text": "Nowadays, text analysis and semantic similarity are subject to a lot of research and experiments due to the growth of social media influence, the increasing usage of forums for finding a solution of common known problems and the Web upgrowth.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.842345803976059}, {"text": "semantic similarity", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.6977230161428452}]}, {"text": "As beginners in the computational linguistics field, we were very interested in dealing with these topics and have found Answer Validation as a good start.", "labels": [], "entities": [{"text": "Answer Validation", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.9223828315734863}]}, {"text": "Our team chose to focus on subtask A of Task 3 in the SemEval-2015 workshop, namely Answer selection in community question answering data.", "labels": [], "entities": [{"text": "Answer selection in community question answering", "start_pos": 84, "end_pos": 132, "type": "TASK", "confidence": 0.7984609603881836}]}, {"text": "In order to achieve good results, we combined most of the techniques familiar to us.", "labels": [], "entities": []}, {"text": "We process the data as question-answer pairs.", "labels": [], "entities": []}, {"text": "The framework GATE) was used for the preprocessing in the system because it offers convenient natural language processing pipelines and has an API allowing for system integration.", "labels": [], "entities": []}, {"text": "For classification we used the Maximum Entropy classifier provided by MALLET ().", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9680297374725342}, {"text": "MALLET", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.5664131045341492}]}, {"text": "We use a combination of surface, morphological, syntactic, and contextual features as well as distance metrics between the question and answer.", "labels": [], "entities": []}, {"text": "Distance metrics are based on word2vec () and DKPro Similarity, ().", "labels": [], "entities": [{"text": "DKPro", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.6905769109725952}, {"text": "Similarity", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.7193374633789062}]}], "datasetContent": [{"text": "Various experiments were conducted to analyse the contribution of the chosen features.", "labels": [], "entities": []}, {"text": "In each of them, training was performed on the combined data from the train and development datasets, provided by the organizers.", "labels": [], "entities": []}, {"text": "Testing was done on the official test dataset used for evaluation of the task, after it was released by the organizers.", "labels": [], "entities": []}, {"text": "The analysis will only focus on the coarse-grained evaluation in the three main classes (Good, Potential, Bad) since our system does not try to target the finer-grained classification.", "labels": [], "entities": []}, {"text": "We defined our baseline system as the one that uses only the lexical and structural features described in the Method section, i.e. word tokens, sentence, question and answer length, as well as the bigrams and trigrams of the question-answer pair.", "labels": [], "entities": []}, {"text": "With only these features, the system is very weakthe accuracy as reported by the scorer script against the gold standard is 44.18% and the F1 score is 24.05%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9997069239616394}, {"text": "F1 score", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9904310703277588}]}, {"text": "Next, we included the features that rely on GATE gazetteers, such as the named entities features.", "labels": [], "entities": [{"text": "GATE gazetteers", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.8217655420303345}]}, {"text": "This improved the system's performance by more than 1%, reaching accuracy of 45.14% and F1 score of 25.33%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9990143775939941}, {"text": "F1 score", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9450004398822784}]}, {"text": "Another experiment we did was to add to the baseline system only the DKPro cosine similarity.", "labels": [], "entities": [{"text": "DKPro cosine similarity", "start_pos": 69, "end_pos": 92, "type": "METRIC", "confidence": 0.591347356637319}]}, {"text": "This approach yielded a significant increase in the scores on the test set over the baseline system, around 4%.", "labels": [], "entities": []}, {"text": "Finally, we tested the baseline system with the word2vec cosine values.", "labels": [], "entities": []}, {"text": "This experiment was not as successful as the others, offering no improvement.", "labels": [], "entities": []}, {"text": "The result maybe attributed to the fact that we use a set of vectors trained on generic Web data instead of vectors specifically trained for the SemEval task.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 145, "end_pos": 157, "type": "TASK", "confidence": 0.9044156670570374}]}, {"text": "However, the community generated datasets are not sufficiently large and cannot be used for adequate word2vec training.", "labels": [], "entities": []}, {"text": "When all features were combined, the scores were boosted to 50% accuracy and 32.02% F1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9997164607048035}, {"text": "F1", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9996576309204102}]}, {"text": "The improvement from the baseline system is greater than the accumulated improvement from adding the single features because those features influence each other.", "labels": [], "entities": []}, {"text": "All of the described experiments were done on the data from the train and development sets.", "labels": [], "entities": []}, {"text": "However, when preparing our final submission for the competition, we trained our system on a training set that included the development data twice.", "labels": [], "entities": []}, {"text": "This way more weight was given to those question-answer pairs.", "labels": [], "entities": []}, {"text": "The result was an impressive 14% increase in our F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9677338600158691}]}, {"text": "In order to further analyse this surprising result, we did train a MaxEnt classifier using only the smaller development dataset.", "labels": [], "entities": []}, {"text": "All described features were combined here as well.", "labels": [], "entities": []}, {"text": "The experiment showed that indeed the larger train dataset provided for the competition has less effect on the performance of our system than the smaller development dataset.", "labels": [], "entities": []}, {"text": "We suspect that the contents of the test dataset are closer to the development dataset because that would mean more common n-gram features are detected.", "labels": [], "entities": []}, {"text": "This would explain the boost in the F1 score and the accuracy.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9874265193939209}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9994357228279114}]}, {"text": "A summary of the results obtained in the experiments can be seen in   It should be noted that the results are greatly impacted by the low score we get on the Potential answers class.", "labels": [], "entities": []}, {"text": "The scores on this label are very close to 0 with all devised systems, which is to be expected since none of our features were specifically targeted at distinguishing Potential answers from Good and Bad ones.", "labels": [], "entities": []}, {"text": "In all experiments, the highest precision and recall were achieved on the Bad answers.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9996541738510132}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9996706247329712}]}], "tableCaptions": [{"text": " Table 1: Accuracy and F1 score achieved using various  combinations of features", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99949049949646}, {"text": "F1 score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9650695323944092}]}, {"text": " Table 2: Accuracy and F1 score achieved using all fea- tures, but extracted from different training datasets", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994921684265137}, {"text": "F1 score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9795638918876648}]}]}