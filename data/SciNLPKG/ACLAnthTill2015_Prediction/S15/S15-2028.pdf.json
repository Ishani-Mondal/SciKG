{"title": [{"text": "FCICU: The Integration between Sense-Based Kernel and Surface- Based Methods to Measure Semantic Textual Similarity", "labels": [], "entities": [{"text": "FCICU", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9131446480751038}]}], "abstractContent": [{"text": "This paper describes FCICU team participation in SemEval 2015 for Semantic Textual Similarity challenge.", "labels": [], "entities": [{"text": "FCICU", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.7964594960212708}, {"text": "SemEval 2015", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.8547925353050232}, {"text": "Semantic Textual Similarity challenge", "start_pos": 66, "end_pos": 103, "type": "TASK", "confidence": 0.8309993743896484}]}, {"text": "Our main contribution is to propose a word-sense similarity method using BabelNet relationships.", "labels": [], "entities": [{"text": "word-sense similarity", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7165095806121826}]}, {"text": "In the English subtask challenge, we submitted three systems (runs) to assess the proposed method.", "labels": [], "entities": []}, {"text": "In Run1, we used our proposed method coupled with a string kernel mapping function to calculate the textual similarity.", "labels": [], "entities": []}, {"text": "In Run2, we used the method with a tree kernel function.", "labels": [], "entities": []}, {"text": "In Run3, we averaged Run1 with a previously proposed surface-based approach as a kind of integration.", "labels": [], "entities": [{"text": "Run3", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.9049033522605896}]}, {"text": "The three runs are ranked 41 st , 57 th , and 20 th of 73 systems, with mean correlation 0.702, 0.597, and 0.759 respectively.", "labels": [], "entities": []}, {"text": "For the interpretable task, we submitted a modified version of Run1 achieving mean F1 0.846, 0.461, 0.722, and 0.44 for alignment, type, score, and score with type respectively.", "labels": [], "entities": [{"text": "interpretable task", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.9036913812160492}, {"text": "F1", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.8471720814704895}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) is the task of measuring the similarity between two text snippets according to their meaning.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8000997851292292}]}, {"text": "Human has an intrinsic ability to recognize the degree of similarity and difference between texts.", "labels": [], "entities": []}, {"text": "Simulating the process of human judgment in computers is still an extremely difficult task and has recently drawn much attention.", "labels": [], "entities": [{"text": "Simulating the process of human judgment", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.687010665734609}]}, {"text": "STS is very important because a wide range of NLP applications such as information retrieval, question answering, machine translation, etc.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9316848516464233}, {"text": "information retrieval", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.7987265288829803}, {"text": "question answering", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.9052455127239227}, {"text": "machine translation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7990822196006775}]}, {"text": "rely heavily on this task.", "labels": [], "entities": []}, {"text": "This paper describes our proposed STS systems by which we participated in two subtasks of STS task (Task2) at SemEval 2015, namely English STS and Interpretable STS.", "labels": [], "entities": []}, {"text": "The former calculates a graded similarity score from 0 to 5 between two sentences (with 5 being the most similar), while the latter is a pilot subtask that requires aligning chunks of two sentences, describing what kind of relation exists between each pair of chunks, and a score for the similarity between the pair of chunks (.", "labels": [], "entities": []}, {"text": "Sense or meaning of natural language text can be inferred from several linguistic concepts, including lexical, syntactic, and semantic knowledge of the language.", "labels": [], "entities": []}, {"text": "Our approach employs those aspects to calculate the similarity between senses of text constituents, phrases or words, relying mainly on BabelNet senses.", "labels": [], "entities": []}, {"text": "The similarity between two text snippets is firstly calculated using kernel functions, which map a text snippet to the feature space based on a proposed word sense similarity method.", "labels": [], "entities": []}, {"text": "Besides, the sense-based similarity score obtained is combined with a surface-based similarity score to study the consolidation impact in the STS task.", "labels": [], "entities": [{"text": "sense-based similarity score", "start_pos": 13, "end_pos": 41, "type": "METRIC", "confidence": 0.7018801967302958}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 explains our proposed word sense similarity method.", "labels": [], "entities": [{"text": "word sense similarity", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.688556432723999}]}, {"text": "Section 3 describes the proposed systems.", "labels": [], "entities": []}, {"text": "Section 4 presents the experiments conducted and analyzes the results achieved.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper and suggests some future directions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Our Results on SemEval-2015 Test Datasets.", "labels": [], "entities": [{"text": "SemEval-2015 Test Datasets", "start_pos": 25, "end_pos": 51, "type": "DATASET", "confidence": 0.8113259871800741}]}, {"text": " Table 2. Results of Run3 vs. SC on SemEval-2014 Test Datasets (SemEval-2015 Training dataset).", "labels": [], "entities": [{"text": "SemEval-2014 Test Datasets", "start_pos": 36, "end_pos": 62, "type": "DATASET", "confidence": 0.8718211650848389}, {"text": "SemEval-2015 Training dataset)", "start_pos": 64, "end_pos": 94, "type": "DATASET", "confidence": 0.7644773572683334}]}]}