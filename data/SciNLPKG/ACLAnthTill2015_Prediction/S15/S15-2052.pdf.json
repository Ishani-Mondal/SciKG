{"title": [{"text": "UTH-CCB: The Participation of the SemEval 2015 Challenge -Task 14 \ud97b\udf59 \ud97b\udf59", "labels": [], "entities": [{"text": "UTH-CCB", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8191965818405151}, {"text": "SemEval 2015 Challenge -Task", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.8610010623931885}]}], "abstractContent": [{"text": "This paper describes the system developed by", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing (NLP) plays a critical role in unlocking important patient information from narrative clinical texts, to support various clinical applications such as decision support systems and translational research.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7521096765995026}, {"text": "translational research", "start_pos": 208, "end_pos": 230, "type": "TASK", "confidence": 0.9271633923053741}]}, {"text": "One of the very important tasks for clinical NLP research is to extract clinical concepts such as diseases and treatments.", "labels": [], "entities": []}, {"text": "Many clinical NLP systems such as MedLEE system, MetaMAP system (Aronson and Lang, 2010) and cTAKES system (, have been developed to extract these important clinical concepts from text.", "labels": [], "entities": []}, {"text": "A number of shared tasks for clinical concepts extraction have been organized by different entities, including i2b2 (The Center for Informatics for Integrating Biology and the Bedside), ShARe/CLEF eHealth Evaluation Lab, and SemEval (International Workshop on Semantic Evaluation) ().", "labels": [], "entities": [{"text": "clinical concepts extraction", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.7323140899340311}, {"text": "ShARe/CLEF eHealth Evaluation Lab", "start_pos": 186, "end_pos": 219, "type": "TASK", "confidence": 0.49482786158720654}, {"text": "SemEval (International Workshop on Semantic Evaluation)", "start_pos": 225, "end_pos": 280, "type": "TASK", "confidence": 0.5251984763890505}]}, {"text": "These challenges have greatly promoted clinical NLP research by building benchmark datasets and innovative methods.", "labels": [], "entities": []}, {"text": "The 2015 SemEval Shared Task 14, entitled \"Analysis of Clinical Text\", is to identify disorders and their modifiers from clinical text, which is an extension of the SemEval-2014 challenge.", "labels": [], "entities": [{"text": "SemEval Shared Task", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7701040903727213}]}, {"text": "The 2015 SemEval challenge consists of two subtasks: Task 1 -disorder recognition, where disorder entities need to be detected and normalized to UMLS CUIs, and Task 2 -disorder slot filling, where the normalized value for nine types of modifiers of disorders are to be identified.", "labels": [], "entities": [{"text": "SemEval challenge", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.8567298352718353}, {"text": "disorder recognition", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.7552424073219299}, {"text": "disorder slot filling", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.708436131477356}]}, {"text": "Task 2 is further divided into two subtasks: 1) Task 2A -identifying modifiers based on gold standard disorders; and 2) Task 2B -identifying modifiers based on disorders recognized by our system, an end-to-end evaluation.", "labels": [], "entities": []}, {"text": "In this paper, we describe our approaches and results for both tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "For this shared task, organizers prepared three datasets: 1) training set -298 clinical documents, 2) development set -133 documents and 3) test set -100 documents.", "labels": [], "entities": []}, {"text": "We developed our models using the training set and optimized parameters using the development set.", "labels": [], "entities": []}, {"text": "For final submissions on the test set, we combined training and development sets to build the machine learning classifiers.", "labels": [], "entities": []}, {"text": "We combined training and development datasets to build our final models for all tasks.", "labels": [], "entities": []}, {"text": "Since each task allows for three submissions, we tried different strategies for the three runs.", "labels": [], "entities": []}, {"text": "For Task 1, run 0 and run 1 used the ensemble MV method to get better F1; while run 2 used the ensemble ML method to get higher precision, in disorder entity recognition.", "labels": [], "entities": [{"text": "F1", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9993657469749451}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9980243444442749}, {"text": "disorder entity recognition", "start_pos": 142, "end_pos": 169, "type": "TASK", "confidence": 0.5979233384132385}]}, {"text": "For Task 2A and Task 2B, run 0 and run 2 used two sets of parameters optimized for better weighted performances; while run 1 used a set of parameters optimized for un-weighted performance.", "labels": [], "entities": []}, {"text": "For body location recognition, only run 2 of Task 2A used SSVMs model, all other runs used CRFs models for better prediction.", "labels": [], "entities": [{"text": "body location recognition", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.9387135108311971}]}, {"text": "The evaluation metrics for this task include F-1 score (strict vs. relaxed), un-weighted accuracy, and weighted accuracy etc., as defined by the organizers.", "labels": [], "entities": [{"text": "F-1 score", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9817283153533936}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9350914359092712}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.8257594108581543}]}, {"text": "For more details, please refer to the task description paper or the task website \u2020 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The performances of the three runs of our system  on Task 1.", "labels": [], "entities": []}, {"text": " Table 2. The overall performances of our system on Task  2.", "labels": [], "entities": []}]}