{"title": [{"text": "Combining Seemingly Incompatible Corpora for Implicit Semantic Role Labeling", "labels": [], "entities": [{"text": "Combining Seemingly Incompatible Corpora", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7168203294277191}, {"text": "Implicit Semantic Role Labeling", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.5789681822061539}]}], "abstractContent": [{"text": "Implicit semantic role labeling, the task of retrieving locally unrealized arguments from wider discourse context, is a knowledge-intensive task.", "labels": [], "entities": [{"text": "Implicit semantic role labeling", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8434334695339203}]}, {"text": "At the same time, the annotated corpora that exist are all small and scattered across different annotation frameworks, genres, and classes of predicates.", "labels": [], "entities": []}, {"text": "Previous work has treated these corpora as incompatible with one another, and has concentrated on optimizing the exploitation of single corpora.", "labels": [], "entities": []}, {"text": "In this paper, we show that corpus combination is effective after all when the differences between corpora are bridged with domain adaptation methods.", "labels": [], "entities": [{"text": "corpus combination", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7617340087890625}]}, {"text": "When we combine the SemEval-2010 Task 10 and Gerber and Chai noun corpora, we obtain substantially improved performance on both corpora, for all roles and parts of speech.", "labels": [], "entities": [{"text": "SemEval-2010 Task 10", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.7209359010060629}]}, {"text": "We also present new insights into the properties of the implicit semantic role labeling task.", "labels": [], "entities": [{"text": "implicit semantic role labeling task", "start_pos": 56, "end_pos": 92, "type": "TASK", "confidence": 0.6547492206096649}]}], "introductionContent": [{"text": "Semantic role labeling (SRL) is the task of identifying semantic arguments of predicates in text.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8506356875101725}]}, {"text": "It is an important step in text analysis and has applications in information extraction (, question answering) and machine translation ( . A large body of work exists on algorithms for SRL ().", "labels": [], "entities": [{"text": "text analysis", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.8695011138916016}, {"text": "information extraction", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.797556608915329}, {"text": "question answering", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.8382912576198578}, {"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.8440820574760437}, {"text": "SRL", "start_pos": 185, "end_pos": 188, "type": "TASK", "confidence": 0.9941956400871277}]}, {"text": "Their success is closely connected to the availability of two large, hand-constructed semantic role resources, FrameNet ( and.", "labels": [], "entities": []}, {"text": "They used to concentrate on overt semantic roles, that is, semantic roles that are realized within the local syntactic structure of the predicate.", "labels": [], "entities": []}, {"text": "Recent years have seen a broadening of the focus in SRL to implicit semantic roles, that is all roles that remain locally unrealized but can be retrieved in the (typically prior) context).", "labels": [], "entities": [{"text": "SRL", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9794685244560242}]}, {"text": "In the following example annotated with PropBank roles (cf. Section 2), the target predicate come has two roles, a locally realized one (A1, the entity in motion), it, and an implicit role mentioned in the previous sentence (A4, the goal): Well, sir, it's [ A4 this lonely, silent house] and the queer thing in the kitchen . ...", "labels": [], "entities": []}, {"text": "I thought [ A1 it] had come again.", "labels": [], "entities": []}, {"text": "Implicit SRL is useful to complete predicates' argument structures for inference and paraphrasing, or to assess the coherence of discourse ().", "labels": [], "entities": [{"text": "SRL", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9345269203186035}]}, {"text": "It however requires (even) more training data than traditional SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9441661834716797}]}, {"text": "One reason is that potential arguments come from the whole text rather than just the sentence.", "labels": [], "entities": []}, {"text": "Another one is that most of the powerful syntactic features that area staple in traditional SRL are unavailable across sentence boundaries.", "labels": [], "entities": [{"text": "SRL", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9567235112190247}]}, {"text": "Unfortunately, existing corpora for implicit SRL are quite small: The task requires full-text annotation, which is time-consuming and pushes semantic role frameworks to their limits . It is also hard to do consistently, and can only be crowdsourced in limited settings).", "labels": [], "entities": [{"text": "SRL", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.6616094708442688}]}, {"text": "Thus, even though multiple systems for implicit SRL exist (among others,,,), results are still relatively poor.", "labels": [], "entities": [{"text": "SRL", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8513909578323364}]}, {"text": "In this paper, we focus on the fact that the corpora that exist for implicit SRL differ not only in the semantic role frameworks used (FrameNet vs. PropBank), but also in genre (newswire vs. novels), and classes of annotated predicates (verbs vs. nouns).", "labels": [], "entities": [{"text": "SRL", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.7805602550506592}]}, {"text": "As a result, they are generally regarded as incompatible, and previous work has concentrated on getting most out of individual corpora, or spending annotation effort on focused extensions of these corpora.", "labels": [], "entities": []}, {"text": "Instead, we will follow the intuition that the performance of implicit SRL can be improved significantly by combining corpora, using simple domain adaptation techniques to bridge the differences between them.", "labels": [], "entities": [{"text": "SRL", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.8119823932647705}]}, {"text": "We combine the two largest datasets for implicit SRL, the SemEval-2010 Task 10 dataset and the Gerber and Chai dataset (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.7208320498466492}, {"text": "SemEval-2010 Task 10 dataset", "start_pos": 58, "end_pos": 86, "type": "DATASET", "confidence": 0.6270013079047203}, {"text": "Gerber and Chai dataset", "start_pos": 95, "end_pos": 118, "type": "DATASET", "confidence": 0.5637736767530441}]}, {"text": "This combination achieves improvements across all target and semantic roles despite the differences in genre, domain, and parts of speech.", "labels": [], "entities": []}, {"text": "Our analyses indicates that the properties of the implicit SRL task -where syntactic features play a relatively minor role compared to semantic and discourse features -are responsible for this picture, and mean that models can actually profit from complementarity between combined corpora.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 59, "end_pos": 67, "type": "TASK", "confidence": 0.8856108784675598}]}, {"text": "Section 2 summarizes the resource and model situation in SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.873792827129364}]}, {"text": "Section 3 defines a simple system for implicit SRL that uses domain adaptation.", "labels": [], "entities": [{"text": "SRL", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.7873000502586365}, {"text": "domain adaptation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7089699357748032}]}, {"text": "Sections 4 and 5 report experiments and provide analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiment 1 extends the SEMEVAL training data with out-of-domain data from GERBERCHAI and evaluates on SEMEVAL.", "labels": [], "entities": [{"text": "SEMEVAL training", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.8167650103569031}, {"text": "GERBERCHAI", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.9276329278945923}, {"text": "SEMEVAL", "start_pos": 104, "end_pos": 111, "type": "DATASET", "confidence": 0.6417443156242371}]}, {"text": "Experiment 2 swaps the setup, extending the GERBER-CHAI dataset with SEMEVAL data and evaluating on GERBERCHAI.", "labels": [], "entities": [{"text": "GERBER-CHAI dataset", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.9462109804153442}, {"text": "SEMEVAL data", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.7203386574983597}, {"text": "GERBERCHAI", "start_pos": 100, "end_pos": 110, "type": "DATASET", "confidence": 0.9252675771713257}]}, {"text": "Experiment 3 aims at providing a better understanding of these observations.", "labels": [], "entities": []}, {"text": "In this experiment, we evaluate our approach on the SEMEVAL dataset (SEMEVAL is the target domain and GERBERCHAI is the source domain).", "labels": [], "entities": [{"text": "SEMEVAL dataset", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.7271941006183624}, {"text": "GERBERCHAI", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9442003965377808}]}, {"text": "Since there is an established split of SEMEVAL into training and test parts, we simply use the test part for evaluation, and designate the SEMEVAL training part as well as GERBERCHAI for training.", "labels": [], "entities": [{"text": "GERBERCHAI", "start_pos": 172, "end_pos": 182, "type": "METRIC", "confidence": 0.9889856576919556}]}, {"text": "We compare four experimental scenarios (cf. Table 3): (1) The standard \"in-domain\" setup that only uses SEMEVAL, as assumed by most studies on the dataset.", "labels": [], "entities": [{"text": "SEMEVAL", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.6359869837760925}]}, {"text": "(2) A pure \"out-of-domain\" setup where we use only GERBERCHAI as training data.", "labels": [], "entities": [{"text": "GERBERCHAI", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.5990175604820251}]}, {"text": "Of course, there is reason to believe that this strategy will perform quite poorly.", "labels": [], "entities": []}, {"text": "(3) A simple \"concatenation\" setup where we train on the union of GERBERCHAI and the SEMEVAL training corpus.", "labels": [], "entities": [{"text": "GERBERCHAI", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.7018572688102722}, {"text": "SEMEVAL training corpus", "start_pos": 85, "end_pos": 108, "type": "DATASET", "confidence": 0.6857510805130005}]}, {"text": "according to the guidelines, the true positives include all predictions that match the gold span indirectly through a (manually annotated) coreference chain.", "labels": [], "entities": []}, {"text": "All previous studies on the SEMEVAL dataset used the FrameNet annotation, and without access to the actual predictions we cannot directly compare our predictions to theirs.", "labels": [], "entities": [{"text": "SEMEVAL dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.8456249535083771}, {"text": "FrameNet", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.8562950491905212}]}, {"text": "We are grateful to Laparra and Rigau who agreed to share the predictions of their 2013 model with us, which is, at the time of writing, the system with the second-best reported scores.", "labels": [], "entities": []}, {"text": "We converted the predictions into the PropBank format, using the FrameNet-to-PropBank mapping provided by the task organizers.", "labels": [], "entities": []}, {"text": "Implicit SRL systems typically trade off recall against precision by restricting the search space.", "labels": [], "entities": [{"text": "SRL", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.8881027102470398}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9987050294876099}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9985620379447937}]}, {"text": "Our system uses two heuristics: It restricts search to the current and two preceding sentences and to the predominant role set (cf. Section 3.2).", "labels": [], "entities": []}, {"text": "The upper bound in recall on SEMEVAL test that can still be achieved in this setting is 60.1%.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9996464252471924}, {"text": "SEMEVAL", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.6137446165084839}]}, {"text": "shows the results of the four experimental conditions defined above and the comparison system, the converted Laparra and Rigau (2013).", "labels": [], "entities": []}, {"text": "Our system, trained in-domain (1), achieves a performance comparable to Laparra and Rigau, albeit with a different precision-recall trade-off.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 115, "end_pos": 131, "type": "METRIC", "confidence": 0.9895166754722595}]}, {"text": "Not surprisingly, pure outof-domain training (2) does not perform well either.", "labels": [], "entities": []}, {"text": "Simple data concatenation (3) leads to a minimal numeric improvement, but indicates that the datasets are indeed rather different.", "labels": [], "entities": []}, {"text": "We see a substantial improvement in performance when feature augmentation (4) is used.", "labels": [], "entities": []}, {"text": "There is not only a major improvement in recall (+10 percentage points) but also a smaller improvement in precision (+3 points).", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.999833345413208}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9996916055679321}]}, {"text": "We tested the difference to the in-domain model (1) for significance with bootstrap resampling and found it to be higly significant (p<0.01).", "labels": [], "entities": []}, {"text": "In sum, we see an improvement of 5% F-Score, despite the differences between the corpora, when feature augmentation is used.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9988858103752136}]}, {"text": "Notably, we achieve a high recall, despite the upper bound imposed by the filtering heuristics.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9996023774147034}]}, {"text": "In Experiment 2, we use a combination of GERBER-CHAI and the complete SEMEVAL for training and evaluate on GERBERCHAI.", "labels": [], "entities": [{"text": "GERBER-CHAI", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.7635475397109985}, {"text": "SEMEVAL", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9188396334648132}, {"text": "GERBERCHAI", "start_pos": 107, "end_pos": 117, "type": "DATASET", "confidence": 0.8871344923973083}]}, {"text": "The main question is whether the addition of the (much smaller) SEMEVAL corpus to GERBERCHAI can improve performance.", "labels": [], "entities": [{"text": "SEMEVAL corpus", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.7349234819412231}, {"text": "GERBERCHAI", "start_pos": 82, "end_pos": 92, "type": "DATASET", "confidence": 0.67091965675354}]}, {"text": "We consider the same four conditions as in Experiment 1.", "labels": [], "entities": []}, {"text": "To obtain reliable results, we split GERBER-CHAI into three equal-sized parts and report averages over three cross-validation runs where we always use two thirds for training and one third for testing.", "labels": [], "entities": [{"text": "GERBER-CHAI", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.699283242225647}]}, {"text": "Evaluation also is performed as before, with the exception that in the absence of manually annotated coreference chains, we only count direct matches as true positives.", "labels": [], "entities": []}, {"text": "The upper bound for recall on this dataset (using the same 3-sentence window and predominant role set) is rather low, at 44%, which reflects the structural tendency of nominalizations to realize few roles locally.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9975863695144653}]}, {"text": "Unfortunately, we do not have a directly comparable competitor, since Laparra and Rigau did not run their system on GERBERCHAI data.", "labels": [], "entities": [{"text": "GERBERCHAI data", "start_pos": 116, "end_pos": 131, "type": "DATASET", "confidence": 0.8155984580516815}]}, {"text": "The results obtained by are not directly comparable, since their approach was hand-tailored towards nominal implicit SRL in the newswire domain.", "labels": [], "entities": []}, {"text": "It incorporates a large number of detailed linguistic resources (Penn Treebank, Penn Discourse Bank, NomBank, FrameNet) and assumes gold standard information on all levels.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.993785172700882}, {"text": "Penn Discourse Bank", "start_pos": 80, "end_pos": 99, "type": "DATASET", "confidence": 0.9362713297208151}]}, {"text": "We therefore see this system as an upper bound rather than as a competitor.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The overall patterns are very similar to Experiment 1: out-of-domain training (2) works worse than in-domain training (1), and simple concatenation (3) does not improve over in-domain training.", "labels": [], "entities": []}, {"text": "With feature augmentation, however, we see a significant improvement of 9% in precision, recall and F 1 . The difference is highly significant at p<0.01.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9997530579566956}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9997754693031311}, {"text": "F 1", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9911992251873016}]}, {"text": "This confirms the effectiveness of corpus combination, despite the small size of the added SEMEVAL dataset compared to GERBERCHAI.", "labels": [], "entities": [{"text": "SEMEVAL dataset", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.8435186147689819}, {"text": "GERBERCHAI", "start_pos": 119, "end_pos": 129, "type": "DATASET", "confidence": 0.7747465968132019}]}, {"text": "It is also clear, however, that the results are much worse than the upper bound set by Gerber and Chai.", "labels": [], "entities": []}, {"text": "subdivides the results by semantic roles for (1), as the in-domain baseline, and (4), as the best model.", "labels": [], "entities": []}, {"text": "Again, we see improvements for both A0 and A1, both regarding precision and recall.", "labels": [], "entities": [{"text": "A0", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.8646464347839355}, {"text": "A1", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9844695329666138}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9996552467346191}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9988903403282166}]}, {"text": "Interestingly, the improvements as well as the performance for A1 exceed those for A0 -a difference to the SEMEVAL results, where we found the best results for A0.", "labels": [], "entities": [{"text": "A1", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9901941418647766}, {"text": "A0", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.7730191946029663}, {"text": "SEMEVAL", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.7917752861976624}]}, {"text": "In Experiments 1 and 2, we have found an improvement for including out-of-domain data.", "labels": [], "entities": []}, {"text": "However, it is unclear so far whether the improvements are simply due to the increased amount of training data, or to the training data becoming more varied.", "labels": [], "entities": []}, {"text": "To distinguish between these two hypotheses, Experiment 3 keeps the total size of the training set constant and varies the proportions of the two source corpora, SE-MEVAL and GERBERCHAI, in 10% increments, from 100% SEMEVAL to 100% GERBERCHAI.", "labels": [], "entities": [{"text": "GERBERCHAI", "start_pos": 175, "end_pos": 185, "type": "METRIC", "confidence": 0.827403724193573}, {"text": "SEMEVAL", "start_pos": 216, "end_pos": 223, "type": "METRIC", "confidence": 0.879123330116272}, {"text": "GERBERCHAI", "start_pos": 232, "end_pos": 242, "type": "METRIC", "confidence": 0.6440050005912781}]}, {"text": "The size of the training set is limited by the smaller one of the training sets (SEMEVAL, cf..", "labels": [], "entities": [{"text": "SEMEVAL", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.8313440084457397}]}, {"text": "As before, we apply feature augmentation and train models, which we now evaluate on both the SEMEVAL and GERBERCHAI test sets.", "labels": [], "entities": [{"text": "SEMEVAL", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.5680050849914551}, {"text": "GERBERCHAI test sets", "start_pos": 105, "end_pos": 125, "type": "DATASET", "confidence": 0.8198865950107574}]}, {"text": "If the improvements we have seen before are solely due to the larger size of the training sets, we expect to seethe highest performance for the 100% in-domain training set, and decreasing performance with more out-ofdomain data.", "labels": [], "entities": []}, {"text": "If however variety is important, we expect to see a maximum somewhere between the two extremes, at the point where there is enough outof-domain training data to introduce variety but not enough to overwhelm the in-domain data.", "labels": [], "entities": []}, {"text": "On both test sets, we do not seethe best result for 100% in-domain data -there is a substantial improvement moving from 100% to 90% in-domain data (from 0.13 to 0.18 FScore on SEMEVAL and from 0.10 to 0.18 on GER-BERCHAI).", "labels": [], "entities": [{"text": "FScore", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.9868424534797668}, {"text": "SEMEVAL", "start_pos": 176, "end_pos": 183, "type": "DATASET", "confidence": 0.7854297161102295}, {"text": "GER-BERCHAI", "start_pos": 209, "end_pos": 220, "type": "DATASET", "confidence": 0.9359487295150757}]}, {"text": "On the SEMEVAL test set, the result for 90% is the (tied) best result.", "labels": [], "entities": [{"text": "SEMEVAL test set", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.8195955157279968}]}, {"text": "We see minor variation until roughly the 50-50 split and then a mild degradation to the cases where the GERBERCHAI training data dominates, consistent with Experiment 1.", "labels": [], "entities": [{"text": "GERBERCHAI training data", "start_pos": 104, "end_pos": 128, "type": "DATASET", "confidence": 0.7893993258476257}]}, {"text": "On the GERBERCHAI test set, we see a more symmetrical picture, with relatively constant performance for almost all mixtures.", "labels": [], "entities": [{"text": "GERBERCHAI test set", "start_pos": 7, "end_pos": 26, "type": "DATASET", "confidence": 0.8193092147509257}]}, {"text": "We see degradation for the both \"pure\" (100%) training sets, but still better performance for in-domain than for out-of-domain (100% GERBERCHAI: 0.10; 100% SEMEVAL: 0.08).", "labels": [], "entities": [{"text": "GERBERCHAI", "start_pos": 133, "end_pos": 143, "type": "METRIC", "confidence": 0.9665416479110718}, {"text": "SEMEVAL", "start_pos": 156, "end_pos": 163, "type": "METRIC", "confidence": 0.8142634630203247}]}, {"text": "Overall, the results are compatible with the second, but not the first hypothesis: the models do seem to profit from the combination of different corpora even when this does not involve larger training sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of available English corpora with implicit semantic role annotation", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of implicit SRL (PropBank roles) on  the SEMEVAL test set", "labels": [], "entities": [{"text": "SEMEVAL test set", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.8641415039698283}]}, {"text": " Table 4: Results on SEMEVAL test, training on SEMEVAL  train plus varying amounts of data from GERBERCHAI", "labels": [], "entities": [{"text": "SEMEVAL", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.7041160464286804}, {"text": "SEMEVAL  train", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.8338243067264557}, {"text": "GERBERCHAI", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.613982081413269}]}, {"text": " Table 5: Evaluation of implicit SRL (PropBank roles) on the SEMEVAL test set, by target part of speech", "labels": [], "entities": [{"text": "SEMEVAL test set", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.8439600467681885}]}, {"text": " Table 6: Evaluation of implicit SRL (PropBank roles) on the SEMEVAL test set, by role", "labels": [], "entities": [{"text": "SEMEVAL test set", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.8394080201784769}]}, {"text": " Table 7: Evaluation of implicit SRL (PropBank roles) on the SEMEVAL test set, by role", "labels": [], "entities": [{"text": "SEMEVAL test set", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.8373997211456299}]}, {"text": " Table 8: Evaluation of implicit SRL (PropBank roles) on  GERBERCHAI (3-fold CV)", "labels": [], "entities": [{"text": "GERBERCHAI", "start_pos": 58, "end_pos": 68, "type": "DATASET", "confidence": 0.6049627065658569}]}]}