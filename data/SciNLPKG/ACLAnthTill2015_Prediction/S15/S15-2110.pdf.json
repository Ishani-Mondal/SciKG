{"title": [{"text": "WarwickDCS: From Phrase-Based to Target-Specific Sentiment Recognition", "labels": [], "entities": [{"text": "WarwickDCS", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.7451180219650269}, {"text": "Target-Specific Sentiment Recognition", "start_pos": 33, "end_pos": 70, "type": "TASK", "confidence": 0.6296378374099731}]}], "abstractContent": [{"text": "We present and evaluate several hybrid systems for sentiment identification for Twit-ter, both at the phrase and document (tweet) level.", "labels": [], "entities": [{"text": "sentiment identification", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.9557166993618011}]}, {"text": "Our approach has been to use a novel combination of lexica, traditional NLP and deep learning features.", "labels": [], "entities": []}, {"text": "We also analyse techniques based on syntactic parsing and token-based association to handle topic specific sentiment in subtask C.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.746911495923996}]}, {"text": "Our strategy has been to identify subphrases relevant to the designated topic/target and assign sentiment according to our subtask A classifier.", "labels": [], "entities": []}, {"text": "Our submitted subtask A classifier ranked fourth in the Se-mEval official results while our BASELINE and \u00b5PARSE classifiers for subtask C would have ranked second.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9831669926643372}]}], "introductionContent": [{"text": "Twitter holds great potential for analyses in the social sciences both due to its explosive popularity, increasing accessibility to large amounts of data and its dynamic nature.", "labels": [], "entities": []}, {"text": "For sentiment analysis on twitter the best performing approaches () have used a set of rich lexical features.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9763538241386414}]}, {"text": "However, the development of lexica can be time consuming and is not always suitable when shifting between domains, which examine new topics and user populations.", "labels": [], "entities": []}, {"text": "Excitingly, the state of the art has recently shifted toward novel semi-supervised techniques such as the incorporation of word embeddings to represent the context of words and concepts ().", "labels": [], "entities": []}, {"text": "Moreover, it is important to be able to identify sentiment in relation to particular entities, topics or events (aspect-based sentiment).", "labels": [], "entities": []}, {"text": "We have followed a hybrid approach which incorporates traditional lexica, unigrams and bigrams as well as word embeddings using word2vec) to train classifiers for subtasks A and B.", "labels": [], "entities": []}, {"text": "For subtask C, sentiment targeted towards a particular topic, we have developed a set of different strategies which use either syntactic dependencies or token-level associations with the topic word in combination with our A classifier to produce sentiment annotations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with Random Forests and Lib-SVM with a linear kernel on the training set (4,769 positive, 2,493 negative and 381 neutral tweets) using 10-fold cross-validation and selected LibSVM as the algorithm which achieved the best average F1 score on the positive and negative classes.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.9825280904769897}]}, {"text": "We then used the development set (387 positive, 229 negative and 25 neutral tweets) to fine-tune the value of parameter C, achieving an F1 score of 86.40.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.984661191701889}]}, {"text": "The final model was applied on the two test sets provided to us; the \"Official 2015 Test\" (\"OT\") included 3,092 instances and the \"Progress Test\" (\"PT\"), including 10,681.", "labels": [], "entities": [{"text": "Official 2015 Test\" (\"OT\")", "start_pos": 70, "end_pos": 96, "type": "DATASET", "confidence": 0.8231839069298336}, {"text": "Progress Test\" (\"PT\")", "start_pos": 131, "end_pos": 152, "type": "METRIC", "confidence": 0.8794554273287455}]}, {"text": "Our results are summarised in.", "labels": [], "entities": []}, {"text": "Our algorithm was ranked fourth in OT and fifth in PT out of 11 competitors, achieving F1 scores of 82.46 and 83.89 respectively.", "labels": [], "entities": [{"text": "OT", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.7754181027412415}, {"text": "PT", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9995893836021423}, {"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9998061060905457}]}, {"text": "It is clear from the table that lexicon-based features have the most important impact on the results.", "labels": [], "entities": []}, {"text": "Interestingly, without ngram features, our results would have been better in both sets; however, there was a 0.8 gain in F1 score with the development set (F1 score 85.60) when these were incorporated in our model.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9869250357151031}, {"text": "F1 score 85.60)", "start_pos": 156, "end_pos": 171, "type": "METRIC", "confidence": 0.961809366941452}]}, {"text": "The comparison between all the different pairwise sets of features illustrates that lexica together with word embeddings contribute the most (the results are most affected when they are removed), whereas from the individual feature sets (not presented due to space limitations), lexicon-based features outperform the rest (79.96, 82.18), followed byword embeddings (77.75, 79.92 in OT and PT respectively).", "labels": [], "entities": [{"text": "PT", "start_pos": 389, "end_pos": 391, "type": "METRIC", "confidence": 0.9651050567626953}]}, {"text": "For our SemEval submission we trained an SVM classifier on 9684 tweets  In we list the average F1 scores of positive and negative tweets in the test data set when removing certain features.", "labels": [], "entities": [{"text": "F1", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9953833222389221}]}, {"text": "The results we submitted were generated by the second classifier.", "labels": [], "entities": []}, {"text": "demonstrates that representing the tweet with positive and negative word embeddings is the most effective feature (performance is affected the most when we remove these) followed by the manually generated lexicon-based features.", "labels": [], "entities": []}, {"text": "This combined with a 2% reduction in F1 score when the embeddings are removed, indicates that the embeddings improve sentiment analysis performance.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9894415140151978}, {"text": "sentiment analysis", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.9519301354885101}]}, {"text": "Contrary to the approach by, we didn't integrate the sentiment information in the word embeddings training process, but rather the sentiment-specific nature of the embeddings was reflected in the choice of different training datasets, yielding different word embedding features for positive and negative tweets.", "labels": [], "entities": []}, {"text": "To measure the contributions of our word embeddings and Tang's sentiment-specific word embeddings separately in the F1 score, we performed a further test.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.978271484375}]}, {"text": "When we only removed Tang's word embeddings features, the F1 score dropped by 0.15%; when we only removed our word embedding features, the F1 score dropped by 1.21%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9898830354213715}, {"text": "F1 score", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9894478619098663}]}, {"text": "This illustrates that for our approach, our word embedding features contribute more.", "labels": [], "entities": []}, {"text": "However, it is the combination of the two types of word embeddings that boosts our classifier's performance.", "labels": [], "entities": []}, {"text": "In subtask C the goal is to identify the sentiment targeted towards a particular topic or entity.", "labels": [], "entities": []}, {"text": "This is closely linked to aspect-based sentiment () and is very important for understanding the reasons behind the manifestation of different reactions.", "labels": [], "entities": []}, {"text": "We develop several strategies for selecting a topic-relevant portion of a tweet and use it to produce a sentiment annotation.", "labels": [], "entities": []}, {"text": "A driving force of our approach has been to use phrase-based sentiment identification from subtask A to annotate the topicrelevant selections.", "labels": [], "entities": [{"text": "phrase-based sentiment identification", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.7085284491380056}]}], "tableCaptions": [{"text": " Table 1. Our al- gorithm was ranked fourth in OT and fifth in PT  out of 11 competitors, achieving F1 scores of 82.46  and 83.89 respectively. It is clear from the table  that lexicon-based features have the most impor- tant impact on the results. Interestingly, without  ngram features, our results would have been bet- ter in both sets; however, there was a 0.8 gain in  F1 score with the development set (F1 score 85.60)  when these were incorporated in our model. The  comparison between all the different pairwise sets  of features illustrates that lexica together with word  embeddings contribute the most (the results are most  affected when they are removed), whereas from the  individual feature sets (not presented due to space  limitations), lexicon-based features outperform the  rest (79.96, 82.18), followed by word embeddings  (77.75, 79.92 in OT and PT respectively).", "labels": [], "entities": [{"text": "PT", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9967358708381653}, {"text": "F1", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.9996318817138672}, {"text": "F1", "start_pos": 374, "end_pos": 376, "type": "METRIC", "confidence": 0.9995306730270386}, {"text": "F1 score", "start_pos": 409, "end_pos": 417, "type": "METRIC", "confidence": 0.9564867317676544}]}, {"text": " Table 1: Average F1 scores of positive/negative classes  on the test set with different features.", "labels": [], "entities": [{"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9870232343673706}]}, {"text": " Table 2: The scores obtained on the test set with different  features.", "labels": [], "entities": []}, {"text": " Table 3: Summary of the performance of our subtask C  classifiers.", "labels": [], "entities": []}]}