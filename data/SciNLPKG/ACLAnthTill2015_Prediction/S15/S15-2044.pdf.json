{"title": [{"text": "CoMiC: Adapting a Short Answer Assessment System for Answer Selection", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.9838321805000305}]}], "abstractContent": [{"text": "Open forum threads exhibit a great variability in the quality and quantity of the answers they attract, making it difficult to manually moderate and separate relevant from irrelevant content.", "labels": [], "entities": []}, {"text": "The goal of SemEval 2015 Task 3 (Subtask A, English) is to build systems that automatically distinguish between relevant and irrelevant content in forum threads.", "labels": [], "entities": [{"text": "SemEval 2015 Task 3", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.8549766093492508}]}, {"text": "We extend a short answer assessment system to build relations between forum questions and answers with respect to similarity, question type, and answer content.", "labels": [], "entities": []}, {"text": "The features are used in a sequence classifier to account for the conversation character of threads.", "labels": [], "entities": []}, {"text": "The performance of this approach is modest in comparison to the other task participants and also to the performance the system usually reaches in short answer assessment.", "labels": [], "entities": []}, {"text": "However, the new features implemented for this task area first step in developing more fine-grained question-answer features and identifying relevant answers.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we discuss the adaptation of our Short Answer Assessment (SAA) system) to Task 3, Subtask A (English) of SemEval 2015, Answer Selection in Community Question Answering.", "labels": [], "entities": [{"text": "Short Answer Assessment (SAA)", "start_pos": 48, "end_pos": 77, "type": "METRIC", "confidence": 0.6943285117546717}, {"text": "Answer Selection in Community Question Answering", "start_pos": 134, "end_pos": 182, "type": "TASK", "confidence": 0.6786974221467972}]}, {"text": "The aim in the task was to distinguish helpful from unhelpful answers in a community forum given a question.", "labels": [], "entities": []}, {"text": "We enter the QA landscape from the perspective of evaluating student answers to reading comprehension questions with respect to whether they contain the targeted content.", "labels": [], "entities": []}, {"text": "In such settings, one generally has a reference answer to which a candidate answer can be compared, making alignment-based systems a natural solution.", "labels": [], "entities": []}, {"text": "This is not the case for QA, where a system has to selector rank candidate answers with regard to a question posed.", "labels": [], "entities": [{"text": "QA", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9016594886779785}]}, {"text": "However, the present task is still interesting to us because it shares a central characteristic with SAA: one needs to identify the relevant part of an answer, given a question.", "labels": [], "entities": [{"text": "SAA", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.7686430215835571}]}, {"text": "In theoretical linguistics, that relevant part is usually called focus (cf., e.g.,), and several research groups have made efforts to annotate it in corpus data).", "labels": [], "entities": []}, {"text": "Automatic approaches to identifying focus have however yet to be proposed, so for the current task, we adapted and used our SAA system to align candidate answers with the forum question, identifying whether and how question material was picked up, which in turn should indicate whether answers are on-topic.", "labels": [], "entities": []}, {"text": "We then used a number of features to characterize the unaligned answer material, from POS classes to temporal expressions.", "labels": [], "entities": []}, {"text": "We also encoded which question words were present in the question in the hope that the resulting classifier would pickup connections between individual question words and the different answer features in an approximation to identifying the focus of the answer.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 briefly discusses the data of the task before section 3 presents the details of our system architecture and the features we used.", "labels": [], "entities": []}, {"text": "Section 4 then shows the results of our efforts and a short error analysis, and finally section 5 concludes and discusses directions for further efforts.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Coarse-grained accuracy and Macro F1 of sys- tems on development and test set for Subtask A, English", "labels": [], "entities": [{"text": "Coarse-grained", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8988873958587646}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9439346194267273}, {"text": "Macro", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9775517582893372}, {"text": "F1", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.6393694877624512}, {"text": "English", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.853142499923706}]}]}