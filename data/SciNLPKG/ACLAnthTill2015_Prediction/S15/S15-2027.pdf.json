{"title": [{"text": "DLS@CU: Sentence Similarity from Word Alignment and Semantic Vector Composition", "labels": [], "entities": [{"text": "Sentence Similarity from Word Alignment", "start_pos": 8, "end_pos": 47, "type": "TASK", "confidence": 0.7209731876850128}]}], "abstractContent": [{"text": "We describe a set of top-performing systems at the SemEval 2015 English Semantic Textual Similarity (STS) task.", "labels": [], "entities": [{"text": "SemEval 2015 English Semantic Textual Similarity (STS) task", "start_pos": 51, "end_pos": 110, "type": "TASK", "confidence": 0.8972660183906556}]}, {"text": "Given two English sentences , each system outputs the degree of their semantic similarity.", "labels": [], "entities": []}, {"text": "Our unsupervised system, which is based on word alignments across the two input sentences, ranked 5th among 73 submitted system runs with a mean correlation of 79.19% with human annotations.", "labels": [], "entities": []}, {"text": "We also submitted two runs of a supervised system which uses word alignments and similarities between compositional sentence vectors as its features.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.7263987362384796}]}, {"text": "Our best supervised run ranked 1st with a mean correlation of 80.15%.", "labels": [], "entities": [{"text": "mean correlation", "start_pos": 42, "end_pos": 58, "type": "METRIC", "confidence": 0.7389100193977356}]}], "introductionContent": [{"text": "Identification of short text similarity is an important research problem with application in a multitude of areas: natural language processing (machine translation, text summarization), information retrieval (question answering), education (short answer scoring), and soon.", "labels": [], "entities": [{"text": "Identification of short text similarity", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8976271986961365}, {"text": "natural language processing (machine translation", "start_pos": 115, "end_pos": 163, "type": "TASK", "confidence": 0.6597601274649302}, {"text": "text summarization)", "start_pos": 165, "end_pos": 184, "type": "TASK", "confidence": 0.7391131619612376}, {"text": "information retrieval (question answering)", "start_pos": 186, "end_pos": 228, "type": "TASK", "confidence": 0.8499358495076498}]}, {"text": "The SemEval Semantic Textual Similarity (STS) task series () has become a central platform for the task: a publicly available corpus of more than 14,000 sentence pairs have been developed over the past four years with human annotations of similarity for each pair; and a total of 290 system runs have been evaluated.", "labels": [], "entities": [{"text": "SemEval Semantic Textual Similarity (STS) task", "start_pos": 4, "end_pos": 50, "type": "TASK", "confidence": 0.8358451128005981}]}, {"text": "In this article, we describe a set of systems that were submitted at the SemEval 2015 English STS task (.", "labels": [], "entities": [{"text": "SemEval 2015 English STS task", "start_pos": 73, "end_pos": 102, "type": "TASK", "confidence": 0.6336453795433045}]}, {"text": "Given two English sentences, the objective is to compute their semantic similarity in the range, where the score increases with similarity (i.e., 0 indicates no similarity and 5 indicates identicality).", "labels": [], "entities": [{"text": "similarity", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9845359921455383}]}, {"text": "The official evaluation metric was the Pearson correlation coefficient with human annotations.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 39, "end_pos": 70, "type": "METRIC", "confidence": 0.9083502093950907}]}, {"text": "The best of our three system runs achieved the highest mean correlation (80.15%) with human annotations among all submitted systems on five test sets (containing a total of 3000 test pairs).", "labels": [], "entities": [{"text": "mean correlation", "start_pos": 55, "end_pos": 71, "type": "METRIC", "confidence": 0.7992504239082336}]}, {"text": "Early work on sentence similarity) established the basic procedural framework under which most modern algorithms operate: computing sentence similarity as a mean of word similarities across the two input sentences.", "labels": [], "entities": [{"text": "sentence similarity)", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.790974497795105}]}, {"text": "With no human annotated STS data set available, these algorithms were unsupervised and were evaluated extrinsically on tasks like paraphrase detection and textual entailment recognition.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.8183574974536896}, {"text": "textual entailment recognition", "start_pos": 155, "end_pos": 185, "type": "TASK", "confidence": 0.7723837494850159}]}, {"text": "The SemEval STS task series has made an important contribution through the large annotated data set, enabling intrinsic evaluation of STS systems and making supervised STS systems a reality.", "labels": [], "entities": [{"text": "SemEval STS task series", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.6004658192396164}]}, {"text": "At SemEval 2012, domain-specific training data was provided for most of the test pairs () and consequently, supervised systems were the most successful.", "labels": [], "entities": [{"text": "SemEval 2012", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.6210116744041443}]}, {"text": "These systems combined different similarity measures, e.g., lexico-semantic, syntactic and string similarity, using regression models.", "labels": [], "entities": []}, {"text": "However, at the 2013 and 2014 STS events, no such training data was provided; instead, the systems were allowed to use all past data to train their systems.", "labels": [], "entities": []}, {"text": "Interestingly, the best systems at these two events were unsupervised (; some super-Robin Warren was awarded a Nobel Prize .: Words aligned by our aligner across two sentences taken from the MSR alignment corpus.", "labels": [], "entities": [{"text": "MSR alignment corpus", "start_pos": 191, "end_pos": 211, "type": "DATASET", "confidence": 0.6658552090326945}]}, {"text": "(We show only part of the second sentence.)", "labels": [], "entities": []}, {"text": "Besides exact word/lemma matches, it identifies and aligns semantically similar word pairs using PPDB (awarded -received in this example).", "labels": [], "entities": []}], "datasetContent": [{"text": "In addition to the official evaluation at SemEval 2015, we report evaluation results on past STS test data.", "labels": [], "entities": [{"text": "SemEval 2015", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.7439640760421753}, {"text": "STS test data", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.753154456615448}]}, {"text": "For all these evaluations, the performance metric is the Pearson correlation coefficient between system output and average human annotations.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 57, "end_pos": 88, "type": "METRIC", "confidence": 0.9618759950002035}]}, {"text": "Correlation is computed for each individual test set, and a weighted sum of all correlations (i.e. overall test sets) is used as the final evaluation metric.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9714420437812805}]}, {"text": "The weight of a test set is proportional to the number of sentence pairs it contains.", "labels": [], "entities": []}, {"text": "Before presenting the results, we describe a preprocessing step for one of the 2015 test sets.", "labels": [], "entities": []}, {"text": "Identifying the right stop words (some of which can be domain-specific) proved key in our past investigation of STS (); therefore we consider it very important to manually examine individual domains to ensure proper categorization of words.", "labels": [], "entities": [{"text": "STS", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.8973531723022461}]}, {"text": "An inspection of the trial data for the answersstudents set indicated that the expressions in the: Performance on STS 2015 data.", "labels": [], "entities": [{"text": "STS 2015 data", "start_pos": 114, "end_pos": 127, "type": "DATASET", "confidence": 0.8249849875768026}]}, {"text": "Each number in rows 1-5 is the correlation between system output and human annotations for the corresponding data set.", "labels": [], "entities": []}, {"text": "The rightmost column shows the best score by any system.", "labels": [], "entities": []}, {"text": "The last two rows show the value of the final evaluation metric and the system rank, respectively, for each run.", "labels": [], "entities": []}, {"text": "following pairs are semantically equivalent for the given domain: {'battery terminal', 'terminal'} and {'electrical state', 'state'}.", "labels": [], "entities": []}, {"text": "Therefore, we treated the two words 'battery' and 'electrical' as special stop words during occurrences of these pairs across the input sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test sets at SemEval STS 2015.", "labels": [], "entities": [{"text": "SemEval STS 2015", "start_pos": 23, "end_pos": 39, "type": "DATASET", "confidence": 0.646419902642568}]}, {"text": " Table 2: Performance on STS 2015 data. Each number  in rows 1-5 is the correlation between system output and  human annotations for the corresponding data set. The  rightmost column shows the best score by any system.  The last two rows show the value of the final evaluation  metric and the system rank, respectively, for each run.", "labels": [], "entities": [{"text": "STS 2015 data", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.6726907193660736}]}, {"text": " Table 3: Performance of our top system (S 1 ) on past STS  test sets (mean correlation with human annotations). The  score of the winning system at each event is shown on  column 3. S 1 outperforms all past winning systems.", "labels": [], "entities": []}, {"text": " Table 4: Performance of each individual feature of our  best run (S 1 ) on STS 2015 test sets. Combining the two  features improves performance on all but one test set.", "labels": [], "entities": [{"text": "STS 2015 test sets", "start_pos": 76, "end_pos": 94, "type": "DATASET", "confidence": 0.8404274135828018}]}]}