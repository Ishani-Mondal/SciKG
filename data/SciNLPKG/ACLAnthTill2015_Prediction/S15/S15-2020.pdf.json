{"title": [{"text": "SemantiKLUE: Semantic Textual Similarity with Maximum Weight Matching", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the SemantiKLUE system (Proisl et al., 2014) used for the SemEval-2015 shared task on Semantic Textual Similarity (STS) for English.", "labels": [], "entities": [{"text": "SemEval-2015 shared task on Semantic Textual Similarity (STS) for English", "start_pos": 79, "end_pos": 152, "type": "TASK", "confidence": 0.8522476007541021}]}, {"text": "The system was developed for SemEval-2013 and extended for SemEval-2014, where it participated in three tasks and ranked 13th out of 38 submissions for the English STS task.", "labels": [], "entities": [{"text": "English STS task", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.6655694444974264}]}, {"text": "While this year's submission ranks 46th out of 73, further experiments on the selection of training data led to notable improvements showing that the system could have achieved rank 22 out of 73.", "labels": [], "entities": []}, {"text": "We report a detailed analysis of those training selection experiments in which we tested different combinations of all the available STS datasets, as well as results of a qualitative analysis conducted on a sample of the sentence pairs for which SemantiKLUE gave wrong STS predictions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The SemEval-2015 task on \"Semantic Textual Similarity for English\") is a rerun of the corresponding task from SemEval-2014 with new test data and updated categories.", "labels": [], "entities": [{"text": "Semantic Textual Similarity for English\")", "start_pos": 26, "end_pos": 67, "type": "TASK", "confidence": 0.679036408662796}]}, {"text": "The predictions of participating systems were evaluated against manually annotated and subsequently filtered data.", "labels": [], "entities": []}, {"text": "STS was measured on a scale ranging from 0 (no similarity at all) to 5 (total equivalence).", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9783899784088135}]}, {"text": "SemantiK-LUE, developed in 2014, uses a distributional bagof-words model as well as a word-to-word alignment for each pair of sentences based on a maximum weight matching algorithm.", "labels": [], "entities": []}, {"text": "Our SemEval-2015 submission for all 5 test categories (headlines, images, belief, answers-forums, answers-students) was based on the training data set from 2014 with 2234 sentence pairs from 3 categories, namely paraphrase sentence pairs (MSRpar), sentence pairs from video descriptions (MSRvid) and MT evaluation sentence pairs (SMTeuroparl).", "labels": [], "entities": [{"text": "MT evaluation sentence", "start_pos": 300, "end_pos": 322, "type": "TASK", "confidence": 0.8582717378934225}]}, {"text": "Follow up experiments conducted after the submission deadline showed us that this training configuration was far from optimal, and that our system would have benefited a lot from a better training, as we managed to significantly improve the overall scores.", "labels": [], "entities": []}, {"text": "With the best training configuration, SemantiKLUE would have ranked 22nd out of 73 submissions (11th out of 28 teams), with a weighted mean of Pearson correlation coefficients overall test categories of 0.7508 (best system: 0.8015) In the following sections, we first give a short overview of the system (Section 2), and then we describe the follow-up experiments that allowed us to define the best training data set in terms of its subsets (Section 3); finally, we present the results of a qualitative analysis of the performance of our system (Section 4).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 143, "end_pos": 162, "type": "METRIC", "confidence": 0.9379733502864838}]}], "datasetContent": [{"text": "This section describes all post-hoc experiments on the STS 2015 test data performed to improve the 4 http://scikit-learn.org/ 112 predictions of the system.", "labels": [], "entities": [{"text": "STS 2015 test data", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.7431105226278305}]}, {"text": "The abbreviations used in the following tables reporting experiment results are listed in  All 39 similarity measures were used by the regression learner to train the system.", "labels": [], "entities": [{"text": "similarity", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9499303698539734}]}, {"text": "SemantiKLUE was tested on different training data with various combinations of training and test sets from 2013 and 2014.", "labels": [], "entities": []}, {"text": "Results for the submitted system are typeset in italics in, the best results in each column are typeset in bold font.", "labels": [], "entities": []}, {"text": "The best results would have been obtained by training on the MSR data from SemEval 2014 for all test sets.", "labels": [], "entities": [{"text": "MSR data from SemEval 2014", "start_pos": 61, "end_pos": 87, "type": "DATASET", "confidence": 0.8710889220237732}]}, {"text": "Considerable improvements can be achieved removing the SMTeuroparl category from the training set.", "labels": [], "entities": [{"text": "SMTeuroparl", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9578219056129456}]}, {"text": "This category consists of MT pairs of sentences whose exclusion would have given the system rank 37 (weighted mean of .7148) instead of 46 (.6717) out of 73 submissions.", "labels": [], "entities": [{"text": "MT pairs of sentences", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.8796776086091995}]}, {"text": "We turned the test data from SemEval 2014 into a training set for the 2015 test data (see).", "labels": [], "entities": []}, {"text": "The figures in show that training sets for images and headlines perform best with the corresponding categories of the test set (images and headlines) from SemEval 2014.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 155, "end_pos": 167, "type": "DATASET", "confidence": 0.7563454806804657}]}, {"text": "STS results appear to be extremely sensitive to the choice of the training dataset.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.45586663484573364}]}, {"text": "For this reason, we    conducted more fine-grained experiments to look for the best combination of training data for the 2015 test sets.", "labels": [], "entities": []}, {"text": "We combined training and test data of SemEval 2014 with the best training categories of SemEval 2013 (see) to test the performance of the system on the optimal training subset defined for SemEval 2014 . That optimal training configuration consists of the FNWN, headlines, MSR and OnWN data sets: the corresponding performance is typeset in italics.", "labels": [], "entities": [{"text": "FNWN", "start_pos": 255, "end_pos": 259, "type": "DATASET", "confidence": 0.8365541100502014}, {"text": "OnWN data sets", "start_pos": 280, "end_pos": 294, "type": "DATASET", "confidence": 0.9330733021100363}]}, {"text": "Comparable or even better results can be achieved with a combination of test and train categories of SemEval 2014 only.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 101, "end_pos": 113, "type": "TASK", "confidence": 0.680738776922226}]}, {"text": "Thus, combining the training category MSR (mp + mv) with another test category of 2014 (such as tweets or headlines) results in about 1.5%-2% improvement.", "labels": [], "entities": []}, {"text": "A more precise investigation helped us to find the best test combination with MSR, headlines, images, and tweet-news categories.", "labels": [], "entities": []}, {"text": "This brought our system to the weighted mean of .7508, corresponding to the 11th place out of 28 teams.", "labels": [], "entities": [{"text": "weighted mean", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.8925125598907471}]}, {"text": "We tried to further improve these results, by adding the optimal categories for training found in 2014 and extended the best training set defined for 2015 with FNWN (mp+mv+hl+img+tn+fn), but this led to slightly worse results in all test categories.", "labels": [], "entities": [{"text": "FNWN", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.8722388744354248}]}, {"text": "A further set of experiments was aimed at testing different subsets of similarity measures used at the answers-forums answers-students headlines belief images mean img+hl .5119    machine learning stage.", "labels": [], "entities": []}, {"text": "Results showed that the use of fewer similarity features (exclusion of all identical words in each pair of sentences from the calculation of similarity scores) resulted in worse performance of the whole system.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 141, "end_pos": 158, "type": "METRIC", "confidence": 0.9452742636203766}]}, {"text": "Our system is based on a relatively large feature set, but we were also interested in discovering how well SemantiKLUE would have performed if trained on a single feature.", "labels": [], "entities": []}, {"text": "We tested a feature based on cosine similarity between the two centroid vectors as a measure of semantic similarity for each sentence pair as suggested by Sch\u00fctze (1998) using either tokens or lemmas (see).", "labels": [], "entities": []}, {"text": "We selected cosine between centroid vectors as a candidate feature, because it is most intuitive and naturally connects to the representation of topical information, crucial in capturing textual similarity.", "labels": [], "entities": []}, {"text": "We found that regardless of the alignment (one to one or one to many both for lemma and tokens), the weighted mean of Pearson correlation coefficients is low (.6904 for the one-to-one alignment) for the cosine similarity value calculated with lemma based centroid vectors, but still higher than what is achieved by the more complex system with a large set of features with a poor training set (.6717) in the submission with mp+mv+smt used for the training set (see for comparison).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 118, "end_pos": 137, "type": "METRIC", "confidence": 0.9401796460151672}]}, {"text": "As we were interested in identifying the most balanced training sets in the test categories of 2014, we tested all categories against each other.", "labels": [], "entities": []}, {"text": "Results are shown in to test subsets, while columns represent training sets.", "labels": [], "entities": []}, {"text": "The results typeset in italics show that there is a high level of overtraining for the cases in which training and test data are identical.", "labels": [], "entities": []}, {"text": "The most balanced and robust test data are those of the image and OnWN categories: they can be used as training data for future experiments.", "labels": [], "entities": [{"text": "OnWN", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.856572687625885}]}, {"text": "To sum up, our results show that the best training configuration for SemEval 2015 involves MSR, headlines, images, and tweet-news categories (see).", "labels": [], "entities": [{"text": "SemEval 2015", "start_pos": 69, "end_pos": 81, "type": "TASK", "confidence": 0.8937012851238251}]}, {"text": "The scatter plots in relate the similarity score in the gold standard (x-axis) to the relatedness score produced by SemantiKLUE (yaxis) in its best training configuration, for three of", "labels": [], "entities": [{"text": "similarity score", "start_pos": 32, "end_pos": 48, "type": "METRIC", "confidence": 0.9706661701202393}]}], "tableCaptions": [{"text": " Table 1: Training set categories: abbreviations.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results for different training sets from  2014.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation results for different training sets  based on the 2014 test categories.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation results for different training sets based on train and test categories of 2014 and 2013.", "labels": [], "entities": []}, {"text": " Table 5: Single-feature experiments with different alignments: correlation based on cosine similarity.", "labels": [], "entities": [{"text": "correlation", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.9538922905921936}]}, {"text": " Table 6: Test data categories of 2014 against each other  (columns = training sets, lines = test sets).", "labels": [], "entities": []}]}