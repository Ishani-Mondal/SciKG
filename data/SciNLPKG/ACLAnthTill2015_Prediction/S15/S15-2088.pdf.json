{"title": [{"text": "CIS-positive: Combining Convolutional Neural Networks and SVMs for Sentiment Analysis in Twitter", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8993677198886871}]}], "abstractContent": [{"text": "This paper describes our automatic sentiment analysis system-CIS-positive-for SemEval", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8154450058937073}, {"text": "SemEval", "start_pos": 78, "end_pos": 85, "type": "TASK", "confidence": 0.9185528755187988}]}], "introductionContent": [{"text": "On the Internet, text containing different forms of sentiment appears everywhere.", "labels": [], "entities": []}, {"text": "Mining this information supports many types of interest groups.", "labels": [], "entities": []}, {"text": "Companies, for instance, are interested in user feedback about the advantages and drawbacks of their products.", "labels": [], "entities": []}, {"text": "Users want to read short reviews or ratings of hotels they want to book for their next vacation.", "labels": [], "entities": []}, {"text": "Politicians try to predict the outcome of the next presidential election.", "labels": [], "entities": []}, {"text": "An automatic sentiment analysis system can support all these different requirements.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8585284650325775}]}, {"text": "One source of these types of information covering many domains and topics is the social networking service Twitter.", "labels": [], "entities": []}, {"text": "Its popularity and the users' productivity in creating new text makes it an interesting research topic.", "labels": [], "entities": []}, {"text": "However, Twitter introduces specific challenges as we will see next.", "labels": [], "entities": []}, {"text": "In general, automatic sentiment analysis is challenging due to many different factors, such as ambiguous word senses, context dependency, sarcasm, etc.", "labels": [], "entities": [{"text": "automatic sentiment analysis", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6901724338531494}]}, {"text": "Specific properties of Twitter text make this task even more challenging.", "labels": [], "entities": []}, {"text": "The limit of 140 character per message leads to countless acronyms and abbreviations.", "labels": [], "entities": []}, {"text": "Moreover, the vast majority of tweets is of informal character and contains intentional miss-spellings and wrong use of grammar.", "labels": [], "entities": []}, {"text": "Hence, the out-of-vocabulary (OOV) rate of Twitter text is rather high, which leads to information loss.", "labels": [], "entities": [{"text": "out-of-vocabulary (OOV) rate", "start_pos": 11, "end_pos": 39, "type": "METRIC", "confidence": 0.777514660358429}]}, {"text": "One of the SemEval 2015 shared tasks -Task 10: Sentiment Analysis in Twitter -addresses these challenges (.", "labels": [], "entities": [{"text": "SemEval 2015 shared tasks -Task 10", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.8588592069489616}, {"text": "Sentiment Analysis in Twitter", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.8397105187177658}]}, {"text": "We participated in Subtask B the \"Message Polarity Classification\" task.", "labels": [], "entities": [{"text": "Message Polarity Classification\" task", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.8421447277069092}]}, {"text": "The goal is to predict the polarity of a given tweet into positive, negative, or neutral.", "labels": [], "entities": []}, {"text": "The task organizers provided tweet IDs and corresponding labels to have a common ground for training polarity classification systems.", "labels": [], "entities": [{"text": "training polarity classification", "start_pos": 92, "end_pos": 124, "type": "TASK", "confidence": 0.6565888524055481}]}, {"text": "More information about the task, its other subtasks as well as information about how the data was selected can be found in.", "labels": [], "entities": []}, {"text": "In this paper, we present our sentiment analysis system for SemEval 2015 -Task 10.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9497855007648468}, {"text": "SemEval 2015 -Task 10", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.8126307010650635}]}, {"text": "Our system addresses the above mentioned challenges in two ways.", "labels": [], "entities": []}, {"text": "First, we normalize the text to maximize the coverage of sentiment lexicons and minimize distracting elements such as user names or URLs.", "labels": [], "entities": []}, {"text": "Second, we combine deep Convolutional Neural Networks (CNN) and support vector machines (SVM) fora better overall classification.", "labels": [], "entities": []}, {"text": "The motivation of using CNNs is to extract not only local features but also context to predict sentiment.", "labels": [], "entities": []}, {"text": "Integrating CNN output into an SVM improves classification.", "labels": [], "entities": []}], "datasetContent": [{"text": "Twitter's terms of service do not allow to provide tweets as text.", "labels": [], "entities": []}, {"text": "Instead, the participants of the SemEval 2015 task had to download the tweets using a list of user and tweet IDs.", "labels": [], "entities": [{"text": "SemEval 2015 task", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8844776352246603}]}, {"text": "However, not all tweets are still available.", "labels": [], "entities": []}, {"text": "After downloading, our training data comprises a total of 8394 tweets, 3133 of which are positive, 1237 negative, and 4023 neutral.", "labels": [], "entities": []}, {"text": "The evaluation is done on two separate test sets.", "labels": [], "entities": []}, {"text": "The first test set, the progress test set, was used as lists all test set sizes in detail.", "labels": [], "entities": [{"text": "progress test set", "start_pos": 24, "end_pos": 41, "type": "DATASET", "confidence": 0.7514606515566508}]}, {"text": "As evaluation measure the organizers chose to report the macro F 1 score of positive and negative examples, i.e., The CNN is trained using minibatch stochastic gradient descent with a batch size of 200 examples.", "labels": [], "entities": [{"text": "macro F 1 score", "start_pos": 57, "end_pos": 72, "type": "METRIC", "confidence": 0.7861777544021606}]}, {"text": "For learning rate adaptation we use) with an initial learning rate of 0.001.", "labels": [], "entities": []}, {"text": "2 with \u03bb = 0.001 is utilized to avoid overfitting as much as possible.", "labels": [], "entities": []}, {"text": "The embeddings size is set to 50.", "labels": [], "entities": []}, {"text": "In the first convolution layer, we use 30 filters with am = 5, which means it spans 5 words.", "labels": [], "entities": [{"text": "am", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9524993896484375}]}, {"text": "The second convolution layer uses 10 filters with m = 3.", "labels": [], "entities": []}, {"text": "The three hidden layers have sizes 200, 40, and 200.", "labels": [], "entities": []}, {"text": "This choice of layer sizes with a bottleneck layer between two larger layers is frequently used in automated speech recognition systems.", "labels": [], "entities": [{"text": "automated speech recognition", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.6318036317825317}]}, {"text": "For example showed that using the bottleneck layer's output leads to lower word error rates than using hidden layer outputs.", "labels": [], "entities": []}, {"text": "However, our experimental results show that using the output of the CNN softmax layer as input for the first SVM achieves slightly better performance than using the output of the bottle-neck layer.", "labels": [], "entities": []}, {"text": "For both linear SVMs we tune the C parameter on the validation data.", "labels": [], "entities": []}, {"text": "Results The last line in lists the F 1 performances of our system on the SemEval 2015 test set.", "labels": [], "entities": [{"text": "F 1", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9092103242874146}, {"text": "SemEval 2015 test set", "start_pos": 73, "end_pos": 94, "type": "DATASET", "confidence": 0.803075760602951}]}, {"text": "The performance on negative examples is much worse than on positive or neutral examples.", "labels": [], "entities": []}, {"text": "This is due to the small number of negative training examples.", "labels": [], "entities": []}, {"text": "The macro F 1 score of 59.57 leads to rank 20 out of 40 participants in this year's SemEval.", "labels": [], "entities": [{"text": "macro F 1 score", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.7294525280594826}, {"text": "SemEval", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9224222898483276}]}, {"text": "The fact that our system scores much better on LifeJournal and the SMS data in terms of F 1,negative suggests that Twitter is an especially difficult medium for automated analysis.", "labels": [], "entities": [{"text": "LifeJournal", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.969707727432251}, {"text": "F 1,negative", "start_pos": 88, "end_pos": 100, "type": "METRIC", "confidence": 0.9882829785346985}]}, {"text": "The performance difference on Twitter from 2013 and 2014 compared to Twitter 2015 suggests that this year's Twitter data was different than in the years before.", "labels": [], "entities": []}, {"text": "Our system scored similarly on Twitter from 2013 and 2014, but worse on 2015.", "labels": [], "entities": []}, {"text": "Even worse results are achieved on the sarcasm data.", "labels": [], "entities": [{"text": "sarcasm data", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.7765895426273346}]}, {"text": "However, the results should betaken with care, because this subset is very small.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set sizes and results  #pos #neg #neu F 1,positive F 1,negative F 1,neutral F 1,macro", "labels": [], "entities": [{"text": "neu F 1,positive F 1,negative F 1,neutral F 1,macro", "start_pos": 49, "end_pos": 100, "type": "METRIC", "confidence": 0.8478231231371561}]}]}