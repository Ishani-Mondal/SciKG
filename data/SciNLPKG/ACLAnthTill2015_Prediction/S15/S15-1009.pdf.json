{"title": [{"text": "A New Dataset and Evaluation for Belief/Factuality", "labels": [], "entities": []}], "abstractContent": [{"text": "The terms \"belief\" and \"factuality\" both refer to the intention of the writer to present the propositional content of an utterance as firmly believed by the writer, not firmly believed, or having some other status.", "labels": [], "entities": []}, {"text": "This paper presents an ongoing annotation effort and an associated evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents an ongoing project aimed at developing a community-wide evaluation of expressed belief, also known as \"factuality\".", "labels": [], "entities": []}, {"text": "Belief and factuality are closely related to hedging, veridicality, and modality.", "labels": [], "entities": []}, {"text": "The project has grown out of the DARPA DEFT project; participants include the Linguistic Data Consortium (LDC) and three performer sites: Columbia University/George Washington University, the Florida Institute for Human and Machine Cognition, and the University of Albany.", "labels": [], "entities": []}, {"text": "The goal of our research project is not linguistic annotation, but the identification of meaning which is expressed in a non-linguistic manner.", "labels": [], "entities": []}, {"text": "Such a meaning representation is useful for many applications; in our project we are specifically interested in knowledge base population.", "labels": [], "entities": []}, {"text": "A different part of the DEFT program is concerned with the representation of propositional meaning, following the tradition of the ACE program in representing entities, relations and events (ERE) ().", "labels": [], "entities": [{"text": "representation of propositional meaning", "start_pos": 59, "end_pos": 98, "type": "TASK", "confidence": 0.8172140121459961}]}, {"text": "The work presented here is concerned with the attitude of agents towards propositional content: do the agents express a committed belief or a non-committed belief in the propositional content?", "labels": [], "entities": []}, {"text": "Our work has several characteristics that set it apart from other work: we are interested in annotation which can be done fairly quickly; we are not interested in annotating linguistic elements (such as trigger words); and we are planning an integration with sentiment annotation.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows: we start out by situating our notion of \"belief\" with respect to other notions of extra-propositional meaning (Section 2); we then present our annotation in some detail, with a special comparison to.", "labels": [], "entities": []}, {"text": "While the goal of this paper is not to talk about computational systems that were run as part of the evaluation (different publications will be available for that purpose), we quickly summarize their main characteristics so that the evaluation results can be interpreted.", "labels": [], "entities": []}, {"text": "We then turn to the pilot evaluation we have performed, presenting first the evaluation with respect to propositions (Section 5) and then a qualitative evaluation.", "labels": [], "entities": []}, {"text": "We conclude with a discussion of plans for the upcoming open evaluation, scheduled for December 2015.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted a multi-site pilot evaluation for the task of identifying beliefs expressed in text.", "labels": [], "entities": []}, {"text": "Three performer sites took part in this evaluation.", "labels": [], "entities": []}, {"text": "In this section, we briefly describe the systems built at these performer sites.", "labels": [], "entities": []}, {"text": "The first two systems are rule-based systems, whereas the third system is a supervised learning system.", "labels": [], "entities": []}, {"text": "We limit the discussion of these systems to a high level, postponing the detailed system descriptions to separate future publications.", "labels": [], "entities": []}, {"text": "We now describe the results obtained on a proposition-oriented quantitative evaluation of these systems.", "labels": [], "entities": []}, {"text": "We focus on a system's ability to correctly identify the propositional heads of each type of belief (CB, NCB, ROB, NA).", "labels": [], "entities": []}, {"text": "Only the words denoting heads of propositions will get one of these tags, and hence the majority of words in our data will not have any tags.", "labels": [], "entities": []}, {"text": "We expect the system to find the propositional heads and to correctly assign their belief tags.", "labels": [], "entities": []}, {"text": "We use the entire Evaluation dataset described in Section 3 for this evaluation (entirely unseen during the development of the systems).", "labels": [], "entities": [{"text": "Evaluation dataset", "start_pos": 18, "end_pos": 36, "type": "DATASET", "confidence": 0.6930432021617889}]}, {"text": "We report precision, recall and F-measure for each belief type.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9996258020401001}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9997190833091736}, {"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9990721940994263}]}, {"text": "We also report their micro-averages as the overall result.", "labels": [], "entities": []}, {"text": "We compute F-measure as the harmonic mean between precision and recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9917036294937134}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9992483258247375}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9898672103881836}]}, {"text": "The best results obtained by each system described in Section 4 are presented in.", "labels": [], "entities": []}, {"text": "For System A, four different configurations were run for the evaluation, in which the NCB and NA tagging was either enabled or disabled.", "labels": [], "entities": []}, {"text": "(The current version does not account for ROB tags.)", "labels": [], "entities": []}, {"text": "In Table 2, Columns 2-4, we show the performance of System A while all 3 tags (CB, NCB and NA) are enabled.", "labels": [], "entities": []}, {"text": "The results of other three configurations are comparable.", "labels": [], "entities": []}, {"text": "Any sentence where the belief target could not be located, either due to parsing error or due to missing coreference (as supplied by ERE), was discarded.", "labels": [], "entities": []}, {"text": "This resulted in a relatively lower recall in the evaluation, but produced high precision in a target-driven pilot evaluation (Section 6).", "labels": [], "entities": [{"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9995554089546204}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9994276165962219}]}, {"text": "The results obtained by System B in the evaluation are shown in: Columns 5-7.", "labels": [], "entities": []}, {"text": "The results of System B, when ignoring the belief categories (i.e., on identifying heads of propositions), were 83.6% precision and 50% recall.: Columns 8-10 shows results obtained by System C trained on 80% of the training dataset (the rest of the corpus was used as a development set).", "labels": [], "entities": [{"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9996157884597778}, {"text": "recall.", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.9994137287139893}]}, {"text": "The supervised learning approach obtained over-all better performance than rule based approach in our evaluation.", "labels": [], "entities": []}, {"text": "ROB and NCB were the most difficult classes to predict for all three systems (e.g., highest recall posted for ROB is only 15.6%).", "labels": [], "entities": [{"text": "ROB", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9498897194862366}, {"text": "NCB", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.766499400138855}, {"text": "recall posted", "start_pos": 92, "end_pos": 105, "type": "METRIC", "confidence": 0.9727837145328522}]}, {"text": "CB was relatively easier to predict.", "labels": [], "entities": [{"text": "CB", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.4678145945072174}]}, {"text": "NA was difficult to predict using the rule based approach, but supervised learning approach obtained reasonable performance of 69.9 F-measure.", "labels": [], "entities": [{"text": "NA", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.5584560632705688}, {"text": "F-measure", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9888010025024414}]}, {"text": "In this section, we describe an initial investigation towards an entity-focused evaluation.", "labels": [], "entities": []}, {"text": "An entityfocused evaluation tests a different kind of question about beliefs: given an entity e, what beliefs does the writer have about e?", "labels": [], "entities": []}, {"text": "This entity-focused evaluation draws its parallels from TAC KBP Sentiment Slot Filling Evaluation (SSF) task.", "labels": [], "entities": [{"text": "TAC KBP Sentiment Slot Filling Evaluation (SSF)", "start_pos": 56, "end_pos": 103, "type": "TASK", "confidence": 0.7468733953105079}]}, {"text": "In the SSF, the task is to determine a target entity given a source entity and a sentiment between them.", "labels": [], "entities": [{"text": "SSF", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9406868815422058}]}, {"text": "The goal is to populate a knowledge base with information regarding entities and the sentiment relations between them.", "labels": [], "entities": []}, {"text": "In the same vein, an entity-focused belief task would provide knowledge about the salient belief relations between entities.", "labels": [], "entities": []}, {"text": "For this purpose, we needed to define what is meant by \"having a belief about an entity\" and agreed on the following preliminary rules.", "labels": [], "entities": []}, {"text": "The rules are entirely syntactic.", "labels": [], "entities": []}, {"text": "In the following examples, the target entity is Mary, and the statement after the arrow shows what the beliefs are about her (and what the level of commitment by the writer is).", "labels": [], "entities": []}, {"text": "If the target entity is contained in a clause (lets call it the \"core clause\") but NOT in an adjunct clause which modifies the core clause, we omit the adjunct clause (even though the adjunct clause in some sense pertains to the core clause but by virtue of being an adjunct, it is omissable).", "labels": [], "entities": []}, {"text": "If the target is in an adjunct clause to a core clause where the target is not mentioned, we retain both the adjunct clause as a standalone belief, and the combination of the adjunct and core (i.e., we have two beliefs about the entity).", "labels": [], "entities": []}, {"text": "(8) John was happy/CB when Mary left/CB Paul \u2212\u2192 CB: John was happy when Mary left Paul ; CB: Mary left Paul We devised similar rules for complement clauses, we omit them here.", "labels": [], "entities": []}, {"text": "For the actual evaluation, we used files which also had been hand-annotated for ACE entities.", "labels": [], "entities": []}, {"text": "However, we did not have a gold annotation for entityfocused belief, as this study is still contributing towards a definition of this notion.", "labels": [], "entities": []}, {"text": "Only two systems participated, System A and System C. System A as described in Section 4.1 already takes the notion of entity into account.", "labels": [], "entities": []}, {"text": "For System C, we used the parse to determine the span associated with the annotated headword, and counted a proposition whose span included an entity to be about that entity.", "labels": [], "entities": []}, {"text": "In order to understand how these two ways of determining entity-focused belief relate to each other, we compared the two systems to each other.", "labels": [], "entities": []}, {"text": "We obtained an F-measure of 52%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9996329545974731}]}, {"text": "We also hand evaluated the positive claims of System C, obtaining an accuracy of 48% on the positive claims.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9997232556343079}]}, {"text": "The errors are due to parse errors, the presence of the entity in adjuncts which do not appear germane (contradicting adjunct clause case 2), the presence of irrelevant adjunct clauses (counter to adjunct clause case 1), and to alack of clarity in the annotation standard.", "labels": [], "entities": [{"text": "clarity", "start_pos": 237, "end_pos": 244, "type": "METRIC", "confidence": 0.9931932091712952}]}, {"text": "As an example of the lack of clarity, consider the following sentence from our evaluation corpus, with two kids as target entity: (9) I didn't see these two kids (sic) names on the news two kids is a possessor of the direct object, and fell into the span of the annotated see for System C, but System A deemed the 'see' belief not to be about it.", "labels": [], "entities": []}, {"text": "We conclude that this purely syntactic definition of \"belief about an entity\" is not satisfactory.", "labels": [], "entities": []}, {"text": "The definition of \"belief about an entity\" remains an open question and we return to it in Section 7.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results obtained for System A, System B, and System C on the final Evaluation dataset.", "labels": [], "entities": [{"text": "Evaluation dataset", "start_pos": 77, "end_pos": 95, "type": "DATASET", "confidence": 0.7383193671703339}]}]}