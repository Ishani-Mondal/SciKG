{"title": [{"text": "JAIST: Combining multiple features for Answer Selection in Community Question Answering", "labels": [], "entities": [{"text": "JAIST", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7583057284355164}, {"text": "Answer Selection", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.9599893987178802}, {"text": "Community Question Answering", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.5975029766559601}]}], "abstractContent": [{"text": "In this paper, we describe our system for SemEval-2015 Task 3: Answer Selection in Community Question Answering.", "labels": [], "entities": [{"text": "SemEval-2015 Task 3", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8851315776507059}, {"text": "Answer Selection in Community Question Answering", "start_pos": 63, "end_pos": 111, "type": "TASK", "confidence": 0.6810816476742426}]}, {"text": "In this task, the systems are required to identify the good or potentially good answers from the answer thread in Community Question Answering collections.", "labels": [], "entities": [{"text": "Community Question Answering collections", "start_pos": 114, "end_pos": 154, "type": "TASK", "confidence": 0.6263507381081581}]}, {"text": "Our system combines 16 features belong to 5 groups to predict answer quality.", "labels": [], "entities": []}, {"text": "Our final model achieves the best result in sub-task A for English, both inaccuracy and F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9982962012290955}]}], "introductionContent": [{"text": "Nowadays, community question answering (cQA) websites like Yahoo!", "labels": [], "entities": [{"text": "community question answering (cQA)", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.7733832399050394}]}, {"text": "Answers play a crucial role in supporting people to seek desired information.", "labels": [], "entities": []}, {"text": "Users can post their questions on these sites for finding help as well as personal advice.", "labels": [], "entities": []}, {"text": "However, the quality of these answers varies greatly.", "labels": [], "entities": []}, {"text": "Typically, only a few of the answers in an answer thread are useful to the users and it may take a lot of efforts to identify them manually.", "labels": [], "entities": []}, {"text": "Thus, a system that automatically identifies answer quality is much needed.", "labels": [], "entities": []}, {"text": "The task of identifying answer quality has been studied by many researchers in the field of Question Answering.", "labels": [], "entities": [{"text": "identifying answer quality", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.8029828667640686}, {"text": "Question Answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7774138748645782}]}, {"text": "Many methods have been proposed: web redundancy information (), non-textual features (), textual entailment (), syntactic features).", "labels": [], "entities": []}, {"text": "However, most of these works used independent dataset and evaluation metrics; thus it is difficult to compare the results of these methods.", "labels": [], "entities": []}, {"text": "addresses this problem by providing a common framework to compare different methods in multiple languages.", "labels": [], "entities": []}, {"text": "Our system incorporates a range of features: word-matching features, special component features, topic-modeling-based features, translationbased features and non-textual features to achieve the best performance in subtask A).", "labels": [], "entities": []}, {"text": "In the remainder of the paper, we will describe our system with the focus on the features.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Detail Class F1-score", "labels": [], "entities": [{"text": "Detail Class F1-score", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.6216934025287628}]}]}