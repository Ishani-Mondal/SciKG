{"title": [{"text": "CDTDS: Predicting Paraphrases in Twitter via Support Vector Regression", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we describe a system that recognizes paraphrases in Twitter for tweets that refer to the same topic.", "labels": [], "entities": []}, {"text": "The system participated in Task1 of SEMEVAL-2015 and uses a support vector regression machine to predict the degree of similarity.", "labels": [], "entities": []}, {"text": "The similarity is then thresholded to create a binary prediction.", "labels": [], "entities": [{"text": "similarity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9478292465209961}]}, {"text": "The model and experimental results are discussed along with future work that could improve the method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, Twitter has gained significant popularity among the social network services.", "labels": [], "entities": []}, {"text": "Lots of users often use Twitter to express feelings or opinions about a variety of subjects.", "labels": [], "entities": []}, {"text": "Analysing this kind of content can lead to useful information for fields such as personalized marketing or social profiling.", "labels": [], "entities": []}, {"text": "However, such a task is not trivial, because the language used on Twitter is often informal, presenting new challenges to text analysis.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.8162733018398285}]}, {"text": "Task1 of SEMEVAL-2015 () focuses on recognition of paraphrases and semantic similarity in Twitter i.e., recognizing if two tweets are alternative linguistic expressions of the same, or similar, meaning.", "labels": [], "entities": []}, {"text": "The task is based on a crowdsourced corpus of 18000 pairs of paraphrases and non-paraphrased sentences from Twitter () and each pair consists of two tweets from the same topic.", "labels": [], "entities": []}, {"text": "A label is provided with each pair, which is the number of yes votes from 5 crowdsourced annotators when asked if the second tweet is a paraphrase of the first one.", "labels": [], "entities": []}, {"text": "The method utilizes a support vector regression machine (SVR).", "labels": [], "entities": []}, {"text": "The regression model tries to predict the degree of semantic similarity between two tweets, by assuming that it can be represented by the probability that random human annotators would annotate the pair as a paraphrase.", "labels": [], "entities": []}, {"text": "The predicted value is transformed into a binary decision via a threshold.", "labels": [], "entities": []}, {"text": "Section 2 describes the data provided by the organizers.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 present the system and its performance respectively.", "labels": [], "entities": []}, {"text": "Finally, Section 6 provides ideas for future work and Section 7 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each system had to submit for each test set instance its prediction (paraphrase or not) (subtask1) and optionally a degree of semantic similarity (subtask2).", "labels": [], "entities": []}, {"text": "To evaluate system performance for subtask1 the organizers used F 1 against human judgements on the predictions.", "labels": [], "entities": [{"text": "F 1", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.992189347743988}]}, {"text": "While for subtask2 they used the Pearson correlation of the predicted similarity scores with the human scores.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 33, "end_pos": 52, "type": "METRIC", "confidence": 0.9697495996952057}]}, {"text": "Our team was ranked 9th on both subtasks 4 and our systems were ranked 13th and 14th on subtask1 and 15th and 16th on subtask2.", "labels": [], "entities": []}, {"text": "illustrates the results and rankings of our systems and the baselines.", "labels": [], "entities": []}, {"text": "The results indicate that the sentiment feature decreases performance and should be removed from our system.", "labels": [], "entities": []}, {"text": "We used the official evaluation script to assess the performance of our systems on the test set for different threshold values.", "labels": [], "entities": []}, {"text": "The results are illustrated in 6 teams did not participate in subtask2.", "labels": [], "entities": []}, {"text": "We used thresholds from 0 to 1 with a step of 0.05 except for the space [0.3, 0.4] where we used a step of 0.01.", "labels": [], "entities": []}, {"text": "The two systems behave similarly and the best performance (0.622) was achieved from the All-Sentiment system using a threshold of 0.36.", "labels": [], "entities": []}, {"text": "Figure 2: F1 for subtask1 on the test set for our systems using different threshold values.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9990469813346863}]}], "tableCaptions": [{"text": " Table 1: Class distribution of the train, development  and test sets.", "labels": [], "entities": []}, {"text": " Table 2: Mapping of the number of positive votes  from the annotators to real valued labels.", "labels": [], "entities": [{"text": "Mapping", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9776985049247742}]}, {"text": " Table 3: Results of our systems, baselines and human annotators on the test set.", "labels": [], "entities": []}]}