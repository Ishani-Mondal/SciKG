{"title": [{"text": "MITRE: Seven Systems for Semantic Similarity in Tweets", "labels": [], "entities": [{"text": "MITRE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8378989100456238}]}], "abstractContent": [{"text": "This paper describes MITRE's participation in the Paraphrase and Semantic Similarity in Twitter task (SemEval-2015 Task 1).", "labels": [], "entities": [{"text": "Paraphrase and Semantic Similarity in Twitter task", "start_pos": 50, "end_pos": 100, "type": "TASK", "confidence": 0.7832946096147809}]}, {"text": "This effort placed first in Semantic Similarity and second in Paraphrase Identification with scores of Pearson's r of 61.9%, F1 of 66.7%, and maxF1 of 72.4%.", "labels": [], "entities": [{"text": "Semantic Similarity", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.823455423116684}, {"text": "Paraphrase Identification", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.9416022896766663}, {"text": "Pearson's r", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.9592394828796387}, {"text": "F1", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.999735414981842}, {"text": "maxF1", "start_pos": 142, "end_pos": 147, "type": "METRIC", "confidence": 0.9924073815345764}]}, {"text": "We detail the approaches we explored including mixtures of string matching metrics, alignments using tweet-specific distributed word representations , recurrent neural networks for model-ing similarity with those alignments, and distance measurements on pooled latent semantic features.", "labels": [], "entities": []}, {"text": "Logistic regression is used to tie the systems together into the ensembles submitted for evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Paraphrase identification is the task of judging if two texts express the same or very similar meaning.", "labels": [], "entities": [{"text": "Paraphrase identification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9423538446426392}]}, {"text": "Automatic identification of paraphrases has practical applications fora range of domains, including news summarization, information retrieval, essay grading, and evaluation of machine translation outputs.", "labels": [], "entities": [{"text": "Automatic identification of paraphrases", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7800307348370552}, {"text": "news summarization", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7004625350236893}, {"text": "information retrieval", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.8034496605396271}, {"text": "essay grading", "start_pos": 143, "end_pos": 156, "type": "TASK", "confidence": 0.7487911880016327}, {"text": "evaluation of machine translation outputs", "start_pos": 162, "end_pos": 203, "type": "TASK", "confidence": 0.6991494119167327}]}, {"text": "Furthermore, work on paraphrase detection tends to advance the state of art in modeling semantics and semantic similarity in natural language in general.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.9748151898384094}]}, {"text": "Current approaches to paraphrase detection vary widely.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.9827382266521454}]}, {"text": "The Microsoft Research Paraphrase Corpus, with pairs of sentences from newswire text, serves as a benchmark for the task ().", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase Corpus", "start_pos": 4, "end_pos": 40, "type": "DATASET", "confidence": 0.841400995850563}]}, {"text": "One top result on this dataset uses features from surface characteristics of text ().", "labels": [], "entities": []}, {"text": "Another system with comparable results models sentences as hierarchical compositions of distributed word embeddings).", "labels": [], "entities": []}, {"text": "SemEval-2015 Task 1 (, with a corpus drawn from Twitter, offers an opportunity to test paraphrase systems in a domain with an expanded vocabulary and informal grammar.", "labels": [], "entities": []}, {"text": "Our contribution builds upon the recent success of distributed representations of language ().", "labels": [], "entities": []}, {"text": "We further aim to minimize reliance on language-and domaindependent tools.", "labels": [], "entities": []}, {"text": "However we do not possess enough labeled paraphrase data to train a generalized model of word composition.", "labels": [], "entities": [{"text": "word composition", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.6958838850259781}]}, {"text": "Instead we explore models that examine low-dimensional relationships between individual pairs of aligned words, and combine the above with string similarity features that generalize well to out-of-vocabulary terms.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we describe our high-performing system for modeling semantic similarity between two tweets.", "labels": [], "entities": []}, {"text": "In Section 2 we describe the data, task, and evaluation.", "labels": [], "entities": []}, {"text": "In Section 3 we discuss details of systems we built to solve the semantic similarity task.", "labels": [], "entities": [{"text": "semantic similarity task", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.8271607160568237}]}, {"text": "We describe our experiments on different parameterizations in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5 we present performance results for our ensembles and all subsystems, and in Section 6 we summarize our findings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Paraphrase and Semantic Similarity in Twitter was a shared task organized within SemEval-2015.", "labels": [], "entities": [{"text": "Paraphrase and Semantic Similarity in Twitter", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7463777661323547}]}, {"text": "The task organizers released 18,762 pairs of English-language tweets with a 70/25/5 split for train, development, and test sets.", "labels": [], "entities": []}, {"text": "The organizers removed URLs, deleted non-alphanumeric characters, and provided part of speech tags.", "labels": [], "entities": []}, {"text": "Tweet pairs were judged by five human annotators to be a paraphrase (e.g. Amber alert gave me a damn heart attack and That Amber alert scared the crap out of me) or not (e.g. My phone is annoying me with these amber alert and Am I the only one who dont get Amber alert).", "labels": [], "entities": []}, {"text": "Approximately 35% of provided pairs are paraphrases.", "labels": [], "entities": []}, {"text": "For each pair, task participants predict a binary label and optionally provide a confidence score.", "labels": [], "entities": []}, {"text": "Systems were evaluated by F1 measure, F1 at the best confidence threshold, and Pearson correlation with expert annotation.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9849328994750977}, {"text": "F1", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9989294409751892}, {"text": "Pearson correlation", "start_pos": 79, "end_pos": 98, "type": "METRIC", "confidence": 0.9441221058368683}]}, {"text": "In all experiments, systems were trained while omitting debatable examples with scores of 2 as suggested by the task organizers.", "labels": [], "entities": []}, {"text": "The development set was used both to fit the hyperparameters (ablations, lambdas) and the eventual ensemble.", "labels": [], "entities": []}, {"text": "String Similarity Ablations The MT evaluation metrics and string similarities contributed varying amounts to that system.", "labels": [], "entities": [{"text": "String Similarity Ablations", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7319316069285074}, {"text": "MT evaluation", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.8769093751907349}]}, {"text": "In we show the score achieved by the logistic regression system built using just that one measure (in the Factored column) as well as the F1 achieved by the logistic regression when only that one measure is left out (Ablated column).", "labels": [], "entities": [{"text": "F1", "start_pos": 138, "end_pos": 140, "type": "METRIC", "confidence": 0.9987120628356934}, {"text": "Ablated", "start_pos": 217, "end_pos": 224, "type": "METRIC", "confidence": 0.9054691195487976}]}, {"text": "BLEU was omitted from the subsystem as a result of this analysis.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9741725921630859}]}, {"text": "Ensemble Construction We focused our ensembles only on the output of our individual components, ignoring the features from the original data they attempt to model.", "labels": [], "entities": []}, {"text": "shows the weights of these components.", "labels": [], "entities": []}, {"text": "Note that NormalizedAvg produced larger outputs than the rest; as a result its coefficient is about 10 times smaller than its effect.: Test scores of Semantic Similarity Systems (%).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dev set F1 scores for string similarities.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9766731560230255}]}, {"text": " Table 2: Test scores of Semantic Similarity Systems (%).", "labels": [], "entities": []}]}