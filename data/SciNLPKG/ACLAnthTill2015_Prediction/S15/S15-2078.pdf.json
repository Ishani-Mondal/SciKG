{"title": [{"text": "SemEval-2015 Task 10: Sentiment Analysis in Twitter", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.9544989168643951}]}], "abstractContent": [{"text": "In this paper, we describe the 2015 iteration of the SemEval shared task on Sentiment Analysis in Twitter.", "labels": [], "entities": [{"text": "SemEval shared task", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8734630743662516}, {"text": "Sentiment Analysis in Twitter", "start_pos": 76, "end_pos": 105, "type": "TASK", "confidence": 0.8355239629745483}]}, {"text": "This was the most popular sentiment analysis shared task to date with more than 40 teams participating in each of the last three years.", "labels": [], "entities": [{"text": "sentiment analysis shared task", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.9269254952669144}]}, {"text": "This year's shared task competition consisted of five sentiment prediction sub-tasks.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.9163011610507965}]}, {"text": "Two were reruns from previous years: (A) sentiment expressed by a phrase in the context of a tweet, and (B) overall sentiment of a tweet.", "labels": [], "entities": []}, {"text": "We further included three new sub-tasks asking to predict (C) the sentiment towards a topic in a single tweet, (D) the overall sentiment towards a topic in a set of tweets, and (E) the degree of prior polarity of a phrase.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social media such as Weblogs, microblogs, and discussion forums are used daily to express personal thoughts, which allows researchers to gain valuable insight into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago.", "labels": [], "entities": []}, {"text": "As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.9369858801364899}]}, {"text": "In particular, opinion mining and opinion detection are applied to product reviews (), for agreement detection (, and even for sarcasm identification.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.8367581367492676}, {"text": "opinion detection", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.8202852606773376}, {"text": "agreement detection", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.8909240663051605}, {"text": "sarcasm identification", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.92415452003479}]}, {"text": "Early work on detecting sentiment focused on newswire text ().", "labels": [], "entities": [{"text": "detecting sentiment", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.9369717538356781}]}, {"text": "As later research turned towards social media, people realized this presented a number of new challenges.", "labels": [], "entities": []}, {"text": "Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (.", "labels": [], "entities": []}, {"text": "Later, specialized shared tasks emerged, e.g., at SemEval (, which compared teams against each other in a controlled environment using the same training and testing datasets.", "labels": [], "entities": []}, {"text": "These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC's Hashtag Sentiment lexicon and the Sentiment140 lexicon (.", "labels": [], "entities": [{"text": "NRC's Hashtag Sentiment lexicon", "start_pos": 144, "end_pos": 175, "type": "DATASET", "confidence": 0.7956451535224914}]}, {"text": "Below, we discuss the public evaluation done as part of.", "labels": [], "entities": []}, {"text": "In its third year, the SemEval task on Sentiment Analysis in Twitter has once again attracted a large number of participants: 41 teams across five subtasks, with most teams participating in more than one subtask.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.9172494113445282}, {"text": "Sentiment Analysis in Twitter", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.8333527743816376}]}, {"text": "This year the task included reruns of two legacy subtasks, which asked to detect the sentiment expressed in a tweet or by a particular phrase in a tweet.", "labels": [], "entities": []}, {"text": "The task further added three new subtasks.", "labels": [], "entities": []}, {"text": "The first two focused on the sentiment towards a given topic in a single tweet or in a set of tweets, respectively.", "labels": [], "entities": []}, {"text": "The third new subtask focused on determining the strength of prior association of Twitter terms with positive sentiment; this acts as an intrinsic evaluation of automatic methods that build Twitter-specific sentiment lexicons with real-valued sentiment association scores.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we first introduce the problem of sentiment polarity classification and our subtasks.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.8987143635749817}]}, {"text": "We then describe the process of creating the training, development, and testing datasets.", "labels": [], "entities": []}, {"text": "We list and briefly describe the participating systems, the results, and the lessons learned.", "labels": [], "entities": []}, {"text": "Finally, we compare the task to other related efforts and we point to possible directions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the process of collecting and annotating our datasets of short social media text messages.", "labels": [], "entities": []}, {"text": "We focus our discussion on the 2015 datasets; more detail about the 2013 and the 2014 datasets can be found in ( and ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Example of a sentence annotated for subjectivity on Mechanical Turk. Words and phrases that were marked  as subjective are in bold italic. The first five rows are annotations provided by Turkers, and the final row shows their  intersection. The last column shows the token-level accuracy for each annotation compared to the intersection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 289, "end_pos": 297, "type": "METRIC", "confidence": 0.955634593963623}]}, {"text": " Table 3: Dataset statistics for subtask A.", "labels": [], "entities": []}, {"text": " Table 4: Dataset statistics for subtask B.", "labels": [], "entities": []}, {"text": " Table 5: Twitter-2015 statistics for subtasks C & D.", "labels": [], "entities": []}, {"text": " Table 7: Example of annotations in Twitter showing differences between topic-and message-level polarity.", "labels": [], "entities": []}, {"text": " Table 8: Average (over all HITs) overlap of the gold an- notations with the worst, average, and the worst Turker  for each HIT, for subtasks A and B.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9882766604423523}, {"text": "overlap", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9454509615898132}, {"text": "Turker", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9892459511756897}]}, {"text": " Table 10: Results for subtask A: Phrase-Level Polarity. The systems are ordered by their score on the Twitter2015  test dataset; the rankings on the individual datasets are indicated with a subscript.", "labels": [], "entities": [{"text": "Twitter2015  test dataset", "start_pos": 103, "end_pos": 128, "type": "DATASET", "confidence": 0.9833645820617676}]}, {"text": " Table 11: Results for subtask B: Message-Level Polarity. The systems are ordered by their score on the Twitter2015  test dataset; the rankings on the individual datasets are indicated with a subscript. Systems with late submissions for  the progress test datasets (but with timely submissions for the official 2015 test dataset) are marked with a .", "labels": [], "entities": [{"text": "Twitter2015  test dataset", "start_pos": 104, "end_pos": 129, "type": "DATASET", "confidence": 0.9821398456891378}, {"text": "2015 test dataset", "start_pos": 311, "end_pos": 328, "type": "DATASET", "confidence": 0.6450342833995819}]}, {"text": " Table 12: Results for Subtask C: Topic-Level Polarity.  The systems are ordered by the official 2015 score.", "labels": [], "entities": []}, {"text": " Table 13: Results for Subtask D: Trend Towards a  Topic. The systems are sorted by the official 2015 score.", "labels": [], "entities": [{"text": "official 2015 score", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.6805122097333273}]}, {"text": " Table 14: Results for Subtask E: Degree of Prior Po- larity. The systems are ordered by their Kendall's \u03c4  score, which was the official score.", "labels": [], "entities": [{"text": "Degree of Prior Po- larity", "start_pos": 34, "end_pos": 60, "type": "METRIC", "confidence": 0.8632008830706278}, {"text": "Kendall's \u03c4  score", "start_pos": 95, "end_pos": 113, "type": "METRIC", "confidence": 0.773705244064331}]}]}