{"title": [{"text": "TATO: Leveraging on Multiple Strategies for Semantic Textual Similarity", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.592600405216217}]}], "abstractContent": [{"text": "In this paper, we describe the TATO system which participated in the SemEval-2015 Task 2a: \"Semantic Textual Similarity (STS) for English\".", "labels": [], "entities": [{"text": "SemEval-2015 Task 2a", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.8665235042572021}, {"text": "Semantic Textual Similarity (STS)", "start_pos": 92, "end_pos": 125, "type": "TASK", "confidence": 0.7539800653855006}]}, {"text": "Our system is trained on published datasets from the previous competitions.", "labels": [], "entities": []}, {"text": "Based on some machine learning techniques , it combines multiple similarity measures of varying complexity ranging from simple lexical and syntactic similarity measures to complex semantic similarity ones to compute semantic textual similarity.", "labels": [], "entities": []}, {"text": "Our final model consists of a simple linear combination of about 30 main features out of a numerous number of features experimented.", "labels": [], "entities": []}, {"text": "The results are promising, with Pearson's coefficients on each individual dataset ranging from 0.6796 to 0.8167 and an overall weighted mean score of 0.7422, well above the task baseline system.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9829304814338684}]}], "introductionContent": [{"text": "Measuring semantic textual similarity (STS) can be defined as the task of computing the degree of semantic equivalence between pairs of texts.", "labels": [], "entities": [{"text": "semantic textual similarity (STS)", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.7513548384110132}]}, {"text": "It has drawn an increasing amount of attention from the NLP community, especially at level of short text fragments, as partly reflected in the SemEval tasks in recent years.", "labels": [], "entities": []}, {"text": "In the SemEval-2015 Task 2, the degree of semantic equivalence for each sentence pair is represented by a similarity score between 0 (no relation) and 5 (semantic equivalence).", "labels": [], "entities": [{"text": "SemEval-2015 Task 2", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7787598967552185}]}, {"text": "STS has a wide range of applications which includes applications for machine translation evaluation, information extraction, question answering, and summarization.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.8869179884592692}, {"text": "information extraction", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.8694628477096558}, {"text": "question answering", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.9374195635318756}, {"text": "summarization", "start_pos": 149, "end_pos": 162, "type": "TASK", "confidence": 0.9893952012062073}]}, {"text": "STS is related to, but different from textual entailment (TE) () and paraphrase recognition (PARA) () as it aims to render a graded notion of semantic equivalence between two textual snippets, rather than a binary yes/no decision.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8436262607574463}, {"text": "paraphrase recognition (PARA)", "start_pos": 69, "end_pos": 98, "type": "METRIC", "confidence": 0.6539188683032989}]}, {"text": "STS requires a bidirectional similarity relation between sentences, while TE annotates them with an unidirectional entailment relation.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.4629743993282318}]}, {"text": "The literature of STS is rife with attempts to compute similarity between texts using a multitude of measures at different levels of depth: lexical, syntactic, and semantic (.", "labels": [], "entities": [{"text": "STS", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9610795378684998}]}, {"text": "() discusses existing works on STS and partitions them into three categories based on the similarity measures used: (i) string-based approaches which operate on string sequences and character composition to compute similarities and can be categorized into two groups: character-based and term-based approaches; (ii) corpus-based approaches () which gain statistics information about words from large corpora and reflect their semantics in distributional high semantic space to determine the similarity, such as Latent Semantic Analysis (LSA) ) and Explicit Semantic Analysis (ESA) (; (iii) knowledge-based approaches () which determine the degree of similarity between texts using information derived from semantic networks, such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 733, "end_pos": 740, "type": "DATASET", "confidence": 0.9398927688598633}]}, {"text": "Though each of these existing measures has its own advantages, they are typically used in separation.", "labels": [], "entities": [{"text": "separation", "start_pos": 90, "end_pos": 100, "type": "TASK", "confidence": 0.9737343192100525}]}, {"text": "In our work, we integrate multiple similarity measures of varying complexity ranging from simple lexical and syntactic similarity measures to complex semantic similarity ones and rely on supervised machine learning to take advantage of the different contributions of different features.", "labels": [], "entities": []}, {"text": "We organize the remainder of the paper as follows: Section 2 describes the features in detail.", "labels": [], "entities": []}, {"text": "Section 3 presents the machine learning setup and our submitted system.", "labels": [], "entities": []}, {"text": "Sections 4 discusses the results.", "labels": [], "entities": []}, {"text": "The conclusions follow in the final section.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on the 2014 test datasets: deft-forum  (DF), deft-news (DN), headlines (H), images (I), OnWN  (OWN), tweet-news (TN).", "labels": [], "entities": [{"text": "2014 test datasets", "start_pos": 25, "end_pos": 43, "type": "DATASET", "confidence": 0.7069593469301859}]}, {"text": " Table 2: Official results on the test datasets: answers- forums (AF), answers-students (AS), belief (B), head- lines (H), and images (I).", "labels": [], "entities": []}]}