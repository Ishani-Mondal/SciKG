{"title": [{"text": "USAAR-SHEFFIELD: Semantic Textual Similarity with Deep Regression and Machine Translation Evaluation Metrics", "labels": [], "entities": [{"text": "USAAR-SHEFFIELD", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9663442969322205}, {"text": "Machine Translation Evaluation", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.802238921324412}]}], "abstractContent": [{"text": "This paper describes the USAAR-SHEFFIELD systems that participated in the Semantic Textual Similarity (STS) English task of SemEval-2015.", "labels": [], "entities": [{"text": "USAAR-SHEFFIELD", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.9566318392753601}, {"text": "Semantic Textual Similarity (STS) English task of SemEval-2015", "start_pos": 74, "end_pos": 136, "type": "TASK", "confidence": 0.8124060958623887}]}, {"text": "We extend the work on using machine translation evaluation metrics in the STS task.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.8320592641830444}, {"text": "STS task", "start_pos": 74, "end_pos": 82, "type": "TASK", "confidence": 0.8348205983638763}]}, {"text": "Different from previous approaches, we regard the metrics' robustness across different text types and conflate the training data across different subcorpora.", "labels": [], "entities": []}, {"text": "In addition, we introduce a novel deep regressor architecture and evaluated its efficiency in the STS task.", "labels": [], "entities": [{"text": "STS task", "start_pos": 98, "end_pos": 106, "type": "TASK", "confidence": 0.8913814723491669}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) is the task of measuring the degree to which two text snippets have the same meaning ().", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7983513573805491}]}, {"text": "For instance, given the two texts, \"a dog sprints across the water\" and \"a dog jumps through water\", participating systems are required to predict areal number similarity score on a scale of 0 (no relation) to 5 (semantic equivalence).", "labels": [], "entities": [{"text": "areal number similarity score", "start_pos": 147, "end_pos": 176, "type": "METRIC", "confidence": 0.6888414546847343}]}, {"text": "This paper presents a collaborative submission between Saarland University and University of Sheffield to the STS English shared task at SemEval-2015.", "labels": [], "entities": [{"text": "STS English shared task at SemEval-2015", "start_pos": 110, "end_pos": 149, "type": "DATASET", "confidence": 0.5402195900678635}]}, {"text": "We have submitted three models that use Machine Translation (MT) evaluation metrics as features to build supervised regressors that predict the similarity scores for the STS task.", "labels": [], "entities": [{"text": "Machine Translation (MT) evaluation", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.8569729427496592}]}, {"text": "We introduce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.8399818539619446}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Spearman's Results for STS English Task @ SemEval-2015.", "labels": [], "entities": [{"text": "STS English Task @ SemEval-2015", "start_pos": 33, "end_pos": 64, "type": "DATASET", "confidence": 0.7607429981231689}]}]}