{"title": [{"text": "NeRoSim: A System for Measuring and Interpreting Semantic Textual Similarity", "labels": [], "entities": []}], "abstractContent": [{"text": "We present in this paper our system developed for SemEval 2015 Shared Task 2 (2a-En-glish Semantic Textual Similarity, STS, and 2c-Interpretable Similarity) and the results of the submitted runs.", "labels": [], "entities": [{"text": "SemEval 2015 Shared Task 2", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.862537145614624}, {"text": "STS", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.8918061852455139}]}, {"text": "For the English STS subtask, we used regression models combining a wide array of features including semantic similarity scores obtained from various methods.", "labels": [], "entities": [{"text": "English STS subtask", "start_pos": 8, "end_pos": 27, "type": "DATASET", "confidence": 0.7537956635157267}]}, {"text": "One of our runs achieved weighted mean correlation score of 0.784 for sentence similarity subtask (i.e., English STS) and was ranked tenth among 74 runs submitted by 29 teams.", "labels": [], "entities": [{"text": "weighted mean correlation score", "start_pos": 25, "end_pos": 56, "type": "METRIC", "confidence": 0.852547362446785}]}, {"text": "For the interpretable similarity pilot task, we employed a rule-based approach blended with chunk alignment labeling and scoring based on semantic similarity features.", "labels": [], "entities": [{"text": "chunk alignment labeling", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.8251650532086691}]}, {"text": "Our system for interpretable text similarity was among the top three best performing systems.", "labels": [], "entities": [{"text": "interpretable text similarity", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.8040043314297994}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) is the task of measuring the degree of semantic equivalence fora given pair of texts.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7985353320837021}]}, {"text": "The importance of semantic similarity in Natural Language Processing is highlighted by the diversity of datasets and shared task evaluation campaigns over the last decade) and by many uses such as in text summarization) and student answer assessment).", "labels": [], "entities": [{"text": "text summarization", "start_pos": 200, "end_pos": 218, "type": "TASK", "confidence": 0.7079248428344727}, {"text": "student answer assessment", "start_pos": 224, "end_pos": 249, "type": "TASK", "confidence": 0.6246052086353302}]}, {"text": "* * These authors contributed equally to this work \u2020 \u2020Work done while at University of Memphis This year's SemEval shared task on semantic textual similarity focused on English STS, Spanish STS, and Interpretable Similarity (.", "labels": [], "entities": [{"text": "SemEval shared task", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8854033549626669}, {"text": "semantic textual similarity", "start_pos": 130, "end_pos": 157, "type": "TASK", "confidence": 0.5410126745700836}]}, {"text": "We participated in the English STS and Interpretable Similarity subtasks.", "labels": [], "entities": [{"text": "English STS", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.5578072369098663}]}, {"text": "We describe in this paper our systems participated in these two subtasks.", "labels": [], "entities": []}, {"text": "The English STS subtask was about assigning a similarity score between 0 and 5 to pairs of sentences; a score of 0 meaning the sentences are unrelated and 5 indicating they are equivalent.", "labels": [], "entities": [{"text": "English STS subtask", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.6096964279810587}]}, {"text": "Our three runs for this subtask combined a wide array of features including similarity scores calculated using knowledge based and corpus based methods in a regression model (cf. Section 2).", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 76, "end_pos": 93, "type": "METRIC", "confidence": 0.9508418440818787}]}, {"text": "One of our systems achieved mean correlation score of 0.784 with human judgment on the test data.", "labels": [], "entities": [{"text": "mean correlation score", "start_pos": 28, "end_pos": 50, "type": "METRIC", "confidence": 0.8643507361412048}]}, {"text": "Although STS systems measure the degree of semantic equivalence in terms of a score which is useful in many tasks, they stop short of explaining why the texts are similar, related, or unrelated.", "labels": [], "entities": []}, {"text": "They do not indicate what kind of semantic relations exist among the constituents (words or chunks) of the target texts.", "labels": [], "entities": []}, {"text": "Finding explicit relations between constituents in the paired texts would enable a meaningful interpretation of the similarity scores.", "labels": [], "entities": []}, {"text": "To this end, and  produced datasets where corresponding words (or multiword expressions) were aligned and in the later case their semantic relations were explicitly labeled.", "labels": [], "entities": []}, {"text": "Similarly, this year's pilot subtask called Interpretable Similarity required systems to align the segments (chunks) either using the chunked texts given by the organizers or chunking the given texts and indicating the type of semantic relations (such as EQUI for equivalent, OPPO for opposite) between each pair of aligned chunks.", "labels": [], "entities": []}, {"text": "Moreover, a similarity score for each alignment (0 \u2212 unrelated, 5 \u2212 equivalent) had to be assigned.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.9759235382080078}]}, {"text": "We applied a set of rules blended with similarity features in order to assign the labels and scores for the chunk-level relations (cf. Section 3).", "labels": [], "entities": []}, {"text": "Our system was among the top performing systems in this subtask.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied above mentioned rules in the training data set by varying thresholds for sim-M ikolov scores and selected the thresholds that produced the best results in the training data set.", "labels": [], "entities": []}, {"text": "Since three runs were allowed to submit, we defined them as follows: Run1(R 1 ) : We applied our full set of rules with limited stop words (375 words).", "labels": [], "entities": []}, {"text": "However EQ 4 was modified such that it would apply when unmatched content words of the bigger chunk were of noun rather than proper noun type.", "labels": [], "entities": [{"text": "EQ 4", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.6592844128608704}]}, {"text": "Run2(R 2 ) : Same as R 1 but with extended stop words (686 words).", "labels": [], "entities": []}, {"text": "Run3(R 3 ) : Applied full set of rules with extended stop words.", "labels": [], "entities": [{"text": "Run3(R 3 )", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9142748951911926}]}, {"text": "The results corresponding to our three runs and that of the baseline are presented in.", "labels": [], "entities": []}, {"text": "In Headlines test data, our system outperformed the rest competing submissions in all evaluation metrics (except when alignment type and score were ignored).", "labels": [], "entities": [{"text": "Headlines test data", "start_pos": 3, "end_pos": 22, "type": "DATASET", "confidence": 0.9075424472490946}]}, {"text": "In Images test data, R 1 was the best in alignment and type metrics.", "labels": [], "entities": [{"text": "Images test data", "start_pos": 3, "end_pos": 19, "type": "DATASET", "confidence": 0.7677648862202963}, {"text": "R 1", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9744295477867126}]}, {"text": "Our submissions were among the top performing submissions for score and type+score metrics.", "labels": [], "entities": [{"text": "score", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.956892192363739}]}, {"text": "R 3 performed better among all runs in case of Headlines data in overall.", "labels": [], "entities": [{"text": "R 3", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.758216142654419}, {"text": "Headlines data", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.914081871509552}]}, {"text": "This was chiefly due to modified EQ 4 rule which reduced the number of incorrect EQUI alignments.", "labels": [], "entities": [{"text": "EQ 4 rule", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.7436613837877909}]}, {"text": "We also observed that performance of our system was least affected by size of stopword list for Headlines data as both R 1 and R 2 recorded similar F 1 -measures for all evaluation metrics.", "labels": [], "entities": [{"text": "Headlines data", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.9731078445911407}, {"text": "F 1 -measures", "start_pos": 148, "end_pos": 161, "type": "METRIC", "confidence": 0.9671483784914017}]}, {"text": "However, R 1 performed relatively better than R 2 in Images data-particularly in correctly aligning chunk relations.", "labels": [], "entities": []}, {"text": "It could be that images are described mostly using common words and thus were filtered by R 2 as stop words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of our submitted runs on test data.", "labels": [], "entities": []}, {"text": " Table 2. Though R1 had the highest correlation  score in a 10-fold cross validation process using the  training data, the results of R2 and R3 on the test  data were consistently better than the results of R1.  It suggests that absolute count features used in R1  tend to overfit the model. The weighted mean cor- relation of R2 was 0.784 -the best among our three  runs and ranked 10th among 74 runs submitted by 29", "labels": [], "entities": [{"text": "weighted mean cor- relation", "start_pos": 296, "end_pos": 323, "type": "METRIC", "confidence": 0.7907544374465942}]}, {"text": " Table 3: F 1 scores for Images and Headlines. A, T and  S refer to Alignment, Type, and Score respectively. The  highlighted scores are the best results produced by our  system.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9681301712989807}, {"text": "Alignment", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9637039303779602}, {"text": "Score", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9897261261940002}]}]}