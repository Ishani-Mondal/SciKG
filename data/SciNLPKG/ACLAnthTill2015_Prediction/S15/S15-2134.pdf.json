{"title": [{"text": "SemEval-2015 Task 5: QA TEMPEVAL -Evaluating Temporal Information Understanding with Question Answering", "labels": [], "entities": [{"text": "SemEval-2015 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.862216591835022}, {"text": "TEMPEVAL", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.7312934994697571}, {"text": "Evaluating Temporal Information Understanding", "start_pos": 34, "end_pos": 79, "type": "TASK", "confidence": 0.6396590322256088}, {"text": "Question Answering", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.699480801820755}]}], "abstractContent": [{"text": "QA TempEval shifts the goal of previous TempEvals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.8137533664703369}]}, {"text": "This evaluation requires systems to capture temporal information relevant to perform an end-user task, as opposed to corpus-based evaluation where all temporal information is equally important.", "labels": [], "entities": []}, {"text": "Evaluation results show that the best automated TimeML annotations reach over 30% recall on questions with 'yes' answer and about 50% on easier questions with 'no' answers.", "labels": [], "entities": [{"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9994661211967468}]}, {"text": "Features that helped achieve better results are event corefer-ence and a time expression reasoner.", "labels": [], "entities": [{"text": "event corefer-ence", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.6901637762784958}]}], "introductionContent": [{"text": "QA TempEval is a followup of the TempEval series in SemEval: TempEval-1 (, TempEval-2 (), and.", "labels": [], "entities": [{"text": "QA TempEval", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8662098944187164}]}, {"text": "TempEval focuses on evaluating systems that extract temporal expressions (timexes), events, and temporal relations as defined in the TimeML standard () (timeml.org).", "labels": [], "entities": [{"text": "TimeML standard", "start_pos": 133, "end_pos": 148, "type": "DATASET", "confidence": 0.9088361859321594}]}, {"text": "QA TempEval is unique in its focus on evaluating temporal information that directly address a QA task.", "labels": [], "entities": [{"text": "QA TempEval", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.5695114731788635}]}, {"text": "TimeML was originally developed to support research in complex temporal QA within the field of artificial intelligence (AI).", "labels": [], "entities": []}, {"text": "However, despite its original goal, the complexity of temporal QA has caused most research on automatic TimeML systems to focus on a more straightforward temporal information extraction (IE) task.", "labels": [], "entities": [{"text": "temporal information extraction (IE)", "start_pos": 154, "end_pos": 190, "type": "TASK", "confidence": 0.8195250034332275}]}, {"text": "QA TempEval still requires systems to extract temporal relations just like previous TempEvals, however, the QA evaluation is solely based on how well the relations answer questions about the documents.", "labels": [], "entities": [{"text": "QA TempEval", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.6691508889198303}]}, {"text": "It is no longer about annotation accuracy, but rather the accuracy for targeted questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.8546016216278076}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9985173344612122}]}, {"text": "Not only does QA represent a more natural way to evaluate temporal information understanding), but also annotating documents with question sets requires much less expertise and effort for humans than corpus-based evaluation which requires full manual annotation of temporal information.", "labels": [], "entities": [{"text": "temporal information understanding", "start_pos": 58, "end_pos": 92, "type": "TASK", "confidence": 0.6639789342880249}]}, {"text": "In QA TempEval a document does not require the markup of all the temporal entities and relations, but rather a markup of a few key relations central to the text.", "labels": [], "entities": []}, {"text": "Although the evaluation schema changes in QA TempEval, the task for participating systems remains the same: extracting temporal information from plain text documents.", "labels": [], "entities": [{"text": "QA TempEval", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.7276824116706848}, {"text": "extracting temporal information from plain text documents", "start_pos": 108, "end_pos": 165, "type": "TASK", "confidence": 0.8252219046865191}]}, {"text": "Here we re-use TempEval-3 task ABC, where systems are required to perform end-to-end TimeML annotation from plain text, including the complete set of temporal relations.", "labels": [], "entities": []}, {"text": "However, unlike TempEval-3, there are no subtasks focusing on specific elements (such as an event extraction evaluation).", "labels": [], "entities": [{"text": "event extraction evaluation", "start_pos": 92, "end_pos": 119, "type": "TASK", "confidence": 0.7811583280563354}]}, {"text": "Also, instead of IE performance measurement for evaluation, a QA performance (on a set of human-created temporal questions on documents) is used to rank systems.", "labels": [], "entities": [{"text": "IE", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9415009617805481}]}, {"text": "The participating systems are supposed to annotate temporal entities relations across the document, and the relations are used to build a larger knowledge base of temporal links to obtain answers to the temporal questions.", "labels": [], "entities": []}, {"text": "In QA TempEval, annotators are not required to tag and order all events, but instead ask questions about temporal relations that are relevant or interesting to the document, hence this evaluation bet-ter captures the understanding of the most important temporal information in a document.", "labels": [], "entities": []}, {"text": "Annotators are not limited to relations between entities appearing in the same or consecutive sentences, i.e., they can ask any question that comes naturally to a reader's mind, e.g., \"did the election happen (e3) before the president gave (e27) the speech\".", "labels": [], "entities": []}, {"text": "Finally, QA TempEval is unique in expanding beyond the news genre and including Wikipedia articles and blog posts.", "labels": [], "entities": [{"text": "QA TempEval", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.5496570765972137}]}, {"text": "In the upcoming sections we will discuss details of the conducted task and evaluation methodology.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main difference between QA TempEval and earlier TempEval editions is that the systems are not scored regarding how similar their annotation to a human annotated key is, but how useful is their TimeML annotation to answer human annotated temporal questions.", "labels": [], "entities": []}, {"text": "There are different kinds of temporal questions that could be answered given a TimeML annotation of a document.", "labels": [], "entities": []}, {"text": "However, this first QA TempEval focuses on yes/no questions in the following format: IS <entityA> <RELATION> <entityB> ? (e.g., is event-A before event-B ?) This makes it easier for human annotators to create accurate question sets with their answers.", "labels": [], "entities": [{"text": "QA TempEval", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.6843361854553223}, {"text": "IS <entityA> <RELATION> <entityB>", "start_pos": 85, "end_pos": 118, "type": "METRIC", "confidence": 0.8133891999721528}]}, {"text": "Other types of questions such as list-based make it more difficult and arguable in edge cases (e.g., list events between event-A and event-B).", "labels": [], "entities": []}, {"text": "Questions about events not included in the document are not possible, but theoretically one could ask about anytime reference.", "labels": [], "entities": []}, {"text": "Due to the difficulty of mapping external time references to a specific time expression in the document, these types of questions are not included in the evaluation.", "labels": [], "entities": []}, {"text": "The questions can involve any of the thirteen relations described above.", "labels": [], "entities": []}, {"text": "Two relations not in the set of thirteen, OVERLAPS and OVERLAPPED BY, cannot be explicitly annotated in TimeML, but they could happen implicitly (i.e., be inferred from other relations) if needed by an application.", "labels": [], "entities": [{"text": "OVERLAPS", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.828134298324585}, {"text": "OVERLAPPED BY", "start_pos": 55, "end_pos": 68, "type": "METRIC", "confidence": 0.7502758204936981}, {"text": "TimeML", "start_pos": 104, "end_pos": 110, "type": "DATASET", "confidence": 0.9217672944068909}]}, {"text": "The evaluation process is illustrated in.", "labels": [], "entities": []}, {"text": "After the testing period, the participants send their TimeML annotations of the test documents.", "labels": [], "entities": []}, {"text": "Organizers evaluate the TimeML annotations of all the participating systems with a set of questions.", "labels": [], "entities": []}, {"text": "The systems are scored comparing the expected answers provided by human annotators against the predicted answers obtained from the system's TimeML annotations.", "labels": [], "entities": [{"text": "TimeML", "start_pos": 140, "end_pos": 146, "type": "DATASET", "confidence": 0.9258454442024231}]}, {"text": "Given a system's TimeML annotated documents, the process consists of three main steps: \u2022 ID Normalization: Entities are referenced by TimeML tag ids (e.g., eid23).", "labels": [], "entities": []}, {"text": "The yes/no questions must contain two entities with IDs (e.g., \"is event[eid23] after event[eid99] ?\").", "labels": [], "entities": []}, {"text": "The entities of the question are annotated in the corresponding key document.", "labels": [], "entities": []}, {"text": "However, systems may provide different ids to the same entities.", "labels": [], "entities": []}, {"text": "Therefore, we align the system annotation IDs with the question IDs that are annotated in the key docs using the TempEval-3 normalization tool 2 . \u2022 Timegraph Generation: The normalized TimeML docs are used to build a graph of time points representing the temporal relations of the events and timexes identified by each system.", "labels": [], "entities": []}, {"text": "Here we use Timegraph () for computing temporal closure as proposed by \u2022 Question Processing: Answering questions requires temporal information understanding and reasoning.", "labels": [], "entities": [{"text": "temporal closure", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7194962054491043}, {"text": "Question Processing", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.705747976899147}, {"text": "temporal information understanding", "start_pos": 123, "end_pos": 157, "type": "TASK", "confidence": 0.6863370140393575}]}, {"text": "Note that asking 'IS <entity1> <relation> <entity2>?' is not only asking if there is that explicit tlink between them, but also, if it is not, if that relation can be inferred from other tlinks implicitly.", "labels": [], "entities": []}, {"text": "Unlike corpus based evaluation, the system gets credit if its annotations provide the correct answer regardless of whether it annotates other irrelevant information or not.", "labels": [], "entities": []}, {"text": "In order to answer the questions about TimeML entities (based on time intervals) using Timegraph, we convert the queries to point-based queries.", "labels": [], "entities": [{"text": "Timegraph", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.957884669303894}]}, {"text": "For answering yes/no questions, we check the necessary point relations in Timegraph to verify an interval relation.", "labels": [], "entities": [{"text": "Timegraph", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.955579936504364}]}, {"text": "For example, to answer the question \"is event1 AFTER event2\", our system verifies whether start(event1) > end(event2); if it is verified then the answer is true (YES), if it conflicts with the Timegraph then it is false (NO), otherwise it is UNKNOWN.", "labels": [], "entities": [{"text": "YES", "start_pos": 162, "end_pos": 165, "type": "METRIC", "confidence": 0.996782660484314}, {"text": "NO", "start_pos": 221, "end_pos": 223, "type": "METRIC", "confidence": 0.9813308119773865}]}, {"text": "In QA TempEval, the creation of datasets does not require the manual annotation of all TimeML elements in source docs.", "labels": [], "entities": []}, {"text": "The annotation task in QA TempEval only requires reading the doc, making temporal questions, providing the correct answers, and identifying entities included in the questions by bounding them in the text and designating an ID.", "labels": [], "entities": [{"text": "QA TempEval", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.44851262867450714}]}, {"text": "The format of the question sets is as follows: Following is an example question and its corresponding annotated document: 3|APW.tml|IS ei21 AFTER ei19| Was he cited after becoming general?|yes APW.tml (KEY) Farkas <event eid=\"e19\">became</event> a general.", "labels": [], "entities": [{"text": "APW.tml", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.6390973329544067}, {"text": "IS ei21", "start_pos": 132, "end_pos": 139, "type": "METRIC", "confidence": 0.8570375740528107}, {"text": "AFTER", "start_pos": 140, "end_pos": 145, "type": "METRIC", "confidence": 0.5098894238471985}, {"text": "APW.tml (KEY) Farkas", "start_pos": 193, "end_pos": 213, "type": "DATASET", "confidence": 0.7380887389183044}]}, {"text": "He was <event eid=\"e21\">cited</event>...", "labels": [], "entities": []}, {"text": "APW.tml (system annotation, full-TimeML) Farkas <event eid=\"e15\"...>became</event> a general.", "labels": [], "entities": [{"text": "APW.tml", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9644942879676819}, {"text": "Farkas", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.9369210004806519}]}, {"text": "He was <event eid=\"e24\"...>cited</event>...", "labels": [], "entities": []}, {"text": "<tlink eventID=e15 relatedToEventID=e24 relType=before />  The objective of this evaluation is to measure and compare QA performance of TimeML annotations of participating and off-the-shelf systems.", "labels": [], "entities": [{"text": "relatedToEventID", "start_pos": 19, "end_pos": 35, "type": "METRIC", "confidence": 0.9183378219604492}]}, {"text": "Participants were given the documents of the previously defined test set (TE3-input format).", "labels": [], "entities": []}, {"text": "They were asked to annotate them with their systems within a 5-day period.", "labels": [], "entities": []}, {"text": "Organizers evaluated the submitted annotations using the test question-sets.", "labels": [], "entities": []}, {"text": "Result tables include Precision (P), Recall (R), F-measure (F1), percentage of the answered questions (awd%) and number of correct answers (corr).", "labels": [], "entities": [{"text": "Result", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9435750842094421}, {"text": "Precision (P)", "start_pos": 22, "end_pos": 35, "type": "METRIC", "confidence": 0.9513969570398331}, {"text": "Recall (R)", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9660804867744446}, {"text": "F-measure (F1)", "start_pos": 49, "end_pos": 63, "type": "METRIC", "confidence": 0.9612615406513214}, {"text": "awd", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9325332045555115}, {"text": "number of correct answers (corr)", "start_pos": 113, "end_pos": 145, "type": "METRIC", "confidence": 0.7375095401491437}]}, {"text": "As mentioned earlier, Recall is the main measure for ranking systems.", "labels": [], "entities": [{"text": "Recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9485061168670654}]}, {"text": "The percentage of the questions which are answered by the system provides a coverage metric, measuring a system's ability to provide more complete set of annotation on entities and relations.", "labels": [], "entities": []}, {"text": "The participant system hlt-fbk-ev2-trel2 system (.30 R) outperformed all the others by a significant margin.", "labels": [], "entities": []}, {"text": "CAEVO performed best among the off-theshelf systems, but behind the winning participant recall by 13% absolute.", "labels": [], "entities": [{"text": "CAEVO", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5096817016601562}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9922083020210266}]}, {"text": "The awd% of the hlt-fbk-ev2-trel2 system doubles the one by the best off-the-shelf system, CAEVO.", "labels": [], "entities": [{"text": "awd", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9812362194061279}, {"text": "CAEVO", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.7283474206924438}]}, {"text": "Interstingly, CAEVO and the two hlt-fbk trel1 systems performed approximately the same.", "labels": [], "entities": [{"text": "Interstingly", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9618508219718933}, {"text": "CAEVO", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.8248084783554077}]}, {"text": "The trel2 versions included event coreference.", "labels": [], "entities": [{"text": "event coreference", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7256824672222137}]}, {"text": "tem, hlt-fbk-ev2-trel2, maintained its top position.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: QA Results over all domains.", "labels": [], "entities": []}, {"text": " Table 3: QA Results broken down by genre, based on 99  News, 130 Wiki, and 65 Blog questions.", "labels": [], "entities": []}, {"text": " Table 4: QA Results augmented with TREFL", "labels": [], "entities": [{"text": "QA", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.5733787417411804}, {"text": "TREFL", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9948780536651611}]}]}