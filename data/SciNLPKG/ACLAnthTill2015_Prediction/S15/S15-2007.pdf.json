{"title": [{"text": "ROB: Using Semantic Meaning to Recognize Paraphrases", "labels": [], "entities": []}], "abstractContent": [{"text": "Paraphrase recognition is the task of identifying whether two pieces of natural language represent similar meanings.", "labels": [], "entities": [{"text": "Paraphrase recognition", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9399203062057495}]}, {"text": "This paper describes a system participating in the shared task 1 of SemEval 2015, which is about paraphrase detection and semantic similarity in twitter.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 97, "end_pos": 117, "type": "TASK", "confidence": 0.870855838060379}]}, {"text": "Our approach is to exploit semantically meaningful features to detect paraphrases.", "labels": [], "entities": []}, {"text": "An existing state-of-the-art model for predicting semantic similarity is adapted to this task.", "labels": [], "entities": [{"text": "predicting semantic similarity", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.896528939406077}]}, {"text": "A wide variety of features is used, ranging from different types of models, to lexical overlap and synset overlap.", "labels": [], "entities": []}, {"text": "A maximum entropy classifier is then trained on these features.", "labels": [], "entities": []}, {"text": "In addition to the detection of paraphrases, a similarity score is also predicted, using the probabilities of the classifier.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 47, "end_pos": 63, "type": "METRIC", "confidence": 0.9711953401565552}]}, {"text": "To improve the results, normalization is used as preprocessing step.", "labels": [], "entities": []}, {"text": "Our final system achieves a F1 score of 0.620 (10th out of 18 teams), and a Pearson correlation of 0.515 (6th out of 13 teams).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9899005591869354}, {"text": "Pearson correlation", "start_pos": 76, "end_pos": 95, "type": "METRIC", "confidence": 0.95793217420578}]}], "introductionContent": [{"text": "A good paraphrase detection system can be useful in many natural language processing tasks, like searching, translating or summarization.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.8660017549991608}, {"text": "translating", "start_pos": 108, "end_pos": 119, "type": "TASK", "confidence": 0.6477973461151123}, {"text": "summarization", "start_pos": 123, "end_pos": 136, "type": "TASK", "confidence": 0.7589263319969177}]}, {"text": "For clean texts, F1 scores as high as 0.84 have been reported on paraphrase detection (.", "labels": [], "entities": [{"text": "F1", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9995291233062744}, {"text": "paraphrase detection", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.9153854250907898}]}, {"text": "However, previous research focused almost solely on clean text.", "labels": [], "entities": []}, {"text": "Thanks to the Twitter Paraphrase Corpus (), this has now changed.", "labels": [], "entities": [{"text": "Twitter Paraphrase Corpus", "start_pos": 14, "end_pos": 39, "type": "DATASET", "confidence": 0.7710182468096415}]}, {"text": "Carrying out this task on noisy texts is anew challenge.", "labels": [], "entities": []}, {"text": "The abundant availability of social media data and the high redundancy that naturally exists in this data makes this task highly relevant.", "labels": [], "entities": []}, {"text": "Our approach is based on the model described by.", "labels": [], "entities": []}, {"text": "This model has proved to achieve state-of-the-art results at predicting semantic similarity.", "labels": [], "entities": [{"text": "predicting semantic similarity", "start_pos": 61, "end_pos": 91, "type": "TASK", "confidence": 0.8852594494819641}]}, {"text": "It is based on overlaps of semantically meaningful properties of sentences.", "labels": [], "entities": []}, {"text": "A random forest regression model) combines these features to predict a semantic similarity score.", "labels": [], "entities": []}, {"text": "We rely heavily on the assumption that semantically meaningful features can also be used to identify paraphrases.", "labels": [], "entities": []}, {"text": "The features of the existing system are also used in the new system.", "labels": [], "entities": []}, {"text": "However, the old system used a regression model, while the new task demands classbased output.", "labels": [], "entities": []}, {"text": "Hence, the machine learning model model is changed to a maximum entropy model.", "labels": [], "entities": []}], "datasetContent": [{"text": "This chapter is divided in the two sub tasks of paraphrase detection and similarity prediction.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.9545941054821014}, {"text": "similarity prediction", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.8241123855113983}]}, {"text": "A strong 2 www.aspell.net baseline is used, namely a state-of-the art model for clean text: a logistic regression model that uses simple lexical overlap features).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Absolute weights of the feature groups and fea- ture group ablation F1-Scores.", "labels": [], "entities": [{"text": "Absolute", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9962837100028992}, {"text": "fea- ture group ablation F1-Scores", "start_pos": 53, "end_pos": 87, "type": "METRIC", "confidence": 0.47212114930152893}]}]}