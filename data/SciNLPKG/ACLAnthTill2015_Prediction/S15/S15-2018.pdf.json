{"title": [{"text": "FBK-HLT: A New Framework for Semantic Textual Similarity", "labels": [], "entities": [{"text": "FBK-HLT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9437856078147888}, {"text": "Semantic Textual Similarity", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.5995545287926992}]}], "abstractContent": [{"text": "This paper reports the description and performance of our system, FBK-HLT, participating in the SemEval 2015, Task #2 \"Semantic Textual Similarity\", English subtask.", "labels": [], "entities": [{"text": "FBK-HLT", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9434447884559631}, {"text": "SemEval 2015, Task", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8777135759592056}]}, {"text": "We submitted three runs with different hypothesis in combining typical features (lexical similarity, string similarity, word n-grams, etc) with syntactic structure features, resulting in different sets of features.", "labels": [], "entities": []}, {"text": "The results evaluated on both STS 2014 and 2015 datasets prove our hypothesis of building a STS system taking into consideration of syntactic information.", "labels": [], "entities": [{"text": "STS 2014 and 2015 datasets", "start_pos": 30, "end_pos": 56, "type": "DATASET", "confidence": 0.7761984467506409}]}, {"text": "We out-perform the best system on STS 2014 datasets and achieve a very competitive result to the best system on STS 2015 datasets.", "labels": [], "entities": [{"text": "STS 2014 datasets", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.8822336594263712}, {"text": "STS 2015 datasets", "start_pos": 112, "end_pos": 129, "type": "DATASET", "confidence": 0.844474713007609}]}], "introductionContent": [{"text": "Semantic related tasks have been a noticed trend in Natural Language Processing (NLP) community.", "labels": [], "entities": []}, {"text": "Particularly, the task Semantic Textual Similarity (STS) has captured a huge attention in the NLP community despite being recently introduced since SemEval 2012).", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.7911053597927094}]}, {"text": "Basically, the task requires to build systems which can compute the similarity degree between two given sentences.", "labels": [], "entities": []}, {"text": "The similarity degree is scaled as areal score from 0 (no relevance) to 5 (semantic equivalence).", "labels": [], "entities": [{"text": "similarity degree", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.8943794369697571}]}, {"text": "The evaluation is done by computing the correlation between human judgment scores and system scores by the mean of Pearson correlation method.", "labels": [], "entities": [{"text": "mean of Pearson correlation", "start_pos": 107, "end_pos": 134, "type": "METRIC", "confidence": 0.750483363866806}]}, {"text": "At SemEval 2015, Task #2 \"Semantic Textual Similarity (STS)\", English STS subtask () evaluates participating systems on five test datasets: image description (image), news headlines (headlines), student answers paired with reference answers (answers-students), answers to questions posted in stach exchange forums (answers-forum), and English discussion forum data exhibiting commited belief (belief ).", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)\"", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.7118529131015142}]}, {"text": "As being inspired by the UKP system (, which was the best system in STS 2012, we build a supervised system on top of it.", "labels": [], "entities": [{"text": "UKP system", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.9641555547714233}]}, {"text": "Our system adopts some word and string similarity features in UKP, such as string similarity, character/word n-grams, and pairwise similarity; however, we also add other distinguished features, like syntactic structure information, word alignment and semantic word similarity.", "labels": [], "entities": [{"text": "UKP", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.9748247861862183}, {"text": "word alignment", "start_pos": 232, "end_pos": 246, "type": "TASK", "confidence": 0.7752887010574341}]}, {"text": "As a result, our team, FBK-HLT, submitted three runs and achieve very competitive results in the top-tier systems of the task.", "labels": [], "entities": [{"text": "FBK-HLT", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.9130024313926697}]}, {"text": "The remainder of this paper is organized as follows: Section 2 presents the System Description, Section 3 describes our Experiment Settings, Section 4 reports the Evaluations of our system.", "labels": [], "entities": []}, {"text": "Finally, Section 5 is Conclusions and Future Work.", "labels": [], "entities": []}], "datasetContent": [{"text": "with Explicit ORdering) () is an automatic metric for machine translation evaluation, which consists of two major components: a flexible monolingual word aligner and a scorer.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 54, "end_pos": 84, "type": "TASK", "confidence": 0.8868611852327982}]}, {"text": "For machine translation evaluation, hypothesis sentences are aligned to reference sentences.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.8720105489095052}]}, {"text": "Alignments are then scored to produce sentence and corpus level scores.", "labels": [], "entities": []}, {"text": "We use this word alignment feature to learn the similarity between words, phrases in two given texts in case of different orders.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.6923117339611053}]}, {"text": "We generate and select 25 optimal features, ranging from lexical level to string level and syntactic level.", "labels": [], "entities": []}, {"text": "We deploy the machine learning toolkit WEKA ( for learning a regression model (GaussianProcesses) to predict the similarity scores.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.6335036754608154}]}, {"text": "We build three models based on three sets of features to verify our hypothesis in which we augment that computing semantic similarity degree is not only about lexical similarity and string similarity, but also taking into consideration a deeper level at syntactic structure where more semantic information is embedded.", "labels": [], "entities": []}, {"text": "In the system development process, we train our system on the given datasets of STS 2012, 2013 and use the STS 2014 datasets for evaluating the system.", "labels": [], "entities": [{"text": "STS 2014 datasets", "start_pos": 107, "end_pos": 124, "type": "DATASET", "confidence": 0.7946969469388326}]}, {"text": "In, we also examine the contribution of different features to the overall accuracy of system, and prove that syntactic structure information also has some impact to the performance of our system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9988728165626526}]}, {"text": "Our model using all features described above outperform the best system DLS@CU in STS 2014 evaluation.", "labels": [], "entities": []}, {"text": "We submitted three runs with different sets of features as below: -Run1: All features described in Section 2 used.", "labels": [], "entities": []}, {"text": "-Run2: The feature obtained by Distributed Tree Kernel approach is excluded as sometimes it returns negative correlation.", "labels": [], "entities": [{"text": "Distributed Tree Kernel", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.6715985933939616}]}, {"text": "-Run3: No syntactic features are included.", "labels": [], "entities": [{"text": "Run3", "start_pos": 1, "end_pos": 5, "type": "DATASET", "confidence": 0.9301351308822632}]}, {"text": "In we report the performance of our three runs achieved on the STS 2015 test datasets.", "labels": [], "entities": [{"text": "STS 2015 test datasets", "start_pos": 63, "end_pos": 85, "type": "DATASET", "confidence": 0.9267573356628418}]}, {"text": "Among three submitted runs, Run1 has the best score, which confirm that exploiting the syntactic structure information benefits the overall performance of our system.", "labels": [], "entities": [{"text": "Run1", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.7335618734359741}]}, {"text": "Besides, although occasionally the features extracted by Distributed Tree Kernel approach returns negative result, it still contributes a small positive portion in the final result, which is shown in the Run2.", "labels": [], "entities": [{"text": "Run2", "start_pos": 204, "end_pos": 208, "type": "DATASET", "confidence": 0.9780116081237793}]}, {"text": "In contrast, the Run3 which excludes all syntactic structure features, eventually, returns 4% lower than the other two runs.", "labels": [], "entities": [{"text": "Run3", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.9417592883110046}]}, {"text": "In overall, our system achieves a very competitive result compared to the best ranked system, DLS@CU-S1.", "labels": [], "entities": [{"text": "CU-S1", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.6219016313552856}]}, {"text": "Specifically, the difference between our Run1 and the DLS@CU-S1 on each test dataset of STS 2015 varies slightly 1%-2%.", "labels": [], "entities": [{"text": "test dataset of STS 2015", "start_pos": 72, "end_pos": 96, "type": "DATASET", "confidence": 0.8515132427215576}]}, {"text": "However, this difference is not statistically significant, as we can understand that each system may perform slightly different on different evaluation datasets.", "labels": [], "entities": []}, {"text": "Generally, by taking into account the results of our system and DLS@CU on both STS 2014 and 2015 evaluation datasets, we can consider that we are almost equivalent in performance.", "labels": [], "entities": [{"text": "DLS@CU", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.6075920363267263}, {"text": "STS 2014 and 2015 evaluation datasets", "start_pos": 79, "end_pos": 116, "type": "DATASET", "confidence": 0.7745656569798788}]}], "tableCaptions": [{"text": " Table 1: Evaluation Results on STS 2014 datasets.", "labels": [], "entities": [{"text": "STS 2014 datasets", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.8083377679189047}]}, {"text": " Table 2: Evaluation Results on STS 2015 datasets.", "labels": [], "entities": [{"text": "STS 2015 datasets", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.8145106633504232}]}]}