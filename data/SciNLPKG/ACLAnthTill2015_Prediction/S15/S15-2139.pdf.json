{"title": [{"text": "HeidelToul: A Baseline Approach for Cross-document Event Ordering", "labels": [], "entities": [{"text": "Cross-document Event Ordering", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.6912498772144318}]}], "abstractContent": [{"text": "In this paper, we give an overview of our participation in the timeline generation task of SemEval-2015 (task 4, TimeLine: Cross-Document Event Ordering).", "labels": [], "entities": [{"text": "timeline generation task", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.7668199042479197}, {"text": "Cross-Document Event Ordering", "start_pos": 123, "end_pos": 152, "type": "TASK", "confidence": 0.6203457017739614}]}, {"text": "The main goals of this new track are, given a collection of news articles and a so-called target entity, to determine events that are relevant for the entity , to resolve event coreferences, and to order the events chronologically.", "labels": [], "entities": [{"text": "resolve event coreferences", "start_pos": 163, "end_pos": 189, "type": "TASK", "confidence": 0.7531093160311381}]}, {"text": "We addressed the sub-tasks, in which event mentions were provided , i.e., no additional event extraction was required.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.7135676443576813}]}, {"text": "For this, we developed an ad-hoc approach based on a temporal tagger and a coref-erence resolution tool for entities.", "labels": [], "entities": [{"text": "coref-erence resolution", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.6841012537479401}]}, {"text": "After determining relevant sentences, relevant events are extracted and anchored on a timeline.", "labels": [], "entities": []}, {"text": "The evaluation conducted on three collections of news articles shows that our approach-despite its simplicity-achieves reasonable results and opens several promising issues for future work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Due to the tremendous amount of documents being constantly published on the Internet, there is a need for more enhanced search facilities to retrieve relevant information.", "labels": [], "entities": []}, {"text": "Consider, for example, a user looking for information about the \"Golden Globe Awards\".", "labels": [], "entities": [{"text": "user looking for information about the \"Golden Globe Awards\"", "start_pos": 25, "end_pos": 85, "type": "TASK", "confidence": 0.6744048974730752}]}, {"text": "It might be possible that the user's information need is about the recent \"72nd edition\".", "labels": [], "entities": [{"text": "72nd edition\"", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.9091463486353556}]}, {"text": "However, it is also reasonable to assume that the user * The work was done during an internship at Heidelberg University.", "labels": [], "entities": []}, {"text": "would appreciate relevant information about previous editions.", "labels": [], "entities": []}, {"text": "Thus, presenting search results for time-and event-sensitive information needs in the form of a complete and updatable timeline would be a promising approach.", "labels": [], "entities": []}, {"text": "While this issue is tackled by some applications, early techniques required manual effort and recent approaches rely on heavily structured information such as Google's entity-related search results, which are based on Google's knowledge graph.", "labels": [], "entities": []}, {"text": "However, instead of listing only structured knowledge on a timeline, e.g., winners of the 71st Golden Globes in our example, search results would become much more valuable when adding temporally anchored event information extracted from text documents (e.g., recent updates about the event).", "labels": [], "entities": []}, {"text": "In the SemEval task 4, 1 the goal is to detect all events in a document collection that are relevant fora target entity, and to anchor these events on a timeline.", "labels": [], "entities": [{"text": "SemEval task 4", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.8990709185600281}]}, {"text": "Thus, events are to be sorted chronologically, and, if possible, specific dates are to be assigned to the events.", "labels": [], "entities": []}, {"text": "As in previous SemEval tasks addressing temporal relation extraction, namely in the TempEval series (see, e.g.,, the TimeML event definition is used.", "labels": [], "entities": [{"text": "temporal relation extraction", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.585780918598175}, {"text": "TimeML event definition", "start_pos": 117, "end_pos": 140, "type": "DATASET", "confidence": 0.8172955314318339}]}, {"text": "However, a special focus is now put on the cross-document aspect, i.e., on cross-document event coreference resolution and cross-document temporal relation extraction.", "labels": [], "entities": [{"text": "cross-document event coreference resolution", "start_pos": 75, "end_pos": 118, "type": "TASK", "confidence": 0.7569315880537033}, {"text": "cross-document temporal relation extraction", "start_pos": 123, "end_pos": 166, "type": "TASK", "confidence": 0.7289501428604126}]}, {"text": "While the document collection contains news articles, target entities can be persons, organizations, products, or financial entities.", "labels": [], "entities": []}, {"text": "The organizers offered the task in two tracks.", "labels": [], "entities": []}, {"text": "While the final goals of timeline construction are identical in both tracks, systems addressing track A had to extract event mentions, while event annotations were provided to participants of track B.", "labels": [], "entities": [{"text": "timeline construction", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.6894348412752151}]}, {"text": "Furthermore, both tracks were evaluated with and without assigning explicit temporal information to the events.", "labels": [], "entities": []}, {"text": "Since we participated in track B, the main challenges for our approach were to \u2022 filter events relevant for the target entities, \u2022 assign date information to relevant events, \u2022 determine cross-document event coreferences, \u2022 and to construct a timeline for each entity.", "labels": [], "entities": [{"text": "cross-document event coreferences", "start_pos": 187, "end_pos": 220, "type": "TASK", "confidence": 0.6220239897569021}]}, {"text": "In the following section, we describe our approach and give an example for cross-document event ordering.", "labels": [], "entities": [{"text": "cross-document event ordering", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.7066583931446075}]}, {"text": "In Section 3, we present and analyze the official evaluation results.", "labels": [], "entities": []}, {"text": "Finally, we discuss open issues for future research in the context of cross-document timeline construction.", "labels": [], "entities": [{"text": "cross-document timeline construction", "start_pos": 70, "end_pos": 106, "type": "TASK", "confidence": 0.7504270474116007}]}], "datasetContent": [{"text": "The evaluation data consists of 3 sets of 30 documents from Wikinews annotated with event men-827: Official results of participating groups in SemEval 2015 task 4. 2 NonTolMatchPrune: non tolerant matching and pruning setting; 3 TolMatchPrune: tolerant matching and pruning setting (cf. Section 2.1).", "labels": [], "entities": [{"text": "SemEval 2015 task 4.", "start_pos": 143, "end_pos": 163, "type": "TASK", "confidence": 0.7885179817676544}]}, {"text": "tions and a total of 38 target entities.", "labels": [], "entities": []}, {"text": "Our system ranked second among only two participating groups.", "labels": [], "entities": []}, {"text": "While there have been a total of four teams participating in the task, only two participated in (sub)track B.", "labels": [], "entities": []}, {"text": "Participants of (sub)track A additionally performed event extraction so that a comparison between results of all four participants is not possible.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7614234089851379}]}, {"text": "Thus, in, we only present the results of the two teams that addressed (sub)track B.", "labels": [], "entities": []}, {"text": "(left) reports the results by means of Micro-FSCORE obtained by our runs and that of the other participating group.", "labels": [], "entities": [{"text": "Micro-FSCORE", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.7337753772735596}]}, {"text": "As shown, our system is outperformed by the system \"GPLSIUA\" for both settings.", "labels": [], "entities": [{"text": "GPLSIUA", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.8757767081260681}]}, {"text": "The performance difference is most significant for corpus 2, especially within TrackB.", "labels": [], "entities": [{"text": "TrackB", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.9261681437492371}]}, {"text": "However, we notice that our tolerant setting gives better overall results than the non tolerant one.", "labels": [], "entities": []}, {"text": "These improvements are less significant for corpora 1 and 2 than for corpus 3.", "labels": [], "entities": []}, {"text": "To get a deep understanding of the results, we report in (right) the overall precision and recall values for our system configurations and that of the other participating group.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9995375871658325}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9981182813644409}]}, {"text": "Our non tolerant setting is slightly outperformed by the run \"GPLSIUA 1\" in terms of precision for trackB.", "labels": [], "entities": [{"text": "GPLSIUA 1", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.7161715626716614}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9993268251419067}]}, {"text": "However, it relatively enhances the other runs within the SubTrackB.", "labels": [], "entities": []}, {"text": "This can be explained by the important number of relevant retrieved events due to the high values of distances and thresholds used to prune the events.", "labels": [], "entities": []}, {"text": "In contrast, in terms of recall, our tolerant setting performs better than the non tolerant one in both subtracks.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9990940093994141}]}, {"text": "Actually, this is not surprising given that the filtering techniques are not strict.", "labels": [], "entities": []}, {"text": "Interestingly, an in-depth analysis of the nature of the target entities and the types of temporal expressions in the documents for which our system fails to provide good timeline, may help to improve the overall performance of our system in the future.", "labels": [], "entities": []}, {"text": "For instance, for the target entities \"Boeing 777\" and \"Airbus A380\" in corpus 1, we obtained the lowest values in terms of MicroFSCORE among all target entities.", "labels": [], "entities": [{"text": "MicroFSCORE", "start_pos": 124, "end_pos": 135, "type": "DATASET", "confidence": 0.7704428434371948}]}, {"text": "Clearly, this is due to the partial matching technique we used, which results in the extraction of many events related to other entities (e.g., \"Boeing 787\" instead of \"Boeing 777\"; cf. and.", "labels": [], "entities": []}, {"text": "Moreover, all events that do not cooccur with a temporal expression in the same sentence are anchored at the document creation time by our system.", "labels": [], "entities": []}, {"text": "This hurts the performance of our system in particular for TrackB, because many of those events are placed at rank 0 in the gold standard.", "labels": [], "entities": [{"text": "TrackB", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.7059134244918823}]}], "tableCaptions": [{"text": " Table 1: Timeline excerpt returned for Boeing 777.  Events are either relevant (1) or not (0).", "labels": [], "entities": [{"text": "Timeline", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9653556942939758}]}, {"text": " Table 2: Official results of participating groups in SemEval 2015 task 4. 2 NonTolMatchPrune: non tolerant  matching and pruning setting; 3 TolMatchPrune: tolerant matching and pruning setting (cf. Section 2.1).", "labels": [], "entities": [{"text": "SemEval 2015 task 4.", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.752174511551857}]}]}