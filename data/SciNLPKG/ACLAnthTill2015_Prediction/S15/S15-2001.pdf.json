{"title": [{"text": "SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT)", "labels": [], "entities": [{"text": "Paraphrase and Semantic Similarity in Twitter (PIT)", "start_pos": 21, "end_pos": 72, "type": "TASK", "confidence": 0.700764450762007}]}], "abstractContent": [{"text": "In this shared task, we present evaluations on two related tasks Paraphrase Identification (PI) and Semantic Textual Similarity (SS) systems for the Twitter data.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (SS)", "start_pos": 100, "end_pos": 132, "type": "TASK", "confidence": 0.6937177280584971}]}, {"text": "Given a pair of sentences, participants are asked to produce a binary yes/no judgement or a graded score to measure their semantic equivalence.", "labels": [], "entities": []}, {"text": "The task features a newly constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs.", "labels": [], "entities": []}, {"text": "A total of 19 teams participated, submitting 36 runs to the PI task and 26 runs to the SS task.", "labels": [], "entities": [{"text": "SS task", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.7641156315803528}]}, {"text": "The evaluation shows encouraging results and open challenges for future research.", "labels": [], "entities": []}, {"text": "The best systems scored a F1-measure of 0.674 for the PI task and a Pearson correlation of 0.619 for the SS task respectively, comparing to a strong baseline using logistic regression model of 0.589 F1 and 0.511 Pearson; while the best SS systems can often reach >0.80 Pearson on well-formed text.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9994255304336548}, {"text": "Pearson correlation", "start_pos": 68, "end_pos": 87, "type": "METRIC", "confidence": 0.9894292950630188}, {"text": "F1", "start_pos": 199, "end_pos": 201, "type": "METRIC", "confidence": 0.9881624579429626}, {"text": "Pearson", "start_pos": 269, "end_pos": 276, "type": "METRIC", "confidence": 0.9896015524864197}]}, {"text": "This shared task also provides insights into the relation between the PI and SS tasks and suggests the importance to bringing these two research areas together.", "labels": [], "entities": []}, {"text": "We make all the data, baseline systems and evaluation scripts publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ability to identify paraphrases, i.e. alternative expressions of the same (or similar) meaning, and the degree of their semantic similarity has proven useful fora wide variety of natural language processing applications  The SemEval-2015 shared task on Paraphrase and Semantic Similarity In Twitter (PIT) uses a training and development set of 17,790 sentence pairs and a test set of 972 sentence pairs with paraphrase annotations (see examples in) that is the same as the Twitter Paraphrase Corpus we developed earlier in and ().", "labels": [], "entities": [{"text": "Paraphrase and Semantic Similarity In Twitter (PIT)", "start_pos": 257, "end_pos": 308, "type": "TASK", "confidence": 0.7933802273538377}]}, {"text": "This PIT-2015 paraphrase dataset is distinct from the data used in previous studies in many aspects: (i) it contains sentences that are opinionated and colloquial, representing realistic informal language usage; (ii) it contains paraphrases that are lexically diverse; and (iii) it contains sentences that are lexically similar but semantically dissimilar.", "labels": [], "entities": [{"text": "PIT-2015 paraphrase dataset", "start_pos": 5, "end_pos": 32, "type": "DATASET", "confidence": 0.7854042053222656}]}, {"text": "It raises many interesting research questions and could lead to a better understanding of our daily used language and how semantics can be captured in such language.", "labels": [], "entities": []}, {"text": "We believe that such a common testbed will facilitate docking of the different approaches for purposes of comparison, lead to a better understanding of how semantics are conveyed in natural language, and help advance other NLP techniques for noisy user-generated text in the long run.", "labels": [], "entities": []}], "datasetContent": [{"text": "The task has two sentence-level sub-tasks: a paraphrase identification task and an optional semantic textual similarity task.", "labels": [], "entities": [{"text": "paraphrase identification task", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.8117406765619913}]}, {"text": "The two sub-tasks share the same data but differ in annotation and evaluation.", "labels": [], "entities": []}, {"text": "The results are very exciting that most systems outperformed the two strong baselines we chose, while still showing room for improvement towards the human upper-bound estimated by the crowdsourcing worker's performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Representative examples from PIT-2015 Twitter Paraphrase Corpus", "labels": [], "entities": [{"text": "PIT-2015 Twitter Paraphrase Corpus", "start_pos": 39, "end_pos": 73, "type": "DATASET", "confidence": 0.8627376109361649}]}, {"text": " Table 2: Statistics of PIT-2015 Twitter Paraphrase Corpus. Debatable cases are those received a medium-score from  annotators. The percentage of paraphrases is lower in the test set because it was constructed without topic selection.", "labels": [], "entities": [{"text": "PIT-2015 Twitter Paraphrase Corpus", "start_pos": 24, "end_pos": 58, "type": "DATASET", "confidence": 0.775372177362442}]}, {"text": " Table 3: Evaluation results. The first column presents the rank of each team in the two tasks based on each team's best  system. The superscripts are the ranks of systems, ordered by F1 for Paraphrase Identification (PI) task and Pearson  for Semantic Similarity (SS) task. indicates unsupervised or semi-supervised system. In total, 19 teams participated  in the PI task, of which 14 teams also participated in the SS task. Note that although the two sub-tasks share the same  test set of 972 sentence pairs, the PI task ignores 134 debatable cases (received a medium-score from expert annotator)  and uses only 838 pairs (663 paraphrases and 175 non-paraphrases) in evaluation, while SS task uses all 972 pairs.  This causes that the F1-score in the PI task can be higher than the maximum F1-score in the SS task. Also note that  the F1-scores of the baselines in the PI task are higher than reported in the", "labels": [], "entities": [{"text": "F1", "start_pos": 184, "end_pos": 186, "type": "METRIC", "confidence": 0.9558145403862}, {"text": "Pearson", "start_pos": 231, "end_pos": 238, "type": "METRIC", "confidence": 0.9558765292167664}, {"text": "F1-score", "start_pos": 737, "end_pos": 745, "type": "METRIC", "confidence": 0.9903406500816345}, {"text": "F1-scores", "start_pos": 837, "end_pos": 846, "type": "METRIC", "confidence": 0.9680088758468628}]}]}