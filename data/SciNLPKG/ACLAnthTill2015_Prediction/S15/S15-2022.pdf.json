{"title": [{"text": "UQeResearch: Semantic Textual Similarity Quantification", "labels": [], "entities": [{"text": "UQeResearch", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.7735756635665894}, {"text": "Semantic Textual Similarity Quantification", "start_pos": 13, "end_pos": 55, "type": "TASK", "confidence": 0.6791026517748833}]}], "abstractContent": [{"text": "This paper presents an approach for estimating the Semantic Textual Similarity of full English sentences as specified in Shared Task 2 of SemEval-2015.", "labels": [], "entities": [{"text": "Semantic Textual Similarity of full English sentences", "start_pos": 51, "end_pos": 104, "type": "TASK", "confidence": 0.7323369085788727}]}, {"text": "The semantic similarity of sentence pairs is quantified from three perspectives structural , syntactical, and semantic.", "labels": [], "entities": []}, {"text": "The numerical representations of the derived similarity measures are then applied to train a regression ensemble.", "labels": [], "entities": []}, {"text": "Although none of these three sets of measures is able to represent the semantic similarity of two sentences individually, our experimental results show that the combination of these features can precisely assess the semantic similarity of the sentences.", "labels": [], "entities": []}, {"text": "In the English subtask our system's best result ranked 35 among 73 system runs with 0.7189 average Pearson correlation over five test sets.", "labels": [], "entities": [{"text": "English subtask", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.8994627296924591}, {"text": "Pearson correlation", "start_pos": 99, "end_pos": 118, "type": "METRIC", "confidence": 0.9581378102302551}]}, {"text": "This was 0.08 correlation points less than the best submitted run.", "labels": [], "entities": [{"text": "correlation", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9847853183746338}]}], "introductionContent": [{"text": "Semantic textual similarity (STS) aims to automatically estimate the relatedness of the meaning of sentences ().", "labels": [], "entities": [{"text": "Semantic textual similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8101227780183157}]}, {"text": "The literature consists of a series of well-established frameworks to explore a deeper understanding of the semantic relationship between entities, ranging from ontological reasoning to compositional as well as distributional semantics).", "labels": [], "entities": []}, {"text": "However, automatically estimating the semantic similarity of full sentences is still a challenging task.", "labels": [], "entities": [{"text": "estimating the semantic similarity of full sentences", "start_pos": 23, "end_pos": 75, "type": "TASK", "confidence": 0.8021765691893441}]}, {"text": "Our system aims to quantify the similarity of pairs of sentences by encoding a variety of relatedness features in a vector of attributes and then predicting their similarity scores by employing machine-learning algorithms.", "labels": [], "entities": []}, {"text": "Different syntactic, semantic, and structural similarity measures have been applied to quantify the similarity of texts.", "labels": [], "entities": []}, {"text": "We have chosen to approach the estimation of similarity as a regression problem.", "labels": [], "entities": [{"text": "estimation of similarity", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.735970159371694}]}, {"text": "Hence, we use the quantified similarity of sentence pairs to train a regressor that can then be applied to predict similarity scores for the unseen pairs.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: Section 2 presents the proposed similarity measures.", "labels": [], "entities": [{"text": "similarity", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9493253231048584}]}, {"text": "In Section 3, the regression models are introduced and the experimental results are discussed in detail.", "labels": [], "entities": []}, {"text": "The conclusions are summarized in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "All the data released in was permitted to be used to develop and train the systems.", "labels": [], "entities": []}, {"text": "All the data sets consist of pairs of sentences along with their human annotated similarity scores.", "labels": [], "entities": []}, {"text": "The similarity scores ranged from 0 to 5, with 0 representing completely dissimilar pairs and 5 representing perfect similarity (or equality).", "labels": [], "entities": [{"text": "similarity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9800834655761719}, {"text": "perfect similarity (or equality)", "start_pos": 109, "end_pos": 141, "type": "METRIC", "confidence": 0.7443191558122635}]}, {"text": "In order to evaluate the English STS systems, five test sets were provided.", "labels": [], "entities": [{"text": "English STS", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.5836226344108582}]}, {"text": "Although the test data in total consists of 8500 pairs, a subset of the instances of each test set was sampled and used for the final official evaluations by the organizers.", "labels": [], "entities": []}, {"text": "The official measurement criterion for evaluation is the Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 57, "end_pos": 76, "type": "METRIC", "confidence": 0.9330106675624847}]}, {"text": "It should be mentioned that prior to computing the measures the punctuations were removed from sentences to avoid na\u00efve token-level matching of them in some similarity measures.", "labels": [], "entities": []}, {"text": "We first performed a number of experiments over the training data in order to prepare the final regression system.", "labels": [], "entities": []}, {"text": "The training set consists of 10592 annotated pairs, achieved by merging previous SemEval STS data sets.", "labels": [], "entities": [{"text": "SemEval STS data sets", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.7786557823419571}]}, {"text": "We approached the semantic similarity estimation as a regression problem.", "labels": [], "entities": [{"text": "semantic similarity estimation", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.7372575203577677}]}, {"text": "Hence, we investigated different regression algorithms and lists their evaluation results.", "labels": [], "entities": []}, {"text": "The WEKA implementations of these algorithms have been used in our system (  The first part of shows the results achieved by selected regression approaches.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.6699565649032593}]}, {"text": "Among these algorithms, K* achieved the best Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 45, "end_pos": 64, "type": "METRIC", "confidence": 0.8070182204246521}]}, {"text": "In regression by classification, the continuous similarity scores are discretised to nominal values.", "labels": [], "entities": [{"text": "continuous similarity scores", "start_pos": 37, "end_pos": 65, "type": "METRIC", "confidence": 0.7198842763900757}]}, {"text": "Then, a classifier was used to categorize instances into the resultant nominal classes.", "labels": [], "entities": []}, {"text": "In our experiments, the continuous range of 0 to 5 scores is discretised into 10 bins.", "labels": [], "entities": []}, {"text": "The best results have been achieved by applying Random Forest as the base classifier.", "labels": [], "entities": []}, {"text": "Finally, the ensemble of regressors is composed of three meta-regressors: bagging, random SubSpace, and regression by discretisation.", "labels": [], "entities": []}, {"text": "Regression by discretisation follows precisely the same methodology as above.", "labels": [], "entities": [{"text": "Regression", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.8549222946166992}]}, {"text": "The bagging strategy uses RepTree as its first level regressor, while the random SubSpace employs the K* algorithm.", "labels": [], "entities": []}, {"text": "The final outputs of the ensemble are the average of the prediction values from all of the regressors.", "labels": [], "entities": []}, {"text": "This ensemble gained the best correlation amongst all of the models.", "labels": [], "entities": [{"text": "correlation", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.9676709771156311}]}], "tableCaptions": [{"text": " Table 1: Experiments on training data (5-fold cross  validation).", "labels": [], "entities": []}, {"text": " Table 2: Our systems' results over test sets.", "labels": [], "entities": []}]}