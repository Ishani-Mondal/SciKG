{"title": [{"text": "Dependency-Based Semantic Role Labeling using Convolutional Neural Networks", "labels": [], "entities": [{"text": "Dependency-Based Semantic Role Labeling", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6830150038003922}]}], "abstractContent": [{"text": "We describe a semantic role labeler with state-of-the-art performance and low computational requirements, which uses convolutional and time-domain neural networks.", "labels": [], "entities": [{"text": "semantic role labeler", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.6145394941171011}]}, {"text": "The system is designed to work with features derived from a dependency parser output.", "labels": [], "entities": []}, {"text": "Various system options and architectural details are discussed.", "labels": [], "entities": []}, {"text": "Incremental experiments were run to explore the benefits of adding increasingly more complex dependency-based features to the system; results are presented for both in-domain and out-of-domain datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic role labeling (Gildea and Jurafsky), the task of identifying and classifying the semantic arguments of verbal and nominal predicates in text, represents one of the most complex NLP tasks to be addressed by supervised machine learning techniques.", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7281012932459513}, {"text": "identifying and classifying the semantic arguments of verbal and nominal predicates in text", "start_pos": 58, "end_pos": 149, "type": "TASK", "confidence": 0.7185796201229095}]}, {"text": "In the standard supervised approach to building SRL systems, collections of multiway classifiers are trained using annotated corpora such as PropBank (Palmer et al.).", "labels": [], "entities": [{"text": "SRL", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9686201810836792}, {"text": "PropBank", "start_pos": 141, "end_pos": 149, "type": "DATASET", "confidence": 0.9451366662979126}]}, {"text": "In this approach, classifiers are trained using features derived directly from the original source text, as well as from subsequent syntactic and semantic processing.", "labels": [], "entities": []}, {"text": "As reported in several shared tasks,Carreras and M` arquez,Haji\u010d et al.), SRL systems trained in this manner can achieve high performance.", "labels": [], "entities": [{"text": "SRL", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9732372164726257}]}, {"text": "State-of-the-art systems employ classifiers such as support vector machines trained with large numbers of relatively complex combinations of features, often combined with re-ranking based on multiple syntactic analyses.", "labels": [], "entities": []}, {"text": "Unfortunately, these approaches have a number of nontrivial limitations including the computational cost of the syntactic parsing and the sparse nature of the complex features on which they rely.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.689543604850769}]}, {"text": "This latter limitation is particularly critical since it leads to significant degradation in performance when the trained system is applied to texts from new domains.", "labels": [], "entities": []}, {"text": "However, recent results using multilayer neural networks and pre-trained word embeddings have demonstrated high performance using a much smaller number of minimalist features.", "labels": [], "entities": []}, {"text": "The architecture described by Collobert et al. combines time delay convolutional neural networks () and pre-trained word embeddings fora number of NLP tasks.", "labels": [], "entities": []}, {"text": "They develop four components and compare their performance to previous benchmarks, one of which is an SRL system which uses features derived from a phrase-structure parse as input, based on the CoNLL 2005 shared task (Carreras and M` arquez).", "labels": [], "entities": [{"text": "CoNLL 2005 shared task", "start_pos": 194, "end_pos": 216, "type": "DATASET", "confidence": 0.8304367661476135}]}, {"text": "The work described here adopts the basic architecture from Collobert et al. and explores issues related to the use of this architecture in the context of the CoNLL 2009 shared task.", "labels": [], "entities": [{"text": "CoNLL 2009 shared task", "start_pos": 158, "end_pos": 180, "type": "DATASET", "confidence": 0.8663340210914612}]}, {"text": "In particular, we present Daisy, a system that (1) employs features derived from dependency parse as input, (2) assigns semantic roles to both verbal and nominal predicates, and (3) automatically assigns word senses to the predicates as described in the CoNLL 2009 shared task (Haji\u010d et al.).", "labels": [], "entities": [{"text": "CoNLL 2009 shared task", "start_pos": 254, "end_pos": 276, "type": "DATASET", "confidence": 0.8009944409132004}]}, {"text": "The following sections will describe the architecture of the Daisy system, present state-of-the-art performance on the plore the utility of features derived from dependency parses, including aversion of the traditional SRL syntactic path feature.", "labels": [], "entities": []}], "datasetContent": [{"text": "The CoNLL 2009 shared task consists of identifying the sense and semantic arguments for each given argument-bearing token (predicate).", "labels": [], "entities": [{"text": "CoNLL 2009 shared task", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8523427993059158}]}, {"text": "In addition to the words themselves, the training data provides the part of speech, syntactic head, and syntactic dependency relation to the head for each word in the sentence.", "labels": [], "entities": []}, {"text": "shows an example sentence and its representation in the dataset.", "labels": [], "entities": []}, {"text": "The PDEPREL and PHEAD features are the headword and dependency relation predicted automatically by a dependency parser.", "labels": [], "entities": [{"text": "PHEAD", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.8710368275642395}]}, {"text": "In the example sentence, there are two predicates identified for labeling: announce, and close.", "labels": [], "entities": []}, {"text": "The system should output two arguments for announce: results:A1 (Object), and after:AM-TEMP (Temporal Marker).", "labels": [], "entities": [{"text": "A1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9717230200767517}, {"text": "AM-TEMP", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.826170027256012}]}, {"text": "Similarly, market:A1 should be output for the predicate close.", "labels": [], "entities": [{"text": "A1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.7520857453346252}]}, {"text": "In addition to role identification, the word sense for each predicate is output, in the example, the expected sense for announce is 01, and for close is 02.", "labels": [], "entities": [{"text": "role identification", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8566829860210419}]}, {"text": "The training, validation, and evaluation datasets are annotated sentences from the Wall Street Journal.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 83, "end_pos": 102, "type": "DATASET", "confidence": 0.9686081409454346}]}, {"text": "An additional out of domain dataset mostly from the Brown corpus was also supplied.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.982819676399231}]}, {"text": "A comprehensive F1 score was generated for both role labels and sense predictions using the provided eval09.pl perl script.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9702848792076111}]}, {"text": "Feature abbreviations used in the descriptions are shown in Labeled paths that occur at least five times in the training data.", "labels": [], "entities": []}, {"text": "Incremental experiments were run to explore the benefits of adding increasingly more complex dependency-based features to the system.", "labels": [], "entities": []}, {"text": "We began with a basic configuration of only words (randomly initialized) and capitalization (W,C), Following this, a simple per-token part of speech was added (W,C,P).", "labels": [], "entities": []}, {"text": "Information from the dependency parser is then added in two steps, first the headword part of speech and dependency relation (W,C,P,HP,DR), and next the generic path (W,C,P,HP,DR,GP).", "labels": [], "entities": []}, {"text": "The word representations were then seeded with the pre-trained embeddings de- scribed in section 3.1.2 (TW,C,P,HP,DR,GP).", "labels": [], "entities": []}, {"text": "Finally, the labeled path was used instead of the generic path, still seeding the words with pre-trained embeddings (TW,C,P,HP,DR,LP5).", "labels": [], "entities": []}, {"text": "For each system configuration, 12 role subsystems and 8 sense subsystems were trained and tested, using the WSJ development F1 score during training to determine the best model parameter state.", "labels": [], "entities": [{"text": "WSJ development F1 score", "start_pos": 108, "end_pos": 132, "type": "METRIC", "confidence": 0.7380067855119705}]}, {"text": "After model generation, the WSJ development scores for different systems don't correlate well with the WSJ eval or Brown scores.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.8479524850845337}, {"text": "Brown", "start_pos": 115, "end_pos": 120, "type": "METRIC", "confidence": 0.8097382187843323}]}, {"text": "For example, models with high development scores don't necessarily correspond to best scoring models for the WSJ or Brown data tests.", "labels": [], "entities": [{"text": "WSJ or Brown data tests", "start_pos": 109, "end_pos": 132, "type": "DATASET", "confidence": 0.8285090923309326}]}, {"text": "The CoNLL2009 results used as benchmarks were given as single data points so statistics are not available.", "labels": [], "entities": [{"text": "CoNLL2009", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.9063479900360107}]}, {"text": "shows the relationship between the development and Evaluation F1 scores, as well as the general performance improvement as features were added.", "labels": [], "entities": [{"text": "Evaluation F1 scores", "start_pos": 51, "end_pos": 71, "type": "METRIC", "confidence": 0.8169535795847574}]}, {"text": "show the statistical performance of the system with WSJ and Brown test data.", "labels": [], "entities": [{"text": "WSJ and Brown test data", "start_pos": 52, "end_pos": 75, "type": "DATASET", "confidence": 0.7704720795154572}]}, {"text": "For the WSJ (evaluation) dataset, the role subsystem F1 improves much more dramatically than the sense subsystem as POS (+1.52%) and dependency parser information (+1.68%) is added.", "labels": [], "entities": [{"text": "WSJ (evaluation) dataset", "start_pos": 8, "end_pos": 32, "type": "DATASET", "confidence": 0.5971362829208374}, {"text": "F1", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9595038294792175}, {"text": "POS", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9318739175796509}]}, {"text": "The mean System F1 score is -0.25% under the benchmark without the pre-trained word embeddings.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.8755138516426086}]}, {"text": "Adding the embeddings boosts performance such that even the lowest scoring systems beat the benchmark, and the mean F1 score is about 0.41% higher.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9885076880455017}]}, {"text": "For the Brown (OOD) dataset, the role subsystem F1 improves significantly with POS and dependency parse information (+2.72%) while the sense subsystem benefits less (0.96%).", "labels": [], "entities": [{"text": "Brown (OOD) dataset", "start_pos": 8, "end_pos": 27, "type": "DATASET", "confidence": 0.8702647089958191}, {"text": "F1", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.9650121331214905}, {"text": "POS", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9640265107154846}, {"text": "dependency parse", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.6221642643213272}]}, {"text": "The role subsystem dramatically improves when pre-trained words are added (2.59%), due in large part to a better ability to handle unseen words.", "labels": [], "entities": []}, {"text": "The mean System F1 scores are higher than the benchmark as soon as dependency parser information is supplied, and the F1 is significantly better for the fully populated system (+2.59%).", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.8831159770488739}, {"text": "F1", "start_pos": 118, "end_pos": 120, "type": "METRIC", "confidence": 0.9993401169776917}]}], "tableCaptions": [{"text": " Table 1: CoNLL format SRL Dependency Parse Input Test Sentence Example", "labels": [], "entities": [{"text": "SRL Dependency Parse Input Test Sentence", "start_pos": 23, "end_pos": 63, "type": "TASK", "confidence": 0.5180217623710632}]}, {"text": " Table 2: SRL Dependency Parse Test F1", "labels": [], "entities": [{"text": "SRL Dependency Parse Test", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.6249244287610054}]}, {"text": " Table 4: Performance on WSJ Eval Dataset for Various System Configurations", "labels": [], "entities": [{"text": "WSJ Eval Dataset", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.8326868414878845}]}, {"text": " Table 5: Performance on Brown Dataset (OOD) for Various System Configurations", "labels": [], "entities": [{"text": "Brown Dataset (OOD)", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.7875379621982574}]}]}