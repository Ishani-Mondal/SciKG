{"title": [{"text": "IITPSemEval: Sentiment Discovery from 140 Characters", "labels": [], "entities": [{"text": "IITPSemEval", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8126120567321777}, {"text": "Sentiment Discovery from 140 Characters", "start_pos": 13, "end_pos": 52, "type": "TASK", "confidence": 0.8969222187995911}]}], "abstractContent": [{"text": "This paper presents an overview of the system developed and submitted as apart of our participation to the SemEval-2015 Task 10 that deals with Sentiment Analysis in Twitter.", "labels": [], "entities": [{"text": "SemEval-2015 Task 10", "start_pos": 107, "end_pos": 127, "type": "TASK", "confidence": 0.8101279338200887}, {"text": "Sentiment Analysis in Twitter", "start_pos": 144, "end_pos": 173, "type": "TASK", "confidence": 0.9056851267814636}]}, {"text": "We build a Support Vector Machine (SVM) based supervised learning model for Subtask A (term level task) and Subtask B (message level task).", "labels": [], "entities": []}, {"text": "We also participate in Subtask E viz., determining degree of polarity, and build a very simple system by employing the available lexical resources.", "labels": [], "entities": []}, {"text": "Experiments with the 2015 official datasets show F1 scores of 81.31% and 58.80% for Task A and Task B, respectively.", "labels": [], "entities": [{"text": "2015 official datasets", "start_pos": 21, "end_pos": 43, "type": "DATASET", "confidence": 0.7528765996297201}, {"text": "F1 scores", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9759284257888794}]}, {"text": "For Subtask E, our model achieves a score of 0.413 on Kendal's Tau metric.", "labels": [], "entities": []}], "introductionContent": [{"text": "The use of social media platforms has become central to many teenager's and adult's lives.", "labels": [], "entities": []}, {"text": "With the emerging forms of communication, much of the freely available texts in the opinionated texts are linguistically unstructured.", "labels": [], "entities": []}, {"text": "People have adopted creative spellings and abbreviations, and are excessively using more intelligent forms of messages that involves typos, hash-tags and emoticons to convey their messages.", "labels": [], "entities": []}, {"text": "The huge abundance of inexpensive data, rich in applications, can prove handy for public and corporate institutions.", "labels": [], "entities": []}, {"text": "This has urged the scientific community to extract the substantive information from these texts.", "labels": [], "entities": []}, {"text": "The proliferation of microblogging sites like Twitter which boasts of user's comments on everything trending in real time opens up an unprecedented opportunity to explore and develop techniques to mine the information.", "labels": [], "entities": []}, {"text": "Task 10 in Semantic Evaluation 2015 provides a research platform promoting the knowledge discovery in Twitter.", "labels": [], "entities": [{"text": "Semantic Evaluation 2015", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.8593522707621256}]}, {"text": "Task 10 consists of five different subtasks: Contextual Polarity Disambiguation (A), Message Polarity Classification (B), TopicBased Message Polarity Classification (C), Detecting Trends Towards a Topic (D) and Determining degree of polarity of Twitter terms with the sentiment (E).", "labels": [], "entities": [{"text": "Message Polarity Classification", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.6026623944441477}]}, {"text": "Complete details of the task can be found at (.", "labels": [], "entities": []}, {"text": "We participated in Subtasks A, B and E, the first two of which require the sentiments to be classified into positive, negative and neutral classes fora given segment of the tweet (for A) or the entire message (for B), while the Task E needs to compute the strength of association of the given terms to the sentiment on a scale of 0 to 1 with 1 denoting the maximum strength.", "labels": [], "entities": []}, {"text": "The technical study of public sentiment has been a subject of trending research and a significant amount of extensive work is being carried out in the domain.", "labels": [], "entities": [{"text": "technical study of public sentiment", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.8091897130012512}]}, {"text": "Sentiment Analysis has been handled at the various levels of granularity.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9524547159671783}]}, {"text": "Early research works () focussed on the document level classification with further studies at message and term level ().", "labels": [], "entities": [{"text": "document level classification", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.6486299932003021}]}, {"text": "Twitter has also been investigated for its possible applications in the fields of commerce (), elections (O', disaster management () etc.", "labels": [], "entities": [{"text": "disaster management", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7073057144880295}]}, {"text": "using varied approaches and different experimental setups.", "labels": [], "entities": []}, {"text": "Semantic Evaluation tasks () continue to pitch in with the newer systems for the sentiment classification of tweets.", "labels": [], "entities": [{"text": "Semantic Evaluation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8371290564537048}, {"text": "sentiment classification of tweets", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.9320538640022278}]}], "datasetContent": [{"text": "To train and tune our system, we use the training and development datasets that were employed for Task 2 in SemEval 2013, respectively.", "labels": [], "entities": [{"text": "SemEval 2013", "start_pos": 108, "end_pos": 120, "type": "TASK", "confidence": 0.6906534731388092}]}, {"text": "The metric used for evaluating the system is average F1-score (averaged F1-positive and averaged F1-negative, and ignoring the F1-neutral) for 2015 test set, while the ranking for progress set is done on the F1 score of the Twitter 2014 subset.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9973617196083069}, {"text": "F1-positive", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9322031140327454}, {"text": "F1-negative", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.942202091217041}, {"text": "F1-neutral", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.8777849674224854}, {"text": "F1 score", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.9728175401687622}]}, {"text": "For Task E, the trial dataset comprise of 200 unique words/phrases with the corresponding scores denoting the strength of the terms with positive or negative sentiment.", "labels": [], "entities": []}, {"text": "The test set contains 1,315 words/phrases which has to be scored in between 0 to 1 indicating their association with the positive or negative sentiment.", "labels": [], "entities": []}, {"text": "We observe that proportion of neutral tweets in the training set of Task A is quite less (4.88%).", "labels": [], "entities": []}, {"text": "In order to create a balanced dataset, we perform oversampling to increase the number of neutral tweets in the training data.", "labels": [], "entities": []}, {"text": "Experiments are carried outwith various oversampling rates.", "labels": [], "entities": []}, {"text": "Based on the evaluation on the development data, we observe that oversampling the neutral tweets by increasing its number twice lead to better scores while constructing the dataset with thrice the number of neutral tweets results in over-fitting, and hence, lowers the F1-score value.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 269, "end_pos": 277, "type": "METRIC", "confidence": 0.9995155334472656}]}, {"text": "For the second task, we also perform this oversampling technique for the better representations of negative tweet instances.", "labels": [], "entities": []}, {"text": "However we notice a reduction in the overall F1-score compared to the performance that we achieved with our original: Experimental results for feature-ablation experiment for Task A and B.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9994773268699646}]}, {"text": "The values in the parenthesis denotes the deviation from the score when all the features were taken into consideration. setup.", "labels": [], "entities": [{"text": "setup", "start_pos": 120, "end_pos": 125, "type": "METRIC", "confidence": 0.84162437915802}]}, {"text": "For subtask A, our system achieves a F1-score of 81.31% for 2015 test set and 82.73% for Twitter 2014 subset of progress set.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9996583461761475}, {"text": "Twitter 2014 subset of progress set", "start_pos": 89, "end_pos": 124, "type": "DATASET", "confidence": 0.7892047564188639}]}, {"text": "For the message level task, i.e. for subtask B, our system obtains the F1-scores of 58.80% for the 2015 test set and 65.09% for the progress test set.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9991708993911743}, {"text": "2015 test set", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.7412289877732595}]}, {"text": "The best ranked team for the term level task shows the F1-score of 84.79% for the 2015 test set and 87.12% for the progress test.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9997714161872864}, {"text": "progress", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9663441181182861}]}, {"text": "For Subtask B, the best performing system produces the F1-scores of 64.84% for the 2015 test set and 74.42% for the progress set.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.999593198299408}]}, {"text": "For Task E, we have to provide a score between 0 and 1 fora word or phrase denoting the associativity of the phrase with the positive sentiment.", "labels": [], "entities": []}, {"text": "The evaluation metric used for this task is based on Kendall's Tau rank correlation coefficient.", "labels": [], "entities": [{"text": "Tau rank correlation coefficient", "start_pos": 63, "end_pos": 95, "type": "METRIC", "confidence": 0.7878001630306244}]}, {"text": "Our model obtains a score of 0.413 with respect to the best team's score of 0.625.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset for Task A.", "labels": [], "entities": []}, {"text": " Table 2: Dataset for Task B.", "labels": [], "entities": []}, {"text": " Table 3: Experimental results for feature-ablation experiment for Task A and B. The values in the parenthesis denotes  the deviation from the score when all the features were taken into consideration.", "labels": [], "entities": []}]}