{"title": [{"text": "Azmat: Sentence Similarity using Associative Matrices", "labels": [], "entities": [{"text": "Sentence Similarity", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.9069419503211975}]}], "abstractContent": [{"text": "This work uses recursive autoencoders (Socher et al., 2011), word embeddings (Pennington et al., 2014), associative matrices (Schuler, 2014) and lexical overlap features to model human judgments of sentential similarity on SemEval-2015 Task 2: English STS (Agirre et al., 2015).", "labels": [], "entities": [{"text": "SemEval-2015 Task 2: English STS", "start_pos": 223, "end_pos": 255, "type": "TASK", "confidence": 0.7200076182683309}]}, {"text": "Results show a modest positive correlation between system predictions and human similarity scores, ranking 69th out of 74 submitted systems.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 80, "end_pos": 97, "type": "METRIC", "confidence": 0.8456972539424896}]}], "introductionContent": [{"text": "This work uses a support vector machine (SVM) to determine the similarity of sentence pairs, taking as input the similarity judgments of four subsystems: a set of surface features, unfolding recursive autoencoders (URAE;), Global Vector word embeddings (GloVe;, and the associative matrix approach using the Generalized Categorial Grammar (GCG).", "labels": [], "entities": []}, {"text": "Evaluation is run on SemEval 2015 task 2, Semantic Textual Similarity (STS), which includes a corpus of human similarity judgments.", "labels": [], "entities": [{"text": "SemEval 2015 task", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8153689901034037}, {"text": "Semantic Textual Similarity (STS)", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.7474880665540695}]}, {"text": "The test set consists of 3000 randomly chosen sentence pairs from a corpus of 8500 pairs, which spans five domains (news headlines, image captions, student answers, forum responses, and sentences about belief).", "labels": [], "entities": []}, {"text": "Similarity scores range from 0 (no similarity) to 5 (complete semantic equivalence).", "labels": [], "entities": []}], "datasetContent": [{"text": "For development, 1000 pairs are held out of the training data in jack-knifed batches.", "labels": [], "entities": []}, {"text": "shows how the system performs when each subsystem is omitted.", "labels": [], "entities": []}, {"text": "Each model is designated using the first letter of each subsystem, so the full model is named SUGA.", "labels": [], "entities": [{"text": "SUGA", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.678041934967041}]}, {"text": "(left) shows the performance of the system when all of the held-out pairs are from a single domain (e.g., news headlines) and thus approximates the system's performance on unknown domains.: Correlations with human judgments when only certain similarity relations are used: only wordlevel similarity (leaf), only compositional non-leaf similarity (comp), only similarity between leaf and non-leaf nodes (cross), and permitting all similarities (full).", "labels": [], "entities": []}, {"text": "The weighted mean accounts for the proportion of test cases in each dataset.", "labels": [], "entities": []}, {"text": "all domains and so estimates the system's performance on domains that are familiar.", "labels": [], "entities": []}, {"text": "SemEval-2015 Task 2 test results are shown in (right).", "labels": [], "entities": [{"text": "SemEval-2015 Task 2 test", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.8187509775161743}]}, {"text": "Omission of the surface features results in a sharp performance decrease, showing they capture complementary information to other features.", "labels": [], "entities": []}, {"text": "See UGA model as compared to the SUGA model in.", "labels": [], "entities": []}, {"text": "Also observable in the table is that excluding anyone of the three main subsystems (URAE, GloVe, AM) improves performance, which implies the full system overfits to the training data.", "labels": [], "entities": [{"text": "URAE", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.482111394405365}]}, {"text": "Since the composition method differs between all three subsystems, and since URAE even uses a different underlying dependency structure, the overfit likely stems from the fact that all three systems are computing leaf/leaf similarity.", "labels": [], "entities": []}, {"text": "Overfitting might be reduced by either only using the leaf/leaf similarity from a single system or by tuning the tolerance of the SVM.", "labels": [], "entities": []}, {"text": "Since the development results suggest that the full system overfits, it maybe informative to test how the different parts of the compositional framework behave.", "labels": [], "entities": []}, {"text": "To test this, the full SUGA system is retrained with some similarity relations removed (see).", "labels": [], "entities": []}, {"text": "When only leaf/leaf similarities are used during training, the system performs the best.", "labels": [], "entities": []}, {"text": "This finding is likely due to the ubiquity of word-level similarity/analogy as a task, for which word embeddings such as GloVe were designed.", "labels": [], "entities": [{"text": "word-level similarity/analogy", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.6412844285368919}]}, {"text": "System performance declines when trained only on similarities between non-leaf nodes, suggesting the compositions are less good at reflecting phrasal-and sentencelevel similarity.", "labels": [], "entities": []}, {"text": "The system becomes even less accurate when only using similarities between leaf nodes and non-leaf nodes, which were hoped to enable the system to capture similarities between more and less general phrases (e.g., between 'red ball' and 'ball').", "labels": [], "entities": []}, {"text": "This finding is somewhat surprising since URAE is thought to capture these types of similarities.", "labels": [], "entities": [{"text": "URAE", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.6958505511283875}]}, {"text": "Although leaf/leaf similarities are useful, overreliance on non-compositional nodes causes problems when comparing pairs with more abstract differences.", "labels": [], "entities": []}, {"text": "For example, the system rates the following unrelated pair as very similar despite completely different subject-predicate and modifier compositions: Zoo worker dies after tiger attack Teacher dies after attack in New Zealand Further, while coarse feature selection (e.g., removing all non-leaf features) improves performance, it is not a foregone conclusion that composition features are completely uninformative.", "labels": [], "entities": []}, {"text": "For example, comparisons between nodes of similar depths (e.g., 0-1, 4-3) might be more informative than node comparisons of dissimilar depths (e.g., 1-7, 6-2), so future work should determine whether there is an information gradient when comparing compositional nodes.", "labels": [], "entities": []}, {"text": "Additionally, the fixed length chosen in this work for each depth-paired subvector guarantees a lossless representation of similarities between balanced trees up to 50 words long, but the similarity vectors involving non-leaf nodes become increasingly lossy as the input trees become less balanced.", "labels": [], "entities": []}, {"text": "Therefore, the current system possibly underestimates the informativity of non-leaf features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Model correlation with human judgments  on unknown and known domains in development as  each subsystem is omitted (included subsystems are  noted: S for surface, U for URAE, G for GloVe, and  A for AM). Final system performance on test data  for the task is also shown at right.", "labels": [], "entities": []}, {"text": " Table 2: Correlations with human judgments when  only certain similarity relations are used: only word- level similarity (leaf), only compositional non-leaf  similarity (comp), only similarity between leaf and  non-leaf nodes (cross), and permitting all similari- ties (full). The weighted mean accounts for the pro- portion of test cases in each dataset.", "labels": [], "entities": []}]}