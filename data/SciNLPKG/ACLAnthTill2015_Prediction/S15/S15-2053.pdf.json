{"title": [{"text": "SemEval-2015 Task 15: A Corpus Pattern Analysis Dictionary-Entry-Building Task", "labels": [], "entities": [{"text": "Corpus Pattern Analysis Dictionary-Entry-Building", "start_pos": 24, "end_pos": 73, "type": "TASK", "confidence": 0.6628765910863876}]}], "abstractContent": [{"text": "This paper describes the first SemEval task to explore the use of Natural Language Processing systems for building dictionary entries, in the framework of Corpus Pattern Analysis.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.9081128537654877}, {"text": "Corpus Pattern Analysis", "start_pos": 155, "end_pos": 178, "type": "TASK", "confidence": 0.6245501438776652}]}, {"text": "CPA is a corpus-driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used.", "labels": [], "entities": []}, {"text": "Task 15 draws on the Pattern Dictionary of English Verbs (www.pdev.org.uk), for the targeted lexical entries, and on the British National Corpus for the input text.", "labels": [], "entities": [{"text": "Pattern Dictionary of English Verbs (www.pdev.org.uk)", "start_pos": 21, "end_pos": 74, "type": "DATASET", "confidence": 0.8600039333105087}, {"text": "British National Corpus", "start_pos": 121, "end_pos": 144, "type": "DATASET", "confidence": 0.9346646865208944}]}, {"text": "Dictionary entry building is split into three subtasks which all start from the same concordance sample: 1) CPA parsing, where arguments and their syntactic and semantic categories have to be identified, 2) CPA clustering, in which sentences with similar patterns have to be clustered and 3) CPA automatic lexicography where the structure of patterns have to be constructed automatically.", "labels": [], "entities": [{"text": "Dictionary entry building", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6868914266427358}, {"text": "CPA parsing", "start_pos": 108, "end_pos": 119, "type": "TASK", "confidence": 0.767551988363266}, {"text": "CPA clustering", "start_pos": 207, "end_pos": 221, "type": "TASK", "confidence": 0.7453635334968567}]}, {"text": "Subtask 1 attracted 3 teams, though none could beat the baseline (rule-based system).", "labels": [], "entities": []}, {"text": "Subtask 2 attracted 2 teams, one of which beat the baseline (majority-class classifier).", "labels": [], "entities": []}, {"text": "Sub-task 3 did not attract any participant.", "labels": [], "entities": []}, {"text": "The task has produced a major semantic multi-dataset resource which includes data for 121 verbs and about 17,000 annotated sentences, and which is freely accessible.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is a central vision of NLP to represent the meanings of texts in a formalised way, amenable to automated reasoning.", "labels": [], "entities": []}, {"text": "Since its birth, SEMEVAL (or SENSEVAL as it was then;) has been part of the programme of enriching NLP analyses of text so they get ever closer to a 'meaning representation'.", "labels": [], "entities": [{"text": "SEMEVAL", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.670708954334259}]}, {"text": "In relation to lexical information, this meant finding a lexical resource which \u2022 identified the different meanings of words in away that made high-quality disambiguation possible, \u2022 represented those meanings in ways that were useful for the next steps of building meaning representations.", "labels": [], "entities": []}, {"text": "Most lexical resources explored to date have had only limited success, on either front.", "labels": [], "entities": []}, {"text": "The most obvious candidates-published dictionaries and WordNets-look like they might support the first task, but are very limited in what they offer to the second.", "labels": [], "entities": [{"text": "WordNets-look", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9319522976875305}]}, {"text": "FrameNet moved the game forward a stage.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9420968294143677}]}, {"text": "Here was a framework with a convincing account of how the lexical entry might contribute to building the meaning of the sentence, and with enough meat in the lexical entries (e.g. the verb frames) so that it might support disambiguation.", "labels": [], "entities": []}, {"text": "Papers such as () looked promising, and in 2007 there was a SEMEVAL task on Frame Semantic Structure Extraction ( and in 2010, one on Linking Events and Their Participants (.", "labels": [], "entities": [{"text": "SEMEVAL task", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.6511286497116089}, {"text": "Frame Semantic Structure Extraction", "start_pos": 76, "end_pos": 111, "type": "TASK", "confidence": 0.7580834478139877}]}, {"text": "While there has been a substantial amount of follow-up work, there are some aspects of FrameNet that make it a hard target.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.8371961116790771}]}, {"text": "\u2022 It is organised around frames, rather than words, so inevitably its priority is to give a co-herent account of the different verb senses in a frame, rather than the different senses of an individual verb.", "labels": [], "entities": []}, {"text": "This will tend to make it less good for supporting disambiguation.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.963940441608429}]}, {"text": "\u2022 Frames are not 'data-driven': they are the work of a theorist (Fillmore) doing his best to make sense of the data fora set of verbs.", "labels": [], "entities": []}, {"text": "The prospects of data-driven frame discovery are, correspondingly, slim.", "labels": [], "entities": [{"text": "frame discovery", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7008632719516754}]}, {"text": "\u2022 While FrameNet has worked hard at being systematic in its use of corpus data, FrameNetters looked only for examples showing the verb being used in the relevant sense.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.8914666175842285}]}, {"text": "From the point of view of a process that could possibly be automated, this is problematic.", "labels": [], "entities": []}, {"text": "An approach which bears many similarities to FrameNet, but which starts from the verb rather than the frame, and is more thoroughgoing in its empiricism, is Hanks's Corpus Pattern Analysis ().", "labels": [], "entities": [{"text": "Hanks's Corpus Pattern Analysis", "start_pos": 157, "end_pos": 188, "type": "TASK", "confidence": 0.6517179548740387}]}], "datasetContent": [{"text": "All subtasks (except the first) include two setups and their associated datasets: the number of patterns for each verb is disclosed in the first dataset but not in the second.", "labels": [], "entities": []}, {"text": "This setup was created to see whether it would influence the results.", "labels": [], "entities": []}, {"text": "The two datasets were also created in the hope that system development would start on the first small and carefully crafted dataset (Microcheck) and only then be tested on a larger and more varied subset of verbs (Wingspread) 5 .  The evaluation was split into 2 phases (one week for each): a feedback phase and a validation phase.", "labels": [], "entities": [{"text": "Microcheck", "start_pos": 133, "end_pos": 143, "type": "DATASET", "confidence": 0.9428487420082092}]}, {"text": "The reason for this was to allow for the detection of unforeseen issues in the output of participants' systems so as to prepare for any major problem.", "labels": [], "entities": []}, {"text": "However, this was not put to use by participants since only one team submitted their output in the first phase which also happened to be their final submission.", "labels": [], "entities": []}, {"text": "subtask allowed it, some systems used external resources such as Wordnet or larger corpora.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9770350456237793}]}, {"text": "BLCUNLP) used the Stanford CoreNLP package 10 to get POS, NE and basic dependency features.", "labels": [], "entities": [{"text": "BLCUNLP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6523731350898743}, {"text": "Stanford CoreNLP package 10", "start_pos": 18, "end_pos": 45, "type": "DATASET", "confidence": 0.9406566768884659}]}, {"text": "These features were used to predict both syntax and semantic information.", "labels": [], "entities": []}, {"text": "The method did not involve the use of a statistical classifier.", "labels": [], "entities": []}, {"text": "CMILLS) used three models to solve the task: one for argument detection, and the other two for each layer.", "labels": [], "entities": [{"text": "CMILLS)", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9169492721557617}, {"text": "argument detection", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7460599839687347}]}, {"text": "Argument detection and syntactic tagging were performed using a MaxEnt supervised classifier, while the last was based on heuristics.", "labels": [], "entities": [{"text": "Argument detection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8033389151096344}, {"text": "syntactic tagging", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.6701792776584625}]}, {"text": "CMILLS also reported the use of an external resource, the enTentTen12 (Jakub\u00ed\u010dek et al., 2013) corpus available in Sketch Engine.", "labels": [], "entities": [{"text": "CMILLS", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9671579599380493}, {"text": "enTentTen12 (Jakub\u00ed\u010dek et al., 2013) corpus", "start_pos": 58, "end_pos": 101, "type": "DATASET", "confidence": 0.8020221061176724}]}, {"text": "FANTASY approached the subtask in a supervised setting to predict first the syntactic tags, and then the semantic tags.", "labels": [], "entities": []}, {"text": "The team used features from the MST parser 11 , as well as Stanford CoreNLP for NE, Wordnet 12 , they also applied word embedding representations to predict the output of each layer.", "labels": [], "entities": [{"text": "MST parser 11", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.5354341665903727}, {"text": "Stanford CoreNLP", "start_pos": 59, "end_pos": 75, "type": "DATASET", "confidence": 0.887232095003128}]}, {"text": "The baseline system was a rule-based system taking as input the output of the BLLIP parser, and mapping heads of relevant dependency relations to the most probable tags from subtask 1 tagset.", "labels": [], "entities": []}, {"text": "The semantic tags were only then added to those headwords based on the most frequent semantic category found in the training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Inter-annotator figures where annotators are compared to the expert (annotator 4) who reviewed all the  annotations (Microcheck Task 1).", "labels": [], "entities": []}, {"text": " Table 3: Statistics on the Wingspread test dataset with V standing for verb, P for patterns, I for instances, IMP for  instances of majority pattern, and %MP for proportion of the majority pattern.", "labels": [], "entities": [{"text": "Wingspread test dataset", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.9645120104153951}, {"text": "IMP", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9965490102767944}]}, {"text": " Table 4: Statistics on the Microcheck test dataset; abbreviations as for previous table.", "labels": [], "entities": [{"text": "Microcheck test dataset", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.9850192268689474}]}, {"text": " Table 6: Detailed scores for subtask 1 (10 most frequent categories).", "labels": [], "entities": [{"text": "Detailed", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8232582211494446}]}, {"text": " Table 7: Official scores for subtask 2.", "labels": [], "entities": []}]}