{"title": [{"text": "HITSZ-ICRC: Exploiting Classification Approach for Answer Selection in Community Question Answering", "labels": [], "entities": [{"text": "Answer Selection in Community Question Answering", "start_pos": 51, "end_pos": 99, "type": "TASK", "confidence": 0.6768765250841776}]}], "abstractContent": [{"text": "This paper describes the participation of the HITSZ-ICRC team on the Answer Selection Challenge in SemEval-2015.", "labels": [], "entities": [{"text": "Answer Selection Challenge in SemEval-2015", "start_pos": 69, "end_pos": 111, "type": "TASK", "confidence": 0.8710947275161743}]}, {"text": "Our team participated in English subtask A, English subtask B and Arabic task.", "labels": [], "entities": []}, {"text": "Two approaches, ensemble learning and hierarchical classification were proposed for answer selection in each task.", "labels": [], "entities": [{"text": "hierarchical classification", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.7520586550235748}, {"text": "answer selection", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.8755331337451935}]}, {"text": "Bag-of-words features, lexical features and non-textual features were employed.", "labels": [], "entities": []}, {"text": "For the Arabic task, features were extracted from both Arabic data and English data that translated from the Arabic data.", "labels": [], "entities": []}, {"text": "Evaluation demonstrated that the proposed methods were effective, achieving a macro-averaged F1 of 56.41% (rank 2 nd) in English subtask A, 53.60 % (rank 3 rd) in English subtask B and 67.70% (rank 3 rd) in Arabic task, respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9702786207199097}]}], "introductionContent": [{"text": "In recent years, community question answering (CQA) systems are becoming more and more popular on the Internet.", "labels": [], "entities": [{"text": "question answering (CQA)", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8427874922752381}]}, {"text": "By using CQA system, a user can post his/her question on CQA portal and receive answers from other users.", "labels": [], "entities": [{"text": "CQA portal", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.9711418151855469}]}, {"text": "All users can post questions and answers on CQA portal freely.", "labels": [], "entities": [{"text": "CQA portal", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.9820529222488403}]}, {"text": "Although it makes CQA users to get answers easily, the answer quality evaluation becomes a challenge for questions with multiple answers.", "labels": [], "entities": []}, {"text": "To reduce the inconvenient in going through plenty of candidate answers, it makes sense to evaluate the quality of answers and select high-quality answers automatically for CQA systems.", "labels": [], "entities": []}, {"text": "As a consequently, the task of answer quality evaluation and answer selection in CQA have attracted more and more attention in recent years.", "labels": [], "entities": [{"text": "answer quality evaluation", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.8134546677271525}, {"text": "answer selection", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.8796168565750122}, {"text": "CQA", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.7610917687416077}]}, {"text": "The Answer Selection in CQA challenge was opened as one new task in.", "labels": [], "entities": [{"text": "Answer Selection in CQA challenge", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.6714367091655731}]}, {"text": "It created avenue and provided annotated datasets for researchers to compare their methods for answer selection in CQA.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.895052433013916}, {"text": "CQA", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.845759391784668}]}, {"text": "This challenge consisted of Subtask A and Subtask B. Subtask A required participant system to classify answers as relevant, potentially useful and bad for each question.", "labels": [], "entities": []}, {"text": "Subtask B required participant system to decide whether the answer to a YES_NO question should be Yes, No or Unsure based on the answer list.", "labels": [], "entities": [{"text": "YES_NO", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.8385824759801229}]}, {"text": "Subtask A was offered for two languages: English and Arabic.", "labels": [], "entities": []}, {"text": "Data for the two languages was in different data set format.", "labels": [], "entities": []}, {"text": "In remainder of this paper, Subtask A in English is abbreviated to English subtask A, Subtask A in Arabic is abbreviated to Arabic task and Subtask B in English is abbreviated to English subtask B.", "labels": [], "entities": []}, {"text": "HITSZ-ICRC team participated in English subtask A, English subtask B and Arabic task.", "labels": [], "entities": [{"text": "HITSZ-ICRC", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7297306060791016}]}, {"text": "This paper describes the ensemble learning method and hierarchical classification method proposed for each subtask in SemEval-2015 Task 3.", "labels": [], "entities": [{"text": "SemEval-2015 Task 3", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.8013027111689249}]}], "datasetContent": [{"text": "Some toolkits were employed to extract features and train classifiers.", "labels": [], "entities": []}, {"text": "NLTK () was used to extract features, include part-of-speech of question and answer, frequent n-gram terms, cosine similarity and soon.", "labels": [], "entities": [{"text": "soon", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9808529019355774}]}, {"text": "WEKA ( toolkit was used to do feature selection and classifier training and choosing.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7644098997116089}]}, {"text": "LIBSVM (Chang and Lin, 2011) and LIBLINEAR were used to train SVM classifier.", "labels": [], "entities": [{"text": "LIBSVM", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5836329460144043}, {"text": "LIBLINEAR", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9775310158729553}, {"text": "SVM classifier", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.602076530456543}]}, {"text": "Scikit-learn toolkit) was used to train classifiers.", "labels": [], "entities": []}, {"text": "We submitted 3 formal results for each subtask including English subtask A, English subtask B and Arabic task following task result submission requests: 1 primary result as team official result, 2 contrastive results to compare effects of different methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Macro F1 and accuracy of English subtask A.", "labels": [], "entities": [{"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.8034464120864868}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.99952232837677}]}, {"text": " Table 2. Macro F1 and accuracy of English subtask B.", "labels": [], "entities": [{"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.8006564974784851}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995447993278503}]}, {"text": " Table 3. Macro F1 and accuracy of Arabic task.", "labels": [], "entities": [{"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.7599319219589233}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995468258857727}]}, {"text": " Table 4. Detailed evaluation results (P, R and F1) of the  best performance result for each task.", "labels": [], "entities": [{"text": "Detailed", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9606516361236572}, {"text": "F1)", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9331462681293488}]}, {"text": " Table 5. Macro F1 of SVM classifier using bag-of-word  features, non-bag-of-word features and all features.", "labels": [], "entities": [{"text": "SVM classifier", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7581508457660675}]}]}