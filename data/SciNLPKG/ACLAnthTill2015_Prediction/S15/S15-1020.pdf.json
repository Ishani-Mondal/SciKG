{"title": [{"text": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach", "labels": [], "entities": []}], "abstractContent": [{"text": "The increasing amount of online content motivated the development of multi-document summarization methods.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.5787018537521362}]}, {"text": "In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 103, "end_pos": 131, "type": "TASK", "confidence": 0.6200829446315765}]}, {"text": "The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.", "labels": [], "entities": []}], "introductionContent": [{"text": "The use of the Internet to fulfill generic information needs motivated pioneer multi-document summarization efforts as NewsInEssence () or Newsblaster (), online since 2001.", "labels": [], "entities": [{"text": "NewsInEssence", "start_pos": 119, "end_pos": 132, "type": "DATASET", "confidence": 0.9334316849708557}, {"text": "Newsblaster", "start_pos": 139, "end_pos": 150, "type": "DATASET", "confidence": 0.9295621514320374}]}, {"text": "In general, multi-document summarization approaches have to address two different problems: passage selection and information ordering.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6177790760993958}, {"text": "passage selection", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.9087846875190735}, {"text": "information ordering", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.8204763531684875}]}, {"text": "Current multi-document systems adopt, for passage selection, approaches similar to the ones used in single-document summarization, and use the chronological order of the documents for information ordering).", "labels": [], "entities": [{"text": "passage selection", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8894460499286652}, {"text": "information ordering", "start_pos": 184, "end_pos": 204, "type": "TASK", "confidence": 0.8112809956073761}]}, {"text": "The problem is that most approaches fail to generate summaries that cover generic topics which comprehend different, equally important, subtopics.", "labels": [], "entities": []}, {"text": "We propose to extend a state-of-the-art single-document summarization method, KP-CENTRALITY (), capable of focusing on diverse important topics while ignoring unimportant ones, to perform multi-document summarization.", "labels": [], "entities": [{"text": "single-document summarization", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.5439720749855042}, {"text": "multi-document summarization", "start_pos": 188, "end_pos": 216, "type": "TASK", "confidence": 0.6325805187225342}]}, {"text": "We explore two hierarchical strategies to perform this extension.", "labels": [], "entities": []}, {"text": "This document is organized as follows: Sect.", "labels": [], "entities": []}, {"text": "2 addresses the related work; Sect.", "labels": [], "entities": []}, {"text": "3 presents our multidocument summarization appproach; experimental results close the paper.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.5201007425785065}]}], "datasetContent": [{"text": "We compare the performance of our methods against other representative models, namely MEAD, MMR, Expected n-call@k (, and the Portfolio Theory (.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.6525127291679382}]}, {"text": "MEAD is a centroid-based method and one of the most popular centrality-based methods.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5048555731773376}]}, {"text": "MMR is one of the most used query-based methods.", "labels": [], "entities": [{"text": "MMR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5430179238319397}]}, {"text": "Expected n-call@k adapts and extends MMR as a probabilistic model (Probabilistic Latent MMR).", "labels": [], "entities": [{"text": "MMR", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.936025083065033}]}, {"text": "The Portfolio Theory also extends MMR based on the idea of ranking under uncertainty.", "labels": [], "entities": [{"text": "MMR", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9803675413131714}]}, {"text": "As baseline, we used the straightforward idea of combining all input documents into a single one, and then submit the document to the single-document summarization method.", "labels": [], "entities": []}, {"text": "Considering that most coverage-based systems explore event information, we opted for not including them in this comparative analysis.", "labels": [], "entities": []}, {"text": "To assess the informativeness of the summaries generated by our methods, we used ROUGE-1 and ROUGE-2 (Lin, 2004) on DUC 2007 and TAC 2009 datasets.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9888572096824646}, {"text": "ROUGE-2", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9784232974052429}, {"text": "DUC 2007 and TAC 2009 datasets", "start_pos": 116, "end_pos": 146, "type": "DATASET", "confidence": 0.8760766685009003}]}, {"text": "The main summarization task in DUC 2007 1 is the generation of 250-word summaries of 45 clusters of 25 newswire documents (from the AQUAINT corpus) and 4 human reference summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9792616367340088}, {"text": "DUC 2007 1", "start_pos": 31, "end_pos": 41, "type": "DATASET", "confidence": 0.9154529174168905}, {"text": "AQUAINT corpus", "start_pos": 132, "end_pos": 146, "type": "DATASET", "confidence": 0.9402163326740265}]}, {"text": "The TAC 2009 Summarization task 2 has 44 topic clusters.", "labels": [], "entities": [{"text": "TAC 2009 Summarization task", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.6035731434822083}]}, {"text": "Each topic has 2 sets of 10 news documents obtained from the AQUAINT 2 corpus.There are 4 human 100-word reference summaries for each set, where the reference summaries for the first set are query-oriented, and for the second set are update summaries.", "labels": [], "entities": [{"text": "AQUAINT 2 corpus.There", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.9357052246729533}]}, {"text": "In this work, we used the first set of reference summaries.", "labels": [], "entities": []}, {"text": "We evaluate the different models by generating summaries with 250 words.", "labels": [], "entities": []}, {"text": "We only present the best results.", "labels": [], "entities": []}, {"text": "The used features include the bag-of-words model representation of the sentences (TF-IDF), the key phrases and the query (obtained from the topics descriptions with N = 1.), Euclidean, Chebyshev, Manhattan, Minkowski, the Jensen-Shannon Divergence, and the cosine similarity.", "labels": [], "entities": []}, {"text": "shows that the best results were obtained by the proposed hierarchical models, in both datasets.", "labels": [], "entities": []}, {"text": "Overal, the best performing distance metric for our centrality-based method was the cosine similarity and the best strategy for combining the information was the waterfall approach, namely, in terms of ROUGE-2.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 202, "end_pos": 209, "type": "METRIC", "confidence": 0.867028534412384}]}, {"text": "In DUC 2007, frac133 using the single-layer method achieved the best ROUGE-1 score, although the difference for cosine is hardly noticeable.", "labels": [], "entities": [{"text": "DUC 2007", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.9537064731121063}, {"text": "ROUGE-1 score", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.9814358949661255}]}, {"text": "Single-layer with frac133 shows a performance improvement of 0.0180 ROUGE-1 points (relative performance improvement of 5.0%) over the best of the other systems, Portfolio, in DUC 2007, and of 0.0845 ROUGE-1 points (19.7% relative performance improvement) in TAC 2009.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.9928350448608398}, {"text": "Portfolio", "start_pos": 162, "end_pos": 171, "type": "DATASET", "confidence": 0.8830299377441406}, {"text": "DUC 2007", "start_pos": 176, "end_pos": 184, "type": "DATASET", "confidence": 0.9516254365444183}, {"text": "ROUGE-1", "start_pos": 200, "end_pos": 207, "type": "METRIC", "confidence": 0.9807392358779907}, {"text": "TAC 2009", "start_pos": 259, "end_pos": 267, "type": "DATASET", "confidence": 0.9350790083408356}]}, {"text": "In terms of ROUGE-2, the waterfall method using cosine achieved an improvement of 0.0112 (relative performance improvement of 14.1%) over Portfolio, in DUC 2007, and of 0.0848 (relative performance improvement of 100.4%) over MEAD, the best performing of the reference systems using this metric, in TAC 2009.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9742927551269531}, {"text": "Portfolio", "start_pos": 138, "end_pos": 147, "type": "DATASET", "confidence": 0.753326952457428}, {"text": "DUC 2007", "start_pos": 152, "end_pos": 160, "type": "DATASET", "confidence": 0.9685800969600677}, {"text": "MEAD", "start_pos": 226, "end_pos": 230, "type": "METRIC", "confidence": 0.6900022625923157}, {"text": "TAC 2009", "start_pos": 299, "end_pos": 307, "type": "DATASET", "confidence": 0.9512077867984772}]}, {"text": "Note that our baseline obtained results similar to the best reference system in DUC 2007 and better results than all reference systems in TAC 2009 (0.0454 ROUGE-1 points corresponding to a 10.6% relative performance improvement; 0.0546 ROUGE-2 points corresponding to a 64.6% relative performance improvement).", "labels": [], "entities": [{"text": "DUC 2007", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.9546985328197479}, {"text": "TAC 2009", "start_pos": 138, "end_pos": 146, "type": "DATASET", "confidence": 0.893176257610321}, {"text": "ROUGE-1", "start_pos": 155, "end_pos": 162, "type": "METRIC", "confidence": 0.9830355644226074}, {"text": "ROUGE-2", "start_pos": 236, "end_pos": 243, "type": "METRIC", "confidence": 0.9808133244514465}]}, {"text": "The better results obtained on the TAC 2009 dataset are due to the small size of the reference summaries and to the fact that the documents sets to be summarized contain topics with higher diversity of subtopics.", "labels": [], "entities": [{"text": "TAC 2009 dataset", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.9529705246289571}]}, {"text": "The shuffle results included in are averages of 10 trials.", "labels": [], "entities": [{"text": "shuffle", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9626833200454712}]}, {"text": "They are lower than the other obtained using the documents organized in chronological order.", "labels": [], "entities": []}, {"text": "This suggests that the order of the input documents is important to the summarization methods.", "labels": [], "entities": [{"text": "summarization", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.9776036739349365}]}, {"text": "shows an example of summary produced by our multi-document method.", "labels": [], "entities": []}, {"text": "The figure also includes the respective reference summary for comparison.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE-1 (R1) and ROUGE-2 (R2) scores.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9917660355567932}, {"text": "ROUGE-2 (R2) scores", "start_pos": 27, "end_pos": 46, "type": "METRIC", "confidence": 0.9257397174835205}]}]}