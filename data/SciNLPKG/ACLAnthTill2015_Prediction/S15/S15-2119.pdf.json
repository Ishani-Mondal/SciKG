{"title": [{"text": "UPF-taln: SemEval 2015 Tasks 10 and 11 Sentiment Analysis of Literal and Figurative Language in Twitter *", "labels": [], "entities": [{"text": "UPF-taln", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9042809009552002}, {"text": "Sentiment Analysis of Literal and Figurative Language in Twitter", "start_pos": 39, "end_pos": 103, "type": "TASK", "confidence": 0.9103835357560052}]}], "abstractContent": [{"text": "In this paper, we describe the approach used by the UPF-taln team for tasks 10 and 11 of SemEval 2015 that respectively focused on \"Sentiment Analysis in Twitter\" and \"Sen-timent Analysis of Figurative Language in Twitter\".", "labels": [], "entities": [{"text": "UPF-taln", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.8819209337234497}, {"text": "Sentiment Analysis in Twitter", "start_pos": 132, "end_pos": 161, "type": "TASK", "confidence": 0.8887647688388824}, {"text": "Sen-timent Analysis of Figurative Language in Twitter", "start_pos": 168, "end_pos": 221, "type": "TASK", "confidence": 0.8809637682778495}]}, {"text": "Our approach achieved satisfactory results in the figurative language analysis task, obtaining the second best result.", "labels": [], "entities": [{"text": "figurative language analysis task", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.8528204262256622}]}, {"text": "In task 10, our approach obtained acceptable performances.", "labels": [], "entities": []}, {"text": "We experimented with both word-based features and domain-independent intrinsic word features.", "labels": [], "entities": []}, {"text": "We exploited two machine learning methods: the supervised algorithm Support Vector Machines for task 10, and Random-Sub-Space with M5P as base algorithm for task 11. 1 Motivation During the last decade the study and characterisa-tion of sentiments and emotions in on-line user-generated content has attracted more and more interest.", "labels": [], "entities": [{"text": "characterisa-tion of sentiments and emotions in on-line user-generated content", "start_pos": 216, "end_pos": 294, "type": "TASK", "confidence": 0.8291484846009148}]}, {"text": "Since 2013 several tasks dealing with Sentiment Analysis have been organised in the context of SemEval.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.95621657371521}]}, {"text": "These tasks have been mainly focused on the analysis of short texts like SMS or tweets.", "labels": [], "entities": []}, {"text": "In this paper we describe the approach adopted by UPF-taln team for tasks 10 and 11 of SemEval 2015, both dealing with the analysis of English tweets.", "labels": [], "entities": [{"text": "UPF-taln team", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.881561815738678}]}, {"text": "Task 10 concerned \"Sentiment Analysis in Twitter\" * The research described in this paper is partially funded by the Spanish fellowship RYC-2009-04291, the SKATER-TALN UPF project (TIN2012-38584-C06-03), and the EU project Dr. Inventor (n. 611383). and included different subtasks.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.900908038020134}, {"text": "Spanish fellowship RYC-2009-04291", "start_pos": 116, "end_pos": 149, "type": "DATASET", "confidence": 0.7030806342760721}]}, {"text": "We participated in the subtask B, named \"Sentiment Polarity Classifi-cation\".", "labels": [], "entities": []}, {"text": "Given a message, we were asked to classify whether the message was of positive, negative, or neutral sentiment.", "labels": [], "entities": []}, {"text": "In Task 11 the participants were asked to determine the polarity score (between-5 to +5) of tweets rich in metaphor and irony.", "labels": [], "entities": []}, {"text": "Our model reaches satisfactory results in the figurative language task 11, however it has suboptimal performance in task 10.", "labels": [], "entities": []}, {"text": "We exploited an extended version of the tweet classification features and approach described in (Barbieri and Saggion, 2014).", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7444936335086823}]}, {"text": "In particular, we experimented the use of intrinsic word features, char-acterising each word in a tweet to try to model and thus automatically determine its polarity.", "labels": [], "entities": []}, {"text": "Thanks to intrinsic word features, we aimed to detect two aspects of tweets: the style used (e.g. register used, frequent or rare words, positive or negative words, etc.) and the unexpectedness in the use of words, particularly important for figurative language.", "labels": [], "entities": []}, {"text": "We also exploited textual features (like word occurrences , bigrams, skipgrams or other word patterns) in order to capture the way words are used in positive and negative tweets.", "labels": [], "entities": []}, {"text": "As machine learning approach we choose the supervised method Support Vector Machines (Platt, 1999) for task 10 and the regression algorithm Random-Sub-Space (Ho, 1998) with M5P (Quinlan, 2014) as base algorithm for task 11.", "labels": [], "entities": []}, {"text": "In Section 2 and 3 we describe the dataset used and the tools we employed to process the tweets.", "labels": [], "entities": []}, {"text": "In Section 4 we introduce the features we built our model on.", "labels": [], "entities": []}, {"text": "In Section 5 we discuss the performance of our model in SemEval 2015 and in Section 6 we 704", "labels": [], "entities": [{"text": "SemEval 2015", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.6319105923175812}]}], "introductionContent": [], "datasetContent": [{"text": "In order to train our systems we used in each task only the dataset provided by the organisers.", "labels": [], "entities": []}, {"text": "For task 10 we were able to retrieve 9689 tweets, tagged as positive, negative and neutral (.", "labels": [], "entities": []}, {"text": "For task 11 the dataset was a collection of 8000 figurative tweets annotated with sentiment scores from -5 to +5 ().", "labels": [], "entities": []}, {"text": "In this section we present our results in the two tasks (see).", "labels": [], "entities": []}, {"text": "We only report final results (mean of Precision, Recall and F-Measure of each class), for more details please refer to the task 10 and task 11 papers ().", "labels": [], "entities": [{"text": "Precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9957777261734009}, {"text": "Recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9948499798774719}, {"text": "F-Measure", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9943934679031372}]}], "tableCaptions": [{"text": " Table 1: Task 10 results. For each test set we report F- Measure and ranking comparing to other systems.", "labels": [], "entities": [{"text": "F- Measure", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.964970072110494}]}, {"text": " Table 2: Task 11 results measured by the Cosine Similar- ity and the Mean Square Error over the test set (Overall)  and for its subsets: sarcasm, irony, metaphor and other  (non-figurative tweets).", "labels": [], "entities": [{"text": "Mean Square Error", "start_pos": 70, "end_pos": 87, "type": "METRIC", "confidence": 0.8776135643323263}]}, {"text": " Table 3: Task 11 contribution of each group of feature.  The best feature group was Sentiment, in particular the  features computed with the NRC Hashtag Sentiment Lex- icon, see Section 4.1.", "labels": [], "entities": [{"text": "NRC Hashtag Sentiment Lex- icon", "start_pos": 142, "end_pos": 173, "type": "DATASET", "confidence": 0.9231464366118113}]}]}