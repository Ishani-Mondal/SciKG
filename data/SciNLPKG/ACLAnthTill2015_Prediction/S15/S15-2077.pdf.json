{"title": [{"text": "SemEval-2015 Task 9: CLIPEval Implicit Polarity of Events", "labels": [], "entities": [{"text": "CLIPEval Implicit Polarity of Events", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.6467614889144897}]}], "abstractContent": [{"text": "Sentiment analysis tends to focus on the polarity of words, combining their values to detect which portion of a text is opinionated.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9553790986537933}]}, {"text": "CLIPEval wants to promote a more holistic approach, looking at psychological researches that frame the connotations of words as the emotional values activated by them.", "labels": [], "entities": [{"text": "CLIPEval", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9555050134658813}]}, {"text": "The implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current research in sentiment analysis (SA, henceforth) is mostly focused on lexical resources that store polarity values.", "labels": [], "entities": [{"text": "sentiment analysis (SA", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.9283693879842758}]}, {"text": "For bag-of-words approaches the polarity of a text depends on the presence/absence of a set of lexical items.", "labels": [], "entities": []}, {"text": "This methodology is successful to detect opinions about entities (such as reviews) but it shows mixed results when complex opinions about events -involving perspectives and points of view -are expressed.", "labels": [], "entities": []}, {"text": "In terms of parts of speech involved, SA approaches tend to focus on lexical items that explicitly convey opinions -mainly adjectives, adverbs and several nouns -leaving verbs on the foreground.", "labels": [], "entities": []}, {"text": "Improvements have been proposed by taking into account syntax and by investigating the connotative polarity of words.", "labels": [], "entities": []}, {"text": "One of the key aspects of sentiment analysis, which has been only marginally tackled so far, is the identification of implicit polarity.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.968812108039856}]}, {"text": "By implicit polarity we refer to the recognition of subjective textual units where no polarity markers are present but still people are able to state whether the text portion under analysis expresses a positive or negative sentiment.", "labels": [], "entities": []}, {"text": "Recently, methodologies trying to address this aspect have been developed, incorporating ideas from linguistic and psychological studies on the subjective aspects of linguistic expressions.", "labels": [], "entities": []}, {"text": "Aiming at promoting a more holistic approach to sentiment analysis, combining the detection of implicit polarity with the expression of opinions on events, we propose CLIPEval, a task based on a dataset of events annotated as instantiations of pleasant and unpleasant events (PE/UPEs henceforth) previously collected in psychological research as the ones that correlate with mood (both good and bad feelings) (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.9215376675128937}]}], "datasetContent": [{"text": "The CLIPEval evaluation exercise is based on the CLIPEval dataset, which consists of two parts: a training set and a test set.", "labels": [], "entities": [{"text": "CLIPEval dataset", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.9599650204181671}]}, {"text": "The final size of the dataset is 1,651 sentences, divided in 1,280 sentences for the training and 371 for the test.", "labels": [], "entities": []}, {"text": "Each event class in the training data contains 160 sentences.", "labels": [], "entities": []}, {"text": "Each class in the training set is available in a separate file composed of four tab separated fields: a sentence id, the sentence extracted from the Gigaword corpus, the polarity value and the class label.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 149, "end_pos": 164, "type": "DATASET", "confidence": 0.9374359846115112}]}, {"text": "Each file is named with the class label.", "labels": [], "entities": []}, {"text": "Some exam-ples of the training data are provided in the examples below (examples from 3.) to 5.)): The test data has been provided in a single file with only two fields: the sentence id and the sentence extracted from the Gigaword corpus: 6.) 12 After having given a friend a lift home I was stopped by police.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 222, "end_pos": 237, "type": "DATASET", "confidence": 0.9471660554409027}]}, {"text": "Since both Task A and Task B of CLIPEval are essentially classification tasks (classification of the polarity value for Task A and classification of the event instance and the polarity value for Task B), we have used Precision, Recall and F1-measure to evaluate the system results against the test set.", "labels": [], "entities": [{"text": "Precision", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9917306303977966}, {"text": "Recall", "start_pos": 228, "end_pos": 234, "type": "METRIC", "confidence": 0.9001085758209229}, {"text": "F1-measure", "start_pos": 239, "end_pos": 249, "type": "METRIC", "confidence": 0.9967665672302246}]}, {"text": "Furthermore, since this is a multi-classification task, we have computed micro average Precision, Recall and F1-measure per class.", "labels": [], "entities": [{"text": "micro average", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9173564910888672}, {"text": "Precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.7032266855239868}, {"text": "Recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9855220317840576}, {"text": "F1-measure", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9983364939689636}]}, {"text": "This latter measure has been used for the final ranking of the systems.", "labels": [], "entities": []}, {"text": "We have adopted standard definitions for these measures, namely: \u2022 Precision: the number of correctly classified positive examples, tp i per class Ci , divided by number of examples labeled by the system as positive (tp i plus false positive fp i ): \u2022 Recall: the number of correctly classified positive examples tp i per class Ci divided by the number of positive examples in the data (tp i plus false negatives fn i ) : \u2022 F-measure: the mean of Precision and Recall calculated as follows: (\u03b2 2 +1)PrecisionRecall To better evaluate systems' performances, we have developed three baselines, one per Task A and two per Task B. In particular: \u2022 Task A baseline has been obtained by assigning to each sentence in the test set the most frequent polarity value on the basis of the data in the training set.", "labels": [], "entities": [{"text": "Recall", "start_pos": 252, "end_pos": 258, "type": "METRIC", "confidence": 0.9024530649185181}, {"text": "F-measure", "start_pos": 424, "end_pos": 433, "type": "METRIC", "confidence": 0.9908485412597656}, {"text": "Recall", "start_pos": 461, "end_pos": 467, "type": "METRIC", "confidence": 0.9503059387207031}]}, {"text": "This resulted in marking all 371 sentences in the test set with NEGATIVE polarity; \u2022 Task B baseline 1 has been obtained in two steps: first, for each class in the training data we have selected the most frequent nouns and verbs lemmas.", "labels": [], "entities": [{"text": "NEGATIVE", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.7804813981056213}]}, {"text": "This has provided us with a list of keywords representing each class.", "labels": [], "entities": []}, {"text": "We have then compared each sentence in the test set with each group of keywords and assigned as correct the class which scored the higher number of matches.", "labels": [], "entities": []}, {"text": "In case of a draw, a random class between the classes with the highest scores is assigned.", "labels": [], "entities": []}, {"text": "If no match is found, a random class is assigned.", "labels": [], "entities": []}, {"text": "As for the polarity, we have used the absolute most frequent polarity values, like in task A (i.e. all test set entries have been assigned to NEGATIVE value).", "labels": [], "entities": []}, {"text": "\u2022 Task B baseline 2 has been obtained following the approach in Task B baseline 1 for the class assignment and we have assigned the most frequent polarity value per class according to training data (e.g. for items classified as ATTENDING EVENTS the assigned polarity value is POSITIVE).", "labels": [], "entities": [{"text": "ATTENDING", "start_pos": 228, "end_pos": 237, "type": "METRIC", "confidence": 0.9257707595825195}, {"text": "POSITIVE", "start_pos": 276, "end_pos": 284, "type": "METRIC", "confidence": 0.9900562763214111}]}, {"text": "We report in the results of both systems for Task A and the Task A baseline.", "labels": [], "entities": []}, {"text": "In we report the results for Task B and both baseline for Task B (baseline 1 and baseline 2, respectively).", "labels": [], "entities": []}, {"text": "SHELLFBK outperforms SIGMA2320 for the Task A; both systems improve the baseline.", "labels": [], "entities": [{"text": "SHELLFBK", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9548079967498779}]}, {"text": "The results are not as good as in classification tasks concerning the polarities of tweets () or reviews () but since this is a novel task about implicit polarity we think they are promising.", "labels": [], "entities": []}, {"text": "For task B SHELLFBK has a better performance both in terms of precision and recall if compared with the two baselines.", "labels": [], "entities": [{"text": "SHELLFBK", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.7716899514198303}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9995700716972351}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9992994070053101}]}, {"text": "At the moment we do not know if the results are due to SHELLFBK methodology or if data sparseness in the classes has an influence on the classification task: maybe classes more cohesive from conceptual and lexical point of view could be easier to detect.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: CLIPEval corpus: Training data.", "labels": [], "entities": [{"text": "CLIPEval corpus", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.9084565937519073}, {"text": "Training data", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.6505816727876663}]}, {"text": " Table 2: CLIPEval corpus: Test data.", "labels": [], "entities": [{"text": "CLIPEval corpus", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.8975122272968292}]}, {"text": " Table 3: Evaluation for Task A : polarity identification.", "labels": [], "entities": [{"text": "polarity identification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7420722842216492}]}, {"text": " Table 4: Evaluation for Task B : event instance and polar- ity identification.", "labels": [], "entities": [{"text": "polar- ity identification", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7156129628419876}]}]}