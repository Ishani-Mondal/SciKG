{"title": [{"text": "Compositional Distributional Semantics with Long Short Term Memory", "labels": [], "entities": []}], "abstractContent": [{"text": "We are proposing an extension of the recur-sive neural network that makes use of a variant of the long short-term memory architecture.", "labels": [], "entities": []}, {"text": "The extension allows information low in parse trees to be stored in a memory register (the 'memory cell') and used much later higher up in the parse tree.", "labels": [], "entities": []}, {"text": "This provides a solution to the vanishing gradient problem and allows the network to capture long range dependencies.", "labels": [], "entities": []}, {"text": "Experimental results show that our composition outperformed the traditional neural-network composition on the Stanford Sentiment Treebank.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 110, "end_pos": 137, "type": "DATASET", "confidence": 0.8902161320050558}]}], "introductionContent": [{"text": "Moving from lexical to compositional semantics in vector-based semantics requires answers to two difficult questions: (i) what is the nature of the composition functions (given that the lambda calculus for variable binding is no longer applicable), and (ii) how do we learn the parameters of those functions (if they have any) from data?", "labels": [], "entities": []}, {"text": "A number of classes of functions have been proposed in answer to the first question, including simple linear functions like vector addition, non-linear functions like those defined by multi-layer neural networks (, and vector matrix multiplication and tensor linear mapping (.", "labels": [], "entities": []}, {"text": "The matrix and tensor-based functions have the advantage of allowing a relatively straightforward comparison with formal semantics, but the fact that multi-layer neural networks with non-linear activation functions like sigmoid can approximate any continuous function already make them an attractive choice.", "labels": [], "entities": []}, {"text": "In trying to answer the second question, the advantages of approaches based on neural network architectures, such as the recursive neural network (RNN) model) and the convolutional neural network model), are even clearer.", "labels": [], "entities": []}, {"text": "Models in this paradigm can take advantage of general learning procedures based on back-propagation, and with the rise of 'deep learning', of a variety of efficient algorithms and tricks to further improve training.", "labels": [], "entities": []}, {"text": "Since the first success of the RNN model) in constituent parsing, two classes of extensions have been proposed.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.6562874019145966}]}, {"text": "One class is to enhance its compositionality by using tensor product) or concatenating RNNs horizontally to make a deeper net).", "labels": [], "entities": []}, {"text": "The other is to extend its topology in order to fulfill a wider range of tasks, like for dependency parsing and for context-dependence sentiment analysis.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8527306914329529}, {"text": "context-dependence sentiment analysis", "start_pos": 116, "end_pos": 153, "type": "TASK", "confidence": 0.713578333457311}]}, {"text": "Our proposal in this paper is an extension of the RNN model to improve compositionality.", "labels": [], "entities": []}, {"text": "Our motivation is that, like training recurrent neural networks, training RNNs on deep trees can suffer from the vanishing gradient problem), i.e., that errors propagated back to the leaf nodes shrink exponentially.", "labels": [], "entities": []}, {"text": "In addition, information sent from a leaf node to the root can be obscured if the path between them is long, thus leading to the problem how to capture long range dependencies.", "labels": [], "entities": []}, {"text": "We therefore borrow the long short-term memory (LSTM) architecture (Hochreiter and Schmidhu- ber, 1997) from recurrent neural network research to tackle those two problems.", "labels": [], "entities": []}, {"text": "The main idea is to allow information low in a parse tree to be stored in a memory cell and used much later higher up in the parse tree, by recursively adding up all memory into memory cells in a bottom-up manner.", "labels": [], "entities": []}, {"text": "In this way, errors propagated back through structure do not vanish.", "labels": [], "entities": []}, {"text": "And information from leaf nodes is still (loosely) preserved and can be used directly at any higher nodes in the hierarchy.", "labels": [], "entities": []}, {"text": "We then apply this composition to sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9637818932533264}]}, {"text": "Experimental results show that the new composition works better than the traditional neural-network-based composition.", "labels": [], "entities": []}, {"text": "The outline of the rest of the paper is as follows.", "labels": [], "entities": []}, {"text": "We first, in Section 2, give a brief background on neural networks, including the multi-layer neural network, recursive neural network, recurrent neural network, and LSTM.", "labels": [], "entities": []}, {"text": "We then propose the LSTM for recursive neural networks in Section 3, and its application to sentiment analysis in Section 4.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.7115142345428467}, {"text": "sentiment analysis", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.9669948220252991}]}, {"text": "Section 5 shows our experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the Stanford Sentiment Treebank) which consists of 5-way fine-grained sentiment labels (very negative, negative, neutral, positive, very positive) for 215,154 phrases of 11,855 sentences.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank)", "start_pos": 12, "end_pos": 40, "type": "DATASET", "confidence": 0.9112610518932343}]}, {"text": "The standard splitting is also given: 8544 sentences for training, 1101 for development, and 2210 for testing.", "labels": [], "entities": []}, {"text": "The average sentence length is 19.1.", "labels": [], "entities": []}, {"text": "In addition, the treebank also supports binary sentiment (positive, negative) classification by removing neutral labels, leading to: 6920 sentences for training, 872 for development, and 1821 for testing.", "labels": [], "entities": [{"text": "binary sentiment (positive, negative) classification", "start_pos": 40, "end_pos": 92, "type": "TASK", "confidence": 0.7252926379442215}]}, {"text": "The evaluation metric is the accuracy, given by 100\u00d7#correct #total .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9996769428253174}]}], "tableCaptions": [{"text": " Table 1: Accuracies of the (tanh) LSTM-RNN compared  with other models.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9989638328552246}]}]}