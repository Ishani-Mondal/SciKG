{"title": [{"text": "FBK-HLT: An Application of Semantic Textual Similarity for Answer Selection in Community Question Answering", "labels": [], "entities": [{"text": "FBK-HLT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9390767216682434}, {"text": "Answer Selection", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.9433672726154327}, {"text": "Community Question Answering", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.5838152468204498}]}], "abstractContent": [{"text": "This paper reports the description and performance of our system, FBK-HLT, participating in the SemEval 2015, Task #3 \"Answer Selection in Community Question Answering\" for English, for both subtasks.", "labels": [], "entities": [{"text": "FBK-HLT", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.938421905040741}, {"text": "SemEval 2015, Task", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8544178754091263}, {"text": "Answer Selection in Community Question Answering\"", "start_pos": 119, "end_pos": 168, "type": "TASK", "confidence": 0.7632436326571873}]}, {"text": "We submit two runs with different classifiers in combining typical features (lexical similarity, string similarity , word n-grams, etc.) with machine translation evaluation metrics and with some ad hoc features (e.g user overlapping, spam filtering).", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 142, "end_pos": 172, "type": "TASK", "confidence": 0.7613755265871683}]}, {"text": "We outperform the baseline system and achieve interesting results on both subtasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Answer selection is an important task inside the wider task of question answering that represents at the momenta topic of great interest for research and business as well.", "labels": [], "entities": [{"text": "Answer selection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9589619338512421}, {"text": "question answering", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.8136008381843567}]}, {"text": "Analyzing social data like answers given inside a forum is away to maximize the value of this type of knowledge source that is usually affected by a very noisy information due to out of topic spam, double posting, cross posting or other issues.", "labels": [], "entities": []}, {"text": "Recognizing useful posts from bad ones, and automatically detecting the main polarity of answers to a given question is away to treat an amount of data that otherwise might be difficult to handle.", "labels": [], "entities": []}, {"text": "A promising way to provide insight into these questions was brought forward as Shared Task #3 in the SemEval-2015 campaign for \"Answer Selection in Community Question Answering\" () for English and Arabic languages.", "labels": [], "entities": [{"text": "Answer Selection in Community Question Answering\"", "start_pos": 128, "end_pos": 177, "type": "TASK", "confidence": 0.799720823764801}]}, {"text": "In the Subtask A, each system is given a set of questions in which each one contains some data like posting date, author's Id, a set of comments, at least one, but usually more; then the participating the system has to classify comments as good, bad or potential according to their relevance with the question.", "labels": [], "entities": [{"text": "Id", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.47140592336654663}]}, {"text": "In Subtask B, a subset of these questions are predefined as yes/no questions, system has to classify them into yes, no or unsure classes based on the individual good answers.", "labels": [], "entities": []}, {"text": "We participate in this shared task (only in English) with a system composing several different features using a multiclass classifier.", "labels": [], "entities": []}, {"text": "We are interested in finding out whether similarity, machine translation evaluation metrics and task specific techniques could increase the accuracy of our system.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.8127086957295736}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9991076588630676}]}, {"text": "In this paper, we outline our method and present the results for the answer selection task; the paper is organized as follows: Section 2 presents the System Description, Section 3 describes the Experiment Settings, Section 4 reports the Evaluations, Section 5 is the Error Analysis and finally, Section 6 presents the Conclusions and Future Work.", "labels": [], "entities": [{"text": "answer selection task", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.9079743027687073}]}], "datasetContent": [{"text": "We also use evaluation metrics for machine translation as suggested in () for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7633123695850372}, {"text": "paraphrase recognition", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.939704567193985}, {"text": "Microsoft Research paraphrase corpus (MSRP)", "start_pos": 104, "end_pos": 147, "type": "DATASET", "confidence": 0.7706569944109235}]}, {"text": "In machine translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7907395660877228}]}, {"text": "We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system.", "labels": [], "entities": []}, {"text": "We use the latest version of METEOR) that finds alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.6941304802894592}]}, {"text": "We used the system as distributed on its website, using only the \"norm\" option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.", "labels": [], "entities": []}, {"text": "We compute the word alignment scores between questions and comments.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.5821027755737305}]}, {"text": "We use the machine learning toolkit WEKA () to obtain robust and efficient implementation of different classifiers, as well as to reduce develop time of the system.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.4692036509513855}]}, {"text": "For Subtask A, we build one model using all the features described in Section 2.", "labels": [], "entities": []}, {"text": "reports some experiments in which we select a good classifier to optimize both the Accuracy and F1-score of the system.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9995799660682678}, {"text": "F1-score", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9915362596511841}]}, {"text": "During the development, we select the default implementation \"1-against-all\" classification algorithm (with logistic regression) for both subtasks.", "labels": [], "entities": []}, {"text": "For Subtask B, we make some modifications to the system due to some important differences between two subtasks.", "labels": [], "entities": []}, {"text": "As the question classification depends on the quality of its comments, we substitute the spam filtering feature by the comments' labels from Subtask A system's output.", "labels": [], "entities": []}, {"text": "In order to examine this hypothesis, we firstly use the gold-standard labels of comments from Subtask A as a feature for the question classification in Subtask B. The high Accuracy and F1-score from this setting proves our hypothesis correct.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9995877146720886}, {"text": "F1-score", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9972430467605591}]}, {"text": "To avoid the overfitting, we again use only the label predictions from Subtask A as a feature for our Subtask B system.", "labels": [], "entities": []}, {"text": "shows that a precise output from Subtask A can significantly benefit the performance of Subtask B system.", "labels": [], "entities": []}, {"text": "As Subtask B does not focus on comment labeling, but question labeling, to achieve this purpose after classifying all comments as yes, no, unsure or Not Applicable, we simply aggregate comments of every question with a majority vote.", "labels": [], "entities": [{"text": "question labeling", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7360965609550476}]}, {"text": "We label a question as yes if the majority of its comments are classified as yes, the same for no; if there no major judgment of either yes or no, the question is classified as unsure.", "labels": [], "entities": []}, {"text": "We submit only one run for both subtasks (English language) using the \"1-against-all\" classification algorithms.", "labels": [], "entities": []}, {"text": "In Subtask A, we achieve good results, especially, we are ranked 4 th out of 12 teams in Accuracy.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9978196620941162}]}, {"text": "In Subtask B, as we only apply the simple approach \"majority vote\", the result is reasonable as expected.", "labels": [], "entities": []}, {"text": "shows our performance in both subtasks in regard to the best systems, both in Macro F1 and Accuracy measures.", "labels": [], "entities": [{"text": "F1", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.814253568649292}, {"text": "Accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9924740195274353}]}], "tableCaptions": [{"text": " Table 1: Result obtained using different classification algorithms for Subtask A (G good; B bad; D dialog; P potential;  NE not-English; O other; WM Weighted Mean) on Development dataset.", "labels": [], "entities": [{"text": "Development dataset", "start_pos": 168, "end_pos": 187, "type": "DATASET", "confidence": 0.9152612388134003}]}, {"text": " Table 2: Subtask B system performances on Development dataset.", "labels": [], "entities": [{"text": "Development dataset", "start_pos": 43, "end_pos": 62, "type": "DATASET", "confidence": 0.9146595597267151}]}, {"text": " Table 3: Evaluation Results on Subtasks A and B.", "labels": [], "entities": []}, {"text": " Table 4: Subtask A -Comparison with best system for 3-classes and 4-classes evaluation (G good; B bad; D dialog; P  potential; Macro F1).", "labels": [], "entities": []}, {"text": " Table 5: Subtask B -Comparison with best system.", "labels": [], "entities": []}]}