{"title": [{"text": "Resolving Discourse-Deictic Pronouns: A Two-Stage Approach to Do It", "labels": [], "entities": [{"text": "Resolving Discourse-Deictic Pronouns", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8543209632237753}]}], "abstractContent": [{"text": "Discourse deixis is a linguistic phenomenon in which pronouns have verbal or clausal, rather than nominal, antecedents.", "labels": [], "entities": []}, {"text": "Studies have estimated that between 5% and 10% of pronouns in non-conversational data are discourse deic-tic.", "labels": [], "entities": []}, {"text": "However, current coreference resolution systems ignore this phenomenon.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.9453081488609314}]}, {"text": "This paper presents an automatic system for the detection and resolution of discourse-deictic pronouns.", "labels": [], "entities": [{"text": "detection and resolution of discourse-deictic pronouns", "start_pos": 48, "end_pos": 102, "type": "TASK", "confidence": 0.8083213816086451}]}, {"text": "We introduce a two-step approach that first recognizes instances of discourse-deictic pronouns, and then resolves them to their verbal antecedent.", "labels": [], "entities": []}, {"text": "Both components rely on linguistically motivated features.", "labels": [], "entities": []}, {"text": "We evaluate the components in isolation and in combination with two state-of-the-art coreference re-solvers.", "labels": [], "entities": []}, {"text": "Results show that our system out-performs several baselines, including the only comparable discourse deixis system, and leads to small but statistically significant improvements over the full coreference resolution systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 192, "end_pos": 214, "type": "TASK", "confidence": 0.8511449992656708}]}, {"text": "An error analysis lays bare the need fora less strict evaluation of this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Coreference resolution is a central problem in Natural Language Processing with abroad range of applications such as summarization (, textual entailment (, information extraction, and dialogue systems.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9308119416236877}, {"text": "summarization", "start_pos": 117, "end_pos": 130, "type": "TASK", "confidence": 0.9899073839187622}, {"text": "information extraction", "start_pos": 156, "end_pos": 178, "type": "TASK", "confidence": 0.8063470721244812}]}, {"text": "Traditionally, the resolution of noun phrases has been the focus of coreference research.", "labels": [], "entities": [{"text": "resolution of noun phrases", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.884253740310669}]}, {"text": "However, NPs are not the only participants in coreference, since verbal or clausal mentions can also take part in coreference relations.", "labels": [], "entities": []}, {"text": "For example, consider: (1) The United States says it may invite Israeli and Palestinian negotiators to Washington.", "labels": [], "entities": []}, {"text": "(2) Without planning it in advance, they chose to settle here.", "labels": [], "entities": []}, {"text": "In (1), the antecedent of the pronoun is an NP, while in (2) the antecedent 1 is a clause 2.", "labels": [], "entities": []}, {"text": "Current state-of-the-art coreference resolution systems () focus on the former and ignore the latter cases.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.9445497393608093}]}, {"text": "Corpus studies across several languages have estimated that between 5% and 10% of pronouns in non-conversational data, and up to 20% in conversational, have verbal antecedents.", "labels": [], "entities": []}, {"text": "A coreference system that is able to handle discourse deixis will thus be more accurate, and benefit downstream applications.", "labels": [], "entities": []}, {"text": "In this paper we present an automatic system that processes discourse-deictic pronouns.", "labels": [], "entities": []}, {"text": "We resolve the three pronouns it, this and that, which can appear in linguistic contexts that reflect the phenomenon illustrated in (2).", "labels": [], "entities": []}, {"text": "Our system has a modular architecture consisting of two independent stages: classification and resolution.", "labels": [], "entities": [{"text": "classification", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.9565056562423706}]}, {"text": "The first stage classifies a pronoun as discourse deictic (or not), and the second stage resolves discourse-deictic pronouns to verbal antecedents.", "labels": [], "entities": []}, {"text": "Both stages use linguistically moti-vated features.", "labels": [], "entities": []}, {"text": "We first evaluate our system by measuring the performance of the detection and resolution components in isolation.", "labels": [], "entities": []}, {"text": "They outperform several baselines, including approach, which is the only other comparable discourse deixis system, to the best of our knowledge.", "labels": [], "entities": []}, {"text": "We also measure the impact of our system on two state-of-the-art coreference resolution systems).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.9040768444538116}]}, {"text": "The results show the benefits of stacking a discourse deixis engine on top of NP coreference resolution.", "labels": [], "entities": [{"text": "NP coreference resolution", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.7183473706245422}]}], "datasetContent": [{"text": "In this section we describe the setup for evaluating our system.", "labels": [], "entities": []}, {"text": "We perform all our experiments on the English section of the CoNLL-2012 corpus (, which is based on OntoNotes ( Given these annotations, we consider a pronoun to be discourse deictic if the preceding mention in its coreference cluster is verbal, or if it is the first mention in the cluster and the next one is verbal.", "labels": [], "entities": [{"text": "CoNLL-2012 corpus", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.8995394706726074}]}, {"text": "The distribution of potentially discourse-deictic pronouns (it, this and that) in the test set is summarized in.", "labels": [], "entities": []}, {"text": "For all our experiments we train, tune and test according to the CoNLL-2012 split of OntoNotes.", "labels": [], "entities": [{"text": "CoNLL-2012 split of OntoNotes", "start_pos": 65, "end_pos": 94, "type": "DATASET", "confidence": 0.9073837846517563}]}, {"text": "The gold analyses provided for the shared task are used for training, and the system analyses for development and testing.", "labels": [], "entities": []}, {"text": "We train the two components of our system separately.", "labels": [], "entities": []}, {"text": "For each of them, a maximum entropy model is learned on the train partition.", "labels": [], "entities": []}, {"text": "Feature selection and threshold tuning are performed by hill climbing on the development set.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7601952850818634}]}, {"text": "We use separate thresholds for it, this, and that, since their distributions in the corpus are quite different.", "labels": [], "entities": []}, {"text": "We perform two evaluations of our system: first classification and resolution are evaluated in isolation, and then both components are stacked on top of an NP coreference engine.", "labels": [], "entities": [{"text": "resolution", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.8925656676292419}]}, {"text": "For classification, we measure system performance on standard precision (P), recall (R) and F1 of correctly predicting whether a pronoun is discourse deictic or not.", "labels": [], "entities": [{"text": "standard precision (P)", "start_pos": 53, "end_pos": 75, "type": "METRIC", "confidence": 0.852384376525879}, {"text": "recall (R)", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9326042681932449}, {"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9991506338119507}]}, {"text": "For resolution, precision is computed as the fraction of predicted antecedents that are correct, and recall as the fraction of gold antecedents that are correctly predicted.", "labels": [], "entities": [{"text": "resolution", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9402127265930176}, {"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9996298551559448}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9994739890098572}]}, {"text": "To decouple the evaluation of both stages, we also include results with oracle classifications as input to the resolution stage.", "labels": [], "entities": []}, {"text": "Finally, we use the output of our system to extend the predictions of two state-of-the-art NP coreference systems: \u2022 BERKELEY (), a joint model for coreference resolution, named entity recognition, and entity linking.", "labels": [], "entities": [{"text": "BERKELEY", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9986063838005066}, {"text": "coreference resolution", "start_pos": 148, "end_pos": 170, "type": "TASK", "confidence": 0.9183396100997925}, {"text": "named entity recognition", "start_pos": 172, "end_pos": 196, "type": "TASK", "confidence": 0.6227858563264211}, {"text": "entity linking", "start_pos": 202, "end_pos": 216, "type": "TASK", "confidence": 0.7446398138999939}]}, {"text": "\u2022 HOTCOREF (), a latent-antecedent model which exploits nonlocal features via beam search.", "labels": [], "entities": [{"text": "HOTCOREF", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.7705179452896118}]}, {"text": "We only add our predictions for pronouns it, this, that that are output as singletons by the NP coreference system.", "labels": [], "entities": [{"text": "NP coreference", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.5452362596988678}]}, {"text": "We report the standard coreference measures on the combined outputs using the updated CoNLL scorer v7 ().", "labels": [], "entities": [{"text": "CoNLL scorer v7", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.8373658458391825}]}, {"text": "Here, the systems are evaluated on all nominal, pronominal, and verbal mentions.", "labels": [], "entities": []}, {"text": "The metrics include precision, recall and F1 for MUC, B and CEAF e , and the CoNLL metric, which is the arithmetic mean of the first three F1 scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9996381998062134}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9994823932647705}, {"text": "F1", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9983193278312683}, {"text": "MUC, B and CEAF e", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.5862661500771841}, {"text": "CoNLL metric", "start_pos": 77, "end_pos": 89, "type": "METRIC", "confidence": 0.9159718751907349}, {"text": "F1", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.968350350856781}]}], "tableCaptions": [{"text": " Table 2: Distribution of discourse-deictic pronouns in the  test set of the CoNLL-2012 English corpus.", "labels": [], "entities": [{"text": "CoNLL-2012 English corpus", "start_pos": 77, "end_pos": 102, "type": "DATASET", "confidence": 0.9670368234316508}]}, {"text": " Table 3: Classification evaluation (TWOSTAGE corresponds to our system).", "labels": [], "entities": [{"text": "Classification evaluation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.9370535016059875}, {"text": "TWOSTAGE", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9681486487388611}]}, {"text": " Table 4: Resolution evaluation with oracle classification (TWOSTAGE corresponds to our system).", "labels": [], "entities": [{"text": "Resolution evaluation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9319887757301331}, {"text": "TWOSTAGE", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9302642941474915}]}, {"text": " Table 5: Resolution evaluation with system classification (TWOSTAGE corresponds to our system).", "labels": [], "entities": [{"text": "Resolution evaluation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9355650246143341}, {"text": "system classification", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.6627222299575806}, {"text": "TWOSTAGE", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9337403178215027}]}, {"text": " Table 7: Distribution of errors.", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9228765368461609}, {"text": "errors", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.6847008466720581}]}, {"text": " Table 6: End-to-end coreference resolution evaluation (TWOSTAGE corresponds to our system). All differences be- tween the baseline system and TWOSTAGE are significant at the 1% level except for the B 3 F1 differences.", "labels": [], "entities": [{"text": "coreference resolution evaluation", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.9273360768953959}, {"text": "TWOSTAGE", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.8708208203315735}, {"text": "B 3 F1", "start_pos": 199, "end_pos": 205, "type": "METRIC", "confidence": 0.8231940269470215}]}]}