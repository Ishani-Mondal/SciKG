{"title": [{"text": "yiGou: A Semantic Text Similarity Computing System Based on SVM", "labels": [], "entities": [{"text": "Semantic Text Similarity Computing", "start_pos": 9, "end_pos": 43, "type": "TASK", "confidence": 0.7038433402776718}, {"text": "SVM", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.4242302179336548}]}], "abstractContent": [{"text": "This paper describes the yiGou system we developed to compute the semantic similarity of two English sentences, which we submitted to the SemEval 2015 Task 2 (English subtask).", "labels": [], "entities": [{"text": "SemEval 2015 Task 2", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.6509344279766083}]}, {"text": "The system uses a support vector machine model with literal similarity, shallow syntactic similarity, WordNet-based similarity and latent semantic similarity to predict the semantic similarity score of two short texts.", "labels": [], "entities": []}, {"text": "In our experiments, WordNet-based and LSA-based features performed better than other features.", "labels": [], "entities": [{"text": "WordNet-based", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.9253929257392883}]}, {"text": "Out of the 73 submitted runs, our two runs ranked 38 th and 42 th , with mean Pearson correlation 0.7114 and 0.6964 respectively.", "labels": [], "entities": [{"text": "Pearson correlation 0.7114", "start_pos": 78, "end_pos": 104, "type": "METRIC", "confidence": 0.9459702571233114}]}], "introductionContent": [{"text": "Semantic Text Similarity (STS) plays an important role in many Natural language processing tasks, such as Question Answering (), Machine Translation (, Automatic Summarization ( and Word Sense Disambiguation ().", "labels": [], "entities": [{"text": "Semantic Text Similarity (STS)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8020344277222952}, {"text": "Question Answering", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.8546472787857056}, {"text": "Machine Translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.8520383834838867}, {"text": "Automatic Summarization", "start_pos": 152, "end_pos": 175, "type": "TASK", "confidence": 0.6941466927528381}, {"text": "Word Sense Disambiguation", "start_pos": 182, "end_pos": 207, "type": "TASK", "confidence": 0.6273331046104431}]}, {"text": "Since STS is an essential challenge in NLP, that has attracted a significant amount of attention by the research community.", "labels": [], "entities": [{"text": "STS", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9516372084617615}]}, {"text": "SemEval has held tasks about STS for four years in a row, from which we can seethe importance and difficulty of this challenge.", "labels": [], "entities": []}, {"text": "Particularly, SemEval focuses on semantic similarity of short texts as a lot of researches about long texts have been done in past years and the demand of finding new methods to measure short texts similarity has become stronger in many new applications.", "labels": [], "entities": []}, {"text": "In this paper, we proposed a SVM-based solution to compute the semantic similarity between two sentences which is the goal of SemEval 2015 Task 2.", "labels": [], "entities": [{"text": "SemEval 2015 Task 2", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.8580745458602905}]}, {"text": "Knowledge-based and corpus-based features were involved in our solution.", "labels": [], "entities": []}, {"text": "We used the combination of the word similarity to estimate sentence similarity.", "labels": [], "entities": []}, {"text": "And the training data of) was used to train our model.", "labels": [], "entities": []}, {"text": "In our experiments, WordNet-based and LSA-based features performed better than other features.", "labels": [], "entities": [{"text": "WordNet-based", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.9253929257392883}]}, {"text": "Out of the 73 submitted runs, our two runs ranked 38 th and 42 th , with mean Pearson correlation 0.7114 and 0.6964 respectively.", "labels": [], "entities": [{"text": "Pearson correlation 0.7114", "start_pos": 78, "end_pos": 104, "type": "METRIC", "confidence": 0.9459702571233114}]}, {"text": "The evaluation results showed that our solution has good generalization ability on the test dataset of SemEval 2015 which is very different from our training set in terms of the sources of the sentences.", "labels": [], "entities": [{"text": "test dataset of SemEval 2015", "start_pos": 87, "end_pos": 115, "type": "DATASET", "confidence": 0.7548020839691162}]}, {"text": "Some of the relatively new technologies such as Word2Vec ( and Sentence2Vec () are potential methods to represent sentences and will be included in our further works.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9437573552131653}]}], "datasetContent": [{"text": "Due to the limitation of the time, in our submitted system, we trained Support Vector Regression (SVR) models using Scikit-learn toolkit (Pedrego Parameter setting in our models. sa et al., 2011).", "labels": [], "entities": [{"text": "Support Vector Regression (", "start_pos": 71, "end_pos": 98, "type": "METRIC", "confidence": 0.8625540733337402}]}, {"text": "shows the features used in our submitted models.", "labels": [], "entities": []}, {"text": "The results with different feature combinations on the test set of SemEval 2012 are shown in. is our parameter settings.", "labels": [], "entities": [{"text": "test set of SemEval 2012", "start_pos": 55, "end_pos": 79, "type": "DATASET", "confidence": 0.6640735507011414}]}, {"text": "The performance of the best system in SemEval 2012 is 0.67 (Mean) with 19 features, and our best performance is 0.596 (Mean) with 7 features.", "labels": [], "entities": [{"text": "SemEval 2012", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.6814709007740021}]}, {"text": "In SemEval 2015, out of the 73 submitted runs, our two runs ranked 38th and 42th (with mean Pearson correlation 0.7114 and 0.6964 respectively).", "labels": [], "entities": [{"text": "Pearson correlation 0.7114", "start_pos": 92, "end_pos": 118, "type": "METRIC", "confidence": 0.9597398638725281}]}, {"text": "And the best performance in 2015 is 0.8015.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 Results of comparing the importance of res_similarity and path_similarity on test set of SemEval 2012. The  WN-based sim included both res_similarity and path_similarity.", "labels": [], "entities": [{"text": "SemEval 2012", "start_pos": 98, "end_pos": 110, "type": "TASK", "confidence": 0.6452836096286774}]}, {"text": " Table 2 Results of using different corpus in res_similarity on test set of SemEval 2012.", "labels": [], "entities": [{"text": "SemEval 2012", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.7116102427244186}]}, {"text": " Table 1. As we can see from the table,  path_similarity contributes more to our overall  performance than res_similarity. According to the  definition of res_similarity, we changed the corpus  to find out the influence of the corpus on our over-", "labels": [], "entities": []}, {"text": " Table 4 Results of SVR on SemEval 2012 test set with different feature combinations.", "labels": [], "entities": [{"text": "SemEval 2012 test set", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.8335124254226685}]}, {"text": " Table 3 All features we used in our submitted model.", "labels": [], "entities": []}, {"text": " Table 5 Parameter setting in our models.", "labels": [], "entities": [{"text": "Parameter setting", "start_pos": 9, "end_pos": 26, "type": "METRIC", "confidence": 0.9567509293556213}]}]}