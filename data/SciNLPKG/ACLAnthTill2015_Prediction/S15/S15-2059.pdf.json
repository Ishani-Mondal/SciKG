{"title": [], "abstractContent": [{"text": "This paper describes our system for SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking.", "labels": [], "entities": [{"text": "SemEval-2015 Task 13", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.9051191409428915}, {"text": "Multilingual All-Words Sense Disambiguation", "start_pos": 58, "end_pos": 101, "type": "TASK", "confidence": 0.6322338953614235}, {"text": "Entity Linking", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7262223362922668}]}, {"text": "We have participated with our system in the sub-task which aims at monolingual all-words disam-biguation and entity linking.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 109, "end_pos": 123, "type": "TASK", "confidence": 0.7147634476423264}]}, {"text": "Aside from system description, we pay closer attention to the evaluation of system outputs.", "labels": [], "entities": [{"text": "system description", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7428820133209229}]}], "introductionContent": [{"text": "Word sense disambiguation (WSD, i.e. picking the right sense fora given word from a fixed inventory) and entity linking (EL, i.e. identifying a particular named entity listed in a database given its mention in a text) are among the fashionable tasks in computational linguistics and natural language processing these days.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6452320714791616}, {"text": "entity linking (EL, i.e. identifying a particular named entity listed in a database given its mention in a text)", "start_pos": 105, "end_pos": 217, "type": "TASK", "confidence": 0.5648318515582518}, {"text": "natural language processing", "start_pos": 283, "end_pos": 310, "type": "TASK", "confidence": 0.6886723836263021}]}, {"text": "WSD has been, after some debate, shown to help machine translation, other applications include knowledge discovery or machine reading in general (;).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7989376485347748}, {"text": "knowledge discovery", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.8046008348464966}, {"text": "machine reading", "start_pos": 118, "end_pos": 133, "type": "TASK", "confidence": 0.7613795399665833}]}, {"text": "WSD and EL are usually applied with large and rich context available), but the arguably harder setting of short context has a wider range of applications, including text similarity measurements), Named Entities Extraction and Named Entities Disambiguation ( or handling data from social networks, such as attempts to translate tweets.", "labels": [], "entities": [{"text": "Named Entities Disambiguation", "start_pos": 226, "end_pos": 255, "type": "TASK", "confidence": 0.6997936964035034}]}, {"text": "Our attempt at WSD and EL can be classified as unsupervised, corpus-based and our implementation relies on an information retrieval tool.", "labels": [], "entities": [{"text": "WSD", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8220470547676086}]}, {"text": "We do not take longer context into account.", "labels": [], "entities": []}], "datasetContent": [{"text": "Having thoroughly reviewed the official scoring script, we find some of its features unusual: \u2022 The precision of a system is not penalized for spans, which don't occur in the golden set.", "labels": [], "entities": [{"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9993119239807129}]}, {"text": "\u2022 The recall should consider only to what extent the expected answers are covered by the system's answers.", "labels": [], "entities": [{"text": "recall", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9988686442375183}]}, {"text": "The official scoring script reduces the recall score for any 'unexpected' answers.", "labels": [], "entities": [{"text": "recall score", "start_pos": 40, "end_pos": 52, "type": "METRIC", "confidence": 0.9932835400104523}]}, {"text": "\u2022 An exact match in span is needed to give any credit to the system answer.", "labels": [], "entities": [{"text": "span", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.848240315914154}]}, {"text": "We thus propose a slightly different evaluation procedure and apply it to all submitted systems.", "labels": [], "entities": []}, {"text": "For future shared tasks, we recommend: \u2022 Define precision and recall to better match the common meaning, e.g. as in our proposal.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9981746673583984}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9992693066596985}]}, {"text": "\u2022 Preserve letter casein IDs to avoid ambiguity in Wikipedia to BabelNet mapping.", "labels": [], "entities": [{"text": "Wikipedia to BabelNet mapping", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.6618860214948654}]}, {"text": "\u2022 Use only one repertoire of IDs in the gold set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: All submissions evaluated on all domains using various official and our scorings.", "labels": [], "entities": []}, {"text": " Table 2: Our system outputs.", "labels": [], "entities": []}]}