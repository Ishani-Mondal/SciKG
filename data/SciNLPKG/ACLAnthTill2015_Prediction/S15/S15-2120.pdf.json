{"title": [{"text": "DsUniPi: An SVM-based Approach for Sentiment Analysis of Figurative Language on Twitter", "labels": [], "entities": [{"text": "Sentiment Analysis of Figurative Language", "start_pos": 35, "end_pos": 76, "type": "TASK", "confidence": 0.9210252881050109}]}], "abstractContent": [{"text": "The DsUniPi team participated in the SemEval 2015 Task#11: Sentiment Analysis of Figurative Language in Twitter.", "labels": [], "entities": [{"text": "SemEval 2015 Task#11", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8738176226615906}, {"text": "Sentiment Analysis of Figurative Language in Twitter", "start_pos": 59, "end_pos": 111, "type": "TASK", "confidence": 0.898764933858599}]}, {"text": "The proposed approach employs syntactical and morphological features, which indicate sentiment polarity in both figurative and non-figurative tweets.", "labels": [], "entities": []}, {"text": "These features were combined with others that indicate presence of figurative language in order to predict a fine-grained sentiment score.", "labels": [], "entities": []}, {"text": "The method is supervised and makes use of struc-tured knowledge resources, such as Senti-WordNet sentiment lexicon for assigning sentiment score to words and WordNet for calculating word similarity.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.9460420608520508}]}, {"text": "We have experimented with different classification algorithms (Na\u00efve Bayes, Decision trees, and SVM), and the best results were achieved by an SVM clas-sifier with linear kernel.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis on figurative speech is a challenging task that becomes even more difficult on short social-media related text.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9657878875732422}]}, {"text": "Tweet text can be rich in irony that is either stated with hashtags explicitly (such as #irony) or implied.", "labels": [], "entities": []}, {"text": "Identifying the underlying sentiment of such text is challenging due to its restricted size and features such as use of abbreviations and slang.", "labels": [], "entities": []}, {"text": "Consequently, assigning positive or negative polarity is quite a difficult task.", "labels": [], "entities": [{"text": "assigning positive or negative polarity", "start_pos": 14, "end_pos": 53, "type": "TASK", "confidence": 0.8711435079574585}]}, {"text": "The actual meaning can be very different than what is stated, since, for example, in ironic language what is said can be the opposite of what it is meant.", "labels": [], "entities": []}, {"text": "To address this challenge, we propose a system for sentiment analysis of figurative language, which relies on feature selection and trains a classifier to predict the label of a tweet.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9570684731006622}]}, {"text": "Given a labelled trial set, the objective of the system is to correctly determine how positive, negative or neutral a tweet is considered to be on a scale of].", "labels": [], "entities": []}], "datasetContent": [{"text": "The SemEval data set consists of 9000 tweets that are rich in figurative language and stemmed from user-generated tags, such as \"#sarcasm\" and \"#iro-ny\".", "labels": [], "entities": [{"text": "SemEval data set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8839527169863383}]}, {"text": "There is a 90-10 split for trial and test data.", "labels": [], "entities": []}, {"text": "We retrieved 8529 tweets in total, 7606 from the trial set and 923 from the test set.", "labels": [], "entities": []}, {"text": "Out of these data sets, positive tweets in total are 8,2%, negative tweets are 85,2% and neutral 6,6%.", "labels": [], "entities": []}, {"text": "We experimented by incrementally adding features, and trying different classifiers.", "labels": [], "entities": []}, {"text": "The results of the features that seem to contribute most were used to make the prediction with which the system participated in the task and are the ones marked with (*) in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The results of the classifiers used on the initial  test data set (t) and the final (f), with the selected fea- tures of the final submission.", "labels": [], "entities": []}, {"text": " Table 3: The final results by category.", "labels": [], "entities": []}]}