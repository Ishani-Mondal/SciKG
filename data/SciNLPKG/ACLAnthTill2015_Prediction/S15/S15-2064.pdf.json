{"title": [{"text": "UTU: Adapting Biomedical Event Extraction System to Disorder Attribute Detection", "labels": [], "entities": [{"text": "UTU", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7261770963668823}, {"text": "Adapting Biomedical Event Extraction", "start_pos": 5, "end_pos": 41, "type": "TASK", "confidence": 0.8857902735471725}, {"text": "Disorder Attribute Detection", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.7563632925351461}]}], "abstractContent": [{"text": "In this paper we describe our entry to the Se-mEval 2015 clinical text analysis task.", "labels": [], "entities": [{"text": "Se-mEval 2015 clinical text analysis task", "start_pos": 43, "end_pos": 84, "type": "TASK", "confidence": 0.7382815976937612}]}, {"text": "We participated only in the disorder attribute detection task 2a.", "labels": [], "entities": [{"text": "disorder attribute detection task", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.6485961079597473}]}, {"text": "Our main goal was to assess how well an information extraction system originally developed fora different task and domain can be utilized in this task.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7214852869510651}]}, {"text": "Our system, based on SVM and CRF classifiers, showed promising results, placing 3rd out of 6 participants in this task with performance of 0.857 measured in weighted accuracy, the official evaluation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.8576240539550781}]}], "introductionContent": [{"text": "SemEval 2015 introduced anew subtask for the clinical text analysis track focusing on disorder mention attribute detection.", "labels": [], "entities": [{"text": "disorder mention attribute detection", "start_pos": 86, "end_pos": 122, "type": "TASK", "confidence": 0.7169714421033859}]}, {"text": "These attributes describe the relevant information extracted from the textual context of the given disease mention, such as the severity or body location of the disease.", "labels": [], "entities": []}, {"text": "The attributes were grouped into 9 separate categories, each with a predefined set of valid attribute classes.", "labels": [], "entities": []}, {"text": "The task was defined as a template filling task where the textual cue words for the attributes have to be first identified and then normalized to the correct class.", "labels": [], "entities": [{"text": "template filling task", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7957162658373514}]}, {"text": "Similar task with slightly different definition has previously been organized as part of the ShARe/CLEF eHealth shared task).", "labels": [], "entities": [{"text": "ShARe/CLEF eHealth shared task", "start_pos": 93, "end_pos": 123, "type": "TASK", "confidence": 0.6120976507663727}]}, {"text": "Due to time limitations we participated only in the task 2a in which the gold standard disorder mentions were given and only the attribute values had to be predicted.", "labels": [], "entities": []}, {"text": "Our main motivation for this years entry was to evaluate the performance of an existing information extraction system, TEES, previously developed fora different domain and to assess how easily it can be adapted to anew task.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.728651762008667}, {"text": "TEES", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.4996870458126068}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Official test set results for our 3 submissions and  the other 5 participating teams. Only the best runs mea- sured in weighted accuracy are shown for other teams.  WA = weighted accuracy, A = non-weighted accuracy.", "labels": [], "entities": [{"text": "mea- sured", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.6484393874804179}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.922248899936676}, {"text": "WA", "start_pos": 175, "end_pos": 177, "type": "METRIC", "confidence": 0.997083842754364}, {"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.8134254217147827}, {"text": "A", "start_pos": 199, "end_pos": 200, "type": "METRIC", "confidence": 0.9943822622299194}, {"text": "accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.9649228453636169}]}, {"text": " Table 2: Performance of our system in each attribute category compared to the best performing system. Run 3 devel  shows our best results for the development set evaluated with the evaluation tool provided by the organizers.", "labels": [], "entities": []}]}