{"title": [{"text": "SemEval-2015 Task 6: Clinical TempEval", "labels": [], "entities": [{"text": "SemEval-2015 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8818802237510681}]}], "abstractContent": [{"text": "Clinical TempEval 2015 brought the temporal information extraction tasks of past Temp-Eval campaigns to the clinical domain.", "labels": [], "entities": [{"text": "temporal information extraction", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.6769991318384806}]}, {"text": "Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification.", "labels": [], "entities": [{"text": "time expression identification", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.6469499965508779}, {"text": "event expression identification", "start_pos": 83, "end_pos": 114, "type": "TASK", "confidence": 0.6657947897911072}, {"text": "temporal relation identification", "start_pos": 119, "end_pos": 151, "type": "TASK", "confidence": 0.6239589750766754}]}, {"text": "Participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain.", "labels": [], "entities": [{"text": "TimeML", "start_pos": 152, "end_pos": 158, "type": "DATASET", "confidence": 0.9425370693206787}]}, {"text": "Three teams submitted a total of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations.", "labels": [], "entities": []}], "introductionContent": [{"text": "The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (.", "labels": [], "entities": [{"text": "temporal information extraction", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.6133246521155039}]}, {"text": "Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations.", "labels": [], "entities": []}, {"text": "However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles.", "labels": [], "entities": []}, {"text": "Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic.", "labels": [], "entities": [{"text": "temporal information extraction", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.7393200397491455}]}, {"text": "This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task (, and broadens our understanding of the language of time beyond newswire expressions and structure.", "labels": [], "entities": [{"text": "temporal information extraction", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.6689014931519827}]}, {"text": "Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, reliable and repeatable evaluation.", "labels": [], "entities": []}, {"text": "Participating systems are expected to take as input raw text such as: April 23, 2014: The patient did not have any postoperative bleeding so we will resume chemotherapy with a larger bolus on Friday even if there is slight nausea.", "labels": [], "entities": []}, {"text": "And output annotations over the text that capture the following kinds of information: That is, the systems should identify the time expressions, event expressions, attributes of those expressions, and temporal relations between them.", "labels": [], "entities": []}], "datasetContent": [{"text": "All of the tasks were evaluated using the standard metrics of precision (P ), recall (R) and F 1 : where S is the set of items predicted by the system and H is the set of items manually annotated by the humans.", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.9372112303972244}, {"text": "recall (R)", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.961150586605072}, {"text": "F 1", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9928084313869476}]}, {"text": "Applying these metrics to the tasks only requires a definition of what is considered an \"item\" for each task.", "labels": [], "entities": []}, {"text": "\u2022 For evaluating the spans of event expressions or time expressions, items were tuples of (begin, end) character offsets.", "labels": [], "entities": []}, {"text": "Thus, systems only received credit for identifying events and times with exactly the same character offsets as the manually annotated ones.", "labels": [], "entities": []}, {"text": "\u2022 For evaluating the attributes of event expressions or time expressions -Class, Contextual Modality, Degree, Polarity and Type -items were tuples of (begin, end, value) where begin and end are character offsets and value is the value that was given to the relevant attribute.", "labels": [], "entities": [{"text": "Degree", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9654521346092224}]}, {"text": "Thus, systems only received credit for an event (or time) attribute if they both found an event (or time) with the correct character offsets and then assigned the correct value for that attribute.", "labels": [], "entities": []}, {"text": "\u2022 For relations between events and the document creation time, items were tuples of (begin, end, value), just as if it were an event attribute.", "labels": [], "entities": []}, {"text": "Thus, systems only received credit if they found a correct event and assigned the correct relation (BEFORE, OVERLAP, BEFORE-OVERLAP or AFTER) between that event and the document creation time.", "labels": [], "entities": [{"text": "BEFORE", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9939462542533875}, {"text": "OVERLAP", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.771341860294342}, {"text": "BEFORE-OVERLAP", "start_pos": 117, "end_pos": 131, "type": "METRIC", "confidence": 0.9924155473709106}, {"text": "AFTER)", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9278391003608704}]}, {"text": "Note that in the second phase of the evaluation, when manual event annotations were given as input, precision, recall and F 1 are all equivalent to standard accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9997667670249939}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9996733665466309}, {"text": "F 1", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.990898996591568}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9971752166748047}]}, {"text": "\u2022 For narrative container relations, items were tuples of ((begin 1 , end 1 ), (begin 2 , end 2 )), where the begins and ends corresponded to the character offsets of the events or times participating in the relation.", "labels": [], "entities": []}, {"text": "Thus, systems only received credit fora narrative container relation if they found both events/times and correctly assigned a CONTAINS relation between them.", "labels": [], "entities": []}, {"text": "For attributes, an additional metric measures how accurately a system predicts the attribute values on just those events or times that the system predicted.", "labels": [], "entities": []}, {"text": "The goal here is to allow a comparison across systems for assigning attribute values, even when different systems produce very different numbers of events and times.", "labels": [], "entities": []}, {"text": "This is calculated by dividing the F 1 on the attribute by the F 1 on identifying the spans: For the narrative container relations, additional metrics were included that took into account temporal closure, where additional relations can be deterministically inferred from other relations (e.g., A CON-808 TAINS B and B CONTAINS C, so A CONTAINS C): These measures take the approach of prior work (UzZaman and Allen, 2011) and TempEval 2013, following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of humanannotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure).", "labels": [], "entities": [{"text": "F 1", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9768888354301453}, {"text": "A CON-808 TAINS B", "start_pos": 295, "end_pos": 312, "type": "METRIC", "confidence": 0.812571719288826}, {"text": "precision", "start_pos": 470, "end_pos": 479, "type": "METRIC", "confidence": 0.9984855055809021}, {"text": "recall", "start_pos": 687, "end_pos": 693, "type": "METRIC", "confidence": 0.9946622252464294}]}], "tableCaptions": [{"text": " Table 1: Number of documents, event expressions, time  expressions and narrative container relations in the train- ing and development portions of the THYME data. (Dev  is the Clinical TempEval 2015 test set.)", "labels": [], "entities": [{"text": "THYME data", "start_pos": 152, "end_pos": 162, "type": "DATASET", "confidence": 0.9022992849349976}, {"text": "Clinical TempEval 2015 test set", "start_pos": 177, "end_pos": 208, "type": "DATASET", "confidence": 0.6892427623271942}]}, {"text": " Table 2: System performance and annotator agreement on TIMEX3 tasks: identifying the time expression's span  (character offsets) and class (DATE, TIME, DURATION, QUANTIFIER, PREPOSTEXP or SET). The best system score  from each column is in bold. The three BluLab runs are combined because they all have identical performance (since  they only differ in their approach to narrative container relations).", "labels": [], "entities": [{"text": "DATE", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9320615530014038}, {"text": "TIME", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.7389105558395386}, {"text": "DURATION", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.7384840250015259}, {"text": "SET", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9553098082542419}]}, {"text": " Table 3: System performance and annotator agreement on EVENT tasks: identifying the event expression's span  (character offsets), contextual modality (ACTUAL, HYPOTHETICAL, HEDGED or GENERIC), degree (MOST, LITTLE or  N/A), polarity (POS or NEG) and type (ASPECTUAL, EVIDENTIAL or N/A). The best system score from each column  is in bold.", "labels": [], "entities": []}, {"text": " Table 4: System performance and annotator agreement on temporal relation tasks: identifying relations between events  and the document creation time (DOCTIMEREL), and identifying narrative container relations (CONTAINS). The best  system score from each column is in bold.", "labels": [], "entities": []}]}