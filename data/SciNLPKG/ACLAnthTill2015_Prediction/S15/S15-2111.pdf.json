{"title": [{"text": "UIR-PKU: Twitter-OpinMiner System for Sentiment Analysis in Twitter at SemEval 2015", "labels": [], "entities": [{"text": "UIR-PKU", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8217371106147766}, {"text": "Sentiment Analysis", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.934482604265213}]}], "abstractContent": [{"text": "Microblogs are considered as We-Media information with many real-time opinions.", "labels": [], "entities": []}, {"text": "This paper presents a Twitter-OpinMiner system for Twitter sentiment analysis evaluation at SemEval 2015.", "labels": [], "entities": [{"text": "Twitter sentiment analysis evaluation", "start_pos": 51, "end_pos": 88, "type": "TASK", "confidence": 0.723945826292038}]}, {"text": "Our approach stems from two different angles: topic detection for discovering the sentiment distribution on different topics and sentiment analysis based on a variety of features.", "labels": [], "entities": [{"text": "topic detection", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7840798795223236}, {"text": "sentiment analysis", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.904007613658905}]}, {"text": "Moreover, we also implemented intra-sentence discourse relations for polarity identification.", "labels": [], "entities": [{"text": "polarity identification", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.7216620594263077}]}, {"text": "We divided the discourse relations into 4 predefined categories, including continuation , contrast, condition, and cause.", "labels": [], "entities": []}, {"text": "These relations could facilitate us to eliminate polarity ambiguities in compound sentences where both positive and negative sentiments are appearing.", "labels": [], "entities": []}, {"text": "Based on the SemEval 2014 and SemEval 2015 Twitter sentiment analysis task datasets, the experimental results show that the performance of Twit-ter-OpinMiner could effectively recognize opinionated messages and identify the polarities.", "labels": [], "entities": [{"text": "SemEval 2015 Twitter sentiment analysis task datasets", "start_pos": 30, "end_pos": 83, "type": "DATASET", "confidence": 0.6978344065802438}]}], "introductionContent": [{"text": "This year comes the third edition of SemEval Twitter sentiment analysis task consisting of new genres, including topic-based polarity classification, trends detection towards a topic, and the sentimental strength of association of terms ().", "labels": [], "entities": [{"text": "SemEval Twitter sentiment analysis task", "start_pos": 37, "end_pos": 76, "type": "TASK", "confidence": 0.9229221105575561}, {"text": "topic-based polarity classification", "start_pos": 113, "end_pos": 148, "type": "TASK", "confidence": 0.6845578451951345}]}], "datasetContent": [{"text": "We trained a SVM classifier on 9,912 annotated tweets in the training set and 1,654 in the development set).", "labels": [], "entities": []}, {"text": "We used the same evaluation metrics with SemEval 2013, including the macroaveraged F-score of the positive and negative classes.", "labels": [], "entities": [{"text": "SemEval 2013", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.7109816074371338}, {"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.8521257638931274}]}, {"text": "The experimental results obtained by our system on the training set (ten-fold cross validation), development set, and test sets on Twitter 2013 were shown in where the baseline was achieved by using the formal text features as well as twitterspecific features.", "labels": [], "entities": []}, {"text": "Since the effectiveness of these two types of features were analyzed in, we mainly evaluated the effectiveness of other features.", "labels": [], "entities": []}, {"text": "showed that the most effective feature on Twitter 2013 dataset turned out to be the word embedding features: they provided gains of about 7%.", "labels": [], "entities": [{"text": "Twitter 2013 dataset", "start_pos": 42, "end_pos": 62, "type": "DATASET", "confidence": 0.9340177973111471}]}, {"text": "For LDA, we set the numbers of topic from 10 to 100, and found it could achieve best performance when equaling 50.", "labels": [], "entities": [{"text": "LDA", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8582180142402649}]}, {"text": "We then constructed the sentiment distribution among 50 topics for the further evaluation.", "labels": [], "entities": []}, {"text": "Besides, we also investigated the effectiveness of discourse features on compound sentences, and the statistics were shown in.", "labels": [], "entities": []}, {"text": "By adopting discourse features, around 59% sentences with discourse relations were identified.", "labels": [], "entities": []}, {"text": "Among these four types of relations, better performance were achieved on cause and condition relations.", "labels": [], "entities": []}, {"text": "Especially for the sentences with condition relation, they were all classified correctly.", "labels": [], "entities": []}, {"text": "It is because that more cue-phrase of cause and condition relations were used to explicitly denote the discourse relations in tweets, but more likely use context to imply contrast and continuation relations.", "labels": [], "entities": []}, {"text": "showed the evaluation results in SemEval 2015 Task 10.", "labels": [], "entities": [{"text": "SemEval 2015 Task 10", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.8172757029533386}]}, {"text": "Compared with the best run in, our system achieved comparable results on Twitter sentiment analysis and better performance on the evaluation of sarcasm.", "labels": [], "entities": [{"text": "Twitter sentiment analysis", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.6165954768657684}, {"text": "evaluation of sarcasm", "start_pos": 130, "end_pos": 151, "type": "TASK", "confidence": 0.6857696771621704}]}, {"text": "In fact, many sarcasm are likely expressed in ironic, hence most feature types are ineffective for this case.", "labels": [], "entities": []}, {"text": "In our system, we also used the features of topical sentiment distribution, which assumed the polarity of sarcasm tweet the same with non-sarcasm tweets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Experimental results on Twitter 2013 dataset.", "labels": [], "entities": [{"text": "Twitter 2013 dataset", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.9645150899887085}]}, {"text": " Table 5. Experimental results of progress test on Average F-value.", "labels": [], "entities": [{"text": "Average", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9561669826507568}, {"text": "F-value", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.8037347793579102}]}, {"text": " Table 6. Distribution of discourse relations and  the contribution in the evaluation.  Discourse  Relation", "labels": [], "entities": [{"text": "Discourse  Relation", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.6818088591098785}]}]}