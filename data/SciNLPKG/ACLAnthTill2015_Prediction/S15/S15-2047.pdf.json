{"title": [{"text": "SemEval-2015 Task 3: Answer Selection in Community Question Answering", "labels": [], "entities": [{"text": "SemEval-2015 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8661310076713562}, {"text": "Answer Selection in Community Question Answering", "start_pos": 21, "end_pos": 69, "type": "TASK", "confidence": 0.6804897040128708}]}], "abstractContent": [{"text": "Community Question Answering (cQA) provides new interesting research directions to the traditional Question Answering (QA) field, e.g., the exploitation of the interaction between users and the structure of related posts.", "labels": [], "entities": [{"text": "Community Question Answering (cQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7347170313199362}, {"text": "Question Answering (QA)", "start_pos": 99, "end_pos": 122, "type": "TASK", "confidence": 0.842633593082428}]}, {"text": "In this context, we organized SemEval-2015 Task 3 on Answer Selection in cQA, which included two subtasks: (a) classifying answers as good, bad, or potentially relevant with respect to the question, and (b) answering a YES/NO question with yes, no, or unsure, based on the list of all answers.", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.7896328866481781}, {"text": "cQA", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8515976071357727}, {"text": "YES", "start_pos": 219, "end_pos": 222, "type": "METRIC", "confidence": 0.8418838977813721}]}, {"text": "We set subtask A for Arabic and English on two relatively different cQA domains, i.e., the Qatar Living website for English, and a Quran-related website for Arabic.", "labels": [], "entities": [{"text": "Qatar Living website", "start_pos": 91, "end_pos": 111, "type": "DATASET", "confidence": 0.946861724058787}]}, {"text": "We used crowdsourcing on Amazon Mechanical Turk to label a large English training dataset, which we released to the research community.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 25, "end_pos": 47, "type": "DATASET", "confidence": 0.9000979661941528}, {"text": "English training dataset", "start_pos": 65, "end_pos": 89, "type": "DATASET", "confidence": 0.5779669086138407}]}, {"text": "Thirteen teams participated in the challenge with a total of 61 submissions: 24 primary and 37 contrastive.", "labels": [], "entities": []}, {"text": "The best systems achieved an official score (macro-averaged F 1) of 57.19 and 63.7 for the English subtasks A and B, and 78.55 for the Arabic subtask A.", "labels": [], "entities": [{"text": "official score (macro-averaged F 1)", "start_pos": 29, "end_pos": 64, "type": "METRIC", "confidence": 0.7478497837271009}]}], "introductionContent": [{"text": "Many social activities on the Web, e.g., in forums and social networks, are accomplished by means of the community Question Answering (cQA) paradigm.", "labels": [], "entities": [{"text": "Question Answering (cQA)", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.8002063810825348}]}, {"text": "User interaction in this context is seldom moderated, is rather open, and thus has little restrictions, if any, on who can post and who can answer a question.", "labels": [], "entities": []}, {"text": "On the positive side, this means that one can freely ask a question and expect some good, honest answers.", "labels": [], "entities": []}, {"text": "On the negative side, it takes efforts to go through all possible answers and to make sense of them.", "labels": [], "entities": []}, {"text": "It is often the case that many answers are only loosely related to the actual question, and some even change the topic.", "labels": [], "entities": []}, {"text": "It is also not unusual fora question to have hundreds of answers, the vast majority of which would not satisfy a user's information needs; thus, finding the desired information in along list of answers might be very time-consuming.", "labels": [], "entities": []}, {"text": "In our SemEval-2015 Task 3, we proposed two subtasks.", "labels": [], "entities": [{"text": "SemEval-2015 Task 3", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.772000769774119}]}, {"text": "First, subtask A asks for identifying the posts in the answer thread that answer the question well vs. those that can be potentially useful to the user (e.g., because they can help educate him/her on the subject) vs. those that are just bad or useless.", "labels": [], "entities": []}, {"text": "This subtask goes in the direction of automating the answer search problem that we discussed above, and we offered it in two languages: English and Arabic.", "labels": [], "entities": [{"text": "answer search problem", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.8719671964645386}]}, {"text": "Second, for the special case of YES/NO questions, we propose an extreme summarization exercise (subtask B), which aims to produce a simple YES/NO overall answer, considering all good answers to the questions (according to subtask A).", "labels": [], "entities": [{"text": "YES/NO questions", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.6289286017417908}]}, {"text": "For English, the two subtasks are built on a particular application scenario of cQA, based on the Qatar Living forum.", "labels": [], "entities": [{"text": "Qatar Living forum", "start_pos": 98, "end_pos": 116, "type": "DATASET", "confidence": 0.9597528775533041}]}, {"text": "However, we decoupled the tasks from the Information Retrieval component in order to facilitate participation, and to focus on aspects that are relevant for the SemEval community, namely on learning the relationship between two pieces of text.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7304421663284302}]}, {"text": "Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see.", "labels": [], "entities": [{"text": "passage reranking", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7646257877349854}]}, {"text": "In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.905670702457428}]}, {"text": "For instance, proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; developed a probabilistic model to learn tree-edit operations on dependency parse trees; and applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers.", "labels": [], "entities": []}, {"text": "One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in.", "labels": [], "entities": []}, {"text": "Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.9027328789234161}, {"text": "textual entailment", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.6883310526609421}]}, {"text": "For Arabic, we also made use of areal cQA portal, the Fatwa website, 3 where questions about Islam are posed by regular users and are answered by knowledgeable scholars.", "labels": [], "entities": []}, {"text": "For subtask A, we used a setup similar to that for English, but this time each question had exactly one correct answer among the candidate answers (see Section 3 for detail); we did not offer subtask B for Arabic.", "labels": [], "entities": []}, {"text": "Overall for the task, we needed manual annotations in two different languages and for two domains.", "labels": [], "entities": []}, {"text": "For English, we built the Qatar Living datasets as a joint effort between MIT and the Qatar Computing Research Institute, co-organizers of the task, using Amazon's Mechanical Turk to recruit human annotators.", "labels": [], "entities": [{"text": "Qatar Living datasets", "start_pos": 26, "end_pos": 47, "type": "DATASET", "confidence": 0.928841213385264}]}, {"text": "For Arabic, we built the dataset automatically from the data available in the Fatwa website, without the need for any manual annotation.", "labels": [], "entities": [{"text": "Fatwa website", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9563360214233398}]}, {"text": "We made all datasets publicly available, i.e., also usable beyond SemEval.", "labels": [], "entities": []}, {"text": "Our SemEval task attracted 13 teams, who submitted a total of 61 runs.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.8621000647544861}]}, {"text": "The participants mainly focused on defining new features that go beyond question-answer similarity, e.g., author-and userbased, and spent less time on the design of complex machine learning approaches.", "labels": [], "entities": []}, {"text": "Indeed, most systems used multi-class classifiers such as MaxEnt and SVM, but some used regression.", "labels": [], "entities": []}, {"text": "Overall, almost all submissions managed to outperform the baselines using the official F 1 -based score.", "labels": [], "entities": [{"text": "F 1 -based score", "start_pos": 87, "end_pos": 103, "type": "METRIC", "confidence": 0.9800330877304078}]}, {"text": "In particular, the best system can detect a correct answer with an accuracy of about 73% in the English task and 83% in the easier Arabic task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9995155334472656}]}, {"text": "For the extreme summarization task, the best accuracy is 72%.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8850661516189575}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9996172189712524}]}, {"text": "An interesting outcome of this task is that the Qatar Living company, a co-organizer of the challenge, is going to use the experience and the technology developed during the evaluation excercise to improve their products, e.g., the automatic search of comments useful to answer users' questions.", "labels": [], "entities": [{"text": "Qatar Living company", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.9660367965698242}]}, {"text": "The remainder of the paper is organized as follows: Section 2 gives a detailed description of the task, Section 3 describes the datasets, Section 4 explains the scorer, Section 5 presents the participants and the evaluation results, Section 6 provides an overview of the various features and techniques used by the participating systems, Section 7 offers further discussion, and finally, Section 8 concludes and points to possible directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We offer the task in two languages, English and Arabic, with some differences in the type of data provided.", "labels": [], "entities": []}, {"text": "For English, there is a question (short title + extended description) and a list of several community answers to that question.", "labels": [], "entities": []}, {"text": "For Arabic, there is a question and a set of possible answers, which include (i) a highly accurate answer, (ii) potentially useful answers from other questions, and (iii) answers to random questions.", "labels": [], "entities": []}, {"text": "The following subsections provide all the necessary details.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics about the English data.", "labels": [], "entities": [{"text": "English data", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.9362418055534363}]}, {"text": " Table 2: Statistics about the Arabic data.", "labels": [], "entities": [{"text": "Arabic data", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.7511411309242249}]}, {"text": " Table 4: Subtask A, English: results for all submissions.", "labels": [], "entities": []}, {"text": " Table 5: Subtask A, English with Dialog as a separate  category: results for the primary submissions. The first  column shows the rank based on macro F 1 , the subindex  in the last column shows the rank based on accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9980176687240601}]}, {"text": " Table 6: Subtask A, Arabic: results for all submissions.  The first column shows the rank for the primary submis- sions according to macro F 1 , and the subindex in the last  column shows the rank based on accuracy. Teams marked  with a include a task co-organizer.", "labels": [], "entities": [{"text": "macro F 1", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.7340825001398722}, {"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.998225748538971}]}, {"text": " Table 7: Subtask A, Arabic: results for the 30 manually  annotated Arabic questions.", "labels": [], "entities": []}, {"text": " Table 8: Subtask B, English: results for all submissions.", "labels": [], "entities": []}]}