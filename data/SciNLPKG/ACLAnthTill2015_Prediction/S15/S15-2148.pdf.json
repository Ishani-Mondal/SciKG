{"title": [{"text": "UCD : Diachronic Text Classification with Character, Word, and Syntactic N-grams", "labels": [], "entities": [{"text": "UCD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.96773761510849}, {"text": "Diachronic Text Classification", "start_pos": 6, "end_pos": 36, "type": "TASK", "confidence": 0.5389671325683594}]}], "abstractContent": [{"text": "We present our submission to SemEval-2015 Task 7: Diachronic Text Evaluation, in which we approach the task of assigning a date to a text as a multi-class classification problem.", "labels": [], "entities": [{"text": "SemEval-2015 Task 7", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7378677527109782}, {"text": "Diachronic Text Evaluation", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.6114361385504404}]}, {"text": "We extract n-gram features from the text at the letter, word, and syntactic level, and use these to train a classifier on date-labeled training data.", "labels": [], "entities": []}, {"text": "We also incorporate date probabilities of syntactic features as estimated from a very large external corpus of books.", "labels": [], "entities": [{"text": "date", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9801780581474304}]}, {"text": "Our system achieved the highest performance of all systems on subtask 2: identifying texts by specific time language use.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes our submission to the SemEval-2015 Task 7, \"Diachronic Text Evaluation\" (.", "labels": [], "entities": [{"text": "SemEval-2015 Task 7", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8596466779708862}, {"text": "Diachronic Text Evaluation", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.6916080315907797}]}, {"text": "The aim of this shared task is to evaluate approaches toward diachronic text analysis of a corpus of Englishlanguage news articles from The Spectator 1 archive, originally published between 1700 and 2014.", "labels": [], "entities": [{"text": "diachronic text analysis", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.7160371740659078}, {"text": "Englishlanguage news articles from The Spectator 1 archive, originally published between 1700 and 2014", "start_pos": 101, "end_pos": 203, "type": "DATASET", "confidence": 0.8071671028931936}]}, {"text": "We solely address subtask 2: \"texts with specific time language usage.\"", "labels": [], "entities": []}, {"text": "The goal of this subtask is to infer the composition date of a text based on implicit clues in language of the text, as opposed to overt mentions of datable named entities or events.", "labels": [], "entities": []}, {"text": "This task has inherent utility, for example, for historians dating texts in an archive with no external datable properties.", "labels": [], "entities": []}, {"text": "However, it is equally interesting as an investigation into methods for quantifying http://www.spectator.co.uk/.", "labels": [], "entities": []}, {"text": "changes in language and writing style over a period of centuries.", "labels": [], "entities": []}, {"text": "We approach this task in a similar manner as previous work on stylistic text classification) in that we aim to model stylistic, rather than topical, features of the text.", "labels": [], "entities": [{"text": "stylistic text classification", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.6262658635775248}]}, {"text": "From each text we extract a variety of character, lexical, and syntactic features, as described in section 3.", "labels": [], "entities": []}, {"text": "We also use a set of syntactic features whose frequencies overtime have been estimated from a very large corpus of books (.", "labels": [], "entities": []}, {"text": "While many of these features have previously been used for stylistic analysis, our approach is not to model style per se.", "labels": [], "entities": []}, {"text": "Many types of variation maybe captured indirectly by our features: the spelling, typography, lexicon, and grammar of English have changed markedly over the past centuries, as has the genre of news writing.", "labels": [], "entities": []}, {"text": "We consider any time-correlated variation to be useful for dating.", "labels": [], "entities": [{"text": "dating", "start_pos": 59, "end_pos": 65, "type": "TASK", "confidence": 0.9618567824363708}]}], "datasetContent": [{"text": "We employ attribute selection as above in all of our cross-validation experiments and our official submission.", "labels": [], "entities": []}, {"text": "illustrates how SVM classification accuracy varies with feature set size.", "labels": [], "entities": [{"text": "SVM classification", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9381191730499268}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9262343645095825}]}, {"text": "The value of 4000 features was chosen to maximize accuracy while minimizing running time, and was used to produce all of the results described in this paper.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9984629154205322}]}, {"text": "Assigning a date to a text is not atypical classifi-cation problem, because the classes are not independent of one another.", "labels": [], "entities": [{"text": "Assigning a date to a text", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8046371241410574}]}, {"text": "We experimented with SVM regression, but this produced lower accuracy than the SVM classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9992384910583496}]}, {"text": "Ordinal classification is a method that maybe used when classes exhibit a natural order, as in this task.", "labels": [], "entities": [{"text": "Ordinal classification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8191058039665222}]}, {"text": "We performed some experiments with the Weka implementation of ordinal regression) using a SVM base classifier, but these produced lower accuracy than the standard SVM classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9988920092582703}]}, {"text": "Therefore, we used a standard multi-class SVM classifier for all of our evaluations and predictions.", "labels": [], "entities": []}, {"text": "lists the cross-validation classification accuracy for our various models.", "labels": [], "entities": [{"text": "cross-validation classification", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.6250124275684357}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9075182676315308}]}, {"text": "The baseline classifier looks only at the class labels and chooses the most frequent class.", "labels": [], "entities": []}, {"text": "The Google nb classifier is a Naive Bayes classifier using only the GSN probabilities and assuming a uniform prior over years.", "labels": [], "entities": []}, {"text": "This represents a classifier with no domain knowledge of the text genre or date range distribution.", "labels": [], "entities": []}, {"text": "The remaining rows show the results for SVM classifiers trained independently on each of the four stylistic feature sets.", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.8883205950260162}]}, {"text": "While each feature type outperforms the baseline, the character n-gram features are clearly the single most effective feature type.", "labels": [], "entities": []}, {"text": "The combination of all four features together (CPWS) outperforms any single feature set individually, and this represents the maximal performance we achieve using solely the training data provided by the task organizers.", "labels": [], "entities": []}, {"text": "The final row shows the performance of a SVM classifier using all of our stylistic features plus features derived from the GSN probabilities.", "labels": [], "entities": []}, {"text": "This achieves the highest accuracy and this is the system we submitted to the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9993452429771423}]}, {"text": "shows the official results of the CPWS+G classifier, trained on the full training set and evaluated on a test set of 1041 texts whose true dates were unknown to us.", "labels": [], "entities": [{"text": "CPWS+G classifier", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.7851805239915848}]}, {"text": "The accuracy values are inline with our cross-validation scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995265007019043}]}, {"text": "The score is a weighted classification metric that rewards predictions that are not fully correct but are near the correct date.", "labels": [], "entities": []}, {"text": "The third row lists the mean deviation of our predictions from the true date.", "labels": [], "entities": [{"text": "mean", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9569095373153687}]}, {"text": "By all three measures, our system was the top performing submission to this subtask.", "labels": [], "entities": []}, {"text": "Our 73.3% accuracy on the 50-year class maybe loosely compared to, who achieve 62% classification accuracy dating words in context to 50-year epochs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99949049949646}, {"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.8584286570549011}]}, {"text": "Their task, word epoch disambiguation, is comparable but different: they classify words, not texts, using local context features and a targeted set of 165 words.", "labels": [], "entities": [{"text": "word epoch disambiguation", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.6132703920205435}]}], "tableCaptions": [{"text": " Table 1: Top 20 CPWS features using Information Gain", "labels": [], "entities": []}, {"text": " Table 2: Effect of feature set size (|F |) on classification  accuracy. (Char+POS+Google features)", "labels": [], "entities": [{"text": "F", "start_pos": 39, "end_pos": 40, "type": "METRIC", "confidence": 0.7431089878082275}, {"text": "classification", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.9415575265884399}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9678918123245239}]}, {"text": " Table 3: Classification accuracy of various feature sets,  using 10-fold cross-validiation on the training data set.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.895395815372467}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.977058470249176}, {"text": "training data set", "start_pos": 99, "end_pos": 116, "type": "DATASET", "confidence": 0.8121331532796224}]}, {"text": " Table 4: Official results on the SemEval test data.", "labels": [], "entities": [{"text": "SemEval test data", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.8454977869987488}]}]}