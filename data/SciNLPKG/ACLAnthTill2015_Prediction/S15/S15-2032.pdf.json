{"title": [{"text": "UBC: Cubes for English Semantic Textual Similarity and Supervised Approaches for Interpretable STS", "labels": [], "entities": [{"text": "UBC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8762449026107788}]}], "abstractContent": [{"text": "In Semantic Textual Similarity, systems rate the degree of semantic equivalence on a graded scale from 0 to 5, with 5 being the most similar.", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7315840323766073}]}, {"text": "For the English subtask, we present a system which relies on several resources for token-to-token and phrase-to-phrase similarity to build a data-structure which holds all the information , and then combine the information to get a similarity score.", "labels": [], "entities": []}, {"text": "We also participated in the pilot on Interpretable STS, where we apply a pipeline which first aligns tokens, then chunks, and finally uses supervised systems to label and score each chunk alignment.", "labels": [], "entities": [{"text": "Interpretable STS", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7769645750522614}]}], "introductionContent": [{"text": "In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence on a graded scale from 0 to 5, with 5 being the most similar.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.79240582883358}]}, {"text": "We participated in two of the subtask for STS in.", "labels": [], "entities": [{"text": "STS", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.8156007528305054}]}, {"text": "For the English subtask, we present a system which relies on several resources for tokento-token and phrase-to-phrase similarity to build a data-structure which holds all the information, and then combine the information to get a similarity score.", "labels": [], "entities": []}, {"text": "We also participated in the pilot on Interpretable STS, where we apply a pipeline which first aligns tokens, then chunks, and finally uses supervised systems to label and score each chunk alignment.", "labels": [], "entities": [{"text": "Interpretable STS", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7769645750522614}]}, {"text": "Note that some of the authors participated in the organization of the task.", "labels": [], "entities": []}, {"text": "We scrupulously separated the tasks in such away that the developers of the systems did not have access to the test sets, and that they only had access to the same training data as the rest of the participants.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Development results for both datasets in the  two scenarios. 'I' stands for the images dataset, and 'H'  stands for the headlines dataset.", "labels": [], "entities": [{"text": "headlines dataset", "start_pos": 130, "end_pos": 147, "type": "DATASET", "confidence": 0.9124589562416077}]}, {"text": " Table 2: Test results for both datasets in the two scenar- ios. 'I' stands for the images dataset, 'H' stands for the  headlines dataset and 'Par' stands for participants.", "labels": [], "entities": [{"text": "headlines dataset", "start_pos": 120, "end_pos": 137, "type": "DATASET", "confidence": 0.7487795650959015}]}]}