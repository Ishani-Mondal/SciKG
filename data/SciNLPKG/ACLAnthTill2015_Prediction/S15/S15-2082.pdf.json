{"title": [{"text": "SemEval-2015 Task 12: Aspect Based Sentiment Analysis", "labels": [], "entities": [{"text": "SemEval-2015 Task 12", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8705069224039713}, {"text": "Aspect Based Sentiment Analysis", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.7783138602972031}]}], "abstractContent": [{"text": "SemEval-2015 Task 12, a continuation of SemEval-2014 Task 4, aimed to foster research beyond sentence-or text-level sentiment classification towards Aspect Based Sentiment Analysis.", "labels": [], "entities": [{"text": "sentence-or text-level sentiment classification", "start_pos": 93, "end_pos": 140, "type": "TASK", "confidence": 0.6272013336420059}, {"text": "Aspect Based Sentiment Analysis", "start_pos": 149, "end_pos": 180, "type": "TASK", "confidence": 0.8051387965679169}]}, {"text": "The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price).", "labels": [], "entities": []}, {"text": "The task provided manually annotated reviews in three domains (restaurants, laptops and hotels), and a common evaluation procedure.", "labels": [], "entities": []}, {"text": "It attracted 93 submissions from 16 teams.", "labels": [], "entities": []}, {"text": "1 A subset of the datasets has been annotated with aspects at the sentence level.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The datasets 7 of the SE-ABSA15 task were provided in an XML format.", "labels": [], "entities": []}, {"text": "They are available under a non-commercial, no redistribution license through META-SHARE 8 , a repository devoted to the sharing and dissemination of language resources (Piperidis, 2012).", "labels": [], "entities": []}, {"text": "Similarly to SE-ABSA14, the evaluation ran in two phases.", "labels": [], "entities": []}, {"text": "In Phase A, the participants were asked to return the {category, OTE} tuples for the restaurants domain and only the category slot (Slot1) for the laptops domain.", "labels": [], "entities": []}, {"text": "Subsequently, in Phase B, the participants were given the gold annotations for the reviews of Phase A and they were asked to return the polarity (Slot3).", "labels": [], "entities": [{"text": "polarity", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9565683603286743}, {"text": "Slot3)", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9387998282909393}]}, {"text": "Each participating team was allowed to submit up to two runs per slot and domain in each phase; one constrained (C), where only the provided training data could be used, and one unconstrained (U), where other resources (e.g., publicly available lexica) and additional data of any kind could be used for training.", "labels": [], "entities": []}, {"text": "In the latter case, the teams had to report the resources they used.", "labels": [], "entities": []}, {"text": "To evaluate aspect category (Slot1) and OTE extraction (Slot2) in Phase A, we used the F-1 measure.", "labels": [], "entities": [{"text": "OTE extraction", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.7795890867710114}]}, {"text": "To evaluate sentiment polarity (Slot 3) in Phase B, we used accuracy.", "labels": [], "entities": [{"text": "sentiment polarity", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8869452178478241}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9996579885482788}]}, {"text": "Furthermore, we implemented and provided three baselines (see below) for the respective slots.", "labels": [], "entities": []}, {"text": "Slot 1: F-1 scores are calculated by comparing the category annotations that a system returned (for all the sentences) to the gold category annotations (using micro-averaging).", "labels": [], "entities": [{"text": "F-1", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9360885620117188}]}, {"text": "These category annotations are extracted from the values of Slot 1 (category).", "labels": [], "entities": []}, {"text": "Duplicate occurrences of categories (for the same sentence) are ignored.", "labels": [], "entities": []}, {"text": "Slot 2: F-1 scores are calculated by comparing the targets that a system returned (for all the sentences) to the corresponding gold targets (using micro-averaging).", "labels": [], "entities": [{"text": "F-1 scores", "start_pos": 8, "end_pos": 18, "type": "METRIC", "confidence": 0.9575063586235046}]}, {"text": "The targets are extracted using their starting and ending offsets.", "labels": [], "entities": []}, {"text": "The calculation for each sentence considers only distinct targets and discards NULL targets, since they do not correspond to explicit mentions.", "labels": [], "entities": []}, {"text": "Slot 1&2 (jointly): Again F-1 scores are calculated by comparing the {category, OTE} tuples of a system to the gold ones (using micro-averaging).", "labels": [], "entities": [{"text": "F-1 scores", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9478912651538849}]}, {"text": "Slot 3: To evaluate sentiment polarity detection in Phase B, we calculated the accuracy of each system, defined as the number of correctly predicted polarity labels of aspect categories, divided by the total number of aspect categories.", "labels": [], "entities": [{"text": "sentiment polarity detection", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.9552870790163676}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9995284080505371}]}, {"text": "Recall that we use the gold aspect categories in Phase B.  In total, the task attracted 92 submissions from 16 teams.", "labels": [], "entities": []}, {"text": "The evaluation results per phase and slot are presented below.", "labels": [], "entities": [{"text": "slot", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9556540250778198}]}, {"text": "For the teams that submitted more than one unconstrained runs per slot and domain, we included in the tables only the run with the highest score.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Datasets provided for ABSA.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.6353923678398132}]}, {"text": " Table 2 for more information.", "labels": [], "entities": []}, {"text": " Table 3). The polarity distribution  is balanced in the laptops domain, while in the res- taurants domain there is a significant imbalance  between the positive and negative classes across  the training and the test sets.", "labels": [], "entities": []}, {"text": " Table  5. The best F1 score (70.05%) was achieved by the  unconstrained submission of EliXa that addressed  the problem using an averaged perceptron with a  BIO tagging scheme. The features EliXa used in- cluded n-grams, token classes, n-gram prefixes and  suffixes, and word clusters learnt from additional  data (Yelp for Brown and Clark clusters; Wikipe- dia for word2vec clusters). Similarly, NLANGP  (67.11%) was based on a Conditional Random  Fields (CRF) model with features based on word  strings, head words (obtained from parse trees),  name lists (e.g. extracted using frequency), and  Brown clusters.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9855204224586487}]}, {"text": " Table 5. Results for OTE extraction (slot 2). * indicate  unconstrained systems.", "labels": [], "entities": [{"text": "OTE extraction", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.8250304162502289}]}, {"text": " Table 6. Results for Slot1&2. * indicate unconstrained  systems.", "labels": [], "entities": [{"text": "Slot1", "start_pos": 22, "end_pos": 27, "type": "TASK", "confidence": 0.642826497554779}]}, {"text": " Table 7. Accuracy scores for slot 3 (polarity extraction).  * indicate unconstrained systems. The evaluated run of  SIEL team was submitted after the deadline had ex- pired, but before the release of the gold polarity labels.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987300038337708}, {"text": "polarity extraction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.6896134316921234}, {"text": "SIEL team", "start_pos": 117, "end_pos": 126, "type": "DATASET", "confidence": 0.8002426326274872}]}, {"text": " Table 8. Accuracy scores for slot 3 (polarity extraction).  * indicate unconstrained systems. The evaluated run of  UMDuluthC team was submitted after the deadline had  expired but before the release of the gold polarity labels.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988981485366821}, {"text": "polarity extraction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.6827738434076309}, {"text": "UMDuluthC team", "start_pos": 117, "end_pos": 131, "type": "DATASET", "confidence": 0.9194430708885193}]}]}