{"title": [{"text": "Combining Open Source Annotators for Entity Linking through Weighted Voting", "labels": [], "entities": [{"text": "Entity Linking", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.6956322193145752}]}], "abstractContent": [{"text": "An English entity linking (EL) workflow is presented, which combines the annotations of five public open source EL services.", "labels": [], "entities": []}, {"text": "The annotations are combined through a weighted voting scheme inspired by the ROVER method , which had not been previously tested on EL outputs.", "labels": [], "entities": [{"text": "ROVER", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9751347899436951}]}, {"text": "The combined results improved over each individual system's results, as evaluated on four different golden sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Entity Linking (EL) literature has shown that the quality of EL systems' results varies widely depending on characteristic of the corpora they are applied to, or on the types of entities we need to link.", "labels": [], "entities": [{"text": "Entity Linking (EL)", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7668023228645324}]}, {"text": "For instance, a system that links to a wide set of entity types can be less accurate at basic types like Person, Location, Organization than systems specializing in those basic types.", "labels": [], "entities": []}, {"text": "A way to makeup for the uneven performance of entity linking methods across corpora would be mixing different annotators' results, so that the annotators' strengths complement each other.", "labels": [], "entities": []}, {"text": "This paper presents a method to combine the outputs of five open source entity linking systems, in order to obtain improved results.", "labels": [], "entities": []}, {"text": "The method involves a weighted voting scheme that had not been previously applied to EL, and improves annotation results across four test-corpora.", "labels": [], "entities": [{"text": "EL", "start_pos": 85, "end_pos": 87, "type": "DATASET", "confidence": 0.7595808506011963}]}, {"text": "The structure of the paper is as follows: Section 2 presents related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the combined entity linking system.", "labels": [], "entities": []}, {"text": "Section 4 provides an evaluation of the system's results and a discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets: The workflow was tested on four golden sets.", "labels": [], "entities": []}, {"text": "First, the two datasets that had also been used as reference sets in order to obtain the weights to vote annotations with (see Section 3.2).", "labels": [], "entities": []}, {"text": "These two datasets were AIDA/CONLL B (231 documents with 4485 annotations; 1039 characters avg., news and sports topics) and IITB (103 documents with 11245 annotations; 3879 characters avg., topics from news, science and others).", "labels": [], "entities": [{"text": "AIDA/CONLL B", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.714292511343956}, {"text": "IITB", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.9133387207984924}]}, {"text": "In order to test whether the annotator weights obtained from those two corpora can improve results when applied to annotator combination on other corpora, we tested on two additional datasets: MSNBC, with 20 documents and 658 annotations (3316 characters avg., news topics) and AQUAINT, with 50 documents and 727 annotations (1415 characters avg., news topics).", "labels": [], "entities": [{"text": "MSNBC", "start_pos": 193, "end_pos": 198, "type": "DATASET", "confidence": 0.9653398990631104}, {"text": "AQUAINT", "start_pos": 278, "end_pos": 285, "type": "DATASET", "confidence": 0.7524387240409851}]}, {"text": "The AQUAINT dataset contains annotations for common noun entities (besides Person, Location, Organization).", "labels": [], "entities": [{"text": "AQUAINT dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9292480051517487}]}, {"text": "For this reason, according to the procedure described in 3.2 above, its annotations were weighted according to annotators' ranking on the IITB corpus, which also contains commonnoun annotations.", "labels": [], "entities": [{"text": "IITB corpus", "start_pos": 138, "end_pos": 149, "type": "DATASET", "confidence": 0.9593169987201691}]}, {"text": "The MSNBC dataset does not contain common-noun annotations, so the annotator ranking for the AIDA/CONLL test-set was used in order to combine annotations in MSNBC.", "labels": [], "entities": [{"text": "MSNBC dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9523940682411194}, {"text": "AIDA/CONLL test-set", "start_pos": 93, "end_pos": 112, "type": "DATASET", "confidence": 0.8500834852457047}]}, {"text": "Measures: The EL literature has stressed the importance of evaluating systems on more than one measure.", "labels": [], "entities": [{"text": "EL literature", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.7156595438718796}]}, {"text": "We tested the workflow on strong annotation match (SAM) and entity match (ENT) (.", "labels": [], "entities": [{"text": "entity match (ENT)", "start_pos": 60, "end_pos": 78, "type": "METRIC", "confidence": 0.8244057416915893}]}, {"text": "SAM requires an annotation's position to exactly match the reference, besides requiring the entity annotated to match the reference entity.", "labels": [], "entities": [{"text": "SAM", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9141496419906616}]}, {"text": "ENT ignores positions and only evaluates whether the entity proposed by the system matches the reference.", "labels": [], "entities": []}, {"text": "Mapping files: Evaluating EL to Wikipedia requires making sure that we consider the same set of target entities for each EL system, since the versions of Wikipedia deployed within each system may differ.", "labels": [], "entities": []}, {"text": "A mapping between current Wikipedia titles for the golden set annotations and noncanonical forms for these titles was created (including e.g. older titles redirecting to the new ones), and applied to golden and system sets before evaluation.", "labels": [], "entities": []}, {"text": "11 Tools: Evaluation was carried outwith the neleval tool 12 from the TAC-KBP Entity Discovery and Linking task ().", "labels": [], "entities": [{"text": "TAC-KBP Entity Discovery and Linking task", "start_pos": 70, "end_pos": 111, "type": "TASK", "confidence": 0.8360461294651031}]}, {"text": "The tool implements several EL-relevant metrics, accepting a common delimited format for golden sets and results across corpora.", "labels": [], "entities": []}, {"text": "The tool's significance testing function via randomized permutation/bootstrap methods was also applied to our results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Strong annotation match (SAM). Optimal confidence thresholds (t), Micro-averaged Precision, Recall, F1", "labels": [], "entities": [{"text": "Optimal confidence thresholds (t)", "start_pos": 41, "end_pos": 74, "type": "METRIC", "confidence": 0.9368100563685099}, {"text": "Micro-averaged", "start_pos": 76, "end_pos": 90, "type": "METRIC", "confidence": 0.920839250087738}, {"text": "Precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.5236691236495972}, {"text": "Recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9910179376602173}, {"text": "F1", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9903668165206909}]}, {"text": " Table 2: Entity match (ENT). Optimal confidence thresholds (t), Micro-averaged Precision, Recall, F1 in for  each annotator and combined system. Babelfy and the combined system use no confidence thresholds (dna).", "labels": [], "entities": [{"text": "Optimal confidence thresholds (t)", "start_pos": 30, "end_pos": 63, "type": "METRIC", "confidence": 0.9458473523457845}, {"text": "Micro-averaged Precision", "start_pos": 65, "end_pos": 89, "type": "METRIC", "confidence": 0.8037075698375702}, {"text": "Recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.980201780796051}, {"text": "F1", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.9799702167510986}]}]}