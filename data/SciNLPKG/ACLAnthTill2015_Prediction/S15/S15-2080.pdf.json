{"title": [{"text": "SemEval-2015 Task 11: Sentiment Analysis of Figurative Language in Twitter", "labels": [], "entities": [{"text": "Sentiment Analysis of Figurative Language", "start_pos": 22, "end_pos": 63, "type": "TASK", "confidence": 0.9182246088981628}]}], "abstractContent": [{"text": "This report summarizes the objectives and evaluation of the SemEval 2015 task on the sentiment analysis of figurative language on Twitter (Task 11).", "labels": [], "entities": [{"text": "SemEval 2015 task", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8736908435821533}, {"text": "sentiment analysis of figurative language on Twitter", "start_pos": 85, "end_pos": 137, "type": "TASK", "confidence": 0.9168039219720023}]}, {"text": "This is the first sentiment analysis task wholly dedicated to analyzing figurative language on Twitter.", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.9420840342839559}]}, {"text": "Specifically, three broad classes of figurative language are considered: irony, sarcasm and metaphor.", "labels": [], "entities": []}, {"text": "Gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform CrowdFlower.", "labels": [], "entities": []}, {"text": "Participating systems were required to provide a fine-grained sentiment score on an 11-point scale (-5 to +5, including 0 for neutral intent) for each tweet, and systems were evaluated against the gold standard using both a Cosine-similarity and a Mean-Squared-Error measure.", "labels": [], "entities": [{"text": "Mean-Squared-Error", "start_pos": 248, "end_pos": 266, "type": "METRIC", "confidence": 0.954567551612854}]}], "introductionContent": [{"text": "The limitations on text length imposed by microblogging services such as Twitter do nothing to dampen our willingness to use language creatively.", "labels": [], "entities": []}, {"text": "Indeed, such limitations further incentivize the use of creative devices such as metaphor and irony, as such devices allow strongly-felt sentiments to be expressed effectively, memorably and concisely.", "labels": [], "entities": []}, {"text": "Nonetheless, creative language can pose certain challenges for NLP tools that do not take account of how words can be used playfully and in original ways.", "labels": [], "entities": []}, {"text": "In the case of language using figurative devices such as irony, sarcasm or metaphor -when literal meanings are discounted and secondary or extended meanings are intentionally profiled -the affective polarity of the literal meaning may differ significantly from that of the intended figurative meaning.", "labels": [], "entities": []}, {"text": "Nowhere is this effect more pronounced than in ironical language, which delights in using affirmative language to convey critical meanings.", "labels": [], "entities": []}, {"text": "Metaphor, irony and sarcasm can each sculpt the affect of an utterance in complex ways, and each tests the limits of conventional techniques for the sentiment analysis of supposedly literal texts.", "labels": [], "entities": [{"text": "sentiment analysis of supposedly literal texts", "start_pos": 149, "end_pos": 195, "type": "TASK", "confidence": 0.8872167666753134}]}, {"text": "Figurative language thus poses an especially significant challenge to sentiment analysis systems, as standard approaches anchored in the dictionarydefined affect of individual words and phrases are often shown to be inadequate in the face of indirect figurative meanings.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.9454764425754547}]}, {"text": "It would be convenient if such language were rare and confined to specific genres of text, such as poetry and literature.", "labels": [], "entities": []}, {"text": "Yet the reality is that figurative language is pervasive in almost any genre of text, and is especially commonplace on the texts of the Web and on social media platforms such as Twitter.", "labels": [], "entities": []}, {"text": "Figurative language often draws attention to itself as a creative artifact, but is just as likely to be viewed as part of the general fabric of human communication.", "labels": [], "entities": []}, {"text": "In any case, Web users widely employ figures of speech (both old and new) to project their personality through a text, especially when their texts are limited to the 140 characters of a tweet.", "labels": [], "entities": []}, {"text": "Natural language researchers have attacked the problems associated with figurative interpretations at multiple levels of linguistic representation.", "labels": [], "entities": []}, {"text": "Some have focused on the conceptual level, of which the text is a surface instantiation, to identify the schemas and mappings that are implied by a figure of speech (see e.g.;;).", "labels": [], "entities": []}, {"text": "These approaches yield a depth of insight but not a robustness of analysis in the face of textual diversity.", "labels": [], "entities": []}, {"text": "More robust approaches focus on the surface level of a text, to consider word choice, syntactic order, lexical properties and affective profiles of the elements that makeup a text (e.g.).", "labels": [], "entities": []}, {"text": "Surface analysis yields a range of discriminatory features that can be efficiently extracted and fed into machine-learning algorithms.", "labels": [], "entities": [{"text": "Surface analysis", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7319194078445435}]}, {"text": "When it comes to analyzing the texts of the Web, the Web can also be used as a convenient source of ancillary knowledge and features.", "labels": [], "entities": []}, {"text": "describe a means of harvesting a commonsense knowledge-base of stereotypes from the Web, by directly targeting simile constructions of the form \"as X as Y\" (e.g. \"as hot as an oven\", \"as humid as a jungle\", \"as big as a mountain\", etc.).", "labels": [], "entities": []}, {"text": "Though largely successful in their efforts, Veale and Hao were surprised to discover that up to 20% of Web-harvested similes are ironic (examples include \"as subtle as a freight train\", \"as tanned as an Irishman\", \"as sober as a Kennedy\", \"as private as a park bench\").", "labels": [], "entities": []}, {"text": "Initially filtering ironic similes manually -as irony is the worst kind of noise when acquiring knowledge from the Web - report good results for an automatic, Web-based approach to distinguishing ironic from non-ironic similes.", "labels": [], "entities": []}, {"text": "Their approach exploits specific properties of similes and is thus not directly transferrable to the detection of irony in general.", "labels": [], "entities": [{"text": "detection of irony", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.6871962149937948}]}, {"text": "Reyes, and Reyes, Rosso and Buscaldi (2012) thus employ a more general approach that applies machine learning algorithms to a range of structural and lexical features to learn a robust basis for detecting humor and irony in text.", "labels": [], "entities": [{"text": "detecting humor and irony in text", "start_pos": 195, "end_pos": 228, "type": "TASK", "confidence": 0.860452006260554}]}, {"text": "The current task is one that calls for such a general approach.", "labels": [], "entities": []}, {"text": "Note that the goal of Task 11 is not to detect irony, sarcasm or metaphor in a text, but to perform robust sentiment analysis on a finegrained 11-point scale over texts in which these kinds of linguistic usages are pervasive.", "labels": [], "entities": [{"text": "detect irony, sarcasm or metaphor in a text", "start_pos": 40, "end_pos": 83, "type": "TASK", "confidence": 0.699971079826355}, {"text": "sentiment analysis", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7394469678401947}]}, {"text": "A system may find detection to be a useful precursor to analysis, or it may not.", "labels": [], "entities": []}, {"text": "We present a description of Task 11 in section 2, before presenting our dataset in section 3 and the scoring functions in section 4.", "labels": [], "entities": []}, {"text": "Descriptions of each participating system are then presented in section 5, before an overall evaluation in reported in section 6.", "labels": [], "entities": []}, {"text": "The report then concludes with some general observations in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Even humans have difficulty in deciding whether a given text is ironic or metaphorical.", "labels": [], "entities": []}, {"text": "Irony can be remarkably subtle, while metaphor takes many forms, ranging the dead to the conventional to the novel.", "labels": [], "entities": []}, {"text": "Sarcasm is easier for humans to detect, and is perhaps the least sophisticated form of nonliteral language.", "labels": [], "entities": []}, {"text": "We sidestep problems of detection by harvesting tweets from Twitter that are likely to contain figurative language, either because they have been explicitly tagged as such (using e.g. the hashtags #irony, #sarcasm, #not, #yeahright) or because they use words commonly associated with the use of metaphor (ironically, the words \"literally\" and \"virtually\" are reliable markers of metaphorical intent, as in \"I literally want to die \").", "labels": [], "entities": []}, {"text": "Datasets were collected using the Twitter4j API (http://twitter4j.org/en/index.html), which supports the harvesting of tweets in real-time using search queries.", "labels": [], "entities": []}, {"text": "Queries for hashtags such as #sarcasm, #sarcastic and #irony, and for words such as \"figuratively\", yielded our initial corpora of candidate tweets to annotate.", "labels": [], "entities": []}, {"text": "We then developed a Latent Semantic Analysis (LSA) model to extend this seed set of hashtags so as to harvest a wider range of figurative tweets (see).", "labels": [], "entities": []}, {"text": "This tweet dataset was collected over a period of 4 weeks, from June 1 st to June 30 th , 2014.", "labels": [], "entities": []}, {"text": "Though URLs have been removed from tweets, all other content, including hashtags -even those used to retrieve each tweet -has been left in place.", "labels": [], "entities": []}, {"text": "Tweets must contain at least 30 characters when hashtags are not counted, or 40 characters when hashtags are counted.", "labels": [], "entities": []}, {"text": "All others are eliminated as too short.", "labels": [], "entities": []}, {"text": "A trial dataset, consisting of 1025 tweets, was first prepared by harvesting tweets from Twitter users that are known for their use of figurative language (e.g. comedians).", "labels": [], "entities": []}, {"text": "Each trial tweet was annotated by seven annotators from an internal team, three of whom are native English speakers, the other four of whom are competent non-native speakers.", "labels": [], "entities": []}, {"text": "Each annotator was asked to assign a score ranging from -5 (for any tweets conveying disgust or extreme discontent) to +5 (for tweets conveying obvious joy and approval or extreme pleasure), where 0 is reserved for tweets in which positive and negative sentiment is balanced.", "labels": [], "entities": []}, {"text": "Annotators were asked to use \u00b15, \u00b13 and \u00b11 as scores for tweets calling for strong, moderate or weak sentiment, and to use \u00b14 and \u00b12 for tweets with nuanced sentiments that fall between these gross scores.", "labels": [], "entities": []}, {"text": "An overall sentiment score for each tweet was calculated as a weighted average of all 7 annotators, where a double weighting was given to native English speakers.", "labels": [], "entities": []}, {"text": "Sentiment was assigned on the basis of the perceived meaning of each tweet -the meaning an author presumably intends a reader to unpack from the text -and not the superficial language of the tweet.", "labels": [], "entities": []}, {"text": "Thus, a sarcastic tweet that expresses a negative message in language that feigns approval or delight should be marked with a negative score (as in \"I just love it when my friends throw me under the bus.\").", "labels": [], "entities": []}, {"text": "Annotators were explicitly asked to consider all of a tweet's content when assigning a score, including any hashtags (such as #sarcasm, #irony, etc.), as participating systems are expected to use all of the tweet's content, including hashtags.", "labels": [], "entities": []}, {"text": "Tweets of the training and test datasetscomprising 8000 and 4000 tweets respectivelywere each annotated on a crowd-sourcing platform, CrowdFlower.com, following the same annotation scheme as for the trial dataset.", "labels": [], "entities": []}, {"text": "Some examples of tweets and their ideal scores, given as guidelines to CrowdFlower annotators, are shown in.", "labels": [], "entities": []}, {"text": "The trial dataset contains a mix of figurative tweets chosen manually from Twitter.", "labels": [], "entities": []}, {"text": "It consists of 1025 tweets annotated by an internal team of seven members.", "labels": [], "entities": []}, {"text": "shows the number of tweets in each category.", "labels": [], "entities": []}, {"text": "The trial dataset is small enough to allow these category labels to be applied manually.", "labels": [], "entities": []}, {"text": "The training and test datasets were annotated by CrowdFlower users from countries where English is spoken as a native language.", "labels": [], "entities": []}, {"text": "The 8,000 tweets of the training set were allocated as in.", "labels": [], "entities": []}, {"text": "As the datasets are simply too large for the category labels Sarcasm, Irony and Metaphor to be assigned manually, the labels here refer to our expectations of the kind of tweets in each segment of the dataset, which were each collated using harvesting criteria specific to different kinds of figurative language.", "labels": [], "entities": []}, {"text": "To provide balance, an additional category Other was also added to the Test dataset.", "labels": [], "entities": [{"text": "balance", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.988128125667572}, {"text": "Test dataset", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.9491162300109863}]}, {"text": "Tweets in this category were drawn from general Twitter content, and so were not chosen to capture any specific figurative quality.", "labels": [], "entities": []}, {"text": "Rather, the category was added to ensure the ecological validity of the task, as sentiment analysis is never performed on texts that are wholly figurative.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.9343902170658112}]}, {"text": "The 4000 tweets of the Test set were drawn from four categories as in.", "labels": [], "entities": [{"text": "Test set", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.769299328327179}]}], "tableCaptions": [{"text": " Table 5: Performance of Three Baseline approaches", "labels": [], "entities": []}, {"text": " Table 6: Overall results, sorted by cosine metric.  Scores are for last run submitted for each system.", "labels": [], "entities": []}, {"text": " Table 7. How well does overall performance correlate  with performance on different kinds of tweets?", "labels": [], "entities": []}, {"text": " Table 9. Detailed evaluation of each submitted run of each system (using the Cosine similarity metric).", "labels": [], "entities": [{"text": "Cosine similarity metric", "start_pos": 78, "end_pos": 102, "type": "METRIC", "confidence": 0.718644360701243}]}, {"text": " Table 10. Detailed evaluation of each submitted run of each system (using the Mean-Squared-Error metric).", "labels": [], "entities": [{"text": "Mean-Squared-Error", "start_pos": 79, "end_pos": 97, "type": "METRIC", "confidence": 0.9864501357078552}]}]}