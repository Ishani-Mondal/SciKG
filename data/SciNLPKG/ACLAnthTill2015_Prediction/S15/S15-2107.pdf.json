{"title": [{"text": "TwitterHawk: A Feature Bucket Approach to Sentiment Analysis", "labels": [], "entities": [{"text": "TwitterHawk", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9483271241188049}, {"text": "Sentiment Analysis", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.978783369064331}]}], "abstractContent": [{"text": "This paper describes TwitterHawk, a system for sentiment analysis of tweets which participated in the SemEval-2015 Task 10, Subtasks A through D.", "labels": [], "entities": [{"text": "sentiment analysis of tweets", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.908491313457489}, {"text": "SemEval-2015 Task 10", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.8385990858078003}]}, {"text": "The system performed competitively , most notably placing 1 st in topic-based sentiment classification (Subtask C) and ranking 4 th out of 40 in identifying the sentiment of sarcastic tweets.", "labels": [], "entities": [{"text": "topic-based sentiment classification", "start_pos": 66, "end_pos": 102, "type": "TASK", "confidence": 0.6295996606349945}]}, {"text": "Our submissions in all four subtasks used a supervised learning approach to perform three-way classification to assign positive, negative, or neutral labels.", "labels": [], "entities": []}, {"text": "Our system development efforts focused on text pre-processing and feature engineering, with a particular focus on handling negation, integrating sentiment lexicons, parsing hash-tags, and handling expressive word modifications and emoticons.", "labels": [], "entities": []}, {"text": "Two separate classifiers were developed for phrase-level and tweet-level sentiment classification.", "labels": [], "entities": [{"text": "tweet-level sentiment classification", "start_pos": 61, "end_pos": 97, "type": "TASK", "confidence": 0.7391782800356547}]}, {"text": "Our success in aforementioned tasks came in part from lever-aging the Subtask B data and building a single tweet-level classifier for Subtasks B, C and D.", "labels": [], "entities": [{"text": "Subtask B data", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.5989609857400259}]}], "introductionContent": [{"text": "In recent years, microblogging has developed into a resource for quickly and easily gathering data about how people feel about different topics.", "labels": [], "entities": []}, {"text": "Sites such as Twitter allow for real-time communication of sentiment, thus providing unprecedented insight into how well-received products, events, and people are in the public's eye.", "labels": [], "entities": []}, {"text": "But working with this new genre is challenging.", "labels": [], "entities": []}, {"text": "Twitter imposes a 140-character limit on messages, which causes users to use novel abbreviations and often disregard standard sentence structures.", "labels": [], "entities": []}, {"text": "For the past three years, the International Workshop on Semantic Evaluation (SemEval) has been hosting a task dedicated to sentiment analysis of Twitter data.", "labels": [], "entities": [{"text": "International Workshop on Semantic Evaluation (SemEval)", "start_pos": 30, "end_pos": 85, "type": "TASK", "confidence": 0.5804266147315502}, {"text": "sentiment analysis of Twitter", "start_pos": 123, "end_pos": 152, "type": "TASK", "confidence": 0.8968128263950348}]}, {"text": "This year, our team participated in four subtasks of the challenge: Contextual Polarity Disambiguation (phrase-level), B: Message Polarity Classification (tweet-level), C: Topic-Based Message Polarity Classification (topic-based), and D: Detecting Trends Towards a Topic (trending sentiment).", "labels": [], "entities": [{"text": "Contextual Polarity Disambiguation", "start_pos": 68, "end_pos": 102, "type": "TASK", "confidence": 0.5930477976799011}]}, {"text": "For a more thorough description of the tasks, see.", "labels": [], "entities": []}, {"text": "Our system placed 1 stout of 7 submissions for topic-based sentiment prediction (Subtask C), 3 rd out of 6 submissions for detecting trends toward a topic (Subtask D), 10 th out of 40 submissions for tweet-level sentiment prediction (Subtask B), and 5 th out of 11 for phrase-level prediction (Subtask A).", "labels": [], "entities": [{"text": "topic-based sentiment prediction", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.7439456681410471}, {"text": "tweet-level sentiment prediction", "start_pos": 200, "end_pos": 232, "type": "TASK", "confidence": 0.7042993704477946}, {"text": "phrase-level prediction", "start_pos": 269, "end_pos": 292, "type": "TASK", "confidence": 0.7491919994354248}]}, {"text": "Our system also ranked 4 th out of 40 submissions in identifying the sentiment of sarcastic tweets.", "labels": [], "entities": []}, {"text": "Most systems that participated in this task over the past two years have relied on basic machine learning classifiers with a strong focus on developing robust and comprehensive feature set.", "labels": [], "entities": []}, {"text": "The top system for Subtask A in both 2013 and 2014 from NRC Canada () used a simple linear SVM while putting great effort into creating and incorporating sentiment lexicons as well as carefully handling negation contexts.", "labels": [], "entities": [{"text": "Subtask A", "start_pos": 19, "end_pos": 28, "type": "TASK", "confidence": 0.8892973363399506}, {"text": "NRC Canada", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.8798626065254211}]}, {"text": "Other teams addressed imbalances in data distributions, but still mainly focused on feature engineering, including an improved spelling correction, POS tagging, and word sense disambiguation ().", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.6100379973649979}, {"text": "POS tagging", "start_pos": 148, "end_pos": 159, "type": "TASK", "confidence": 0.8477125763893127}, {"text": "word sense disambiguation", "start_pos": 165, "end_pos": 190, "type": "TASK", "confidence": 0.6953121225039164}]}, {"text": "The second place submission for the 2014 Task B competition also used a neural network setup to learn sentiment-specific word embedding features along with state-of-the-art hand-crafted features ().", "labels": [], "entities": []}, {"text": "Our goal in developing TwitterHawk was to build on the success of feature-driven approaches established as state-of-the-art in the two previous years of SemEval Twitter Sentiment Analysis competitions.", "labels": [], "entities": [{"text": "SemEval Twitter Sentiment Analysis competitions", "start_pos": 153, "end_pos": 200, "type": "TASK", "confidence": 0.9273227214813232}]}, {"text": "We therefore focused on identifying and incorporating the strongest features used by the best systems, most notably, sentiment lexicons that showed good performance in ablation studies.", "labels": [], "entities": []}, {"text": "We also performed multiple rounds of pre-processing which included tokenization, spelling correction, hashtag segmentation, wordshape replacement of URLs, as well as handling negated contexts.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.9676002264022827}, {"text": "spelling correction", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.8802574276924133}, {"text": "hashtag segmentation", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.7151728272438049}, {"text": "wordshape replacement of URLs", "start_pos": 124, "end_pos": 153, "type": "TASK", "confidence": 0.8317743837833405}]}, {"text": "Our main insight for Task C involved leveraging additional training data, since the provided training data was quite small (489 examples between training and dev).", "labels": [], "entities": []}, {"text": "Although not annotated with respect to a particular topic, we found that message-level sentiment data (Subtask B) generalized better to topic-level sentiment tracking than span-level data (Subtask A).", "labels": [], "entities": [{"text": "topic-level sentiment tracking", "start_pos": 136, "end_pos": 166, "type": "TASK", "confidence": 0.6421172817548116}]}, {"text": "We therefore used Subtask B data to train a more robust model for topic-level sentiment detection.", "labels": [], "entities": [{"text": "topic-level sentiment detection", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.8315037687619528}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss text preprocessing and normalization, describe the two classifiers we created for different subtasks, and present the features used by each model.", "labels": [], "entities": [{"text": "text preprocessing", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7375124096870422}]}, {"text": "We report system results in Section 3, and discuss system performance and future directions in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Both phrase-level and tweet-level systems were tuned in 10-fold cross-validation using the 2013 training, dev, and test data (.", "labels": [], "entities": []}, {"text": "We used fixed data folds in order to compare different runs.", "labels": [], "entities": []}, {"text": "Feature ablation studies, parameter tuning, and comparison of different pre-processing steps were performed using this setup.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7337169051170349}]}, {"text": "We conducted ablation studies for lexicon features using tweet-level evaluation.", "labels": [], "entities": []}, {"text": "shows ablation results obtained in 10-fold cross-validation.", "labels": [], "entities": [{"text": "ablation", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9761967658996582}]}, {"text": "The figures are bolded if withholding the features derived from a given lexicon produced a higher score.", "labels": [], "entities": []}, {"text": "Note that these experiments were conducted using a Linear SVM classifier with a limited subset of basic text features.", "labels": [], "entities": []}, {"text": "Our best cross-validation results using the configuration described in sections 2.2 and 2.3 above were 87.12 average F-measure for phrase-level analysis (Subtask A), and 68.50 for tweet-level analysis (Subtask B).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9985583424568176}]}, {"text": "For topic-level sentiment detection in Subtask C, we investigated three different approaches: (1) using our phrase-level classifier \"as is\", (2) training our phrase level classifier only on phrases that resembled topics 3 , and (3) using our tweet-level classifier \"as is\".", "labels": [], "entities": [{"text": "topic-level sentiment detection", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8177693684895834}]}, {"text": "We found that our phrase-level classifiers did not perform well (F-scores in the 35-38 range), which could be explained by the fact that the Subtask A data was annotated so that the target phrases actually carried sentiment (e.g., the phrase \"good luck\"), whereas the Subtask C assumption was that the topic itself had no sentiment and that the topics context determined the expressed sentiment.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9857351183891296}]}, {"text": "For example, in the tweet \"Gotta go see Flight tomorrow Denzel is the greatest actor ever\", positive sentiment is carried by the phrase \"the greatest actor ever\", rather than the token \"Denzel\" (corresponding to the topic).", "labels": [], "entities": []}, {"text": "It is therefore not surprising that our tweetlevel classifier achieved an F-score of 54.90, since tweet-level analysis is better able to capture longrange dependencies between sentiment-carrying expressions and the target topic.", "labels": [], "entities": [{"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9994342923164368}]}, {"text": "Consequently, we  used the tweet-level classifier in our submission for Subtask C.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Which lexicons we used for each classifier.", "labels": [], "entities": []}, {"text": " Table 2: Ablation results for lexicons features in tweet- level classification.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.997722327709198}, {"text": "tweet- level classification", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6193893402814865}]}, {"text": " Table 4: Contribution of different features in tweet-level  classification. nBOW stands for normalized bag-of-words  features.", "labels": [], "entities": [{"text": "tweet-level  classification", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.7245974838733673}]}]}