{"title": [{"text": "SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.6604500611623129}, {"text": "Pilot", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9521146416664124}]}], "abstractContent": [{"text": "In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets.", "labels": [], "entities": [{"text": "semantic textual similarity (STS)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.786034827431043}]}, {"text": "This year, the participants were challenged with new datasets in English and Spanish.", "labels": [], "entities": []}, {"text": "The annotations for both subtasks leveraged crowdsourcing.", "labels": [], "entities": []}, {"text": "The En-glish subtask attracted 29 teams with 74 system runs, and the Spanish subtask engaged 7 teams participating with 16 system runs.", "labels": [], "entities": [{"text": "Spanish subtask", "start_pos": 69, "end_pos": 84, "type": "DATASET", "confidence": 0.9119771420955658}]}, {"text": "In addition, this year we ran a pilot task on in-terpretable STS, where the systems needed to add an explanatory layer, that is, they had to align the chunks in the sentence pair, explicitly annotating the kind of relation and the score of the chunk pair.", "labels": [], "entities": []}, {"text": "The train and test data were manually annotated by an expert, and included headline and image sentence pairs from previous years.", "labels": [], "entities": []}, {"text": "7 teams participated with 29 runs.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "This Section reports the results for the English and Spanish subtasks.", "labels": [], "entities": []}, {"text": "Note that participants could submit a maximum of three runs per subtask.", "labels": [], "entities": []}, {"text": "Participating runs were evaluated using four different metrics: F1 where alignment type and score are ignored; F1 where alignment types need to match, but scores are ignored; F1 where alignment type is ignored, but each alignment is penalized when scores do not match; and, F1 where alignment types need to match, and each alignment is penalized when scores do not match.", "labels": [], "entities": [{"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9983019828796387}, {"text": "F1", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.9966384172439575}, {"text": "F1", "start_pos": 175, "end_pos": 177, "type": "METRIC", "confidence": 0.9957875609397888}, {"text": "F1", "start_pos": 274, "end_pos": 276, "type": "METRIC", "confidence": 0.9983404874801636}]}, {"text": "On a separate note, we felt filtering was specifically needed for new datasets, in order to guarantee a minimum quality.", "labels": [], "entities": []}, {"text": "For datasets like images and headlines, where the sampling strategy was already shown to work, it might not be as necessary.", "labels": [], "entities": []}, {"text": "For completeness, we also evaluated the systems on the full set of annotations.", "labels": [], "entities": []}, {"text": "The system scoring best was the same as in the official test set (DLS@CU-S1), with a mean correlation of 73.4%.", "labels": [], "entities": [{"text": "correlation", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.5725857615470886}]}, {"text": "The baseline scored 49.6%, and it would rank in position 55.", "labels": [], "entities": []}, {"text": "The best results in each dataset decreased more or less uniformly.", "labels": [], "entities": []}, {"text": "The filtering ensured a test set of better quality, but we interpret that the full set can also be used for development.", "labels": [], "entities": []}, {"text": "It's available from the task website.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Task 2a: English evaluation results in terms of Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 58, "end_pos": 77, "type": "METRIC", "confidence": 0.8340592682361603}]}, {"text": " Table 4: Task 2b: Spanish evaluation results in terms of Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 58, "end_pos": 77, "type": "METRIC", "confidence": 0.8207716941833496}]}, {"text": " Table 5: STS interpretable results for the gold chunks scenario. Best results have been marked in bold. 'H' stands  for Headlines data set and 'I' stands for Images data set. + symbol denotes resubmissions and  *  symbol denotes task  organizers.", "labels": [], "entities": [{"text": "STS interpretable", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8210807144641876}, {"text": "Headlines data set", "start_pos": 121, "end_pos": 139, "type": "DATASET", "confidence": 0.7672486007213593}]}, {"text": " Table 6: STS interpretable results for the system chunks scenario. Best results have been marked in bold. 'H' stands  for Headlines data set and 'I' stands for Images data set. + symbol denotes resubmissions and  *  symbol denotes task  organizers.", "labels": [], "entities": [{"text": "STS interpretable", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8079301118850708}, {"text": "Headlines data set", "start_pos": 123, "end_pos": 141, "type": "DATASET", "confidence": 0.7507823407649994}]}]}