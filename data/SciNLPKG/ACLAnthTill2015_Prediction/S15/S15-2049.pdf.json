{"title": [{"text": "SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking", "labels": [], "entities": [{"text": "Multilingual All-Words Sense Disambiguation", "start_pos": 22, "end_pos": 65, "type": "TASK", "confidence": 0.6360684409737587}, {"text": "Entity Linking", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.720726877450943}]}], "abstractContent": [{"text": "In this paper we present the Multilingual All-Words Sense Disambiguation and Entity Linking task.", "labels": [], "entities": [{"text": "Multilingual All-Words Sense Disambiguation", "start_pos": 29, "end_pos": 72, "type": "TASK", "confidence": 0.6704059913754463}, {"text": "Entity Linking task", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7535820007324219}]}, {"text": "Word Sense Disambiguation (WSD) and Entity Linking (EL) are well-known problems in the Natural Language Processing field and both address the lexical ambiguity of language.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7620509763558706}, {"text": "Entity Linking (EL)", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8169724583625794}]}, {"text": "Their main difference lies in the kind of meaning inventories that are used: EL uses encyclopedic knowledge, while WSD uses lexicographic information.", "labels": [], "entities": []}, {"text": "Our aim with this task is to analyze whether, and if so, how, using a resource that integrates both kinds of inventories (i.e., BabelNet 2.5.1) might enable WSD and EL to be solved by means of similar (even, the same) methods.", "labels": [], "entities": []}, {"text": "Moreover, we investigate this task in a multilingual setting and for some specific domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Senseval and SemEval evaluation series represent key moments in the community of computational linguistics and related areas.", "labels": [], "entities": []}, {"text": "Their focus has been to provide objective evaluations of methods within the wide spectrum of semantic techniques for tasks mainly related to automatic text understanding.", "labels": [], "entities": [{"text": "automatic text understanding", "start_pos": 141, "end_pos": 169, "type": "TASK", "confidence": 0.6163661281267802}]}, {"text": "Through SemEval-2015 task 13 we both continue and renew the longstanding tradition of disambiguation tasks, by addressing multilingual WSD and EL in a joint manner.", "labels": [], "entities": []}, {"text": "WSD) is a historical task aimed at explicitly assigning meanings to single-word and multi-word occurrences within text, a task which today is more alive than ever in the research community.", "labels": [], "entities": [{"text": "WSD)", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.7472473084926605}, {"text": "assigning meanings to single-word and multi-word occurrences within text", "start_pos": 46, "end_pos": 118, "type": "TASK", "confidence": 0.5876840353012085}]}, {"text": "EL) is a more recent task which aims at discovering mentions of entities within a text and linking them to the most suitable entry in a knowledge base.", "labels": [], "entities": []}, {"text": "Both these tasks aim at handling the inherent ambiguity of natural language, however WSD tackles it from a lexicographic perspective, while EL tackles it from an encyclopedic one.", "labels": [], "entities": []}, {"text": "Specifically, the main difference between the two tasks lies in the kind of inventory they use.", "labels": [], "entities": []}, {"text": "For instance, WordNet (), a manually curated semantic network for the English language, has become the main reference inventory for English WSD systems thanks to its wide coverage of verbs, adverbs, adjectives and common nouns.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.9182752370834351}]}, {"text": "More recently, Wikipedia has been shown to bean optimal resource for recovering named entities, and has consequently become -together with all its semi-automatic derivations such as DBpedia ( and) -the main reference inventory for EL systems.", "labels": [], "entities": []}, {"text": "Over the years, the research community has typically focused on each of these tasks separately.", "labels": [], "entities": []}, {"text": "Recently, however, joint approaches have been proposed).", "labels": [], "entities": []}, {"text": "One of the reasons for pursuing the unification of these tasks derives from the current trend in knowledge acquisition which consists of the seamless integration of encyclopedic and lexicographic knowledge within structured language resources.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.7163712978363037}]}, {"text": "A casein point here is BabelNet 1 , a multilingual semantic network and encyclopedic dictionary.", "labels": [], "entities": []}, {"text": "Resources like BabelNet provide a common ground for the tasks of WSD and EL.", "labels": [], "entities": [{"text": "WSD", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.7697052955627441}]}, {"text": "In this task our goal is to promote research in the direction of joint word sense and named entity disambiguation, so as to concentrate research efforts on the aspects that differentiate these two tasks without duplicating research on common problems such as identifying the right meaning in context.", "labels": [], "entities": [{"text": "word sense and named entity disambiguation", "start_pos": 71, "end_pos": 113, "type": "TASK", "confidence": 0.6994322886069616}]}, {"text": "However, we are also interested in systems that perform only one of the two tasks, and even systems which tackle one particular setting of WSD, such as allwords sense disambiguation vs. any subset of partof-speech tags.", "labels": [], "entities": []}, {"text": "Moreover, given the recent upsurge of interest in multilingual approaches, we developed the task dataset in three different languages (English, Italian and Spanish) on parallel texts which have been independently and manually annotated by different native/fluent speakers.", "labels": [], "entities": []}, {"text": "In contrast to the SemEval-2013 task 12 on Multilingual Word Sense Disambiguation ( ), our focus in task 13 is to present a dataset containing both kinds of inventories (i.e., named entities and word senses) in different specific domains (biomedical domain, maths and computer domain, and a broader domain about social issues).", "labels": [], "entities": [{"text": "SemEval-2013 task 12 on Multilingual Word Sense Disambiguation", "start_pos": 19, "end_pos": 81, "type": "TASK", "confidence": 0.6850281171500683}]}, {"text": "Our goal is to further investigate the distance between research efforts regarding the dichotomy EL vs. WSD and those regarding the dichotomy open domain vs. closed domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "The manual annotation of documents was performed in a language-specific manner, i.e., different taggers worked on the various translated versions of the input documents.", "labels": [], "entities": []}, {"text": "More precisely, we had two taggers for each language, who annotated each fragment of text recognized as linkable with all the senses deemed appropriate.", "labels": [], "entities": []}, {"text": "During the annotation procedure, for all languages, each tagger was shown an HTML page containing the sentence within which the target fragment was boldfaced.", "labels": [], "entities": []}, {"text": "Then a gether with the available synonyms and hypernyms (as found in WordNet and the Wikipedia Bitaxonomy ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9623480439186096}]}, {"text": "The taggers agreed on at least one meaning for 68% of the instances.", "labels": [], "entities": []}, {"text": "A third tagger acted as judge by going through all the items and discarding overly general or irrelevant annotations, especially in the case of disagreement between the two taggers.", "labels": [], "entities": []}, {"text": "To enforce coherence and spot missing annotations, we projected the English annotations to the other two languages.", "labels": [], "entities": []}, {"text": "Finally, the third tagger determined if the projected English annotations that were missing in one of the other two languages were either correctly not included, or if the taggers had actually missed a correct annotation.", "labels": [], "entities": []}, {"text": "As a result of this procedure we obtained a dataset with around 1.2k items, but with only around 80 named entity mentions per language.", "labels": [], "entities": []}, {"text": "Please refer to for general statistics about the dataset: we show the number of annotated instances per language and domain, together with their classification as single-or multi-word expressions and named entities.", "labels": [], "entities": []}, {"text": "We then show the degree of ambiguity both per POS and per instance and lemma (i.e., multiple instances with the same lemma count as a single instance) and, finally, we show how many of the instances have Wikipedia pages or WordNet keys as annotations 2 .  To evaluate the performance of the participating systems we used the classical precision, recall and F1 measures: 2 Please note that the sum of Wikipedia pages and WordNet keys does not amount to the number of instances, as BabelNet can have integrated synsets that contain both WordNet keys and Wikipedia pages.", "labels": [], "entities": [{"text": "precision", "start_pos": 335, "end_pos": 344, "type": "METRIC", "confidence": 0.9993385672569275}, {"text": "recall", "start_pos": 346, "end_pos": 352, "type": "METRIC", "confidence": 0.9982692003250122}, {"text": "F1", "start_pos": 357, "end_pos": 359, "type": "METRIC", "confidence": 0.9986870884895325}]}, {"text": "To handle systems that output multiple answers fora single instance we followed the standard scorer of previous Senseval and SemEval challenges in uniformly weighting the multiple answers when computing the TP counts.", "labels": [], "entities": []}, {"text": "Moreover, we decided not to take into account fragments annotated by the systems which were not contained in the gold standard, similarly to the D2KB setting of the GERBIL evaluation framework for EL ().", "labels": [], "entities": [{"text": "GERBIL evaluation framework", "start_pos": 165, "end_pos": 192, "type": "DATASET", "confidence": 0.6685783664385477}]}, {"text": "In we show the detailed performances of all the systems over different classes of items, and on different domains.", "labels": [], "entities": []}, {"text": "One of the main goals of this task is to investigate the performance of disambiguation methods over different domains.", "labels": [], "entities": []}, {"text": "Our documents derive from the biomedical domain, the maths and computer domain, and a broader domain (a document discussing social issues, especially for elderly workers and possible solutions).", "labels": [], "entities": []}, {"text": "In we show the performance of the systems on the biomedical documents.", "labels": [], "entities": []}, {"text": "The first thing to notice is the much higher best score of the first ranked system (i.e., LIMSI), which attains an F1 score of 71.3%.", "labels": [], "entities": [{"text": "LIMSI", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.7840120792388916}, {"text": "F1 score", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9912163317203522}]}, {"text": "This is due to the lower ambiguity of nouns and named entities (see) resulting from the greater numbers of domain-specific concepts used within this kind of documents.", "labels": [], "entities": []}, {"text": "This can also be seen from the higher scores obtained by the BFS.", "labels": [], "entities": [{"text": "BFS", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.8263001441955566}]}, {"text": "Overall, all systems obtained a better performance than in the other domains, with again of more than four percentage points each.", "labels": [], "entities": []}, {"text": "The second ranked system (i.e., SUDOKU) shows its ability to exploit monosemous words obtaining a 0.1 difference from the first ranked system and a 0.9 point distance from the BFS baseline.", "labels": [], "entities": [{"text": "BFS baseline", "start_pos": 176, "end_pos": 188, "type": "DATASET", "confidence": 0.9435521364212036}]}, {"text": "This is of particular interest as the system does not explicitly exploit any sense relevance information.", "labels": [], "entities": []}, {"text": "Moreover, the DFKI system obtains the best scores for nouns and verbs, and is the only system able to obtain a 100% F1 score on NE disambiguation.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9824329614639282}, {"text": "NE disambiguation", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.7824976742267609}]}, {"text": "However, several other systems performed above 90%, showing that in this particular set of documents named entities are easy to disambiguate.", "labels": [], "entities": []}, {"text": "On the other two languages the performances area little bit lower, but the SUDOKU system confirms its ability to exploit monosemous words at a quality comparable to the one obtained in the English dataset.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 189, "end_pos": 204, "type": "DATASET", "confidence": 0.6891532391309738}]}, {"text": "The LIMSI system, instead, obtains a reduction of around 20% due to its exploitation of the BabelSynsetComparator, which performs badly in these languages (see the BFS scores).", "labels": [], "entities": [{"text": "BFS scores", "start_pos": 164, "end_pos": 174, "type": "DATASET", "confidence": 0.7896094024181366}]}, {"text": "In we show the results for the maths and computer domain.", "labels": [], "entities": []}, {"text": "As can be seen in, this is the most ambiguous domain and the best systems obtain much lower performances than in the other domains.", "labels": [], "entities": []}, {"text": "Interestingly, the DFKI system is notable to achieve the best performance on any of the considered item classes, while UNIBA and SUDOKU show the best results for nouns and verbs.", "labels": [], "entities": [{"text": "UNIBA", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.8923044800758362}]}, {"text": "As regards named en-  tities, the system EBL-Hope obtains the best results in all languages.", "labels": [], "entities": []}, {"text": "This system, in addition to exploiting a Lesk-based measure combined with the Jiang & Conrath similarity measure, uses the BabelNet semantic relations, which have already been shown to be useful for attaining state-of-the-art performances in EL ().", "labels": [], "entities": []}, {"text": "Interestingly, in the Italian dataset the system UNIBA (which is based on an extended version of the Lesk measure and a semantic relatedness measure) obtains the same performance for NE as the EBL-Hope system.", "labels": [], "entities": [{"text": "Italian dataset", "start_pos": 22, "end_pos": 37, "type": "DATASET", "confidence": 0.7853588759899139}, {"text": "UNIBA", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.8714252710342407}, {"text": "NE", "start_pos": 183, "end_pos": 185, "type": "TASK", "confidence": 0.8976894617080688}]}, {"text": "In we show the performance on our last domain.", "labels": [], "entities": []}, {"text": "In this social issues domain DFKI confirms its quality on disambiguating nouns and named entities, while for verbs the best system is vua-background, which is based on  the predominant sense algorithm () and, as a fallback routine, on the \"It Makes Sense\" supervised WSD system (.", "labels": [], "entities": []}, {"text": "For the other two languages the SUDOKU system obtains the best scores, with the exception of adverbs in the Italian dataset where the UNIBA system is able to reach an F1 score of 100%.", "labels": [], "entities": [{"text": "Italian dataset", "start_pos": 108, "end_pos": 123, "type": "DATASET", "confidence": 0.8453097641468048}, {"text": "F1 score", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9846541881561279}]}], "tableCaptions": [{"text": " Table 1: Statistics of the datasets.", "labels": [], "entities": []}, {"text": " Table 2: Precision, Recall and F1 on all domains.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9985895752906799}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9987309575080872}, {"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9992145299911499}]}, {"text": " Table 3: F1 performance by item class and language on  all domains.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9967809915542603}]}, {"text": " Table 4: F1 performance by item class and language on  biomedical domain.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9933122992515564}]}, {"text": " Table 5: F1 performance by item class and language on  maths and computer domain.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9977067708969116}]}, {"text": " Table 6: F1 performance by item class and language on  social issues domain.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9935212731361389}]}]}