{"title": [{"text": "KLUEless: Polarity Classification and Association", "labels": [], "entities": [{"text": "KLUEless", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9328533411026001}, {"text": "Polarity Classification and Association", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.7878019064664841}]}], "abstractContent": [{"text": "This paper describes the KLUEless system which participated in the SemEval-2015 task on \"Sentiment Analysis in Twitter\".", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter", "start_pos": 89, "end_pos": 118, "type": "TASK", "confidence": 0.9077789485454559}]}, {"text": "This year the updated system based on the developments for the same task in 2014 (Evert et al., 2014) and 2013 (Proisl et al., 2013) participated in all five subtasks.", "labels": [], "entities": []}, {"text": "The paper gives an overview of the core features extended by different additional features and parameters required for individual subtasks.", "labels": [], "entities": []}, {"text": "Experiments carried out after the evaluation period on the test dataset 2015 with the gold standard available are integrated into each subtask to explain the submitted feature selection.", "labels": [], "entities": []}], "introductionContent": [{"text": "The SemEval-2015 shared task on \"Sentiment Analysis in Twitter\" () is a rerun of the shared task from) with three new subtasks.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.8342574536800385}]}, {"text": "While subtasks A and B were identical to the tasks of and dealt with the identification of polarity in a given message, subtask C, D and E were new.", "labels": [], "entities": [{"text": "identification of polarity in a given message", "start_pos": 73, "end_pos": 118, "type": "TASK", "confidence": 0.8146632058279855}]}, {"text": "In subtask Ca topic was given, towards which the sentiment in a message had to be identified.", "labels": [], "entities": []}, {"text": "Subtask D was similar to subtask C, as the sentiment towards a given topic had to be identified, but in this subtask several messages were given from which the sentiment had to be drawn.", "labels": [], "entities": []}, {"text": "Ultimately in subtask E, the sentiment of a given word or phrase had to be measured on a score ranging, indicating its association with positive sentiment.", "labels": [], "entities": []}, {"text": "The training data for subtasks A and B are the same as in and).", "labels": [], "entities": []}, {"text": "For subtask A, there are 9,505 training items with 6,769 items in development set and 3,912 items in the test set.", "labels": [], "entities": []}, {"text": "For subtask B, there are 10,239 training items, 5,907 items in the development set and 3,861 in the test set.", "labels": [], "entities": []}, {"text": "For subtasks C and D the same training sets as for subtasks A and B were used by our team.", "labels": [], "entities": []}, {"text": "A pilot task E aimed at evaluation of automatic methods of generating sentiment lexicons had no training set, a detailed approach used for this subtask will be given in Section 3.", "labels": [], "entities": []}, {"text": "This paper describes the updated system with our efforts to improve it after the evaluation period.", "labels": [], "entities": []}, {"text": "The KLUEless system was ranked within the top 3 participants to subtasks A (rank 2 out of 11), C (rank 2 out of 7) and D (best result out of 6 teams).", "labels": [], "entities": [{"text": "KLUEless", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.5745559930801392}]}, {"text": "It scored 5th place in subtask E, but only 13th place in subtask B (rank 13 out of 40 teams).", "labels": [], "entities": []}, {"text": "In the following chapters, we will describe the way KLUEless dealt with the tasks stated and our results for these tasks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Evaluation results for subtask A on the test set  2015.", "labels": [], "entities": [{"text": "test set  2015", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.828680415948232}]}, {"text": " Table 2: Evaluation results for subtask B on the test set  2015.", "labels": [], "entities": [{"text": "test set  2015", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.8252021074295044}]}, {"text": " Table 3: Average absolute difference depending on factor  A on the test set 2015.", "labels": [], "entities": [{"text": "Average absolute difference", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.8536746501922607}]}, {"text": " Table 4: Results for different settings for frequency and  cluster threshold parameters (t f : frequency threshold for  back-off, t c : cluster proportion threshold).", "labels": [], "entities": []}, {"text": " Table 5: Results for different bias correction settings (b:  assumed proportion of positive tweets in population).", "labels": [], "entities": []}, {"text": " Table 6: Results for conservative estimates using differ- ent confidence levels (b: assumed proportion of positive  tweets in population, c: confidence level for conservative  estimates).", "labels": [], "entities": [{"text": "differ- ent confidence", "start_pos": 51, "end_pos": 73, "type": "METRIC", "confidence": 0.8936380594968796}]}, {"text": " Table 7: Results for conservative estimates using differ- ent bias correction settings (b: assumed proportion of  positive tweets in population, c: confidence level for con- servative estimates).", "labels": [], "entities": [{"text": "differ- ent bias correction", "start_pos": 51, "end_pos": 78, "type": "METRIC", "confidence": 0.8365700006484985}]}]}