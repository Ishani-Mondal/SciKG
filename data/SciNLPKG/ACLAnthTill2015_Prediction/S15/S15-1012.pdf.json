{"title": [{"text": "Collective Document Classification with Implicit Inter-document Semantic Relationships", "labels": [], "entities": [{"text": "Collective Document Classification", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6881428162256876}]}], "abstractContent": [{"text": "This paper addresses the question of how document classifiers can exploit implicit information about document similarity to improve document classifier accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.8081004023551941}]}, {"text": "We infer document similarity using simple n-gram overlap , and demonstrate that this improves overall document classification performance over two datasets.", "labels": [], "entities": [{"text": "document classification", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.6681665480136871}]}, {"text": "As part of this, we find that collective classification based on simple iterative classifiers outperforms the more complex and computationally-intensive dual classifier approach.", "labels": [], "entities": [{"text": "collective classification", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.6360022276639938}]}], "introductionContent": [{"text": "In machine learning, there is a rich tradition of research into the two tasks of: (1) \"point-wise\" classification, where each instance is represented as an independent instance, and the predictive model attempts to learn a decision boundary to capture instances of a given class; and (2) graphical learning and inference, where instances are connected in a graph, and learning/inference take place relative to the graph structure connecting those instances, based primarily on either conditional dependence (i.e. one event is dependent on the outcome of another) or \"homophily\" (i.e. the tendency for connected instances to share various properties).", "labels": [], "entities": [{"text": "point-wise\" classification", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.7314162055651346}]}, {"text": "Various joint models that combine the two have also been proposed, although in natural language processing at least, these have focused largely on conditional dependence, in the form of models such as hidden Markov models and conditional random fields (), where independent properties of words, e.g., are combined with conditional dependencies based on their context of use to jointly predict the senses of all words in a given sentence.", "labels": [], "entities": []}, {"text": "This paper explores the utility of homophily within joint models for document-level semantic classification, focusing specifically on tasks which are not associated with any explicit graph structure.", "labels": [], "entities": [{"text": "document-level semantic classification", "start_pos": 69, "end_pos": 107, "type": "TASK", "confidence": 0.7234597404797872}]}, {"text": "That is, we examine whether implicit semantic document links can improve the results of a point-wise (content-based) classification approach.", "labels": [], "entities": []}, {"text": "Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents), direct name-references in congressional debates (, citations in scientific papers (, and user mentions or retweets in social media).", "labels": [], "entities": []}, {"text": "However, document collections often don't contain explicit inter-document links, limiting the practical usefulness of such methods.", "labels": [], "entities": []}, {"text": "In this paper, we seek to expand the reach of research which incorporates linking information, in inducing implicit linking information between documents, and demonstrating that the resultant (noisy) network structure improves document classification accuracy.", "labels": [], "entities": [{"text": "document classification", "start_pos": 227, "end_pos": 250, "type": "TASK", "confidence": 0.731508880853653}, {"text": "accuracy", "start_pos": 251, "end_pos": 259, "type": "METRIC", "confidence": 0.7180060148239136}]}, {"text": "The intuition underlying this work is that some types of documents have features which are either absent or ambiguous in training data, but which have the special characteristic of indicating relationships between the labels of documents.", "labels": [], "entities": []}, {"text": "Most often, an inter-document relationship indicates that two documents have the same label, but depending of the task, it may also indicate that they have different labels.", "labels": [], "entities": []}, {"text": "In either case, classifiers gain an advantage if they can consider these features as well as conventional content-based features.", "labels": [], "entities": []}, {"text": "The major contribution of this paper is in showing that document classification accuracy can be improved over a range of datasets using automaticallyinduced implicit semantic inter-document links, using collective classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.7718658447265625}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.8756133317947388}]}, {"text": "We are the first to achieve this using a general-purpose setup, as applied to a range of datasets.", "labels": [], "entities": []}, {"text": "Our results are achieved using n-gram overlap features for both the CON-VOTE and BITTERLEMONS corpora, without the use of annotations for explicit semantic inter-document relationships.", "labels": [], "entities": [{"text": "BITTERLEMONS", "start_pos": 81, "end_pos": 93, "type": "METRIC", "confidence": 0.9264912009239197}]}, {"text": "A second contribution of this work is the finding that simple iterative classifiers outperform more complex dual classifiers when using implicit inter-document links.", "labels": [], "entities": []}, {"text": "This finding contradicts earlier work using explicit document links, where the dual classifier approach has generally been found to perform best ().", "labels": [], "entities": []}, {"text": "While the work presented here is conceptually quite simple, the findings are significant and potentially open the door to accuracy improvements on a range of document-level semantic tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9965711832046509}]}], "datasetContent": [{"text": "We assess the accuracy of the dual classifier and iterative classifier approaches described above over CONVOTE and BITTERLEMONS in terms of classification accuracy, micro-averaging across the 53 folds of cross-validation in the case of CONVOTE.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9984064698219299}, {"text": "BITTERLEMONS", "start_pos": 115, "end_pos": 127, "type": "METRIC", "confidence": 0.985917866230011}, {"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9144728183746338}]}, {"text": "When quoted, statistical significance has been determined: Collective classification performance on BITTERLEMONS ( signifies a statistically significant improvement over the content-only baseline, p < 0.05).", "labels": [], "entities": [{"text": "Collective classification", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.6724371612071991}, {"text": "BITTERLEMONS", "start_pos": 100, "end_pos": 112, "type": "METRIC", "confidence": 0.7930382490158081}]}, {"text": "using approximate randomisation with p < 0.05.", "labels": [], "entities": []}, {"text": "Two baseline scores are shown in the tables for collective classification results: (1) \"Majority\" gives the performance of the simplest possible classifier, which classifies every instance with the label that is most frequent in training data; and (2) \"Contentonly\" gives the performance of the bag-of-words linear-kernel SVM used to perform base classification.", "labels": [], "entities": []}, {"text": "shows the overall collective classifier performance on CONVOTE.", "labels": [], "entities": [{"text": "CONVOTE", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.8931372165679932}]}, {"text": "The best performer is the iterative classifier with 4-grams, with an accuracy of 79.05%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9995365142822266}]}, {"text": "This is a statistically significant 2.65% absolute gain over the content-only baseline.", "labels": [], "entities": []}, {"text": "The iterative classifier is the best performer in general, obtaining the next four best results with statistically significant absolute gains of 2.41%, 1.76%, 1.70% and 1.59% for 3-grams, 5-grams, 2-grams and 1-grams respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics for CONVOTE.", "labels": [], "entities": [{"text": "CONVOTE", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.8155763149261475}]}, {"text": " Table 3: Collective classification performance on CONVOTE ( signifies a statistically significant improve- ment over the content-only baseline, p < 0.05).", "labels": [], "entities": [{"text": "CONVOTE", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.8799700736999512}]}, {"text": " Table 4: Collective classification performance on BITTERLEMONS ( signifies a statistically significant  improvement over the content-only baseline, p < 0.05).", "labels": [], "entities": [{"text": "Collective classification", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.5962381958961487}, {"text": "BITTERLEMONS", "start_pos": 51, "end_pos": 63, "type": "METRIC", "confidence": 0.6103683114051819}]}]}