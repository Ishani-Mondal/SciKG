{"title": [{"text": "CPH: Sentiment analysis of Figurative Language on Twitter #easypeasy #not", "labels": [], "entities": [{"text": "CPH", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9320333003997803}, {"text": "Sentiment analysis of Figurative Language", "start_pos": 5, "end_pos": 46, "type": "TASK", "confidence": 0.8810415387153625}]}], "abstractContent": [{"text": "This paper describes the details of our sys", "labels": [], "entities": [{"text": "sys", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.8315058946609497}]}], "introductionContent": [{"text": "Sentiment analysis (SA) is the task of determining the sentiment of a given piece of text.", "labels": [], "entities": [{"text": "Sentiment analysis (SA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9411750674247742}, {"text": "determining the sentiment of a given piece of text", "start_pos": 39, "end_pos": 89, "type": "TASK", "confidence": 0.6883139080471463}]}, {"text": "The amplitude of user-generated content produced everyday raises the importance of accurate automatic sentiment analysis, for applications ranging from, e.g., reputation analysis) to election results prediction.", "labels": [], "entities": [{"text": "accurate automatic sentiment analysis", "start_pos": 83, "end_pos": 120, "type": "TASK", "confidence": 0.6370259448885918}, {"text": "reputation analysis", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.8137715756893158}, {"text": "election results prediction", "start_pos": 183, "end_pos": 210, "type": "TASK", "confidence": 0.6664643088976542}]}, {"text": "However, figurative language is pervasive in usergenerated content, and figures of speech like irony, sarcasm and metaphors impose relevant challenges fora sentiment analysis system usually trained on literal meanings.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.812796950340271}]}, {"text": "For instance, consider the following example: 2 @CIA We hear you're looking for sentiment analysis to detect sarcasm in Tweets.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.908212274312973}]}, {"text": "Irony or sarcasm does not result always in the exact opposite sentiment and therefore it is not as simple as just inverting the scores from a general SA system.", "labels": [], "entities": [{"text": "Irony", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9172802567481995}]}, {"text": "Only few studies have attempted SA on figurative language so far.", "labels": [], "entities": []}, {"text": "The prediction of a fine-grained sentiment score (between -5 and 5) fora tweet poses a series of challenges.", "labels": [], "entities": []}, {"text": "First of all, accurate language technology on tweets is hard due to sample bias, i.e., collections of tweets are inherently biased towards the particular time (or way, cf. \u00a72) they were collected).", "labels": [], "entities": []}, {"text": "Secondly, the notion of figurativeness (or its complementary notion of literality) does not have a strong definition, let alone do irony, sarcasm, or satire.", "labels": [], "entities": []}, {"text": "As pointed out by, \"there is not a clear distinction about the boundaries among these terms\".", "labels": [], "entities": []}, {"text": "Yet alone attaching a fine-grained score is far from straightforward.", "labels": [], "entities": []}, {"text": "In fact, the gold standard consists of the average score assigned by humans through crowdsourcing reflecting an uncertainty in ground truth.", "labels": [], "entities": []}], "datasetContent": [{"text": "Train Trial Test Instances 7988 920 4000 % Non-figurative 1% 7% 25% Since there are obvious subgroups in the data, our hypothesis is that this fact can be used to construct a more informed baseline.", "labels": [], "entities": [{"text": "Train Trial Test Instances 7988 920", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.8548180957635244}]}, {"text": "In fact ( \u00a7 4.1), simply predicting the mean per subgroup pushed the constant mean baseline performance considerably (from 0.73 to 0.81 Cosine, compared to random 0.59).", "labels": [], "entities": []}, {"text": "plots predicted scores (ridge model, \u00a73.1) of three subgroups against the gold scores on the trial data.", "labels": [], "entities": []}, {"text": "It can be seen that certain subgroups have similar behaviour, 'sarcasm' has a generally negative cloud and the model performs well in predicting these values, while other groups such as 'SoToSpeak' have more intra-group variance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Features used in Single Models.", "labels": [], "entities": []}, {"text": " Table 5: Baseline and Single Systems On Trial Data.", "labels": [], "entities": []}, {"text": " Table 7: Submission System Test Results. 5", "labels": [], "entities": [{"text": "Submission System Test", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.856217642625173}]}, {"text": " Table 8: Cosine Test Results Breakdown.", "labels": [], "entities": [{"text": "Cosine Test Results Breakdown", "start_pos": 10, "end_pos": 39, "type": "DATASET", "confidence": 0.6751624643802643}]}]}