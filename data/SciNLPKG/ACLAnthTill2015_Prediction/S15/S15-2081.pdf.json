{"title": [], "abstractContent": [{"text": "CLaC Labs participated in two shared tasks for SemEval2015, Task 10 (subtasks B and E) and Task 11.", "labels": [], "entities": [{"text": "CLaC Labs", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.941730260848999}, {"text": "SemEval2015", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.8445483446121216}]}, {"text": "The underlying system configuration is nearly identical and consists of two major components: a large Twitter lexicon compiled from tweets that carry certain selected hashtags (assumed to guarantee a sentiment polarity) and then inducing that same polarity for the words that occur in the tweets.", "labels": [], "entities": []}, {"text": "We also use standard sentiment lexica and combine the results.", "labels": [], "entities": []}, {"text": "The lexical sentiment features are further differentiated according to some linguistic contexts in which their triggers occur, including bigrams, negation, modality, and dependency triples.", "labels": [], "entities": []}, {"text": "We studied feature combinations comprehensively for their interoperability and effectiveness on different datasets using the exhaustive feature combination technique of (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b).", "labels": [], "entities": []}, {"text": "For Subtask 10B we used a SVM, and a decision tree regressor for Task 11.", "labels": [], "entities": []}, {"text": "The resulting systems ranked ninth for Subtask 10B, fourth for Subtask 10E, and first for Task 11.", "labels": [], "entities": []}], "introductionContent": [{"text": "The field of Sentiment Analysis is in its second phase: initially, the task was defined, annotation standards, corpora, and feature resources were identified and provided to the research community (see).", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.9668264091014862}]}, {"text": "Now, we have regular community challenges such as the SemEval Twitter Sentiment shared tasks which allow us to compare different feature choice and combination across research labs and across successive data sets.", "labels": [], "entities": [{"text": "SemEval Twitter Sentiment shared tasks", "start_pos": 54, "end_pos": 92, "type": "TASK", "confidence": 0.8056142926216125}]}, {"text": "We describe here the systems we submitted to SemEval15 for Twitter Sentiment Analysis at the tweet level (Task 10B) and Figurative Language in Twitter (Task 11).", "labels": [], "entities": [{"text": "Twitter Sentiment Analysis", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.6056545178095499}]}, {"text": "The tasks and the design of the datasets is described in detail in ( for Task 10 and in () for Task 11.", "labels": [], "entities": []}, {"text": "We also submitted a sentiment lexicon transformed from our in-house lexical resource for Task 10E.", "labels": [], "entities": []}, {"text": "Our system is based on a pipeline design in 5 major phases, described below.", "labels": [], "entities": []}, {"text": "Following standard text preprocessing, we use Stanford dependencies) and linguistic features negation, modality and their scope in connection with standard sentiment lexica from the literature and an in-house lexical resource compiled with the technique used for the NRC lexicon ().", "labels": [], "entities": [{"text": "NRC lexicon", "start_pos": 267, "end_pos": 278, "type": "DATASET", "confidence": 0.935003399848938}]}, {"text": "These features were successful in both Task 10B (rank 9 on 40 for Twitter 2015 data, seventh on 40 for Twitter 2015 sarcasm data) and Task 11 (rank 1 of 35 runs by 15 teams).", "labels": [], "entities": [{"text": "Twitter 2015 data", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.9202792843182882}, {"text": "Twitter 2015 sarcasm data", "start_pos": 103, "end_pos": 128, "type": "DATASET", "confidence": 0.8637156933546066}]}, {"text": "Our sentiment lexicon submitted to Task 10E ranked fourth often.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison NRC and Gezi.", "labels": [], "entities": [{"text": "NRC", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.7498953342437744}, {"text": "Gezi", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.8216436505317688}]}, {"text": " Table 2: aFinn features for Example 1.", "labels": [], "entities": [{"text": "aFinn", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.7306387424468994}]}, {"text": " Table 3: Feature subset bundles.", "labels": [], "entities": []}, {"text": " Table 4: Official CLaC-SentiPipe results for Task 10B: rank 9.", "labels": [], "entities": []}, {"text": " Table 5: CLaC-SentiPipe in Task 11: rank 1.", "labels": [], "entities": []}]}