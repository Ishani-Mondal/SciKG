{"title": [{"text": "ECNU: Using Multiple Sources of CQA-based Information for Answer Selection and YES/NO Response Inference", "labels": [], "entities": [{"text": "ECNU", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8720682859420776}, {"text": "Answer Selection", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.9211041629314423}, {"text": "YES/NO Response Inference", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.7418881535530091}]}], "abstractContent": [{"text": "This paper reports our submissions to community question answering task in SemEval-2015, which consists of two subtasks: (1) predict the quality of answers to given question as good, bad, or potentially relevant and (2) identify yes, no or unsure response to a given YES/NO question based on the good answers identified by subtask 1.", "labels": [], "entities": [{"text": "community question answering task", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.6800115257501602}]}, {"text": "For both subtasks, we adopted supervised classification method and examined the effects of heterogeneous features generated from community question answering data, such as bag-of-words, string matching, semantic similarity, answerer information , answer-specific features, question-specific features, etc.", "labels": [], "entities": []}, {"text": "Our submitted primary systems ranked the forth and the second for the two subtasks of English data respectively.", "labels": [], "entities": [{"text": "English data", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.7546904385089874}]}], "introductionContent": [{"text": "Community Question Answering (CQA) systems such as Yahoo!Answers rely on users to provide answers (i.e., user generated content) for questions posted.", "labels": [], "entities": [{"text": "Community Question Answering (CQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7542950262626013}, {"text": "Yahoo!Answers", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.8913665612538656}]}, {"text": "Generally such systems are quite open and the answers provided by users are not always of high quality.", "labels": [], "entities": []}, {"text": "For example, a bad answer may present irrelevant opinions or issues, contain only URL links without direct answer, or even be written informally.", "labels": [], "entities": []}, {"text": "Therefore, in order to achieve high-quality user experience and maintain high levels of adherence, it is critical to present high-quality answers and provide direct responses for users.", "labels": [], "entities": []}, {"text": "The CQA task in) provides such a universal platform for researchers to make a comparison between different approaches.", "labels": [], "entities": []}, {"text": "This task consists of two subtasks: (1) subtask A is to classify the quality of answers as good, potential or bad, which also refers to the task of answer quality prediction (; (2) subtask B is to infer the global answer of a YES/NO question to be yes, no or unsure based on individual good answers.", "labels": [], "entities": [{"text": "answer quality prediction", "start_pos": 148, "end_pos": 173, "type": "TASK", "confidence": 0.6003758907318115}, {"text": "YES/NO question", "start_pos": 226, "end_pos": 241, "type": "TASK", "confidence": 0.4508538246154785}]}, {"text": "Most of the previous research on answer quality prediction has focused on extracting various features to employ ranking or classification methods, such as textual features including the length of an answer, overlapped words between a question-answer (QA) pair, etc.", "labels": [], "entities": [{"text": "answer quality prediction", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.8971359332402548}]}, {"text": "Another kind of widely used feature is extracted from answerer profile information, such as the number of best answers, the achieved levels and the earned points.", "labels": [], "entities": []}, {"text": "However, such information is not often available in real world.", "labels": [], "entities": []}, {"text": "Moreover, a recent study () has taken question type into consideration to make the answers quality prediction.", "labels": [], "entities": []}, {"text": "In this paper, we built two classification systems for the two tasks respectively.", "labels": [], "entities": []}, {"text": "For Task A, we extracted six types of features from multiple sources of CQA-based information to predict the answer quality, such as answer-, question-, answerer-specific information, surface word similarity and semantic similarity between question-answer pair, ect.", "labels": [], "entities": []}, {"text": "For Task B, the global answer of a YES/NO question is summarized just from the individual good answers identified by Task A. Specifically, we first built a classifier to predict Yes/No/Unsure labels for each predicted good answer, then we performed a majority voting to summarize the global answer for each question.", "labels": [], "entities": [{"text": "YES/NO question", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.5906485915184021}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our systems, including features, algorithms, etc.", "labels": [], "entities": []}, {"text": "Section 3 shows experiments on training data and results on test data.", "labels": [], "entities": []}, {"text": "Finally, conclusions and future work are given in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The official evaluation measures for both tasks is macro-averaged F 1 . For Task A the official score is calculated on three labels: Good, Bad, Potential (where Bad includes Dialogue, Not English and Other).", "labels": [], "entities": []}, {"text": "We performed algorithm choosing experiments using all designed features.", "labels": [], "entities": []}, {"text": "All the parameters of algorithms are set to be default values from scikit-learn (Pedregosa et al., 2011).", "labels": [], "entities": []}, {"text": "lists the preliminary algorithm comparison experimental results.", "labels": [], "entities": []}, {"text": "We found SVM with linear kernel outperforms other algorithm choices for both tasks.", "labels": [], "entities": []}, {"text": "Moreover, we tuned the trade-off parameter c of SVM and when set c to 0.8 we obtained a better score 54.78% and 58.82% for Task A and B respectively.", "labels": [], "entities": []}, {"text": "Therefore, in the following experiments on training and test data, we set the algorithm to SVM with linear kernel.", "labels": [], "entities": []}, {"text": "We performed a series of experiments for both tasks to explore the effects of various feature types using SVM (linear).", "labels": [], "entities": []}, {"text": "In Task B we always chose the predicted good answers from the system with the best macro-F 1 in Task A.   First, for both tasks the most effective feature type is bag-of-words from answer and this feature alone achieves 48.91% for Task A and 47.82% for Task B, which both outperforms the baseline system provided by organizers respectively.", "labels": [], "entities": []}, {"text": "The baseline of Task A which predicts all answers as good just achieves 22.36% and for Task B it achieves 25.0% which predict all answers as yes.", "labels": [], "entities": []}, {"text": "Moreover, in Task A the performance of other five feature types alone is far lower than bag-of-words, ranging from 23% to 38% approximately.", "labels": [], "entities": []}, {"text": "Second, for Task A, when combining all the features together the system achieves the best performance, which indicates that all types of features make contribution more or less.", "labels": [], "entities": []}, {"text": "Specially, among the six types of features, answerer information and semantic similarity between QA pairs make more contribution than others.", "labels": [], "entities": []}, {"text": "This indicates that answerer profile information is important, which is consistent with the findings in (.", "labels": [], "entities": []}, {"text": "Besides, the semantic similarity captures deep relationship between Q-A pair than the surface word, which is helpful for performance improvement.", "labels": [], "entities": []}, {"text": "In Task B, we also observed the similar findings, i.e., the system using all types of features achieves the best performance.", "labels": [], "entities": []}, {"text": "Moreover, the YES/NO word list feature makes great contribution to the performance improvement.", "labels": [], "entities": [{"text": "YES/NO word list", "start_pos": 14, "end_pos": 30, "type": "METRIC", "confidence": 0.7170600056648254}]}, {"text": "This is consistent with our expectation.", "labels": [], "entities": []}, {"text": "Besides, although in this work the word vector feature improves the performance, this improvements is not as much as our expectation.", "labels": [], "entities": []}, {"text": "The possible reason maybe the simple way of using the vector by only summing up.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on training data for different algorithms.", "labels": [], "entities": []}, {"text": " Table 3: Configurations and results of our three submitted  systems and top three results, the numbers in bracket are  the official ranking out of all submitted systems.", "labels": [], "entities": []}]}