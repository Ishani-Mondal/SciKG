{"title": [{"text": "A State-of-the-Art Mention-Pair Model for Coreference Resolution", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.963260293006897}]}], "abstractContent": [{"text": "Most recent studies on coreference resolution advocate accurate yet relatively complex models, relying on, for example, entity-mention or graph-based representations.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.978220522403717}]}, {"text": "As it has been convincingly demonstrated at the recent CoNLL 2012 shared task, such algorithms considerably outperform popular basic approaches, in particular mention-pair models.", "labels": [], "entities": [{"text": "CoNLL 2012 shared task", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.8545264303684235}]}, {"text": "This study advocates a novel approach that keeps the simplicity of a mention-pair framework, while showing state-of-the-art results.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9952126741409302}]}, {"text": "Apart from being very efficient and straightforward to implement, our model facilitates experimental work on the pairwise classifier, in particular on feature engineering.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 151, "end_pos": 170, "type": "TASK", "confidence": 0.7986443638801575}]}, {"text": "The proposed model achieves the performance level of up to 61.82% (MELA F, v4 scorer) on the CoNLL test data, on par with complex state-of-the-art systems.", "labels": [], "entities": [{"text": "MELA F, v4 scorer)", "start_pos": 67, "end_pos": 85, "type": "METRIC", "confidence": 0.8252713779608408}, {"text": "CoNLL test data", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.963564376036326}]}], "introductionContent": [{"text": "The mention-pair model, as proposed by has been used for over a decade now.", "labels": [], "entities": []}, {"text": "It combines a simple classifier trained to discriminate between coreferent and not-coreferent pairs of mentions (\"links\") with fast heuristic procedures for merging the classifier's decisions at the decoding stage.", "labels": [], "entities": []}, {"text": "Several decoding heuristics have been advocated in the literature, the most commonly used ones including first-link () and best-link ().", "labels": [], "entities": []}, {"text": "Most state-of-the-art algorithms for coreference resolution, on the contrary, rely on complex modeling, ranging from entity-ranking to structural perceptron and other graph-based approaches (for an overview of state-of-the-art coreference resolvers, see).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.9784805774688721}, {"text": "coreference resolvers", "start_pos": 227, "end_pos": 248, "type": "TASK", "confidence": 0.8429142832756042}]}, {"text": "Such algorithms show a clearly superior performance: thus, at the CoNLL-2012 shared task, the best-performing ()-style system loses around 8% to the winning algorithm.", "labels": [], "entities": [{"text": "CoNLL-2012 shared task", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.7684908310572306}]}, {"text": "However, more traditional mention-pair approaches still have some important advantages.", "labels": [], "entities": []}, {"text": "Thus, a mention-pair model is easy to implement and allows for fast prototyping.", "labels": [], "entities": []}, {"text": "It relies on a simple binary classifier making it very fast to train compared to state-of-the-art models that are based on complex structural representations.", "labels": [], "entities": []}, {"text": "This efficiency at the training step allows for straightforward automatic parameter optimization.", "labels": [], "entities": [{"text": "parameter optimization", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.7340124845504761}]}, {"text": "Most importantly, mention-pair models can be useful for understanding low-level system behavior, and, in particular, for feature engineering.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.8602791130542755}]}, {"text": "This can in turn help improve more complex models, since many of them rely on mention-pairs as their basic building blocks.", "labels": [], "entities": []}, {"text": "In this paper, we advocate anew easy-first mention-pair algorithm (EFMP): while it is based solely on pairs of mentions and does not attempt any global inference, it benefits from the decision propagation strategy to create a coreference partition.", "labels": [], "entities": []}, {"text": "Augmented with the sieve-style prefiltering, the system achieves a performance level comparable to the state of the art.", "labels": [], "entities": []}, {"text": "The contribution of this paper is two-fold.", "labels": [], "entities": []}, {"text": "First, we propose a novel decoding approach that combines predictions of the mention-pair classifier based on its confidence score, taking into account-in contrast to the previous studies, e.g. ()-both positive and negative links.", "labels": [], "entities": []}, {"text": "We 289 thus propose a procedure for propagating positive and negative links to create the final coreference partition: we start from the most confident among all the classifier's decisions and iteratively construct coreference partitions by merging coreference chains (positive links) or blacklisting future merges (negative links).", "labels": [], "entities": []}, {"text": "This decoding strategy is slower than the commonly used best-link model, but considerably faster than ILP-based decoding.", "labels": [], "entities": []}, {"text": "Second, we show that our approach, being very fast and easy to implement, can be used fora variety of low-level experiments on coreference resolution, in particular, for studies on feature engineering or selection.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.9510135054588318}, {"text": "feature engineering or selection", "start_pos": 181, "end_pos": 213, "type": "TASK", "confidence": 0.7959642857313156}]}, {"text": "Thus, we augment our system with two feature combination techniques, Jaccard Item Mining () and Entropy Guided Feature Inductions.", "labels": [], "entities": [{"text": "Jaccard Item Mining", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.6892330845197042}]}, {"text": "While the latter has been used for coreference resolution before, Jaccard Item Mining (JIM), to our knowledge, has never been applied to any NLP task.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.9762206375598907}, {"text": "Jaccard Item Mining (JIM", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.6054753720760345}]}, {"text": "The JIM algorithm has been developed within the data mining community and aims at finding combinations that tend to occur in a particular set of unlabeled transactions.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a post-filtering technique to re-score JIM output w.r.t. the class labels (\u00b1coreferent).", "labels": [], "entities": []}, {"text": "We show empirically that JIM is more suitable for coreference: it provides smaller and more meaningful feature combinations leading to a better performance level.", "labels": [], "entities": [{"text": "coreference", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9816814064979553}]}, {"text": "The combination of our decoding approach with the JIM feature induction technique allows us to achieve a performance level of 61.82% on the CoNLL-2012 test data, just 1.5% percent below the (much more complex) winning system and above all the other submissions (cf.).", "labels": [], "entities": [{"text": "JIM feature induction", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.5394296745459238}, {"text": "CoNLL-2012 test data", "start_pos": 140, "end_pos": 160, "type": "DATASET", "confidence": 0.9771068294843038}]}], "datasetContent": [{"text": "Our first group of experiments assesses the quality of the baseline setting, with no feature combination techniques.", "labels": [], "entities": []}, {"text": "We compare against the CoNLL submission of the BART group to make sure that our ()-style mention-pair baseline shows an acceptable performance.", "labels": [], "entities": [{"text": "CoNLL submission", "start_pos": 23, "end_pos": 39, "type": "DATASET", "confidence": 0.8624108731746674}, {"text": "BART group", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.7126935422420502}]}, {"text": "We then evaluate the EFMP approach to confirm that it provides much higher performance figures and is on par with the state of the art.", "labels": [], "entities": []}, {"text": "In our second experiment, we use EFMP to assess the impact of the feature combination techniques on the performance of a coreference resolution system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.9358424544334412}]}, {"text": "We evaluate our approach on the English portion of the CoNLL-2012 dataset).", "labels": [], "entities": [{"text": "English portion of the CoNLL-2012 dataset", "start_pos": 32, "end_pos": 73, "type": "DATASET", "confidence": 0.7660690446694692}]}, {"text": "To asses the system's performance, we use the official scorer, provided by the CoNLL organizers.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.9228266477584839}]}, {"text": "However, the version used at the competition time (v4) was later found to contain errors and replaced with another implementation (v7).", "labels": [], "entities": []}, {"text": "This procedure resulted in a performance drop for all the systems, but didn't affect their ranking.", "labels": [], "entities": []}, {"text": "To facilitate comparison against previous and future studies, we report both v4 and v7 MELA scores.", "labels": [], "entities": [{"text": "MELA", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.8205893039703369}]}, {"text": "All the experiments are performed on automatically extracted mentions and use no gold information.", "labels": [], "entities": []}, {"text": "For our study, we use the publicly available BART toolkit ( ).", "labels": [], "entities": [{"text": "BART toolkit", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.5956354439258575}]}, {"text": "We have made several adjustments, starting from the configuration, suggested in the BART distribution for the OntoNotes/CoNLL data.", "labels": [], "entities": [{"text": "BART", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.8490580916404724}, {"text": "OntoNotes/CoNLL data", "start_pos": 110, "end_pos": 130, "type": "DATASET", "confidence": 0.8843055665493011}]}, {"text": "Thus, we have modified the mention detection module, improving the treatment of coordinations and eliminating numeric named entities (PERCENT, MONEY etc).", "labels": [], "entities": [{"text": "mention detection", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.8059707581996918}, {"text": "PERCENT", "start_pos": 134, "end_pos": 141, "type": "METRIC", "confidence": 0.9952579140663147}, {"text": "MONEY", "start_pos": 143, "end_pos": 148, "type": "METRIC", "confidence": 0.9366785287857056}]}, {"text": "We have replaced the original split architecture with a single-classifier approach to be able to estimate the impact of our feature combination techniques in a more principled way.", "labels": [], "entities": []}, {"text": "We have also replaced Decision Trees (Weka J48) with the LibLinear SVM package, to get a classifier outputting reliable confidence values, as needed by EFMP.", "labels": [], "entities": [{"text": "Weka J48)", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.8559485673904419}, {"text": "EFMP", "start_pos": 152, "end_pos": 156, "type": "DATASET", "confidence": 0.8975947499275208}]}, {"text": "We have considerably expanded the feature set, mainly reimplementing features from the winning system of).", "labels": [], "entities": []}, {"text": "Altogether, we have around 170 individual features (string, nominal or binary values), corresponding to around 20k features after the binarization step.", "labels": [], "entities": []}, {"text": "The full list of our feature templates can be found at http://bart-coref.eu/papers/ sem15-suppl.pdf.", "labels": [], "entities": []}, {"text": "Finally, we have augmented BART with a rulebased prefiltering module, motivated by Stanford Sieves (), the winning approach of the CoNLL-2011 shared task.", "labels": [], "entities": [{"text": "BART", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.9182930588722229}]}, {"text": "Our sieve-style prefiltering algorithm splits all the training instances into confidently positive, confidently negative, irrelevant and relevant.", "labels": [], "entities": []}, {"text": "module, we have started with the original sieves and the version used by.", "labels": [], "entities": []}, {"text": "We have changed some sieves and introduced several additional filters (cf..", "labels": [], "entities": []}, {"text": "shows the performance levels for different baseline algorithms, learners and features on both CoNLL-2012 development and test sets.", "labels": [], "entities": [{"text": "CoNLL-2012 development and test sets", "start_pos": 94, "end_pos": 130, "type": "DATASET", "confidence": 0.8827957391738892}]}, {"text": "Note that the development set was used for parameter tuning and does not therefore provide an accurate estimation of the system's performance.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7254270613193512}]}], "tableCaptions": [{"text": " Table 2: Baseline performance vs. plain EFMP: MELA  score, different versions of the CoNLL scorer.", "labels": [], "entities": [{"text": "MELA  score", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9750235974788666}, {"text": "CoNLL scorer", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.77370885014534}]}, {"text": " Table 3: Feature combinations, JIM vs. EFI: MELA  score, different versions of the CoNLL scorer.", "labels": [], "entities": [{"text": "MELA  score", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.9690574109554291}, {"text": "CoNLL scorer", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.7320216298103333}]}, {"text": " Table 4: EFMP and top-5 CoNLL-2012 systems: MELA  score, systems ranked by the v7 score on the test set.", "labels": [], "entities": [{"text": "EFMP", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8209055662155151}, {"text": "CoNLL-2012", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.85761559009552}, {"text": "MELA  score", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.9554521441459656}]}]}