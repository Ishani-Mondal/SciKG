{"title": [{"text": "CICBUAPnlp: Graph-Based Approach for Answer Selection in Community Question Answering Task", "labels": [], "entities": [{"text": "CICBUAPnlp", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.826734721660614}, {"text": "Answer Selection in Community Question Answering", "start_pos": 37, "end_pos": 85, "type": "TASK", "confidence": 0.6877835690975189}]}], "abstractContent": [{"text": "This paper describes our approach for the Community Question Answering Task, which was presented at the SemEval 2015.", "labels": [], "entities": [{"text": "Community Question Answering Task", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.7516785711050034}]}, {"text": "The system should read a given question and identify good, potentially relevant, and bad answers for that question.", "labels": [], "entities": []}, {"text": "Our approach transforms the answers of the training set into a graph based representation for each answer class, which contains lexical, morphological, and syntactic features.", "labels": [], "entities": []}, {"text": "The answers in the test set are also transformed into the graph based representation individually.", "labels": [], "entities": []}, {"text": "After this, different paths are traversed in the training and test sets in order to find relevant features of the graphs.", "labels": [], "entities": []}, {"text": "As a result of this procedure, the system constructs several vectors of features: one for each traversed graph.", "labels": [], "entities": []}, {"text": "Finally, a cosine similarity is calculated between the vectors in order to find the class that best matches a given answer.", "labels": [], "entities": []}, {"text": "Our system was developed for the English language only, and it obtained an accuracy of 53.74 for subtask A and 44.0 for subtask B.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9995716214179993}]}], "introductionContent": [{"text": "In this paper we present the experiments carried out as part of our participation in the SemEval-2015 Task 3 (Answer Selection in Community Question Answering).", "labels": [], "entities": [{"text": "SemEval-2015 Task 3", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.826454758644104}, {"text": "Answer Selection in Community Question Answering)", "start_pos": 110, "end_pos": 159, "type": "TASK", "confidence": 0.8864661370004926}]}, {"text": "The Answer Selection in Community Question Answering task is proposed for the first time this year in the International Workshop on Semantic Evaluation.", "labels": [], "entities": [{"text": "Answer Selection in Community Question Answering task", "start_pos": 4, "end_pos": 57, "type": "TASK", "confidence": 0.8988196934972491}, {"text": "International Workshop on Semantic Evaluation", "start_pos": 106, "end_pos": 151, "type": "TASK", "confidence": 0.6357118725776673}]}, {"text": "The task is based on an application scenario, which is related to textual entailment, semantic similarity and NL inference.", "labels": [], "entities": []}, {"text": "Community question answering (CQA) websites enable people to post questions and answers in various domains.", "labels": [], "entities": [{"text": "Community question answering (CQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.758219579855601}]}, {"text": "In this way, users can obtain specific answers to their questions, instead of searching in the large volume of information available in the web.", "labels": [], "entities": []}, {"text": "However, it takes effort to go through all possible answers and select which one is the most accurate one fora specific question.", "labels": [], "entities": []}, {"text": "The task proposes to automate this process by predicting the quality of existing answers with respect to a question.", "labels": [], "entities": []}, {"text": "There are few works in the literature on evaluating the quality of answers provided in CQA sites.", "labels": [], "entities": []}, {"text": "Most of such works employ non-textual and temporal features in order to built classification models for predicting the best answer fora given question.", "labels": [], "entities": []}, {"text": "In (), the authors extract 13 nontextual features from the Naver data set and build a maximum entropy classification model to predict the quality (three classes: Bad, Medium and Good) of a given answer.", "labels": [], "entities": [{"text": "Naver data set", "start_pos": 59, "end_pos": 73, "type": "DATASET", "confidence": 0.9538395404815674}]}, {"text": "A similar approach is used in, but extracting 21 features (mainly non-textual) from Yahoo!", "labels": [], "entities": []}, {"text": "Answers; the authors employ a logistic regression and classification model to predict the best answer.", "labels": [], "entities": []}, {"text": "Besides, a set of temporal features is proposed in) in order to predict the best answer fora given question.", "labels": [], "entities": []}, {"text": "In this work the authors argue that the traditional classification approaches are not well suited for this problem because of the highly imbalanced ratio of the best answer and the non-best answers in their data set, so they propose to use learning to rank approaches.", "labels": [], "entities": []}, {"text": "Unlike these approaches, we use only textual information for predicting the quality of the answers.", "labels": [], "entities": []}, {"text": "Our approach is based on our previous research ( ) and (), where we propose the graph-based representation model (Integrated Syntactic Graph) and the soft similarity measure (soft cosine measure).", "labels": [], "entities": []}, {"text": "Our experimental results are promising, they overcome the baseline system for this challenge.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our approach.", "labels": [], "entities": []}, {"text": "Section 3 presents the configuration of the submitted runs and the evaluation results.", "labels": [], "entities": []}, {"text": "Finally, Section 4 presents the conclusions and outlines some directions of future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results of the subtask A, English", "labels": [], "entities": [{"text": "English", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.8440412878990173}]}, {"text": " Table 2: Results of the subtask B, English", "labels": [], "entities": [{"text": "English", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.8358217477798462}]}]}