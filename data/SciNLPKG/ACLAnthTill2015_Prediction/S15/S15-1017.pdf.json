{"title": [{"text": "Dissecting the Practical Lexical Function Model for Compositional Distributional Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "The Practical Lexical Function model (PLF) is a recently proposed compositional distribu-tional semantic model which provides an elegant account of composition, striking a balance between expressiveness and robustness and performing at the state-of-the-art.", "labels": [], "entities": []}, {"text": "In this paper, we identify an inconsistency in PLF between the objective function at training and the prediction at testing which leads to an over-counting of the predicate's contribution to the meaning of the phrase.", "labels": [], "entities": [{"text": "PLF", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.8818749785423279}]}, {"text": "We investigate two possible solutions of which one (the exclusion of simple lexical vector attest time) improves performance significantly on two out of the three composition datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Compositional distributional semantic models (CDSMs) make an important theoretical contribution, explaining the meaning of a phrase by the meanings of its parts.", "labels": [], "entities": []}, {"text": "They have also found application in psycholinguistics, in sentiment analysis (, and in machine translation.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.9758410751819611}, {"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8333432376384735}]}, {"text": "A first generation of CDSMs represented all words as vectors and combined them by component-wise operations.", "labels": [], "entities": []}, {"text": "Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors.", "labels": [], "entities": []}, {"text": "For example, one-place predicates such as adjectives or intransitive verbs can be modeled as matrices (order-2 tensors), and two-place predicates, e.g., transitive verbs, as order-3 tensors, and so forth.", "labels": [], "entities": []}, {"text": "While such tensors enable mathematically elegant accounts of composition, their large degrees of freedom lead to severe sparsity issues when they are learned from corpora.", "labels": [], "entities": []}, {"text": "The recently proposed Practical Lexical Function model (PLF;) represents a compromise between these two extremes by restricting itself to vectors and matrices, effectively reducing sparsity while retaining state-of-the-art performance across multiple datasets.", "labels": [], "entities": []}, {"text": "It does away with tensors by ignoring interactions among the arguments of predicates p.", "labels": [], "entities": []}, {"text": "Instead, each argument position arg is modeled as a matrix arg p that is applied to a vector for the argument's meaning, \u2212 \u2192 a . The meaning of the phrase is then defined as the sum of the lexical meaning of the predicate, \u2212 \u2192 p , and the contributions of each argument (see).", "labels": [], "entities": []}, {"text": "The matrices can be learned in a supervised manner with regression from pairs of corpus-extracted vectors for arguments and phrases.", "labels": [], "entities": []}, {"text": "In this paper, we identify an inconsistency between the training and testing phases of the PLF.", "labels": [], "entities": [{"text": "PLF", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.6876426339149475}]}, {"text": "More specifically, we show that its composition procedure leads to over-counting of the contribution of the predicate.", "labels": [], "entities": []}, {"text": "We propose two remedies to harmonize the training and prediction phases -by excluding the predicate meaning from either training or testing.", "labels": [], "entities": []}, {"text": "In an evaluation of the standard PLF and our variants on three datasets, we find that modifying the training phase fails, but that modifying testing phase improves performance on two out of three datasets.", "labels": [], "entities": []}, {"text": "We analyze this effect in terms of a bias-variance tradeoff.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main results are shown in Table 2.", "labels": [], "entities": []}, {"text": "Our PLF re-implementation in the first column almost replicates the results reported by   The results for the training phase modification are overwhelmingly negative.", "labels": [], "entities": []}, {"text": "There is a minor degradation when the adjective is subtracted at training time, and major degradation when the verb is subtracted.", "labels": [], "entities": []}, {"text": "We will comeback to this result below.", "labels": [], "entities": []}, {"text": "In contrast, we obtain improvements when we modify the test phase, when we either leave out the verb or both the verb and the adjective in the composition.", "labels": [], "entities": []}, {"text": "For two out of the three datasets, the respective best models perform statistically significantly better than the PLF as determined by a bootstrap resampling test): ANVAN1 (+1.5%, p<0.05) and NVN (+5.2%, p<0.01).", "labels": [], "entities": [{"text": "PLF", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.7042738795280457}, {"text": "ANVAN1", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.877581000328064}]}, {"text": "The improvement for ANVAN2 (+0.5%) is not large enough to reach significance.", "labels": [], "entities": [{"text": "ANVAN2", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.8312287330627441}, {"text": "significance", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.9447060227394104}]}, {"text": "These results leave us with two main questions: (a), why does the modification at training time fail so completely; and (b), can we develop a better understanding of the kind of improvement that the modification attest time introduces?", "labels": [], "entities": []}, {"text": "Regarding question (a), we believe that the difference between the phrase vector and the predicate vector that we are training the matrix to predict in Eq. is, in practice, a very brittle representation.", "labels": [], "entities": []}, {"text": "The reason is that typically the phrase nv is much less frequent than v, and therefore \u2212 \u2192.", "labels": [], "entities": []}, {"text": "Consequently, the matrix attempts to predict the verb vector from the noun -not only a very hard problem, but one that does not help solve the task at hand.", "labels": [], "entities": []}, {"text": "To answer question (b), we perform a mixed effects linear regression analysis) on the three datasets, concentrating on a comparison of the standard PLF and the best respective test phase modification.", "labels": [], "entities": []}, {"text": "We follow the intuition that the frequency and ambiguity of the target verbs should influence the quality of the prediction both in the PLF  and in the modified model, and that it might be informative to look at differences in these effects.", "labels": [], "entities": []}, {"text": "To this effect, we construct a mixed-effects model which predicts, for each experimental item (cf), the absolute rank difference between the item's rank in the gold standard ratings and the item's rank in the model prediction.", "labels": [], "entities": []}, {"text": "Thus, high values of the output variable denote items which are difficult to predict, while low values of the output variable denote items which are easy to predict.", "labels": [], "entities": []}, {"text": "As fixed effects, we include the target verbs' logarithmized corpus frequencies (logf ), their ambiguities, measured as the number of WordNet top nodes subsuming their synsets (ambig), the presence of the test phase modification (NoVerb for ANVAN2 and NVN, NoBoth for ANVAN1; ModTest) as well as interaction terms between ModTest and the two other predictors.", "labels": [], "entities": []}, {"text": "We also include the identity of the target verb as random effect.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "There are considerable differences between the datasets, but the overall patterns are nevertheless comparable.", "labels": [], "entities": []}, {"text": "Notably, frequency has a negative effect on rank difference.", "labels": [], "entities": []}, {"text": "In other words, more frequent verbs are easier to predict.", "labels": [], "entities": []}, {"text": "Conversely, the ambiguity of the target verb has a positive effect on rank difference, that is, higher ambiguity makes predictions more difficult.", "labels": [], "entities": []}, {"text": "Both of these effects are very strong on ANVAN1 and NVN and not significant on ANVAN2, which appears to be a more controlled dataset.", "labels": [], "entities": [{"text": "ANVAN1", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.717173159122467}, {"text": "NVN", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.8438788652420044}, {"text": "ANVAN2", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.7859375476837158}]}, {"text": "Taken together, the models still seem to struggle with ambiguous and infrequent target verbs.", "labels": [], "entities": []}, {"text": "The coefficients that we obtain for ModTest look puzzling at first glance: we obtain a negative coefficient (i.e., an overall improvement) only for AN-VAN2 and NVN while the coefficient is positive for ANVAN1.", "labels": [], "entities": [{"text": "ModTest", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.900098443031311}, {"text": "ANVAN1", "start_pos": 202, "end_pos": 208, "type": "DATASET", "confidence": 0.8509621620178223}]}, {"text": "For ANVAN1, the improvement is brought about by the interaction with the frequency variable: when the test phase is modified, the (beneficial) effect of frequency becomes much stronger, that is, the predictions for high-frequency verbs improve.", "labels": [], "entities": []}, {"text": "In contrast, the effect of frequency becomes weaker for the test phase modification on ANVAN2 and NVN.", "labels": [], "entities": [{"text": "frequency", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9833967089653015}, {"text": "ANVAN2", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.7247185111045837}, {"text": "NVN", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.8821777701377869}]}, {"text": "What is true for all three datasets is that the effect of ambiguity gets stronger when the test phase is modified: ambiguous verbs become significantly more difficult to model.", "labels": [], "entities": []}, {"text": "On the basis of this analysis, we believe that this difference between the standard PLF and our test phase modification can be understood as a classical bias-variance tradeoff: the addition of the predicate meaning in the standard PLF reduces variance, ensuring that the phrase meaning stays close to the predicate meaning prior even for matrices that are difficult to learn, e.g., due to sparse data or high ambiguity.", "labels": [], "entities": []}, {"text": "At the same time, this dilutes the disambiguating effect of composition.", "labels": [], "entities": []}, {"text": "In our modified scheme, the situation is reversed: the composed representations vary more freely, which benefits well-learned matrices but leads to worse predictions for poorly learned ones.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental results (Spearman's \u03c1) on three dataset. Significant improvements over the PLF results are  indicated with stars (  *  : p<0.05,  *  *  : p<0.01 ), -denotes non-applicability of parameter.", "labels": [], "entities": []}]}