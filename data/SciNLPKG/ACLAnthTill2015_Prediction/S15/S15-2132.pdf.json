{"title": [{"text": "SemEval-2015 Task 4: TimeLine: Cross-Document Event Ordering", "labels": [], "entities": [{"text": "Cross-Document Event Ordering", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.6673432091871897}]}], "abstractContent": [{"text": "This paper describes the outcomes of the TimeLine task (Cross-Document Event Ordering), that was organised within the Time and Space track of SemEval-2015.", "labels": [], "entities": [{"text": "Cross-Document Event Ordering)", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.6768985167145729}, {"text": "Time and Space track of SemEval-2015", "start_pos": 118, "end_pos": 154, "type": "DATASET", "confidence": 0.6089917719364166}]}, {"text": "Given a set of documents and a set of target entities, the task consisted of building a timeline for each entity , by detecting, anchoring in time and ordering the events involving that entity.", "labels": [], "entities": []}, {"text": "The TimeLine task goes a step further than previous evaluation challenges by requiring participant systems to perform both event corefer-ence and temporal relation extraction across documents.", "labels": [], "entities": [{"text": "temporal relation extraction across documents", "start_pos": 146, "end_pos": 191, "type": "TASK", "confidence": 0.7504414439201355}]}, {"text": "Four teams submitted the output of their systems to the four proposed subtracks fora total of 13 runs, the best of which obtained an F 1-score of 7.85 in the main track (timeline creation from raw text).", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9909095466136932}]}], "introductionContent": [{"text": "In any domain, it is important that professionals have access to high quality knowledge for taking wellinformed decisions.", "labels": [], "entities": []}, {"text": "As daily tasks of information professionals revolve around reconstructing a chain of previous events, an insightful way of presenting information to them is by means of timelines.", "labels": [], "entities": []}, {"text": "The aim of the Cross-Document Event Ordering task is to build timelines from English news articles.", "labels": [], "entities": [{"text": "Cross-Document Event Ordering", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.7364608645439148}]}, {"text": "To provide focus to the timeline creation, the task is presented as an ordering task in which events involving a particular target entity are to be ordered chronologically.", "labels": [], "entities": [{"text": "timeline creation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7395375967025757}]}, {"text": "The task focuses on cross-document event coreference resolution and cross-document temporal relation extraction.", "labels": [], "entities": [{"text": "cross-document event coreference resolution", "start_pos": 20, "end_pos": 63, "type": "TASK", "confidence": 0.7967486828565598}, {"text": "cross-document temporal relation extraction", "start_pos": 68, "end_pos": 111, "type": "TASK", "confidence": 0.7511210292577744}]}, {"text": "The latter has been the topic of the three previous TempEval tasks within the SemEval challenges: Additionally, it has also been the focus of the 6th i2b2 NLP Challenge for clinical records).", "labels": [], "entities": []}, {"text": "The cross-document aspect, however, has not often been explored.", "labels": [], "entities": []}, {"text": "One example is the work described in () using the ACE 2005 training corpora.", "labels": [], "entities": [{"text": "ACE 2005 training corpora", "start_pos": 50, "end_pos": 75, "type": "DATASET", "confidence": 0.973830446600914}]}, {"text": "Here the authors link pre-defined events involving the same centroid entities (i.e. entities frequently participating in events) on a timeline.", "labels": [], "entities": []}, {"text": "Nominal coreference resolution has been the topic of SemEval 2010 Task on Coreference Resolution in Multiple.", "labels": [], "entities": [{"text": "Nominal coreference resolution", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7515013019243876}, {"text": "SemEval 2010 Task", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7681246201197306}, {"text": "Coreference Resolution", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.8243813812732697}]}, {"text": "TimeLine is a pilot task that goes beyond the above-mentioned evaluation exercises by addressing coreference resolution for events and temporal relation extraction at across document level.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.9102821946144104}, {"text": "temporal relation extraction", "start_pos": 135, "end_pos": 163, "type": "TASK", "confidence": 0.6843064030011495}]}, {"text": "This task was motivated by work done in the NewsReader project . The goal of the NewsReader project is to reconstruct story lines across news articles in order to provide policy and decision makers with an overview of what happened, to whom, when, and where.", "labels": [], "entities": []}, {"text": "Thus, the NewsReader project aims to present end-users with cross-document storylines.", "labels": [], "entities": []}, {"text": "Timelines are intermediate event represen- (sentence id: 2) Steve Jobs, founder of Apple, has chosen to step down from his post as CEO of the company.", "labels": [], "entities": [{"text": "Timelines", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.939898669719696}]}, {"text": "(sentence id: 7) Steve Jobs has been fighting pancreatic cancer since 2004 and has been on medical leave since January of this year.", "labels": [], "entities": []}, {"text": "(sentence id: 4) He has been fighting pancreatic cancer since 2004.", "labels": [], "entities": []}, {"text": "(sentence id: 18) The current Microsoft CEO described Jobs as \"one of the founders of our industry and a true visionary\".", "labels": [], "entities": []}, {"text": "The remainder of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce the task.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the data annotation protocol.", "labels": [], "entities": []}, {"text": "In Section 4, we present the characteristics of our dataset and gold standard timelines.", "labels": [], "entities": [{"text": "gold standard timelines", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.8731967210769653}]}, {"text": "In Section 5, we describe our evaluation methodology, followed by the description of participant systems in Section 6 and the results obtained by the participants to the task in Section 7.", "labels": [], "entities": []}, {"text": "Lessons learnt and limitations of our setup are discussed in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset used for this task is composed of articles from Wikinews, a collection of multilingual online news articles written collaboratively in a wikilike manner.", "labels": [], "entities": []}, {"text": "The reason for choosing Wikinews as a source is its creative commons license allowing us to freely release this dataset to the research community.", "labels": [], "entities": []}, {"text": "For this task, we selected Wikinews articles around four topics: \u2022 Apple Inc.", "labels": [], "entities": []}, {"text": "(trial corpus); \u2022 Airbus and Boeing (corpus 1); \u2022 General Motors, Chrysler and Ford (corpus 2); \u2022 Stock Market (corpus 3).", "labels": [], "entities": []}, {"text": "The trial data consists of one corpus of 30 documents and gold standard timelines for six target entities.", "labels": [], "entities": []}, {"text": "The other three corpora, each consisting of 30 documents (about 30,000 tokens each) were used as the evaluation dataset.", "labels": [], "entities": []}, {"text": "As reported in, the total number of target entities in the evaluation dataset amounts to 38, but for the evaluation we used 37 timelines instead as one of the timelines contained no events.", "labels": [], "entities": []}, {"text": "The trial data contains one target entity of type ORGANISATION, one of type PERSON and 4 of type PRODUCT.", "labels": [], "entities": [{"text": "ORGANISATION", "start_pos": 50, "end_pos": 62, "type": "METRIC", "confidence": 0.9543922543525696}, {"text": "PERSON", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9516890645027161}]}, {"text": "The distribution of target entity types in the evaluation dataset is the following: 18 of type ORGANISATION, 10 of type FINANCIAL, 7 of type PERSON and 3 of type PRODUCT.", "labels": [], "entities": [{"text": "ORGANISATION", "start_pos": 95, "end_pos": 107, "type": "METRIC", "confidence": 0.9523167610168457}, {"text": "FINANCIAL", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9464877247810364}, {"text": "PERSON", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.8238406181335449}]}, {"text": "The evaluation methodology of this task is based on the evaluation metric used for TempEval-3 (UzZaman et al., 2013) to evaluate relations in terms of recall, precision and F 1 -score.", "labels": [], "entities": [{"text": "recall", "start_pos": 151, "end_pos": 157, "type": "METRIC", "confidence": 0.9993357062339783}, {"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9992141723632812}, {"text": "F 1 -score", "start_pos": 173, "end_pos": 183, "type": "METRIC", "confidence": 0.9845851957798004}]}, {"text": "The metric captures the temporal awareness of an annotation (UzZaman and Allen, 2011).", "labels": [], "entities": []}, {"text": "Temporal awareness is defined as the performance of an annotation as identifying and categorizing temporal relations, which implies the correct recognition and classification of the temporal entities involved in the relations.", "labels": [], "entities": [{"text": "Temporal awareness", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9348921179771423}]}, {"text": "We calculate the Precision by checking the number of reduced system relations that can be verified from the reference annotation temporal closure graph, out of number of temporal relations in the reduced system relations.", "labels": [], "entities": [{"text": "Precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9784881472587585}]}, {"text": "Similarly, we calculate the Recall by checking the number of reduced reference annotation rela- Before evaluating temporal awareness, each timeline needs to be transformed into a set of temporal relations.", "labels": [], "entities": [{"text": "Recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.706348180770874}]}, {"text": "shows the explicit relations resulting from the timeline of as well as the implicit relations captured by the temporal graph.", "labels": [], "entities": []}, {"text": "In order to convert each timeline, we defined the following transformation steps: 3.", "labels": [], "entities": []}, {"text": "If one event happens before another one, a BE-FORE relation type is created between both events.", "labels": [], "entities": [{"text": "BE-FORE", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9971784353256226}]}, {"text": "4. If one event happens at the same time as another one, a SIMULTANEOUS relation type is created between both events.", "labels": [], "entities": [{"text": "SIMULTANEOUS", "start_pos": 59, "end_pos": 71, "type": "METRIC", "confidence": 0.9113292694091797}]}, {"text": "Note that the evaluation of subtracks (ordering only), requires steps 3 and 4 alone.", "labels": [], "entities": []}, {"text": "For this first pilot on timelines, we decided to simplify the representation of durative events in the timelines by anchoring them in time considering their starting point.", "labels": [], "entities": []}, {"text": "For this reason we represent relations between each event and its time anchor with the SIMULTANEOUS relation type (instead of other possibilities like BEGUN BY or INCLUDES).", "labels": [], "entities": [{"text": "BEGUN BY", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.8692611157894135}, {"text": "INCLUDES", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.8257626891136169}]}, {"text": "Events placed at the beginning of the timeline at position 0, i.e. events that were not ordered, are not considered in the evaluation.", "labels": [], "entities": []}, {"text": "The official scores are based on the micro-average of the individual F 1 -scores for each timeline, i.e. the scores are averaged over the events of the timelines of each corpus.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9512514919042587}]}, {"text": "The micro-average precision and recall values are also provided.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.8905776143074036}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9996323585510254}]}, {"text": "The official results are presented in.", "labels": [], "entities": []}, {"text": "For each corpus we present the micro F 1 -score and in the last three columns the micro precision, micro recall and micro F 1 -score overall the three corpora.", "labels": [], "entities": [{"text": "micro F 1 -score", "start_pos": 31, "end_pos": 47, "type": "METRIC", "confidence": 0.8000216722488404}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.596933901309967}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.8561965823173523}, {"text": "micro F 1 -score", "start_pos": 116, "end_pos": 132, "type": "METRIC", "confidence": 0.7778639793395996}]}, {"text": "In the main track, Track A, WHUNLP 1 was the best run and achieved an F 1 of 7.28%.", "labels": [], "entities": [{"text": "WHUNLP 1", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9620454013347626}, {"text": "F 1", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9866234064102173}]}, {"text": "In Track B, GPLSIUA 1 obtained the best scores with an F 1 of 25.36%.", "labels": [], "entities": [{"text": "GPLSIUA 1", "start_pos": 12, "end_pos": 21, "type": "DATASET", "confidence": 0.7299577593803406}, {"text": "F 1", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9962659180164337}]}, {"text": "The subtracks were proposed in order to evaluate systems that do not perform time normalisation or event anchoring in time but focus on temporal relations between events.", "labels": [], "entities": [{"text": "event anchoring", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.7450812757015228}]}, {"text": "In the end, the events ordering of the runs submitted to the subtracks was the same as those submitted to the main tracks.", "labels": [], "entities": []}, {"text": "In SubTrack A the best results are obtained with the run 1 of SPINOZAVU team, achieving an F 1 -score of 1.69%.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9908299446105957}]}, {"text": "In SubTrack B, the best system is the same as in Track B, GPLSIUA 1, with an F 1 -score of 23.15%.", "labels": [], "entities": [{"text": "GPLSIUA 1", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.7617557644844055}, {"text": "F 1 -score", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9871195256710052}]}, {"text": "We evaluate the selection of the relevant events involving a target entity using the classic evaluation metrics: recall, precision and F 1 -score.", "labels": [], "entities": [{"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9996548891067505}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9987381100654602}, {"text": "F 1 -score", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.9853243380784988}]}, {"text": "All events are taken into account independently of their ordering in timelines; events placed at position 0 are also evaluated.", "labels": [], "entities": []}, {"text": "The number of true positives and F 1 -scores obtained on each corpus as well as the microaverage F 1 -scores are presented in.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9866151064634323}, {"text": "microaverage F 1 -scores", "start_pos": 84, "end_pos": 108, "type": "METRIC", "confidence": 0.8166988611221313}]}, {"text": "In we also provide the evaluation of time anchors assignment in terms of accurracy.", "labels": [], "entities": [{"text": "time anchors assignment", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.7787174880504608}, {"text": "accurracy", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9778514504432678}]}, {"text": "For each timeline, the accurracy is computed by dividing the number of matching events/time anchors by the number of: Evaluation of the selection of events in which a target entity is involved and of time anchors assignment; TP: number of correctly identified events; F 1 : micro-average F 1 -score for the selection of events; Acc: accurracy in assignment of time anchors.", "labels": [], "entities": [{"text": "accurracy", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9916144609451294}, {"text": "TP", "start_pos": 225, "end_pos": 227, "type": "METRIC", "confidence": 0.9804350137710571}, {"text": "F 1 : micro-average F 1 -score", "start_pos": 268, "end_pos": 298, "type": "METRIC", "confidence": 0.8972177430987358}, {"text": "Acc", "start_pos": 328, "end_pos": 331, "type": "METRIC", "confidence": 0.9995211362838745}, {"text": "accurracy", "start_pos": 333, "end_pos": 342, "type": "METRIC", "confidence": 0.9821439981460571}]}, {"text": "correctly identified events (TP in the table).", "labels": [], "entities": [{"text": "TP", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9813281297683716}]}, {"text": "The results obtained in SubTracks, when evaluating only events ordering, are mainly lower than in Tracks, except on the \"GM\" corpus.", "labels": [], "entities": [{"text": "GM\" corpus", "start_pos": 121, "end_pos": 131, "type": "DATASET", "confidence": 0.8375693162282308}]}, {"text": "For example the HEIDELTOUL 1 system achieved an F 1 -score of 17.03% overall the 3 corpora in Track B and 14.42% We found an error in the format of some event ids and reprocessed the evaluation on a corrected version of the timelines.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9929661899805069}]}, {"text": "7 HEIDELTOUL 1 and HEIDELTOUL 2 are shorthand for HEIDELTOUL NONTOLMATCHPRUNE and HEIDEL-TOUL TOLMATCHPRUNE respectively.", "labels": [], "entities": [{"text": "HEIDELTOUL NONTOLMATCHPRUNE", "start_pos": 50, "end_pos": 77, "type": "METRIC", "confidence": 0.6377929151058197}, {"text": "TOLMATCHPRUNE", "start_pos": 94, "end_pos": 107, "type": "METRIC", "confidence": 0.5386416912078857}]}, {"text": "But on \"GM\" corpus, the HEIDEL-TOUL 1 system obtained an F 1 -score twice as high as in Track B, obtaining an F 1 -score of 14.78% (vs. 7.25% in Track B).", "labels": [], "entities": [{"text": "GM\" corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.8259691198666891}, {"text": "HEIDEL-TOUL", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.7816033363342285}, {"text": "F 1 -score", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9895472377538681}, {"text": "F 1 -score", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9882645905017853}]}, {"text": "In evaluating the time anchors assignment (see), we observed that HEI-DELTOUL and GPLSIUA systems performed better on the \"Airbus\" and \"Stock\" corpora than on \"GM\".", "labels": [], "entities": [{"text": "time anchors assignment", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.7563489377498627}, {"text": "HEI-DELTOUL", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.9385801553726196}, {"text": "GPLSIUA", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.8816055059432983}, {"text": "Airbus\" and \"Stock\" corpora", "start_pos": 123, "end_pos": 150, "type": "DATASET", "confidence": 0.7348928238664355}, {"text": "GM", "start_pos": 160, "end_pos": 162, "type": "DATASET", "confidence": 0.9505059719085693}]}, {"text": "This explains in part the better performance of their systems on the \"GM\" corpus when evaluating only events ordering (SubTrack B) than when evaluating both time anchors assignment and events ordering (Track B).", "labels": [], "entities": [{"text": "GM\" corpus", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.8149365981419882}, {"text": "time anchors assignment", "start_pos": 157, "end_pos": 180, "type": "TASK", "confidence": 0.7477806508541107}]}, {"text": "Furthermore, the task of time expression extraction and normalisation has been the topic of different shared tasks and the obtained results are high with an F 1 -score of 90.30 for time expression detection and of 77.61 for normalisation (results obtained by at TempEval-3).", "labels": [], "entities": [{"text": "time expression extraction", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.6803802649180094}, {"text": "F 1 -score", "start_pos": 157, "end_pos": 167, "type": "METRIC", "confidence": 0.9906176179647446}, {"text": "time expression detection", "start_pos": 181, "end_pos": 206, "type": "TASK", "confidence": 0.5817342301209768}, {"text": "TempEval-3", "start_pos": 262, "end_pos": 272, "type": "DATASET", "confidence": 0.8241506814956665}]}, {"text": "However, the performance of temporal relation extraction systems is quite low with an F 1 -score of 36.26 obtained by), the best system at TempEval-3 on Task C. Observing the results by corpus in, we notice that, except for Track A, the best results are obtained on the \"Stock Market\" corpus.", "labels": [], "entities": [{"text": "temporal relation extraction", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6603725453217825}, {"text": "F 1 -score", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9878323078155518}, {"text": "Stock Market\" corpus", "start_pos": 271, "end_pos": 291, "type": "DATASET", "confidence": 0.9031364470720291}]}, {"text": "One of the reasons is that in the timelines related to this corpus all events were ordered (only one event was placed at position 0), while in \"Airbus\" and \"GM\" corpora less than 70% of the events were ordered.", "labels": [], "entities": [{"text": "Airbus", "start_pos": 144, "end_pos": 150, "type": "DATASET", "confidence": 0.9191734194755554}, {"text": "GM\" corpora", "start_pos": 157, "end_pos": 168, "type": "DATASET", "confidence": 0.8385031421979269}]}, {"text": "In the \"GM\" corpus, one timeline was empty (\"General Motors creditors\"), i.e. the corpus does not contain any event that have this target entity as Arg0 or Arg1, therefore this timeline was removed from the evaluation.", "labels": [], "entities": [{"text": "GM\" corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.9265110492706299}, {"text": "General Motors creditors", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.8706385095914205}, {"text": "Arg0", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9768348336219788}, {"text": "Arg1", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.8712384700775146}]}, {"text": "We observed that SPINOZAVU systems in Track A and GPLSIUA systems in Track B correctly returned an empty timeline, while WHUNLP created a timeline with 3 events in Track A and HEIDELTOUL 1 and HEIDEL-TOUL 2 produced a timeline containing respectively 32 and 78 events for this target entity in Track B. Track B was proposed as a simplified task given that annotated texts with events were distributed to participants.", "labels": [], "entities": [{"text": "WHUNLP", "start_pos": 121, "end_pos": 127, "type": "DATASET", "confidence": 0.8364295363426208}]}, {"text": "Unfortunately no results from the same system run on both Tracks A and B were submitted, therefore, at the moment, we cannot evaluate the impact of pre-annotation of events.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Quantitative data about the dataset.", "labels": [], "entities": []}, {"text": " Table 2: Official results of the TimeLine task of the four participating teams 7 presented per subcorpus  and over the whole dataset. (Track A: timelines with time anchors from raw text; SubTrack A: timelines  without time anchors from raw text; Track B: timelines with time anchors from texts annotated with events;  SubTrack B: timelines without time anchors from texts annotated with events.)", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of the selection of events in which a target entity is involved and of time anchors  assignment; TP: number of correctly identified events; F 1 : micro-average F 1 -score for the selection of  events; Acc: accurracy in assignment of time anchors.", "labels": [], "entities": [{"text": "time anchors  assignment", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.6527615884939829}, {"text": "TP", "start_pos": 118, "end_pos": 120, "type": "METRIC", "confidence": 0.9964159727096558}, {"text": "F 1 : micro-average F 1 -score", "start_pos": 161, "end_pos": 191, "type": "METRIC", "confidence": 0.8496482521295547}, {"text": "Acc", "start_pos": 222, "end_pos": 225, "type": "METRIC", "confidence": 0.999431312084198}, {"text": "accurracy", "start_pos": 227, "end_pos": 236, "type": "METRIC", "confidence": 0.9814460873603821}]}]}