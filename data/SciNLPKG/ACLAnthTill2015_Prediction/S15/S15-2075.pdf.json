{"title": [{"text": "CMILLS: Adapting Semantic Role Labeling Features to Dependency Parsing", "labels": [], "entities": [{"text": "Adapting Semantic Role Labeling", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.878126785159111}]}], "abstractContent": [{"text": "We describe a system for semantic role labeling adapted to a dependency parsing framework.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.6649827758471171}, {"text": "dependency parsing", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7885684669017792}]}, {"text": "Verb arguments are predicted over nodes in a dependency parse tree instead of nodes in a phrase-structure parse tree.", "labels": [], "entities": []}, {"text": "Our system participated in SemEval-2015 shared Task 15, Subtask 1: CPA parsing and achieved an F-score of 0.516.", "labels": [], "entities": [{"text": "CPA parsing", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.7140890061855316}, {"text": "F-score", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9992165565490723}]}, {"text": "We adapted features from prior semantic role labeling work to the dependency parsing paradigm, using a series of supervised classifiers to identify arguments of a verb and then assigning syntactic and semantic labels.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7196916937828064}, {"text": "dependency parsing paradigm", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.8377628922462463}]}, {"text": "We found that careful feature selection had a major impact on system performance.", "labels": [], "entities": []}, {"text": "However , sparse training data still led rule-based systems like the baseline to be more effective than learning-based approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "We describe our submission to the SemEval-2015 Task 15, Subtask 1 on Corpus Pattern Analysis ().", "labels": [], "entities": [{"text": "SemEval-2015 Task 15", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7797240217526754}, {"text": "Corpus Pattern Analysis", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.6980589926242828}]}, {"text": "This task is similar to semantic role labeling but with arguments based on nodes in dependency parses instead of a syntactic parse tree.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.6835759480794271}]}, {"text": "The verb's arguments are identified and labeled with both their syntactic and semantic roles.", "labels": [], "entities": []}, {"text": "For example, consider the sentence \"But he said Labour did not agree that Britain could or should abandon development, either for itself or for the developing world.\"", "labels": [], "entities": []}, {"text": "This subtask involves taking that sentence and making the following determinations relative to the given verb \"abandon\": \uf0b7 \"Britain\" is the syntactic subject of \"abandon\" and falls under the \"Institution\" semantic type \uf0b7 \"development\" is the syntactic object of \"abandon\" and is of semantic type \"Activity\" We organize the remainder of our paper as follows: Section 2 describes our system, Section 3 presents experiments, and Section 4 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "The system was evaluated using leave-one-out cross-validation on each verb in the train data.", "labels": [], "entities": []}, {"text": "For the initial baseline configuration, only the features present in prior work were included, with a total of 31 feature classes.", "labels": [], "entities": []}, {"text": "This configuration achieved an fscore of 0.238.", "labels": [], "entities": [{"text": "fscore", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9994215965270996}]}, {"text": "The system was then run with our new features added, which outperformed the baseline by a relative 4% with an f-score of 0.248.", "labels": [], "entities": [{"text": "f-score", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9972018003463745}]}, {"text": "In 1 Predicate Lemma is a critical feature in prior SRL work.", "labels": [], "entities": [{"text": "SRL", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9793857932090759}]}, {"text": "In the test data, which only included unseen verbs, we used Sketch Engine data to identify the verb in the train data most similar to the verb in the test sentence, the Similar Predicate Lemma feathese cross-validation experiments, for each training example we used its Similar Predicate Lemma in place of its Predicate Lemma feature.", "labels": [], "entities": [{"text": "Sketch Engine data", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.7031062344710032}]}, {"text": "This was a pessimistic assumption that we did not apply to the final system submitted for evaluation.", "labels": [], "entities": []}, {"text": "We suspect this explains why the final f-score on the test data was twice as good as that of the cross-validation experiments.", "labels": [], "entities": [{"text": "f-score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.7644807696342468}]}, {"text": "The argument identification module performed well on its own with an f-score of 0.627, which is an upper bound on our overall system performance.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8424536287784576}]}, {"text": "We used a hill climbing heuristic search for the best possible subset of the available features.", "labels": [], "entities": []}, {"text": "This was a time-consuming process that involved running cross-validation for each feature class being evaluated with our three-stage classifier resulting in 63 classifiers being trained per iteration.", "labels": [], "entities": []}, {"text": "All the feature removals or additions that improved performance were greedily accepted, yielding 22% feature churn.", "labels": [], "entities": []}, {"text": "The best individual feature changes predicted 0.5% improvements to overall performance, but together they produced only a 0.9% improvement.", "labels": [], "entities": []}, {"text": "We repeated this a second time but only made the five most valuable changes, yielding a 0.8% point improvement.", "labels": [], "entities": []}, {"text": "We did not have time to continue this greedy search, leaving further performance gains from searching for the best collection of features unrealized.", "labels": [], "entities": []}, {"text": "We ended our search with 39 feature classes included, with only 21 of these from the original set.", "labels": [], "entities": []}, {"text": "Through the course of these experiments, 10 of the original feature classes were removed while 18 new feature classes were added in our best model.", "labels": [], "entities": []}, {"text": "A final series of experiments were used to heuristically improve the semantic component which was significantly overgenerating.", "labels": [], "entities": []}, {"text": "This yielded the Semantics Consistency Heuristics Filter which results in a 5% improvement to the overall system performance.", "labels": [], "entities": []}, {"text": "The final results on the test data are shown in Table 1.", "labels": [], "entities": []}, {"text": "The baseline system still outperformed all teams including ours.", "labels": [], "entities": []}, {"text": "The baseline was a heuristic system that used two dependency parsers to be more robust to parsing errors.", "labels": [], "entities": []}, {"text": "It mapped dependency parse relations to syntax output directly, with logic to handle conjunctions, passives, and other phenomena.", "labels": [], "entities": []}, {"text": "Semantic labels were a mixture of hard-coded ture.", "labels": [], "entities": []}, {"text": "In an attempt to mirror the features and avoid the possibility of cheating during our experiments, we repeated the same process during the cross-validation experiments, treating the other most similar verb in the training data as the Similar Predicate values for particular syntactic predictions and the most common value in the train data for the corresponding word or syntactic label.", "labels": [], "entities": []}], "tableCaptions": []}