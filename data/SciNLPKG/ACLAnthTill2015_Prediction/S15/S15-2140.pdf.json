{"title": [{"text": "HITSZ-ICRC: An Integration Approach for QA TempEval Challenge", "labels": [], "entities": [{"text": "QA TempEval", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.6423839330673218}]}], "abstractContent": [{"text": "This paper presents the HITSZ-ICRC system designed for the QA TempEval challenge in SemEval-2015.", "labels": [], "entities": [{"text": "QA TempEval challenge in SemEval-2015", "start_pos": 59, "end_pos": 96, "type": "DATASET", "confidence": 0.7291814923286438}]}, {"text": "The system used an integration approach to annotate temporal information by merging temporal annotation results from different temporal annotators.", "labels": [], "entities": []}, {"text": "TIPSemB, ClearTK and TARSQI were used as temporal annotators to get candidate temporal annotation results.", "labels": [], "entities": [{"text": "TIPSemB", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7404012680053711}, {"text": "ClearTK", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.760892927646637}, {"text": "TARSQI", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9270790219306946}]}, {"text": "Evaluation demonstrated that our system was effective for improving the performance of temporal information annotation, and achieved recalls of 0.18, 0.26 and 0.19 on Blog, News and Wikipedeia test sets.", "labels": [], "entities": [{"text": "temporal information annotation", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.6512022217114767}, {"text": "recalls", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9983587861061096}, {"text": "Blog, News and Wikipedeia test sets", "start_pos": 167, "end_pos": 202, "type": "DATASET", "confidence": 0.7157909486974988}]}], "introductionContent": [{"text": "The QA TempEval () in SemEval-2015 is a temporal information annotation challenge, which is a follow-up task after TempEval-1 (, TempEval-2 ( and.", "labels": [], "entities": []}, {"text": "QA TempEval task is similar to the task ABC in TempEval-3, requires participant system (1) extracting and normalizing temporal expressions, (2) extracting events and (3) identifying temporal relations on plain documents.", "labels": [], "entities": []}, {"text": "Temporal information annotation should follow TimeML scheme ().", "labels": [], "entities": [{"text": "Temporal information annotation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8263758619626363}, {"text": "TimeML", "start_pos": 46, "end_pos": 52, "type": "DATASET", "confidence": 0.7931408882141113}]}, {"text": "Difference between QA TempEval task and task ABC in TempEval-3 is evaluation method: in all previous TempEval tasks, annotated result was evaluated by the temporal information annotation accuracy based on manually annotated test corpus; in QA TempEval, annotated result was evaluated by temporal question-answering(QA) accuracy in the given temporal QA system ( based on temporal knowledge produced from participant's annotation.", "labels": [], "entities": []}, {"text": "Temporal annotation is useful in information retrieval, QA, natural language understanding and soon.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.8201665580272675}, {"text": "QA", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9358973503112793}, {"text": "natural language understanding", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.6282191276550293}]}, {"text": "A lot of researches have been attracted on this topic in the past years.", "labels": [], "entities": []}, {"text": "Many methods were proposed and many toolkits were implemented for temporal information annotation.", "labels": [], "entities": [{"text": "temporal information annotation", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.6583429872989655}]}, {"text": "TIMEN) is a communitydriven tool using rule-based method based on knowledge base to solve the temporal expression normalization problem.", "labels": [], "entities": [{"text": "temporal expression normalization", "start_pos": 94, "end_pos": 127, "type": "TASK", "confidence": 0.6334776182969412}]}, {"text": "TARSQI Toolkit) is a modular system for automatic temporal information annotation.", "labels": [], "entities": [{"text": "TARSQI Toolkit", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.7313085496425629}, {"text": "automatic temporal information annotation", "start_pos": 40, "end_pos": 81, "type": "TASK", "confidence": 0.5543578118085861}]}, {"text": "The toolkit can extract temporal expressions, events and recognize temporal relations by its different components.", "labels": [], "entities": []}, {"text": "used CRF models based on semantic information to annotate temporal information according to TimeML scheme, and their TIPSem system got outstanding performance results in TempEval-2.", "labels": [], "entities": []}, {"text": "Steve (2013) piped machine-learning models in his ClearTK system to annotate temporal information using a small set of features.", "labels": [], "entities": []}, {"text": "His system got best performance for temporal relation identification in TempEval-3.", "labels": [], "entities": [{"text": "temporal relation identification", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.6791155139605204}]}, {"text": "The TIMEN toolkit was integrated into the ClearTK system for temporal expression normalization.", "labels": [], "entities": [{"text": "ClearTK", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.861859142780304}, {"text": "temporal expression normalization", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.6998151739438375}]}, {"text": "proposed an automatic method to improve the correctness of each individual annotation by merging different annotation results with different strategies.", "labels": [], "entities": []}, {"text": "This paper described the method HITSZ-ICRC system used for QA TempEval challenge.", "labels": [], "entities": [{"text": "QA TempEval challenge", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.8165754477183024}]}, {"text": "This was first time for HITSZ-ICRC team to do the temporal annotation task.", "labels": [], "entities": []}, {"text": "An integration approach was chosen to get improved annotation result on currently available temporal annotation toolkits for QA TempEval task.", "labels": [], "entities": [{"text": "QA TempEval task", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.4872558116912842}]}, {"text": "Annotation results from those toolkits were merged using a temporal annotation merging method ().", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows: Section 2 describes the system modules used for temporal information annotation.", "labels": [], "entities": [{"text": "temporal information annotation", "start_pos": 102, "end_pos": 133, "type": "TASK", "confidence": 0.6338456869125366}]}, {"text": "Section 3 introduces the data sets and toolkits used, explains and analysis the evaluation results.", "labels": [], "entities": []}, {"text": "Section 4 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Train dataset provided for QA TempEval task is the same dataset in TempEval-3, includes TBAQcleaned dataset and TE3-Platinum () dataset.", "labels": [], "entities": [{"text": "QA TempEval task", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.6017914613087972}, {"text": "TBAQcleaned dataset", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.8321329355239868}, {"text": "TE3-Platinum () dataset", "start_pos": 112, "end_pos": 135, "type": "DATASET", "confidence": 0.7924246390660604}]}, {"text": "TBAQ-cleaned contains cleaned and improved AQUAINT and TimeBank corpus).", "labels": [], "entities": [{"text": "TBAQ-cleaned", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.6334198713302612}, {"text": "AQUAINT", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.7512328624725342}, {"text": "TimeBank corpus", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.9177265465259552}]}, {"text": "The TE3-Platinum is the evaluation corpus for TempEval-3 manually annotated by organizers.", "labels": [], "entities": [{"text": "TE3-Platinum", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.867301881313324}]}, {"text": "All the datasets are annotated in TimeML format.", "labels": [], "entities": []}, {"text": "The test dataset was in TempEval-3 format, and includes 28 plain text documents in Blog (8 documents), News (Wikinews, NYT, WSJ) (10 documents) and Wikipedia (10 documents).", "labels": [], "entities": [{"text": "Blog", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.9036360383033752}, {"text": "Wikipedia", "start_pos": 148, "end_pos": 157, "type": "DATASET", "confidence": 0.9524986743927002}]}, {"text": "Results evaluation was based on 294 temporal questions, 65 questions for Blog documents, 99 for News and 130 for Wikipedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 113, "end_pos": 122, "type": "DATASET", "confidence": 0.9415816068649292}]}, {"text": "The question set was created by human experts based on the test documents.", "labels": [], "entities": []}, {"text": "Annotated result was evaluated by the temporal QA system () using the question set.", "labels": [], "entities": []}, {"text": "The three annotation toolkits TARSQI, ClearTK and TIPSemB were used as temporal annotators.", "labels": [], "entities": [{"text": "TARSQI", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.8862773180007935}]}, {"text": "Default models in the toolkits were used for TARSQI and TIPSemB.", "labels": [], "entities": [{"text": "TARSQI", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.5376030206680298}, {"text": "TIPSemB", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.8373779654502869}]}, {"text": "Models in ClearTK were re-trained with the training data.", "labels": [], "entities": [{"text": "ClearTK", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.9130921959877014}]}, {"text": "In merging step, the temporal annotation merging toolkit () was used to get the final result.", "labels": [], "entities": []}, {"text": "Giving a test document, firstly it was annotated by three temporal annotators separately, including ClearTK, TIPSemB and TARSQI; then the annotated results were corrected to follow TimeML scheme by corrector module and were used as candidate results; finally, the three candidate results were merged using three different strategies.", "labels": [], "entities": [{"text": "TIPSemB", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.7349874377250671}, {"text": "TARSQI", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9019448161125183}]}, {"text": "The models in ClearTK toolkit were trained with TBAQ-cleaned dataset.", "labels": [], "entities": [{"text": "ClearTK toolkit", "start_pos": 14, "end_pos": 29, "type": "DATASET", "confidence": 0.9179833233356476}, {"text": "TBAQ-cleaned dataset", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.754741758108139}]}, {"text": "Six results from different annotators and merging strategies were compared, including three results annotated by different annotators and three results annotated by different merging strategies.", "labels": [], "entities": []}, {"text": "For the system had not been finished before submission deadline, only the result of TARSQI was submitted to QA TempEval challenge.", "labels": [], "entities": [{"text": "TARSQI", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.8530278205871582}, {"text": "QA TempEval challenge", "start_pos": 108, "end_pos": 129, "type": "DATASET", "confidence": 0.7734596729278564}]}, {"text": "The evaluation results for the six temporal annotation results are shown in table 1, 2 and 3 in domain Blog, News and Wikipedia separately.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 118, "end_pos": 127, "type": "DATASET", "confidence": 0.7587312459945679}]}, {"text": "awd% is the percentage of the answered questions and corr is the number of correct answers.", "labels": [], "entities": [{"text": "awd", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9688670635223389}, {"text": "corr", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9787704348564148}]}, {"text": "Run TARSQI, TIPSemB and ClearTK is the result annotated by corresponding temporal annotator.", "labels": [], "entities": [{"text": "TARSQI", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9590127468109131}, {"text": "TIPSemB", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.8423445820808411}]}, {"text": "Run BSTF_VOTE, BTRF_VOTE and RES_UNION is the result produced with different merging strategies.", "labels": [], "entities": [{"text": "BSTF_VOTE", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.49504754940668744}, {"text": "BTRF_VOTE", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9161670804023743}, {"text": "RES_UNION", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9022374153137207}]}, {"text": "F1 value of each annotator result was used as its weight in merging step.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9932414293289185}]}, {"text": "BSTF_VOTE is the result merging with best F1 prior voting strategy.", "labels": [], "entities": [{"text": "BSTF_VOTE", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.6637235283851624}]}, {"text": "BTRF_VOTE is the result with better F1 prior voting strategy.", "labels": [], "entities": [{"text": "BTRF_VOTE", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7566956877708435}, {"text": "F1", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9921640157699585}]}, {"text": "RES_UNION is the result with union strategy.", "labels": [], "entities": [{"text": "RES_UNION", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8441004951794943}]}, {"text": "Results in table 1, 2 and 3 shows that performance of all merged results are better than results annotated by single annotator in each test domain.", "labels": [], "entities": []}, {"text": "It means integration approach is effective for improving temporal information annotation performance.", "labels": [], "entities": []}, {"text": "The union strategy performs best in all the six run results in all domains.", "labels": [], "entities": []}, {"text": "So merging results from all annotators with union strategy is an effective way to get better annotation results based on QA TempEval evaluation method.", "labels": [], "entities": []}, {"text": "Evaluation results show that annotation results from different annotators could be used to improve temporal information annotation performance by results merging.", "labels": [], "entities": []}, {"text": "The precision of all merging results cannot achieve to the highest, and are lower than some annotator results.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995583891868591}]}, {"text": "It means that the merging step merged wrong annotation into final result.", "labels": [], "entities": []}, {"text": "The merging strategies tried in our experiments were more effective on improving the recall of temporal information annotation, which increased the chance that the temporal question could be answered, but were useless for question answering precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9985983967781067}, {"text": "question answering", "start_pos": 222, "end_pos": 240, "type": "TASK", "confidence": 0.7857134938240051}, {"text": "precision", "start_pos": 241, "end_pos": 250, "type": "METRIC", "confidence": 0.7525636553764343}]}, {"text": "So balancing the precision and recall is necessary for improving the performance of annotation results merging.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9995216131210327}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.999504804611206}]}, {"text": "Improving performance of single annotator also is important job forgetting better final annotation result.", "labels": [], "entities": []}, {"text": "We have tried the integration approach using results of the top 3 best performance systems in QA TempEval challenge, and the result still can be improved.", "labels": [], "entities": [{"text": "QA TempEval challenge", "start_pos": 94, "end_pos": 115, "type": "DATASET", "confidence": 0.8245669404665629}]}], "tableCaptions": [{"text": " Table 1. Evaluation results on Blog test data.", "labels": [], "entities": [{"text": "Blog test data", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.9495549201965332}]}, {"text": " Table 2. Evaluation results on News test data.", "labels": [], "entities": [{"text": "News test data", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.9796662529309591}]}, {"text": " Table 3. Evaluation results on Wikipedia test data.", "labels": [], "entities": [{"text": "Wikipedia test data", "start_pos": 32, "end_pos": 51, "type": "DATASET", "confidence": 0.9412357807159424}]}]}