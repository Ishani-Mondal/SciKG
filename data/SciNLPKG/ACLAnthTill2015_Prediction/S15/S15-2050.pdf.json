{"title": [{"text": "LIMSI: Translations as Source of Indirect Supervision for Multilingual All-Words Sense Disambiguation and Entity Linking", "labels": [], "entities": [{"text": "LIMSI", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7738812565803528}, {"text": "Multilingual All-Words Sense Disambiguation", "start_pos": 58, "end_pos": 101, "type": "TASK", "confidence": 0.5723127201199532}, {"text": "Entity Linking", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7076556980609894}]}], "abstractContent": [{"text": "We present the LIMSI submission to the Multilingual Word Sense Disambiguation and Entity Linking task of SemEval-2015.", "labels": [], "entities": [{"text": "Multilingual Word Sense Disambiguation and Entity Linking task", "start_pos": 39, "end_pos": 101, "type": "TASK", "confidence": 0.7070756740868092}]}, {"text": "The system exploits the parallelism of the multilingual test data and uses translations as source of indirect supervision for sense selection.", "labels": [], "entities": [{"text": "sense selection", "start_pos": 126, "end_pos": 141, "type": "TASK", "confidence": 0.778128445148468}]}, {"text": "The LIMSI system gets best results in English in all domains and shows that alignment information can successfully guide disambigua-tion.", "labels": [], "entities": []}, {"text": "This simple but effective method can serve to generate high quality sense annotated data for WSD system training.", "labels": [], "entities": [{"text": "WSD system training", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.9123585025469462}]}], "introductionContent": [{"text": "This paper describes the LIMSI system at the Multilingual Word Sense Disambiguation (WSD) and Entity Linking (EL) task of.", "labels": [], "entities": [{"text": "Multilingual Word Sense Disambiguation (WSD)", "start_pos": 45, "end_pos": 89, "type": "TASK", "confidence": 0.7121286221912929}, {"text": "Entity Linking (EL) task", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.7622078359127045}]}, {"text": "The system performs sense selection by combining translation information obtained through alignment of the multilingual test set with sense ranking.", "labels": [], "entities": [{"text": "sense selection", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.8877172470092773}]}, {"text": "It can thus be described as semisupervised given the indirect supervision provided by the translations.", "labels": [], "entities": []}, {"text": "The alignment correspondences serve as constraints for reducing the search space for each word to BabelNet synsets (hereafter, BabelSynsets) containing the translation and the retained synsets are sorted according to the BabelNet sense ranking.", "labels": [], "entities": []}, {"text": "Our goal is to test the contribution of translations in multilingual WSD with no recourse to context information.", "labels": [], "entities": []}, {"text": "The system needs no training and can be applied directly to parallel data.", "labels": [], "entities": []}, {"text": "The evaluation results show that the LIMSI system outperforms all systems in all domains in English and highlight the important role of translations in guiding disambiguation.", "labels": [], "entities": []}, {"text": "This simple yet effective approach can serve to generate high quality sense annotations for WSD system training.", "labels": [], "entities": [{"text": "WSD system training", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.9155425826708475}]}, {"text": "In what follows, we provide a detailed description of the system, an analysis of the results and a discussion of the factors that determine the efficiency of the method.", "labels": [], "entities": []}], "datasetContent": [{"text": "The test data contains four parallel documents in English, Spanish and Italian.", "labels": [], "entities": []}, {"text": "Our system exploits the parallelism of the test set, a feature overlooked by previous systems . In order to avoid some discrepancies observed at the level of sentence correspondences, we first align the texts pairwise using the Hunalign sentence aligner ().", "labels": [], "entities": []}, {"text": "Then we run GIZA++ ( in both directions at the lemma level and retain only intersecting alignments to rule out spurious correspondences.", "labels": [], "entities": []}, {"text": "For each instance of an English content word in the test set we identify its Spanish translation in context and, alternatively, the English translations of Spanish and Italian words.", "labels": [], "entities": []}, {"text": "We use the lemma and part-of-speech information provided by the task organizers.", "labels": [], "entities": []}, {"text": "The LIMSI alignment-based system yields the top performance in English among the 17 submitted systems, in all domains.", "labels": [], "entities": [{"text": "LIMSI alignment-based", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.49505722522735596}]}, {"text": "This result is very interesting given that our method is very simple: it needs no training and is very easy to compute as it only relies on alignment and sense ranking.", "labels": [], "entities": []}, {"text": "Note that the BFS baseline for English is a very strong one that none of the systems manages to beat.", "labels": [], "entities": [{"text": "BFS baseline", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.8967055082321167}]}, {"text": "As the test set is very small (\u223c 138 parallel sentences), we expect the method to perform even better on larger corpora where the automatic alignment will have higher accuracy and coverage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9988372921943665}, {"text": "coverage", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9540355205535889}]}, {"text": "Our system performs poorly in Spanish and Italian in comparison to English, and is ranked in the fourth position.", "labels": [], "entities": []}, {"text": "The scores obtained in these languages are given in and are compared to the best performing system and the baseline.", "labels": [], "entities": []}, {"text": "A close analysis of the results reveals that the weaker system performance is due to the way the BabelNet API carries out sense ranking in these languages.", "labels": [], "entities": []}, {"text": "In English, WordNet senses are ranked first sorted  by sense number and are followed by Wikipedia senses in lexicographic order.", "labels": [], "entities": []}, {"text": "For languages other than English where frequency information is not available, senses are sorted in lexicographic order, 8 a criterion that often fails to reflect their relevance (i.e. rare senses might be placed higher than more frequent ones).", "labels": [], "entities": []}, {"text": "This certainly affects our system which relies on sense ranking a) when multiple senses are retained after filtering by alignment, and b) when the BFS is needed.", "labels": [], "entities": [{"text": "BFS", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.6863159537315369}]}, {"text": "The low values of the Spanish and Italian BFS baseline reported by the task organizers confirm this finding.", "labels": [], "entities": [{"text": "Spanish and Italian BFS baseline", "start_pos": 22, "end_pos": 54, "type": "DATASET", "confidence": 0.6221693217754364}]}, {"text": "As the first sense retained by the BabelNet API in these languages often is not the most frequent sense, the baseline is outperformed by almost all participating systems.", "labels": [], "entities": []}, {"text": "The higher scores obtained by our system compared to the baseline show that the alignment-based filtering remains beneficial in spite of the problematic sense ranking, as the aligned translation might occur in only one BabelSynset.", "labels": [], "entities": []}, {"text": "provides a detailed analysis of the results.", "labels": [], "entities": []}, {"text": "The top part of the table shows the accuracy of the alignment-based predictions, which might coincide with the BFS or not.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9995926022529602}, {"text": "BFS", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.7587766051292419}]}, {"text": "Our system improves over the BFS in 37 cases in English, 136 in Spanish and 142 in Italian.", "labels": [], "entities": [{"text": "BFS", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.6321852803230286}]}, {"text": "On the contrary, the BFS does better only 13, 11 and 16 times in the three languages.", "labels": [], "entities": [{"text": "BFS", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.6357642412185669}]}, {"text": "The system falls back to the BFS in case of unaligned words or when the translations are not found in some BabelNet synset.", "labels": [], "entities": [{"text": "BFS", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.866487979888916}]}, {"text": "As shown in the lower part of, the BFS predictions are often wrong, especially in.", "labels": [], "entities": [{"text": "BFS", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.5560495257377625}]}, {"text": "This analysis shows the limited impact of the BFS on the performance of the LIMSI system which manages to improve over the baseline in numerous cases.", "labels": [], "entities": [{"text": "BFS", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.46855366230010986}]}, {"text": "The system fails to provide the correct sense in cases of parallel ambiguities where a word and its translation carry the same senses.", "labels": [], "entities": []}, {"text": "For exemple, this instance of window: Here's a screenshot of kalgebra main window. is aligned to ventana in the Spanish text, which translates both the \"opening\" and the \"computer\" sense of the word.", "labels": [], "entities": []}, {"text": "Although the Spanish translation helps to rule out 11 of the 15 BabelSynsets of window, ranking the remaining four synsets puts forward the more frequent \"opening\" sense (00081285n) which is incorrect for this instance.", "labels": [], "entities": []}, {"text": "Using translations in multiple languages could improve accuracy in these cases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9980133771896362}]}], "tableCaptions": [{"text": " Table 1: Best performing systems at the SemEval-2015 Multilingual WSD and Entity Linking task for English.", "labels": [], "entities": [{"text": "SemEval-2015 Multilingual WSD and Entity Linking task", "start_pos": 41, "end_pos": 94, "type": "TASK", "confidence": 0.7267539756638663}]}, {"text": " Table 2: LIMSI, best system and BFS scores in Spanish and Italian.", "labels": [], "entities": [{"text": "LIMSI", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9782308340072632}, {"text": "BFS scores", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.960121214389801}]}, {"text": " Table 3: The top part of the table gives the # of cor- rect/wrong annotations made by the LIMSI system. The  lower part shows the # of correct/wrong predictions when  the system falls back to the BFS.", "labels": [], "entities": [{"text": "cor- rect/wrong annotations made", "start_pos": 51, "end_pos": 83, "type": "METRIC", "confidence": 0.7814988834517342}, {"text": "BFS", "start_pos": 197, "end_pos": 200, "type": "DATASET", "confidence": 0.9650813937187195}]}]}