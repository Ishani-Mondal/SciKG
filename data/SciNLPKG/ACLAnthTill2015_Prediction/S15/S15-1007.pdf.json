{"title": [{"text": "A Methodology for Word Sense Disambiguation at 90% based on large-scale CrowdSourcing", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.8195602893829346}]}], "abstractContent": [{"text": "Word Sense Disambiguation has been stuck for many years.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8357765674591064}]}, {"text": "In this paper we explore the use of large-scale crowdsourcing to cluster senses that are often confused by non-expert annotators.", "labels": [], "entities": []}, {"text": "We show that we can increase performance at will: our in-domain experiment involving 45 highly polysemous nouns, verbs and adjective (9.8 senses on average), yields an average accuracy of 92.6 using a supervised classifier for an average polysemy of 6.1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.996266782283783}]}, {"text": "Our proposal has the advantage of being cost-effective and being able to produce different levels of granularity.", "labels": [], "entities": []}, {"text": "Our analysis shows that the error reduction with respect to fine-grained senses is higher, and manual inspection show that the clusters are sensible when compared to those of OntoNotes and WordNet Supersenses.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.9785418510437012}]}], "introductionContent": [{"text": "Word sense ambiguity is a major hurdle for accurate information extraction, summarization and machine translation.", "labels": [], "entities": [{"text": "Word sense ambiguity", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6281130711237589}, {"text": "accurate information extraction", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.6214984754721323}, {"text": "summarization", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.9912537932395935}, {"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8002763986587524}]}, {"text": "The utility of Word Sense Disambiguation (WSD) depends on the accuracy and on how useful the sense distinctions are.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.7556337813536326}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9992971420288086}]}, {"text": "The first issue is quantitative, as it can be measured using a WSD system on certain dataset.", "labels": [], "entities": []}, {"text": "The second examines whether the sense distinctions are appropriate, which varies from application to application.", "labels": [], "entities": []}, {"text": "Although usefulness can be explored in a downstream application, it is usually assessed subjectively, discussing the quality of the sense distinctions (.", "labels": [], "entities": []}, {"text": "Both issues (performance and usefulness) are linked to the granularity of the sense inventory, and conflict with each other: finer granularity might produce more useful distinctions but the accuracy would be worse, and vice-versa.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9984316229820251}]}, {"text": "WordNet) is the most widely used resource to build word sense disambiguation tools and word sense annotated corpora, including recent large efforts (), but its fine-grainedness has been mentioned to be a problem (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9645311236381531}, {"text": "word sense disambiguation", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.6789940198262533}]}, {"text": "We think that a desiderata fora sense inventory would be that it provides useful sense distinctions and useful performance across a large range of applications.", "labels": [], "entities": []}, {"text": "We would also add that it should be tightly integrated with WordNet, given its prevalence on NLP applications, and we thus focus on sense inventories which are mapped to WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9673471450805664}, {"text": "WordNet", "start_pos": 170, "end_pos": 177, "type": "DATASET", "confidence": 0.9650792479515076}]}, {"text": "In order to asses usefulness, we need specific measures.", "labels": [], "entities": []}, {"text": "Downstream application is difficult, and unfeasible for new proposals, as a full-fledged sense inventories and associated annotations are necessary.", "labels": [], "entities": []}, {"text": "We can instead estimate usefulness of proposed sense inventories using several proxy measures: \u2022 High polysemy.", "labels": [], "entities": []}, {"text": "Note that polysemy alone could be misleading, as a word with many senses might be skewed to a single sense: 99% of occurrences could belong to a single sense, while the rest are only seen once.", "labels": [], "entities": []}, {"text": "Besides the absolute polysemy, we can use the accuracy of the most frequent sense (MFS, estimated in train data and applied to test data) as a simple 61 and effective indication of skewness.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9994316697120667}, {"text": "MFS", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.8127008676528931}]}, {"text": "High polysemy and low MFS are desirable properties.", "labels": [], "entities": [{"text": "MFS", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9604575037956238}]}, {"text": "\u2022 High performance, as measured the accuracy of a supervised system trained on hand-annotated data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9993531107902527}]}, {"text": "The higher the accuracy, the better.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9998242259025574}]}, {"text": "\u2022 Flexible sense granularity, that is, the ability to produce different degrees of polysemy and accuracy, from fine-grained to coarse-grained.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9991574287414551}]}, {"text": "When comparing sense inventories with different granularities, absolute MFS and supervised performance are not enough.", "labels": [], "entities": []}, {"text": "We propose to use error reduction of the supervised system with respect to the MFS as a measure of the balance between low MFS and high supervised performance.", "labels": [], "entities": []}, {"text": "The larger the error reduction the better.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 15, "end_pos": 30, "type": "METRIC", "confidence": 0.9763004183769226}]}, {"text": "\u2022 Manual inspection of the sense distinctions, as a complement to quantitative measures.", "labels": [], "entities": []}, {"text": "We propose to use crowdsourced annotations () to cluster WordNet senses that are often confused by non-expert annotators.", "labels": [], "entities": []}, {"text": "Our method can provide clusters at different levels of granularity.", "labels": [], "entities": []}, {"text": "We show that we can construct clusters yielding around 90% accuracy for 45 words, with higher error reduction with respect to MFS than fine-grained senses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9984176158905029}, {"text": "error reduction", "start_pos": 94, "end_pos": 109, "type": "METRIC", "confidence": 0.9756143093109131}]}, {"text": "By construction, we merge senses which are often confused by annotators, yielding sensible sense clusters, as corroborated by manual inspection.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 mentions related work.", "labels": [], "entities": []}, {"text": "We then present the annotations, followed by the clustering procedure.", "labels": [], "entities": []}, {"text": "Section 5 report the main experiments.", "labels": [], "entities": []}, {"text": "Section 6 compares our clustering to that of OntoNotes followed by a comparison to WordNet Supersenses.", "labels": [], "entities": [{"text": "WordNet Supersenses", "start_pos": 83, "end_pos": 102, "type": "DATASET", "confidence": 0.9413244724273682}]}, {"text": "Section 8 draws the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The gold standard is based on the multiple annotations in the corpus, but a single sense was selected as the correct one, following), which use a probabilistic annotation model.", "labels": [], "entities": []}, {"text": "We split the 1000 examples for each word into development and test, sampling 85% (and 15% respectively) at random, preserving the overall sense distribution.", "labels": [], "entities": []}, {"text": "The Word Sense Disambiguation algorithm of choice is It Make Sense (IMS) (, which reports the best WSD results to date.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6590261360009512}, {"text": "It Make Sense (IMS)", "start_pos": 53, "end_pos": 72, "type": "METRIC", "confidence": 0.6112107386191686}]}, {"text": "IMS is a freely available Java implementation 1 , which provides an extensible and flexible platform for researchers interested in using a WSD component.", "labels": [], "entities": [{"text": "IMS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9559687972068787}]}, {"text": "Following, IMS adopts support vector machines as the classifier and integrates the state of the features extractors including parts-of-speech of the surrounding words, bag of words features, and local collocations as features.", "labels": [], "entities": [{"text": "IMS", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.8758842349052429}]}, {"text": "IMS provides ready-to-use models trained with examples collected from parallel texts , SEM-COR (, and the DSO corpus ().", "labels": [], "entities": [{"text": "IMS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9479416012763977}, {"text": "DSO corpus", "start_pos": 106, "end_pos": 116, "type": "DATASET", "confidence": 0.9128371775150299}]}, {"text": "In our experiments we train IMS with the train examples of the crowdsourced MASC.", "labels": [], "entities": [{"text": "IMS", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8873373866081238}, {"text": "MASC", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.7711148262023926}]}, {"text": "We used IMS out-of-the-box, using the default parametrization and built-in feature extraction.", "labels": [], "entities": []}, {"text": "We compare results obtained with IMS against the Most Frequent Sense (MFS), which was estimated using the training corpus.", "labels": [], "entities": [{"text": "IMS", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8499020338058472}, {"text": "Most Frequent Sense (MFS)", "start_pos": 49, "end_pos": 74, "type": "METRIC", "confidence": 0.8299445807933807}]}, {"text": "Both systems (IMS and MFS) could be trained on fine-grained senses, on coarse-grained senses induced from the confusion matrix using the 90% threshold described above (Coarse conf ) and coarse-grained senses induced from random clustering using the 90% threshold (Coarse random ).", "labels": [], "entities": []}, {"text": "We also used sense clusters from OntoNotes and WordNet Supersenses (cf. Sections 6 and 7) .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Development and test results using cross- validation (left side) and test results (right side) for IMS  and MFS using three sense inventories.", "labels": [], "entities": []}, {"text": " Table 2: The 45 words, with PoS, polysemy, IMS and Mfs accuracy for fine-grained, our clustering (Conf.), random  clustering, OntoNotes coarse-grained senses (ON, cf. Section 6) and Supersenses (SS, cf. Section 7). The bottom  rows report averages for the 45 words and the 18 words in OntoNotes.", "labels": [], "entities": [{"text": "IMS", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.902750551700592}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9336255788803101}, {"text": "OntoNotes", "start_pos": 286, "end_pos": 295, "type": "DATASET", "confidence": 0.9169037938117981}]}, {"text": " Table 4: Senses for level-v in WN, OntoNotes (ON) and our clusters (Conf), including frequencies in train and glosses.", "labels": [], "entities": [{"text": "WN", "start_pos": 32, "end_pos": 34, "type": "DATASET", "confidence": 0.7698421478271484}]}, {"text": " Table 5: Senses for help-n in WN, OntoNotes (ON) and our clusters (Conf), including frequencies in train and glosses.", "labels": [], "entities": []}, {"text": " Table 7: Senses for level-n in WN, Supersenses (SS) and our clusters (Conf), including frequencies in train and glosses.", "labels": [], "entities": []}, {"text": " Table 8: Senses for help-n in WN, Supersenses (SS) and our clusters (Conf), including freq. in train and glosses.", "labels": [], "entities": []}]}