{"title": [{"text": "ECNU: Multi-level Sentiment Analysis on Twitter Using Traditional Linguistic Features and Word Embedding Features", "labels": [], "entities": [{"text": "ECNU", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7696791887283325}, {"text": "Sentiment Analysis", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.8210365176200867}]}], "abstractContent": [{"text": "This paper reports our submission to task 10 (Sentiment Analysis on Tweet, SAT) (Rosen-thal et al., 2015) in SemEval 2015 , which contains five subtasks, i.e., contextual polarity disambiguation (subtask A: expression-level), message polarity classification (subtask B: message-level), topic-based message polarity classification and detecting trends towards a topic (subtask C and D: topic-level), and determining sentiment strength of twitter terms (subtask E: term-level).", "labels": [], "entities": [{"text": "contextual polarity disambiguation", "start_pos": 160, "end_pos": 194, "type": "TASK", "confidence": 0.6447818179925283}, {"text": "message polarity classification", "start_pos": 226, "end_pos": 257, "type": "TASK", "confidence": 0.7844627896944681}, {"text": "topic-based message polarity classification", "start_pos": 286, "end_pos": 329, "type": "TASK", "confidence": 0.6402628496289253}]}, {"text": "For the first four sub-tasks, we built supervised models using traditional features and word embedding features to perform sentiment polarity classification.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 123, "end_pos": 156, "type": "TASK", "confidence": 0.8727578719456991}]}, {"text": "For subtask E, we first expanded the training data with the aid of external sentiment lexicons and then built a regression model to estimate the sentiment strength.", "labels": [], "entities": []}, {"text": "Despite the simplicity of features, our systems rank above the average.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the past few years, hundreds of millions of people shared and expressed their opinions through microblogging websites, such as Twitter.", "labels": [], "entities": []}, {"text": "The study on this platform is increasingly drawing attention of many researchers and organizations.", "labels": [], "entities": []}, {"text": "Given the character limitations on tweets, the sentiment orientation classification on tweets is usually analogous to the sentence-level sentiment analysis).", "labels": [], "entities": [{"text": "sentiment orientation classification", "start_pos": 47, "end_pos": 83, "type": "TASK", "confidence": 0.7791495521863302}, {"text": "sentence-level sentiment analysis", "start_pos": 122, "end_pos": 155, "type": "TASK", "confidence": 0.7374295790990194}]}, {"text": "However, considering opinions adhering on different topics and expressed by various expression words in tweets, () have investigated various ways to settle these target dependent issues.", "labels": [], "entities": []}, {"text": "Recently, inspired by) using neural network to construct distributed word representation (word embedding), several researchers employed neural network to perform sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.9508554637432098}]}, {"text": "For example,) adopted convolutional neural networks to learn sentiment-bearing sentence vectors, and ( proposed Paragraph vector which outperformed bag-of-words model for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.9405187666416168}]}, {"text": "The task of Sentiment Analysis in Twitter (SAT) in SemEval 2015 consists of five subtasks.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter (SAT) in SemEval 2015", "start_pos": 12, "end_pos": 63, "type": "TASK", "confidence": 0.78714819252491}]}, {"text": "The first three subtasks focus on determining the polarity of the given tweet, phrase or topic (i.e., subtask A aims at classifying the sentiment of a marked instance in a given message, subtask B is to determine the polarity of the whole message and subtask C focuses on identifying the sentiment of the message towards the given topic).", "labels": [], "entities": []}, {"text": "The fourth subtask Dis to detect the sentiment trends of a given set of messages towards a topic from the same period of time.", "labels": [], "entities": [{"text": "Dis", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9245904088020325}]}, {"text": "The last subtask E is to predict a score between 0 and 1, which is indicative of the strength of association of twitter terms with positive sentiment.", "labels": [], "entities": []}, {"text": "Following previous works), we adopted a rich set of traditional features, e.g., linguistic features (e.g., n-gram at word level, part-of-speech (POS) tags, negations, etc), sentiment lexicon features (e.g., MPQA, Bing Liu opinion lexicon, SentiWordNet, etc) and twitter specif-ical features (e.g., the number of URL, emoticons, capital words, elongated words, hashtags, etc).", "labels": [], "entities": []}, {"text": "Besides, inspired by), we also employed novel word embedding features in these tasks.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reports our systems including preprocessing, feature engineering, evaluation metrics, etc.", "labels": [], "entities": []}, {"text": "The data sets and experiments descriptions are shown in Section 3.", "labels": [], "entities": []}, {"text": "Finally, we conclude this paper in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "For subtask A, B and C, we used the macroaveraged F score of positive and negative classes (i.e., ) to evaluate the performance, which considers a sense of effectiveness on small classes.", "labels": [], "entities": [{"text": "F score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9170347154140472}]}, {"text": "For subtask D, the averaged absolute difference (i.e., avgAbsDif f = 1 n \u2211 n i=1 |x i \u2212 \u00af x|) is employed, which is a common measure of how much a set of observations differ from the average.", "labels": [], "entities": [{"text": "averaged absolute difference", "start_pos": 19, "end_pos": 47, "type": "METRIC", "confidence": 0.7174347837766012}]}, {"text": "Since the subtask E aims at predicting the sentiment score for target term, in order to make the comparison of predicted strength of different terms reasonable, the Kendall rank correlation coefficient (usually measures the association between two measured quantities) and Spearman rank correlation (a nonparametric measure of statistical dependence between two variables) are adopted in this subtask, where the Kendall rank correlation coefficient is the official evaluation criteria.", "labels": [], "entities": [{"text": "Kendall rank correlation coefficient", "start_pos": 165, "end_pos": 201, "type": "METRIC", "confidence": 0.9397001415491104}, {"text": "Spearman rank correlation", "start_pos": 273, "end_pos": 298, "type": "METRIC", "confidence": 0.8890199263890585}, {"text": "Kendall rank correlation coefficient", "start_pos": 412, "end_pos": 448, "type": "METRIC", "confidence": 0.7551960945129395}]}, {"text": "The organizers provided tweet ids and a script for all participants to collect data.", "labels": [], "entities": []}, {"text": "shows the statistics of the data sets we used in our experiments.", "labels": [], "entities": []}, {"text": "For subtask A and B, the training data set is composed of SemEval 2013 Task 2 training and development data () and the development data set is made up of the test sets from the same tasks in previous two years.", "labels": [], "entities": [{"text": "SemEval 2013 Task 2 training and development data", "start_pos": 58, "end_pos": 107, "type": "DATASET", "confidence": 0.7047000229358673}]}, {"text": "For subtask C and D, this data is divided into many topic sets.", "labels": [], "entities": []}, {"text": "With regard to subtask E, the organizers provided 200 terms labeled with a decimal in the range of 0 to 1.", "labels": [], "entities": []}, {"text": "We observed that among these 200 given terms, 22% are hashtags and 15% contain negator.", "labels": [], "entities": []}, {"text": "In consideration of the lack of training data, we expanded it with 1, 346 terms collected from following sources: 916 terms which are present in all above mentioned 7 sentiment lexicons, 230 terms with hashtag and 200 terms with negator extracted from NRC Hashtag sentiment lexicon randomly.", "labels": [], "entities": [{"text": "NRC Hashtag sentiment lexicon", "start_pos": 252, "end_pos": 281, "type": "DATASET", "confidence": 0.950197160243988}]}, {"text": "The provided 200 terms were used as development data.", "labels": [], "entities": []}, {"text": "To predict the strength values of the extended data, we used the M-PQA sentiment lexicon label as reference.", "labels": [], "entities": [{"text": "M-PQA sentiment lexicon label", "start_pos": 65, "end_pos": 94, "type": "DATASET", "confidence": 0.7081074342131615}]}, {"text": "There are 6 polarity types in MPQA, i.e., strong positive, weak positive, both strong, both weak, weak negative and strong negative.", "labels": [], "entities": [{"text": "MPQA", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.8157474398612976}]}, {"text": "We converted them to numeric score as 1, 0.75, 0.5, 0.5, 0.25, 0 respectively.", "labels": [], "entities": []}, {"text": "By doing so, if a target term is present in this expanded lexicon, the output is its corresponding score.", "labels": [], "entities": []}, {"text": "Otherwise we split the term to several words and calculated their averaged sentiment score as output., it is interesting to find that: (1) SentiLexi and unigram are the most effective feature types to detect the polarities; (2) POS feature makes contribution to improve the performance for subtask B but no improvement for A.", "labels": [], "entities": []}, {"text": "It maybe because the neutral instances in subtask B (i.e., 45.58%) are much more than that in subtask A (i.e., 5.01%); The emoticons features are not as effective as expected since most emoticons are already present in unigram.", "labels": [], "entities": []}, {"text": "Besides, following) we adopted sentence modeling and extracted the penultimate hidden layer content as novel word embedding feature to build another classifier.", "labels": [], "entities": []}, {"text": "Furthermore, we combined the intermediate results (i.e., the distances between point to multiple hyperplanes returned from SVM) of two classifiers.", "labels": [], "entities": []}, {"text": "The experimental results of using word embedding features in isolation and in combination are shown in.", "labels": [], "entities": []}, {"text": "From, we find that the word embedding alone performs a bit worse than the traditional features.", "labels": [], "entities": []}, {"text": "This maybe because the traditional features are dozens of times more than word embedding features and as a result the effectiveness of word embeddings is impaired.", "labels": [], "entities": []}, {"text": "However, when we combined the two experimental results, we find that the combination result of two classifiers achieves the best performances in both subtasks.", "labels": [], "entities": []}, {"text": "This indicates that although the size of word embeddings is small, it still makes contribution to performance improvement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of data sets in training (train), devel- opment (dev), test (test) set for subtask A, B, C and D.  Twitter2014S stands for Twitter2014Sarcasm.", "labels": [], "entities": []}, {"text": " Table 3: Results of subtask A and B using traditional fea- tures, word embedding features and their combination in  terms of F macro on training data.", "labels": [], "entities": []}, {"text": " Table 2: Results of feature selection experiments for subtask A, B and C in terms of F macro on the training data.  The numbers in the brackets are the performance increments compared with the previous results. PAHE stands for  Punctuation&All-caps&Hashtag&Enlongated features. \".+\" means to add current feature to the previous feature set.", "labels": [], "entities": []}, {"text": " Table 5: Results of feature section experiments for sub- task E on training data.", "labels": [], "entities": []}]}