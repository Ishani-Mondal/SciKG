{"title": [{"text": "AMRITA \u00af CEN@SemEval-2015: Paraphrase Detection for Twitter using Unsupervised Feature Learning with Recursive Autoencoders", "labels": [], "entities": [{"text": "AMRITA \u00af CEN@SemEval-2015", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.8325403809547425}, {"text": "Paraphrase Detection", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.9236502647399902}]}], "abstractContent": [{"text": "We explore using recursive autoencoders for SemEval 2015 Task 1: Paraphrase and Semantic Similarity in Twitter.", "labels": [], "entities": [{"text": "SemEval 2015 Task 1", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8231033831834793}]}, {"text": "Our paraphrase detection system makes use of phrase-structure parse tree embeddings that are then provided as input to a conventional supervised classification model.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8903295397758484}]}, {"text": "We achieve an F1 score of 0.45 on paraphrase identification and a Pear-son correlation of 0.303 on computing semantic similarity.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9834697246551514}, {"text": "paraphrase identification", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.904423177242279}, {"text": "Pear-son correlation", "start_pos": 66, "end_pos": 86, "type": "METRIC", "confidence": 0.8466528058052063}]}], "introductionContent": [{"text": "The process of rewriting text with a different choice of words or using a different sentence structure while preserving meaning is called paraphrasing.", "labels": [], "entities": []}, {"text": "Identifying paraphrases can be a difficult task owing to the fact that evaluating surface level similarity is often not enough, but rather systems must take into account the underlying semantics of the content being assessed.", "labels": [], "entities": []}, {"text": "Paraphrasing and paraphrase detection are important and challenging tasks, which find their application in various subfields of Natural Language Processing (NLP) such as information retrieval, question answering), plagiarism detection), text summarization and evaluation of machine translation.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.9112888276576996}, {"text": "information retrieval", "start_pos": 170, "end_pos": 191, "type": "TASK", "confidence": 0.759313315153122}, {"text": "question answering", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.8215082883834839}, {"text": "plagiarism detection", "start_pos": 214, "end_pos": 234, "type": "TASK", "confidence": 0.7847844958305359}, {"text": "text summarization", "start_pos": 237, "end_pos": 255, "type": "TASK", "confidence": 0.802675873041153}, {"text": "evaluation of machine translation", "start_pos": 260, "end_pos": 293, "type": "TASK", "confidence": 0.6529947072267532}]}, {"text": "We explore using recursive autoencoders for paraphrase detection and similarity scoring as apart of SemEval 2015 Task 1: Paraphrase and Semantic Similarity in Twitter.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.9171445071697235}, {"text": "similarity scoring", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.6877550035715103}, {"text": "SemEval 2015 Task 1", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7503489404916763}]}, {"text": "Twitter is an online social networking service with millions of users who casually converse about diverse topics in a continuous and contemporaneous manner.", "labels": [], "entities": []}, {"text": "gives an example of real tweets, some of which are paraphrases of each other.", "labels": [], "entities": []}, {"text": "The very casual style of the Twitter corpus makes it more challenging to work with for many NLP tools.", "labels": [], "entities": []}, {"text": "We use vector space embeddings, in part, since they are relatively good at dealing with noisy data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use a general domain parsing model distributed with the Stanford Parser, englishPCFG v1.6.9 (.", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.9418980777263641}, {"text": "englishPCFG v1.6.9", "start_pos": 76, "end_pos": 94, "type": "DATASET", "confidence": 0.818144828081131}]}, {"text": "Prior to training the RAE vectors, we pre-trained word embedding vectors for use as the word level representations.", "labels": [], "entities": [{"text": "RAE", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.8481321334838867}]}, {"text": "The hyperparameter values used for our system are as follows: (1) the size of the pooling matrix n p = 13; (2) the regularization for the softmax classifier c = 0.05; (3) Both the RAE and word embeddings are 100-dimensional vectors.", "labels": [], "entities": []}, {"text": "For the unsupervised unfolding RAE training, we experimented with using subsets of different sized Twitter corpora of 50,000, 80,000 and 95,000 sentences to evaluate the proposed system.", "labels": [], "entities": [{"text": "RAE", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9240415096282959}]}, {"text": "Using PIT-2015, we trained using tweets from the training set and evaluated the resulting series of systems on the dev set (Wei).", "labels": [], "entities": [{"text": "PIT-2015", "start_pos": 6, "end_pos": 14, "type": "DATASET", "confidence": 0.8346108794212341}]}, {"text": "For supervised training, we used the training set from PIT-2015.", "labels": [], "entities": [{"text": "PIT-2015", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.7990049719810486}]}, {"text": "For training the unsupervised unfolding RAE vectors, we collected additional data using the Twitter Developer API.", "labels": [], "entities": [{"text": "RAE", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.8814730644226074}]}, {"text": "As shown in, we found that increasing the size of the data set used to train the RAE embeddings leads to strong gains in system performance.", "labels": [], "entities": [{"text": "RAE embeddings", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.7761083841323853}]}, {"text": "Notice that as the amount of data used to train the RAE vectors increases, the preci-1 Due to time constraints we did not explore using more than 95,000 sentences to train our embedding model.", "labels": [], "entities": [{"text": "RAE", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.765397846698761}]}, {"text": "sion value for paraphrase detection increases significantly while the recall value is actually falling.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.9481951296329498}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9990450739860535}]}, {"text": "The official evaluation metrics for SemEval-2015 Task 1 are F1-score for paraphrase identification and Pearson correlation for the semantic similarity scores.", "labels": [], "entities": [{"text": "SemEval-2015 Task 1", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8878816366195679}, {"text": "F1-score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9995530247688293}, {"text": "paraphrase identification", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.8912371695041656}, {"text": "Pearson correlation", "start_pos": 103, "end_pos": 122, "type": "METRIC", "confidence": 0.9567181766033173}]}, {"text": "The performance of our system on the shared task evaluation data using these metrics is presented in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of PIT-2015 Twitter Paraphrase Corpus.", "labels": [], "entities": [{"text": "PIT-2015 Twitter Paraphrase Corpus", "start_pos": 24, "end_pos": 58, "type": "DATASET", "confidence": 0.853745624423027}]}, {"text": " Table 3: PIT-2015 dev set performance using varying amounts of training data.", "labels": [], "entities": []}, {"text": " Table 4: Results from the SemEval-2015.", "labels": [], "entities": []}]}