{"title": [{"text": "Learning to predict script events from domain-specific text", "labels": [], "entities": []}], "abstractContent": [{"text": "The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works.", "labels": [], "entities": [{"text": "automatic induction of scripts (Schank and Abelson, 1977)", "start_pos": 4, "end_pos": 61, "type": "TASK", "confidence": 0.7799159505150535}]}, {"text": "In this paper, we employ a variety of these methods to learn Schank and Abelson's canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called \"Din-ners from Hell.\"", "labels": [], "entities": []}, {"text": "Our models learn narrative chains, script-like structures that we evaluate with the \"narrative cloze\" task (Chambers and Jurafsky, 2008).", "labels": [], "entities": []}], "introductionContent": [{"text": "A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (.", "labels": [], "entities": []}, {"text": "Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques.", "labels": [], "entities": []}, {"text": "In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora).", "labels": [], "entities": [{"text": "script induction", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.8340607285499573}]}, {"text": "These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts maybe learned, but the acquisition of any particular set of scripts is not guaranteed.", "labels": [], "entities": [{"text": "open-domain script acquisition", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.5949315627415975}]}, {"text": "For many specialized applications, however, knowledge of a few relevant scripts maybe more useful than knowledge of many irrelevant scripts.", "labels": [], "entities": []}, {"text": "With this scenario in mind, we attempt to learn the famous \"restaurant script\" by applying the aforementioned narrative chain learning methods to a specialized corpus of dinner narratives we compile from the website \"Dinners from Hell.\"", "labels": [], "entities": []}, {"text": "Our results suggest that applying these techniques to a domain-specific dataset maybe reasonable way to learn domain-specific scripts.", "labels": [], "entities": []}], "datasetContent": [{"text": "The source of our data for this experiment is a blog called \"Dinners From Hell\" 2 where readers submit stories about their terrible restaurant experiences.", "labels": [], "entities": []}, {"text": "For an example story, see.", "labels": [], "entities": []}, {"text": "To process the raw data, we stripped all HTML and other non-story content from each file and processed the remaining text with the Stanford CoreNLP pipeline version 3.3.1 ().", "labels": [], "entities": [{"text": "Stanford CoreNLP pipeline", "start_pos": 131, "end_pos": 156, "type": "DATASET", "confidence": 0.9215627908706665}]}, {"text": "Of the 237 stories obtained, we manually filtered out 94 stories that were \"off-topic\" (e.g., letters to the webmaster, dinners not at restaurants), leaving a total of 143 stories.", "labels": [], "entities": []}, {"text": "The average story length is 352 words.", "labels": [], "entities": []}], "tableCaptions": []}