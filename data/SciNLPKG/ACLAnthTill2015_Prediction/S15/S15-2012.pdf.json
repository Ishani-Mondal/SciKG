{"title": [{"text": "TKLBLIIR: Detecting Twitter Paraphrases with TweetingJay", "labels": [], "entities": [{"text": "Detecting Twitter Paraphrases", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.8971044818560282}]}], "abstractContent": [{"text": "When tweeting on a topic, Twitter users often post messages that convey the same or similar meaning.", "labels": [], "entities": []}, {"text": "We describe TweetingJay, a system for detecting paraphrases and semantic similarity of tweets, with which we participated in Task 1 of SemEval 2015.", "labels": [], "entities": []}, {"text": "TweetingJay uses a supervised model that combines semantic overlap and word alignment features, previously shown to be effective for detecting semantic textual similarity.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.7188999354839325}, {"text": "detecting semantic textual similarity", "start_pos": 133, "end_pos": 170, "type": "TASK", "confidence": 0.7749399840831757}]}, {"text": "TweetingJay reaches 65.9% F1-score and ranked fourth among the 18 participating systems.", "labels": [], "entities": [{"text": "TweetingJay", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8489201664924622}, {"text": "F1-score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9995211362838745}]}, {"text": "We additionally provide an analysis of the dataset and point to some peculiarities of the evaluation setup.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recognizing tweets that convey the same meaning (paraphrases) or similar meaning is useful in applications such as event detection, tweet summarization, and tweet retrieval).", "labels": [], "entities": [{"text": "event detection", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.7654685378074646}, {"text": "tweet summarization", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7808263301849365}, {"text": "tweet retrieval", "start_pos": 157, "end_pos": 172, "type": "TASK", "confidence": 0.8390147686004639}]}, {"text": "Paraphrase detection in tweets is a more challenging task than paraphrase detection in other domains such as news (.", "labels": [], "entities": [{"text": "Paraphrase detection in tweets", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9228338599205017}, {"text": "paraphrase detection", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.7723913490772247}]}, {"text": "Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (), such as informality, ungrammaticality, disfluency, and excessive use of jargon.", "labels": [], "entities": []}, {"text": "In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (.", "labels": [], "entities": [{"text": "SemEval 2015 evaluation exercise", "start_pos": 125, "end_pos": 157, "type": "TASK", "confidence": 0.6942399740219116}]}, {"text": "Our system builds on findings from a large body of work on semantic textual similarity (STS)) and recent breakthroughs in distributed word representations ().", "labels": [], "entities": [{"text": "semantic textual similarity (STS", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.6753117084503174}]}, {"text": "We design a set of measures that capture the semantic similarity of tweets and train a support vector machine (SVM) using these measures as features.", "labels": [], "entities": []}, {"text": "Positioning of our system at rank four among 18 teams, with only point and a half lower performance compared to the the best-performing system, suggests that STS measures are useful for detecting paraphrases in Twitter.", "labels": [], "entities": []}, {"text": "We make our system freely available.", "labels": [], "entities": []}, {"text": "Besides providing the description of the TweetingJay system, in this paper we analyze the evaluation setup, with special focus on the provided dataset and its subsets (train, validation, and test), and discuss the stability of the evaluation results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each team was allowed to submit two runs on the test set provided by the task organizers (.", "labels": [], "entities": []}, {"text": "Participants were provided with a training set (13,063 pairs) and a development set (4,727 pairs).", "labels": [], "entities": []}, {"text": "We used the train and development set to optimize the hyperparameters C and \u03b3 of our SVM model with the RBF kernel.", "labels": [], "entities": [{"text": "RBF kernel", "start_pos": 104, "end_pos": 114, "type": "DATASET", "confidence": 0.8928739726543427}]}, {"text": "For the final evaluation, the organizers used a test set of 972 tweet pairs.", "labels": [], "entities": []}, {"text": "We divided the features in three groups: (1) semantic overlap features (SO) from Section 3.1, (2) aligned word pairs (AWP) features, and (3) the anchor count feature (ANC) from Section 3.2.", "labels": [], "entities": [{"text": "anchor count feature (ANC)", "start_pos": 145, "end_pos": 171, "type": "METRIC", "confidence": 0.7265660365422567}]}, {"text": "There are three ways how the optimization of the SVM model (hyperparameters C and \u03b3) could have been carried out: (1) training and optimization on the train set using 10-folded crossvalidation, with no use of the development set (model M1); (2) training on the train set and optimization on the development set (model M2), and (3) training on the union of the train and development set using 10-folded cross-validation (model M3).", "labels": [], "entities": []}, {"text": "Following the advice of the task organizers, we removed debatable cases from both the train and dev sets.", "labels": [], "entities": []}, {"text": "We submitted models M1 and M2 for the official evaluation (our team name was TKLBLIIR).", "labels": [], "entities": [{"text": "TKLBLIIR", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.8247635364532471}]}, {"text": "We obtain lower results on the test set (61.3% F1 vs. 69.6%).", "labels": [], "entities": [{"text": "F1", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9993725419044495}]}, {"text": "This is likely caused by the use of slightly different features and perhaps by differences in implementation.", "labels": [], "entities": []}, {"text": "In.2 we show the performance of the models M1, M2, and M3 on the development and test set.", "labels": [], "entities": []}, {"text": "We observe an unusual behavior for all three models: a model that performs good on the development set typically performs bad on the test set, and vice versa.", "labels": [], "entities": []}, {"text": "Furthermore, optimal cross-validated F 1 performance on the train set is 72%, which is 7 points above the best performance on the validation set.", "labels": [], "entities": [{"text": "F 1", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.8693711161613464}]}, {"text": "We believe this maybe indicative of significant differences in the distributions underlying the datasets.", "labels": [], "entities": []}, {"text": "To investigate this further, we applied the Kolmogorov-Smirnov two-sample goodness-of-fit test (K-S test)) for each of the used features to determine whether the train set is drawn from the same distribution as the development and test set.", "labels": [], "entities": [{"text": "goodness-of-fit test (K-S test))", "start_pos": 74, "end_pos": 106, "type": "METRIC", "confidence": 0.8748883306980133}]}, {"text": "The K-S testis a nonparametric test that determines whether two independent samples differ in some respect, both in the measure of locations (means, median) and the shapes of the distributions (skewness, dispersion, kurtosis).", "labels": [], "entities": []}, {"text": "The assumptions for the K-S test (independence of random samples and continuous variables) are met for all our features.", "labels": [], "entities": []}, {"text": "We tested all features at the level of significance of 0.05 and rejected the null hypothesis for all features but one (bigram overlap).", "labels": [], "entities": []}, {"text": "This confirms our initial assumption that the features in the train set are not identically distributed to those in the test set, bringing into question the representativeness of the test set.", "labels": [], "entities": []}, {"text": "Reasons for this may include different annotation sources (crowdsourcing vs experts) and differences in time periods of tweets.", "labels": [], "entities": []}, {"text": "Moreover, due to differences in the datasets, the performance is very much affected by the choice of the model optimization setup.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Official SemEval Task 1 evaluation.", "labels": [], "entities": [{"text": "SemEval Task 1", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.6230462590853373}]}, {"text": " Table 2: Model optimization using different datasets.", "labels": [], "entities": [{"text": "Model optimization", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8279661238193512}]}]}