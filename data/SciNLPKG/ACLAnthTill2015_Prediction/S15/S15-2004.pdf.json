{"title": [{"text": "HLTC-HKUST: A Neural Network Paraphrase Classifier using Translation Metrics, Semantic Roles and Lexical Similarity Features", "labels": [], "entities": [{"text": "HLTC-HKUST", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.934673547744751}]}], "abstractContent": [{"text": "This paper describes the system developed by our team (HLTC-HKUST) for task 1 of Se-mEval 2015 workshop about paraphrase classification and semantic similarity in Twitter.", "labels": [], "entities": [{"text": "paraphrase classification", "start_pos": 110, "end_pos": 135, "type": "TASK", "confidence": 0.8592385053634644}]}, {"text": "We trained a neural network classifier over a range of features that includes translation met-rics, lexical and syntactic similarity score and semantic features based on semantic roles.", "labels": [], "entities": []}, {"text": "The neural network was trained taking into consideration in the objective function the six different similarity levels provided in the corpus, in order to give as output a more fine-grained estimation of the similarity level of the two sentences, as required by subtask 2.", "labels": [], "entities": []}, {"text": "With an F-score of 0.651 in the binary paraphrase classification subtask 1, and a Pearson coefficient of 0.697 for the sentence similarity subtask 2, we achieved respectively the 6th place and the 3rd place, above the average of what obtained by the other contestants.", "labels": [], "entities": [{"text": "F-score", "start_pos": 8, "end_pos": 15, "type": "METRIC", "confidence": 0.9985382556915283}, {"text": "Pearson coefficient", "start_pos": 82, "end_pos": 101, "type": "METRIC", "confidence": 0.9887640178203583}]}], "introductionContent": [{"text": "Paraphrase identification is the problem to determine whether two sentences have the same meaning, and is the objective of the task 1 of SemEval 2015 workshop (.", "labels": [], "entities": [{"text": "Paraphrase identification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9289425611495972}, {"text": "SemEval 2015 workshop", "start_pos": 137, "end_pos": 158, "type": "TASK", "confidence": 0.6555881102879842}]}, {"text": "Conventionally this task has been mainly evaluated on the Microsoft Research Paraphrase corpus, which consists of pairs of sentences taken out from news headlines and articles.", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase corpus", "start_pos": 58, "end_pos": 94, "type": "DATASET", "confidence": 0.8809956908226013}]}, {"text": "News domain sentences are usually grammatically correct and of average to long length.", "labels": [], "entities": []}, {"text": "The current state-of-the-art method to our knowledge on this corpus ( trains an SVM over latent semantic vectors, lexical and syntactic similarity features.", "labels": [], "entities": []}, {"text": "Although their main objective was to show the effectiveness of a method based on latent semantic analysis, it is also evident that other features pertinent to different aspects of sentence similarity are able to boost the results.", "labels": [], "entities": []}, {"text": "Previously used a recursive autoencoder to similarly obtain a vector representation of each sentence, again combining other lexical similarity features to improve the results.", "labels": [], "entities": []}, {"text": "Other methods, such as or used instead a more traditional supervised classification approach over different sets of features and different classifiers, most of which improved previous results.", "labels": [], "entities": []}, {"text": "Task 1 of SemEval 2015 workshop required to evaluate paraphrases on anew corpus, consisting of sentences taken from Twitter posts ().", "labels": [], "entities": [{"text": "SemEval 2015 workshop", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8289582331975301}]}, {"text": "Twitter sentences notoriously differ from those taken from news articles: the 140 characters limit makes the sentences short, with few words, lots of different abbreviations; they also include many misspelled and invented words, and often lack a correct grammatical structure.", "labels": [], "entities": []}, {"text": "Another important difference is the sixlevel classification labels provided, compared to the binary labels of MSRP corpus, which allows a finegrained evaluation of the similarity level between the sentences.", "labels": [], "entities": [{"text": "MSRP corpus", "start_pos": 110, "end_pos": 121, "type": "DATASET", "confidence": 0.8476714193820953}]}, {"text": "The task was divided into two subtasks.", "labels": [], "entities": []}, {"text": "Subtask 1 was the classical binary paraphrase classification task, where given a pair of sentences the system had to identify if it is a paraphrase or not.", "labels": [], "entities": [{"text": "paraphrase classification task", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.8123543461163839}]}, {"text": "Subtask 2 instead required the system to provide a score in the range that measures the actual similarity level of the two sentences.", "labels": [], "entities": []}], "datasetContent": [{"text": "The neural network was setup with a hidden layer dimension of three times the input.", "labels": [], "entities": []}, {"text": "The development set was used to tune the L2 regularization coefficient, set at \u03b3 = 0.01, as well as the learning rate and the other hyperparameters, and to have a measure of improvement against the official thresholding baseline provided for the task.", "labels": [], "entities": []}, {"text": "To implement the neural network we used THEANO Python toolkit (.", "labels": [], "entities": [{"text": "THEANO Python toolkit", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.7773299217224121}]}, {"text": "We train the network with all the sentences provided in the training set.", "labels": [], "entities": []}, {"text": "The objective label of the cross-entropy objective function was set to 1.0 for pairs labeled (5, 0) and (4, 1), 0.75 for pairs labeled (3, 2), 0.5 for pairs labeled (2, 3) and 0.0 for pairs labeled (0, 5).", "labels": [], "entities": []}, {"text": "This choice allowed a more fine training for task 2, where a continuous similarity value must be estimated, without altering too much the behavior in the binary estimation task 1.", "labels": [], "entities": []}, {"text": "The training procedure was repeated several times, each time with a different random initialization of the weights and with a different random pair order.", "labels": [], "entities": []}, {"text": "In order to avoid overfitting, in each run the training was  stopped when the best results on the development set were obtained.", "labels": [], "entities": []}, {"text": "The final results were taken from the run that yielded the best accuracy, and in case of tie the best F1 score, on the development set for subtask 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9994713664054871}, {"text": "F1 score", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9824697375297546}]}, {"text": "Run 2 instead was an attempt to include latent semantic vectors obtained through the procedure described in and added to the network from an extra layer whose output was concatenated to the features input vector.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Result comparison between our method and the winners of subtask 1 and subtask 2.", "labels": [], "entities": []}]}