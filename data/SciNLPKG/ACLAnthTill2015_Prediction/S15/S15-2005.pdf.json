{"title": [{"text": "FBK-HLT: An Effective System for Paraphrase Identification and Semantic Similarity in Twitter", "labels": [], "entities": [{"text": "FBK-HLT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9093859791755676}, {"text": "Paraphrase Identification", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.8676758110523224}]}], "abstractContent": [{"text": "This paper reports the description and performance of our system, FBK-HLT, participating in the SemEval 2015, Task #1 \"Paraphrase and Semantic Similarity in Twitter\", for both sub-tasks.", "labels": [], "entities": [{"text": "FBK-HLT", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.934615969657898}, {"text": "SemEval 2015, Task #1", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.8490282992521921}]}, {"text": "We submitted two runs with different classifiers in combining typical features (lexi-cal similarity, string similarity, word n-grams, etc) with machine translation metrics and edit distance features.", "labels": [], "entities": []}, {"text": "We outperform the baseline system and achieve a very competitive result to the best system on the first subtask.", "labels": [], "entities": []}, {"text": "Eventually, we are ranked 4 th out of 18 teams participating in subtask \"Paraphrase Identification\".", "labels": [], "entities": [{"text": "Paraphrase Identification", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.8582171499729156}]}], "introductionContent": [{"text": "Paraphrase identification/recognition is an important task that can be used as a feature to improve many other NLP tasks as Information Retrieval, Machine Translation Evaluation, Text Summarization, Question and Answering, and others.", "labels": [], "entities": [{"text": "Paraphrase identification/recognition", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.9347313493490219}, {"text": "Information Retrieval", "start_pos": 124, "end_pos": 145, "type": "TASK", "confidence": 0.831714928150177}, {"text": "Machine Translation Evaluation", "start_pos": 147, "end_pos": 177, "type": "TASK", "confidence": 0.8840096990267435}, {"text": "Text Summarization", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.831105649471283}, {"text": "Question and Answering", "start_pos": 199, "end_pos": 221, "type": "TASK", "confidence": 0.7416889866193136}]}, {"text": "Besides this, analyzing social data like tweets of social network Twitter is afield of growing interest for different purposes.", "labels": [], "entities": []}, {"text": "The interesting combination of these two tasks was brought forward as Shared Task #1 in the SemEval 2015 campaign for \"Paraphrase and Semantic Similarity in Twitter\" (.", "labels": [], "entities": [{"text": "Paraphrase and Semantic Similarity in Twitter", "start_pos": 119, "end_pos": 164, "type": "TASK", "confidence": 0.7905275722344717}]}, {"text": "In this task, given a set of sentence pairs, which are not necessarily full tweets, their topic and the same sentences with partof-speech and named entity tags; participating system is required to predict for each pair of sentences is a paraphrase (Subtask 1) and optionally compute a graded score between 0 and 1 for their semantic equivalence (Subtask 2).", "labels": [], "entities": []}, {"text": "We participate in this shared task with a system combining different features using a binary classifier.", "labels": [], "entities": []}, {"text": "We are interested in finding out whether semantic similarity, textual entailment and machine translation evaluation techniques could increase the accuracy of our system.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.8280291755994161}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9988952279090881}]}, {"text": "This paper is organized as follows: Section 2 presents the System Description, Section 3 describes the Experiment Settings, Section 4 reports the Evaluations, Section 5 shows the Error Analysis, and finally Section 6 is the Conclusions and Future Work.", "labels": [], "entities": [{"text": "Error Analysis", "start_pos": 179, "end_pos": 193, "type": "METRIC", "confidence": 0.9368214905261993}]}], "datasetContent": [{"text": "Other than similarity features, we also use evaluation metrics for machine translation as suggested in () for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7255360931158066}, {"text": "paraphrase recognition", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.9362457692623138}, {"text": "Microsoft Research paraphrase corpus (MSRP)", "start_pos": 136, "end_pos": 179, "type": "DATASET", "confidence": 0.7800795989377158}]}, {"text": "In machine translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7907394170761108}]}, {"text": "We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system.", "labels": [], "entities": []}, {"text": "Thus, we use two metrics for word alignment in our system, the METEOR and BLEU.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7813120782375336}, {"text": "METEOR", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9747813940048218}, {"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9976134300231934}]}, {"text": "We actually also take into consideration the metric TERp (), but it does not make any improvement on system performance, hence, we exclude it.", "labels": [], "entities": [{"text": "TERp", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9778695702552795}]}, {"text": "Translation with Explicit ORdering) We use the latest version of METEOR) that find alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.7635635733604431}]}, {"text": "We used the system as distributed on its website, using only the \"norm\" option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.", "labels": [], "entities": []}, {"text": "We compute the word alignment scores on sentences and on sentences with part-of-speech and named entity tags, as our idea is that if two sentences are similar, their tagged version also should be similar.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.6445954144001007}]}, {"text": "We use another metric for machine translation BLEU () that is one of the most commonly used and because of that has an high reliability.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7367653846740723}, {"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9611364006996155}]}, {"text": "It is computed as the amount of n-gram overlap, for different values of n=1,2,3, and 4, between the system output and the reference translation, in our case between sentence pairs.", "labels": [], "entities": []}, {"text": "The score is tempered by a penalty for translations that might be too short.", "labels": [], "entities": []}, {"text": "BLEU relies on exact matching and has no concept of synonymy or paraphrasing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8124205470085144}]}, {"text": "For Subtask 1, we train two models with different feature settings using the VotedPerceptron and MultilayerPerception classification algorithms on the training dataset and we evaluate these models on the development dataset.", "labels": [], "entities": [{"text": "VotedPerceptron", "start_pos": 77, "end_pos": 92, "type": "DATASET", "confidence": 0.8539955615997314}]}, {"text": "Finally, we use the same models for the evaluation on the test dataset.", "labels": [], "entities": []}, {"text": "In table 1, we report the Accuracy results obtained by using different classifiers with different features.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9991635084152222}]}, {"text": "Our chosen classification algorithms outperform the baseline and EOP EditDistance (standalone setting).", "labels": [], "entities": [{"text": "EOP EditDistance", "start_pos": 65, "end_pos": 81, "type": "METRIC", "confidence": 0.7171410322189331}]}, {"text": "shows F1-score obtained with different classifiers on our best set of features, and our classification algorithms again perform much better the baseline and EOP EditDistance.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9991474151611328}, {"text": "EOP EditDistance", "start_pos": 157, "end_pos": 173, "type": "DATASET", "confidence": 0.7014431655406952}]}, {"text": "For Subtask 2, due to no training data is given for computing the semantic similarity, a different approach is needed.", "labels": [], "entities": []}, {"text": "We do not use a classifier, our similarity score is simply the average between ME-TEOR score and edit distance score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 32, "end_pos": 48, "type": "METRIC", "confidence": 0.9653764069080353}, {"text": "ME-TEOR score", "start_pos": 79, "end_pos": 92, "type": "METRIC", "confidence": 0.9713742434978485}, {"text": "edit distance score", "start_pos": 97, "end_pos": 116, "type": "METRIC", "confidence": 0.7887624104817709}]}, {"text": "We submit two runs using two models described in the Section 3 for both subtasks.", "labels": [], "entities": []}, {"text": "In the, we report the performance of our two runs against the baselines and best systems in each subtask.", "labels": [], "entities": []}, {"text": "In Subtask 1, our runs outperform all three baselines and achieve very competitive results to the best system ASOBEK.", "labels": [], "entities": [{"text": "ASOBEK", "start_pos": 110, "end_pos": 116, "type": "DATASET", "confidence": 0.5885463953018188}]}, {"text": "In the run FBK-HLT (voted) , we even achieve a better Precision than the best system.", "labels": [], "entities": [{"text": "FBK-HLT", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.8714560270309448}, {"text": "Precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9985706806182861}]}, {"text": "In Subtask 2, though we apply a simple computation method for semantic similarity by averaging the word alignment score and EditDistance, we still have better results than two our of three baselines.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7340755760669708}]}], "tableCaptions": [{"text": " Table 1: Accuracy obtained on development dataset using different classifiers with different features.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9951677322387695}]}, {"text": " Table 2: F1-score obtained using different classifiers on  the best set of features (baseline + METEOR + BLEU +  EditDistance).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993072748184204}, {"text": "METEOR", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9903193712234497}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.8638478517532349}, {"text": "EditDistance", "start_pos": 114, "end_pos": 126, "type": "METRIC", "confidence": 0.6834985613822937}]}, {"text": " Table 3: Paraphrase and Semantic Similarity Results.", "labels": [], "entities": []}]}