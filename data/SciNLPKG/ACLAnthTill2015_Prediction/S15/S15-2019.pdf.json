{"title": [{"text": "UMDuluth-BlueTeam : SVCSTS -A Multilingual and Chunk Level Semantic Similarity System", "labels": [], "entities": [{"text": "UMDuluth-BlueTeam", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9176482558250427}]}], "abstractContent": [{"text": "This paper describes SVCSTS, a system that was submitted in SemEval-2015 Task 2: Semantic Textual Similarity(STS)(Agirre et al., 2015).", "labels": [], "entities": [{"text": "SemEval-2015 Task 2: Semantic Textual Similarity(STS)(Agirre et al., 2015)", "start_pos": 60, "end_pos": 134, "type": "TASK", "confidence": 0.7836029976606369}]}, {"text": "The task has 3 subtasks viz., English STS, Spanish STS and Interpretable STS.", "labels": [], "entities": []}, {"text": "SVCSTS uses Monolingual word aligner (Sul-tan et al., May 2014), supervised machine learning, Google and Bing translator API's.", "labels": [], "entities": [{"text": "SVCSTS", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8860012292861938}]}, {"text": "Various runs of the system outperformed all other participating systems in Interpretable STS for non-chunked sentence input.", "labels": [], "entities": [{"text": "Interpretable STS", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7236638069152832}]}], "introductionContent": [{"text": "Semantic Textual Similarity gives a quantifier to evaluate semantic equivalence between two sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.741665780544281}]}, {"text": "Earlier SemEval tasks (,,) focused on finding the semantic equivalence between sentences in English and Spanish.", "labels": [], "entities": []}, {"text": "A new pilot task was introduced this year to find which parts (chunks) of the sentences are equivalent in meaning.", "labels": [], "entities": []}, {"text": "SVCSTS is an extension to () and it handles both Spanish STS and Interpretable STS.", "labels": [], "entities": [{"text": "SVCSTS", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.924406886100769}, {"text": "Spanish STS", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.7595203220844269}]}, {"text": "SVCSTS uses Monolingual word aligner (Sultan et al.,), supervised machine learning techniques, Google and Bing translator API's.", "labels": [], "entities": [{"text": "SVCSTS", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8822841048240662}]}, {"text": "Section 2 describes a brief overview of SVCSTS's approach for various subtasks.", "labels": [], "entities": []}, {"text": "Section 3 outlines the performance of SVCSTS in various subtasks of SemEval 2015 Task-2.", "labels": [], "entities": [{"text": "SVCSTS", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.6707863807678223}, {"text": "SemEval 2015 Task-2", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7424766023953756}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Avg. alignment type scores", "labels": [], "entities": [{"text": "Avg. alignment type", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.4593183323740959}]}, {"text": " Table 3: Scores for English STS", "labels": [], "entities": []}, {"text": " Table 4: Scores for Spanish STS", "labels": [], "entities": []}, {"text": " Table 3. SVCSTS was ranked 14th among 73 runs.  The results of Spanish STS are shown in", "labels": [], "entities": [{"text": "SVCSTS", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.8297249674797058}, {"text": "Spanish STS", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.8815630078315735}]}, {"text": " Table 5: Scores for Interpretable STS (Chunked Input)", "labels": [], "entities": []}]}