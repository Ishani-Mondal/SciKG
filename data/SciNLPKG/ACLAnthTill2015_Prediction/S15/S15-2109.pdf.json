{"title": [{"text": "INESC-ID: Sentiment Analysis without hand-coded Features or Liguistic Resources using Embedding Subspaces", "labels": [], "entities": [{"text": "INESC-ID", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8328938484191895}, {"text": "Sentiment Analysis", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9650719165802002}]}], "abstractContent": [{"text": "We present the INESC-ID system for the message polarity classification task of SemEval 2015.", "labels": [], "entities": [{"text": "INESC-ID", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.6322759985923767}, {"text": "message polarity classification task", "start_pos": 39, "end_pos": 75, "type": "TASK", "confidence": 0.9085035920143127}]}, {"text": "The proposed system does not make use of any hand-coded features or linguistic resources.", "labels": [], "entities": []}, {"text": "It relies on projecting pre-trained structured skip-gram word embeddings into a small subspace.", "labels": [], "entities": []}, {"text": "The word embeddings can be obtained from large amounts of Twitter data in unsupervised form.", "labels": [], "entities": []}, {"text": "The sentiment analysis supervised training is thus reduced to finding the optimal projection which can be carried out efficiently despite the little data available.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.930439293384552}]}, {"text": "We analyze in detail the proposed approach and show that a competitive system can be attained with only a few configuration parameters .", "labels": [], "entities": []}], "introductionContent": [{"text": "Web-based social networks area rich data source for both businesses and academia.", "labels": [], "entities": []}, {"text": "However, the sheer volume, diversity and rate of creation of social media, imposes the need for automated analysis tools.", "labels": [], "entities": []}, {"text": "The growing interest in this problem motivated the creation of a shared task for Twitter Sentiment Analysis ().", "labels": [], "entities": [{"text": "Twitter Sentiment Analysis", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.654471625884374}]}, {"text": "The Message Polarity Classification task consists in classifying a message as positive, negative, or neutral in sentiment.", "labels": [], "entities": [{"text": "Message Polarity Classification", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8191136519114176}]}, {"text": "A great deal of research has been done on methods for sentiment analysis on user generated content.", "labels": [], "entities": [{"text": "sentiment analysis on user generated content", "start_pos": 54, "end_pos": 98, "type": "TASK", "confidence": 0.8761772612730662}]}, {"text": "However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning.", "labels": [], "entities": []}, {"text": "Indeed, if we look at the best performing systems from SemEval 2014 (), (), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.6945030987262726}]}, {"text": "In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (.", "labels": [], "entities": [{"text": "SemEval message polarity classification task", "start_pos": 58, "end_pos": 102, "type": "TASK", "confidence": 0.8997889161109924}]}, {"text": "The system is able to learn good message representations for message polarity classification directly from raw text with a simple tokenization scheme.", "labels": [], "entities": [{"text": "message polarity classification", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.8390873074531555}]}, {"text": "Our approach is based on using large amounts of unlabeled data to induce word embeddings, that is, continuous word representations containing contextual information.", "labels": [], "entities": []}, {"text": "Instead of using these word embeddings directly with, for instance, a logistic regression classifier, we estimate a sentiment subspace of the embeddings.", "labels": [], "entities": []}, {"text": "The idea is to find a projection of the embedding space that is meaningful for the supervised task.", "labels": [], "entities": []}, {"text": "In the proposed model, we jointly learn the sentiment subspace projection and the classifier using the SemEval training data.", "labels": [], "entities": [{"text": "sentiment subspace projection", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.7234721183776855}, {"text": "SemEval training data", "start_pos": 103, "end_pos": 124, "type": "DATASET", "confidence": 0.6544986168543497}]}, {"text": "The resulting system attains state-of-theart performance without hand-coded features or linguistic resources and only a few configuration parameters.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Avg. F-measure on SemEval development and  test sets varying with embedding size e. Sub-space size  s = 10. Best model per column in bold.", "labels": [], "entities": [{"text": "Avg", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9766449928283691}, {"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.6837452054023743}]}, {"text": " Table 2: Avg. F-measure on SemEval test sets varying  with embedding sub-space size s. Embedding size e =  600. Best model per column in bold.", "labels": [], "entities": [{"text": "Avg", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9882093667984009}, {"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.7527288794517517}, {"text": "SemEval test sets", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.8019147713979086}]}, {"text": " Table 3.  The results on 2015, confirm the sensibility analy- sis of e and s. The high performance of the e = 600,  s = 10 model on the 2015 dataset was however un- expected, since it tops the submitted system by more  than a 1% absolute. The second model selected, us- ing a smaller e size displayed a performance compa- rable to that of the submitted system thus showing  the overall robustness of the approach.", "labels": [], "entities": []}, {"text": " Table 3: Avg. F-measure of the submitted system (top)  and posteriorly selected candidates (bottom). Best model  per column in bold.", "labels": [], "entities": [{"text": "Avg", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9971219897270203}, {"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.6697946786880493}]}]}