{"title": [{"text": "UWM: A Simple Baseline Method for Identifying Attributes of Disease and Disorder Mentions in Clinical Text", "labels": [], "entities": [{"text": "UWM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9234257340431213}, {"text": "Identifying Attributes of Disease and Disorder Mentions", "start_pos": 34, "end_pos": 89, "type": "TASK", "confidence": 0.9143191831452506}]}], "abstractContent": [{"text": "In this paper the system that was developed by Team UWM for the Task 14 of SemEval 2015 competition is described.", "labels": [], "entities": [{"text": "UWM", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.8185915946960449}, {"text": "SemEval 2015 competition", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.7098899682362875}]}, {"text": "Task 14 included two tasks: Task 1 was identification of disorder mentions and their normalization, and Task 2 was identification of the following attributes for disorder mentions: the CUI of the disorder, negation indicator, subject, uncertainty indicator, course, severity, conditional, generic indicator, and body location.", "labels": [], "entities": [{"text": "CUI", "start_pos": 185, "end_pos": 188, "type": "METRIC", "confidence": 0.929075300693512}]}, {"text": "For Task 1, an earlier system was applied that uses Conditional Random Fields (CRFs) for disorder recognition and learned edit distance patterns for normalization.", "labels": [], "entities": [{"text": "disorder recognition", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.6954293847084045}]}, {"text": "Task 2 was implemented by a simple method that finds the attribute terms around the disease mentions by matching them in the training data.", "labels": [], "entities": []}, {"text": "Among all participants Team UWM was ranked fourth in Task 1, fourth in Task 2A (over gold-standard mentions) and third in Task 2B (over extracted mentions).", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated extraction tools are crucial for managing huge amount of clinical texts.", "labels": [], "entities": [{"text": "Automated extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7092974483966827}]}, {"text": "These tools have the potential to enable many automated applications in healthcare.", "labels": [], "entities": []}, {"text": "The Task 14 of SemEval 2015 was designed to serve as a platform for evaluating one such extraction tool.", "labels": [], "entities": []}, {"text": "Its Task 1 involved extracting and normalizing disorder mentions from clinical text and its Task 2 involved assertion identification for the mentions.", "labels": [], "entities": [{"text": "extracting and normalizing disorder mentions from clinical text", "start_pos": 20, "end_pos": 83, "type": "TASK", "confidence": 0.7758601978421211}, {"text": "assertion identification", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.9580669701099396}]}, {"text": "Task 1 is challenging because there is a lot of variability in which diseases and disorders are mentioned in clinical text and hence a pre-defined list of mentions is not sufficient to extract them.", "labels": [], "entities": []}, {"text": "The task also required normalizing the extracted mentions by mapping them to UMLS CUIs if they exist in the SNOMED-CT part of UMLS and are marked as disease/disorder, otherwise they were to be declared as \"CUI-less.\"", "labels": [], "entities": []}, {"text": "This normalization process is also challenging because disorder names are frequently mentioned in modified forms which prevents them from exactly matching the concepts in UMLS.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 171, "end_pos": 175, "type": "DATASET", "confidence": 0.9247511029243469}]}, {"text": "Task 2 required finding certain attributes for the mentions and finding the spans of these attributes in text.", "labels": [], "entities": []}, {"text": "This task is also challenging due to the variability in which attributes are attributed to disease and disorder mentions in clinical text.", "labels": [], "entities": []}, {"text": "Our team, UWM, participated in both Task 1 and Task 2.", "labels": [], "entities": [{"text": "UWM", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.9386045336723328}]}, {"text": "For Task 1, we used the same system that we had previously developed for the.", "labels": [], "entities": []}, {"text": "For Task 2, we used a simple method that finds attributes of mentions by first collecting lists of attribute terms from the training data and then matching in this list.", "labels": [], "entities": []}, {"text": "The nearest attribute terms to a mention are assigned to that mention.", "labels": [], "entities": []}, {"text": "The attribute terms are normalized by finding their normalized values in the training data.", "labels": [], "entities": []}, {"text": "Despite being simple, this method gave competitive results.", "labels": [], "entities": []}, {"text": "The methods used in this paper are described in more details in the next section.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of attribute terms for each attribute  type in the training data.", "labels": [], "entities": []}, {"text": " Table 2: Results of Task1 (mention extraction and  normalization).", "labels": [], "entities": [{"text": "mention extraction", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7550556361675262}]}, {"text": " Table 3: Results of Task 2A.", "labels": [], "entities": []}, {"text": " Table 4: Results of Task 2B.", "labels": [], "entities": []}, {"text": " Table 5: Accuracy for each attribute type in Task 2A  and Task 2B.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9979181885719299}]}]}