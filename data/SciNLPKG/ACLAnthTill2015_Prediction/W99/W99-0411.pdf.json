{"title": [{"text": "Automated Essay Scoring for Nonnative English Speakers", "labels": [], "entities": [{"text": "Automated Essay Scoring", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7495947380860647}]}], "abstractContent": [{"text": "The e-rater system TM ~ is an operational automated essay scoring system, developed at Educational Testing Service (ETS).", "labels": [], "entities": [{"text": "Educational Testing Service (ETS)", "start_pos": 87, "end_pos": 120, "type": "DATASET", "confidence": 0.8498076697190603}]}, {"text": "The average agreement between human readers, and between independent human readers and e-rater is approximately 92%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9824057817459106}]}, {"text": "There is much interest in the larger writing community in examining the system's performance on nonnative speaker essays.", "labels": [], "entities": []}, {"text": "This paper focuses on results of a study that show e-rater's performance on Test of Written English (TWE) essay responses written by nonnative English speakers whose native language is Chinese, Arabic, or Spanish.", "labels": [], "entities": [{"text": "Test of Written English (TWE) essay responses written by nonnative English speakers whose native language", "start_pos": 76, "end_pos": 181, "type": "TASK", "confidence": 0.7001135717420017}]}, {"text": "In addition, one small sample of the data is from US-born English speakers, and another is from non-US-born candidates who report that their native language is English.", "labels": [], "entities": []}, {"text": "As expected, significant differences were found among the scores of the English groups and the nonnative speakers.", "labels": [], "entities": []}, {"text": "While there were also differences between e-rater and the human readers for the various language groups, the average agreement rate was as high as operational agreement.", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 117, "end_pos": 131, "type": "METRIC", "confidence": 0.8881650567054749}]}, {"text": "At least four of the five features that are included in e-rater's current operational models (including discourse, topical, and syntactic features) also appear in the TWE models.", "labels": [], "entities": []}, {"text": "This suggests that the features generalize well over a wide range of linguistic variation, as e-rater was not 1 The e-rater system TM is a trademark of Educational Testing Service.", "labels": [], "entities": [{"text": "Educational Testing Service", "start_pos": 152, "end_pos": 179, "type": "DATASET", "confidence": 0.8839767972628275}]}, {"text": "In the paper, we will refer to the e-rater system TM as e-rater.", "labels": [], "entities": []}, {"text": "confounded by non-standard English syntactic structures or stylistic discourse structures which one might expect to be a problem fora system designed to evaluate native speaker writing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research and development in automated essay scoring has begun to flourish in the past five years or so, bringing about a whole new field of interest to the NLP community,,,).", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.7397664785385132}]}, {"text": "Research at Educational Testing Service (ETS) has led to the recent development of e-rater, an operational automated essay scoring system.", "labels": [], "entities": [{"text": "Educational Testing Service (ETS)", "start_pos": 12, "end_pos": 45, "type": "DATASET", "confidence": 0.8184502472480139}]}, {"text": "E-rater is based on features in holistic scoring guides for human reader scoring.", "labels": [], "entities": [{"text": "E-rater", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.743579089641571}, {"text": "human reader scoring", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.5846365590890249}]}, {"text": "Scoring guides have a 6-point score scale.", "labels": [], "entities": []}, {"text": "Six's are assigned to the \"best\" essays, and \"l's\" to the least well-written.", "labels": [], "entities": []}, {"text": "Scoring guide criteria are based on structural (syntax and discourse) and vocabulary usage in essay responses (see http://www.gmat.org).", "labels": [], "entities": []}, {"text": "E-rater builds new models for each topic (prompt-specific models) by evaluating approximately 52 syntactic, discourse and topical analysis variables for 270 human reader scored training essays.", "labels": [], "entities": []}, {"text": "Relevant features for each model are based on the predictive feature set identified by a stepwise linear regression.", "labels": [], "entities": []}, {"text": "In operational scoring, when compared to a human reader, e-rater assigns an exactly matching or adjacent score (on the 6-point scale) about 92% of the time.", "labels": [], "entities": [{"text": "operational scoring", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8474718928337097}]}, {"text": "This is the same as the agreement rate typically found between two human readers.", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 24, "end_pos": 38, "type": "METRIC", "confidence": 0.9807917773723602}]}, {"text": "Correlations between erater scores and those of a single human reader are about .73; correlations between two human readers are .75.", "labels": [], "entities": []}, {"text": "The scoring guide criteria assume standard written English.", "labels": [], "entities": []}, {"text": "Non-standard English may show up in the writing of native English speakers of non-standard dialects.", "labels": [], "entities": []}, {"text": "For general NLP research purposes, it is useful to have computer-based corpora that represent language variation).", "labels": [], "entities": []}, {"text": "Such corpora allow us to explore issues with regard to how the system will handle responses that might be written in nonstandard English.", "labels": [], "entities": []}, {"text": "Current research at ETS for the Graduate Record Examination (GRE)) is making use of essay corpora that represent subgroups where variations in standard written English might be found, such as in the writing of African Americans, Latinos and Asians and).", "labels": [], "entities": [{"text": "ETS", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.9312556982040405}, {"text": "Graduate Record Examination (GRE))", "start_pos": 32, "end_pos": 66, "type": "DATASET", "confidence": 0.7389754056930542}]}, {"text": "In addition, ETS is accumulating essay corpora of nonnative speakers that can be used for research.", "labels": [], "entities": []}, {"text": "This paper focuses on preliminary data that show e-rater's performance on Test of Written English (TWE) essay responses written by nonnative English speakers whose native language is Chinese, Arabic, or Spanish.", "labels": [], "entities": [{"text": "Test of Written English (TWE) essay responses written by nonnative English speakers whose native language", "start_pos": 74, "end_pos": 179, "type": "TASK", "confidence": 0.70190190392382}]}, {"text": "A small sample of the data is from US-born English speakers and a second small sample is from non-US-born candidates who report that their native language is English.", "labels": [], "entities": []}, {"text": "The data were originally collected fora study by in which analyses of the essays are also discussed.", "labels": [], "entities": []}, {"text": "The current work is only the beginning of a program of research at ETS that will examine automated scoring for nonnative English speakers.", "labels": [], "entities": [{"text": "ETS", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9596157670021057}]}, {"text": "Overall goals include determining how features used in automated scoring may also be used to (a) examine the difficulty of an essay question for speakers of particular language groups, and (b) automatically formulate diagnostics and instruction for nonnative English speakers, with customization for different language groups.", "labels": [], "entities": [{"text": "formulate diagnostics", "start_pos": 207, "end_pos": 228, "type": "TASK", "confidence": 0.8537498712539673}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of Human Readers Final Score (GDF) & e-rater Score (E) Over All  Language Groups in TWE1 and TWE2", "labels": [], "entities": [{"text": "Comparison of Human Readers Final Score (GDF)", "start_pos": 10, "end_pos": 55, "type": "METRIC", "confidence": 0.6143782701757219}, {"text": "e-rater Score (E)", "start_pos": 58, "end_pos": 75, "type": "METRIC", "confidence": 0.8964250445365906}, {"text": "TWE1", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.9573023915290833}, {"text": "TWE2", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.5985187292098999}]}, {"text": " Table 2: Comparison of Human Readers Final Score (GDF) & e.rater Score (E) By  Language Groups in TWE1", "labels": [], "entities": [{"text": "Comparison of Human Readers Final Score (GDF)", "start_pos": 10, "end_pos": 55, "type": "METRIC", "confidence": 0.6265462670061324}, {"text": "Score (E)", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.6821775212883949}, {"text": "TWE1", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.8943802714347839}]}, {"text": " Table 3: Comparison of Human Readers Final Score (GDF) & e.rater Score (E) By  Language Groups in TWE2", "labels": [], "entities": [{"text": "Comparison of Human Readers Final Score (GDF)", "start_pos": 10, "end_pos": 55, "type": "METRIC", "confidence": 0.6297853059238858}, {"text": "Score (E)", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.6888715922832489}, {"text": "TWE2", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.889055073261261}]}]}