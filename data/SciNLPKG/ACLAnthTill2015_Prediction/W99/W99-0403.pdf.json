{"title": [{"text": "SPEECH COMPARISON IN The Rosetta Stone rM", "labels": [], "entities": [{"text": "SPEECH COMPARISON IN The Rosetta Stone rM", "start_pos": 0, "end_pos": 41, "type": "DATASET", "confidence": 0.6102518907615117}]}], "abstractContent": [{"text": "The Rosetta Stone TM is a successful CD-ROM based interactive program for teaching foreign languages, that uses speech comparison to help students improve their pronunciation.", "labels": [], "entities": [{"text": "Rosetta Stone TM", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8681756854057312}, {"text": "speech comparison", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7097999453544617}]}, {"text": "The input to a speech comparison system is N+I digitised utterances.", "labels": [], "entities": []}, {"text": "The output is a measure of the similarity of the last utterance to each of the N others.", "labels": [], "entities": []}, {"text": "Which language is being spoken is irrelevant.", "labels": [], "entities": []}, {"text": "This differs from classical speech recognition where the input data includes but one utterance, a set of expectations tuned to the particular language in use (typically digraphs or similar), and a grammar of expected words or phrases, and the output is recognition in the utterance of one of the phrases in the grammar (or rejection).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7906101047992706}]}, {"text": "This paper describes a speech comparison system and its application in The Rosetta Stone TM.", "labels": [], "entities": [{"text": "speech comparison", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.6864164173603058}, {"text": "The Rosetta Stone TM", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.9334233850240707}]}], "introductionContent": [{"text": "Funding for this research came from the developers 1, of The Rosetta Stone TM (TRS), a highly successful interactive multimedia program for teaching foreign languages.", "labels": [], "entities": [{"text": "The Rosetta Stone TM (TRS)", "start_pos": 57, "end_pos": 83, "type": "DATASET", "confidence": 0.8126853150980813}]}, {"text": "The developers wanted to use speech recognition technology to help students of foreign languages improve their pronunciation and their active vocabulary.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7024213373661041}]}, {"text": "As of this writing TRS is available in twenty languages, which was part of the motivation to develop a language independent approach to speech recognition.", "labels": [], "entities": [{"text": "TRS", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8747817277908325}, {"text": "speech recognition", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7880510091781616}]}, {"text": "Classical approaches require extensive development per language.", "labels": [], "entities": []}, {"text": "1 FLT, 165 South Main St., Harrisonburg, VA 22801.", "labels": [], "entities": [{"text": "FLT, 165 South Main St., Harrisonburg, VA 22801", "start_pos": 2, "end_pos": 49, "type": "DATASET", "confidence": 0.6977156211029399}]}, {"text": "540-432-6166 www.trstone.com TRS provides an immersion experience, where images, movies and sounds are used to build knowledge of a language from scratch.", "labels": [], "entities": [{"text": "TRS", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.677387535572052}]}, {"text": "Since there is no concession to the native language of the learner, a German speaker and a Korean speaker both learning Vietnamese have the same experience--all in Vietnamese.", "labels": [], "entities": []}, {"text": "The most recent release of TRS includes EAR, the speech comparison system described in this paper.", "labels": [], "entities": [{"text": "EAR", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9712947010993958}, {"text": "speech comparison", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.6731935143470764}]}, {"text": "The input to a speech comparison system is N+I digitized utterances--in the case of TRS, that includes N utterances by native speakers recorded in a studio with quality microphones, and one utterance by a student recorded in a sometimes very noisy environment with a builtin or handheld microphone.", "labels": [], "entities": [{"text": "speech comparison", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.672276109457016}]}, {"text": "The output is a measure of the similarity of the last utterance to each of the N others.", "labels": [], "entities": []}, {"text": "Which language is being spoken is irrelevant.", "labels": [], "entities": []}, {"text": "Speech comparison differs from classical speech recognition, where the input data includes one utterance, a set of expectations tuned to the particular language in use (typically digraphs or similar), and a grammar of expected words or phrases, and the output is recognition of the utterance as one of the phrases in the grammar, or rejection.", "labels": [], "entities": [{"text": "Speech comparison", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.792566567659378}, {"text": "speech recognition", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.8292424380779266}]}, {"text": "The TRS CD-ROM contains tens of thousands of utterances by native speakers.", "labels": [], "entities": [{"text": "TRS CD-ROM", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9161974489688873}]}, {"text": "Thus the TRS data set already included the necessary input for speech comparison, but not for classical speech recognition.", "labels": [], "entities": [{"text": "TRS data set", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.8609090646107992}, {"text": "speech comparison", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7558452486991882}, {"text": "speech recognition", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.758408933877945}]}, {"text": "The first application we developed was a pronunciation guide (see).", "labels": [], "entities": []}, {"text": "The user clicks on a picture, hears a native speaker's utterance, attempts to mimic that utterance, sees a display of two images visually portraying the two utterances, and observes a gauge which shows a measure of the similarity between the two utterances.", "labels": [], "entities": []}, {"text": "The system normalizes both voices (native speaker's and student's) to a", "labels": [], "entities": []}], "datasetContent": [{"text": "Clicking on an image brings up the speech comparison panel, seen here imposed over the lower two images.", "labels": [], "entities": [{"text": "speech comparison", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7282189130783081}]}, {"text": "The upper half of this panel displays a visualization of the native speaker's phrase describing the image.", "labels": [], "entities": []}, {"text": "The student then attempts to mimic the pronunciation of the native speaker.", "labels": [], "entities": []}, {"text": "The visualization of the student's utterance is displayed in real time.", "labels": [], "entities": []}, {"text": "Each visualization includes pitch (the fine line at the top), emphasis (the line varying in thickness) and an image of highly processed spectral information of the normalized voice.", "labels": [], "entities": []}, {"text": "The meter to the right gives an evaluation.", "labels": [], "entities": []}, {"text": "common standard, and displays various abstract or at least highly processed features of the normalized voices, so that differences irrelevant to speech (such as how deep your voice is, or the frequency response curve of the microphone) hopefully do not play a role.", "labels": [], "entities": []}, {"text": "The second application, currently underdevelopment, is active vocabulary building.", "labels": [], "entities": [{"text": "vocabulary building", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8959676623344421}]}, {"text": "The user sees four pictures and hears four phrases semantically related to the pictures.", "labels": [], "entities": []}, {"text": "This is material they have already worked over in other learning modes designed to build passive vocabulary, i.e. the ability to recognize the meaning of speech.", "labels": [], "entities": []}, {"text": "However in this exercise the user must be able to generate the speech with less prompting.", "labels": [], "entities": []}, {"text": "The order of the pictures is scrambled, and they are flashed one at a time.", "labels": [], "entities": []}, {"text": "The user must respond to each with the phrase that was given for that picture.", "labels": [], "entities": []}, {"text": "The system evaluates their success, i.e. whether they responded with the correct phrase, one of the other phrases, or some unrelated utterance.", "labels": [], "entities": []}, {"text": "One difficulty for the system is that frequently the four phrases are very similar, so that the difference between them might hinge on a short piece in the middle of otherwise nearly identical utterances (for example \"the girl is cutting the blue paper\", \"the girl is cutting the red paper\").", "labels": [], "entities": []}, {"text": "EAR is written in C.", "labels": [], "entities": []}, {"text": "Since TRS is written in MacroMedia Director TM, EAR is interfaced to TRS using Director's interface for extending Director with C code.", "labels": [], "entities": [{"text": "EAR", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.7178882360458374}]}, {"text": "TRS is multithreaded, so EAR is able to do its work incrementally since it must not take the CPU for extended periods of time.", "labels": [], "entities": [{"text": "TRS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7949835062026978}, {"text": "EAR", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.6006506681442261}]}, {"text": "Indeed EAR itself contains multiple threads of two kinds: description threads and comparison threads.", "labels": [], "entities": []}, {"text": "Since the system might load several prerecorded utterances of native speakers at once, it is desirable that the work of computing the normalized high-level description of each utterance be done while the user is listening to those utterances, in parallel.", "labels": [], "entities": []}, {"text": "Thus each stream of sound data (22050 Hz sound samples) is analyzed by a separate description thread, with a visual display in real time being an option.", "labels": [], "entities": []}, {"text": "Similarly, sound data from the microphone is analyzed in real time while the student is speaking by a description thread, and the resulting visual display is displayed in real time.", "labels": [], "entities": []}, {"text": "Description threads are discussed in Section 1.", "labels": [], "entities": []}, {"text": "Once the user has finished speaking, a comparison thread can be launched for each of the native speaker descriptions, which compare those descriptions to the description of the student's utterance.", "labels": [], "entities": []}, {"text": "Comparison threads are discussed in Section 2.", "labels": [], "entities": []}], "tableCaptions": []}