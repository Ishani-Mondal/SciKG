{"title": [], "abstractContent": [{"text": "Preface The general purpose of this symposium is to strengthen collaboration between researchers and users of language learning tools, by fostering common applied and theoretical interests in the communities of two independent conferences held at the University of Maryland: the 38 ~ annual meeting of the Association for Computational Linguistics (ACL-99) and the biennial meeting of the International Association of Language Learning Technologies (IALL-99).", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Both learners and systems can be evaluated in terms of competence on various tasks (including oral and written production and comprehension) and statistical standards developed for rating competence.", "labels": [], "entities": []}, {"text": "Some standards are better developed in certain areas than others: parsing standards, for example, are much better articulated in the NLP literature than numerical assessments of grammatical competence for language learners.", "labels": [], "entities": [{"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9637683629989624}]}, {"text": "Conversely, pronunciation assessment in learners (and what possible errors to look for, based on contrastive analysis between mother and target languages) is more well-developed than standards for speech synthesis and recognition at the level of phonemes and words.", "labels": [], "entities": [{"text": "pronunciation assessment", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.855795830488205}, {"text": "speech synthesis", "start_pos": 197, "end_pos": 213, "type": "TASK", "confidence": 0.7438993752002716}]}, {"text": "Robust NLP systems can be evaluated on the explicit models of languages they require.", "labels": [], "entities": []}, {"text": "The coverage of the system can be measured by the size of the components: how big is the lexicon, how many constructions is the grammar designed to cover, how general is the discourse strategy encoded in the system, etc..", "labels": [], "entities": []}, {"text": "These components are not amenable to inspection inhuman language assessment.", "labels": [], "entities": []}, {"text": "In addition, task-based evaluation maybe conducted without regard for the affective components, such as fatigue, cultural norms, politeness, etc.", "labels": [], "entities": []}, {"text": "In contrast, human language assessment is inseparable from such considerations.", "labels": [], "entities": [{"text": "human language assessment", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.7221132715543112}]}, {"text": "Even if the teaching/assessment process could be effectively modeled (passing some version of a Turing test), psychometric evalutation may reveal variation due solely to whether the student is interacting with a computer or a person.", "labels": [], "entities": []}, {"text": "Both oral and text assessment task maybe subdivided into test creation, administration, and scoring.", "labels": [], "entities": [{"text": "test creation", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.727138102054596}]}, {"text": "With respect to oral language, Fairfield outlines a method using Rosetta Stone TM software for automatic test creation and administration.", "labels": [], "entities": [{"text": "Rosetta Stone TM software", "start_pos": 65, "end_pos": 90, "type": "DATASET", "confidence": 0.8546076714992523}, {"text": "automatic test creation and administration", "start_pos": 95, "end_pos": 137, "type": "TASK", "confidence": 0.6373451113700866}]}, {"text": "Scoring is implicit: students assess their own speech, using automatically generated feedback.", "labels": [], "entities": [{"text": "Scoring", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9494652152061462}]}, {"text": "Malabonga and Kenyon describe an oral assessment for Spanish language learners, with systems underdevelopment for Arabic and Mandarin Chinese.", "labels": [], "entities": []}, {"text": "The test items were created and will be assessed manually; however, each testis compiled interactively in collaboration with the student, a design intended to reduce the examiness affective reaction to a computer-administered exam.", "labels": [], "entities": []}, {"text": "Levow and Olsen discuss what is involved in automating the sconng of such a test, further dividing evaluation into a process (how it is done) and a result (assigning a particular score).", "labels": [], "entities": []}, {"text": "In principle, computer-generated speech could be assessed using one or more of the paradigms above.", "labels": [], "entities": []}, {"text": "Such an assessment could provide a standard of evaluation for speech generation.", "labels": [], "entities": [{"text": "speech generation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7868382632732391}]}, {"text": "Decrozant and Voss propose using the same standards and system for evaluating human and machine-generated text.", "labels": [], "entities": []}, {"text": "They evaluate English-speaking learners of French and an EnglishFrench machine translation system on spatial expressions, based on contrastive analysis of the languages.", "labels": [], "entities": [{"text": "EnglishFrench machine translation", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.7331822713216146}]}, {"text": "Lenci, Montemagni, Pirrelli and Sofia similarly suggest that a single annotation scheme can be used to permit comparison and evaluation of parsers that take different languages as input, and in either oral or written form.", "labels": [], "entities": []}, {"text": "Fairon describes a European Union project to create and administer written language tests automatically from manually crafted test items and language models.", "labels": [], "entities": [{"text": "Fairon", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9333414435386658}]}, {"text": "The paper focuses on native French; Italian and German test projects are also underdevelopment.", "labels": [], "entities": []}, {"text": "Burstein and Chodorow discuss automatic assessment of English tests that have been manually created and administered.", "labels": [], "entities": []}, {"text": "They outline a method for scoring the exams electronically, identifying lexical, syntactic and discourse features.", "labels": [], "entities": []}, {"text": "It is our hope that this symposium provides answers to what it means for both people and systems to 'know' and 'evaluate' language, by highlighting the benefits in cross-fertilization of the language learning and natural language processing fields.", "labels": [], "entities": []}], "tableCaptions": []}