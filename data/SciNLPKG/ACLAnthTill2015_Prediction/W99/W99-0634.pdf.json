{"title": [{"text": "Corpus-Based Learning for Noun Phrase Coreference Resolution", "labels": [], "entities": [{"text": "Noun Phrase Coreference Resolution", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.7950391098856926}]}], "abstractContent": [{"text": "In this paper, we present a learning approach for coreference resolution of noun phrases in unrestricted text.", "labels": [], "entities": [{"text": "coreference resolution of noun phrases in unrestricted text", "start_pos": 50, "end_pos": 109, "type": "TASK", "confidence": 0.8964683338999748}]}, {"text": "The approach learns from a small, annotated corpus and the task includes resolving not just pronouns but rather general noun phrases.", "labels": [], "entities": []}, {"text": "In contrast to previous work, we attempt to evaluate our approach on a common data set, the MUC-6 coreference corpus.", "labels": [], "entities": [{"text": "MUC-6 coreference corpus", "start_pos": 92, "end_pos": 116, "type": "DATASET", "confidence": 0.8802273472150167}]}, {"text": "We obtained encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to non-learning approaches.", "labels": [], "entities": [{"text": "general noun phrase coreference task", "start_pos": 56, "end_pos": 92, "type": "TASK", "confidence": 0.6475114405155182}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9991661310195923}]}], "introductionContent": [{"text": "Coreference resolution refers to the process of determining if two expressions in natural language refer to the same entity in the world.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9082415997982025}]}, {"text": "It is an important subtask in natural language processing systems.", "labels": [], "entities": []}, {"text": "In particular, information extraction (IE) systems like those builtin the DAI:tPA Message Understanding Conferences have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since MUC-6.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.8491051077842713}, {"text": "DAI:tPA Message Understanding Conferences", "start_pos": 74, "end_pos": 115, "type": "DATASET", "confidence": 0.8036766250928243}, {"text": "coreference resolution", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.9154167771339417}, {"text": "MUC-6", "start_pos": 274, "end_pos": 279, "type": "DATASET", "confidence": 0.939781904220581}]}, {"text": "In this paper, we focus on the task of determining coreference relations as defined in MUC-6.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.7933054566383362}]}, {"text": "Specifically, a coreference relation denotes an identity of reference and holds between two textual elements known as markables, which are nouns, noun phrases, or pronouns.", "labels": [], "entities": []}, {"text": "Thus, our coreference task resolves general noun phrases and not just pronouns, unlike in some previous work on anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7638338506221771}]}, {"text": "The ability to link co-referring noun phrases both within and across sentences is critical to discourse analysis and language understanding in general.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7198991626501083}, {"text": "language understanding", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.7478244304656982}]}], "datasetContent": [{"text": "In order to evaluate the performance of our learning approach to coreference resolution on a common data set, we utilized the annotated corpus and scoring program from MUC-6, which assembled a set of newswire documents annotated with coreference chains.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.9604598581790924}, {"text": "MUC-6", "start_pos": 168, "end_pos": 173, "type": "DATASET", "confidence": 0.9634764790534973}]}, {"text": "Although we did not participate in MUC-6, we were able to obtain the MUC-6 training and test corpus from the MUC organizers for research purpose.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.7868108153343201}, {"text": "MUC-6 training and test corpus", "start_pos": 69, "end_pos": 99, "type": "DATASET", "confidence": 0.8795681476593018}, {"text": "MUC organizers", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.879685640335083}]}, {"text": "1 30 dry-run documents annotated with coreference information were used as the training documents for our coreference engine.", "labels": [], "entities": []}, {"text": "After training the engine, we tested its accuracy on the 30 formal test documents in MUC-6.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.999327540397644}, {"text": "MUC-6", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.9154025316238403}]}, {"text": "These 30 test documents are exactly those used to evaluate the systems that participated in the MUC-6 evaluation.", "labels": [], "entities": [{"text": "MUC-6 evaluation", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.5363778173923492}]}, {"text": "Our implemented system runs on a Pentium II 400MHz PC.", "labels": [], "entities": []}, {"text": "The total size of the 30 training documents is close to 13,000 words.", "labels": [], "entities": []}, {"text": "It took less than five minutes to generate the training examples from these training documents.", "labels": [], "entities": []}, {"text": "The training time for the C4.5 algorithm to generate a decision tree from all the training examples was about 30 seconds.", "labels": [], "entities": []}, {"text": "The decision tree classifier learned (using a pruning confidence level of 25%) is shown in.", "labels": [], "entities": []}, {"text": "One advantage of using a decision tree learning algorithm is that the resulting decision tree classifier built can be interpreted by human.", "labels": [], "entities": []}, {"text": "The decision tree in seems to encapsulate a reasonable rule-of-thumb that matches our intuitive linguistic notion of when two noun phrases can co-refer.", "labels": [], "entities": []}, {"text": "It is also interesting to note that only five out of the ten available features in the training examples are actually used in the final decision tree built.: The decision tree classifier learned When given new test documents, the output of the coreference engine is in the form of SGML files with the coreference chains properly annotated according to the MUC-6 guidelines.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 356, "end_pos": 361, "type": "DATASET", "confidence": 0.8998669981956482}]}, {"text": "The time taken to generate the coreference chains for 30 test documents of close to 14,000 words was less than three minutes.", "labels": [], "entities": []}, {"text": "We then used the scorer program of MUC-6 to generate the recall and precision score for our coreference engine.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.8984123468399048}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9994468092918396}, {"text": "precision score", "start_pos": 68, "end_pos": 83, "type": "METRIC", "confidence": 0.9785800874233246}]}, {"text": "Our coreference engine achieves a recall of 52% and a precision of 68%, yielding a balanced F-measure of 58.9%.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9997183680534363}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9995282888412476}, {"text": "F-measure", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9985460042953491}]}, {"text": "We plotted the score of our coreference engine (square-shaped) against the other official test scores of MUC-6 systems (cross-shaped) in.", "labels": [], "entities": []}, {"text": "We also plotted the learning curve of our coreference engine in, showing its accuracy averaged over five random trials when trained on 5, 10, ..., 30 training documents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9991739392280579}]}, {"text": "Our score is in the upper region of the MUC-6 systems.", "labels": [], "entities": [{"text": "MUC-6 systems", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.9091558754444122}]}, {"text": "We performed a simple two-tailed, paired t-test at p = 0.05 to determine whether the difference between our system's F-measure scores and each of the other MUC-6 systems' F-measure scores on the 30 formal test documents is statistically significant.", "labels": [], "entities": [{"text": "MUC-6 systems' F-measure scores on the 30 formal test documents", "start_pos": 156, "end_pos": 219, "type": "DATASET", "confidence": 0.7121123880147934}]}, {"text": "We found that at the 95% significance level, our system performed worse than one, better than two, and as well as the rest of the MUC-6 systems.", "labels": [], "entities": [{"text": "significance", "start_pos": 25, "end_pos": 37, "type": "METRIC", "confidence": 0.9004526734352112}, {"text": "MUC-6", "start_pos": 130, "end_pos": 135, "type": "DATASET", "confidence": 0.8816940188407898}]}, {"text": "Our result is encouraging as it indicates that a learning approach using relatively shallow features and a small number of training documents can lead to scores that are comparable to systems built us- It should be noted that the accuracy of our coreference resolution engine depends to a large extent on the performance of the NLP modules that are executed before the coreference engine.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 230, "end_pos": 238, "type": "METRIC", "confidence": 0.9979422688484192}, {"text": "coreference resolution engine", "start_pos": 246, "end_pos": 275, "type": "TASK", "confidence": 0.8821417291959127}]}, {"text": "Our current learning-based, HMM named entity recognition module is trained on 318 documents (a disjoint set from the 30 formal test documents) tagged with named entities, and its score on the MUC-6 named entity task for the 30 formal test documents is only 88.9%, which is not considered very high by MUC-6 standard.", "labels": [], "entities": [{"text": "HMM named entity recognition", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.7332748025655746}, {"text": "MUC-6", "start_pos": 192, "end_pos": 197, "type": "DATASET", "confidence": 0.9157547950744629}, {"text": "MUC-6", "start_pos": 301, "end_pos": 306, "type": "DATASET", "confidence": 0.9446389079093933}]}, {"text": "For example, our named entity recognizer could not identify the two named entities \"USAir\" and \"Piedmont\" in the expression \"USAir and Piedmont\" but instead treat it as one single named entity.", "labels": [], "entities": [{"text": "USAir", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.9827941656112671}, {"text": "USAir and Piedmont", "start_pos": 125, "end_pos": 143, "type": "DATASET", "confidence": 0.797698974609375}]}, {"text": "Also, some of the features such as number agreement, gender agreement and semantic class agreement are difficult to determine at times.", "labels": [], "entities": []}, {"text": "For example, \"they\" is sometimes used to refer to \"the government\" even though superficially both do not seem to agree in number.", "labels": [], "entities": []}, {"text": "All these problems hurt the performance of the coreference engine.", "labels": [], "entities": []}], "tableCaptions": []}