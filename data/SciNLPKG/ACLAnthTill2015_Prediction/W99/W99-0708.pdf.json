{"title": [{"text": "MDL-based DCG Induction for NP Identification", "labels": [], "entities": [{"text": "NP Identification", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7838364839553833}]}], "abstractContent": [{"text": "We introduce a learner capable of automatically extending large, manually written natural language Definite Clause Grammars with missing syntactic rules.", "labels": [], "entities": []}, {"text": "It is based upon the Minimum Description Length principle , and can be trained upon either just raw text, or else raw text additionally annotated with parsed corpora.", "labels": [], "entities": []}, {"text": "As a demonstration of the learner, we show how full Noun Phrases (NPs that might contain pre or post-modifying phrases and might also be recursively nested) can be identified in raw text.", "labels": [], "entities": []}, {"text": "Preliminary results obtained by varying the amount of syntactic information in the training set suggests that raw text is less useful than additional NP bracketing information.", "labels": [], "entities": [{"text": "NP bracketing", "start_pos": 150, "end_pos": 163, "type": "TASK", "confidence": 0.7815626561641693}]}, {"text": "However, using all syntactic information in the training set does not produce a significant improvement over just bracketing information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Identification of Noun Phrases (NPs) in free text has been tackled in a number of ways (for example,).", "labels": [], "entities": [{"text": "Identification of Noun Phrases (NPs) in free text", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.9181359589099884}]}, {"text": "Usually however, only relatively simple NPs, such as 'base' NPs (NPs that do not contain nested NPs or postmodifying clauses) are recovered.", "labels": [], "entities": []}, {"text": "The motivation for this decision seems to be pragmatic, driven in part by alack of technology capable of parsing large quantities of free text.", "labels": [], "entities": [{"text": "parsing large quantities of free text", "start_pos": 105, "end_pos": 142, "type": "TASK", "confidence": 0.8683485090732574}]}, {"text": "With the advent of broad coverage grammars (for example and attendant efficient parsers, however, we need not make this restriction: we now can identify 'full' NPs, NPs that might contain pre and/or post-modifying complements, in free text.", "labels": [], "entities": []}, {"text": "Full NPs m'e more interesting than base NPs to estimate: \u2022 They are (at least) context free, unlike base NPs which are finite state.", "labels": [], "entities": []}, {"text": "They can contain pre-and post-modifying phrases, and so proper identification can in the worst case imply full-scale parsing/grammar learning.", "labels": [], "entities": []}, {"text": "\u2022 Recursive nesting of NPs means that each nominal head needs to be associated with each NP.", "labels": [], "entities": [{"text": "Recursive nesting of NPs", "start_pos": 2, "end_pos": 26, "type": "TASK", "confidence": 0.819854125380516}]}, {"text": "Base NPs simply group all potential heads together in a flat structure.", "labels": [], "entities": []}, {"text": "As a (partial) response to these challenges, we identify full NPs by treating the task as a special case of full-scale sentential Definite Clause Grammar (DCG) learning.", "labels": [], "entities": [{"text": "sentential Definite Clause Grammar (DCG) learning", "start_pos": 119, "end_pos": 168, "type": "TASK", "confidence": 0.6753347963094711}]}, {"text": "Our approach is based upon the Minimum Description Length (MDL) principle.", "labels": [], "entities": [{"text": "Minimum Description Length (MDL)", "start_pos": 31, "end_pos": 63, "type": "METRIC", "confidence": 0.7296025902032852}]}, {"text": "Here, we do not explain MDL, but instead refer the reader to the literature (for example, see).", "labels": [], "entities": []}, {"text": "Although a DCG learning approach to NP identification is far more computationally demanding than any other NP learning technique reported, it does provide a useful test-bed for exploring some of the (syntactic) factors involved with NP identification.", "labels": [], "entities": [{"text": "NP identification", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.955596536397934}, {"text": "NP identification", "start_pos": 233, "end_pos": 250, "type": "TASK", "confidence": 0.8969925343990326}]}, {"text": "By contrast, other approaches at NP identification more usually only consider lexical/part-of-speech influences.", "labels": [], "entities": [{"text": "NP identification", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9818158745765686}]}, {"text": "In this paper, we consider, from an estimation perspective, how dependent NPs are upon their (surrounding) syntactic context.", "labels": [], "entities": []}, {"text": "We varied the information content of the training set and measured the effect this had upon NP identification accuracy.", "labels": [], "entities": [{"text": "NP identification", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.9281442165374756}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.8598487377166748}]}, {"text": "Results suggest that: \u2022 Use of any syntactic information, in addition\" to raw text during estimation, produces better results than estimation from raw text alone.", "labels": [], "entities": []}, {"text": "\u2022 NPs containing an internal annotation (nonterminals in addition to NPs) are harder to estimate than NPs that do not contain these additional nonterminals.", "labels": [], "entities": []}, {"text": "\u2022 Training with NP annotated sentences and training with sentences annotated with full sentential parses produce very similar results to each other.", "labels": [], "entities": []}, {"text": "We stress that the last finding is provisional, and further investigation is necessary to verify it.", "labels": [], "entities": []}, {"text": "The structure of the rest of this paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives an overview of our approach, whilst section 3 goes into estimation and modelling details.", "labels": [], "entities": []}, {"text": "We do not start induction ab initio, but instead base estimation upon manually written grammars.", "labels": [], "entities": []}, {"text": "Section 4 briefly describes the particular grammar used in this research, whilst section 5 relates our work to others.", "labels": [], "entities": []}, {"text": "Section 6 presents an experimental evaluation of our learner.", "labels": [], "entities": []}, {"text": "The paper ends with a discussion Of our findings.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we used material supplied by the CoNLL99 workshop organisers.", "labels": [], "entities": [{"text": "CoNLL99 workshop organisers", "start_pos": 53, "end_pos": 80, "type": "DATASET", "confidence": 0.948556383450826}]}, {"text": "This consisted of 48,224 fully parsed training sentences, and a disjoint set of 984 testing sentences.", "labels": [], "entities": []}, {"text": "Both sets were randomly drawn from the parsed section of the Wall Street Journal.", "labels": [], "entities": [{"text": "parsed section of the Wall Street Journal", "start_pos": 39, "end_pos": 80, "type": "DATASET", "confidence": 0.7556370837347848}]}, {"text": "The test set came in two versions, differing from each other in how the sentences were marked:up.", "labels": [], "entities": []}, {"text": "The first version consisted of sentences with NP bracketings marked (results using this test set are given in table 1).", "labels": [], "entities": []}, {"text": "The second version had NP bracketings marked, and within each marked NP, there was an internal parse (results for this version are in table 2).", "labels": [], "entities": [{"text": "NP bracketings", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.656047061085701}]}, {"text": "These parses were labelled with Penn Nonterminals.", "labels": [], "entities": [{"text": "Penn Nonterminals", "start_pos": 32, "end_pos": 49, "type": "METRIC", "confidence": 0.8896395862102509}]}, {"text": "Each test sentence was trivially rooted with an S symbol (necessary for the evaluation software).", "labels": [], "entities": []}, {"text": "To make this clearer, if an original Wall Street Journal parse, stripped of tags and nonterminal decorations was: For computational reasons, we could not deal with all sentences in the training set. and when learning ! rules, we limited ourselves to ser/tences with a maximum length of 15 tokens.", "labels": [], "entities": [{"text": "Wall Street Journal parse", "start_pos": 37, "end_pos": 62, "type": "DATASET", "confidence": 0.9387763291597366}]}, {"text": "During evaluation, we used sentences with a maximum length of 30 tokens.", "labels": [], "entities": []}, {"text": "This reduced the training set to 10,249 parsed sentences, and the test set to 739 sentences.", "labels": [], "entities": []}, {"text": "Finally, we retagged the CoNLL99 material with the Claws2 tagset (required by TSG).", "labels": [], "entities": [{"text": "CoNLL99 material", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9128970205783844}, {"text": "Claws2 tagset", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.8588279783725739}, {"text": "TSG", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.8913401365280151}]}, {"text": "Evaluation was carried out by Parseval (which reports unlabelled bracketing results: we do not report labelled results as TSG does not use the Penn Nonterminal set).", "labels": [], "entities": [{"text": "Parseval", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.8730113506317139}, {"text": "TSG", "start_pos": 122, "end_pos": 125, "type": "DATASET", "confidence": 0.8653550148010254}, {"text": "Penn Nonterminal set", "start_pos": 143, "end_pos": 163, "type": "DATASET", "confidence": 0.9251559575398763}]}, {"text": "Note that evaluation is based upon bracketing, and not word accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9230801463127136}]}, {"text": "For example, if we failed to include one word in a NP that contains four other words, we would have a bracketing accuracy of 0.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.910646915435791}]}, {"text": "On the other hand, a word accuracy result would be 80%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9814637303352356}]}, {"text": "Asa comparison, we evaluated TSG upon the testing material.", "labels": [], "entities": [{"text": "TSG", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9911967515945435}]}, {"text": "This is experiment 5 in tables 1 and 2.", "labels": [], "entities": []}, {"text": "The other four experiments differed from each other in terms of what the learner was trained upon: 1.", "labels": [], "entities": []}, {"text": "2. Tagged Sentences with NP bracketings marked.", "labels": [], "entities": [{"text": "NP bracketings", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.6276480406522751}]}, {"text": "We reduced WSJ parses to include just their NP bracketings.", "labels": [], "entities": [{"text": "WSJ parses", "start_pos": 11, "end_pos": 21, "type": "TASK", "confidence": 0.6926825642585754}]}, {"text": "3. Tagged sentences with NPs bracketing annotated with an internal parse.", "labels": [], "entities": []}, {"text": "Again, we mapped WSJ parses to reduced parses containing just annotated NPs.", "labels": [], "entities": [{"text": "WSJ parses", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.5764168798923492}]}, {"text": "4. Tagged sentences with a full Wall Street Journal parse.", "labels": [], "entities": [{"text": "Wall Street Journal parse", "start_pos": 32, "end_pos": 57, "type": "DATASET", "confidence": 0.9538231194019318}]}, {"text": "For each experiment, we report the size of the final grammar, the percentage of testing sentences covered (assigned a full parse), crossing rates, recall and precision results with respect to testing sentences with NPs bracketed and those containing annotated NPs.", "labels": [], "entities": [{"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9994544386863708}, {"text": "precision", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.9994431138038635}]}, {"text": "For the bracketing task, we mapped full parses, produced by nmdels, to parses just containing NP bracketing.", "labels": [], "entities": [{"text": "bracketing task", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.9195239841938019}]}, {"text": "we mapped full parses to parses containing just NPs with an internal annotation.", "labels": [], "entities": []}, {"text": "Note that within our grammatical framework, the best mapping is not clear (since parses produced by our models have categories using multiple bar levels, whilst WSJ parses ouly use a single level).", "labels": [], "entities": []}, {"text": "As a guess, we treated bar 1 and bar 2 nominal categories as being NPs.", "labels": [], "entities": []}, {"text": "This means that our precision results are lowered, since in general, we produce more NPs than would be predicted by a WSJ parse.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9993302822113037}]}, {"text": "we evaluate the performance of a model in terms of the highest ranked parse, and secondly, in terms of the \"best' parse, out of the top 10 parses produced.", "labels": [], "entities": []}, {"text": "Here \"best' means the parse produced that is closest, in terms of a weighted sum crossing rates, precision and recall, to the manually selected parse.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.998870313167572}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9976740479469299}]}, {"text": "This final set of results gives an indication of how well our system would perform if it had a much better parse selection mechanism.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 107, "end_pos": 122, "type": "TASK", "confidence": 0.9353421926498413}]}, {"text": "Best figures are marked in parentheses.", "labels": [], "entities": []}, {"text": "gives our results for the bracketing task, whilst gives our results for the annotation task.", "labels": [], "entities": [{"text": "bracketing task", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.9143134951591492}]}, {"text": "Model size and coverage results were id~-ltical for both tests, so the second  Firstly, when compared with other work on NP recovery, our results are poor.", "labels": [], "entities": [{"text": "coverage", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9827460050582886}, {"text": "NP recovery", "start_pos": 121, "end_pos": 132, "type": "TASK", "confidence": 0.9317904710769653}]}, {"text": "As was mentioned in the search section, this is largely due to our system being based upon a language model that has well known limitations.", "labels": [], "entities": []}, {"text": "Furthermore, as was argued in the iutroduction, full NPs are by definition harder to identify than base NPs, so we would expect our results to be worse.", "labels": [], "entities": []}, {"text": "Secondly, we see that the bracketing task is easier than the annotation task: generally, the results in table 1 are better than the results in table 2.", "labels": [], "entities": [{"text": "bracketing task", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.9064833521842957}]}, {"text": "Given the fact that the annotation search space is larger than the bracketing space, this should come as no surprise.", "labels": [], "entities": []}, {"text": "Turning now to the individual experiments, we see that parsed corpora (experiments 2, 3 and 4) is all informative constraint upon NP induction.", "labels": [], "entities": [{"text": "NP induction", "start_pos": 130, "end_pos": 142, "type": "TASK", "confidence": 0.8486187160015106}]}, {"text": "Rules learnt using parsed corpora better capture regularities than do rules learnt from just raw text (experiment 1).", "labels": [], "entities": []}, {"text": "This is shown by the increased coverage results of experiments 2.3 and 4 over 1.", "labels": [], "entities": [{"text": "coverage", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.993925154209137}]}, {"text": "In terms of crossing rates, recall and precision, no clear story has emerged.", "labels": [], "entities": [{"text": "crossing", "start_pos": 12, "end_pos": 20, "type": "TASK", "confidence": 0.9703340530395508}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9993143081665039}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9995613694190979}]}, {"text": "Surprisingly, there seems to be minimal difference in coverage when using either annotated NPs or full parses.", "labels": [], "entities": [{"text": "coverage", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9945711493492126}]}, {"text": "This could be due to a number of reasons, such as WSJ NPs being more reliably annotated than other phrases, simple artifactual problems with the learner, the evaluation metrics being too coarse to show any real differences, etc.", "labels": [], "entities": [{"text": "WSJ NPs", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.7937783598899841}]}, {"text": "Further, qualitative investigation should determine whether there are any differences in the parses that TSG alone cannot assign to sentences.", "labels": [], "entities": []}, {"text": "Due to time constraints, we did not measure statistical significance tests between the various experiments.", "labels": [], "entities": []}, {"text": "A later version of this paper (available from the author, osborne@let.rug.nl) will report these tests.", "labels": [], "entities": []}], "tableCaptions": []}