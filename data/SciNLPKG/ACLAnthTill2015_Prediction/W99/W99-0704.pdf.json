{"title": [], "abstractContent": [{"text": "Constructive induction transforms the representation of instances in order to produce a more accurate model of the concept to be learned.", "labels": [], "entities": []}, {"text": "For this purpose, a variety of operators has been proposed in the literature, including a Cartesian product operator forming pair-wise higher-order attributes.", "labels": [], "entities": []}, {"text": "We study the effect of the Cartesian product operator on memory-based language learning, and demonstrate its effect on generalization accuracy and data compression fora number of linguistic classification tasks, using k-nearest neighbor learning algorithms.", "labels": [], "entities": [{"text": "generalization", "start_pos": 119, "end_pos": 133, "type": "TASK", "confidence": 0.9551007747650146}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.8347593545913696}, {"text": "linguistic classification tasks", "start_pos": 179, "end_pos": 210, "type": "TASK", "confidence": 0.796888510386149}]}, {"text": "These results are compared to a baseline approach of backward sequential elimination of attributes.", "labels": [], "entities": []}, {"text": "It is demonstrated that neither approach consistently outperforms the other, and that attribute elimination can be used to derive compact representations for memory-based language learning without noticeable loss of generalization accuracy.", "labels": [], "entities": [{"text": "attribute elimination", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.6986673921346664}, {"text": "accuracy", "start_pos": 231, "end_pos": 239, "type": "METRIC", "confidence": 0.8191475868225098}]}], "introductionContent": [], "datasetContent": [{"text": "The effects of forming Cartesian product attributes on generalization accuracy and reduction of dimensionality (compression) were compared with those of backward sequential elimination of attributes.", "labels": [], "entities": [{"text": "generalization", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.9511103630065918}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.8810245990753174}]}, {"text": "The following 7 linguistic datasets were used.", "labels": [], "entities": []}, {"text": "STRESS is a selection of secondary stress assignment patterns from the Dutch version of the Celex lexical database, on the basis of phonemic representations of syllabified words.", "labels": [], "entities": [{"text": "STRESS", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5858750939369202}, {"text": "Celex lexical database", "start_pos": 92, "end_pos": 114, "type": "DATASET", "confidence": 0.877397894859314}]}, {"text": "Also derived from Celex is the DIMIN task, a selection of diminutive formation patterns for Dutch.", "labels": [], "entities": []}, {"text": "This task consists of assigning Dutch diminutive suffixes'to a noun, based on phonetic properties of (maximally) the last three syllables of the noun.", "labels": [], "entities": [{"text": "assigning Dutch diminutive suffixes'to a noun", "start_pos": 22, "end_pos": 67, "type": "TASK", "confidence": 0.783497820297877}]}, {"text": "Attribute values are phoneme representations as well as stress markers for the syllables.", "labels": [], "entities": []}, {"text": "The WSJ-NPVP set consists of part-of speech tagged Wall Street Journal material, supplemented with syntactic tags indicating noun phrase and verb phrase boundaries ().", "labels": [], "entities": [{"text": "WSJ-NPVP set", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9327212572097778}, {"text": "Wall Street Journal material", "start_pos": 51, "end_pos": 79, "type": "DATASET", "confidence": 0.9246539175510406}]}, {"text": "wsJ-POS is a fragment of the Wall Street Journal part-of-speech tagged material.", "labels": [], "entities": [{"text": "Wall Street Journal part-of-speech tagged material", "start_pos": 29, "end_pos": 79, "type": "DATASET", "confidence": 0.9297552307446798}]}, {"text": "Attributes values are parts of speech, which are assigned using a windowing approach, with a window size of 5.", "labels": [], "entities": []}, {"text": "INL-POS is a part-of-speech tagging task for Dutch, using tl~e DutchTale tagset [van der Voort van der, attribute values are parts of speech.", "labels": [], "entities": [{"text": "part-of-speech tagging task", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.7781723737716675}, {"text": "DutchTale", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.9402480125427246}]}, {"text": "Using a windowing approach, on the basis of a 7-cell window, part of speech tags are disambiguated.", "labels": [], "entities": []}, {"text": "GRAPHON constitutes a grapheme-to-phoneme learning task for English, based on the Celex lexical database.", "labels": [], "entities": [{"text": "GRAPHON", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5758318305015564}, {"text": "Celex lexical database", "start_pos": 82, "end_pos": 104, "type": "DATASET", "confidence": 0.9588536222775778}]}, {"text": "Attribute values are graphemes (single characters), to be classified as phonemes.", "labels": [], "entities": []}, {"text": "PP-ATTACH, finally, is a prepositional phrase (PP) attachment task for English..", "labels": [], "entities": [{"text": "prepositional phrase (PP) attachment task", "start_pos": 25, "end_pos": 66, "type": "TASK", "confidence": 0.7295410803386143}]}, {"text": "where PP's are attached to either noun or verb projections, based on lexical context.", "labels": [], "entities": []}, {"text": "Attribute values are word forms for verb, the head noun of the following nouu phrase, the preposition of the following PP, and the head noun of the PP-internal noun phrase (like bring attention to problem).", "labels": [], "entities": []}, {"text": "The material has been extracted by from the Penn Treebank Wall Street Journal corpus.", "labels": [], "entities": [{"text": "Penn Treebank Wall Street Journal corpus", "start_pos": 44, "end_pos": 84, "type": "DATASET", "confidence": 0.977741410334905}]}, {"text": "Key numerical characteristics of the datasets are summarized in table 1.", "labels": [], "entities": []}, {"text": "Each of these datasets was subjected to the BSJ-IG and the BSE wrapper algorithms, embedding either the IBI-IG or IGTREE architecture.", "labels": [], "entities": [{"text": "BSJ-IG", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.9818390607833862}, {"text": "BSE wrapper", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9814341068267822}]}, {"text": "Both the Naive Bayes and PEBLS classifier investigated by allow for certain frequency tendencies hidden in the data to bear on the classification.", "labels": [], "entities": [{"text": "PEBLS classifier", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.8951537013053894}]}, {"text": "This has a smoothing effect on the handling of low-frequency events, which benefit from analogies with more reliable higher-frequency events.", "labels": [], "entities": []}, {"text": "In order to assess the effects of smoothing, the following additional experiments were carried out.", "labels": [], "entities": [{"text": "smoothing", "start_pos": 34, "end_pos": 43, "type": "TASK", "confidence": 0.9843670725822449}]}, {"text": "Embeddded into BSE and BSJ-IG, the PEBLS approximation IBI-IG with MVDM was applied to three datasets: STRESS, DIMIN and PP-ATTACH, for three values of k, the size of the nearest neighbor set.", "labels": [], "entities": [{"text": "BSE", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.9394283890724182}, {"text": "BSJ-IG", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.8950366377830505}, {"text": "PEBLS approximation IBI-IG", "start_pos": 35, "end_pos": 61, "type": "METRIC", "confidence": 0.7954060435295105}, {"text": "STRESS", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9824506640434265}, {"text": "DIMIN", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.8311247825622559}]}, {"text": "Values fork larger than 1, i.e. non-singleton nearest neighbor sets.", "labels": [], "entities": []}, {"text": "have been found to reproduce some of the smoothing inherent to statistical back-off models (.", "labels": [], "entities": []}, {"text": "Generalization accuracy for every attribute joining or elimination step was measured using 10-fold crossvalidation, and significance was measured using a twotailed paired t-test at the .05 level.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.771385133266449}, {"text": "attribute joining or elimination step", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.8355631351470947}, {"text": "significance", "start_pos": 120, "end_pos": 132, "type": "METRIC", "confidence": 0.964030385017395}]}, {"text": "All experiments were carried out on a Digital Alpha XL-266 (Linux) and a Sun UltraSPARC-IIi (Solaris).", "labels": [], "entities": []}, {"text": "Due to slow performance of the IBI-IG model on certain datasets with the used equipment, IBI-IG experiments with %VSJ-NPVP could not be completed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of instmlces, attributes and original accuracies for the datasets.", "labels": [], "entities": []}, {"text": " Table 2: Average compression rates.", "labels": [], "entities": [{"text": "compression", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.656791627407074}]}, {"text": " Table 3: Number of remaining attributes and accuracies for BSE. A '+' indicates a siguificant increase in accuracy  compared to the original algorithm; a '_' indicates the experiment could not be completed.", "labels": [], "entities": [{"text": "BSE", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.6118953824043274}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9916204214096069}]}]}