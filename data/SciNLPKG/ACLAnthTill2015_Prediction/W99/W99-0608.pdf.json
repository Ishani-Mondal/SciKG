{"title": [{"text": "Improving POS Tagging Using Machine-Learning Techniques", "labels": [], "entities": [{"text": "Improving POS Tagging", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8371675809224447}]}], "abstractContent": [{"text": "In this paper we show how machine learning techniques for constructing and combining several classifiers can be applied to improve the accuracy of an existing English POS tagger (MSxquez and Rodrfguez, 1997).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9983300566673279}]}, {"text": "Additionally, the problem of data sparseness is also addressed by applying a technique of generating convez pseudo-data (Breiman, 1998).", "labels": [], "entities": []}, {"text": "Experimental results and a comparison to other state-of-the-art tuggers are reported.", "labels": [], "entities": []}], "introductionContent": [{"text": "The study of general methods to improve the performance in classification tasks, by the combination of different individual classifiers, is a currently very active area of research in supervised learning.", "labels": [], "entities": []}, {"text": "In the machine learning literature this approach is known as ensemble, stacked, or combined classifiers.", "labels": [], "entities": []}, {"text": "Given a classification problem, the main goal is to construct several independent classifiers, since it has been proven that when the errors committed by individual classifiers are uncorrelated to a sufficient degree, and their error rates are low enough, the resulting combined classifier performs better than all the individual systems.", "labels": [], "entities": []}, {"text": "Several methods have been proposed in order to construct ensembles of classifiers that make uncorrelated errors.", "labels": [], "entities": []}, {"text": "Some of them are general, and they can be applied to any learning algorithln, while other are specific to particular algorithms.", "labels": [], "entities": []}, {"text": "From a different perspective, there exist methods for constructing homogeneous ensembles, in the sense that a unique learning algorithm has been used to acquire each individual classifier, and heterogeneous ensembles that combine different types of learning paradigms 1.", "labels": [], "entities": []}, {"text": "Impressive results have been obtained by applying these techniques on the so-called unstable learning algorithms (e.g. induction of decision trees, neural networks, rule-induction systems, etc.).", "labels": [], "entities": []}, {"text": "Several applications to real tasks have been performed, and, regarding NLP, we find ensembles of classifiers in context-sensitive spelling correction), text categorization (, and text filtering ( ).", "labels": [], "entities": [{"text": "context-sensitive spelling correction", "start_pos": 112, "end_pos": 149, "type": "TASK", "confidence": 0.5854921440283457}, {"text": "text categorization", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.7760584950447083}, {"text": "text filtering", "start_pos": 179, "end_pos": 193, "type": "TASK", "confidence": 0.7861120998859406}]}, {"text": "Combination of classitiers have also been applied to POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.78802689909935}]}, {"text": "For instance, van Halteren (1996) combined a number of similar tuggers byway of a straightforward majority vote.", "labels": [], "entities": []}, {"text": "More recently, two parallel works) combined, with a remarkable success, the output of a set of four tuggers based on different principles and feature modelling.", "labels": [], "entities": []}, {"text": "Finally, in the work by the combination of taggers is used in a bootstrapping algorithm to train apart of speech tagger from a limited amount of training material.", "labels": [], "entities": [{"text": "speech tagger", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.7312102317810059}]}, {"text": "The aim of the present work is to improve an existing POS tagger based on decision trees (, by using ensembles of classifiers.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.7223294079303741}]}, {"text": "This tagger treats separately the different types (classes) of ambiguity by considering a different decision tree for each class.", "labels": [], "entities": []}, {"text": "This fact allows a selective construction of ensembles of decision trees focusing on the most relevant ambiguity classes, which greatly vary in size and difficulty.", "labels": [], "entities": []}, {"text": "Another goal of the present work is to try to alleviate the problem of data sparseness by applying a method, due 1An excellent survey covering all these topics call be found in to, for generating new pseudoexamples from existing data.", "labels": [], "entities": []}, {"text": "As we will see in section 4.2 this technique will be combined with the construction of an ensemble of classifiers.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: we start by presenting the two versions of the POS tagger and their evaluation on the reference corpus (sections 2 and 3).", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 82, "end_pos": 92, "type": "TASK", "confidence": 0.5790251791477203}]}, {"text": "Sections 4 and 5 are, respectively, devoted to present the machine-learning improvements and to test their implementation.", "labels": [], "entities": []}, {"text": "Finally, section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Information about the WSJ training and test corpora. S: number of sentences; W: number  of words; W/S: average number of words per sentence; AW: number and percentage of ambiguous  words; T/W: average number of tags per word; T/AW: average number of tags per ambiguous", "labels": [], "entities": [{"text": "WSJ training", "start_pos": 32, "end_pos": 44, "type": "TASK", "confidence": 0.6431390345096588}, {"text": "AW", "start_pos": 151, "end_pos": 153, "type": "METRIC", "confidence": 0.9918603301048279}]}, {"text": " Table 2: Number of ambiguity classes that cover the x% of the ambiguous words of the training  corpus", "labels": [], "entities": []}, {"text": " Table 3: Tagging accuracy, speed, and storage requirement of RTT and STT taggers", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9597023129463196}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9858680367469788}, {"text": "STT taggers", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.5448902696371078}]}, {"text": " Table 5: Tagging accuracy, speed, and storage requirements of enriched RTT and 5TT taggers", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9581834673881531}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.981795608997345}]}, {"text": " Table 6: Comparison of different tuggers on the WSJ corpus", "labels": [], "entities": [{"text": "WSJ", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.6480733752250671}]}]}