{"title": [], "abstractContent": [{"text": "This paper introduces anew, unsupervised algorithm for noun phrase coreference resolution.", "labels": [], "entities": [{"text": "noun phrase coreference resolution", "start_pos": 55, "end_pos": 89, "type": "TASK", "confidence": 0.8657363951206207}]}, {"text": "It differs from existing methods in that it views corer-erence resolution as a clustering task.", "labels": [], "entities": [{"text": "corer-erence resolution", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.7813604772090912}]}, {"text": "In an evaluation on the MUC-6 coreference resolution corpus , the algorithm achieves an F-measure of 53.6%~ placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation.", "labels": [], "entities": [{"text": "MUC-6 coreference resolution corpus", "start_pos": 24, "end_pos": 59, "type": "DATASET", "confidence": 0.8046088218688965}, {"text": "F-measure", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9993971586227417}, {"text": "MUC-6 evaluation", "start_pos": 180, "end_pos": 196, "type": "DATASET", "confidence": 0.9154618084430695}]}, {"text": "More importantly , the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.9371190667152405}]}, {"text": "The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into corefer-ence equivalence classes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many natural language processing (NLP) applications require accurate noun phrase coreference resolution: They require a means for determining which noun phrases in a text or dialogue refer to the same real-world entity.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 5, "end_pos": 38, "type": "TASK", "confidence": 0.7704086204369863}, {"text": "noun phrase coreference resolution", "start_pos": 69, "end_pos": 103, "type": "TASK", "confidence": 0.7402946949005127}]}, {"text": "The vast majority of algorithms for noun phrase coreference combine syntactic and, less often, semantic cues via a set of hand-crafted heuristics and filters.", "labels": [], "entities": [{"text": "noun phrase coreference", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7356863419214884}]}, {"text": "All but one system in the MUC-6 coreference performance evaluation, for example, handled coreference resolution in this manner.", "labels": [], "entities": [{"text": "MUC-6 coreference performance evaluation", "start_pos": 26, "end_pos": 66, "type": "DATASET", "confidence": 0.5920280814170837}, {"text": "coreference resolution", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.9267477989196777}]}, {"text": "This same reliance on complicated hand-crafted algorithms is true even for the narrower task of pronoun resolution.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7455384731292725}]}, {"text": "Some exceptions exist, however.) present a probabilistic model for pronoun resolution trained on a small subset of the Penn Treebank Wall Street Journal corpus (.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8120431005954742}, {"text": "Penn Treebank Wall Street Journal corpus", "start_pos": 119, "end_pos": 159, "type": "DATASET", "confidence": 0.9801619946956635}]}, {"text": "develop a statistical filter for resolution of the pronoun \"it\" that selects among syntactically viable antecedents based on relevant subject-verb-object cooccurrences. and employ decision tree algorithms to handle a broader subset of general noun phrase coreference problems.", "labels": [], "entities": [{"text": "noun phrase coreference", "start_pos": 243, "end_pos": 266, "type": "TASK", "confidence": 0.6546524167060852}]}, {"text": "This paper presents anew corpus-based approach to noun phrase coreference.", "labels": [], "entities": [{"text": "noun phrase coreference", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.7948329647382101}]}, {"text": "We believe that it is the first such unsupervised technique developed for the general noun phrase coreference task.", "labels": [], "entities": [{"text": "general noun phrase coreference task", "start_pos": 78, "end_pos": 114, "type": "TASK", "confidence": 0.7329196631908417}]}, {"text": "In short, we view the task of noun phrase coreference resolution as a clustering task.", "labels": [], "entities": [{"text": "noun phrase coreference resolution", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.8100998550653458}]}, {"text": "First, each noun phrase in a document is represented as a vector of attribute-value pairs.", "labels": [], "entities": []}, {"text": "Given the feature vector for each noun phrase, the clustering algorithm coordinates the application of context-independent and context-dependent coreference constraints and preferences to partition the noun phrases into equivalence classes, one class for each real-world entity mentioned in the text.", "labels": [], "entities": []}, {"text": "Context-independent coreference constraints and preferences are those that apply to two noun phrases in isolation.", "labels": [], "entities": []}, {"text": "Context-dependent coreference decisions, on the other hand, consider the relationship of each noun phrase to surrounding noun phrases.", "labels": [], "entities": []}, {"text": "In an evaluation on the MUC-6 coreference resolution corpus, our clustering approach achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation.", "labels": [], "entities": [{"text": "MUC-6 coreference resolution corpus", "start_pos": 24, "end_pos": 59, "type": "DATASET", "confidence": 0.8385958969593048}, {"text": "F-measure", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9994280934333801}, {"text": "MUC-6 evaluation", "start_pos": 189, "end_pos": 205, "type": "DATASET", "confidence": 0.9315251111984253}]}, {"text": "More importantly, the clustering approach outperforms the only MUC-6 system to view coreference resolution as a learning problem: The RESOLVE system employs decision tree induction and achieves an Fmeasure of 47% on the MUC-6 data set.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.9317713677883148}, {"text": "RESOLVE", "start_pos": 134, "end_pos": 141, "type": "METRIC", "confidence": 0.9236206412315369}, {"text": "decision tree induction", "start_pos": 157, "end_pos": 180, "type": "TASK", "confidence": 0.6647555828094482}, {"text": "Fmeasure", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9990768432617188}, {"text": "MUC-6 data set", "start_pos": 220, "end_pos": 234, "type": "DATASET", "confidence": 0.9745107293128967}]}, {"text": "Furthermore, our approach has a number of important advantages over existing learning and non-learning methods for coreference resolution: \u2022 The approach is largely unsupervised, so no annotated training corpus is required.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.956646203994751}]}, {"text": "\u2022 Although evaluated in an information extraction context, the approach is domainindependent.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.8220634460449219}]}, {"text": "\u2022 As noted above, the clustering approach provides a flexible mechanism for coordinating context-independent and context-dependent coreference constraints and preferences for partitioning noun phrases into coreference equivalence classes.", "labels": [], "entities": []}, {"text": "As a result, we believe that viewing noun phrase coreference as clustering provides a promising framework for corpus-based coreference resolution.", "labels": [], "entities": [{"text": "noun phrase coreference as clustering", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.705325573682785}, {"text": "coreference resolution", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.8511098027229309}]}, {"text": "The remainder of the paper describes the details of our approach.", "labels": [], "entities": []}, {"text": "The next section provides a concrete specification of the noun phrase coreference resolution task.", "labels": [], "entities": [{"text": "noun phrase coreference resolution task", "start_pos": 58, "end_pos": 97, "type": "TASK", "confidence": 0.7370845198631286}]}, {"text": "Section 3 presents the clustering algorithm.", "labels": [], "entities": [{"text": "clustering", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.968065083026886}]}, {"text": "Evaluation of the approach appears in Section 4.", "labels": [], "entities": []}, {"text": "Qualitative and quantitative comparisons to related work are included in Section 5.", "labels": [], "entities": [{"text": "Qualitative", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9254920482635498}]}], "datasetContent": [{"text": "We developed and evaluated the clustering approach to coreference resolution using the \"dry run\" and \"formal evaluation\" MUC-6 coreference cotpora.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.9667044878005981}, {"text": "MUC-6 coreference cotpora", "start_pos": 121, "end_pos": 146, "type": "DATASET", "confidence": 0.921255350112915}]}, {"text": "Each corpus contains 30 documents that have been annotated with NP coreference links.", "labels": [], "entities": [{"text": "NP coreference", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.6119861006736755}]}, {"text": "We used the dryrun data for development of the distance measure and selection of the clustering radius rand reserved the formal evaluation materials for testing.", "labels": [], "entities": [{"text": "dryrun data", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.8754616975784302}]}, {"text": "All results are reported using the standard mensures of recall and precision or F-measure (which combines recall and precision equally).", "labels": [], "entities": [{"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9878022074699402}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9783772826194763}, {"text": "F-measure", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9973645806312561}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9980884194374084}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9158827662467957}]}, {"text": "They were calculated automatically using the MUC-6 scoring program (.", "labels": [], "entities": [{"text": "MUC-6 scoring program", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.8435850739479065}]}, {"text": "summarizes our results and compares them to three baselines.", "labels": [], "entities": []}, {"text": "For each algorithm, we show the F-measure for the dryrun evaluation (column 2) and the formal evaluation (column 4).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9829919934272766}]}, {"text": "(The \"adjusted\" results are described below.)", "labels": [], "entities": []}, {"text": "For the dryrun data set, the clustering algorithm obtains 48.8% recall and 57.4% precision.", "labels": [], "entities": [{"text": "dryrun data set", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.8773470520973206}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9997197985649109}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9989544153213501}]}, {"text": "The formal evaluation produces similar scores: 52.7% recall and 54.6% precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9997660517692566}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9994988441467285}]}, {"text": "Both runs user = 4, which was obtained by testing different values on the dryrun corpus.", "labels": [], "entities": [{"text": "dryrun corpus", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9388913214206696}]}, {"text": "summarizes the results on the dryrun data set for r values from 1.0 to 10.0. 3 As expected, increasing r also increases recall, but decreases precision.", "labels": [], "entities": [{"text": "dryrun data set", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.8976422945658366}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.999553382396698}, {"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.998066246509552}]}, {"text": "Subsequent tests with different values for r on the formal evaluation data set also obtained optimal performance with r= 4.", "labels": [], "entities": [{"text": "formal evaluation data set", "start_pos": 52, "end_pos": 78, "type": "DATASET", "confidence": 0.7521170824766159}]}, {"text": "This provides partial support for our hypothesis that r need not be recalculated for new corpora.", "labels": [], "entities": []}, {"text": "The remaining rows in show the performance of the three baseline algorithms.", "labels": [], "entities": []}, {"text": "The first baseline marks every pair of noun phrases as coreferent, i.e. all noun phrases in the document form one class.", "labels": [], "entities": []}, {"text": "This baseline is useful because it establishes an upper bound for recall on our clustering algorithm (67% for the dryrun and 69% for the formal evaluation).", "labels": [], "entities": [{"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9992985725402832}]}, {"text": "The second baseline marks as coreferent any two noun phrases that have a word in common.", "labels": [], "entities": []}, {"text": "The third baseline marks as coreferent any two noun phrases whose head nouns match.", "labels": [], "entities": []}, {"text": "Although the baselines perform better one might expect (they outperform one MUC-6 system), the clustering algorithm performs significantly better.", "labels": [], "entities": []}, {"text": "In part because we rely on base noun phrases, our aNote that r need not bean integers especially when the distance metric is returning non-integral values.", "labels": [], "entities": []}, {"text": "reflect this upper bound on recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.998601496219635}]}, {"text": "Considering only coreference links between base noun phrases, the clustering algorithm obtains a recall of 72.4% on the dryrun, and 75.9% on the formal evaluation.", "labels": [], "entities": [{"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9996455907821655}]}, {"text": "Another Source of error is inaccurate and inadequate NP feature vectors.", "labels": [], "entities": []}, {"text": "Our procedure for computing semantic class values, for example, is responsible for many errors --it sometimes returns incorrect values and the coarse semantic class distinctions are often inadequate.", "labels": [], "entities": []}, {"text": "Without a better named entity finder, computing feature vectors for proper nouns is difficult.", "labels": [], "entities": []}, {"text": "Other errors result from alack of thematic and grammatical role information.", "labels": [], "entities": []}, {"text": "The lack of discourse-related topic and focus information also limits System performance.", "labels": [], "entities": []}, {"text": "In addition, we currently make no special attempt to handle reflexive pronouns and pleonastic \"it\".", "labels": [], "entities": []}, {"text": "Lastly, errors arise from the greedy nature of the clustering algorithm.", "labels": [], "entities": []}, {"text": "Noun phrase NPj is linked to every preceding noun phrase NP~ that is compatible and within the radius r, and that link can never be undone.", "labels": [], "entities": []}, {"text": "We are considering three possible ways to make the algorithm less aggressively greedy.", "labels": [], "entities": []}, {"text": "First, for each NPj, instead of considering every previous noun phrase, the algorithm could stop on finding the first compatible antecedent.", "labels": [], "entities": []}, {"text": "Second, for each NPj, the algorithm could rank all possible antecedents and then choose the best one and link only to that one.", "labels": [], "entities": []}, {"text": "Lastly,: the algorithm could rank all possible coreference links (all pairs of noun phrases in the document) and then proceed through them in ranked order, thus progressing from the links it is most confident about to those it is less certain of.", "labels": [], "entities": []}, {"text": "Future work will include a more detailed error analysis.", "labels": [], "entities": [{"text": "error", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9643122553825378}]}], "tableCaptions": [{"text": " Table 1: Noun Phrase Instance Representation For All Base NPs in the Sample Text", "labels": [], "entities": [{"text": "Noun Phrase Instance Representation", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.7009351998567581}, {"text": "Sample Text", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.8642283976078033}]}, {"text": " Table 2: Incompatibility Functions and Weights for Each Term in the Distance Metric", "labels": [], "entities": []}, {"text": " Table 4: F-measure Results for the Clustering Algorithm and Baseline Systems on the MUC-6 Data Sets", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9600353240966797}, {"text": "MUC-6 Data Sets", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.9703688621520996}]}, {"text": " Table 5: Performance on the Dryrun Data Set for  Different r", "labels": [], "entities": [{"text": "Dryrun Data Set", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.9949905673662821}]}, {"text": " Table 6: Results on the MUC-6 Formal Evaluation", "labels": [], "entities": [{"text": "MUC-6 Formal Evaluation", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.6389026045799255}]}]}