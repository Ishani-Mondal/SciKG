{"title": [{"text": "Hiding a Semantic Hierarchy in a Markov Model", "labels": [], "entities": [{"text": "Hiding a Semantic Hierarchy", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8267277926206589}]}], "abstractContent": [{"text": "We introduce anew model of selectional preference induction.", "labels": [], "entities": [{"text": "selectional preference induction", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.8028408487637838}]}, {"text": "Unlike previous approaches , we provide a stochastic generation model for the words that appear as arguments of a predicate.", "labels": [], "entities": []}, {"text": "More specifically , we define a hidden Markov model with the general shape of a given semantic class hierarchy.", "labels": [], "entities": []}, {"text": "This model has a number of attractive features, among them that selectional preference can be seen as distributions over words.", "labels": [], "entities": []}, {"text": "However, unsupervised parameter estimation has proven problematic.", "labels": [], "entities": []}, {"text": "A central problem is word sense ambiguity in the training corpora.", "labels": [], "entities": [{"text": "word sense ambiguity", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7009177803993225}]}, {"text": "We describe attempts to modify the forward-backward algorithm, an EM algorithm, to handle such disam-biguation.", "labels": [], "entities": []}, {"text": "Although these attempts were unsuccessful at improving performance, we believe they give insight into the nature of the bottlenecks and into the behavior of the EM algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "We describe here an approach to inducing selectional preferences from text corpora.", "labels": [], "entities": []}, {"text": "In the traditional view, a predicate constrains its arguments by selecting for particular semantic classes, or concepts.", "labels": [], "entities": []}, {"text": "Selectional restriction of the traditional sort can be characterized as a relation p(v, r, c) over predicates v, syntactic roles r, and argument concepts c.", "labels": [], "entities": [{"text": "Selectional restriction", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7863409221172333}]}, {"text": "Individual instances (v, r, c) are selectional tuples. of preference of v for c with respect to role r.", "labels": [], "entities": []}, {"text": "Positive degrees of preference are intended to correlate with intuitive judgments of \"plausibility\" or \"typicality,\" and negative judgments are intended to correlate with intuitive judgments of \"implausibility.\"", "labels": [], "entities": []}, {"text": "We have chosen to characterize such selectional preference as a side-effect of a stochastic model for generating what we will call co-occurrence tuples: triples (v, r, n) for v a predicate, r a syntactic role, and n the headword of the argument filling the role r with respect to v.", "labels": [], "entities": []}, {"text": "An example of a co-occurrence tuple is (splatter, obj, water).", "labels": [], "entities": []}, {"text": "Co-occurrence tuples can be obtained from text corpora, and can be used to make inferences about the probability of selectional tuples.", "labels": [], "entities": []}, {"text": "For example, the co-occurrence tuple (splatter, obj, water) maybe taken as evidence for the selectional tuple (splatter, obj, FLUID).", "labels": [], "entities": [{"text": "FLUID", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.9854318499565125}]}, {"text": "More concretely, such co-occurrence tuples makeup the training corpora, from which we train our stochastic models.", "labels": [], "entities": []}, {"text": "For this study, we have used the British National Corpus (100M words), from which we have extracted co-occurrence tuples using the Cass parser.", "labels": [], "entities": [{"text": "British National Corpus (100M words)", "start_pos": 33, "end_pos": 69, "type": "DATASET", "confidence": 0.9177165031433105}]}, {"text": "By way of illustration, table 2 shows the values of n in tuples (eat, obj, n) along with their frequencies in the corpus.", "labels": [], "entities": []}, {"text": "This \"subcorpus\" would be used to train a stochastic model specific to the object role of the verb eat and is the first of two inputs to our induction process.", "labels": [], "entities": []}, {"text": "There are two problems with such training data: it is noisy and it contains ambiguity.", "labels": [], "entities": []}, {"text": "The noise is sometimes due to tagging or parsing errors, and sometimes due to metaphorical uses.", "labels": [], "entities": []}, {"text": "However, note that the \"good\" examples such as food and meal are much greater in number and frequency'.", "labels": [], "entities": [{"text": "frequency", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9861380457878113}]}, {"text": "Thus, the signal is stronger than the noise inmost cases and most reasonably robust training methods will be able to handle the noise.", "labels": [], "entities": []}, {"text": "The second problem, that of word sense ambiguity, is more difficult.", "labels": [], "entities": [{"text": "word sense ambiguity", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7786566019058228}]}, {"text": "The word bread in table 2 provides an example.", "labels": [], "entities": []}, {"text": "Bread can be used to refer to a food, e.g., the multigrain bread in Germany is wonderlul, but it can also refer to money, e.g., I could really use some bread since my car just broke down.", "labels": [], "entities": []}, {"text": "For this reason, it is not immediately clear which concepts the 14 tokens of bread provide evidence for.", "labels": [], "entities": []}, {"text": "If the wrong choice is made fora high frequency word, incorrect selectional preferences will result.", "labels": [], "entities": []}, {"text": "The model we propose represents this sort of uncertainty in a natural way: the two senses of bread are represented as different paths through a stochastic model, both of which generate the same observation.", "labels": [], "entities": []}, {"text": "This stochastic model is a hidden Markov model (HMM) which has the shape of a given semantic hierarchy.", "labels": [], "entities": []}, {"text": "In the work discussed here, we made use of the WordNet semantic hierarchy.", "labels": [], "entities": [{"text": "WordNet semantic hierarchy", "start_pos": 47, "end_pos": 73, "type": "DATASET", "confidence": 0.8842882712682089}]}, {"text": "This hierarchy is the second input to our induction process.", "labels": [], "entities": []}, {"text": "We hoped that the forward-backward algorithm, an EM algorithm, would properly disambiguate word senses in the training data as aside effect of its quest to maximize the likelihood of the training data given the model.", "labels": [], "entities": []}, {"text": "However, for reasons we will discuss in section 4, this was not the case.", "labels": [], "entities": []}, {"text": "In the following section we discuss work on selectional preference induction that also assumes as input (i) subcorpora corresponding to predicate role pair and (ii) a semantic class hierarchy.", "labels": [], "entities": [{"text": "selectional preference induction", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.7826046148935953}]}, {"text": "Then we formally define our stochastic model.", "labels": [], "entities": []}, {"text": "Next we look at a number of ultimately unsuccessful attempts to modify the forward-backward algorithm to perform effective word-sense disambiguation of the training data.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 123, "end_pos": 148, "type": "TASK", "confidence": 0.7397570610046387}]}, {"text": "Despite these problems we did obtain some encouraging results which we present at the end of the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Objects of eat in the BNC", "labels": [], "entities": [{"text": "BNC", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.8405699133872986}]}, {"text": " Table 3: Word Sense Disambiguation Results", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7048145333925883}]}]}