{"title": [{"text": "An Information-Theoretic Empirical Analysis of Dependency-Based Feature Types for Word Prediction Models", "labels": [], "entities": [{"text": "Word Prediction", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.7645182609558105}]}], "abstractContent": [{"text": "Over the years, many proposals have been made to incorporate assorted types of feature in language models.", "labels": [], "entities": []}, {"text": "However, discrepancies between training sets, evaluation criteria, algorithms, and hardware environments make it difficult to compare the models objectively.", "labels": [], "entities": []}, {"text": "In this paper, we take an information theoretic approach to select feature types in a systematic manner.", "labels": [], "entities": []}, {"text": "We describe a quantitative analysis of the information gain and the information redundancy for various combinations of feature types inspired by both dependency structure and bigram structure, using a Chinese treebank and taking word prediction as the object.", "labels": [], "entities": []}, {"text": "The experiments yield several conclusions on the predictive value of several feature types and feature types combinations for word prediction, which are expected to provide guidelines for feature type selection in language modeling.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 126, "end_pos": 141, "type": "TASK", "confidence": 0.7926163673400879}]}], "introductionContent": [{"text": "There are many types of features that a language model can use to predict a word in a sentence.", "labels": [], "entities": []}, {"text": "Standard n-gram models use the immediately preceding words.", "labels": [], "entities": []}, {"text": "Other fixed physical distance feature types may inspect word classes or parts of speech.", "labels": [], "entities": []}, {"text": "Grammatically-based feature types may also be used, such as the suizf}@cs.ust.hk incident syntactic and semantic relations or the other words involved in those relations.", "labels": [], "entities": []}, {"text": "Our ultimate aim is to determine which combination of feature types is optimal for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.6977458298206329}]}, {"text": "Unfortunately, the state of knowledge in this regard is very limited.", "labels": [], "entities": []}, {"text": "Many language models have been published inspired by one or more of these feature types I11121131141151, but discrepancies between training sets, evaluation criteria, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively.", "labels": [], "entities": [{"text": "I11121131141151", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.8972722291946411}]}, {"text": "The paper uses an information theoretic approach to select feature types for language modeling in a systematic manner.", "labels": [], "entities": []}, {"text": "We are concerned with quantitative analysis of the information quantity, information gain and the information redundancy for various feature type combinations in both dependency grammar structure and adjacent bigram structure.", "labels": [], "entities": []}, {"text": "The experiments yield a number of conclusions on the predictive value of various feature types and the combinations thereof, which can provide useful information on what level of performance gain can be expected in principle from a bigram model augmented with long distance dependency features.", "labels": [], "entities": []}, {"text": "The results are expected to provide a reliable reference for feature type selection in language modeling.", "labels": [], "entities": [{"text": "feature type selection", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.6213168203830719}]}, {"text": "We have used Chinese data for the experiments in this paper.", "labels": [], "entities": []}, {"text": "Strictly speaking, our I conclusions apply only to Chinese.", "labels": [], "entities": []}, {"text": "However, we actually expect very similar results on English, and all our preliminary experiments on English data do bear this out I61.", "labels": [], "entities": [{"text": "I61", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.6822993755340576}]}, {"text": "We believe the general methodology as well as many of the specific conclusions apply tO a wide range of languages.", "labels": [], "entities": []}, {"text": "We will begin by introducing an information theoretic framework for feature type selection and analysis.", "labels": [], "entities": [{"text": "feature type selection and analysis", "start_pos": 68, "end_pos": 103, "type": "TASK", "confidence": 0.6414930760860443}]}, {"text": "We then describe the experimental setup.", "labels": [], "entities": []}, {"text": "Finally, we discuss a number of claims deriving from the eXperimental evidence.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training corpus used in our experiments is a treebank consisting of Chinese primary school texts mjt12].", "labels": [], "entities": [{"text": "Chinese primary school texts mjt12", "start_pos": 72, "end_pos": 106, "type": "DATASET", "confidence": 0.7123905658721924}]}, {"text": "The basic statistics characterizing the training set are summarized in.", "labels": [], "entities": []}, {"text": "In the experiments, we use 80% of the above corpus as a training set for estimating the various co-occurrence probabilities, while 10% of the corpus is used as a testing set to compute the information gain, information quantity, and information redundancy.", "labels": [], "entities": [{"text": "information redundancy", "start_pos": 233, "end_pos": 255, "type": "TASK", "confidence": 0.7298378944396973}]}, {"text": "The feature types we used in the experiments are those shown in.", "labels": [], "entities": []}, {"text": "Our experiments aim to quantitatively establish the amount of information intrinsically present in each feature type, and the information gain of each feature type on the top of various baselines.", "labels": [], "entities": []}, {"text": "We were led to a number of conclusions on the predictive power of various feature types and feature types combinations, some in support of traditional linguistic intuition and some more surprising.", "labels": [], "entities": []}, {"text": "These observations provide guidelines for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8391626179218292}]}, {"text": "Below, we warm up with a well-known observation, and then move onto more focussed analysis.", "labels": [], "entities": []}, {"text": "It seems somehow obvious that R (\"~/gua4/hang\") should be more predictive for 0 (\"~ [] /di4tu2/map \") than B (the aspectual marker \"~/zheO\").", "labels": [], "entities": []}, {"text": "However, as is well known in speech recognition and statistical NLP research, the opposite turns out to be true.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8007606267929077}]}, {"text": "This is corroborated by the empirical information quantities shown in, which shows that B has the largest information quantity in all of the feature types.", "labels": [], "entities": []}, {"text": "That bigram features outperform the grammatically-based features is commonly attributed to the predictive power of lexical association.", "labels": [], "entities": []}, {"text": "For example, consider the sentence showed in, where O is \"~/zou3/walk\", then B is \" [] /gong lyuan2/garden\" and M is \" /cong2/from\".", "labels": [], "entities": []}, {"text": "Again, it seems that M (\",hJk /cong2/from\") ought to be more predictive to O (\" j~ /zou3/walk\") than B (\" ~_~ [] /gonglyuan2/garden\"), but from Consider the' following measurements from our experiments': IQ(R;O)=l.581 bits which is less than IQ(M;O)=2.237 bits, whereas IG(R;OIB)=0.683 bits which is greater than IG(M;OIB)=0.541 bits.", "labels": [], "entities": []}, {"text": "That is, given a baseline bigram model employing only B features, augmenting thei model with R features brings more information than augmenting it with M features.", "labels": [], "entities": []}, {"text": "Therefore, in principle, the language model which incorporates bigram and feature type R can achieve higher performance than the model which incorporates bigram and M.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the training corpus  Corpus Size (words)  Total Sentences(sentences)  Average Sentence Length (words)  Vocabulary Size(words)  POS Tags  Phrase Types", "labels": [], "entities": [{"text": "Average Sentence Length", "start_pos": 94, "end_pos": 117, "type": "METRIC", "confidence": 0.7235272626082102}, {"text": "Vocabulary Size(words)  POS Tags  Phrase Types", "start_pos": 127, "end_pos": 173, "type": "TASK", "confidence": 0.43474770916832817}]}]}