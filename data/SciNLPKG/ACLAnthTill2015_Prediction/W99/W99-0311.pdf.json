{"title": [{"text": "Discourse-level argumentation in scientific articles: human and automatic annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present a rhetorically defined annotation scheme which is part of our corpus-based method for the summari-sation of scientific articles.", "labels": [], "entities": [{"text": "summari-sation of scientific articles", "start_pos": 115, "end_pos": 152, "type": "TASK", "confidence": 0.908339187502861}]}, {"text": "The annotation scheme consists of seven non-hierarchical labels which model prototypical academic argumentation and expected intentional 'moves'.", "labels": [], "entities": []}, {"text": "Ina large-scale experiments with three expert coders, we found the scheme stable and reproducible.", "labels": [], "entities": []}, {"text": "We have built a resource consisting of 80 papers annotated by the scheme, and we show that this kind of resource can be used to train a system to automate the annotation work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Work on summarisation has suffered from alack of appropriately annotated corpora that can be used for building, training and evaluating summarisation systems.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.9883196353912354}]}, {"text": "Typically, corpus work in this area has taken as its starting point texts target summaries: abstracts written by the researchers, supplied by the original authors or provided by professional abstractors.", "labels": [], "entities": []}, {"text": "Training a summarisation system then involves learning the properties of sentences in those abstracts and using this knowledge to extract similax abstract-worthy sentences from unseen texts.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.9690984487533569}]}, {"text": "In this scenario, system performance or development progress can be evaluated by taking texts in a test sample and comparing the sentences extracted from these texts with the sentences in the target abstract.", "labels": [], "entities": []}, {"text": "But this approach has a number of shortcomings.", "labels": [], "entities": []}, {"text": "First, sentence extraction on its own is a very general methodology, which can produce extracts that are incoherent or under-informative especially when used for high-compression summarisation (i.e. reducing a document to a small percentage of its original size).", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.8140938580036163}]}, {"text": "It is difficult to overcome this problem, because once sentences have been extracted from the source text, the context that is needed for their interpretation is not available anymore and cannot be used to produce more coherent abstracts.", "labels": [], "entities": []}, {"text": "Our proposed solution to this problem is to extract sentences but also to classify them into one of a small number of possible argumentative roles, reflecting whether the sentence expresses a main goal of the source text, a shortcoming in someone else's work, etc.", "labels": [], "entities": []}, {"text": "The summarisation system can then use this information to generate template-like abstracts: Main goal of the text:...", "labels": [], "entities": []}, {"text": "; Builds on work by:...", "labels": [], "entities": []}, {"text": "Second, the question of what constitutes a useful gold standard has not yet been solved satisfactorily.", "labels": [], "entities": []}, {"text": "Researchers developing corpus resources for summarisation work have often defined their own gold standard, relying on their own intuitions (see, e.g. or have used abstracts supplied by authors or by professional abstractors as their gold standard (e.g..", "labels": [], "entities": [{"text": "summarisation", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9830065369606018}]}, {"text": "Neither approach is very satisfactory.", "labels": [], "entities": []}, {"text": "Relying only on your own intuitions inevitably creates a biased resource; indeed, report low agreement between human judges carrying out this kind of task.", "labels": [], "entities": []}, {"text": "On the other hand, using abstracts as targets is not necessarily a good gold standard for comparison of the systems' results, although abstracts are the only kind of gold standard that comes for free with the papers.", "labels": [], "entities": []}, {"text": "Even if the abstracts are written by professional abstractors, there are considerable differences in length, structure, and information content.", "labels": [], "entities": []}, {"text": "This is due to differences in the common abstract presentation style in different disciplines and to the projected use of the abstracts (cf..", "labels": [], "entities": []}, {"text": "In the case of our corpus, an additional problem was the fact that the abstracts are written by the authors themselves and thus susceptible to differences in individual writing style.", "labels": [], "entities": []}, {"text": "For the task of summarisation and relevance decision between similar papers, however, it is essential that the information contained in the gold standard is comparable between papers.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9939040541648865}]}, {"text": "In our approach, the vehicle for comparability of information is similarity in argumentative roles of the associated sentences.", "labels": [], "entities": []}, {"text": "We argue that it is more difficult to find the kind of information that preserves similarity of argumentative roles, and that it is not guaranteed that it will occur in the abstract.", "labels": [], "entities": []}, {"text": "A related problem concerns fair evaluation Of the extraction methodology.", "labels": [], "entities": []}, {"text": "The evaluation of extracted material necessarily consists of a comparison of sentences, whereas one would really want to compare the informational content of the extracted sentences and the target abstract.", "labels": [], "entities": []}, {"text": "Thus it will often be the case that a system extracts a sentence which in that form does not appear in the supplied abstract (resulting in a low performance score) but which is nevertheless an abstract-worthy sentence.", "labels": [], "entities": []}, {"text": "The mismatch often arises simply because a similar idea is expressed in the supplied abstract in a very different form.", "labels": [], "entities": []}, {"text": "But comparison of content is difficult to perform: it would require sentences to be mapped into some underlying meaning representations and then comparing these to the representations of the sentences in the gold standard.", "labels": [], "entities": [{"text": "comparison of content", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8922606309254965}]}, {"text": "As this is technically not feasible, system performance is typically performed against a fixed gold standard (e.g. the aforementioned abstracts), which is ultimately undesirable.", "labels": [], "entities": []}, {"text": "Our proposed solution to this problem is to build a corpus which details not only what the abstractworthy sentences are but also what their argumentative role is.", "labels": [], "entities": []}, {"text": "This corpus can then be used as a resource to build a system to similarly classify sentences in unseen texts, and to evaluate that system.", "labels": [], "entities": []}, {"text": "This paper reports on the development of a set of such argumentative roles that we have been using in our work.", "labels": [], "entities": []}, {"text": "In particular, we employ human intuition to annotate argumentatively defined information.", "labels": [], "entities": []}, {"text": "We ask our annotators to classify every sentence in the source text in terms of its argumentative role (e.g. that it expresses the main goal of the source text, or identifies open problems in earlier work, etc).", "labels": [], "entities": []}, {"text": "Under this scenario, system evaluation is no longer a comparison of extracted sentences against a supplied abstract, or against a single sentence that was chosen as expressing (e.g.) the main goal of the source text.", "labels": [], "entities": []}, {"text": "Instead, every sentence in the source text which expresses the main goal will have been identified, and the system's performance is evaluated against that classification.", "labels": [], "entities": []}, {"text": "Of course, having someone annotate text in this way may still lead to a biased or careless annotation.", "labels": [], "entities": []}, {"text": "We therefore needed an annotation scheme which is simple enough to be usable in a stable and intuitive way for several annotators.", "labels": [], "entities": []}, {"text": "This paper also reports on how we tested the stability of the annotation scheme we developed.", "labels": [], "entities": []}, {"text": "A second design criterion for our annotation scheme was that we wanted the roles to be annotated automatically.", "labels": [], "entities": []}, {"text": "This paper reports on preliminary results which show that the annotation process can indeed be automated.", "labels": [], "entities": []}, {"text": "To summarise, we have argued that discourse structure information will improve summarisation.", "labels": [], "entities": [{"text": "summarise", "start_pos": 3, "end_pos": 12, "type": "TASK", "confidence": 0.9907512068748474}, {"text": "summarisation", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.9879051446914673}]}, {"text": "Other researchers ( have argued similarly, although most previous work on discourse-based summarisation follows a different discourse model, namely Rhetorical Structure Theory.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory", "start_pos": 148, "end_pos": 175, "type": "TASK", "confidence": 0.7249715526898702}]}, {"text": "In contrast to RST, we stress the importance of rhetorical moves which are global to the argumentation of the paper, as opposed to more local RST-type relations.", "labels": [], "entities": [{"text": "RST", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9508566856384277}]}, {"text": "Our categories are not hierarchical, and they are much less fine-grained than RST-relations.", "labels": [], "entities": []}, {"text": "As mentioned above, we wanted them to a) provide context information for flexible summarisation, b) provide a higher degree of comparability between papers, and c) provide a fairer evaluation of superficially different sentences.", "labels": [], "entities": []}, {"text": "In the rest of this paper, we will first describe how we chose the categories (section 2).", "labels": [], "entities": []}, {"text": "Second, we had to construct training and evaluation material such that we could be sure that the proposed categorisation yielded a reliable resource of annotated text to train a system against, a gold standard.", "labels": [], "entities": []}, {"text": "The human annotation experiments are reported in section 3.", "labels": [], "entities": []}, {"text": "Finally, in section 4, we describe some of the automated annotation work which we have started recently and which uses a corpus annotated according to our scheme as its training material.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}