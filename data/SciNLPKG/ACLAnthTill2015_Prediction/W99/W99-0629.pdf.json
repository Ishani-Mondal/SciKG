{"title": [], "abstractContent": [{"text": "In this paper we discuss cascaded Memory-Based grammatical relations assignment.", "labels": [], "entities": [{"text": "cascaded Memory-Based grammatical relations assignment", "start_pos": 25, "end_pos": 79, "type": "TASK", "confidence": 0.7114905893802643}]}, {"text": "In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal).", "labels": [], "entities": []}, {"text": "In the last stage, we assign grammatical relations to pairs of chunks.", "labels": [], "entities": []}, {"text": "We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.", "labels": [], "entities": [{"text": "relation finder", "start_pos": 159, "end_pos": 174, "type": "TASK", "confidence": 0.7625287175178528}]}], "introductionContent": [{"text": "When dealing with large amounts of text, finding structure in sentences is often a useful preprocessing step.", "labels": [], "entities": []}, {"text": "Traditionally, full parsing is used to find structure in sentences.", "labels": [], "entities": [{"text": "full parsing", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.5703083723783493}]}, {"text": "However, full parsing is a complex task and often provides us with more information then we need.", "labels": [], "entities": [{"text": "full parsing", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.601665735244751}]}, {"text": "For many tasks detecting only shallow structures in a sentence in a fast and reliable way is to be preferred overfull parsing.", "labels": [], "entities": [{"text": "detecting only shallow structures in a sentence", "start_pos": 15, "end_pos": 62, "type": "TASK", "confidence": 0.7250264797891889}]}, {"text": "For example, in information retrieval it can be enough to find only simple NPs and VPs in a sentence, for information extraction we might also want to find relations between constituents as for example the subject and object of a verb.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7414964437484741}, {"text": "information extraction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.8079975247383118}]}, {"text": "In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence.", "labels": [], "entities": [{"text": "Memory-Based (MB) shallow parsing", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.5871700793504715}]}, {"text": "Several MB modules have been developed in previous work, such as: a POS tagger), a chunker) and a grammatical relation (GR) assigner).", "labels": [], "entities": [{"text": "grammatical relation (GR) assigner", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.6335382014513016}]}, {"text": "The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers?", "labels": [], "entities": []}, {"text": "What is the effect of cascading?", "labels": [], "entities": []}, {"text": "Will errors at a lower level percolate to higher modules?", "labels": [], "entities": []}, {"text": "Recently, many people have looked at cascaded and/or shallow parsing and GR assignment. is one of the first who proposed to split up parsing into several cascades.", "labels": [], "entities": [{"text": "GR assignment.", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.8529027998447418}]}, {"text": "He suggests to first find the chunks and then the dependecies between these chunks.", "labels": [], "entities": []}, {"text": "Grefenstette (1996) describes a cascade of finite-state transducers, which first finds noun and verb groups, then their heads, and finally syntactic functions.", "labels": [], "entities": []}, {"text": "describe a partially automated annotation tool which constructs a complete parse of a sentence by recursively adding levels to the tree.) use cascaded processing for full parsing with good results.", "labels": [], "entities": []}, {"text": "applied Memory-Based Sequence Learning (MBSL) to NP chunking and subject/object identification.", "labels": [], "entities": [{"text": "Memory-Based Sequence Learning (MBSL)", "start_pos": 8, "end_pos": 45, "type": "TASK", "confidence": 0.709030439456304}, {"text": "NP chunking", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.8188851475715637}, {"text": "subject/object identification", "start_pos": 65, "end_pos": 94, "type": "TASK", "confidence": 0.658533364534378}]}, {"text": "However, their subject and object finders are independent of their chunker (i.e. not cascaded).", "labels": [], "entities": []}, {"text": "Drawing from this previous work we will explicitly study the effect of adding steps to the grammatical relations assignment cascade.", "labels": [], "entities": [{"text": "grammatical relations assignment cascade", "start_pos": 91, "end_pos": 131, "type": "TASK", "confidence": 0.6925437301397324}]}, {"text": "Through experiments with cascading several classifiers, we will show that even using imperfect classifiers can improve overall performance of the cascaded classifier.", "labels": [], "entities": []}, {"text": "We illustrate this claim on the task of finding grammatical relations (e.g. subject, object, locative) to verbs in text.", "labels": [], "entities": []}, {"text": "The GR assigner uses several sources of information step by step such as several types of XP chunks (NP, VP, PP, ADJP and ADVP), and adverbial functions assigned to these chunks (e.g. temporal, local).", "labels": [], "entities": [{"text": "GR assigner", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8992698490619659}]}, {"text": "Since not all of these entities are predicted reliably, it is the question whether each source leads to an improvement of the overall GR assignment.", "labels": [], "entities": []}, {"text": "In the rest of this paper we will first briefly describe Memory-Based Learning in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3.1, we discuss the chunking classifiers that we later use as steps in the cascade.", "labels": [], "entities": []}, {"text": "Section 3.2 describes the basic GR classifier.", "labels": [], "entities": [{"text": "GR classifier", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.8375701308250427}]}, {"text": "Section 3.3 presents the architecture and results of the cascaded GR assignment experiments.", "labels": [], "entities": [{"text": "GR assignment", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.8810446560382843}]}, {"text": "We discuss the results in Section 4 and conclude with Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have already seen from the example that the level of structure in the input text can influence the composition of the instances.", "labels": [], "entities": []}, {"text": "We are interested in the effects of different sorts of partial structure in the input data on the classification performance of the final classifier.", "labels": [], "entities": []}, {"text": "Therefore, we ran a series of experiments.", "labels": [], "entities": []}, {"text": "The classification task was always that of finding grammatical relations to verbs and performance was always measured by precision and recall on those relations (the test set contained 45825 relations).", "labels": [], "entities": [{"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.99943608045578}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9984448552131653}]}, {"text": "The amount of structure in the input data varied.", "labels": [], "entities": []}, {"text": "shows the results of the experiments.", "labels": [], "entities": []}, {"text": "In the first experiment, only POS tagged input is used.", "labels": [], "entities": []}, {"text": "Then, NP chunks are added.", "labels": [], "entities": []}, {"text": "Other sorts of chunks are inserted at each subsequent step.", "labels": [], "entities": []}, {"text": "Finally, the adverbial function labels are added.", "labels": [], "entities": []}, {"text": "We can see that the more structure we add, the better precision and recall of the grammatical relations get: precision increases from 60.7% to 74.8%, recall from 41.3% to 67.9%.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9993926286697388}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9985024929046631}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9995288848876953}, {"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.9990859031677246}]}, {"text": "This in spite of the fact that the added information is not always correct, because it was predicted for the test material on the basis of the training material by the classitiers described in Section 3.1.", "labels": [], "entities": []}, {"text": "As we have seen in, especially ADJP and ADVP chunks and adverbial function labels did not have very high precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9992330074310303}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9982345104217529}]}], "tableCaptions": [{"text": " Table 4: Results of grammatical relation assignment with more and more structure in the test data  added by earlier modules in the cascade. Columns show the number of features in the instances,  the mlmber of instances constructed front the test input, the average distance between the verb  and the tbcus element, precision, recall and FZ=i over all relations, and F\u00a2~=i over some selected  relations.", "labels": [], "entities": [{"text": "grammatical relation assignment", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6793244282404581}, {"text": "precision", "start_pos": 316, "end_pos": 325, "type": "METRIC", "confidence": 0.9995417594909668}, {"text": "recall", "start_pos": 327, "end_pos": 333, "type": "METRIC", "confidence": 0.999424934387207}, {"text": "FZ", "start_pos": 338, "end_pos": 340, "type": "METRIC", "confidence": 0.9924356341362}, {"text": "F\u00a2~=i", "start_pos": 367, "end_pos": 372, "type": "METRIC", "confidence": 0.9446013768513998}]}, {"text": " Table 5: Comparison of performance of several modules on realistic input structurally enriched by  previous modules in the cascade) vs. on \"perfect\" input (enriched with partial treebank annotation).  For PPs, this means perfect POS tags and chunk labels/boundaries, for ADVFUNC additionally  perfect PP chunks, for GR assignment also perfect ADVFUNC labels.", "labels": [], "entities": []}]}