{"title": [{"text": "HMM Specialization with Selective Lexicalization*", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a technique which complements Hidden Markov Models by incorporating some lexicalized states representing syntactically uncommon words.", "labels": [], "entities": []}, {"text": "'Our approach examines the distribution of transitions, selects the uncommon words, and makes lexicalized states for the words.", "labels": [], "entities": []}, {"text": "We perfor'med a part-of-speech tagging experiment on the Brown corpus to evaluate the resultant language model and discovered that this technique improved the tagging accuracy by 0.21% at the 95% level of confidence.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.6920349597930908}, {"text": "Brown corpus", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9401737153530121}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.963935136795044}]}], "introductionContent": [{"text": "Hidden Markov 'Models are widely used for statistical language modelling in various fields, e.g., part-of-speech tagging or speech recognition.", "labels": [], "entities": [{"text": "statistical language modelling", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.746551513671875}, {"text": "part-of-speech tagging", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.8036261200904846}, {"text": "speech recognition", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7912989556789398}]}, {"text": "The models are based on Markov assumptions, which make it possible to view the language prediction as a Markov process.", "labels": [], "entities": [{"text": "language prediction", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7295696139335632}]}, {"text": "'In general, we make the firstorder Markov ass'umptions that the current tag is only dependant on the previous tag and that the current word is only dependant on the current tag.", "labels": [], "entities": []}, {"text": "These are very 'strong' assumptions, so that the first-order Hidden Markov Models have the advantage of drastically reducing the number of its parameters.", "labels": [], "entities": []}, {"text": "On the other hand, the assumptions restrict the model from utilizing enough constraints provided by the local context and the resultant model consults only a single category 'as the contex.", "labels": [], "entities": []}, {"text": "A lot of effort has been devoted in the past to makeup for the insufficient contextual information of the first-order probabilistic model.", "labels": [], "entities": []}, {"text": "The second order Hidden Markov Models with \" The research underlying this paper was supported t) 3\" research grants fl'om Korea appropriate smoothing techniques show better performance than the first order models and is considered a state-of-the-art technique.", "labels": [], "entities": []}, {"text": "The complexity of the model is however relatively very high considering the small improvement of the performance.", "labels": [], "entities": [{"text": "complexity", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9588525891304016}]}, {"text": "Garside describes IDIOMTAG ( which is a component of a part-ofspeech tagging system named CLAWS.", "labels": [], "entities": []}, {"text": "ID-IOMTAG serves as a front-end to the tagger and modifies some initially assigned tags in order to reduce the amount of ambiguity to be dealt with by the tagger.", "labels": [], "entities": []}, {"text": "IDIOMTAG can look at any combination of words and tags, with or without intervening words.", "labels": [], "entities": [{"text": "IDIOMTAG", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8424204587936401}]}, {"text": "By using the IDIOMTAG, CLAWS system improved tagging accuracy from 94% to 96-97%.", "labels": [], "entities": [{"text": "tagging", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.9569492936134338}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.983161449432373}]}, {"text": "However, the manual-intensive process of producing idiom tags is very expensive although IDIOMTAG proved fruitful.) describes a technique of augmenting the Hidden Markov Models for part-of-speech tagging by the use of networks.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 181, "end_pos": 203, "type": "TASK", "confidence": 0.7298662066459656}]}, {"text": "Besides the original states representing each part-of-speech, the network contains additional states to reduce the noun/adjective confusion, and to extend the context for predicting past participles from preceding auxiliary verbs when they are separated by adverbs.", "labels": [], "entities": [{"text": "predicting past participles from preceding auxiliary verbs", "start_pos": 171, "end_pos": 229, "type": "TASK", "confidence": 0.860038127217974}]}, {"text": "By using these additional states, the tagging system improved the accuracy from 95.7% to 96.0%.", "labels": [], "entities": [{"text": "tagging", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.9711400866508484}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9997286200523376}]}, {"text": "However, the additional context is chosen by analyzing the tagging errors manually.", "labels": [], "entities": []}, {"text": "An automatic refining technique for Hidden Markov Models has been proposed by Brants.", "labels": [], "entities": []}, {"text": "It starts with some initial first order Markov Model.", "labels": [], "entities": []}, {"text": "Some states of the model are selected to be split or merged to take into account their predecessors.", "labels": [], "entities": []}, {"text": "As a result, each of new states represents a extended context.", "labels": [], "entities": []}, {"text": "With this technique, Brants reported a performance cquivalent to the second order Hidden Markov Models.", "labels": [], "entities": []}, {"text": "In this paper, we present an automatic refining technique for statistical language models.", "labels": [], "entities": []}, {"text": "First, we examine the distribution of transitions of lexicalized categories.", "labels": [], "entities": []}, {"text": "Next, we breakout the uncommon ones from their categories and make new states for them.", "labels": [], "entities": []}, {"text": "All processes are automated and the user has only to determine the extent of the breaking-out.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have tested our technique through part-ofspeech tagging eXperiments with the Hidden Markov Models which are variously lexicalized.", "labels": [], "entities": [{"text": "part-ofspeech tagging eXperiments", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.7501434286435446}]}, {"text": "In ordcr to conduct the tagging experiments, we divided the whole Brown (tagged) corpus containing 53,887 sentences words) into two parts.", "labels": [], "entities": [{"text": "Brown (tagged) corpus containing 53,887 sentences words", "start_pos": 66, "end_pos": 121, "type": "DATASET", "confidence": 0.8588208357493082}]}], "tableCaptions": [{"text": " Table 3: Overview of Our Corpora", "labels": [], "entities": []}, {"text": " Table 4. The second column shows  that words to the ratio of 52% (the number  of 57,808) are not ambiguous. The tagger at- tempts to resolve the ambiguity of the remain- ing words.", "labels": [], "entities": []}, {"text": " Table 4: Amount of Ambiguity of Test Set", "labels": [], "entities": [{"text": "Amount", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9915950894355774}, {"text": "Ambiguity", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.7946244478225708}]}]}