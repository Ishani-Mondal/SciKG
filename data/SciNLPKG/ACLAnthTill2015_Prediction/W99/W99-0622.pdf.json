{"title": [{"text": "Guiding a Well-Founded Parser with Corpus Statistics", "labels": [], "entities": [{"text": "Corpus Statistics", "start_pos": 35, "end_pos": 52, "type": "DATASET", "confidence": 0.8870154023170471}]}], "abstractContent": [{"text": "We present a parsing system built from a handwritten lexicon ~ and grammar, and trained on a selection of the Brown Corpus.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.98652184009552}]}, {"text": "On the sentences it can parse, the parser performs as well as purely corpus-based parsers.", "labels": [], "entities": []}, {"text": "Its advantage lies in the fact that its syntactic analyses readily support semantic interpretation.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.7481428980827332}]}, {"text": "Moreover, the system's handwritten foundation allows fora more fully lexicalized probabilistic model, i.e. one sensitive to co-occurrence of lexical heads of phrase constituents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical approaches to parsing have received a great deal of attention over recent years.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.985970675945282}]}, {"text": "The availability of large tagged and syntactically bracketed corpora make the programmatic extraction of lexica and grammars feasible.", "labels": [], "entities": [{"text": "programmatic extraction of lexica and grammars", "start_pos": 78, "end_pos": 124, "type": "TASK", "confidence": 0.8016189932823181}]}, {"text": "Researchers have tackled parsing by substituting these automatically derived resources for hand-coded ones.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9706616997718811}]}, {"text": "While these approaches have had some success to date, their usability as parsers in systems for natural language understanding is suspect.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.6582213342189789}]}, {"text": "1 The 'reconstruction of Treebank-style bracketings does not serve as an adequate basis for semantic interpretation.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.8170839548110962}]}, {"text": "The phrase structure rules are too numerous, and the analyses too coarse (especially at the lower levels) to allow association of deterministic semantic rules with ph~:ase structure rules.", "labels": [], "entities": []}, {"text": "Chaxniak himself (1997b) notes that most of the parses constructed by a \"wide-coverage\" grammar axe \"pretty senseless\".", "labels": [], "entities": []}, {"text": "1Collins, Ch~niak, etc.", "labels": [], "entities": [{"text": "1Collins", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9219805598258972}]}, {"text": "make no claims about their programs being Well suited as parsers for language understanding applications.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.7310490757226944}]}, {"text": "Certainly, this type of parsing has had success t'o-date in applications such as Information Retrieval.", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9636485576629639}, {"text": "Information Retrieval", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.767150342464447}]}, {"text": "As an example, consider the fiat NP structures that are in the Penn Treebank (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9969561994075775}]}, {"text": "Nouns, determiners, and adjectives are all sisters of each other in the syntactic annotation, e.g. (NP (DT the) (JJ mechanical) (NN engineering) (NN industry)).", "labels": [], "entities": []}, {"text": "A parser which constructs structures such as this fails to solve an ambiguity problem that has generally been considered syntactic: Are we talking about the industry of mechanical engineering, or is the entire engineering industry perceived as mechanical?", "labels": [], "entities": []}, {"text": "If our goal is language understanding, including semantic interpretation, the Treebank bracketings must be considered underspecified.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7547422647476196}, {"text": "semantic interpretation", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.7310300022363663}]}, {"text": "We describe here a system which combines hand-coded linguistic resources with corpusderived probabilistic information to enable (fairly) wide-coverage syntactic parsing.", "labels": [], "entities": [{"text": "wide-coverage syntactic parsing", "start_pos": 137, "end_pos": 168, "type": "TASK", "confidence": 0.5846144656340281}]}, {"text": "Most importantly, the use of these linguistic resources allows fora better-informed probabilistic model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We sequestered 421 sentences from our corpus of 4892 sentences (with trees and sense-tags), and used the balance for training the probabilities in equation.", "labels": [], "entities": []}, {"text": "These 4892 are the parseable segment of the 16,374 trees for which we were able to \"match up\" the Treebank syntactic annotation with the semantic concordance.", "labels": [], "entities": []}, {"text": "(Random errors and inconsistencies seem to account for why not all 19,843 trees align.", "labels": [], "entities": []}, {"text": "In fact, these 19,843 themselves exclude all trees which appear to be headlines or some other irregular text.", "labels": [], "entities": []}, {"text": "We do not, however, exclude any trees on the basis of the type of their root category.", "labels": [], "entities": []}, {"text": "The corpus contains sentences as well as verb phrases, noun phrases, etc..)", "labels": [], "entities": []}, {"text": "We then tested the parser varying two binary parameters: \u2022 whether or not the semantic backoff procedure was used --If not, an unobserved conditioning event would immediately have us drop the lexical information.", "labels": [], "entities": []}, {"text": "For example, (X,w / would immediately be backed off to simply (X).", "labels": [], "entities": []}, {"text": "\u2022 whether or not we simply estimated the joint probability P(~tl X,w, rulename) as I-~i y (kt ] X, w, rulename ).", "labels": [], "entities": []}, {"text": "This we will call the \"binary\" assumption, as opposed to \"rt-ary\".", "labels": [], "entities": []}, {"text": "Effectively, it means that each daughter's headword sense is introduced independently of the others.", "labels": [], "entities": []}, {"text": "display the results for the four different settings, along with the results fora straight PCFG model (as a baseline).", "labels": [], "entities": []}, {"text": "Note that t, our threshold parameter from above, was set to 10 for these experiments.", "labels": [], "entities": []}, {"text": "Labeled precision and recall are the same as in other reports on statistical parsing: they measure how often a particular syntactic category was correctly calculated to span a particular portion of the input.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9862316250801086}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9996311664581299}, {"text": "statistical parsing", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.6905863881111145}]}, {"text": "Recall that our corpus aWe actually stop short of this in our estimations.", "labels": [], "entities": []}, {"text": "We search upward for the top-most nodes in WordNet, but we do not continue to the synthetic T0P node.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9772498607635498}]}, {"text": "Instead, we drop the lexeme from the conditioning information and restart the search. was derived using a hand-crafted grammar.", "labels": [], "entities": []}, {"text": "It makes sense, then, to add an additional criterion for correctness: we can check the actual expansions (rulenames) used and see if they were correct.", "labels": [], "entities": []}, {"text": "This metric speaks to an issue raised by Charniak (1997b) when he notes that the rule NP -> NP NP has (at least) two different interpretations: one for appositive NPs and one for \"unit\" phrases like \"5 dollars a share\".4 A hand-written grammar will differentiate these two constructions.", "labels": [], "entities": []}, {"text": "Thus shows precision and recall figures for this more strict criterion, for the four models in question plus PCFG again as a baseline.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9996765851974487}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9996711015701294}, {"text": "PCFG", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.7635392546653748}]}, {"text": "Also note that the \"full\" model, using both rLary lexical statistics and semantic backoff, performs (statistically) significantly better than both of the models which do not use semantic backoff.", "labels": [], "entities": []}, {"text": "The lone exception is that the precision of the labeled bracketings is not significantly different for the \"full\" model and the \"minimal\" model.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9989042282104492}]}, {"text": "5 4In fact there should be syntactic differences for these two constructions, since phrases like \"the dollars the share\" are syntactically ill-formed unit noun phrases.", "labels": [], "entities": []}, {"text": "~Two-sided tests were used, with o\u00a2 = 0.05.", "labels": [], "entities": [{"text": "o\u00a2", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9362928867340088}]}, {"text": "Interestingly, the \"minimal\" model is not significantly different from either of the two models gotten by adding one o/ rt-axy statistics or semantic backoff.", "labels": [], "entities": []}, {"text": "The improvement is only significant when both features are added.", "labels": [], "entities": []}, {"text": "Our results for word sense disambiguation (obtained as a by-product of parsing) are shown in.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 16, "end_pos": 41, "type": "TASK", "confidence": 0.7355800867080688}]}, {"text": "Clearly, using WordNet to back off semantically enables the parser to do a better job at getting Senses right.", "labels": [], "entities": []}, {"text": "The sense recall figures for the two models which use semantic backoff are significantly better than for those models which do not.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8942139148712158}]}, {"text": "Additionally, the improvement over baseline is significantly better for those models which use semantic backoff (11 percentage points improvement) than for those which do not (4 points better", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Syntactic Expansion Precision/Recall  Results", "labels": [], "entities": [{"text": "Syntactic Expansion Precision/Recall", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.8392102122306824}]}, {"text": " Table 3. Clearly, using WordNet to back off  semantically enables the parser to do a better  job at getting Senses right. The sense recall  figures for the two models which use semantic  backoff are significantly better than for those  models which do not. Additionally, the im- provement over baseline is significantly better  for those models which use semantic backoff (11  percentage points improvement) than for those  which do not (4 points better", "labels": [], "entities": [{"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.740454912185669}]}, {"text": " Table 3: Sense Recall: baseline/model. The  baseline results:are gotten by choosing the most  frequent sense for the word, given the part of  speech assigned by the parser. (Hence it may be  different across,different models for the parser.)", "labels": [], "entities": []}]}