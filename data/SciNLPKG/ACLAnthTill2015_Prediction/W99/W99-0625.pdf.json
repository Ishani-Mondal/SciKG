{"title": [{"text": "Detecting Text Similarity over Short Passages: Exploring Linguistic Feature Combinations via Machine Learning", "labels": [], "entities": [{"text": "Detecting Text Similarity", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9039974808692932}]}], "abstractContent": [{"text": "We present anew composite similarity metric that combines information from multiple linguistic indicators to measure semantic distance between pairs of small textual units.", "labels": [], "entities": []}, {"text": "Several potential features are investigated and an opti-real combination is selected via machine learning.", "labels": [], "entities": []}, {"text": "We discuss a more restrictive definition of similarity than traditional, document-level and information retrieval-oriented, notions of similarity, and motivate it by showing its relevance to the multi-document text summariza-tion problem.", "labels": [], "entities": [{"text": "multi-document text summariza-tion problem", "start_pos": 195, "end_pos": 237, "type": "TASK", "confidence": 0.6821357607841492}]}, {"text": "Results from our system are evaluated against standard information retrieval techniques, establishing that the new method is more effective in identifying closely related textual units.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.6954299062490463}]}], "introductionContent": [], "datasetContent": [{"text": "For evaluation, we use a set of articles already classified into topical subsets which we obtained from the Reuters part of the 1997 pilot Topic Detection and Tracking (TDT) corpus.", "labels": [], "entities": [{"text": "Reuters part", "start_pos": 108, "end_pos": 120, "type": "DATASET", "confidence": 0.9285473823547363}, {"text": "Topic Detection and Tracking (TDT)", "start_pos": 139, "end_pos": 173, "type": "TASK", "confidence": 0.8227806006159101}]}, {"text": "The TDT corpus, developed by NIST and DARPA, is a collection of 16,000 news articles from Reuters and CNN where many of the articles and transcripts have been manually grouped into 25 categories each of which corresponds to a single event (see http://morph.ldc. uperm, edu/Cat alog/LDC98T25, html).", "labels": [], "entities": [{"text": "TDT corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8436254560947418}]}, {"text": "Using the Reuters part of the corpus, we selected five of the larger categories and extracted all articles assigned to them from severM randomly chosen days, fora total of 30 articles.", "labels": [], "entities": [{"text": "Reuters part of the corpus", "start_pos": 10, "end_pos": 36, "type": "DATASET", "confidence": 0.8765516161918641}]}, {"text": "Since paragraphs in news stories tend to be short--typically one or two sentences--in this study we use paragraphs as our small text units, although sentences would also be a possibility.", "labels": [], "entities": []}, {"text": "In total, we have 264 text units and 10,345 comparisons between units.", "labels": [], "entities": []}, {"text": "As comparisons are made between all pairs of paragraphs from the same topic, the total number of comparisons is equal to where Ni is the number of paragraphs in all selected articles from topical category i.", "labels": [], "entities": []}, {"text": "Training of our machine learning component was done by three-fold cross-validation, randomly splitting the 10,345 pairs of paragraphs into three (almost) equally-sized subsets.", "labels": [], "entities": []}, {"text": "In each of the three runs, two of these subsets were used for training and one for testing.", "labels": [], "entities": []}, {"text": "To create a reference standard, the entire collection of 10,345 paragraph pairs was marked for similarity by two reviewers who were given our definition and detailed instructions.", "labels": [], "entities": [{"text": "similarity", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9688360691070557}]}, {"text": "Each re--viewer independently marked each pair of paragraphs as similar or not similar.", "labels": [], "entities": []}, {"text": "Subsequently, the two reviewers jointly examined eases where there was disagreement, discussed reasons, and reconciled the differences.", "labels": [], "entities": []}, {"text": "In order to independently validate our definition of similarity, we performed two additional experiments.", "labels": [], "entities": []}, {"text": "In the first, we asked three additional judges to determine similarity fora random sample of 40 paragraph pairs.", "labels": [], "entities": [{"text": "similarity", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9770057201385498}]}, {"text": "High agreement between judges would indicate that our definition of similarity reflects an objective reality and can be mapped unambiguously to an operational procedure for marking text units as similar or not.", "labels": [], "entities": []}, {"text": "At the same time, it would also validate the judgments between text units that we use for our experiments (see Section 5.1).", "labels": [], "entities": []}, {"text": "In this task, judges were given the opportunity to provide reasons for claiming similarity or dissimilarity, and comments on the task were logged for future analysis.", "labels": [], "entities": []}, {"text": "The three additional judges agreed with the manually marked and standardized corpus on 97.6% of the comparisons.", "labels": [], "entities": []}, {"text": "Unfortunately, approximately 97% (depending on the specific experiment) of the comparisons in both our model and the subsequent validation experiment receive the value \"not similar\".", "labels": [], "entities": []}, {"text": "This large percentage is due to our finegrained notion of similarity, and is parallel to what happens in randomly sampled IR collections, since in that case most documents will not be relevant to any given query.", "labels": [], "entities": []}, {"text": "Nevertheless, we can account for the high probability of inter-reviewer agreement expected by chance, 0.97.0.97+ (1-0.97)-(1-0.97) --0.9418, by referring to the kappa statistic.", "labels": [], "entities": []}, {"text": "The kappa statistic is defined as where PA is the probability that two reviewers agree in practice, and P0 is the probability that they would agree solely by chance.", "labels": [], "entities": [{"text": "kappa statistic", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.649595782160759}, {"text": "PA", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9712196588516235}]}, {"text": "In our case, PA = 0.976, P0 = 0.9418, and K = 0.5876, indicating that the observed agreement by the reviewers is indeed significant.", "labels": [], "entities": [{"text": "PA", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9984215497970581}, {"text": "P0", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9671162962913513}]}, {"text": "2 If P0 is estimated from the particular sample used in this experiment rather than from our entire corpus, it would be only 0.9, producing a value of 0.76 for K.", "labels": [], "entities": [{"text": "P0", "start_pos": 5, "end_pos": 7, "type": "METRIC", "confidence": 0.9780393242835999}]}, {"text": "In addition to this validation experiment that used randomly sampled pairs of paragraphs (and reflected the disproportionate rate of occurrence of dissimilar pairs), we performed a balanced experiment by randomly selecting 50 of the dissimilar pairs and 50 of the similar pairs, in a manner that guaranteed generation of an independent sample.", "labels": [], "entities": []}, {"text": "3 Pairs in this subset were rated for similarity by two additional independent reviewers, who agreed on their decisions 91% of the time, versus 50% expected by chance; in this case, K ---0.82.", "labels": [], "entities": [{"text": "similarity", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9973342418670654}, {"text": "K ---0.82", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9109455148379008}]}, {"text": "Thus, we feel confident in the reliability of our annotation 2K is always between 0 and I, with 0 indicating no better agreement than expected by chance and 1 indicating perfect agreement.", "labels": [], "entities": [{"text": "reliability", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9837225079536438}]}, {"text": "3To guarantee independence, pairs of paragraphs were randomly selected for inclusion in the sample but a pair (A, B) was immediately rejected if there were paragraphs X1,...,X,~ for n > 0 such that all pairs (A, X1), (X1, X2), \u2022 \u2022 \u2022, (Xn, B) had already been included in the sample.", "labels": [], "entities": []}, {"text": "process, and can use the annotated corpus to assess the performance of our similarity measure and compare it to measures proposed earlier in the information retrieval literature.", "labels": [], "entities": []}, {"text": "Our system was able to recover 36.6% of the similar paragraphs with 60.5% precision, as shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9992281198501587}]}, {"text": "In comparison, the unmodified TF*IDF approach obtained only 32.6% precision when recall is 39.1%, i.e., close to our system's recall; and only 20.8% recall at precision of 62.2%, comparable to our classifier's aWe used version 11.0 of SMART, released in July 1992.: Experimental results for different similarity metrics.", "labels": [], "entities": [{"text": "TF*IDF", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.4970170557498932}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9992050528526306}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9926233291625977}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.997579038143158}, {"text": "recall", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.995642900466919}, {"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9590786099433899}, {"text": "SMART", "start_pos": 235, "end_pos": 240, "type": "DATASET", "confidence": 0.7357631921768188}]}, {"text": "For comparison purposes, we list the average recall, precision, and accuracy obtained by TF*IDF and SMART at the two points in the precision-recall curve identified for each method in the text (i.e., the point where the method's precision is most similar to ours, and the point where its recall is most similar to ours).", "labels": [], "entities": [{"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9989497065544128}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9985601305961609}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9995304346084595}, {"text": "TF*IDF", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.4702983796596527}, {"text": "SMART", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.5571538209915161}, {"text": "precision-recall", "start_pos": 131, "end_pos": 147, "type": "METRIC", "confidence": 0.9816615581512451}, {"text": "precision", "start_pos": 229, "end_pos": 238, "type": "METRIC", "confidence": 0.9815785884857178}, {"text": "recall", "start_pos": 288, "end_pos": 294, "type": "METRIC", "confidence": 0.9943706393241882}]}], "tableCaptions": [{"text": " Table 2: Statistics for a selected subset of features. Performance measures are occasionally given  multiple times for the same feature and normalization option, highlighting the effect of different  decision thresholds.", "labels": [], "entities": []}]}