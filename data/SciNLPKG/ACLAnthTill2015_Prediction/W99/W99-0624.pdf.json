{"title": [{"text": "Lexical ambiguity and Information Retrieval revisited", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.7569923996925354}]}], "abstractContent": [{"text": "A number of previous experiments on the role of lexical ambiguity, in Information Retrieval are reproduced on the'IR-Semcor test collection (derived from Semcor), where both queries and documents are hand-tagged ;with phrases, Part-Of-Speech and WordNet 1.5 senses.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.756687343120575}]}, {"text": "Our results indicate that a) Word Sense Disambigua-tion can be more beneficial to Information Retrieval than the experiments of Sanderson (1994) with artificially ambiguous pseudo-words suggested, b) Part-Of-Speech tagging does not seem to help Improving retrieval, even if it is manually annotated, c) Using phrases as indexing terms is not a good strategy if no partial credit is given to the phrase components.", "labels": [], "entities": [{"text": "Word Sense Disambigua-tion", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.6828007201353709}, {"text": "Information Retrieval", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.812058836221695}, {"text": "Part-Of-Speech tagging", "start_pos": 200, "end_pos": 222, "type": "TASK", "confidence": 0.7278737425804138}, {"text": "Improving retrieval", "start_pos": 245, "end_pos": 264, "type": "TASK", "confidence": 0.9141522943973541}]}], "introductionContent": [{"text": "A major difficulty to experiment with lexical ambiguity issues in Information Retrieval is always to differentiate the effects of the indexing and retrieval strategy being tested from the effects of tagging errors.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.6941365301609039}]}, {"text": "Some examples are: 1.", "labels": [], "entities": []}, {"text": "In (, a sophisticated retrieval system based on conceptual similarity resultled in a decrease of IR performance.", "labels": [], "entities": [{"text": "IR", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9550130367279053}]}, {"text": "It was not possible, however, to distinguish the effects of the strategy and the effects of automatic Wordl Sense Disambiguation (WSD) errors.", "labels": [], "entities": [{"text": "Wordl Sense Disambiguation (WSD)", "start_pos": 102, "end_pos": 134, "type": "TASK", "confidence": 0.6830648829539617}]}, {"text": "In (, a similar strategy and a combination of manual disambiguation and very short documents -image captions-pioduced, however, an improvement of IR perforinance.", "labels": [], "entities": [{"text": "IR", "start_pos": 146, "end_pos": 148, "type": "TASK", "confidence": 0.9565058350563049}]}, {"text": "2. In (, discriminating word senses with differefit Part-Of-Speech (as annotated by the Church :POS tagger) also harmed retrieval efficiency.", "labels": [], "entities": []}, {"text": "Krovetz noted than more than half of the words in a dictionary that differ in POS are related in meaning, but he could not decide whether the decrease of performance was due to the loss of such semantic relatedness or to automatic POS tagging errors.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 231, "end_pos": 242, "type": "TASK", "confidence": 0.8397767543792725}]}, {"text": "3. In, the problem of discerning the effects of differentiating word senses from the effects of inaccurate disambiguation was overcome using artificially created pseudo-words (substituting, for instance, all occurrences of banana or kalashnikov for banana/kalashnikov) that could be disambiguated with 100% accuracy (substituting banana/kalashnikov back to the original term in each occurrence, either banana or kalashnikov).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 307, "end_pos": 315, "type": "METRIC", "confidence": 0.9956369996070862}]}, {"text": "He found that IR processes were quite resistant to increasing degrees of lexical ambiguity, and that disambiguation harmed IR efficiency if performed with less that 90% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9945354461669922}]}, {"text": "The question is whether real ambiguous words would behave as pseudo-words.", "labels": [], "entities": []}, {"text": "4. In () it was shown that sense discriminations extracted from the test collections may enhance text retrieval.", "labels": [], "entities": [{"text": "text retrieval", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.8108811378479004}]}, {"text": "However, the static sense inventories in dictionaries or thesauri -such as WordNet-have not been used satisfactorily in IR.", "labels": [], "entities": [{"text": "WordNet-have", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.9545649290084839}, {"text": "IR", "start_pos": 120, "end_pos": 122, "type": "TASK", "confidence": 0.9877610206604004}]}, {"text": "For instance, in, manual expansion of TREC queries with semantically related words from WordNet only produced slight improvements with the shortest queries.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.965888261795044}]}, {"text": "In order to deal with these problems, we designed an IR test collection which is hand annotated with Part-Of-Speech and semantic tags from WordNet 1.5.", "labels": [], "entities": [{"text": "WordNet 1.5", "start_pos": 139, "end_pos": 150, "type": "DATASET", "confidence": 0.9243500232696533}]}, {"text": "This collection was first introduced in () and it is described in Section 2.", "labels": [], "entities": []}, {"text": "This collection is quite small for current IR standards (it is only slightly bigger than the TIME collection), but offers a unique chance to analyze the behavior of semantic approaches to IR before scaling them up to TREC-size collections (where manual tagging is unfeasible).", "labels": [], "entities": [{"text": "TIME collection", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.8846514821052551}]}, {"text": "In (, we used the manual annotations in the IR-Semcor collection to show that indexing with WordNet synsets can give significant improvements to Text Retrieval, even for large queries.", "labels": [], "entities": [{"text": "IR-Semcor collection", "start_pos": 44, "end_pos": 64, "type": "DATASET", "confidence": 0.9709191024303436}, {"text": "Text Retrieval", "start_pos": 145, "end_pos": 159, "type": "TASK", "confidence": 0.7711330950260162}]}, {"text": "Such strategy works better than the synonymy expansion in, probably because it identifies synonym terms but, at the same time, it differentiates word senses.", "labels": [], "entities": [{"text": "synonymy expansion", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.749803900718689}]}, {"text": "In this paper we use a variant of the IR-Semcor collection to revise the results of the experiments by and) cited above.", "labels": [], "entities": [{"text": "IR-Semcor collection", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.8986168801784515}]}, {"text": "The first one is reproduced using both ambiguous pseudo-words and real ambiguous words, and the qualitative results compared.", "labels": [], "entities": []}, {"text": "This permits us to know if our results are compatible with Sanderson experiments or not.", "labels": [], "entities": []}, {"text": "The effect of lexical ambiguity on IR processes is discussed in Section 3, and the sensitivity of recall/precision to Word Sense Disambiguation errors in Section 4.", "labels": [], "entities": [{"text": "IR processes", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.9235715270042419}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9983730316162109}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.7133846879005432}]}, {"text": "Then, the experiment by Krovetz is reproduced with automatic and manually produced POS annotations in Section 5, in order to discern the effect of annotating POS from the effect of erroneous annotations.", "labels": [], "entities": []}, {"text": "Finally, the richness of multiwords in WordNet 1.5 and of phrase annotations in the IR-Semcor collection are exploited in Section 6 to test whether phrases are good indexing terms or not.", "labels": [], "entities": [{"text": "WordNet 1.5", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.9459395706653595}, {"text": "IR-Semcor collection", "start_pos": 84, "end_pos": 104, "type": "DATASET", "confidence": 0.9431943595409393}]}], "datasetContent": [], "tableCaptions": []}