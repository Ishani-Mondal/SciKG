{"title": [], "abstractContent": [{"text": "This paper describes how to automatically extract grounding features and segment a dialogue into discourse units, once the dialogue has been annotated with the DR/ backward-and forward-looking tags.", "labels": [], "entities": []}, {"text": "Such an approach eliminates the need for separate annotation of grounding, making dialogue annotation quicker and removing a possible source of error.", "labels": [], "entities": []}, {"text": "A preliminary test of the mapping against a human annotator is presented.", "labels": [], "entities": []}], "introductionContent": [{"text": "The annotation scheme (AC97) developed by the Discourse Research Initiative's Backward-and Forward-Looking Group (henceforth referred to as the BF scheme) provides a set of tags that can be applied to individual utterances in a dialogue, describing the utterance's illocutionary force.", "labels": [], "entities": []}, {"text": "The BF scheme provides a standard top-level tag set that allows researchers to reuse corpora that have been annotated for other projects, and also allows tags to be refined by individual projects to provide detail on particular phenomena being studied.", "labels": [], "entities": [{"text": "BF", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.7121241092681885}]}, {"text": "There area number of dialogue features that are of interest to researchers, and for which tagging schemes have been developed.", "labels": [], "entities": []}, {"text": "One feature that we are concerned with is grounding, the mechanism by which dialogue participants augment their mutual beliefs.", "labels": [], "entities": []}, {"text": "In his dissertation work (Tra94), Traum establishes a set of tags to describe grounding behavior, and then uses this taxonomy of grounding acts to describe a computational model of how dialogue participants achieve a state of mutual understanding.", "labels": [], "entities": []}, {"text": "Traum's model describes how grounding acts can be combined to form discourse units, segments of a dialogue that correspond to individual contributions to the common ground.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to determine whether the mapping we propose here results in accurate grounding annotation, we wrote a Perl script to perform the mapping on SGML-format files containing dialogues annotated with the BF tags.", "labels": [], "entities": []}, {"text": "We used the script on a set of four TRAINS-93 dialogues containing a total of 325 utterances, that had been previously tagged with BF tags (HA95; CA97).", "labels": [], "entities": [{"text": "HA95; CA97", "start_pos": 140, "end_pos": 150, "type": "DATASET", "confidence": 0.6866565545399984}]}, {"text": "The procedure for tagging the dialogues with BF tags was to have an annotator segment and annotate the dialogue, pass the segmented (but untagged) dialogue to a second annotator to tag independently, and finally for the two annotators to meet and produce a reconciled version of the tagged dialogue.", "labels": [], "entities": []}, {"text": "To evaluate the quality of the tags that were output by the script, we had a human annotator tag the 0: \"Partial Credit\" Scores same four TRAINS-93 dialogues with grounding acts.", "labels": [], "entities": []}, {"text": "Our grounding annotator is a computational linguist familiar with the concept of grounding but with no prior knowledge of Tranm's coding scheme, the BF coding scheme, or the mapping scheme we were using.", "labels": [], "entities": [{"text": "BF coding scheme", "start_pos": 149, "end_pos": 165, "type": "DATASET", "confidence": 0.8610000212987264}]}, {"text": "Before performing the annotation task, the annotator read Traum's descriptions of the grounding tags, tagged a preliminary dialogue (found in Traum's dissertation), and compared the tags he assigned to those assigned by Traum.", "labels": [], "entities": []}, {"text": "show the similarity of the human annotator's grounding tags to those automatically derived.", "labels": [], "entities": []}, {"text": "The analysis is split into two parts to deal with the ability of annotators to give an utterance multiple labels.", "labels": [], "entities": []}, {"text": "show a per tag analysis.", "labels": [], "entities": []}, {"text": "If both the annotators (the human and the Perl script) gave a tag such as INIT to an utterance (in addition to possibly other tags) then it is counted as agreement with respect to the INIT tag.", "labels": [], "entities": [{"text": "INIT", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9221553206443787}]}, {"text": "shows the number of times a tag appeared and the number of times there was disagreement.", "labels": [], "entities": []}, {"text": "See (Car96; SC88) for more details on these measures and the significance levels listed.", "labels": [], "entities": [{"text": "Car96; SC88)", "start_pos": 5, "end_pos": 17, "type": "DATASET", "confidence": 0.9566702544689178}, {"text": "significance", "start_pos": 61, "end_pos": 73, "type": "METRIC", "confidence": 0.984027624130249}]}, {"text": "The kappa of the \"All-or-nothing\" analysis is somewhat low compared with the 0.67 standard for tentative conclusions and the 0.8 standard for reliable results as reported in (Car96).", "labels": [], "entities": [{"text": "kappa", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.998150110244751}, {"text": "Car96)", "start_pos": 175, "end_pos": 181, "type": "DATASET", "confidence": 0.9466791450977325}]}, {"text": "The \"partial credit\" analysis is more favorable as the kappas for always starts anew discourse unit whether or not it also acknowledges a previous discourse unit.", "labels": [], "entities": []}, {"text": "Thus, the partial credit analysis is likely to be closer to the actual reliability we want to measure.", "labels": [], "entities": [{"text": "reliability", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.9666461944580078}]}, {"text": "The remaining \"partial credit\" kappas have low significance levels indicating that more examples are needed to calculate these measures.", "labels": [], "entities": []}, {"text": "Another limitation of this study was that technical papers were used for annotator training rather than an annotation manual designed to explain how tags apply in different situations.", "labels": [], "entities": []}, {"text": "This was especially problematic when several tags seemed to apply at once.", "labels": [], "entities": []}, {"text": "The BF tags themselves were not perfect as explained in (CA97).", "labels": [], "entities": [{"text": "BF tags", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.7043500989675522}]}, {"text": "Kappas for these annotations varied from the lowest at 0.15 to 0.77 for the highest.", "labels": [], "entities": [{"text": "Kappas", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9217369556427002}]}, {"text": "Given these limitations, the results of this experiment are promising.", "labels": [], "entities": []}, {"text": "An annotation manual needs to be developed for labeling grounding and more dialogs need to be labeled.", "labels": [], "entities": [{"text": "labeling grounding", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8967374265193939}]}, {"text": "When these sources of confusion are addressed, analysis of remaining differences will reveal any minor changes necessary to the mapping.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: \"Partial Credit\" Analysis", "labels": [], "entities": [{"text": "Partial Credit\"", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.9157969951629639}]}, {"text": " Table 4: \"Partial Credit\" Scores", "labels": [], "entities": [{"text": "Partial Credit\" Scores", "start_pos": 11, "end_pos": 33, "type": "METRIC", "confidence": 0.6296669617295265}]}]}