{"title": [{"text": "Corpus-Based Anaphora Resolution Towards Antecedent Preference", "labels": [], "entities": [{"text": "Corpus-Based Anaphora Resolution", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5498818854490916}]}], "abstractContent": [{"text": "In this paper we propose a corpus-based approach to anaphora resolution combining a machine learning method and statistical information.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7959020435810089}]}, {"text": "First, a decision tree trained on an annotated corpus determines the coreference relation of a given anaphor and antecedent candidates and is utilized as a filter in order to reduce the number of potential candidates.", "labels": [], "entities": []}, {"text": "In the second step, preference selection is achieved by taking into account the frequency information of coreferential and non-referential pairs tagged in the training corpus as well as distance features within the current discourse.", "labels": [], "entities": [{"text": "preference selection", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.7357653677463531}]}, {"text": "Preliminary experiments concerning the resolution of Japanese pronouns in spoken-language dialogs result in a success rate of 80.6%.", "labels": [], "entities": [{"text": "resolution of Japanese pronouns in spoken-language dialogs", "start_pos": 39, "end_pos": 97, "type": "TASK", "confidence": 0.8261023078645978}]}], "introductionContent": [{"text": "Coreference information is relevant for numerous NLP systems.", "labels": [], "entities": []}, {"text": "Our interest in anaphora resolution is based on the demand for machine translation systems to be able to translate (possibly omitted) anaphoric expressions in agreement with the morphosyntactic characteristics of the referred object in order to prevent contextual misinterpretations.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8092084228992462}]}, {"text": "So far various approaches 1 to anaphora resolution have been proposed.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.818045437335968}]}, {"text": "In this paper a machine learning approach (decision tree) is combined with a preference selection method based on the frequency information of non-/coreferential pairs tagged in the corpus as well as distance features within the current discourse.", "labels": [], "entities": []}, {"text": "The advantage of machine learning approaches is that they result in modular anaphora resolution systems automatically trainable from a corpus with no 1See section 4 fora more detailed comparison with related research. or only a minimal amount of human intervention.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8016558885574341}]}, {"text": "In the case of decision trees, we do have to provide information about possible antecedent indicators (syntactic, semantic, and pragmatic features) contained in the corpus, but the relevance of features for the resolution task is extracted automatically from the training data.", "labels": [], "entities": []}, {"text": "Machine learning approaches using decision trees proposed so far have focused on preference selection criteria directly derived from the decision tree results.", "labels": [], "entities": []}, {"text": "The work described in utilized a decision tree capable of judging which one of two given anaphor-antecedent pairs is \"better\".", "labels": [], "entities": []}, {"text": "Due to the lack of a strong assumption on \"transitivity\", however, this sorting algorithm is more like a greedy heuristic search as it maybe unable to find the \"best\" solution.", "labels": [], "entities": []}, {"text": "The preference selection fora single antecedent in) is based on the maximization of confidence values returned from a pruned decision tree forgiven anaphor-candidate pairs.", "labels": [], "entities": []}, {"text": "However, decision trees are characterized by an independent learning of specific features, i.e., relations between single attributes cannot be obtained automatically.", "labels": [], "entities": []}, {"text": "Accordingly, the use of dependency factors for preference selection during decision tree training requires that the artificially created attributes expressing these dependencies be defined.", "labels": [], "entities": []}, {"text": "However, this not only extends human intervention into the automatic learning procedure (i.e., which dependencies are important?), but can also result in some drawbacks on the contextual adaptation of preference selection methods.", "labels": [], "entities": []}, {"text": "The preference selection in our approach is based on the combination of statistical frequency information and distance features in the discourse.", "labels": [], "entities": []}, {"text": "Therefore, our decision tree is not applied directly to the task of preference selection, but aims at the elimination of irrelevant candidates based on the knowledge obtained from the training data.", "labels": [], "entities": [{"text": "preference selection", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7011065036058426}]}, {"text": "The decision tree is trained on syntactic (lexical word attributes), semantic, and primitive discourse (distance, frequency) information and determines the coreferential relation between an anaphor and antecedent Candidate in the given context.", "labels": [], "entities": []}, {"text": "Irrelevant antecedent candidates are filtered out, achieving a noise reduction for the preference selection algorithm.", "labels": [], "entities": []}, {"text": "A preference value is assigned to each \" potential anaphor-candidate pair depending on the proportion of non-/coreferential occurrences of the pair in the training corpus (frequency ratio) and the relative position of both elements in the discourse (distance).", "labels": [], "entities": []}, {"text": "The candidate with the maximal preference value is resolved as the antecedent of the anaphoric expression.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation of the experimental results described in this section we use F-measure metrics calculated by the recall and precision of the system performance.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9862943291664124}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9992541670799255}, {"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9909658432006836}]}, {"text": "Let ~]t denote the total number of tagged 4In order to keep the formula simple the frequency types are omitted (cf. anaphor-antecedent pairs contained in the test data, El the number of these pairs passing the decision tree filter, and ~ the number of correctly selected antecedents.", "labels": [], "entities": []}, {"text": "During evaluation we distinguish three classes: whether the correct antecedent is the first element of the candidate list (f), is in the candidate list (i), or is filtered out by the decision tree (o).", "labels": [], "entities": []}, {"text": "The metrics F, recall (R) and precision (P) are defined as follows: In order to prove the feasibility of our approach we compare the four preference selection methods listed in.", "labels": [], "entities": [{"text": "F", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.9965547323226929}, {"text": "recall (R)", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9510390013456345}, {"text": "precision (P)", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.9597007930278778}]}], "tableCaptions": [{"text": " Table 2: Frequency and distance dependency", "labels": [], "entities": []}]}