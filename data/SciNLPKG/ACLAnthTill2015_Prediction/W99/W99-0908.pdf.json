{"title": [{"text": "Text Classification by Bootstrapping with Keywords, EM and Shrinkage", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7306659817695618}, {"text": "Shrinkage", "start_pos": 59, "end_pos": 68, "type": "TASK", "confidence": 0.4716090261936188}]}], "abstractContent": [{"text": "When applying text classification to complex tasks, it is tedious and expensive to hand-label the large amounts of training data necessary for good performance.", "labels": [], "entities": [{"text": "text classification", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7145531177520752}]}, {"text": "This paper presents an alternative approach to text classification that requires no labeled documentsi instead, it uses a small set of keywords per class, a class hierarchy and a large quantity of easily-obtained unlabeled documents.", "labels": [], "entities": [{"text": "text classification", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7675293982028961}]}, {"text": "The keywords are used to assign approximate labels to the unlabeled documents by term-matching.", "labels": [], "entities": []}, {"text": "These preliminary labels become the starting point fora bootstrap-ping process that learns a naive Bayes clas-sifier using Expectation-Maximization and hierarchical shrinkage.", "labels": [], "entities": []}, {"text": "When classifying a complex data set of computer science research papers into a 70-leaf topic hierarchy , the keywords alone provide 45% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.998386025428772}]}, {"text": "The classifier learned by bootstrap-ping reaches 66% accuracy, a level close to human agreement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9990543723106384}]}], "introductionContent": [{"text": "When provided with enough labeled training examples, a variety of text classification algorithms can learn reasonably accurate classifiers.", "labels": [], "entities": [{"text": "text classification", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7392610907554626}]}, {"text": "However, when applied to complex domains with many classes, these algorithms often require extremely large training sets to provide useful classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.7954612374305725}]}, {"text": "Creating these sets of labeled data is tedious and expensive, since typically they must be labeled by a person.", "labels": [], "entities": []}, {"text": "This leads us to consider learning algorithms that do not require such large amounts of labeled data.", "labels": [], "entities": []}, {"text": "While labeled data is difficult to obtain, unlabeled data is readily available and plentiful.", "labels": [], "entities": []}, {"text": "show in a theoretical framework that unlabeled data can indeed be used to improve classification, although it is exponentially less valuable than labeled data.", "labels": [], "entities": [{"text": "classification", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.972236156463623}]}, {"text": "Fortunately, unlabeled data can often be obtained by completely automated methods.", "labels": [], "entities": []}, {"text": "Consider the problem of classifying news articles: a short Perl script and a night of automated Internet downloads can fill a hard disk with unlabeled examples of news articles.", "labels": [], "entities": [{"text": "classifying news articles", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.9233343799908956}]}, {"text": "In contrast, it might take several days of human effort and tedium to label even one thousand of these.", "labels": [], "entities": []}, {"text": "In previous work ) it has been shown that with just a small number of labeled documents, text classification error can be reduced by up to 30% when the labeled documents are augmented with a large collection of unlabeled documents.", "labels": [], "entities": [{"text": "text classification", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7474938035011292}, {"text": "error", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.777229905128479}]}, {"text": "This paper considers the task of learning text classifiers with no labeled documents at all.", "labels": [], "entities": []}, {"text": "Knowledge about the classes of interest is provided in the form of a few keywords per class and a class hierarchy.", "labels": [], "entities": []}, {"text": "Keywords are typically generated more quickly and easily than even a small number of labeled documents.", "labels": [], "entities": []}, {"text": "Many classification problems naturally come with hierarchically-organized classes.", "labels": [], "entities": []}, {"text": "Our algorithm proceeds by using the keywords to generate preliminary labels for some documents by term-matching.", "labels": [], "entities": []}, {"text": "Then these labels, the hierarchy, and all the unlabeled documents become the input to a bootstrapping algorithm that produces a naive Bayes classifier.", "labels": [], "entities": []}, {"text": "The bootstrapping algorithm used in this paper combines hierarchical shrinkage and ExpectationMaximization (EM) with unlabeled data.", "labels": [], "entities": [{"text": "ExpectationMaximization (EM)", "start_pos": 83, "end_pos": 111, "type": "METRIC", "confidence": 0.9564482122659683}]}, {"text": "EM is an iterative algorithm for maximum likelihood estimation in parametric estimation problems with missing data.", "labels": [], "entities": [{"text": "maximum likelihood estimation", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.5331661701202393}]}, {"text": "In our scenario, the class labels of the documents are treated as missing data.", "labels": [], "entities": []}, {"text": "Here, EM works by first training a classifier with only the documents: A subset of Cora's topic hierarchy.", "labels": [], "entities": []}, {"text": "Each node contains its title, and the five most probable words, as calculated by naive Bayes and shrinkage with vertical word redistribution (.", "labels": [], "entities": []}, {"text": "Words among the initial keywords for that class are indicated in plain font; others are in italics.", "labels": [], "entities": []}, {"text": "preliminarily-labeled by the keywords, and then uses the classifier to re-assign probahilistically-weighted class labels to all the documents by calculating the expectation of the missing class labels.", "labels": [], "entities": []}, {"text": "It then trains anew classifier using all the documents and iterates.", "labels": [], "entities": []}, {"text": "We further improve classification by incorporating shrinkage, a statistical technique for improving parameter estimation in the face of sparse data.", "labels": [], "entities": [{"text": "classification", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.9663863182067871}]}, {"text": "When classes are provided in a hierarchical relationship, shrinkage is used to estimate new parameters by using a weighted average of the specific (but unreliable) local class estimates and the more general (but also more reliable) ancestors of the class in the hierarchy.", "labels": [], "entities": []}, {"text": "The optimal weights in the average are calculated by an EM process that runs simultaneously with the EM that is re-estimating the class labels.", "labels": [], "entities": []}, {"text": "Experimental evaluation of this bootstrapping approach is performed on a data set of thirty-thousand computer science research papers.", "labels": [], "entities": []}, {"text": "A 70-leaf hierarchy of computer science and a few keywords for each class are provided as input.", "labels": [], "entities": []}, {"text": "Keyword matching alone provides 45% accuracy.", "labels": [], "entities": [{"text": "Keyword matching", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.810012698173523}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9986728429794312}]}, {"text": "Our bootstrapping algorithm uses this as input and outputs a naive Bayes text classifier that achieves 66% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9973651766777039}]}, {"text": "Interestingly, this accuracy approaches estimated human agreement levels of 72%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9990597367286682}]}, {"text": "The experimental domain in this paper originates as part of the Ra research project, an effort to build domain-specific search engines on the Web with machine learning techniques.", "labels": [], "entities": []}, {"text": "Our demonstration system, Cora, is a search engine over computer science research papers ).", "labels": [], "entities": []}, {"text": "The bootstrapping classification algorithm described in this paper is used in Corn to place research papers into a Yahoo-like hierarchy specific to computer science.", "labels": [], "entities": [{"text": "bootstrapping classification", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6420846581459045}]}, {"text": "The-search engine, including this hierarchy, is publicly available at www.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we provide empirical evidence that bootstrapping a text classifier from unlabeled data can produce a high-accuracy text classifier.", "labels": [], "entities": []}, {"text": "As a test domain, we use computer science research papers.", "labels": [], "entities": []}, {"text": "We have created a 70-1ear hierarchy of computer science topics, part of which is shown in.", "labels": [], "entities": []}, {"text": "Creating the hierarchy took about 60 minutes, during which we examined conference proceedings, and explored computer science sites on the Web.", "labels": [], "entities": []}, {"text": "Selecting a few keywords associated with each node took about 90 minutes.", "labels": [], "entities": []}, {"text": "A test set was created by expert hand-labeling of a random sample of 625 research papers from the 30,682 papers in the Cora archive at the time we began these experiments.", "labels": [], "entities": [{"text": "Cora archive", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.8018231689929962}]}, {"text": "Of these, 225 (about one-third) did not fit into any category, and were discarded--resulting in a 400 document test set.", "labels": [], "entities": []}, {"text": "Labeling these 400 documents took about six hours.", "labels": [], "entities": []}, {"text": "Some of these papers were outside the area of computer science (e.g. astrophysics papers), but most of these were papers that with a more complete hierarchy would be considered computer science papers.", "labels": [], "entities": []}, {"text": "The class frequencies of the data are not too skewed; on the test set, the most populous class accounted for only 7% of the documents.", "labels": [], "entities": []}, {"text": "Each research paper is represented as the words of the title, author, institution, references, and abstract.", "labels": [], "entities": []}, {"text": "A detailed description of how these segments are automatically extracted is provided elsewhere (", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Classification results with different techniques:  keyword matching, human agreement, naive Bayes (NB),  and naive Bayes combined with hierarchical shrink- age (S), and EM. The classification accuracy (Acc),  and the number of labeled (Lab), keyword-matched  preliminarily-labeled (P-Lab), and unlabeled (Unlab)  documents used by each method are shown.", "labels": [], "entities": [{"text": "keyword matching", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7482624351978302}, {"text": "hierarchical shrink- age (S)", "start_pos": 145, "end_pos": 173, "type": "METRIC", "confidence": 0.7707973165171487}, {"text": "EM", "start_pos": 179, "end_pos": 181, "type": "METRIC", "confidence": 0.9819691777229309}, {"text": "accuracy", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.9331124424934387}, {"text": "Acc)", "start_pos": 212, "end_pos": 216, "type": "METRIC", "confidence": 0.948993444442749}]}]}