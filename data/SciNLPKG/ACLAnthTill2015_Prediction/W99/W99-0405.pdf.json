{"title": [{"text": "Modeling the language assessment process and result: Proposed architecture for automatic oral proficiency assessment", "labels": [], "entities": [{"text": "automatic oral proficiency assessment", "start_pos": 79, "end_pos": 116, "type": "TASK", "confidence": 0.6181414499878883}]}], "abstractContent": [{"text": "We outline challenges for modeling human language assessment in automatic systems, both in terms of the process and the reliability of the result.", "labels": [], "entities": [{"text": "human language assessment", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.6531057357788086}]}], "introductionContent": [{"text": "Computer-mediated language assessment appeals to educators and language evaluators because it has the potential for making language assessment widely available with minimal human effort and limited expense.", "labels": [], "entities": [{"text": "Computer-mediated language assessment", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.5887958407402039}]}, {"text": "Fairly robust results (n '~ 0.8) have been achieved in the commercial domain modeling the human rater results, with both the Electronic Essay Rater (erater) system for written essay scoring (, and the PhonePass pronunciation assessment.", "labels": [], "entities": [{"text": "PhonePass pronunciation", "start_pos": 201, "end_pos": 224, "type": "TASK", "confidence": 0.6292227506637573}]}, {"text": "There are at least three reasons why it is not possible to model the human rating process.", "labels": [], "entities": []}, {"text": "First, there is a mismatch between what the technology is able to handle and what people manipulate, especially in the assessment of speech features.", "labels": [], "entities": [{"text": "assessment of speech features", "start_pos": 119, "end_pos": 148, "type": "TASK", "confidence": 0.8158597648143768}]}, {"text": "Second, we lack a wellarticulated model of the human process, often characterized as holistic.", "labels": [], "entities": []}, {"text": "Certain assessment features have been identified, but their relative importance is not clear.", "labels": [], "entities": []}, {"text": "Furthermore, unlike automatic assessments, human raters of oral proficiency exams are trained to focus on competencies, which are difficult to enumerate.", "labels": [], "entities": []}, {"text": "In contrast, automatic assessments of spoken language fluency typically use some type of error counting, comparing duration, silence, speaking rate and pronunciation mismatches with native speaker models.", "labels": [], "entities": [{"text": "duration", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9670379161834717}]}, {"text": "There is, therefore, a basic tension within the field of computer-mediated language assessment, between modeling the assessment process of human raters or achieving comparable, consistent assessments, perhaps through different means.", "labels": [], "entities": [{"text": "computer-mediated language assessment", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.7085140446821848}]}, {"text": "Neither extreme is entirely satisfactory.", "labels": [], "entities": []}, {"text": "A spoken assessment system that achieves human-comparable performance based only, for example, on the proportion of silence in an utterance would seem not to be capturing a number of critical elements of language competence, regardless of how accurate the assessments are.", "labels": [], "entities": []}, {"text": "Such a system would also be severely limited in its ability to provide constructive feedback to language learners or teachers.", "labels": [], "entities": []}, {"text": "The e-rater system has received similar criticism for basing essay assessments on a number of largely lexical features, rather than on a deeper, more humanstyle rating process.", "labels": [], "entities": []}, {"text": "Thirdly, however, even if we could articulate and model human performance, it is not clear that we want to model all aspects of the human rating process.", "labels": [], "entities": []}, {"text": "For example, human performance varies due to fatigue.", "labels": [], "entities": []}, {"text": "Transcribers often inadvertently correct examinees' errors of omitted or incorrect articles, conjugations, or affixes.", "labels": [], "entities": []}, {"text": "These mistakes area natural effect of a cooperative listener; however, they result in an over-optimistic assessment of the speaker's actual proficiency.", "labels": [], "entities": []}, {"text": "We arguably do not wish to build this sort of cooperation into an automated assessment system, though it is likely desirable for other sorts of human-computer interaction systems.", "labels": [], "entities": []}, {"text": "Furthermore, if we focus on modeling human processes we may end up underutillzing the technology.", "labels": [], "entities": []}, {"text": "Balancing human-derived features with machine learning techniques may actually allow us to discuss more about the human rating process by making the entire process available for inspection and evaluation.", "labels": [], "entities": []}, {"text": "For example, if we are able to articulate human rating features, machine learning techniques may allow us to 'learn' the relative weighting of these features fora particular assessment value.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}