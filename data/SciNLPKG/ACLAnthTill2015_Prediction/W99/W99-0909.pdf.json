{"title": [{"text": "Unsupervised Lexical Learning with Categorial Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we report on an unsupervised approach to learning Categorial Grammar (CG) lexicons.", "labels": [], "entities": [{"text": "learning Categorial Grammar (CG) lexicons", "start_pos": 55, "end_pos": 96, "type": "TASK", "confidence": 0.7444296138627189}]}, {"text": "The learner is provided with a set of possible lexical CG categories, the forward and backward application rules of CG and un-marked positive only corpora.", "labels": [], "entities": []}, {"text": "Using the categories and rules, the sentences from the corpus are probabilistically parsed.", "labels": [], "entities": []}, {"text": "The parses and the history of previously parsed sentences are used to build a lexicon and annotate the corpus.", "labels": [], "entities": []}, {"text": "We report the results from experiments on a number of small generated corpora, that contain examples from subsets of the English language.", "labels": [], "entities": []}, {"text": "These show that the system is able to generate reasonable lexicons and provide accurately parsed corpora in the process.", "labels": [], "entities": []}, {"text": "We also discuss ways in which the approach can be scaled up to deal with larger and more diverse corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we discuss a potential solution to two problems in Natural Language Processing (NLP), using a combination of statistical and symbolic machine learning techniques.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.757445365190506}]}, {"text": "The first problem is learning the syntactic roles, or categories, of words of a language i.e. learning a lexicon.", "labels": [], "entities": []}, {"text": "Secondly, we discuss a method of annotating a corpus with parses.", "labels": [], "entities": []}, {"text": "The aim is to learn Categorial Grammar (CG) lexicons, starting from a set of lexical categories, the functional application rules of CG and an unannotated corpus of positive examples.", "labels": [], "entities": []}, {"text": "The CG formalism (discussed in Section 2) is chosen because it assigns distinct categories to words of different types, and the categories describe the exact syntactic role each word can play in a sentence.", "labels": [], "entities": []}, {"text": "This problem is similar to the unsupervised part of speech tagging work of, for example, and.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.7385295331478119}]}, {"text": "In Brill's work a lexicon containing the parts of speech available to each word is provided and a simple tagger attaches a complex tag to each word in the corpus, which represents all the possible tags that word can have.", "labels": [], "entities": []}, {"text": "Transformation rules are then learned which use the context of a word to determine which simple tag it should be assigned.", "labels": [], "entities": []}, {"text": "The results are good, generally achieving around 95% accuracy on large corpora such as the Penn Treebank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9991621971130371}, {"text": "Penn Treebank", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.9947352707386017}]}, {"text": "Kupiec uses an unsupervised version of the Baum-Welch algorithm, which is away of using examples to iteratively estimate the probabilities of a Hidden Markov Model for part of speech tagging.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 176, "end_pos": 190, "type": "TASK", "confidence": 0.718435749411583}]}, {"text": "Instead of supplying a lexicon, he places the words in equivalence classes.", "labels": [], "entities": []}, {"text": "Words in the same equivalence class must take one of a specific set of parts of speech.", "labels": [], "entities": []}, {"text": "This improves the accuracy of this algorithm to about the same level as Brill's approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9996335506439209}]}, {"text": "In both cases, the learner is provided with a large amount of background knowledge -either a complete lexicon or set of equivalence classes.", "labels": [], "entities": []}, {"text": "In the approach presented here, the most that is provided is a small partial lexicon.", "labels": [], "entities": []}, {"text": "In fact the system learns the lexicon.", "labels": [], "entities": []}, {"text": "The second problem -annotating the corpus -is solved because of the approach we use to learn the lexicon.", "labels": [], "entities": []}, {"text": "The system uses parsing to determine which are the correct lexical entries fora word, thus annotating the corpus with the parse derivations (also providing less probable parses if desired).", "labels": [], "entities": []}, {"text": "An example of another approach to doing this is the Fidditch parser of Hindle) (based on the deterministic parser of Marcus), which was used to annotate the Penn Treebank.", "labels": [], "entities": [{"text": "Fidditch parser of Hindle)", "start_pos": 52, "end_pos": 78, "type": "DATASET", "confidence": 0.8248461484909058}, {"text": "Penn Treebank", "start_pos": 157, "end_pos": 170, "type": "DATASET", "confidence": 0.9897462129592896}]}, {"text": "However, instead of learning the lexicon, a complete grammar and lexicon must be supplied to the Fidditch parser.", "labels": [], "entities": []}, {"text": "Our work also relates to CG induction, which has been attempted by a number of people.", "labels": [], "entities": [{"text": "CG induction", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.9435867667198181}]}, {"text": "Osborne) has an algorithm that.", "labels": [], "entities": []}, {"text": "learns a grammar for sequences of part-of-speech tags from a tagged corpora, using the Minimum Description Length (MDL) principle -a welldefined form of compression.", "labels": [], "entities": []}, {"text": "While this is a supervised setting of the problem, the use of the more formal approach to compression is of interest for future work.", "labels": [], "entities": []}, {"text": "Also, results of 97% coverage are impressive, even though the problem is rather simpler.", "labels": [], "entities": [{"text": "coverage", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9940783977508545}]}, {"text": "Kanazawa and use a unification based approach with a corpus annotated with semantic structure, which in CG is a strong indicator of the syntactic structure.", "labels": [], "entities": []}, {"text": "Unfortunately, they do not present results of experiments on natural language corpora and again the approach is essentially supervised.", "labels": [], "entities": []}, {"text": "Two unsupervised approaches to learning CGs are presented by Adriaans and Solomon.", "labels": [], "entities": []}, {"text": "Adriaans, describes a purely symbolic method that uses the context of words to define their category.", "labels": [], "entities": []}, {"text": "An oracle is required for the learner to test its hypotheses, thus providing negative evidence.", "labels": [], "entities": []}, {"text": "This would seem to be awkward from a engineering viewpoint i.e. how one could provide an oracle to achieve this, and implausible from a psychological point of view, as humans do not seem to receive such evidence.", "labels": [], "entities": []}, {"text": "Unfortunately, again no results on natural language corpora seem to be available.", "labels": [], "entities": []}, {"text": "Solomon's approach uses unannotated corpora, to build lexicons for simple CG.", "labels": [], "entities": []}, {"text": "He uses a simple corpora of sentences from children's books, with a slightly ad hoc and non-incremental, heuristic approach to developing categories for words.", "labels": [], "entities": []}, {"text": "The results show that a wide range of categories can be learned, but the current algorithm, as the author admits, is probably too naive to scale up to working on full corpora.", "labels": [], "entities": []}, {"text": "No results on the coverage of the CGs learned are provided.", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9622567296028137}]}, {"text": "In Section 3 we discuss our learner.", "labels": [], "entities": []}, {"text": "In Section 4 we describe experiments on three corpora containing examples of a subset of English and Section 5 contains the results, which are encouraging with respect to both problems.", "labels": [], "entities": []}, {"text": "Finally, in Section 6, we compare the results with the systems mentioned above and discuss ways the system can be expanded and larger scale experiments maybe carried out.", "labels": [], "entities": []}, {"text": "Next, however, we describe Categorial Grammar.", "labels": [], "entities": [{"text": "Categorial Grammar", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8089293539524078}]}], "datasetContent": [{"text": "Experiments were performed on three different corpora all containing only positive examples.", "labels": [], "entities": []}, {"text": "Experiments were performed with and without a partial lexicon of closed-class words (words of categories with a finite number of members) with fixed categories and probabilities, e.g. determiners and prepositions.", "labels": [], "entities": []}, {"text": "All experiments were carried out on a SGI Origin 2000.", "labels": [], "entities": [{"text": "SGI Origin 2000", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.8476112683614095}]}, {"text": "Experiments on Corpus 1 The first corpus was built from a context-free grammar (CFG), using a simple random generation algorithm.", "labels": [], "entities": []}, {"text": "The CFG (shown in) covers a range of simple declarative sentences with intransitive, transitive and ditransitive verbs and with adjectives.", "labels": [], "entities": [{"text": "CFG", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8759735822677612}]}, {"text": "The lexicon of the CFG contained 39 words with an example of noun-verb ambiguity.", "labels": [], "entities": [{"text": "CFG", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.8331789970397949}]}, {"text": "The corpus consisted of 500 such sentences.", "labels": [], "entities": []}, {"text": "As the size of the lexicon was small and there was only a small amount of ambiguity, it was unnecessary to supply the partial lexicon, but the experiment was carried out for comparison.", "labels": [], "entities": []}, {"text": "We also performed an experiment on 100 unseen examples to see how accurately they were parsed with the learned lexicon.", "labels": [], "entities": []}, {"text": "The results were manually verified to determine how many sentences were parsed correctly.", "labels": [], "entities": []}, {"text": "Experiments on Corpus 2 The second corpus was generated in the same way, but using extra rules (see) to include prepositions, thus making the fragment of English ex ( ).", "labels": [], "entities": []}, {"text": "ex([john, gave, john, a, boy]).", "labels": [], "entities": []}, {"text": "ex([a, dog, called, the, fish, a, small, ugly, desk]).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracies and timings for the different  learning experiments", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9987234473228455}, {"text": "timings", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.8442748785018921}]}, {"text": " Table 3: Unseen example parsing accuracy", "labels": [], "entities": [{"text": "Unseen example parsing", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6436386406421661}]}]}