{"title": [{"text": "Cross-Document Event Coreference: Annotations, Experiments, and Observations", "labels": [], "entities": [{"text": "Cross-Document Event Coreference", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6082812448342642}]}], "abstractContent": [{"text": "1 Abstract We have developed cross document event tracking technology that extends our earlier efforts in cross document person coreference.", "labels": [], "entities": [{"text": "cross document event tracking", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.6861082762479782}, {"text": "cross document person coreference", "start_pos": 106, "end_pos": 139, "type": "TASK", "confidence": 0.6740600019693375}]}, {"text": "The software takes class of events, like \"resignations\" and clusters documents that mention resignations into equivalence classes.", "labels": [], "entities": []}, {"text": "Documents belong to the same equivalence class if they mention the same \"resignation\" event, i.e. resignations involving the same person, time, and organization.", "labels": [], "entities": []}, {"text": "Other events evaluated include \"elections\" and \"espionage\" events.", "labels": [], "entities": []}, {"text": "Results range from 45-90% F-measure scores and we present a brief interannotator study for the \"elections\" data set.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9905930161476135}, {"text": "elections\" data set", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.8145695924758911}]}], "introductionContent": [{"text": "Events form the backbone of the reasons why people communicate to one another.", "labels": [], "entities": []}, {"text": "News is interesting and important because it describes actions, changes of state and new relationships between individuals.", "labels": [], "entities": []}, {"text": "While the communicative importance of described events is evident, the phenomenon has proved difficult to recognize and manipulate in automated ways (example: MUC information extraction efforts).", "labels": [], "entities": [{"text": "MUC information extraction", "start_pos": 159, "end_pos": 185, "type": "TASK", "confidence": 0.7739283839861552}]}, {"text": "We began this research program by developing algorithms to determine whether two mentions of a name, example \"John Smith\", in different documents actually referred to the same individual in the world.", "labels": [], "entities": [{"text": "John Smith\"", "start_pos": 110, "end_pos": 121, "type": "DATASET", "confidence": 0.8864360650380453}]}, {"text": "The system that we built was quite successful at resolving cross-document entoty coreference (Bagga, 98b).", "labels": [], "entities": [{"text": "cross-document entoty coreference", "start_pos": 59, "end_pos": 92, "type": "TASK", "confidence": 0.6442756752173106}, {"text": "Bagga, 98b)", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.899643287062645}]}, {"text": "We, therefore, decided to extend the system so that it could handle events as well.", "labels": [], "entities": []}, {"text": "Our goal was to determine whether events in separate documents, example \"resignations\", referred to the same event in the world (is it the same person resigning from the same company at the same time).", "labels": [], "entities": []}, {"text": "This new classof coreference has proved to be more challenging.", "labels": [], "entities": []}, {"text": "Below we will present our approach and results as follows: First we discuss how this research is different from Information Extraction and Topic Detection and Tracking.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.7657359838485718}, {"text": "Topic Detection and Tracking", "start_pos": 139, "end_pos": 167, "type": "TASK", "confidence": 0.8406257778406143}]}, {"text": "Then we present the core algorithm for cross document person coreference and our method of scoring the the system's output.", "labels": [], "entities": [{"text": "cross document person coreference", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.7092419564723969}]}, {"text": "The method for determining event reference follows with presentation and discussion of results.", "labels": [], "entities": [{"text": "determining event reference", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.695168137550354}]}, {"text": "We finish with an interannotator agreement experiment and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our cross-document coreference system on several data sets.", "labels": [], "entities": [{"text": "cross-document coreference", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.6599333882331848}]}, {"text": "The goal was to identify crossdocument coreference chains about the same event.", "labels": [], "entities": []}, {"text": "Figures 8 -15 shows the results from the experiments we conducted.", "labels": [], "entities": []}, {"text": "For each experiment conducted, the following conditions hold: \u2022 shows, for each data set, the number of articles chosen for the experiment.", "labels": [], "entities": []}, {"text": "\u2022 All of the articles in the data sets were chosen randomly from the 1996 and 1997 editions of the New York Times.", "labels": [], "entities": [{"text": "1996 and 1997 editions of the New York Times", "start_pos": 69, "end_pos": 113, "type": "DATASET", "confidence": 0.7328786253929138}]}, {"text": "The sole criterion used when choosing an article was the presence/ absence of the event of interest in the data set.", "labels": [], "entities": []}, {"text": "For example, an article containing the word \"election\" would be put in the elections data set.", "labels": [], "entities": [{"text": "elections data set", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.8415670593579611}]}, {"text": "\u2022 The answer keys for each data set were constructed manually, although scoring was automated.", "labels": [], "entities": []}, {"text": "shows for each data set, the optimal threshold, and the best precision, recall, and FMeasure obtained at that threshold.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9995101690292358}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9991078972816467}, {"text": "FMeasure", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9986670017242432}]}], "tableCaptions": []}