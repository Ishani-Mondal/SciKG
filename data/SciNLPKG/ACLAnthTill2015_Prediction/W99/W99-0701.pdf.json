{"title": [{"text": "Unsupervised Learning of Word Boundary with Description Length Gain", "labels": [], "entities": [{"text": "Unsupervised Learning of Word Boundary", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.5183384776115417}]}], "abstractContent": [{"text": "This paper presents an unsupervised approach to lexical acquisition with the goodness measure description length gain (DLG) formulated following classic information theory within the minimum description length (MDL) paradigm.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7442539036273956}, {"text": "goodness measure description length gain (DLG)", "start_pos": 77, "end_pos": 123, "type": "METRIC", "confidence": 0.835160106420517}]}, {"text": "The learning algorithm seeks for an optimal segmentation of an utterance that maximises the description length gain from the individual segments.", "labels": [], "entities": []}, {"text": "The resultant segments show a nice correspondence to lexical items (in particular, words) in a natural language like English.", "labels": [], "entities": []}, {"text": "Learning experiments on large-scMe corpora (e.g., the Brown corpus) have shown the effectiveness of both the learning algorithm and the goodness measure that guides that learning.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.8742533624172211}]}], "introductionContent": [{"text": "Detecting and handling unknown words properly has become a crucial issue in today's practical natural language processing (NLP) technology.", "labels": [], "entities": [{"text": "Detecting and handling unknown words properly", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.84489905834198}, {"text": "natural language processing (NLP)", "start_pos": 94, "end_pos": 127, "type": "TASK", "confidence": 0.7860733568668365}]}, {"text": "No matter how large the dictionary that is used in a NLP system, there can be many new words in running/real texts, e.g., in scientific articles, newspapers and Web pages, that the dictionary does not include.", "labels": [], "entities": []}, {"text": "Many such words are proper names and special terminology that provide critical information.", "labels": [], "entities": []}, {"text": "It is unreliable to rest on delimiters such as white spaces to detect new lexical units, because many basic lexical items contain one or more spaces, e.g., as in \"New York\", \"Hong Kong\" and \"hot dog\".", "labels": [], "entities": []}, {"text": "It appears that unsupervised learning techniques are necessary in order to alleviate the problem of unknown words in the NLP domain.", "labels": [], "entities": []}, {"text": "There have been a number of studies on lexical acquisition from language data of different types.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7955024838447571}]}, {"text": "Wolff attempts to infer word boundaries from artificially-generated natural language sentences, heavily relying on the co-occurrence frequency of adjacent characters [Wolff1975,.", "labels": [], "entities": []}, {"text": "Nevill-Manning's text compression program Sequitur can also identify word boundaries and gives a binary tree structure for an identified word.", "labels": [], "entities": [{"text": "text compression", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7009762823581696}]}, {"text": "de Marcken explores unsupervised lexical acquisition from Enghsh spoken and written corpora and from a Chinese written corpus.", "labels": [], "entities": []}, {"text": "In this paper, we present all unsupervised approach to lexical acquisition within the minimum description length (MDL) paradigm [], with a goodness measure, namely, the description length gain (DLG), which is formulated in] following classic information theory.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7312093675136566}, {"text": "description length gain (DLG)", "start_pos": 169, "end_pos": 198, "type": "METRIC", "confidence": 0.7996064325173696}]}, {"text": "This measure is used, following the MDL principle, to evaluate the goodness of identifying a (sub)sequence of characters in a corpus as a lexical item.", "labels": [], "entities": []}, {"text": "In order to rigorously evaluate the effectiveness of this unsupervised learning approach, we do not limit ourselves to the detection of unknown words with respect to ally given dictionary.", "labels": [], "entities": []}, {"text": "Rather, we use it to perform unsupervised lexical acquisition from large-scale English text corpora.", "labels": [], "entities": [{"text": "unsupervised lexical acquisition from large-scale English text corpora", "start_pos": 29, "end_pos": 99, "type": "TASK", "confidence": 0.7764148786664009}]}, {"text": "Since it is a learning-via-compression approach, the algorithm can be further extended to deal with text compression and, very likely, other data sequencing problems.", "labels": [], "entities": [{"text": "text compression", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.7652795910835266}]}, {"text": "The rest of the paper is organised as follows: Section 2 presents the formulation of the DLG mea- sure in terms of classic information theory; Section 3 formulates the learning algorithm within the MDL framework, which aims to achieve an optimal segmentation of the given corpus into lexical items with regard to the DLG measure; Section 4 presents experiments and discusses experimental results with respect to previous studies; and finally, the conclusions of the paper are given in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have conducted a series of lexical acquisition experiments with the above algorithm on largescale English corpora, e.g., the Brown corpus and the PTB WSJ corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 128, "end_pos": 140, "type": "DATASET", "confidence": 0.9426367878913879}, {"text": "PTB WSJ corpus", "start_pos": 149, "end_pos": 163, "type": "DATASET", "confidence": 0.9217210014661154}]}, {"text": "Below is the segmentation result on the first few sentences in the Brown corpus: where uppercase letters are converted to lowercase ones, the spaces are visualised by all underscore and the full-stops are all replaced by (@'s.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.9528567790985107}]}, {"text": "Although a space is not distinguished from any other characters for the learner, we have to rely on the spaces to judge the correctness of a word boundary prediction: a predicted word boundary immediately before or after a space is judged as correct.", "labels": [], "entities": [{"text": "word boundary prediction", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.6240024169286092}]}, {"text": "But we also have observed that this criterion overlooks many meaningful predictions like \"-.-charge] [d_by-..\", \"---are_outmode] [d_.-.\" and \".--government] [s...:'.", "labels": [], "entities": []}, {"text": "If this is taken into account, the learning pcrformance is evidently better than the precision and recall figures reported in below.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9995920062065125}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9959995746612549}]}, {"text": "Interestingly, it is observed that n-gram counts derived from a larger volume of data can significantly improve the precision but decrease the recall of the word boundary prediction.", "labels": [], "entities": [{"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9992721676826477}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9989462494850159}, {"text": "word boundary prediction", "start_pos": 157, "end_pos": 181, "type": "TASK", "confidence": 0.6312054097652435}]}, {"text": "The correlation betwee, the volume of data used tbr deriving ngram counts and the change of precision and recall is shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9690264463424683}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9990869760513306}]}, {"text": "The effectiveness of the unsupervised learning is evidenced by the fact that its precision and recall are, respectively, ~tll tl~ree times as high as the precision and recall by random guessing.", "labels": [], "entities": [{"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9995231628417969}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9974210262298584}, {"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9990987777709961}, {"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.9951070547103882}]}, {"text": "The best learning performance, in terms of both precision and recall, in the experiments is o the one with 79.33% precision and 63.01~ recall, obtained from the experiment on the e,ltire Brown corpus.: The correlation between corpus size (million char.) and precision/recall It is straightforwardly understandable that the increase of data volume leads to a significant increase of precision in the learning, because prediction based on more data is more reliable.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9981903433799744}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9961165189743042}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9955808520317078}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9991921782493591}, {"text": "Brown corpus.", "start_pos": 187, "end_pos": 200, "type": "DATASET", "confidence": 0.7358344793319702}, {"text": "precision", "start_pos": 258, "end_pos": 267, "type": "METRIC", "confidence": 0.9961976408958435}, {"text": "recall", "start_pos": 268, "end_pos": 274, "type": "METRIC", "confidence": 0.8188270330429077}, {"text": "precision", "start_pos": 382, "end_pos": 391, "type": "METRIC", "confidence": 0.9975754618644714}]}, {"text": "The reason for the drop of recall is that when the volume of data increases, more multi-word strings have a higher compression effect (than individual words) and, consequently: they are learned by the learner as lexical items, e.g.,, and.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9984442591667175}]}, {"text": "If the credit in such nmlti-word lexical items is counted, the recall nmst be much better than the one in.", "labels": [], "entities": [{"text": "recall nmst", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9487468302249908}]}, {"text": "Of course, this also reflects a limitation of the learning algorithm: it only conducts an optimal segmentation instead of a hierarchical chunking on an utterance.", "labels": [], "entities": []}, {"text": "The precision and recall reported above is not a big surprise.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996249675750732}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9992823004722595}]}, {"text": "To our knowledge, however, it is the first time that the performance of unsupervised learning of word boundaries is examined with the criteria of both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9993353486061096}, {"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9919032454490662}]}, {"text": "Unfortunately, this performance can't be compared with any previous studies, for several reasons.", "labels": [], "entities": []}, {"text": "One is that the learning results of previous studies are not presented in a comparable manner, for example, and, as noted by as well.", "labels": [], "entities": []}, {"text": "Another is that the learning outcomes are different.", "labels": [], "entities": []}, {"text": "For example, the output of lexical learning from an utterance (as a character sequence) in and is a hierarchical chunking of the utterance.", "labels": [], "entities": []}, {"text": "The chance to hit the correct words in such chunking is obviously many times higher than that in a flat segmentation.", "labels": [], "entities": []}, {"text": "The hierarchical chunking leads to a recall above 90% in de Marken's work.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9994258880615234}]}, {"text": "Interestingly, however, de Marken does not report the precision, which seems too low, therefore meaningless, to report, because the learner produces so many chunks.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9966359734535217}]}], "tableCaptions": []}