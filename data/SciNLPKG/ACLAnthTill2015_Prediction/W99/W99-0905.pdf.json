{"title": [{"text": "Resolving Translation Ambiguity using Non-parallel Bilingual Corpora", "labels": [], "entities": [{"text": "Resolving Translation Ambiguity", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9696034590403239}]}], "abstractContent": [{"text": "This paper presents an unsupervised method for choosing the correct translation of a word in context.", "labels": [], "entities": [{"text": "choosing the correct translation of a word in context", "start_pos": 47, "end_pos": 100, "type": "TASK", "confidence": 0.7943264113532172}]}, {"text": "It learns disambiguation information from non-parallel bilinguM corpora (preferably in the same domain) free from tagging.", "labels": [], "entities": []}, {"text": "Our method combines two existing unsupervised disambiguation algorithms: a word sense disam-biguation algorithm based on distributional clustering and a translation disambiguation algorithm using target language corpora.", "labels": [], "entities": [{"text": "translation disambiguation", "start_pos": 153, "end_pos": 179, "type": "TASK", "confidence": 0.8789659738540649}]}, {"text": "For the given word in context, the former algorithm identifies its meaning as one of a number of predefined usage classes derived by clustering a large amount of usages in the source language corpus.", "labels": [], "entities": []}, {"text": "The latter algorithm is responsible for associating each usage class (i.e., cluster) with a target word that is most relevant to the usage.", "labels": [], "entities": []}, {"text": "This paper also shows preliminary results of translation experiments.", "labels": [], "entities": [{"text": "translation", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.9776580333709717}]}], "introductionContent": [{"text": "Choosing the correct translation of a content word in context, referred to as \"translation disambiguation (of content word)\", is a key task in machine translation.", "labels": [], "entities": [{"text": "Choosing the correct translation of a content word in context", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.7890471994876862}, {"text": "translation disambiguation", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.9282567501068115}, {"text": "machine translation", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.7963704466819763}]}, {"text": "It is also crucial in cross-language text processing including cross-language information retrieval and abstraction.", "labels": [], "entities": [{"text": "cross-language text processing", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.7198929091294607}, {"text": "cross-language information retrieval", "start_pos": 63, "end_pos": 99, "type": "TASK", "confidence": 0.710651030143102}]}, {"text": "Due to the recent availability of large text corpora, various statistical approaches have been tried including using 1) parallel corpora (,,), 2) non-parallel bilingual corpora tagged with topic area () and 3) un-tagged mono-language corpora in the target language,,.", "labels": [], "entities": []}, {"text": "A problem with the first two approaches is that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted.", "labels": [], "entities": []}, {"text": "Although the third approach eases the problem of preparing corpora, it suffers from alack of useful information in the source language.", "labels": [], "entities": []}, {"text": "For example, suppose the proper name, \"Dodgers\", provides good context to identify the usage of \"hit\" in the training corpus in English.", "labels": [], "entities": []}, {"text": "If the translation of \"Dodgers\" rarely occurs in the target language corpora, it does not contribute to target word selection.", "labels": [], "entities": [{"text": "translation of \"Dodgers\"", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.7828287005424499}, {"text": "target word selection", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.6139267782370249}]}, {"text": "The method presented in this paper solves this problem by choosing the target word that corresponds to the usage identified in the source language corpora.", "labels": [], "entities": []}, {"text": "This method is totally unsupervised in the sense that it acquires disambiguation information from non-parallel bilingual corpora (preferably in the same domain) free from tagging.", "labels": [], "entities": []}, {"text": "It combines two unsupervised disambiguation algorithms: one is the word sense disambiguation algorithm based on distributional clustering( and the other is the translation disambiguation algorithm using target language corpora.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.6822078625361124}, {"text": "translation disambiguation", "start_pos": 160, "end_pos": 186, "type": "TASK", "confidence": 0.8386883735656738}]}, {"text": "For the given word in context, the former algorithm identifies its usage as one of several predefined usage classes derived by clustering a large amount of usages in the source language corpus.", "labels": [], "entities": []}, {"text": "The latter algorithm is responsible for associating each usage class (i.e., cluster) with a target word that best expresses the usage.", "labels": [], "entities": []}, {"text": "The following sections are organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we overview the entire method.", "labels": [], "entities": []}, {"text": "The following two sections (i.e., Section 3 and 4) then introduce the two major components of the method including the two unsupervised disambiguation algorithms.", "labels": [], "entities": []}, {"text": "Section 5 and 6 are devoted respectively to a preliminary evaluation and discussions on related research.", "labels": [], "entities": []}, {"text": "source language corpus and creates a profile, referred to as the \"sense profile\" for each class.", "labels": [], "entities": []}, {"text": "The categorization process chooses the profile most relevant to the input word whose sense is implicitly given by its surrounding context.", "labels": [], "entities": []}, {"text": "Located to the right is what we call the sense..", "labels": [], "entities": []}, {"text": "It is responsible for as--sociating each semantic profile with the most likely translation of the source word (for which the seman-. tic profile is derived).", "labels": [], "entities": []}, {"text": "The result of this process is registered in the sense-$ranslation table.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted English-to-Japanese translation experiments using newspaper articles.", "labels": [], "entities": [{"text": "English-to-Japanese translation", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6296767443418503}]}, {"text": "The results of the proposed algorithm were compared against those of the previous algorithm which relies solely on target language corpora).", "labels": [], "entities": []}, {"text": "The bilingual dictionary, from English to Japanese, was an inversion of the EDICT, a free Japanese-to-English dictionary.", "labels": [], "entities": [{"text": "EDICT", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.9307939410209656}]}, {"text": "The co-occurrence statistics were extracted from the 1994 New York Times (420MB) for English and 1994 Mainichi Shinbun (Japanese newspaper) (90MB) for Japanese.", "labels": [], "entities": [{"text": "New York Times", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.8098740975062052}, {"text": "Mainichi Shinbun (Japanese newspaper)", "start_pos": 102, "end_pos": 139, "type": "DATASET", "confidence": 0.9009067714214325}]}, {"text": "Note that 100 articles were randomly separated from the former corpus as the test set described below.", "labels": [], "entities": []}, {"text": "Although these two kinds of newspaper articles were both written in 1994, topics and contents greatly differ.", "labels": [], "entities": []}, {"text": "This is because each newspaper publishing company edited its paper primarily for domestic readers.", "labels": [], "entities": []}, {"text": "Note that the domains of these texts range from business to sports.", "labels": [], "entities": []}, {"text": "The initial size of each co-occurrence matrix was 50000-by-1000, where rows and columns correspond to the 50,000 and 1000 most frequent words in the corpus 4.", "labels": [], "entities": []}, {"text": "Each initial matrix was then reduced by using SVD into a matrix of 50000-by-100 using SVD-PACKC(.", "labels": [], "entities": []}, {"text": "Test data, a set of word-lists, were automatically generated from the 120 articles from the New York Times separated from the training set.", "labels": [], "entities": []}, {"text": "A word-list was extracted from an article by choosing the topmost n words ranked by their tf-idfscores, given in Section 5.", "labels": [], "entities": []}, {"text": "In the following experiments, we set n to 6 since it gave the \"best\" result for this corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Contexts surrounding \"suit\"", "labels": [], "entities": []}]}