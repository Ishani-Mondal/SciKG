{"title": [{"text": "A Computational Approach to Deciphering Unknown Scripts", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose and evaluate computational techniques for deciphering unknown scripts.", "labels": [], "entities": []}, {"text": "We focus on the casein which an unfamiliar script encodes a known language.", "labels": [], "entities": []}, {"text": "The decipherment of a brief document or inscription is driven by data about the spoken language.", "labels": [], "entities": [{"text": "decipherment of a brief document or inscription", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.7636736631393433}]}, {"text": "We consider which scripts are easy or hard to decipher, how much data is required, and whether the techniques are robust against language changeover time.", "labels": [], "entities": []}], "introductionContent": [{"text": "With surprising frequency, archaeologists dig up documents that no modern person can read.", "labels": [], "entities": []}, {"text": "Sometimes the written characters are familiar (say, the Phoenician alphabet), but the language is unknown.", "labels": [], "entities": []}, {"text": "Other times, it is the reverse: the written script is unfamiliar but the language is known.", "labels": [], "entities": []}, {"text": "Or, both script and language maybe unknown.", "labels": [], "entities": []}, {"text": "Cryptanalysts also encounter unreadable documents, but they try to read them anyway.", "labels": [], "entities": []}, {"text": "With patience, insight, and computer power, they often succeed.", "labels": [], "entities": []}, {"text": "Archaeologists and linguists known as epigraphers apply analogous techniques to ancient documents.", "labels": [], "entities": []}, {"text": "Their decipherment work can have many resources as input, not all of which will be present in a given case: (1) monolingual inscriptions, (2) accompanying pictures or diagrams, (3) bilingual inscriptions, (4) the historical record, (5) physical artifacts, (6) bilingual dictionaries, (7) informal grammars, etc.", "labels": [], "entities": []}, {"text": "In this paper, we investigate computational approaches to deciphering unknown scripts, and report experimental results.", "labels": [], "entities": []}, {"text": "We concentrate on the following case: \u2022 unfamiliar script \u2022 known language \u2022 minimal input (monolingual inscriptions only) This situation has arisen in many famous cases of decipherment--for example, in the Linear B documents from Crete (which turned out to be a \"non-Greek\" script for writing ancient Greek) and in the Mayan documents from Mesoamerica.", "labels": [], "entities": []}, {"text": "Both of these cases lay unsolved until the latter half of the 20th century.", "labels": [], "entities": []}, {"text": "In computational linguistic terms, this decipherment task is not really translation, but rather text-to-speech conversion.", "labels": [], "entities": [{"text": "text-to-speech conversion", "start_pos": 96, "end_pos": 121, "type": "TASK", "confidence": 0.7141343653202057}]}, {"text": "The goal of the decipherment is to \"make the text speak,\" after which it can be interpreted, translated, etc.", "labels": [], "entities": []}, {"text": "Of course, even after an ancient document is phonetically rendered, it will still contain many unknown words and strange constructions.", "labels": [], "entities": []}, {"text": "Making the text speak is therefore only the beginning of the story, but it is a crucial step.", "labels": [], "entities": []}, {"text": "Unfortunately, current text-to-speech systems cannot be applied directly, because they require upfront a clearly specified sound/writing connection.", "labels": [], "entities": []}, {"text": "For example, a system designer may create a large pronunciation dictionary (for English or Chinese) or a set of manually constructed character-based pronunciation rules (for Spanish or Italian).", "labels": [], "entities": []}, {"text": "But in decipherment, this connection is unknown!", "labels": [], "entities": []}, {"text": "It is exactly what we must discover through analysis.", "labels": [], "entities": []}, {"text": "There are no rule books, and literate informants are long-since dead.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper, we do not decipher any ancient scripts.", "labels": [], "entities": []}, {"text": "Rather, we develop algorithms and apply them to the \"decipherment\" of known, modern scripts.", "labels": [], "entities": []}, {"text": "We pretend to be ignorant of the connection between sound and writing.", "labels": [], "entities": []}, {"text": "Once our algorithms have come up with a proposed phonetic decipherment of a given document, we route the sound sequence to a speech synthesizer.", "labels": [], "entities": []}, {"text": "If a native speaker can understand the speech and make sense of it, then we consider the decipherment a success.", "labels": [], "entities": []}, {"text": "(Note that the native speaker need not even be literate, theoretically).", "labels": [], "entities": []}, {"text": "We experiment with modern writing systems that span the categories described above.", "labels": [], "entities": []}, {"text": "We are interested in the following questions: \u2022 Can automatic techniques decipher an unknown script?", "labels": [], "entities": []}, {"text": "If so, how accurately?", "labels": [], "entities": []}, {"text": "\u2022 What quantity of written text is needed for successful decipherment?", "labels": [], "entities": []}, {"text": "(this maybe \u2022 quite limited by circumstances) \u2022 What knowledge of the spoken language is needed?", "labels": [], "entities": []}, {"text": "Can it to be extracted automatically from available resources?", "labels": [], "entities": []}, {"text": "\u2022 Are some writing systems easier to decipher than others?", "labels": [], "entities": []}, {"text": "Are there systematic differences among alphabetic, syllabic, and logographic systems?", "labels": [], "entities": []}, {"text": "\u2022 Are word separators necessary or helpful?", "labels": [], "entities": []}, {"text": "\u2022 Can automatic techniques be robust against language evolution (e.g., modern versus ancient forms of a language)?", "labels": [], "entities": []}, {"text": "\u2022 Can automatic techniques identify the language behind a script as a precursor to deciphering it?", "labels": [], "entities": []}, {"text": "4 Alphabetic Writing (Spanish) Five hundred years ago, Spaniards invaded Mayan lands, burning documents and effectively eliminating everyone who could read and write.", "labels": [], "entities": [{"text": "Alphabetic Writing", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.7134007215499878}]}, {"text": "(Modern Spaniards will be quick to point out that most of the work along those lines had already been carried out by the Aztecs).", "labels": [], "entities": []}, {"text": "Mayan hieroglyphs remained uninterpreted for many centuries.", "labels": [], "entities": []}, {"text": "We imagine that if the Mayans have invaded Spain, then 20th century Mayan scholars might be deciphering ancient Spanish documents instead.", "labels": [], "entities": []}, {"text": "We begin with an analysis of Spanish writing.", "labels": [], "entities": []}, {"text": "The task of decipherment will be to re-invent these rules and apply them to written documents in reverse.", "labels": [], "entities": []}, {"text": "First, is necessary to settle on the basic inventory of sounds and characters.", "labels": [], "entities": []}, {"text": "Characters are easy; we simply tabulate the distinct ones observed in text.", "labels": [], "entities": []}, {"text": "For sounds, we need something that will serve as reasonable input to a speech synthesizer.", "labels": [], "entities": []}, {"text": "We use a Spanish-relevant subset of the International Phonetic Alphabet (IPA), which seeks to capture all sounds in all languages.", "labels": [], "entities": [{"text": "International Phonetic Alphabet (IPA)", "start_pos": 40, "end_pos": 77, "type": "DATASET", "confidence": 0.7774445414543152}]}, {"text": "Actually, we use an ASCII version of the IPA called SAMPA (Speech Assessment Methods Phonetic Alphabet), originally developed under ESPRIT project 1541.", "labels": [], "entities": [{"text": "Speech Assessment Methods Phonetic Alphabet)", "start_pos": 59, "end_pos": 103, "type": "TASK", "confidence": 0.6362011134624481}, {"text": "ESPRIT project 1541", "start_pos": 132, "end_pos": 151, "type": "DATASET", "confidence": 0.8678117990493774}]}, {"text": "There is also a public-domain Castillian speech synthesizer (called Mbrola) for the Spanish SAMPA sound set.", "labels": [], "entities": [{"text": "Spanish SAMPA sound set", "start_pos": 84, "end_pos": 107, "type": "DATASET", "confidence": 0.75090092420578}]}, {"text": "shows the sound and character inventories.", "labels": [], "entities": []}, {"text": "The left-hand side of each rule contains a Spanish sound (and possible conditions), while theright-hand side contains zero or more Spanish characters.", "labels": [], "entities": []}, {"text": "\u2022 silence can produce a character (h) Moreover, there are ambiguities.", "labels": [], "entities": []}, {"text": "The sound L (English y-sound) maybe written as either ll or y.", "labels": [], "entities": []}, {"text": "The sound i may also produce the character y, so the pronunciation of y varies according to context.", "labels": [], "entities": []}, {"text": "The sound rr (trilled r) is written rr in the middle of a word and rat the beginning of a word.", "labels": [], "entities": []}, {"text": "shows a sample set of Spanish spelling rules.", "labels": [], "entities": []}, {"text": "We formalized these rules computationally in a finite-state transducer).", "labels": [], "entities": []}, {"text": "Given a specific sound sequence, we can extract all possible character sequences, and vice versa.", "labels": [], "entities": []}, {"text": "It turns out that while there are many ways to write a given Spanish sound sequence with these rules, it is fairly clear how to pronounce a written sequence\u2022 In our decipherment experiment, we blithely ignore many of the complications just described, and pretend that Spanish writing is, in fact, a one-for-one proposition.", "labels": [], "entities": []}, {"text": "That is, to write down a sound sequence, one replaces each sound with a single character.", "labels": [], "entities": []}, {"text": "We do allow ambiguity, however.", "labels": [], "entities": []}, {"text": "A given sound may produce one character sometimes, and another character other times.", "labels": [], "entities": []}, {"text": "Decipherment is driven by knowledge about the spoken language.", "labels": [], "entities": []}, {"text": "In the case of archeological decipherment, this knowledge may include vocabulary, grammar, and meaning.", "labels": [], "entities": []}, {"text": "We collect frequencies of soundtriples in spoken Spanish.", "labels": [], "entities": []}, {"text": "If we know that triple \"t 1 k\" is less frequent than \"a s t,\" then we should ultimately prefer a decipherment that contains the latter instead of the former, all other things being equal.", "labels": [], "entities": []}, {"text": "This leads naturally into a statistical approach to decipherment.", "labels": [], "entities": []}, {"text": "Our goal is to settle on a sound-to-character scheme that somehow maximizes the probability of the observed written document.", "labels": [], "entities": []}, {"text": "Like many linguistic problems, this one can be formalized in the noisy-channel framework.", "labels": [], "entities": []}, {"text": "Our sound-triple frequencies can be turned into conditional probabilities such as P(t Ia s).", "labels": [], "entities": []}, {"text": "We can estimate the probability of a sound sequence as the product of such local probabilities.", "labels": [], "entities": []}, {"text": "P(sl ... sn) P(s3 I Sl s2) \u2022 P(s4 I s2 s3) \u2022 P(s5 I s3 s4) ....", "labels": [], "entities": []}, {"text": "A specific sound-to-character scheme can be represented as a set of conditional probabilities such as P(v I B).", "labels": [], "entities": []}, {"text": "Read this as \"the probability that Spanish sound B is written with character v.\"", "labels": [], "entities": []}, {"text": "We can estimate the conditional probability of entire character sequence given an entire sound sequence as a product of such probabilities.", "labels": [], "entities": []}, {"text": "And second, we can compute the most probable phonetic decipherment sl ...", "labels": [], "entities": []}, {"text": "s, of a particular written sequence of characters cl ...", "labels": [], "entities": []}, {"text": "This will be the one that maximizes P(sl ...s~ I cl \u2022 ..cn), or equivalently, maximizes P(sl ...s~) \u2022 P(cl ...c, I Sl ...sn).", "labels": [], "entities": []}, {"text": "The trick is that the P(character I sound ) probabilities are unknown to us.", "labels": [], "entities": []}, {"text": "We want to assign values that maximize P(cl ...c,).", "labels": [], "entities": []}, {"text": "These same values can then be used to decipher.", "labels": [], "entities": []}, {"text": "We adapt the EM algorithm, for decipherment, starting with a uniform probability over P(character.", "labels": [], "entities": []}, {"text": "That is, any sound will produce any character with probability 0.0333.", "labels": [], "entities": []}, {"text": "The algorithm successively refines these probabilities, guaranteeing to increase P(cl ... cn) at each iteration.", "labels": [], "entities": []}, {"text": "EM requires us to consider an exponential number of decipherments at each iteration, but this can be done efficiently with a dynamic-programming implementation.", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7872559428215027}]}, {"text": "The training scheme is illustrated in.", "labels": [], "entities": []}, {"text": "In our experiment, we use the first page of the novel Don Quixote as our \"ancient\" Spanish document cl ...cn.", "labels": [], "entities": []}, {"text": "To get phonetic data, we might tape-record modern Spanish speakers and transcribe the recorded speech into the IPA alphabet.", "labels": [], "entities": []}, {"text": "Or we might use documents written in an alternate, known script, if any existed.", "labels": [], "entities": []}, {"text": "In this work, we take a shortcut by reverse-engineering a set of medical Spanish documents, using the finite-state transducer described above, to obtain along phonetic training sequence Sl \u2022 \u2022 \u2022 s,~.", "labels": [], "entities": []}, {"text": "At each EM iteration, we extract the most probable decipherment and synthesize it into audible form.", "labels": [], "entities": []}, {"text": "At iteration 0, with uniform probabil!ties, the result is pure babble.", "labels": [], "entities": []}, {"text": "At iteration 1, Spanish speakers report that \"it sounds like someone speaking Spanish, but using words I don't know.\"", "labels": [], "entities": []}, {"text": "At iteration 15, the decipherment can be readily understood.", "labels": [], "entities": []}, {"text": "We first train a phonetic model on phonetic data.", "labels": [], "entities": []}, {"text": "We then combine the phonetic model with a generic (uniform) spelling model to create a probabilistic generator of character sequences.", "labels": [], "entities": []}, {"text": "Given a particular character sequence (\"ancient document\"), the EM algorithm searches for adjustments to the spelling model that will increase the probability of that character sequence.", "labels": [], "entities": []}, {"text": "la.nguago/mt/decipher, html).", "labels": [], "entities": []}, {"text": "If we reverse-engineer Don Quixote, we can obtain a gold standard phonetic decipherment.", "labels": [], "entities": []}, {"text": "Our automatic decipherment correctly identifies 96% of the sounds.", "labels": [], "entities": []}, {"text": "Incorrect or dropped sounds are due to our naive one-for-one model, and not to weak algorithms or small corpora.", "labels": [], "entities": []}, {"text": "For example, \"de la Mancha\" is deciphered as \"d e la ma n Ti a\" even though the characters ch really represent the single sound tS rather than the two sounds Ti. shows how performance changes at each EM iteration.", "labels": [], "entities": []}, {"text": "The worst-performing curve reflects the accuracy of the most-probable decipherment using the formula above, i.e., the one that maximizes P(sl \u2022 ..sn) \u2022 P(cl ...ca ] sl ...sn).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9993464350700378}]}, {"text": "We find that it is better to ignore the P(Sl ...s,~) factor altogether, because while the learned sound-to- character probabilities are fairly good, they are still somewhat unsure, and this leaves room for the phonetic model to overrule them incorrectly.", "labels": [], "entities": [{"text": "P", "start_pos": 40, "end_pos": 41, "type": "METRIC", "confidence": 0.9836793541908264}]}, {"text": "However, the P(sl ...sn) model does have useful things to contribute.", "labels": [], "entities": []}, {"text": "Our best performance, shown in the highest curve, is obtained by weighing the learned sound-to-character probabilities more highly, i.e., by maximizing P(sl \u2022 ..Sn)\" P(Cl ...ca I sl ...sn) 3.", "labels": [], "entities": []}, {"text": "We performed some alternate experiments\u2022 Using phoneme pairs instead of triples is workable--it results in a drop from 96% accuracy to 92%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.986642599105835}]}, {"text": "Our main experiment uses word separators; removing these degrades performance.", "labels": [], "entities": []}, {"text": "For example, it becomes more difficult to tell whether the r character should be trilled or not.", "labels": [], "entities": []}, {"text": "In our experiments with Japanese and Chinese, described next, we did not use word separators, as these languages are usually written without them.", "labels": [], "entities": []}], "tableCaptions": []}