{"title": [{"text": "Using Coreference Chains for Text Summarization", "labels": [], "entities": [{"text": "Text Summarization", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7580123841762543}]}], "abstractContent": [{"text": "We describe the use of coreference chains for the production of text summaries, using a variety of criteria to select a 'best' chain to represent the main topic of a text.", "labels": [], "entities": []}, {"text": "The approach has been implemented within an existing MUC coreference system, which constructs a full discourse model of texts, including information about changes of focus, which can be used in the selection of chains.", "labels": [], "entities": [{"text": "MUC coreference", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.6505904197692871}]}, {"text": "Some preliminary experiments on the automatic evaluation of summaries are also described, using existing tools to attempt to replicate some of the recent SUMMAC manual evaluations.", "labels": [], "entities": [{"text": "evaluation of summaries", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.6069686611493429}, {"text": "SUMMAC manual evaluations", "start_pos": 154, "end_pos": 179, "type": "TASK", "confidence": 0.5643383065859476}]}], "introductionContent": [{"text": "In this paper we report preliminary work which explores the use of coreference chains to construct text summaries.", "labels": [], "entities": []}, {"text": "Sparck Jones (1993) has described summarization as a two stage process of (1) building a representation of the source text and (2) generating a summary representation from the source representation and producing an output text from this summary representation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9907054305076599}]}, {"text": "Our source representation is a set of coreference chains -specifically those chains of referring expressions produced by an information extraction system designed to participate in the MUC-7 coreference task.", "labels": [], "entities": [{"text": "MUC-7 coreference task", "start_pos": 185, "end_pos": 207, "type": "TASK", "confidence": 0.5609244803587595}]}, {"text": "Our summary representation is a 'best chain', selected from the set of coreference chains by the application of one or more heuristics.", "labels": [], "entities": []}, {"text": "The output summary is simply the concatenation of (some subset of) sentences from the source text which contain one or more expressions occurring in the selected coreference chain.", "labels": [], "entities": []}, {"text": "The intuition underlying this approach is that texts are in large measure 'about' some central entity, which is effectively the topic, or focus of the discourse.", "labels": [], "entities": []}, {"text": "This intuition maybe falsethere maybe more than one entity of central concern, or events or relations between entities maybe the principal topic of the text.", "labels": [], "entities": []}, {"text": "However, it is at the very least an interesting experiment to see to what extent a principal coreference chain can be used to generate a summary.", "labels": [], "entities": []}, {"text": "Further, this approach, which we have implemented and preliminarily evaluated, could easily be extended to allow summaries to be generated from (parts of) the best n coreference chains, or from event, as well as object , coreference chains.", "labels": [], "entities": []}, {"text": "The use of document extracts formed from coreference chains is not novel.", "labels": [], "entities": []}, {"text": "describe a technique for crossdocument coreference which involves extracting the set of all sentences containing expressions in a coreference chain fora specific entity (e.g. John Smith) from each of several documents.", "labels": [], "entities": []}, {"text": "They then employ a thresholded vector space similarity measure between these document extracts to decide whether the documents are discussing the same entity (i.e. the same John Smith).", "labels": [], "entities": []}, {"text": "describe a query-sensitive (i.e. user-focused) summarization technique that involves extracting sentences from a document which contain phrases that corefer with expressions in the query.", "labels": [], "entities": []}, {"text": "The resulting extract is used to support relevancy judgments with respect to the query.", "labels": [], "entities": []}, {"text": "The use of chains of related expressions in documents to select sentences for inclusion in a generic (i.e. non-user-focused) summary is also not novel.", "labels": [], "entities": []}, {"text": "describe a technique for text summarization based on lexical chains.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7178223133087158}]}, {"text": "Their technique, which builds on work of, and ultimately who stressed the role of lexical cohesion in text coherence, is to form chains of lexical items across a text based on the items' semantic relatedness as in-dicated by a thesaurus (WordNet in their case).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 238, "end_pos": 245, "type": "DATASET", "confidence": 0.9310780167579651}]}, {"text": "These lexical chains serve as their source representation, from which a summary representation is produced using heuristics for choosing the 'best' lexical chains.", "labels": [], "entities": []}, {"text": "From these the summary is produced by employing a further heuristic to select the 'best' sentences from each of the selected lexical chains.", "labels": [], "entities": []}, {"text": "The novelty in our work is to combine the idea of a document extract based on coreference chains with the idea of chains of related expressions serving to indicate sentences for inclusion in a generic summary (though we explore the use of coreference between query and text as a technique for generating user-focused summaries as well).", "labels": [], "entities": []}, {"text": "Returning to Halliday and Hasan, one can see how this idea has merit within their framework.", "labels": [], "entities": []}, {"text": "They identify four principal mechanisms by which text coherence is achieved -reference, substitution and ellipsis, conjunction and lexical cohesion.", "labels": [], "entities": []}, {"text": "If lexical cohesion is a useful relation to explore forgetting at the 'aboutness' of a text, and hence for generating summaries, then so too may reference (separately, or in conjunction with lexical cohesion).", "labels": [], "entities": []}, {"text": "Indeed, identifying chains of coreferential expressions in text has certain strengths over identifying chains of expressions related merely on lexical semantical grounds.", "labels": [], "entities": []}, {"text": "For, there is no doubt that common reference, correctly identified, directly ties different parts of a text together -they are literally 'about' the same thing; lexical semantic relatedness, as indicated by an external resource, can never conclusively establish this degree of relatedness, nor indeed can the resource guarantee that semantic relatedness will be found when it exists.", "labels": [], "entities": []}, {"text": "Further, lexical cohesion techniques ignore pronomial anaphora, and hence their frequency counts of key terms, used both for identifying best chains and best sentences within best chains, may often be inaccurate, as focal referents will often be pronominalised.", "labels": [], "entities": []}, {"text": "Of course there are drawbacks to a coreference-based approach.", "labels": [], "entities": []}, {"text": "Lexical cohesion relations are relatively easy to compute and do not rely on full text processing -this makes summarisation techniques based on them rapid and robust.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.984074592590332}]}, {"text": "Coreference relations tend to require more complex techniques to compute.", "labels": [], "entities": []}, {"text": "Our view, however, is that summarisation research is still in early stages and that we need to explore many techniques to understand their strengths and weaknesses in terms of the type and quality of the summaries they produce.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9901632070541382}]}, {"text": "If coreference-based techniques can yield good summaries, this will provide impetus to make coreference technologies better and faster.", "labels": [], "entities": []}, {"text": "The basic coreference chain technique we describe in this paper yields generic summaries as opposed to user-focused summaries, as these terms have been used in relation to the TIPSTER SUMMAC text summarization evaluation exercise (.", "labels": [], "entities": [{"text": "TIPSTER SUMMAC text summarization evaluation", "start_pos": 176, "end_pos": 220, "type": "TASK", "confidence": 0.6339626371860504}]}, {"text": "That is, the summaries aim to satisfy a wide readership by supplying information about the 'most important' entity in the text.", "labels": [], "entities": []}, {"text": "But of course this technique could also be used to generate summaries tailored to a user(group) through use with a preprocessor that analyzed a user-supplied topic description and selected one or more entities from the topic description to use in filtering coreference chains found in the full source document.", "labels": [], "entities": []}, {"text": "The rest of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we briefly describe the system we use for computing coreference relations.", "labels": [], "entities": [{"text": "coreference relations", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.7805105447769165}]}, {"text": "Section 3 describes various heuristics we have implemented for extracting a 'best' coreference chain from the set of coreference chains computed fora text; and, it discusses how we select 'best' sentences to include in the summary from those source text sentences containing referring expressions in the 'best' chain.", "labels": [], "entities": []}, {"text": "Section 4 presents a simple example and shows the different summaries that different heuristics produce.", "labels": [], "entities": []}, {"text": "Section 5 describes the limited evaluation we have been able to carryout to date, but more importantly introduces what we believe to be a novel and interesting way of reusing some of the MUC materials for assessing summaries.", "labels": [], "entities": [{"text": "MUC materials", "start_pos": 187, "end_pos": 200, "type": "DATASET", "confidence": 0.8612942397594452}, {"text": "summaries", "start_pos": 215, "end_pos": 224, "type": "TASK", "confidence": 0.7844183444976807}]}], "datasetContent": [{"text": "This exercise involved a number of different tasks and a number of different evaluation measures.", "labels": [], "entities": []}, {"text": "The measures divided into extrinsic measures -those that ignore the content of the summary and assess it solely according to how useful it is in enabling an agent to perform some measurable task -and intrinsic measures -those that examine the content of the summary and attempt to pass some judgment on it directly.", "labels": [], "entities": []}, {"text": "In brief, the four SUMMAC tasks were: 1.", "labels": [], "entities": [{"text": "SUMMAC tasks", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.8169389367103577}]}, {"text": "Ad Hoc Task The summariser is given a topic description and a set of documents and produces user-focused summaries based on the topic description.", "labels": [], "entities": []}, {"text": "The summaries and the topic description (along with some source documents for control) are passed to a judge who reads the summaries passes relevance judgments on them with respect to the topic.", "labels": [], "entities": []}, {"text": "These judgments are scored against 'true' relevance judgments previously established for the full documents with respect to the topic.", "labels": [], "entities": []}, {"text": "This is an extrinsic evaluation that measures the utility of a summarization system at generating user-focused summaries capable of supporting a relevance judgment.", "labels": [], "entities": []}, {"text": "We have not to date been able to carryout the proposed evaluations in full on our summarization system.", "labels": [], "entities": []}, {"text": "In particular, since the goal of the basic summarization model we have implemented is to produce generic summaries, only the automated categorization evaluation is really appropriate.", "labels": [], "entities": [{"text": "summarization", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9741429686546326}]}, {"text": "However, since the MUC materials and scoring software were available to us, we were eager to attempt some form of an automated question answering evaluation, as described in the last section, using the existing LaSIE system.", "labels": [], "entities": [{"text": "MUC materials", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.923382431268692}, {"text": "question answering evaluation", "start_pos": 127, "end_pos": 156, "type": "TASK", "confidence": 0.8270934025446574}]}, {"text": "We therefore adopted a crude technique to simulate topic processing by the generic summarization system: the topic description (a TRECstyle narrative) from the MUC-6 management succession IE task definition, was prepended, as the first paragraph, to each text.", "labels": [], "entities": [{"text": "topic processing", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7600325644016266}, {"text": "MUC-6 management succession IE task definition", "start_pos": 160, "end_pos": 206, "type": "TASK", "confidence": 0.6205125153064728}]}, {"text": "We then ran our summarisation system using the 'start of chain' criteria to select only those coreference chains which included a link between the topic description and the text.", "labels": [], "entities": []}, {"text": "This scenario is similar to who also use coreference between query, or topic description, and text to generate a user-focused, 'indicative' summary.", "labels": [], "entities": []}, {"text": "Our approach, however, differs from theirs in that no special purpose mechanism is used to relate a query to a text, and also the detail of selecting sentences for inclusion in the summary is much simpler.", "labels": [], "entities": []}, {"text": "Further, our evaluation is completely different in that they judge the summaries in terms of their capacity to support a relevancy judgment (i.e. the SUMMAC ad hoc task) whereas we evaluate the summaries in terms of their capacity tosupport a question answering-type task (MUC template filling).", "labels": [], "entities": [{"text": "question answering-type task", "start_pos": 243, "end_pos": 271, "type": "TASK", "confidence": 0.7803236444791158}, {"text": "MUC template filling", "start_pos": 273, "end_pos": 293, "type": "TASK", "confidence": 0.7035902142524719}]}, {"text": "Our technique was tried for 30 of the MUC-6 evaluation texts, and summaries produced using the length and spread criteria, with and without the initial selection of focus chains.", "labels": [], "entities": [{"text": "MUC-6 evaluation texts", "start_pos": 38, "end_pos": 60, "type": "DATASET", "confidence": 0.8260706861813863}, {"text": "length", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9668179154396057}, {"text": "spread", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.5048496723175049}]}, {"text": "The full LaSIE MUC-6 IE system was then run with the summaries as input, and the resulting extracted templates scored.", "labels": [], "entities": [{"text": "LaSIE MUC-6 IE system", "start_pos": 9, "end_pos": 30, "type": "DATASET", "confidence": 0.7958518266677856}]}, {"text": "As a baseline, performance of the LaSIE system with the full texts as input was 45% recall, 64% precision, with 13 of 16 relevant texts correctly identified, by the production of a filled template, and 1 text proposed spuriously.", "labels": [], "entities": [{"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9994346499443054}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.999562680721283}]}, {"text": "Performance using the summaries from each set of criteria was: The relevance figures alone give some measure of the information loss between the full texts and the summaries, but the very low recall in all cases suggests that the summaries are really not suited to this task.", "labels": [], "entities": [{"text": "recall", "start_pos": 192, "end_pos": 198, "type": "METRIC", "confidence": 0.9979227185249329}]}, {"text": "One significant reason for this is that the MUC-6 topic description requires the identification of an event involving 3 instance types -an organisation, a management post and a person -while the production of the summaries is based on the selection of a single chain representing a single instance.", "labels": [], "entities": [{"text": "MUC-6 topic description", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.590978999932607}]}, {"text": "The occurrence of all 3 required instances within the sentences of a single chain can therefore be expected to be rare.", "labels": [], "entities": []}, {"text": "A more fruitful use of the topic description would be to produce a summary from all chains linking the text to the topic, rather than a single 'best' chain.", "labels": [], "entities": []}, {"text": "Alternatively, for event-based topics, chains of event coreference relations could be used.", "labels": [], "entities": []}, {"text": "Out of interest, the evaluation was also run on summaries produced without the criteria requiring a link between the topic and the text, i.e. the generic summaries aiming to capture the single main topic of each text.", "labels": [], "entities": []}, {"text": "The summaries produced are considerably longer than with the topic description, but this gives considerably higher recall for the question answering task.", "labels": [], "entities": [{"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.999278724193573}, {"text": "question answering task", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.8164952794710795}]}, {"text": "However, given that the summariser here is attempting to capture the main topic of each text, independently of the question topic, these results are really a measure of the extent to which the text topics and question topic coincide in the corpus, rather than the suitability of the generic summaries for the question answering task.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 309, "end_pos": 332, "type": "TASK", "confidence": 0.8252737522125244}]}, {"text": "A much more detailed analysis of these initial results is required before firm conclusions can be drawn, but the methodology does represent a useful step towards an automated evaluation procedure.", "labels": [], "entities": []}], "tableCaptions": []}