{"title": [{"text": "Identification of Coreference Between Names and Faces", "labels": [], "entities": [{"text": "Identification of Coreference Between Names and Faces", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.9043289763586861}]}], "abstractContent": [{"text": "To retrieve multimedia contents by their meaning , it is necessary to use not only the contents of distinct media, such as image or language, but also a certain semantic relation holding between them.", "labels": [], "entities": []}, {"text": "For this purpose, in this paper, we propose a method to find coreferences between human names in the article of newspaper and human faces in the accompanying photograph.", "labels": [], "entities": [{"text": "coreferences between human names in the article of newspaper", "start_pos": 61, "end_pos": 121, "type": "TASK", "confidence": 0.675079776181115}]}, {"text": "The method we proposed is based on the machine learning and the hypothesis driven combining method for identifying names and corresponding faces.", "labels": [], "entities": []}, {"text": "Our experimental results show that the recall and precision rate of our method are better than those of the system which uses information exclusively from either text media or image media.", "labels": [], "entities": [{"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9996285438537598}, {"text": "precision rate", "start_pos": 50, "end_pos": 64, "type": "METRIC", "confidence": 0.9841088950634003}]}], "introductionContent": [{"text": "In multimedia contents retrieval, almost all of researches have ibcused on information extracted from single media, e.g. ()).", "labels": [], "entities": [{"text": "multimedia contents retrieval", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.6354870895544688}]}, {"text": "These methods don't take into account semantic relations, like coreference between faces and names, holding between the contents of individual media.", "labels": [], "entities": [{"text": "coreference between faces and names", "start_pos": 63, "end_pos": 98, "type": "TASK", "confidence": 0.8642540335655212}]}, {"text": "In order to retrieve multimedia contents with this kind of relations, it is necessary to find out such relations.", "labels": [], "entities": []}, {"text": "In this research, we use photograph news articles distributed on the Internet and develop a system which identifies a person's name in texts of this type of news articles and her/his face on the accompanying photograph image, based on 1) the machine learning technology applied to individual media contents to build decision trees which extract face regions and human names, and 2) hypothesis based combining method for the results extracted by decision trees of 1).", "labels": [], "entities": []}, {"text": "Since, in general, the number of candidates from image and that from language are more than one, the output of our system is the coreference between a set of face regions and a set of names.", "labels": [], "entities": []}, {"text": "There are many researches in the area of human face recognition ()() and human name extraction, e.g..", "labels": [], "entities": [{"text": "human face recognition", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6176328361034393}, {"text": "human name extraction", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.7218993902206421}]}, {"text": "However, almost all of them deal with the contents of single media and don't take into account the combination of multimedia contents.", "labels": [], "entities": []}, {"text": "As a case of combining multimedia contents, there is a research of captioned images).", "labels": [], "entities": []}, {"text": "Their system analyzes an image and the corresponding caption to identify the coreference between faces in the image and names in the caption.", "labels": [], "entities": []}, {"text": "The text in their research is restricted to captions, which describes contents of the corresponding images.", "labels": [], "entities": []}, {"text": "However, in newspapers or photo news, captions don't always exist and long captions like the captions used in their research are rare.", "labels": [], "entities": []}, {"text": "Therefore, in general, we have to develop a method to capture effective linguistic expressions not from captions but from the body of text itself.", "labels": [], "entities": []}, {"text": "In the research field of the video contents retrieval, although there are many researches,etc), few researches have been done to combine image and language media ( ) ))( . In this field, as language media, there are soundtracks or captions in the video or sometimes in its transcriptions.", "labels": [], "entities": []}, {"text": "For analysis of video contents, the information which consists along the time axis is effective and is used in such systems.", "labels": [], "entities": []}, {"text": "On the other hand, for analysis of still images, some other methods that are different from the methods for video contents retrieval are required because the relatively small amount of and limited information than information from videos are provided.", "labels": [], "entities": [{"text": "video contents retrieval", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.6662054856618246}]}, {"text": "In section 2, the background and our system's overview are stated.", "labels": [], "entities": [{"text": "overview", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9864953756332397}]}, {"text": "In section 3 and 4, we describe the language module and the image module, respectively.", "labels": [], "entities": []}, {"text": "Section 5 describes the com-bining method of the results of the language module and the image module.", "labels": [], "entities": []}, {"text": "In section 6, the experimental results are shown.", "labels": [], "entities": []}, {"text": "Section 7 is our conclusions.", "labels": [], "entities": []}, {"text": "2 System architecture for combining To find coreferences between names in the text and faces in the image of the same photograph news article, we have to extract human names from the text and recognize faces in the image).", "labels": [], "entities": []}, {"text": "The problem is that the face of the person whose name is appearing in a text is not always appearing in the image, and vice versa.", "labels": [], "entities": []}, {"text": "Therefore, we have to develop a method by which we automatically extracts a person whose name appears in the text and simultaneously his/her face appears on the image of the same article.", "labels": [], "entities": []}, {"text": "For the convenience, we define common person, common name and common face as follows.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have experimentally evaluated the system we proposed by comparing with the simple systems which contain only the language module or the image module respectively to confirm the effect of the combining process.", "labels": [], "entities": []}, {"text": "The language module and the image module work under three kinds of hypothesis in the simple systems as well.", "labels": [], "entities": []}, {"text": "Thus, we use the system's result which has the minimum distance between the output of media and the hypothesis defined by formula (6), a.s the baseline of evaluation.", "labels": [], "entities": []}, {"text": "In our experiments, we use the photograph news in the web page called \"AULOS\" distributed by The Mainichi Newspapers.", "labels": [], "entities": [{"text": "AULOS", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9436686038970947}, {"text": "The", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.84538334608078}, {"text": "Mainichi Newspapers", "start_pos": 97, "end_pos": 116, "type": "DATASET", "confidence": 0.8794066905975342}]}, {"text": "The average length of the text of the article is about 300 characters or 100 words.", "labels": [], "entities": []}, {"text": "The almost all of the images are full colored, and the average size of them is about 250 x 200 pixels.", "labels": [], "entities": []}, {"text": "Moreover, the images are not accompanied with captions.", "labels": [], "entities": []}, {"text": "On this evaluation, we use articles with full colored images published on May and June 1997.", "labels": [], "entities": []}, {"text": "As for common name extraction, we did fourfold cross-validation for 228 articles of this period which contains common human names.", "labels": [], "entities": [{"text": "common name extraction", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.8329625527064005}]}, {"text": "As for common face extraction, we did threefold crossvalidation for the set of color photograph images which are contained by the articles used by the language module.", "labels": [], "entities": [{"text": "common face extraction", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.617500881354014}]}, {"text": "To evaluate how accurate the system identifies the given person being a common person, we calculated the recall and precision rate of the system's decision about a person being common.", "labels": [], "entities": [{"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9997592568397522}, {"text": "precision rate", "start_pos": 116, "end_pos": 130, "type": "METRIC", "confidence": 0.9835720658302307}]}, {"text": "Since the outputs of our system are certainties, recall and precision rates are defined as follows.", "labels": [], "entities": [{"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9992571473121643}, {"text": "precision rates", "start_pos": 60, "end_pos": 75, "type": "METRIC", "confidence": 0.9890787303447723}]}, {"text": "Ew where W(i) is the certainty of person i, and cc means a set of all correctly identified persons.", "labels": [], "entities": [{"text": "certainty", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9921695590019226}]}, {"text": "The evaluation results of each module is shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The evaluation results of the outputs  from each module.", "labels": [], "entities": []}]}