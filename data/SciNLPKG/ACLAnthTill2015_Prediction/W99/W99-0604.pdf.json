{"title": [{"text": "Improved Alignment Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Improved Alignment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8649696409702301}, {"text": "Statistical Machine Translation", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.8745033144950867}]}], "abstractContent": [{"text": "In this paper, we describe improved alignment models for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.7337440053621928}]}, {"text": "The statistical translation approach uses two types of information: a translation model and a language model.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7632838785648346}]}, {"text": "The language model used is a bigram or general m-gram model.", "labels": [], "entities": []}, {"text": "The translation model is decomposed into a lexical and an alignment model.", "labels": [], "entities": []}, {"text": "We describe two different approaches for statistical translation and present experimental results.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.8323289155960083}]}, {"text": "The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words.", "labels": [], "entities": []}, {"text": "We present results using the Verbmobil task (German-English, 6000-word vocabulary) which is a limited-domain spoken-language task.", "labels": [], "entities": []}, {"text": "The experimental tests were performed on both the text transcription and the speech recognizer output.", "labels": [], "entities": [{"text": "text transcription", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7173051238059998}, {"text": "speech recognizer", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.6682342439889908}]}, {"text": "1 Statistical Machine Translation The goal of machine translation is the translation of a text given in some source language into a target language.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 2, "end_pos": 33, "type": "TASK", "confidence": 0.7972224354743958}, {"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7479638159275055}]}, {"text": "We are given a source string f/= fl...fj...fJ, which is to be translated into a target string e{ = el...ei...ex.", "labels": [], "entities": []}, {"text": "Among all possible target strings, we will choose the string with the highest probability: = argmax {Pr(ezIlflJ)} e 1 = argmax {Pr(e[).", "labels": [], "entities": []}, {"text": "Pr(f/le~) } \u2022 (1) The argmax operation denotes the search problem , i.e. the generation of the output sentence in the target language.", "labels": [], "entities": []}, {"text": "Pr(e{) is the language model of the target language, whereas Pr (ff~lel I) is the translation model.", "labels": [], "entities": []}, {"text": "Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-to-word correspondences between source and target words.", "labels": [], "entities": []}, {"text": "The model is often further restricted that each source word is assigned exactly one target word.", "labels": [], "entities": []}, {"text": "These alignment models are sire-ilar to the concept of Hidden Markov models (HMM) in speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.743913322687149}]}, {"text": "The alignment mapping is j ~ i = aj from source position j to target position i = aj.", "labels": [], "entities": []}, {"text": "The use of this alignment model raises major problems as it fails to capture dependencies between groups of words.", "labels": [], "entities": []}, {"text": "As experiments have shown it is difficult to handle different word order and the translation of compound nouns\u2022 In this paper, we will describe two methods for statistical machine translation extending the baseline alignment model in order to account for these problems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 160, "end_pos": 191, "type": "TASK", "confidence": 0.6285397509733835}]}, {"text": "In section 2, we shortly review the single-word based approach described in (Tillmann et al., 1997) with some recently ira-plemented extensions allowing for one-to-many alignments.", "labels": [], "entities": []}, {"text": "In section 3 we describe the alignment template approach which explicitly models shallow phrases and in doing so tries to overcome the above mentioned restrictions of single-word alignments.", "labels": [], "entities": []}, {"text": "The described method is an improvement of (Och and Weber, 1998), resulting in an improved training and a faster search organization.", "labels": [], "entities": []}, {"text": "The basic idea is to model two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words within these phrases.", "labels": [], "entities": []}, {"text": "Similar aims are pursued by (Alshawi et al., 1998; Wang and Waibel, 1998) but differently approached.", "labels": [], "entities": []}, {"text": "In section 4 we compare the two methods using the Verbmobil task.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Here the term word  refers to full-form word as there is no morpho- logical processing involved. In some of our ex-", "labels": [], "entities": []}, {"text": " Table 1: Training and test conditions for the  Verbmobil task. The extended vocabulary in- cludes the words of the manual dictionary. The  trigram perplexity (PP) is given.", "labels": [], "entities": [{"text": "trigram perplexity (PP)", "start_pos": 140, "end_pos": 163, "type": "METRIC", "confidence": 0.9529570817947388}]}, {"text": " Table 2: Experiments for Text and Speech Input: Word error rate (WER), position- independent word error rate (PER) and subjective sentence error rate (SSER) with/without pre- processing (147 sentences = 1 968 words of the Verbmobil task).", "labels": [], "entities": [{"text": "Text and Speech Input", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.6024911999702454}, {"text": "Word error rate (WER)", "start_pos": 49, "end_pos": 70, "type": "METRIC", "confidence": 0.9182169934113821}, {"text": "position- independent word error rate (PER)", "start_pos": 72, "end_pos": 115, "type": "METRIC", "confidence": 0.8198354012436337}, {"text": "subjective sentence error rate (SSER)", "start_pos": 120, "end_pos": 157, "type": "METRIC", "confidence": 0.8209303064005715}]}]}