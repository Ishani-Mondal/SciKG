{"title": [{"text": "Boosting Applied to Tagging and PP Attachment", "labels": [], "entities": [{"text": "Tagging", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9733272194862366}, {"text": "PP Attachment", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.8517535328865051}]}], "abstractContent": [{"text": "Boosting is a machine learning algorithm that is not well known in computational linguistics.", "labels": [], "entities": []}, {"text": "We apply it to part-of-speech tagging and prepositional phrase attachment.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7251339852809906}, {"text": "prepositional phrase attachment", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6185211141904196}]}, {"text": "We also show how to improve data quality by using boosting to identify annotation errors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Boosting is a machine learning algorithm that has been applied successfully to a variety of problems, but is almost unknown in computational linguistics.", "labels": [], "entities": []}, {"text": "We describe experiments in which we apply boosting to part-of-speech tagging and prepositional phrase attachment.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.6994919776916504}, {"text": "prepositional phrase attachment", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.6032781501611074}]}, {"text": "Results on both PP-attachment and tagging are within sampling error of the best previous results.", "labels": [], "entities": [{"text": "tagging", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9708372950553894}]}, {"text": "The current best technique for PP-attachment (backed-off density estimation) does not perform well for tagging, and the current best technique for tagging (maxent) is below state-of-the-art on PPattachment.", "labels": [], "entities": [{"text": "tagging", "start_pos": 103, "end_pos": 110, "type": "TASK", "confidence": 0.9724346995353699}]}, {"text": "Boosting achieves state-of-the-art performance on both tasks simultaneously.", "labels": [], "entities": []}, {"text": "The idea of boosting is to combine many simple \"rules of thumb,\" such as \"the current word is a noun if the previous word is the.\"", "labels": [], "entities": []}, {"text": "Such rules often give incorrect classifications.", "labels": [], "entities": []}, {"text": "The main idea of boosting is to combine many such rules in a principled manner to produce a single highly accurate classification rule.", "labels": [], "entities": [{"text": "boosting", "start_pos": 17, "end_pos": 25, "type": "TASK", "confidence": 0.9659613966941833}]}, {"text": "There are similarities between boosting and transformation-based learning: both build classifiers by combining simple rules, and both are noted for their resistance to overfitting.", "labels": [], "entities": []}, {"text": "But boosting, unlike transformation-based learning, rests on firm theoretical foundations; and it outperforms transformation-based learning in our experiments.", "labels": [], "entities": []}, {"text": "There are also superficial similarities between boosting and maxent.", "labels": [], "entities": []}, {"text": "In both, the parameters are weights in a log-linear function.", "labels": [], "entities": []}, {"text": "But in maxent, the log-linear function defines a probability, and the objective is to maximize likelihood, which may not minimize classification error.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.5154066681861877}]}, {"text": "In boosting, the loglinear function defines a hyperplane dividing examples into (binary) classes, and boosting minimizes classification error directly.", "labels": [], "entities": []}, {"text": "Hence boosting is usually more appropriate when the objective is classification rather than density estimation.", "labels": [], "entities": []}, {"text": "A notable property of boosting is that it maintains an explicit measure of how difficult it finds particular training examples to be.", "labels": [], "entities": []}, {"text": "The most difficult examples are very often mislabelled examples.", "labels": [], "entities": []}, {"text": "Hence, boosting can contribute to improving data quality by identifying annotation errors.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance of the multi-discriminator approach.", "labels": [], "entities": []}, {"text": " Table 1. These results are  all based on the Treebank corpus, but it appears that  they do not all use the same training-test split, nor  the same preprocessing, hence there may be differ- ences in details of examples and labels. The \"MF  tag\" method simply uses the most-frequent tag from  training as the predicted label. The voting scheme  combines the outputs of four other taggers.", "labels": [], "entities": [{"text": "Treebank corpus", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.9250443875789642}]}]}