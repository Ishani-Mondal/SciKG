{"title": [{"text": "K-means and Graph-based Approaches for Chinese Word Sense Induction Task", "labels": [], "entities": [{"text": "Chinese Word Sense Induction", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.6344996392726898}]}], "abstractContent": [{"text": "This paper details our experiments carried out at Word Sense Induction task.", "labels": [], "entities": [{"text": "Word Sense Induction task", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.7885209918022156}]}, {"text": "For the foreign language (especially English), there have been many studies of word sense induction (WSI), and the approaches and the techniques are more and more mature.", "labels": [], "entities": [{"text": "word sense induction (WSI)", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.8423200647036234}]}, {"text": "However, the study of Chinese WSI is just getting started, and there has not been a better way to solve the problems encountered.", "labels": [], "entities": [{"text": "Chinese WSI", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.6939347982406616}]}, {"text": "WSI can be divided into two categories: supervised manner and unsupervised manner.", "labels": [], "entities": [{"text": "WSI", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8741630911827087}]}, {"text": "But in the light of the high cost of supervised manner, we introduce novel solutions to automatic and unsupervised WSI.", "labels": [], "entities": [{"text": "WSI", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.897431492805481}]}, {"text": "In this paper, we propose two different systems.", "labels": [], "entities": []}, {"text": "The first one is called K-means-based Chinese word sense induction in an unsupervised manner while the second one is graph-based Chinese word sense induction.", "labels": [], "entities": [{"text": "K-means-based Chinese word sense induction", "start_pos": 24, "end_pos": 66, "type": "TASK", "confidence": 0.5344158172607422}, {"text": "Chinese word sense induction", "start_pos": 129, "end_pos": 157, "type": "TASK", "confidence": 0.6115694642066956}]}, {"text": "In the experiments, the first system has achieved a 0.7729 Fscore on average while the second one has achieved a 0.6067 Fscore.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9939190149307251}, {"text": "Fscore", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9823222756385803}]}], "introductionContent": [{"text": "No matter in which kind of language, ambiguous terms always exist, Chinese is also not exceptional.", "labels": [], "entities": []}, {"text": "According to statistics, although the percent of ambiguous terms in Chinese dictionary is only about 14.8%, the frequency of them is up to 42% in Chinese corpora.", "labels": [], "entities": [{"text": "frequency", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9663385152816772}]}, {"text": "This phenomenon shows that the number of ambiguous terms is small in natural language, but their frequency is extremely high.", "labels": [], "entities": [{"text": "frequency", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9743825197219849}]}, {"text": "Therefore, the key step in natural language processing (NLP) is to identify the specific meaning of a given target word according to its context.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.8275726735591888}]}, {"text": "In this task, the input to a WSI algorithm is the sentences including the same ambiguous term, and our task is to cluster these sentences into different categories according to the meanings of this ambiguous term in every sentence.", "labels": [], "entities": []}, {"text": "The study of WSI is earlier abroad and there has been a set of well-developed theories by now.", "labels": [], "entities": [{"text": "WSI", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.8804897665977478}]}, {"text": "However, the start of studying Chinese WSI is later and we need to find a better and appropriate way for Chinese WSI.", "labels": [], "entities": [{"text": "Chinese WSI", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.5952160060405731}]}, {"text": "In this paper, we develop two different systems.", "labels": [], "entities": []}, {"text": "The first one is based on K-means algorithm which optimizes the initial centers and a Chinese thesaurus -TongYiCi CiLin is used to solve the problem of sparseness of a sentence's vector.", "labels": [], "entities": []}, {"text": "The second one is a combination approach of graph-based clustering and K-means algorithm.", "labels": [], "entities": []}, {"text": "We choose Chinese Whisper as the graph-based clustering approach.", "labels": [], "entities": [{"text": "Chinese Whisper", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.9286069869995117}]}], "datasetContent": [{"text": "K-means algorithm has a good performance for small corpus, but when the corpus size is too big, vector dimension will increase rapidly.", "labels": [], "entities": []}, {"text": "So At first we use Chinese Whisper to cluster the words in the corpus after preprocessing, such as splitting the sentences, filtering stopwords and selecting context.", "labels": [], "entities": []}, {"text": "Secondly we construct corpus vectors with VSM, and now the vector dimension is decreased to the number of clusters.", "labels": [], "entities": []}, {"text": "At last we cluster the vectors using K-means algorithm analogous to the first system.", "labels": [], "entities": []}, {"text": "The choice of parameters is an important factor in Chinese Whisper and different parameters will result in different clusters.", "labels": [], "entities": [{"text": "Chinese Whisper", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.8885407149791718}]}, {"text": "In this experiment we use batch process method in order to select the best parameters on training set.", "labels": [], "entities": []}, {"text": "We select a group of parameters: convergence constant is from 0 to 1 and the step length is 0.1; iterations is from 1 to 30 and the step length is 1, which depends on the size of corpus.", "labels": [], "entities": [{"text": "convergence", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9936743974685669}]}, {"text": "The process of experiment is as follows: (1) Get a pair of parameters from the parameter group, cluster the corpus using Chinese Whisper, and then remove this pair of parameter from the parameter group.", "labels": [], "entities": [{"text": "Chinese Whisper", "start_pos": 121, "end_pos": 136, "type": "DATASET", "confidence": 0.8994121253490448}]}, {"text": "(2) Construct vectors using the result of step (1).", "labels": [], "entities": []}, {"text": "(3) Cluster the vectors using K-means.", "labels": [], "entities": []}, {"text": "The results are as the following two tables.", "labels": [], "entities": []}, {"text": "From we can see that if we use JC method to add new edges, the precision has a great improvement.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9995529055595398}]}, {"text": "In the experimental result, we have achieved 0.6067 Fscore on 100 ambiguous words with the parameters: 0.8 and 12.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9980742931365967}]}], "tableCaptions": [{"text": " Table 1 Information gain of every position of  context  Left context  Right context  Position Information  gain", "labels": [], "entities": []}, {"text": " Table 2 Experimental results without using JC  method  converge  constance", "labels": [], "entities": []}, {"text": " Table 3 Experimental results using JC method  converge  constance", "labels": [], "entities": []}]}