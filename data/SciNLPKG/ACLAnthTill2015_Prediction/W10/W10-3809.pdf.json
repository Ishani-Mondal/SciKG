{"title": [{"text": "A Discriminative Approach for Dependency Based Statistical Machine Translation", "labels": [], "entities": [{"text": "Dependency Based Statistical Machine Translation", "start_pos": 30, "end_pos": 78, "type": "TASK", "confidence": 0.6535878419876099}]}], "abstractContent": [{"text": "In this paper, we propose a dependency based statistical system that uses discrim-inative techniques to train its parameters.", "labels": [], "entities": []}, {"text": "We conducted experiments on an English-Hindi parallel corpora.", "labels": [], "entities": []}, {"text": "The use of syntax (dependency tree) allows us to address the large word-reorderings between English and Hindi.", "labels": [], "entities": []}, {"text": "And, discriminative training allows us to use rich feature sets, including linguistic features that are useful in the machine translation task.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 118, "end_pos": 142, "type": "TASK", "confidence": 0.8439263900121053}]}, {"text": "We present results of the experimental implementation of the system in this paper.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntax based approaches for Machine Translation (MT) have gained popularity in recent times because of their ability to handle long distance reorderings (, especially for divergent language pairs such as English-Hindi (or English-Urdu).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8713352918624878}]}, {"text": "Languages such as Hindi are also known for their rich morphology and long distance agreement of features of syntactically related units.", "labels": [], "entities": []}, {"text": "The morphological richness can be handled by employing techniques that factor the lexical items into morphological factors.", "labels": [], "entities": []}, {"text": "This strategy is also useful in the context of EnglishHindi MT () where there is very limited parallel corpora available, and breaking words into smaller units helps in reducing sparsity.", "labels": [], "entities": [{"text": "EnglishHindi MT", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.8213294446468353}]}, {"text": "In order to handle phenomenon such as long-distance word agreement to achieve accurate generation of target language words, the inter-dependence between the factors of syntactically related words need to be modelled effectively.", "labels": [], "entities": []}, {"text": "Some of the limitations with the syntax based approaches such as () are, (1) They do not offer flexibility for adding linguistically motivated features, and (2) It is not possible to use morphological factors in the syntax based approaches.", "labels": [], "entities": []}, {"text": "Ina recent work, linguistic and contextual information was effectively used in the framework of a hierarchical machine translation system.", "labels": [], "entities": [{"text": "hierarchical machine translation", "start_pos": 98, "end_pos": 130, "type": "TASK", "confidence": 0.6704613069693247}]}, {"text": "In their work, four linguistic and contextual features are used for accurate selection of translation rules.", "labels": [], "entities": []}, {"text": "In our approach in contrast, linguistically motivated features can be defined that directly effect the prediction of various elements in the target during the translation process.", "labels": [], "entities": []}, {"text": "This features use syntactic labels and collocation statistics in order to allow effective training of the model.", "labels": [], "entities": []}, {"text": "Some of the other approaches related to our model are the Direct Translation Model 2 (DTM2), End-to-End Discriminative Approach to MT () and Factored Translation Models (.", "labels": [], "entities": [{"text": "Direct Translation", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.6585103571414948}, {"text": "MT", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.8395922780036926}, {"text": "Factored Translation", "start_pos": 141, "end_pos": 161, "type": "TASK", "confidence": 0.6980746686458588}]}, {"text": "In DTM2, a discriminative trans-lation model is defined in the setting of a phrase based translation system.", "labels": [], "entities": [{"text": "phrase based translation", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.642960766951243}]}, {"text": "In their approach, the features are optimized globally.", "labels": [], "entities": []}, {"text": "In contrast to their approach, we define a discriminative model for translation in the setting of a syntax based machine translation system.", "labels": [], "entities": [{"text": "translation", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.971674382686615}]}, {"text": "This allows us to use both the power of a syntax based approach, as well as, the power of a large feature space during translation.", "labels": [], "entities": []}, {"text": "In our approach, the weights are optimized in order to achieve an accurate prediction of the individual target nodes, and their relative positions.", "labels": [], "entities": []}, {"text": "We propose an approach for syntax based statistical machine translation which models the following aspects of language divergence effectively.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.6225690941015879}, {"text": "language divergence", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7063322812318802}]}, {"text": "\u2022 Word-order variation including longdistance reordering which is prevalent between language pairs such as EnglishHindi and English-Japanese.", "labels": [], "entities": []}, {"text": "\u2022 Generation of word-forms in the target language by predicting the word and its factors.", "labels": [], "entities": []}, {"text": "During prediction, the inter-dependence of factors of the target word form with the factors of syntactically related words is considered.", "labels": [], "entities": [{"text": "prediction", "start_pos": 7, "end_pos": 17, "type": "TASK", "confidence": 0.9705018997192383}]}, {"text": "To accomplish this goal, we visualize the problem of MT as transformation from a morphologically analyzed source syntactic structure to a target syntactic structure 1 (See.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9949133396148682}]}, {"text": "The transformation is factorized into a series of minitransformations, which we address as features of the transformation.", "labels": [], "entities": []}, {"text": "The features denote the various linguistic modifications in the source structure to obtain the target syntactic structure.", "labels": [], "entities": []}, {"text": "Some of the examples of features are lexical translation of a particular source node, the ordering at a particular source node etc.", "labels": [], "entities": [{"text": "lexical translation of a particular source node", "start_pos": 37, "end_pos": 84, "type": "TASK", "confidence": 0.7875994741916656}]}, {"text": "These features can be entirely local to a particular node in the syntactic structure or can span across syntactically related entities.", "labels": [], "entities": []}, {"text": "More about the features (or mini-transformations) is explained in section 3.", "labels": [], "entities": []}, {"text": "The transformation of a source syntactic structure is scored by taking a weighted sum of its features 2 . Let \u03c4 represent 1 Note that target structure contains only the target factors.", "labels": [], "entities": []}, {"text": "An accurate and deterministic morphological generator combines these factors to produce the target word form.", "labels": [], "entities": []}, {"text": "The features can be either binary-values or real-valued the transformation of source syntactic structure s, the score of transformation is computed as represented in Equation 1.", "labels": [], "entities": []}, {"text": "In Equation 1, f i s are the various features of transformation and w i s are the weights of the features.", "labels": [], "entities": []}, {"text": "The strength of our approach lies in the flexibility it offers in incorporating linguistic features that are useful in the task of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.806234747171402}]}, {"text": "These features are also known as prediction features as they map from source language information to information in the target language that is being predicted.", "labels": [], "entities": []}, {"text": "During decoding a source sentence, the goal is to choose a transformation that has the highest score.", "labels": [], "entities": []}, {"text": "The source syntactic structure is traversed in a bottom-up fashion and the target syntactic structure is simultaneously built.", "labels": [], "entities": []}, {"text": "We used a bottom-up traversal while decoding because it builds a contiguous sequence of nodes for the subtrees during traversal enabling the application of a wide variety of language models.", "labels": [], "entities": []}, {"text": "In the training phase, the task is to learn the weights of features.", "labels": [], "entities": []}, {"text": "We use an online largemargin training algorithm, MIRA), for learning the weights.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9073411226272583}]}, {"text": "The weights are locally updated at every source node during the bottom-up traversal of the source structure.", "labels": [], "entities": []}, {"text": "For training the translation model, automatically obtained word-aligned parallel corpus is used.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9676448702812195}]}, {"text": "We used GIZA++ along with the growing heuristics to word-align the training corpus.", "labels": [], "entities": []}, {"text": "The basic factors of the word used in our experiments are root, part-of-speech, gender, number and person.", "labels": [], "entities": []}, {"text": "In Hindi, common nouns and verbs have gender information whereas, English doesn't contain that information.", "labels": [], "entities": []}, {"text": "Apart from the basic factors, we also consider the role information provided by labelled dependency parsers.", "labels": [], "entities": []}, {"text": "For computing the dependency tree on the source side, We used stanford parser ( in the experiments presented in this chapter . The function words such as prepositions and auxiliary verbs largely express the grammatical roles/functions of the content words in the sentence.", "labels": [], "entities": []}, {"text": "In fact, in many agglutinative languages, these words are commonly attached to the content word to form one word form.", "labels": [], "entities": []}, {"text": "In this paper, we also conduct experiments where we begin by grouping the function words with their corresponding function words.", "labels": [], "entities": []}, {"text": "These groups of words are called local-word groups.", "labels": [], "entities": []}, {"text": "In these cases, the function words are considered as factors of the content words.", "labels": [], "entities": []}, {"text": "Section 2 explains more about the local word groups in English and Hindi.", "labels": [], "entities": []}], "datasetContent": [{"text": "The important aspects of the translation model proposed in this paper have been implemented.", "labels": [], "entities": [{"text": "translation", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9803243279457092}]}, {"text": "Some of the components that handle word insertions and non-projective transformations have not yet been implemented in the decoder, and should be considered beyond the scope of this paper.", "labels": [], "entities": [{"text": "word insertions", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.751640647649765}]}, {"text": "The focus of this work has been to build a working syntax based statistical machine translation system, which can act as a platform for further experiments on similar lines.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.6222048004468282}]}, {"text": "The system would be available for download at http://shakti.iiit.ac.in/\u223csriram/vaanee.html.", "labels": [], "entities": []}, {"text": "To evaluate this experimental system, a restricted set of experiments are conducted.", "labels": [], "entities": []}, {"text": "The experiments are conducted on the English-Hindi language pair using a corpus in tourism domain containing 11300 sentence pairs 6 .  The dataset currently available for EnglishHindi language pair is noisy.", "labels": [], "entities": []}, {"text": "This is an extremely large limiting factor fora model which uses rich linguistic information within the statistical framework.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Training Statistics -Effect of Beam Size", "labels": [], "entities": [{"text": "Beam Size", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.8592965006828308}]}, {"text": " Table 3: Training Statistics -Effect of maximum  update attempts", "labels": [], "entities": [{"text": "Effect", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9718531966209412}, {"text": "update", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.8421717286109924}]}, {"text": " Table 4: Training Statistics -Effect of number of  iterations", "labels": [], "entities": []}, {"text": " Table 5: Weights of dice coefficient based features", "labels": [], "entities": [{"text": "Weights", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.935849130153656}]}]}