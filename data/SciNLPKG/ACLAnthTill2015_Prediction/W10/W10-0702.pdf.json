{"title": [{"text": "Corpus Creation for New Genres: A Crowdsourced Approach to PP Attachment", "labels": [], "entities": [{"text": "Corpus Creation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7751676440238953}, {"text": "PP Attachment", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.8724136650562286}]}], "abstractContent": [{"text": "This paper explores the task of building an accurate prepositional phrase attachment corpus for new genres while avoiding a large investment in terms of time and money by crowd-sourcing judgments.", "labels": [], "entities": [{"text": "prepositional phrase attachment corpus", "start_pos": 53, "end_pos": 91, "type": "TASK", "confidence": 0.7264804989099503}]}, {"text": "We develop and present a system to extract prepositional phrases and their potential attachments from ungrammati-cal and informal sentences and pose the subsequent disambiguation tasks as multiple choice questions to workers from Amazon's Mechanical Turk service.", "labels": [], "entities": []}, {"text": "Our analysis shows that this two-step approach is capable of producing reliable annotations on informal and potentially noisy blog text, and this semi-automated strategy holds promise for similar annotation projects in new genres.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent decades have seen rapid development in natural language processing tools for parsing, semantic role-labeling, machine translation, etc., and much of this success can be attributed to the study of statistical techniques and the availability of large annotated corpora for training.", "labels": [], "entities": [{"text": "parsing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9711697697639465}, {"text": "machine translation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.8093296885490417}]}, {"text": "However, the performance of these systems is heavily dependent on the domain and genre of their training data, i.e. systems trained on data from a particular domain tend to perform poorly when applied to other domains and adaptation techniques are not always able to compensate (.", "labels": [], "entities": []}, {"text": "For this reason, achieving high performance on new domains and genres frequently necessitates the collection of annotated training data from those domains and genres, a timeconsuming and frequently expensive process.", "labels": [], "entities": []}, {"text": "This paper examines the problem of collecting high-quality annotations for new genres with a focus on time and cost efficiency.", "labels": [], "entities": []}, {"text": "We explore the wellstudied but non-trivial task of prepositional phrase (PP) attachment and describe a semi-automated system for identifying accurate attachments in blog data, which is frequently noisy and difficult to parse.", "labels": [], "entities": [{"text": "prepositional phrase (PP) attachment", "start_pos": 51, "end_pos": 87, "type": "TASK", "confidence": 0.6914633363485336}]}, {"text": "PP attachment disambiguation involves finding a correct attachment fora prepositional phrase in a sentence.", "labels": [], "entities": [{"text": "PP attachment disambiguation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8793143630027771}]}, {"text": "For example, in the sentence \"We went to John's house on Saturday\", the phrase \"on Saturday\" attaches to the verb \"went\".", "labels": [], "entities": []}, {"text": "In another example, \"We went to John's house on 12th Street\", the PP \"on 12th street\" attaches to the noun \"John's house\".", "labels": [], "entities": [{"text": "John's house on 12th Street", "start_pos": 32, "end_pos": 59, "type": "DATASET", "confidence": 0.8058445354302725}, {"text": "PP", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9320335984230042}]}, {"text": "This sort of disambiguation requires semantic knowledge about sentences that is difficult to glean from their surface form, a problem which is compounded by the informal nature and irregular vocabulary of blog text.", "labels": [], "entities": []}, {"text": "In this work, we investigate whether crowdsourced human judgments are capable of distinguishing appropriate attachments.", "labels": [], "entities": []}, {"text": "We present a system that simplifies the attachment problem and represents it in a format that can be intuitively tackled by humans.", "labels": [], "entities": []}, {"text": "Our approach to this task makes use of a heuristicbased system built on a shallow parser that identifies the likely words or phrases to which a PP can attach.", "labels": [], "entities": []}, {"text": "To subsequently select the correct attachment, we leverage human judgments from multiple untrained annotators (referred to here as workers) through Amazon's Mechanical Turk 1 , an online marketplace for work.", "labels": [], "entities": []}, {"text": "This two-step approach of-fers distinct advantages: the automated system cuts down the space of potential attachments effectively with little error, and the disambiguation task can be reduced to small multiple choice questions which can be tackled quickly and aggregated reliably.", "labels": [], "entities": []}, {"text": "The remainder of this paper focuses on the PP attachment task over blog text and our analysis of the resulting aggregate annotations.", "labels": [], "entities": [{"text": "PP attachment task", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.92860480149587}]}, {"text": "We note, however, that this type of semi-automated approach is potentially applicable to any task which can be reliably decomposed into independent judgments that untrained annotators can tackle (e.g., quantifier scoping, conjunction scope).", "labels": [], "entities": []}, {"text": "This work is intended as an initial step towards the development of efficient hybrid annotation tools that seamlessly incorporate aggregate human wisdom alongside effective algorithms.", "labels": [], "entities": []}], "datasetContent": [{"text": "An experimental study was undertaken to test our hypothesis that we could obtain reliable annotations on informal genres using MTurk workers.", "labels": [], "entities": []}, {"text": "Here we describe the dataset and our methods.", "labels": [], "entities": []}, {"text": "We used a corpus of blog posts made on LiveJournal for system development and evaluation.", "labels": [], "entities": []}, {"text": "Only posts from English-speaking countries (i.e. USA, Canada, UK, Australia and New Zealand) were considered for this study.", "labels": [], "entities": []}, {"text": "The interface provided to MTurk workers showed the sentence on a plain background with the PP highlighted and a statement prompting them to pick the phrase in the sentence that the given PP modified.", "labels": [], "entities": []}, {"text": "The question was followed by a list of options.", "labels": [], "entities": []}, {"text": "In addition, we provided MTurk workers the option to indicate problems with the given PP or the listed options.", "labels": [], "entities": []}, {"text": "Workers could write in the correct attachment if they determined that it wasn't present in the list of options, or the correct PP if the one they were presented with was malformed.", "labels": [], "entities": []}, {"text": "This allowed them to correct errors made by the chunker and automated attachment point predictor.", "labels": [], "entities": []}, {"text": "In all cases, workers were forced to pick the best answer among the options regardless of errors.", "labels": [], "entities": []}, {"text": "We also supplied a num-4 http://www.livejournal.com ber of examples covering both well-formed and erroneous cases to aid them in identifying appropriate attachments.", "labels": [], "entities": []}, {"text": "For our experiment, we randomly selected 1000 questions from the output produced by the system and provided each question to five different MTurk workers, thereby obtaining five different judgments for each PP attachment case.", "labels": [], "entities": []}, {"text": "Workers were paid four cents per question and the average completion time per task was 48 seconds.", "labels": [], "entities": [{"text": "completion time", "start_pos": 58, "end_pos": 73, "type": "METRIC", "confidence": 0.9607468247413635}]}, {"text": "In total $225 was spent on the full study with $200 spent on the workers and $25 on MTurk fees.The total time taken for the study was approximately 16 hours.", "labels": [], "entities": []}, {"text": "A pilot study was carried outwith 50 sentences before the full study to test the annotation interface and experiment with different ways of presenting the PP and attachment options to workers.", "labels": [], "entities": []}, {"text": "During this study, we observed that while workers were willing to suggest correct answers or PPs when faced with erroneous questions, they often opted to not pick any of the options provided unless the question was well-formed.", "labels": [], "entities": []}, {"text": "This was problematic because, in many cases, expert annotators were able to identify the most appropriate attachment option.", "labels": [], "entities": []}, {"text": "Therefore, in the final study we forced them to pick the most suitable option from the given choices before indicating errors and writing in alternatives.", "labels": [], "entities": []}, {"text": "In order to determine if the MTurk results were reliable, worker responses had to be validated by having expert annotators perform the same task.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 29, "end_pos": 34, "type": "TASK", "confidence": 0.9273102283477783}]}, {"text": "For this purpose, two of the authors annotated the 1000 questions used for the experiment independently and compared their judgments.", "labels": [], "entities": []}, {"text": "Disagreements were observed in 127 cases; these were then resolved by a pool of non-author annotators.", "labels": [], "entities": []}, {"text": "If all three annotators on a case disagreed with each other the question was discarded; this situation occured 43 times.", "labels": [], "entities": []}, {"text": "An additional 16 questions were discarded because they did not have a valid PP.", "labels": [], "entities": []}, {"text": "For example, \"I am painting with my blanket on today\".", "labels": [], "entities": []}, {"text": "Here \"on today\" is incorrectly extracted as a PP because the particle \"on\" is tagged as a preposition.", "labels": [], "entities": []}, {"text": "The rest of the analysis presented in this section was performed on the remaining 941 sentences.", "labels": [], "entities": []}, {"text": "The annotators' judgments were compared to the answers provided by the MTurk workers and, in the case of disagreement between the experts and the majority of workers, the sentences were manually inspected to determine the reason.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.8802869915962219}]}, {"text": "In five cases, more than one valid attachment was possible; for example, in the sentence \"The video below is of my favourite song on the album -A Real Woman\", the PP \"of my favourite song\" could attach to either the noun phrase \"the video\" or the verb \"is\" and conveys the same meaning.", "labels": [], "entities": []}, {"text": "In such cases, both the experts and the workers were considered to have chosen the correct answer.", "labels": [], "entities": []}, {"text": "In 149 cases, the workers also augmented their choices by providing corrections to incomplete answers and badly constructed PPs.", "labels": [], "entities": []}, {"text": "For example, the PP \"of the Rings and Mikey\" in the sentence \"Samwise from Lord of the Rings and Mikey from The Goonies are the same actor ?\" was corrected to \"of the Rings\".", "labels": [], "entities": []}, {"text": "In 34/39 of the cases where the correct answer was not present in the options provided, at least one worker indicated correct attachment for the PP.", "labels": [], "entities": []}, {"text": "We measure the recall for our attachment point predictor as the number of questions for which the correct attachment appeared among the generated options divided by the total number of questions.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9992068409919739}]}, {"text": "The system achieves a recall of 95.85% (902/941 questions).", "labels": [], "entities": [{"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9897432923316956}]}, {"text": "We observed that in many cases where the correct attachment point was not predicted, it was due to a chunker error.", "labels": [], "entities": []}, {"text": "For example, in the following sentence, \"Stop all the clocks , cutoff the telephone , Prevent the dog from barking with a juicy bone...\", the PP \"from barking\" attaches to the verb \"Prevent\"; however, due to an error in chunking \"Prevent\" is tagged as a noun phrase and hence is not picked by our system.", "labels": [], "entities": []}, {"text": "The correct attachment was also occasionally missed when the attachment point was too far from the PP.", "labels": [], "entities": [{"text": "correct attachment", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.8805923759937286}]}, {"text": "For example, in the sentence \"Fitting as many people as possible on one sofa and under many many covers and getting intimate\", the correct attachment for the PP \"under many many covers\" is the verb \"Fitting\" but it is not picked by our system.", "labels": [], "entities": []}, {"text": "Even though the correct attachment was not always given, the workers could still provide their own correct answer.", "labels": [], "entities": []}, {"text": "In the first example above, 3/5 workers indicated that the correct attachment was not in the list of options and wrote it in. summarizes the results of the experiment.", "labels": [], "entities": []}, {"text": "We assess both the coverage and reliability of worker predictions at various levels of worker agreement.", "labels": [], "entities": [{"text": "coverage", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9914615154266357}, {"text": "reliability", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9805979132652283}]}, {"text": "This serves as an indicator of the effectiveness of the MTurk results: the accuracy can betaken as a general confidence measure for worker predictions; when five workers agree we can be 97.43% confident in the correctness of their prediction, when at least four workers agree we can be 94.63% confident, etc.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 56, "end_pos": 61, "type": "TASK", "confidence": 0.849184513092041}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9982744455337524}]}, {"text": "Unanimity indicates that all workers agreed on an answer, majority indicates that more than half of workers agreed on an answer, and plurality indicates that two workers agreed on a single answer, while the remaining three workers each selected different answers.", "labels": [], "entities": [{"text": "Unanimity", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9366682767868042}]}, {"text": "We observe that at high levels of worker agreement, we get extremely high accuracy but limited coverage of the data set; as we decrease our standard for agreement, coverage increases rapidly while accuracy remains relatively high.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.998281717300415}, {"text": "coverage", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9972952008247375}, {"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9987200498580933}]}, {"text": "shows the number of workers providing the correct answer on a per-question basis.", "labels": [], "entities": []}, {"text": "This illustrates the distribution of worker agreements across questions.", "labels": [], "entities": []}, {"text": "Note that in the majority of cases (69.2%), at least four workers provided the correct answer; in only 3.6% of cases were no workers able to select the correct attachment.", "labels": [], "entities": []}, {"text": "shows the distribution of worker agreements.", "labels": [], "entities": []}, {"text": "Unlike, these figures are not cumulative and include non-plurality two-worker agreements.", "labels": [], "entities": []}, {"text": "Note that the number of agreements discussed in this figure is greater than the 941 evaluated because in some cases there were multiple agreements on a single question.", "labels": [], "entities": []}, {"text": "As an example, three workers may choose one answer while the remaining two workers choose another; this question then produces both a three-worker agreement as well as a two-worker agreement.", "labels": [], "entities": []}, {"text": "All questions on which there is agreement also produce a majority vote, with one exception: the 2/2/1 agreement.", "labels": [], "entities": []}, {"text": "Although the correct answer was selected by one set of two workers in every case of 2/2/1 agreement, this is not particularly useful for corpus-building as we have noway to identify a priori which set is correct.", "labels": [], "entities": []}, {"text": "Fortunately, 2/2/1 agreements were also quite rare and occurred in only 3% of cases.", "labels": [], "entities": []}, {"text": "appears to indicate that instances of agreement between two workers are unlikely to produce good attachments; they have a an average accuracy of 37.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9989175796508789}]}, {"text": "However, this is due in large part to cases of 3/2 agreement, in which the two workers in the minority are usually wrong, as well as cases of 2/2/1 agreement which contain at least one incorrect instance of two-worker agreement.", "labels": [], "entities": []}, {"text": "However, if we only consider cases in which the two-worker agreement forms a plurality (i.e. all other workers disagree amongst themselves), we observe an average accuracy of 64.3% which is similar to that of cases of three-worker agreement (67.7%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9990401864051819}]}], "tableCaptions": [{"text": " Table 2: Accuracy and coverage over agreement thresholds", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990230798721313}, {"text": "coverage", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9908779263496399}]}, {"text": " Table 3: Variation in worker performance with the num- ber of attachment options presented", "labels": [], "entities": []}]}