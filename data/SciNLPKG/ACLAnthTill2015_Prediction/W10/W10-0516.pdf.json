{"title": [{"text": "Detecting controversies in Twitter: a first study", "labels": [], "entities": [{"text": "Detecting controversies", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9042205512523651}]}], "abstractContent": [{"text": "Social media gives researchers a great opportunity to understand how the public feels and thinks about a variety of topics, from political issues to entertainment choices.", "labels": [], "entities": []}, {"text": "While previous research has explored the likes and dislikes of audiences, we focus on a related but different task of detecting controversies involving popular entities, and understanding their causes.", "labels": [], "entities": []}, {"text": "Intuitively, if people hotly debate an entity in a given period of time, there is a good chance of a controversy occurring.", "labels": [], "entities": []}, {"text": "Consequently, we use Twit-ter data, boosted with knowledge extracted from the Web, as a starting approach: This paper introduces our task, an initial method and encouraging early results.", "labels": [], "entities": []}, {"text": "We focus on detecting controversies involving known entities in Twit-ter data.", "labels": [], "entities": [{"text": "Twit-ter data", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.7801913917064667}]}, {"text": "Leta snapshot denote a triple s = (e, \u2206t, tweets), where e is an entity, \u2206t is a time period and tweets is the set of tweets from the target time period which refer to the target entity.", "labels": [], "entities": []}, {"text": "1. Let cont(s) denote the level of controversy associated with entity e in the context of the snapshot s.", "labels": [], "entities": []}, {"text": "Our task is as follows: Task.", "labels": [], "entities": []}, {"text": "Given an entity set E and a snapshot set S = {(e, \u2206t, tweets)|e \u2208 E}, compute the controversy level cont(s) for each snapshot sin Sand rank S with respect to the resulting scores.", "labels": [], "entities": []}, {"text": "Figure 1 gives an overview of our solution.", "labels": [], "entities": []}, {"text": "We first select the set B \u2282 S, consisting of candidate snapshots that are likely to be controversial (buzzy snapshots).", "labels": [], "entities": []}, {"text": "Then, for each snapshot in B, we compute the controversy score cont, by combining a timely controversy score (tcont) and a historical controversy score (hcont).", "labels": [], "entities": [{"text": "timely controversy score (tcont)", "start_pos": 84, "end_pos": 116, "type": "METRIC", "confidence": 0.8291013936201731}]}, {"text": "Our method uses a sentiment lexicon SL (7590 terms) and a controversy lexicon CL 1 We use 1-day as the time period \u2206t.", "labels": [], "entities": []}, {"text": "E.g. s=('Brad Pitt',12/11/2009,tweets) Algorithm 0.1: CONTROVERSYDETECTION(S, T witter) select buzzy snapshots B \u2282 S for s \u2208 B tcont(s) = \u03b1 * M ixSent(s) + (1 \u2212 \u03b1) * Controv(s)) cont(s) = \u03b2 * tcont(s) + (1 \u2212 \u03b2) * hcont(s) rank B on scores return (B) Figure 1: Controversy Detection: Overview (750 terms).", "labels": [], "entities": [{"text": "CONTROVERSYDETECTION", "start_pos": 54, "end_pos": 74, "type": "METRIC", "confidence": 0.888555109500885}, {"text": "Controversy Detection", "start_pos": 260, "end_pos": 281, "type": "TASK", "confidence": 0.7187967747449875}]}, {"text": "The sentiment lexicon is composed by augmenting the set of positive and negative polarity terms in OpinionFinder 1.5 2 (e.g. 'love','wrong') with terms bootstrapped from a large set of user reviews.", "labels": [], "entities": []}, {"text": "The controversy lexicon is compiled by mining controversial terms (e.g. 'trial', 'apology') from Wikipedia pages of people included in the Wikipedia controversial topic list.", "labels": [], "entities": []}, {"text": "We make the simple assumption that if in a given time period, an entity is discussed more than in the recent past, then a controversy involving the entity is likely to occurr in that period.", "labels": [], "entities": []}, {"text": "We model the intuition with the score: b(s) = |tweetss| (i\u2208prev(s,N) |tweetsi|)/N where tweets sis the set of tweets in the snapshot s; and prev(s, N) is the set of snapshots referring to the same entity of s, in N time periods previous to s.", "labels": [], "entities": []}, {"text": "In our experiment, we use N = 2, i.e. we focus on two days before s.", "labels": [], "entities": []}, {"text": "We retain as buzzy snapshots only those with b(s) > 3.0.", "labels": [], "entities": []}, {"text": "The hcont score estimates the overall controversy level of an entity in Web data, independently of time.", "labels": [], "entities": []}, {"text": "We consider hcont our baseline system, to which we compare the Twitter-based models.", "labels": [], "entities": []}, {"text": "The score is estimated on Web document data using the CL lexicon as fol-2 J.", "labels": [], "entities": []}, {"text": "Wiebe, T. Wilson, and C. Cardie.", "labels": [], "entities": []}, {"text": "2005. Annotating expressions of opinions and emotions in language.", "labels": [], "entities": [{"text": "Annotating expressions of opinions and emotions in language", "start_pos": 6, "end_pos": 65, "type": "TASK", "confidence": 0.8914999887347221}]}, {"text": "In Language Resources and Evaluation.", "labels": [], "entities": [{"text": "Language Resources and Evaluation", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.5829580426216125}]}], "introductionContent": [], "datasetContent": [{"text": "We evaluate our model on the task of ranking snapshots according to their controversy level.", "labels": [], "entities": []}, {"text": "Our corpus is a large set of Twitter data from.", "labels": [], "entities": []}, {"text": "The set of entities E is composed of 104,713 celebrity names scraped from Wikipedia for the Actor, Athlete, Politician and Musician categories.", "labels": [], "entities": []}, {"text": "The overall size of S amounts to 661,226 (we consider only snapshots with a minimum of 10 tweets).", "labels": [], "entities": []}, {"text": "The number of buzzy snapshots in B is 30,451.", "labels": [], "entities": [{"text": "B", "start_pos": 33, "end_pos": 34, "type": "METRIC", "confidence": 0.8486188054084778}]}, {"text": "For evaluation, we use a gold standard of 120 snapshots randomly sampled from B, and manually annotated as controversial or not-controversial by two expert annotators (detailed guidelines will be presented at the workshop).", "labels": [], "entities": []}, {"text": "Kappa-agreement between the annotators, estimated on a subset of 20 snapshots, is 0.89 ('almost perfect' agreement).", "labels": [], "entities": [{"text": "almost perfect' agreement)", "start_pos": 89, "end_pos": 115, "type": "METRIC", "confidence": 0.5635505437850952}]}, {"text": "We experiment with different \u03b1 and \u03b2 values, as reported in, in order to discern the value of final score components.", "labels": [], "entities": []}, {"text": "We use Average Precision show that all Twitter-based models perform better than the Web-based baseline.", "labels": [], "entities": [{"text": "Average Precision", "start_pos": 7, "end_pos": 24, "type": "METRIC", "confidence": 0.8743880391120911}]}, {"text": "The most effective basic model is M ixSent, suggesting that the presence of mixed polarity sentiment terms in a snapshot is a good indicator of controversy.", "labels": [], "entities": []}, {"text": "For example, 'Claudia Jordan' appears in a snapshot with a mix of positive and negative terms -in a debate about a red carpet appearance-but the hcont and Controv scores are low as there is no record of historical controversy or explicit controversy terms in the target tweets.", "labels": [], "entities": [{"text": "Controv", "start_pos": 155, "end_pos": 162, "type": "METRIC", "confidence": 0.9888070821762085}]}, {"text": "Best overall performance is achieved by a mixed model combining the hcont and the M ixSent score (last row in).", "labels": [], "entities": [{"text": "M ixSent score", "start_pos": 82, "end_pos": 96, "type": "METRIC", "confidence": 0.9090621670087179}]}, {"text": "There are indeed cases in which the evidence from M ixSent is not enough -e.g., a snapshot discussing 'Jesse Jackson' 's appearance on a tv show lacks common positive or negative terms, but reflects users' confusion nevertheless; however, 'Jesse Jackson' has a high historical controversy score, which leads our combined model to correctly assign a high controversy score to the snapshot.", "labels": [], "entities": [{"text": "M ixSent", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.8061073422431946}]}, {"text": "Interestingly, most controversies in the gold standard refer to micro-events (e.g., tv show, award show or athletic event appearances), rather than more traditional controversial events found in news streams (e.g., speeches about climate change, controversial movie releases, etc.); this further strengthens the case that Twitter is a complementary information source wrt news corpora.", "labels": [], "entities": []}, {"text": "We plan to followup on this very preliminary investigation by improving our Twitter-based sentiment detection, incorporating blog and news data and generalizing our controversy model (e.g., discovering the 'what' and the 'why' of a controversy, and tracking common controversial behaviors of entities over time).", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7302417755126953}, {"text": "discovering the 'what' and the 'why' of a controversy", "start_pos": 190, "end_pos": 243, "type": "TASK", "confidence": 0.665604673899137}]}], "tableCaptions": [{"text": " Table 1: Controversial Snapshot Detection: results over  different model parametrizations", "labels": [], "entities": [{"text": "Controversial Snapshot Detection", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7085766990979513}]}]}