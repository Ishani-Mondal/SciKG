{"title": [{"text": "Uncertainty reduction as a measure of cognitive processing effort", "labels": [], "entities": [{"text": "Uncertainty reduction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7524160444736481}]}], "abstractContent": [{"text": "The amount of cognitive effort required to process a word has been argued to depend on the word's effect on the uncertainty about the incoming sentence, as quantified by the entropy over sentence probabilities.", "labels": [], "entities": []}, {"text": "The current paper tests this hypothesis more thoroughly than has been done before by using recurrent neural networks for entropy-reduction estimation.", "labels": [], "entities": [{"text": "entropy-reduction estimation", "start_pos": 121, "end_pos": 149, "type": "TASK", "confidence": 0.756647139787674}]}, {"text": "A comparison between these estimates and word-reading times shows that entropy reduction is positively related to processing effort , confirming the entropy-reduction hypothesis.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.8107729256153107}]}, {"text": "This effect is independent from the effect of surprisal.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the field of computational psycholinguistics, a currently popular approach is to account for reading times on a sentence's words by estimates of the amount of information conveyed by these words.", "labels": [], "entities": []}, {"text": "Processing a word that conveys more information is assumed to involve more cognitive effort, which is reflected in the time required to read the word.", "labels": [], "entities": []}, {"text": "In this context, the most common formalization of a word's information content is its surprisal.", "labels": [], "entities": []}, {"text": "If word string wt 1 (short for w 1 , w 2 , . .", "labels": [], "entities": []}, {"text": "wt ) is the sentence so far and P (w t+1 |w t 1 ) the occurrence probability of the next word w t+1 , then that word's surprisal is defined as \u2212 log P (w t+1 |w t 1 ).", "labels": [], "entities": []}, {"text": "It is well established by now that word-reading times indeed correlate positively with surprisal values as estimated by any sufficiently accurate generative language model (.", "labels": [], "entities": []}, {"text": "A lesser known alternative operationalization of a word's information content is based on the uncertainty about the rest of the sentence, quantified by) as the entropy of the probability distribution over possible sentence structures.", "labels": [], "entities": []}, {"text": "The reduction in entropy that results from processing a word is taken to be the amount of information conveyed by that word, and was argued by Hale to be predictive of word-reading time.", "labels": [], "entities": []}, {"text": "However, this entropy-reduction hypothesis has not yet been comprehensively tested, possibly because of the difficulty of computing the required entropies.", "labels": [], "entities": []}, {"text": "Although shows how sentence entropy can be computed given a PCFG, this computation is not feasible when the grammar is of realistic size.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.9133016467094421}]}, {"text": "Here, we empirically investigate the entropyreduction hypothesis more thoroughly than has been done before, by using recurrent neural networks as language models.", "labels": [], "entities": []}, {"text": "Since these networks do not derive any structure, they provide estimates of sentence entropy rather than sentence-structure entropy.", "labels": [], "entities": []}, {"text": "In practice, these two entropies will generally be similar: If the rest of the sentence is highly uncertain, so is its structure.", "labels": [], "entities": []}, {"text": "Sentence entropy can therefore be viewed as a simplification of structure entropy; one that is less theory dependent since it does not rely on any particular grammar.", "labels": [], "entities": [{"text": "Sentence entropy", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9068951606750488}]}, {"text": "The distinction between entropy over sentences and entropy over structures will simply be ignored in the remainder of this paper.", "labels": [], "entities": []}, {"text": "Results show that, indeed, a significant fraction of variance in reading-time data is accounted for by entropy reduction, over and above surprisal.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}