{"title": [{"text": "Chinese Word Segmentation with Conditional Support Vector In- spired Markov Models", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6503327886263529}]}], "abstractContent": [{"text": "In this paper, we present the proposed method of participating SIGHAN-2010 Chi-nese word segmentation bake-off.", "labels": [], "entities": [{"text": "SIGHAN-2010 Chi-nese word segmentation bake-off", "start_pos": 63, "end_pos": 110, "type": "TASK", "confidence": 0.6739284276962281}]}, {"text": "In this year, our focus aims to quick train and test the given data.", "labels": [], "entities": []}, {"text": "Unlike the most structural learning algorithms, such as conditional random fields, we design an in-house development conditional support vector Markov model (CMM) framework.", "labels": [], "entities": []}, {"text": "The method is very quick to train and also show better performance inaccuracy than CRF.", "labels": [], "entities": []}, {"text": "To give a fair comparison, we compare our method to CRF with three additional tasks, namely, CoNLL-2000 chunking, SIGHAN-3 Chi-nese word segmentation.", "labels": [], "entities": [{"text": "CoNLL-2000 chunking", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.627302885055542}, {"text": "SIGHAN-3 Chi-nese word segmentation", "start_pos": 114, "end_pos": 149, "type": "TASK", "confidence": 0.684337928891182}]}, {"text": "The results were encourage and indicated that the proposed CMM produces better not only accuracy but also training time efficiency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9990172386169434}]}, {"text": "The official results in SIGHAN-2010 also demonstrates that our method perform very well in traditional Chinese with fine-tuned features set.", "labels": [], "entities": [{"text": "SIGHAN-2010", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.6979441046714783}]}], "introductionContent": [{"text": "Since 2006 Chinese word segmentation bakeoff in), this is the third time to join the competition (.", "labels": [], "entities": []}, {"text": "In this year, we join the SIGHAN bakeoff task in both traditional and simplified Chinese closed word segmentation.", "labels": [], "entities": [{"text": "SIGHAN bakeoff task", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7806360522905985}, {"text": "Chinese closed word segmentation", "start_pos": 81, "end_pos": 113, "type": "TASK", "confidence": 0.6834922134876251}]}, {"text": "Unlike most western languages, there is no explicit space between words.", "labels": [], "entities": []}, {"text": "The goal of word segmentation is to identify words given the sentence.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7289398610591888}]}, {"text": "This technique provides important features for downstream purposes.", "labels": [], "entities": []}, {"text": "Examples include Chinese part-of-speech (POS) tagging (, Chinese word dependency parsing (.", "labels": [], "entities": [{"text": "Chinese part-of-speech (POS) tagging", "start_pos": 17, "end_pos": 53, "type": "TASK", "confidence": 0.5610787471135458}, {"text": "Chinese word dependency parsing", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.6544769629836082}]}, {"text": "With the rapid growth of structural learning algorithms, such as conditional random fields (CRFs) () and maximummargin Markov models (M 3 N) () have received a great attention and become a prominent learning algorithm to many sequential labeling tasks.", "labels": [], "entities": [{"text": "sequential labeling tasks", "start_pos": 226, "end_pos": 251, "type": "TASK", "confidence": 0.7753728032112122}]}, {"text": "Examples include partof-speech (POS) tagging and syntactic phrase chunking (.", "labels": [], "entities": [{"text": "partof-speech (POS) tagging", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.7016607761383057}, {"text": "syntactic phrase chunking", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.6162189940611521}]}, {"text": "The Chinese word segmentation can also be treated as a character-based tagging task in).", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6238069534301758}]}, {"text": "One feature of sequential labeling is that it aims at finding non-recursive chunk fragments in a given sentence.", "labels": [], "entities": [{"text": "sequential labeling", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8115460872650146}]}, {"text": "Among these approaches, CRF has been wildly used in recent SIGHAN bakeoff tasks.", "labels": [], "entities": [{"text": "CRF", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8317511677742004}, {"text": "SIGHAN bakeoff tasks", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.869173268477122}]}, {"text": "Although these approaches do not suffer from so-called label-bias problems (), one limitation is that they are inefficient to train with large-scale, especially large category data.", "labels": [], "entities": []}, {"text": "On the other hand, non-structural learning approaches (e.g. maximum entropy models) which learn local predictors usually cost much better training time performance than structural learning algorithms.", "labels": [], "entities": []}, {"text": "These methods condition on local context features and incorporate fix-length history information.", "labels": [], "entities": []}, {"text": "Although higher order feature (longer history) maybe useful to some tasks, the exponential scaled inference time is also intractable in practice.", "labels": [], "entities": [{"text": "exponential scaled inference time", "start_pos": 79, "end_pos": 112, "type": "METRIC", "confidence": 0.7779185473918915}]}, {"text": "Support vector machines (SVMs) which is one of the state-of-the-art supervised learning algorithms have been widely employed as local classifiers to many sequential labeling tasks.", "labels": [], "entities": [{"text": "sequential labeling tasks", "start_pos": 154, "end_pos": 179, "type": "TASK", "confidence": 0.716546873251597}]}, {"text": "Specially, the training time of linear kernel SVM with either L 1 -norm or L 2 -norm ( can now be obtained in linear time.", "labels": [], "entities": []}, {"text": "Even local classifier-based approaches have the drawbacks of label-bias problems, training nonstructural linear SVM is scalable to large-scale data.", "labels": [], "entities": []}, {"text": "By means of so-called one-versus-all multiclass SVM training, it is also scalable to large-category data.", "labels": [], "entities": []}, {"text": "In this paper, we present our Chinese word segmentation based on the proposed conditional support vector Markov models for sequential labeling tasks, especially Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.5926545659701029}, {"text": "Chinese word segmentation", "start_pos": 161, "end_pos": 186, "type": "TASK", "confidence": 0.610778828461965}]}, {"text": "Unlike structural learning algorithms, our method can be simply trained without considering the entire structures and hence the training time scales linearly with the number of training examples.", "labels": [], "entities": []}, {"text": "In this framework, to alleviate the ease of label-bias problems, the state transition probability is ignored.", "labels": [], "entities": []}, {"text": "Instead, we merely utilize the property of label relationships between chunks ( . To demonstrate our method, we compare to several well-known structural learning algorithms, like CRF (), and SVM-HMM () on two well-known data, namely, CoNLL-2000 syntactic chunking, SIGHAN-3 Chinese word segmentation tasks.", "labels": [], "entities": [{"text": "SIGHAN-3 Chinese word segmentation tasks", "start_pos": 265, "end_pos": 305, "type": "TASK", "confidence": 0.7022790431976318}]}, {"text": "By following this, we apply the model to the Chinese word segmentation tasks of SIGHAN-2010 this year.", "labels": [], "entities": [{"text": "Chinese word segmentation tasks", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6322476118803024}, {"text": "SIGHAN-2010", "start_pos": 80, "end_pos": 91, "type": "DATASET", "confidence": 0.6776698231697083}]}, {"text": "The empirical results showed that our method is not only fast but also achieving more superior accuracy than structural learning methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9973416924476624}]}, {"text": "In traditional Chinese, our method also achieves the state-of-the-art performance inaccuracy with fined-tune features. and the optimal tag sequence can be efficiently searched by using conventional Viterbi algorithm.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Syntactic chunking results of the proposed  CMM and the selected structural learning methods.", "labels": [], "entities": [{"text": "Syntactic chunking", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8101135194301605}]}, {"text": " Table 2: SIGHAN-3 word segmentation results", "labels": [], "entities": [{"text": "SIGHAN-3 word segmentation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.8716594576835632}]}, {"text": " Table 4: Empirical results of the development set of  single CRF and our CMM", "labels": [], "entities": [{"text": "Empirical", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9559990763664246}]}]}