{"title": [{"text": "UDel: Refining a Method of Named Entity Generation", "labels": [], "entities": [{"text": "UDel", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7987399101257324}, {"text": "Named Entity Generation", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.8563987016677856}]}], "abstractContent": [{"text": "This report describes the methods and results of a system developed for the GREC Named Entity Challenge 2010.", "labels": [], "entities": [{"text": "GREC Named Entity Challenge 2010", "start_pos": 76, "end_pos": 108, "type": "DATASET", "confidence": 0.8355269432067871}]}, {"text": "We detail the refinements made to our 2009 submission and present the output of the self-evaluation on the development data set.", "labels": [], "entities": [{"text": "development data set", "start_pos": 107, "end_pos": 127, "type": "DATASET", "confidence": 0.8542557954788208}]}], "introductionContent": [{"text": "The GREC Named Entity Challenge 2010 (NEG) is an NLG shared task whereby submitted systems must select a referring expression from a list of options for each mention of each person in a text.", "labels": [], "entities": [{"text": "GREC Named Entity Challenge 2010 (NEG)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.5524705797433853}]}, {"text": "The corpus is a collection of 2,000 introductory sections from Wikipedia articles about individual people in which all mentions of person entities have been annotated.", "labels": [], "entities": []}, {"text": "An in-depth description of the task, along with the evaluation results from the previous year, is provided by.) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR)).", "labels": [], "entities": [{"text": "GREC Main Subject Reference Generation Challenge (MSR))", "start_pos": 164, "end_pos": 219, "type": "TASK", "confidence": 0.65808990266588}]}, {"text": "Although our system performed reasonably-well in predicting REG08-Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task.", "labels": [], "entities": [{"text": "predicting REG08-Type", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.6090076565742493}, {"text": "NEG task", "start_pos": 78, "end_pos": 86, "type": "TASK", "confidence": 0.6621487736701965}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.8542584776878357}, {"text": "MSR task", "start_pos": 228, "end_pos": 236, "type": "TASK", "confidence": 0.8682959377765656}]}, {"text": "As suggested by the evaluators (), this was due in large part to our reliance on the list of REs being in a particular order, which had changed for the NEG task.", "labels": [], "entities": [{"text": "NEG task", "start_pos": 152, "end_pos": 160, "type": "TASK", "confidence": 0.8340466320514679}]}], "datasetContent": [], "tableCaptions": []}