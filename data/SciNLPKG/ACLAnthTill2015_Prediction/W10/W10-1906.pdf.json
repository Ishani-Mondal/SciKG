{"title": [{"text": "Arguments of Nominals in Semantic Interpretation of Biomedical Text", "labels": [], "entities": [{"text": "Semantic Interpretation of Biomedical Text", "start_pos": 25, "end_pos": 67, "type": "TASK", "confidence": 0.7149935007095337}]}], "abstractContent": [{"text": "Based on linguistic generalizations, we enhanced an existing semantic processor, SemRep, for effective interpretation of a wide range of patterns used to express arguments of nominalization in clinically oriented biomedical text.", "labels": [], "entities": []}, {"text": "Nominaliza-tions are pervasive in the scientific literature , yet few text mining systems adequately address them, thus missing a wealth of information.", "labels": [], "entities": []}, {"text": "We evaluated the system by assessing the algorithm independently and by determining its contribution to SemRep generally.", "labels": [], "entities": []}, {"text": "The first evaluation demonstrated the strength of the method through an F-score of 0.646 (P=0.743, R=0.569), which is more than 20 points higher than the baseline.", "labels": [], "entities": [{"text": "F-score", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9995585083961487}, {"text": "R", "start_pos": 99, "end_pos": 100, "type": "METRIC", "confidence": 0.938391387462616}]}, {"text": "The second evaluation showed that overall SemRep results were increased to F-score 0.689 (P=0.745, R=0.640), approximately 25 points better than processing without nominalizations.", "labels": [], "entities": [{"text": "F-score", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9992356300354004}, {"text": "P", "start_pos": 90, "end_pos": 91, "type": "METRIC", "confidence": 0.8917437195777893}, {"text": "R", "start_pos": 99, "end_pos": 100, "type": "METRIC", "confidence": 0.846460223197937}]}], "introductionContent": [{"text": "Extracting semantic relations from text and representing them as predicate-argument structures is increasingly seen as foundational for mining the biomedical literature (.", "labels": [], "entities": [{"text": "Extracting semantic relations from text", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8617228269577026}]}, {"text": "Most research has focused on relations indicated by verbs ().", "labels": [], "entities": []}, {"text": "However nominalizations, gerunds, and relational nouns also take arguments.", "labels": [], "entities": []}, {"text": "For example, the following sentence has three nominalizations, treatment, suppression, and lactation (nominalized forms of the verbs treat, suppress, and lactate, respectively).", "labels": [], "entities": []}, {"text": "Agonist is derived from agonize, but indicates an agent rather than an event.", "labels": [], "entities": [{"text": "Agonist", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8694301247596741}]}, {"text": "Bromocriptine, an ergot alkaloid dopamine agonist, is a recent common treatment for suppression of lactation in postpartum women.", "labels": [], "entities": [{"text": "Bromocriptine", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9854789972305298}, {"text": "suppression of lactation", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.8311633467674255}]}, {"text": "In promoting economy of expression, nominalizations are pervasive in scientific discourse, particularly the molecular biology sublanguage, due to the highly nested and complex biomolecular interactions described).", "labels": [], "entities": []}, {"text": "However, point out that nominalizations are more difficult to process than verbs.", "labels": [], "entities": []}, {"text": "Although a few systems deal with them, the focus is often limited in both the nominalizations recognized and the patterns used to express their arguments.", "labels": [], "entities": []}, {"text": "Inability to interpret nominal constructions in a general way limits the effectiveness of such systems, since a wealth of knowledge is missed.", "labels": [], "entities": []}, {"text": "In this paper, we discuss our recent work on interpreting nominal forms and their arguments.", "labels": [], "entities": [{"text": "interpreting nominal forms", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.9223616719245911}]}, {"text": "We concentrate on nominalizations; however, the analysis also applies to other argument-taking nouns.", "labels": [], "entities": []}, {"text": "Based on training data, we developed a set of linguistic generalizations and enhanced an existing semantic processor, SemRep, for effective interpretation of a wide range of patterns used to express arguments of nominalization in clinically oriented biomedical text.", "labels": [], "entities": []}, {"text": "We evaluated the enhancements in two ways: by examining the ability to identify arguments of nominals independently and the effect these enhancements had on the overall quality of SemRep output.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three-hundred sentences from 239 MEDLINE citations (titles and abstracts) were selected for annotating a test set.", "labels": [], "entities": []}, {"text": "Some had previously been selected for various aspects of SemRep evaluation; others were chosen randomly.", "labels": [], "entities": [{"text": "SemRep evaluation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.9434138536453247}]}, {"text": "A small number (30) were sentences in the GENIA event corpus () with bio-eventtriggering nominalizations.", "labels": [], "entities": [{"text": "GENIA event corpus", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.9103508591651917}]}, {"text": "Annotation was conducted by three of the authors.", "labels": [], "entities": []}, {"text": "One, a linguist (A), judged all sentences, while the other two, a computer scientist (B) and a medical informatics researcher (C), annotated a subset.", "labels": [], "entities": []}, {"text": "Annotation was not limited to nominalizations.", "labels": [], "entities": []}, {"text": "The statistics regarding the individual annotations are given below.", "labels": [], "entities": []}, {"text": "The numbers in parentheses show the number of annotated predications indicated by nominalizations.", "labels": [], "entities": []}, {"text": "As guidance, annotators were provided UMLS Metathesaurus concepts for the sentences.", "labels": [], "entities": [{"text": "UMLS Metathesaurus", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.8008476793766022}]}, {"text": "However, they consulted the Metathesaurus directly to check questionable mappings.", "labels": [], "entities": [{"text": "Metathesaurus", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9535558223724365}]}, {"text": "Annotation focused on the 25 predicate types SemRep addresses.", "labels": [], "entities": []}, {"text": "We measured inter-annotator agreement, defined as the F-score of one set of annotations, when the second is taken as the gold standard.", "labels": [], "entities": [{"text": "agreement", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.6656398773193359}, {"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9959821701049805}]}, {"text": "After individual annotations were complete, two annotators (A and C) assessed all three sets of annotations and created the final reference standard.", "labels": [], "entities": []}, {"text": "The reference standard has 569 predications, 300 of which (52.7%) are indicated by nominalizations.", "labels": [], "entities": []}, {"text": "We further measured the agreement between individual sets of annotations and the reference standard.", "labels": [], "entities": []}, {"text": "Results are given below:  We performed two evaluations.", "labels": [], "entities": []}, {"text": "The first (eval1) evaluated nominalizations in isolation, while the second (eval2) assessed the effect of the enhancements on overall semantic interpretation in SemRep.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 134, "end_pos": 157, "type": "TASK", "confidence": 0.7510294020175934}]}, {"text": "For eval1, we restricted SemRep to extract predications indicated by nominalizations only.", "labels": [], "entities": []}, {"text": "The baseline was a nominalization argument identification rule which simply stipulates that the subject of a predicate is a concept to the left (starting from the modifier of the nominalization, if any), and the object is a concept to the right.", "labels": [], "entities": [{"text": "nominalization argument identification", "start_pos": 19, "end_pos": 57, "type": "TASK", "confidence": 0.7877122362454733}]}, {"text": "This baseline implements the underspecification principle of SemRep, without any additional logic.", "labels": [], "entities": []}, {"text": "We compared the results from this baseline to those from the algorithm described above to identify arguments of nominalizations.", "labels": [], "entities": []}, {"text": "The gold standard for eval1 was limited to predications indicated by nominalizations.", "labels": [], "entities": []}, {"text": "We investigated the effect of nominalization processing on SemRep generally in eval2, for which the baseline implementation was SemRep with no nominalization processing.", "labels": [], "entities": [{"text": "nominalization processing", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.923048585653305}]}, {"text": "The results for this baseline were evaluated against those obtained using SemRep with no restrictions.", "labels": [], "entities": []}, {"text": "Typical evaluation metrics, precision, recall, and F-score, were calculated.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9997350573539734}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9993569254875183}, {"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9993485808372498}]}], "tableCaptions": [{"text": " Table 5. Results for molecular biology sentences", "labels": [], "entities": [{"text": "molecular biology", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.81023770570755}]}]}