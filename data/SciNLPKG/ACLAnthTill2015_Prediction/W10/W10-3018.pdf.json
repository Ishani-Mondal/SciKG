{"title": [{"text": "Exploiting CCG Structures with Tree Kernels for Speculation Detection", "labels": [], "entities": [{"text": "Speculation Detection", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.9103409945964813}]}], "abstractContent": [{"text": "Our CoNLL-2010 speculative sentence detector disambiguates putative keywords based on the following considerations: a speculative keyword maybe composed of one or more word tokens; a speculative sentence may have one or more speculative keywords; and if a sentence contains at least one real speculative keyword, it is deemed speculative.", "labels": [], "entities": [{"text": "CoNLL-2010 speculative sentence detector disambiguates putative keywords", "start_pos": 4, "end_pos": 76, "type": "TASK", "confidence": 0.6040410314287458}]}, {"text": "A tree kernel classi-fier is used to assess whether a potential speculative keyword conveys speculation.", "labels": [], "entities": []}, {"text": "We exploit information implicit in tree structures.", "labels": [], "entities": []}, {"text": "For prediction efficiency, only a segment of the whole tree around a speculation keyword is considered, along with morphological features inside the segment and information about the containing document.", "labels": [], "entities": [{"text": "prediction", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9742609858512878}]}, {"text": "A maximum entropy classifier is used for sentences not covered by the tree kernel classifier.", "labels": [], "entities": []}, {"text": "Experiments on the Wikipedia data set show that our system achieves 0.55 F-measure (in-domain).", "labels": [], "entities": [{"text": "Wikipedia data set", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.9880996346473694}, {"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9973287582397461}]}], "introductionContent": [{"text": "Speculation and its impact on argumentation has been studied by linguists and logicians since at least as far back as Aristotle, and under the category of linguistic \"hedges\" since.", "labels": [], "entities": []}, {"text": "Practical application of this research has emerged due to the efforts to create a biomedical database of sentences tagged with speculation information: BioScope () and because of the association of some kinds of Wikipedia data with the speculation phenomenon (.", "labels": [], "entities": []}, {"text": "It is clear that specific words can be considered as clues that can qualify a sentence as speculative.", "labels": [], "entities": []}, {"text": "However, the presence of a speculative keyword not always conveys a speculation assertion which makes the speculation detection a tough problem.", "labels": [], "entities": [{"text": "speculation detection", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.7426140308380127}]}, {"text": "For instance, the sentences below contain the speculative keyword \"may\", but only the sentence (a) is speculative.", "labels": [], "entities": []}, {"text": "(a) These effects maybe reversible.", "labels": [], "entities": []}, {"text": "(b) Members of an alliance may not attack each other.", "labels": [], "entities": []}, {"text": "The, \"Learning to detect hedges and their scope in natural language text\" proposed two tasks related to speculation research.", "labels": [], "entities": []}, {"text": "Task 1 aims to detect sentences containing uncertainty and Task 2 aims to resolve the intra-sentential scope of hedge cues.", "labels": [], "entities": []}, {"text": "We engaged in the first task in the biomedical and Wikipedia domains as proposed by the organizers, but eventually we got to submit only Wikipedia domain results.", "labels": [], "entities": []}, {"text": "However, in this paper we include results in the biomedical domain as well.", "labels": [], "entities": []}, {"text": "The BioScope corpus is a linguistically hand annotated corpus of negation and speculation phenomena for medical free texts, biomedical article abstracts and full biomedical articles.", "labels": [], "entities": [{"text": "BioScope corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8151734173297882}, {"text": "negation and speculation phenomena", "start_pos": 65, "end_pos": 99, "type": "TASK", "confidence": 0.82235948741436}]}, {"text": "The aforesaid phenomena have been annotated at sentence level with keyword tags and linguistic scope tags.", "labels": [], "entities": []}, {"text": "Some previous research on speculation detection and boundary determination over biomedical data has been done by from a computational view using machine learning methods.", "labels": [], "entities": [{"text": "speculation detection", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.9716304540634155}, {"text": "boundary determination", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7012781053781509}]}, {"text": "The Wikipedia speculation dataset was generated by exploiting a weasel word marking.", "labels": [], "entities": [{"text": "Wikipedia speculation dataset", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.9372511506080627}]}, {"text": "As weasel words convey vagueness and ambiguity by providing an unsupported opinion, they are discouraged by Wikipedia editors.", "labels": [], "entities": []}, {"text": "proposed a system to detect hedges based on frequency measures and shallow information, achieving a F-score of 0.69 . We formulate the speculation detection problem as a word disambiguation problem and developed a system as a pipelined set of natural language processing tools and procedures to preprocess the datasets.", "labels": [], "entities": [{"text": "F-score", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9994875192642212}, {"text": "speculation detection", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.7447465360164642}, {"text": "word disambiguation", "start_pos": 170, "end_pos": 189, "type": "TASK", "confidence": 0.7082815617322922}]}, {"text": "A Combinatory Categorial Grammar parsing (CCG)) tool and a Tree Kernel (TK) classifier constitute the core of the system.", "labels": [], "entities": [{"text": "Combinatory Categorial Grammar parsing (CCG))", "start_pos": 2, "end_pos": 47, "type": "TASK", "confidence": 0.7781253329345158}]}, {"text": "The Section 2 of this paper describes the overall architecture of our system.", "labels": [], "entities": []}, {"text": "Section 3 depicts the dataset pre-processing.", "labels": [], "entities": []}, {"text": "Section 4 shows how we built the speculation detection module, outlines the procedure of examples generation and the use of the Tree-kernel classifier.", "labels": [], "entities": [{"text": "speculation detection", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.8524370193481445}, {"text": "examples generation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.763525515794754}]}, {"text": "Section 5 presents the experiments and results, we show that sentence CCG derivation information helps to differentiate between apparent and real speculative words for speculation detection.", "labels": [], "entities": [{"text": "sentence CCG derivation", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.659244030714035}, {"text": "speculation detection", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.8429877161979675}]}, {"text": "Finally Section 6 gives our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The pre-processing module extracts keywords, sentences and document information.", "labels": [], "entities": []}, {"text": "All sentences are processed by the tokenizer/lemmatizer and at the same time specific information about the keywords is extracted.", "labels": [], "entities": []}, {"text": "In the CoNLL-2010 Task 1, biomedical and Wikipedia datasets were provided for development, training and evaluation in the BioScope XML format.", "labels": [], "entities": [{"text": "Wikipedia datasets", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.7720009684562683}]}, {"text": "Development and training datasets are tagged with cue labels and a certainty feature.", "labels": [], "entities": [{"text": "certainty", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9818883538246155}]}, {"text": "The number of sentences for each dataset 5 is detailed in.", "labels": [], "entities": []}, {"text": "After manual revision of sentences not parsed by C&C parser, we found that they contain equations, numbering elements (e.g. (i), (ii)..", "labels": [], "entities": []}, {"text": "1), 2) ), or long n-grams of named-entities, for instance: ...mannose-capped lipoarabinomannan ( ManLAM ) of Mycobacterium tuberculosis ( M. tuberculosis )... that out of a biomedical domain appear to be ungrammatical.", "labels": [], "entities": []}, {"text": "Similarly, in the Wikipedia datasets, some sentences have many named entities.", "labels": [], "entities": [{"text": "Wikipedia datasets", "start_pos": 18, "end_pos": 36, "type": "DATASET", "confidence": 0.9358295798301697}]}, {"text": "This suggests the need of a specific pre-processor or a parser for this kind of sentences like a named entity tagger.", "labels": [], "entities": []}, {"text": "In, we present the number of parsed sentences, processed sentences by the TK model and examples obtained in the tree structure extraction.: Count of processed sentences.", "labels": [], "entities": []}, {"text": "The CoNLL-2010 organizers proposed in-domain and cross-domain evaluations.", "labels": [], "entities": []}, {"text": "In cross-domain experiments, test datasets of one domain can be used with classifiers trained on the other or on the union of both domains.", "labels": [], "entities": []}, {"text": "We report here our results for the Wikipedia and biomedical datasets.", "labels": [], "entities": [{"text": "Wikipedia and biomedical datasets", "start_pos": 35, "end_pos": 68, "type": "DATASET", "confidence": 0.7306042015552521}]}, {"text": "So far, we mentioned two settings for our classifier: a TK classifier complemented by a baseline classifier (BL) and TK classifier complemented by a bag of features classifier (TK+BF).", "labels": [], "entities": []}, {"text": "shows the scores of our submitted system (indomain Task 1) on the Wikipedia dataset, whereas gives the scores of the baseline system.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.9782631397247314}]}, {"text": "Additionally, we consider a bag of features classifier (BF) and a classifier that combines the baseline applied to the sentences that have at least one keyword plus the BF classifier for the remaining sentences (BL+BF).", "labels": [], "entities": [{"text": "BL+BF)", "start_pos": 212, "end_pos": 218, "type": "METRIC", "confidence": 0.6959726214408875}]}, {"text": "In, results for the four classifiers (TK, TK+BF, BF, BL+BF) with evaluations in-domain and cross-domain are presented . The baseline scores confirm that relying on just the keywords is not enough to identify speculative sentences.", "labels": [], "entities": [{"text": "BF", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9560868144035339}, {"text": "BL+BF", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.7895089189211527}]}, {"text": "In the biomedical domain, the classifiers give high recall but too low precision resulting in low F-scores.", "labels": [], "entities": [{"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9991726279258728}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9984763264656067}, {"text": "F-scores", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9947558641433716}]}, {"text": "Still, the TK, TK+BF and BF (in-domain configurations) gives much better results than BL and BL+BF which indicates that the information from CCG improves the performance It is worth to note that the keyword lexicons have been not used in cross-domain way, so the TK and TK+BF models have not been tested in regards to keywords.       of the classifiers when compared to the baseline classifier.", "labels": [], "entities": [{"text": "BF", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9769733548164368}]}, {"text": "Even though in the Wikipedia domain the TK+BF score is less than the baseline score, still the performance of the classifiers do not fall much in any of the in-domain and cross-domain experiments.", "labels": [], "entities": [{"text": "TK+BF score", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.9229603409767151}]}, {"text": "On the other hand, BF does not have a good performance in 5 of 6 the experiments.", "labels": [], "entities": [{"text": "BF", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9705548882484436}]}, {"text": "To make a more precise comparison between TK and BF, the TK and BL+BF scores show that BL+BF performs better than TK in only 2 of the 6 experiments but the better performances achieved by BL+BF are very small.", "labels": [], "entities": [{"text": "BL+BF", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9028611381848654}]}, {"text": "This suggests that the complex processing made by tree kernels is more useful when disambiguating speculative keywords than BF.", "labels": [], "entities": []}, {"text": "Nonetheless, the bag-of-features approach is also of importance for the task at hand when combined with TK.", "labels": [], "entities": []}, {"text": "We observe that the TK classifer and BF classifier perform well making us believe that the CCG derivations provide relevant information for speculation detection.", "labels": [], "entities": [{"text": "BF", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.9811633229255676}, {"text": "speculation detection", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.9344896972179413}]}, {"text": "The use of tree kernels needs further investigations in order to evaluate the suitability of this approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Count of processed sentences.", "labels": [], "entities": [{"text": "Count", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9803255200386047}]}, {"text": " Table 3: Comparative scores for our system with  CoNLL official maximum and minimum scores in  Task 1, Wikipedia dataset in-domain.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.7811384797096252}, {"text": "Wikipedia dataset", "start_pos": 104, "end_pos": 121, "type": "DATASET", "confidence": 0.9676844775676727}]}, {"text": " Table 5: Results for Wikipedia dataset in-domain.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.9492040574550629}]}, {"text": " Table 6: Wikipedia data classified with biomedical  model scores (cross-domain).", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.9307678937911987}]}, {"text": " Table 7: Wikipedia data classified with biomedical  + Wikipedia model scores (cross-domain).", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.8919525742530823}]}, {"text": " Table 8: Biomedical data scores (in-domain).", "labels": [], "entities": []}, {"text": " Table 9:  Biomedical data classified with  Wikipedia model scores (cross-domain).", "labels": [], "entities": []}, {"text": " Table 10: Biomedical data classified with biomed- ical + Wikipedia model scores (cross-domain).", "labels": [], "entities": []}]}