{"title": [{"text": "A Statistical NLP Approach for Feature and Sentiment Identification from Chinese Reviews", "labels": [], "entities": [{"text": "Feature and Sentiment Identification from Chinese", "start_pos": 31, "end_pos": 80, "type": "TASK", "confidence": 0.7108858128388723}]}], "abstractContent": [{"text": "Existing methods for extracting features from Chinese reviews only use simplistic syntactic knowledge, while those for identifying sentiments rely heavily on a semantic dictionary.", "labels": [], "entities": []}, {"text": "In this paper, we present a systematic technique for identifying features and sentiments, using both syntactic and statistical analysis.", "labels": [], "entities": []}, {"text": "We firstly identify candidate features using a proposed set of common syntactic rules.", "labels": [], "entities": []}, {"text": "We then prune irrelevant candidates with topical relevance scores below a cutoff point.", "labels": [], "entities": []}, {"text": "We also propose an association analysis method based on likelihood ratio test to infer the polarity of opinion word.", "labels": [], "entities": [{"text": "association analysis", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.8444437086582184}, {"text": "likelihood ratio test", "start_pos": 56, "end_pos": 77, "type": "METRIC", "confidence": 0.9373945792516073}]}, {"text": "The sentiment of a feature is finally adjusted by analyzing the negative modifiers in the local context of the opinion word.", "labels": [], "entities": []}, {"text": "Experimental results show that our system performs significantly better than a well-known opinion mining system.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.7608966827392578}]}], "introductionContent": [{"text": "There were 420 million Internet users in China by the end of June 2010.", "labels": [], "entities": []}, {"text": "As a result, online social media in China has accumulated massive amount of valuable peer reviews on almost anything.", "labels": [], "entities": []}, {"text": "Mining this pool of Chinese reviews to detect features (e.g. \"\u624b\u673a\" mobile phone) and identify the corresponding sentiment (e.g. positive, negative) has recently become a hot research area.", "labels": [], "entities": []}, {"text": "However, the vast majority of previous work on feature detection only uses simplistic syntactic natural language processing (NLP) approaches, while those on sentiment identification depend heavily on a semantic dictionary.", "labels": [], "entities": [{"text": "feature detection", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.8773533999919891}, {"text": "sentiment identification", "start_pos": 157, "end_pos": 181, "type": "TASK", "confidence": 0.9665553271770477}]}, {"text": "Syntactic approaches are often prone to errors due to the informal nature of online reviews.", "labels": [], "entities": []}, {"text": "Dictionary-based approaches are more robust than syntactic approaches, but must be constantly updated with new terms and expressions, which are constantly evolving in online reviews.", "labels": [], "entities": []}, {"text": "To overcome these limitations, we propose a statistical NLP approach for Chinese feature and sentiment identification.", "labels": [], "entities": [{"text": "Chinese feature and sentiment identification", "start_pos": 73, "end_pos": 117, "type": "TASK", "confidence": 0.7258365333080292}]}, {"text": "The technique is in fact the core of our Chinese review mining system, called Idea Miner or iMiner.", "labels": [], "entities": [{"text": "Idea Miner", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.8388078212738037}]}, {"text": "shows the architectural overview of iMiner, which comprises five modules, of which Module \u2162 (Opinion Feature Detection) and \u2163 (Contextual Sentiment Identification) are the main focus of this paper.", "labels": [], "entities": []}, {"text": "used syntactic analysis to identify features 1 in Chinese sentences, which is similar to the methods proposed by and.", "labels": [], "entities": []}, {"text": "However, syntactic analysis alone tends to extract many invalid features due to the colloquial nature of online reviews, which are often abruptly concise or grammatically incorrect.", "labels": [], "entities": []}, {"text": "To address the issue, our approach employs an additional step to prune candidates with low topical relevance, which is a statistical measure of how frequently a term appears in one review and across different reviews.", "labels": [], "entities": []}, {"text": "examined the effectiveness of using supervised learning methods to identify document level sentiments.", "labels": [], "entities": [{"text": "identify document level sentiments", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.6378399953246117}]}, {"text": "But the technique requires a large amount of training data, and must be re-trained whenever it is applied to anew domain.", "labels": [], "entities": []}, {"text": "Furthermore, it does not perform well at the sentence level. and proposed dictionary-based approaches to infer contextual sentiments from Chinese sentences.", "labels": [], "entities": []}, {"text": "However, it is difficult to maintain an up-to-date dictionary, as new expressions emerge frequently online.", "labels": [], "entities": []}, {"text": "In contrast, to identify the sentiment expressed in a review region 2 , our method first infers the polarity of an opinion word by using statistical association analysis, and subsequently analyzes the local context of the opinion word.", "labels": [], "entities": []}, {"text": "Our method is domain independent and uses only a small set of 80 polarized words instead of a huge dictionary.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since features are detected prior to the sentiments, there is a possibility for an erroneous feature (i.e., a false positive feature) to be associated with a sentiment.", "labels": [], "entities": []}, {"text": "We thus conducted two different experiments.", "labels": [], "entities": []}, {"text": "In the first case, we enumerate all extracted feature-sentiment pairs, including the wrong features.", "labels": [], "entities": []}, {"text": "In the second scenario, we enumerate the feature-sentiment pairs only for those correctly extracted features.", "labels": [], "entities": []}, {"text": "For each experiment, we further evaluated the result with (C) and without (N.C.) contextual information.", "labels": [], "entities": []}, {"text": "We select the best case of feature detection and then run our sentiment identification algorithm on the review dataset described in section 3.3; the polarity thresholds V-and V+ are set to 0.45 and 0.5, respectively.", "labels": [], "entities": [{"text": "feature detection", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.8037533760070801}, {"text": "sentiment identification", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.9322783648967743}]}, {"text": "shows the results for all detected features (correct and incorrect).", "labels": [], "entities": []}, {"text": "As shown inline 2, our method achieved an F-measure of 57.63% without considering contextual information, while precision and recall are 57% and 58.21%, respectively.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9996764659881592}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9997034668922424}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9992675185203552}]}, {"text": "Adding contextual information, as line 3 shows, boosts the F-measure to 71%, a remarkable 13.37% improvement.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9894326329231262}]}, {"text": "shows the results for just the correctly extracted features.", "labels": [], "entities": []}, {"text": "As shown inline 2, in the case of not considering contextual information, our method achieved an F-measure of 63.17%, while precision and recall were 69.05% and 58.21%, respectively.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9997912049293518}, {"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9997556805610657}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.999517560005188}]}, {"text": "By considering contextual information, line 3 shows that the F-measure improved to 77.82% which is 14.65% better, with precision and recall at 85.06% and 71.72%, respectively.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9968403577804565}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9997010231018066}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9993939399719238}]}, {"text": "The above results show that local contextual analysis of double and single negation can significantly improve the accuracy of sentiment orientation identification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.99813312292099}, {"text": "sentiment orientation identification", "start_pos": 126, "end_pos": 162, "type": "TASK", "confidence": 0.947315533955892}]}, {"text": "By examining the results shown inline 3 (in bold) of both, the F-measure on correctly identified features increases from 71% to 77.82%, while the precision increases drastically from 70.3% to 85.06%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9993985891342163}, {"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9996594190597534}]}, {"text": "The results show that our two-step approach of identifying sentiment orientation is reasonable and effective and that a great many of sentiments can be identified correctly for related features, especially for those correctly detected one.", "labels": [], "entities": [{"text": "identifying sentiment orientation", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.8088710308074951}]}, {"text": "However, in practice there is noway to tell the correctly identified features from the incorrect ones, thus is a more realistic gauge of our approach..", "labels": [], "entities": []}, {"text": "Lastly, we compared our approach to sentiment identification with FBS (see).", "labels": [], "entities": [{"text": "sentiment identification", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.9643265902996063}, {"text": "FBS", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.5638687610626221}]}, {"text": "The best results are used, as shown in the last rows of and 6.", "labels": [], "entities": []}, {"text": "When considering all features extracted, the F-measure of FBS is only 47.67%, which is 23.33% lower than that of iMiner, where both precision and recall are 49.70% and 45.80%, respectively.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9993467926979065}, {"text": "FBS", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9311053156852722}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9996335506439209}, {"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9987290501594543}]}, {"text": "Considering only the correctly detected features, iMiner widens its lead over FBS to 25% in terms of F-measure.", "labels": [], "entities": [{"text": "FBS", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.8557417392730713}, {"text": "F-measure", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9279129505157471}]}, {"text": "There are several explanations for the poor results of FBS: (1) The inferior results of feature detection affect the subsequent task of sentiment identification; and (2) the polarity inference depends heavily on a semantic dictionary WordNet.", "labels": [], "entities": [{"text": "FBS", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.6668463349342346}, {"text": "feature detection", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7153734564781189}, {"text": "sentiment identification", "start_pos": 136, "end_pos": 160, "type": "TASK", "confidence": 0.94320148229599}, {"text": "WordNet", "start_pos": 234, "end_pos": 241, "type": "DATASET", "confidence": 0.9291552901268005}]}, {"text": "In our experiments for FBS, we used an extended version of the \"\u540c\u4e49\u8bcd\u8bcd\u6797\" Thesaurus containing 77,492 words, and a sentiment lexicon with 8,856 words that is part of mini (free) HowNet, and lastly our seed word list containing 80 words.", "labels": [], "entities": [{"text": "FBS", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.9087182283401489}, {"text": "HowNet", "start_pos": 175, "end_pos": 181, "type": "DATASET", "confidence": 0.9510718584060669}]}], "tableCaptions": [{"text": " Table 2: Feature detection results.", "labels": [], "entities": [{"text": "Feature detection", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8209748864173889}]}, {"text": " Table 5: Results for all features.", "labels": [], "entities": []}, {"text": " Table 6: Results for correctly detected features.", "labels": [], "entities": []}]}