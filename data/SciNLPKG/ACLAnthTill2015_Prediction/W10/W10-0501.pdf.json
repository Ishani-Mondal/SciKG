{"title": [{"text": "The \"Nays\" Have It: Exploring Effects of Sentiment in Collaborative Knowledge Sharing", "labels": [], "entities": [{"text": "Sentiment in Collaborative Knowledge Sharing", "start_pos": 41, "end_pos": 85, "type": "TASK", "confidence": 0.681059616804123}]}], "abstractContent": [{"text": "In this paper we study what effects sentiment have on the temporal dynamics of user interaction and content generation in a knowledge sharing setting.", "labels": [], "entities": [{"text": "content generation", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7879756987094879}]}, {"text": "We try to identify how sentiment influences interaction dynamics in terms of answer arrival, user ratings arrival, community agreement and content popularity.", "labels": [], "entities": []}, {"text": "Our study suggests that \"Negativity Bias\" triggers more community attention and consequently more content contribution.", "labels": [], "entities": [{"text": "Negativity Bias", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.8170651495456696}]}, {"text": "Our findings provide insight into how users interact in online knowledge sharing communities, and helpful for improving existing systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, Collaborative Knowledge Sharing sites( or CQA sites), such as Naver and Yahoo!", "labels": [], "entities": []}, {"text": "Answers have exploded in popularity.", "labels": [], "entities": [{"text": "Answers", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.892295777797699}]}, {"text": "Already, for many information needs, these sites are becoming valuable alternatives to search engines.", "labels": [], "entities": []}, {"text": "Previous studies identified visibility as an important factor for content popularity and developed models in static settings.", "labels": [], "entities": [{"text": "content popularity", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.6898313909769058}]}, {"text": "However, when users post social media content, they might either explicitly or implicitly express their personal attitudes or sentiment.", "labels": [], "entities": []}, {"text": "The following example illustrates a question with negative sentiment.", "labels": [], "entities": []}, {"text": "Q :Obama keeps saying we need to sacrifice.", "labels": [], "entities": []}, {"text": "What sacrifices has he and the gov made collectively and individually?", "labels": [], "entities": []}, {"text": "A 1 : Our hard earned tax dollars.", "labels": [], "entities": []}, {"text": "17 \u2191, 2 \u2193 A 2 : None and they never will.", "labels": [], "entities": []}, {"text": "18 \u2191, 2 \u2193 Psychological studies ( suggest that our brain has \"Negativity Bias\" -that is, people automatically devote more attention to negative information than to positive information.", "labels": [], "entities": []}, {"text": "Thus, our attitudes maybe more heavily influenced by negative opinions.", "labels": [], "entities": []}, {"text": "Our hypothesis is that this kind of human cognitive bias would have measurable effects on how users respond to information need in CQA communities.", "labels": [], "entities": []}, {"text": "Our goal in this paper is to understand how question sentiment influence the dynamics of the user interactions in CQA -that is, to understand how users respond to questions of different sentiment, how question sentiment affects community agreement on best answer and question popularity.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our study we tracked a total of approximately 10,000 questions, sampled from 20 categories from Yahoo!", "labels": [], "entities": []}, {"text": "Specifically, each new question in our tracking list crawled every five minutes until it's closed.", "labels": [], "entities": []}, {"text": "As a result, we obtained approximately 22 million question-answer-feedback snapshots in total.", "labels": [], "entities": []}, {"text": "Since labeling all the questions would be expensive, we randomly selected 2000 questions from this dataset for human labeling.", "labels": [], "entities": [{"text": "human labeling", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.7373909056186676}]}, {"text": "We then utilized the Amazon Mechanical Turk Service . Five workers labeled each question as either positive, negative or neutral; the ratings were filtered by using majority opinion (at least 3 out of 5 labels).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk Service", "start_pos": 21, "end_pos": 51, "type": "DATASET", "confidence": 0.9078268855810165}]}, {"text": "Overall statistics of this dataset are reported in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the Temporal dataset", "labels": [], "entities": [{"text": "Temporal dataset", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.8874889314174652}]}, {"text": " Table 2: Agreement, Question length, Answer Length  and Star count averaged over question type", "labels": [], "entities": [{"text": "Agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9890785813331604}, {"text": "Answer Length", "start_pos": 38, "end_pos": 51, "type": "METRIC", "confidence": 0.8142840266227722}, {"text": "Star count", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9861226975917816}]}]}