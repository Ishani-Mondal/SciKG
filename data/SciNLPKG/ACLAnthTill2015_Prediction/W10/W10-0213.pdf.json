{"title": [], "abstractContent": [{"text": "We investigate the effect of text summarisa-tion in the problem of rating-inference-the task of associating a fine-grained numerical rating to an opinionated document.", "labels": [], "entities": [{"text": "text summarisa-tion", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.5848528444766998}]}, {"text": "We setup a comparison framework to study the effect of different summarisation algorithms of various compression rates in this task and compare the classification accuracy of summaries and documents for associating documents to classes.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.9676053524017334}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.8558774590492249}]}, {"text": "We make use of SVM algorithms to associate numerical ratings to opinionated documents.", "labels": [], "entities": []}, {"text": "The algorithms are informed by linguistic and sentiment-based features computed from full documents and summaries.", "labels": [], "entities": []}, {"text": "Preliminary results show that some types of summaries could be as effective or better as full documents in this problem.", "labels": [], "entities": [{"text": "summaries", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.9628177285194397}]}], "introductionContent": [{"text": "Public opinion has a great impact on company and government decision making.", "labels": [], "entities": [{"text": "company and government decision making", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.6511223673820495}]}, {"text": "In particular, companies have to constantly monitor public perception of their products, services, and key company representatives to ensure that good reputation is maintained.", "labels": [], "entities": []}, {"text": "Recent cases of public figures making headlines for the wrong reasons have shown how companies take into account public opinion to distance themselves from figures which can damage their public image.", "labels": [], "entities": []}, {"text": "The Web has become an important source for finding information, in the field of business intelligence, business analysts are turning their eyes to the Web in order to monitor public perception on products, services, policies, and managers.", "labels": [], "entities": []}, {"text": "The field of sentiment analysis has recently emerged as an important area of research in Natural Language Processing (NLP) which can provide viable solutions for monitoring public perception on a number of issues; with evaluation programs such as the Text REtrieval Conference track on blog mining 1 , the Text Analysis Conference 2 track on opinion summarisation, and the DEfi Fouille de Textes program () advances in the state of the art have been produced.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.928382396697998}, {"text": "Text REtrieval Conference track on blog mining 1", "start_pos": 251, "end_pos": 299, "type": "TASK", "confidence": 0.706320371478796}, {"text": "Text Analysis Conference 2 track on opinion summarisation", "start_pos": 306, "end_pos": 363, "type": "TASK", "confidence": 0.8163938373327255}]}, {"text": "Although sentiment analysis involves various different problems such as identifying subjective sentences or identifying positive and negative opinions in text, here we concentrate on the opinion classification task; and more specifically on rating-inference, the task of identifying the author's evaluation of an entity with respect to an ordinal-scale based on the author's textual evaluation of the entity (Pang and).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.9289609491825104}, {"text": "opinion classification", "start_pos": 187, "end_pos": 209, "type": "TASK", "confidence": 0.7623848617076874}]}, {"text": "The specific problem we study in this paper is that of associating a fine-grained rating (1=worst,...5=best) to a review.", "labels": [], "entities": []}, {"text": "This is in general considered a difficult problem because of the fuzziness inherent of mid-range ratings ( . A considerable body of research has recently been produced to tackle this problem () and reported figures showing accuracies ranging from 30% to 50% for such complex task; most approaches derive features for the classification task from the full document.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 223, "end_pos": 233, "type": "METRIC", "confidence": 0.981029212474823}]}, {"text": "In this research we ask whether extracting features from document summaries could help a classification system.", "labels": [], "entities": []}, {"text": "Since text summaries are meant to contain the essential content of a document, we investigate whether filtering noise through text summarisation is of any help in the rating-inference task.", "labels": [], "entities": [{"text": "text summaries", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.6465387344360352}, {"text": "text summarisation", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.6269838511943817}]}, {"text": "In re-cent years, text summarisation has been used to support both manual and automatic tasks; in the SUM-MAC evaluation), text summaries were tested in document classification and question answering tasks where summaries were considered suitable surrogates for full documents; studied summarisation in the context of a cross-document coreference task and found that summaries improved the performance of a clustering-based coreference mechanism; more recently have proposed text summarisation as a preprocessing step for student essay assessment finding that summaries could be used instead of full essays to group \"similar\" quality essays.", "labels": [], "entities": [{"text": "text summarisation", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.6312270313501358}, {"text": "document classification and question answering tasks", "start_pos": 153, "end_pos": 205, "type": "TASK", "confidence": 0.701263502240181}, {"text": "summarisation", "start_pos": 286, "end_pos": 299, "type": "TASK", "confidence": 0.9702041149139404}, {"text": "text summarisation", "start_pos": 475, "end_pos": 493, "type": "TASK", "confidence": 0.6642096191644669}]}, {"text": "Summarisation has been studied in the field of sentiment analysis with the objective of producing opinion summaries, however, to the best of our knowlegde there has been little research on the study of document summarisation as a text processing step for opinion classification.", "labels": [], "entities": [{"text": "Summarisation", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9729759097099304}, {"text": "sentiment analysis", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8849604725837708}, {"text": "document summarisation", "start_pos": 202, "end_pos": 224, "type": "TASK", "confidence": 0.7116927802562714}, {"text": "opinion classification", "start_pos": 255, "end_pos": 277, "type": "TASK", "confidence": 0.804387778043747}]}, {"text": "This paper presents a framework and extensive experiments on text summarisation for opinion classification, and in particular, for the rating-inference problem.", "labels": [], "entities": [{"text": "text summarisation", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.5826273709535599}, {"text": "opinion classification", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.8020314276218414}]}, {"text": "We will present results indicating that some types of summaries could be as effective or better than the full documents in this task.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows: Section 2 will compile the existing work with respect to the inference-rating problem; Section 3 and Section 4 will describe the corpus and the NLP tools used for all the experimental set-up.", "labels": [], "entities": []}, {"text": "Next, the text summarisation approaches will be described in Section 5, and then Section 6 will show the experiments conducted and the results obtained together with a discussion.", "labels": [], "entities": []}, {"text": "Finally, we will draw some conclusions and address further work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since there is no standard dataset for carrying out the rating-inference task, the corpus used for our experiments was one associated to a current project on business intelligence we are working on.", "labels": [], "entities": []}, {"text": "These data consisted of 89 reviews of several English banks (Abbey, Barcalys, Halifax, HSBC, Lloyds TSB, and National Westminster) gathered from the Internet.", "labels": [], "entities": [{"text": "Abbey, Barcalys, Halifax", "start_pos": 61, "end_pos": 85, "type": "DATASET", "confidence": 0.8288609981536865}, {"text": "National Westminster", "start_pos": 109, "end_pos": 129, "type": "DATASET", "confidence": 0.980406254529953}]}, {"text": "In particular the documents were collected from Ciao 3 , a Website where users can write reviews about different products and services, depending on their own experience.", "labels": [], "entities": []}, {"text": "lists some of the statistical properties of the data.", "labels": [], "entities": []}, {"text": "It is worth stressing upon the fact that the reviews have on average 2,603 words, which means that we are dealing with long documents rather than short ones, making the rating-inference task even more challenging.", "labels": [], "entities": []}, {"text": "The shortest document contains 1,491 words, whereas the longest document has more than 5,000 words.", "labels": [], "entities": []}, {"text": "Since the aim of the task we are pursuing focuses on classifying correctly the star fora review (ranging from 1 to 5 stars), it is necessary to study how 3 http://www.ciao.co.uk/ many reviews we have for each class, in order to see whether we have a balanced distribution or not.", "labels": [], "entities": []}, {"text": "shows this numbers for each star-rating.", "labels": [], "entities": []}, {"text": "It is worth mentioning that one-third of the reviews belong to the 4-star class.", "labels": [], "entities": []}, {"text": "In contrast, we have only 9 reviews that have been rated as 3-star, consisting of the 10% of the corpus, which is a very low number.", "labels": [], "entities": []}, {"text": "In this Section we are going to describe in detail all the experimental set-up.", "labels": [], "entities": []}, {"text": "Firstly, we will explain the corpus we used together with some figures regarding some statistics computed.", "labels": [], "entities": []}, {"text": "Secondly, we will describe in-depth all the experiments we ran and the results obtained.", "labels": [], "entities": []}, {"text": "Finally, an extensive discussion will be given in order to analyse all the results and draw some conclusions.", "labels": [], "entities": []}, {"text": "The main objective of the paper is to investigate the influence of summaries in contrast to full reviews for the rating-inference problem.", "labels": [], "entities": []}, {"text": "The purpose of the experiments is to analyse the performance of the different suggested text summarisation approaches and compare them to the performance of the full review.", "labels": [], "entities": []}, {"text": "Therefore, the experiments conducted were the following: for each proposed summarisation approach, we experimented with five different types of compression rates for summaries (ranging from 10% to 50%).", "labels": [], "entities": [{"text": "summarisation", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.988680362701416}]}, {"text": "Apart from the full review, we dealt with 14 different summarisation approaches (4 for generic, 5 for queryfocused and 5 for opinion-oriented summarisation), as well as 2 baselines (lead and final, taking the first or the last sentences according to a specific compression rate, respectively).", "labels": [], "entities": [{"text": "summarisation", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.9770077466964722}]}, {"text": "Each experiment consisted of predicting the correct star of a review, either with the review as a whole or with one of the summarisation approaches.", "labels": [], "entities": []}, {"text": "As we previously said in Section 4, for predicting the correct star-rating, we used machine learning techniques.", "labels": [], "entities": []}, {"text": "In particular, different features were used to train a SVM classifier with 10-fold cross validation 4 , using the whole review: the root of each word, its category, and the calculated value employing the SentiWordNet lexicon, as well as their combinations.", "labels": [], "entities": []}, {"text": "As a baseline for the full document we took into account a totally uninformed approach with respect to the class with higher number of reviews, i.e. considering all documents as if they were scored with 4 stars.", "labels": [], "entities": []}, {"text": "The different results according different features can be seen in  Regarding the features for training the summaries, it is worth mentioning that the best performing feature when no sentiment-based features are taken into account is the one using the root of the words.", "labels": [], "entities": []}, {"text": "Consequently, this feature was used to train the summaries.", "labels": [], "entities": []}, {"text": "Moreover, since the best results using the full review were obtained using the combination of the all the features (root+category+sentiWN), we also selected this combination to train the SVM classifier with our summaries.", "labels": [], "entities": []}, {"text": "Conducting both experiments, we could analyse to what extent the sentiment-based feature benefit the classification process.", "labels": [], "entities": []}, {"text": "The results obtained are shown in and, respectively.", "labels": [], "entities": []}, {"text": "These tables show the Fmeasure value obtained for the classification task, when features extracted from summaries are used instead from the full review.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9953294992446899}, {"text": "classification task", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.9086962342262268}]}, {"text": "On the one hand, results using the root feature extracted from summaries can be seen in.", "labels": [], "entities": []}, {"text": "On the other hand, shows the results when the combination of all the linguistic and sentiment-based features (root+category+sentiWN), that has been extracted from summaries, are used for training the SVM classifier.", "labels": [], "entities": [{"text": "SVM classifier", "start_pos": 200, "end_pos": 214, "type": "TASK", "confidence": 0.802287369966507}]}, {"text": "We also performed two statistical tests in order to measure the significance for the results obtained.", "labels": [], "entities": [{"text": "significance", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.9818190336227417}]}, {"text": "The tests we performed were the one-way Analysis of Variance (ANOVA) and the t-test.", "labels": [], "entities": [{"text": "Analysis of Variance (ANOVA)", "start_pos": 40, "end_pos": 68, "type": "METRIC", "confidence": 0.6472316781679789}]}, {"text": "Given a group of experiments, we first run ANOVA for analysing the difference between their means.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.987885057926178}]}, {"text": "In case some differences are found, we run the t-test between those pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: F-measure results using the full review for clas- sification", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9918097853660583}]}, {"text": " Table 4: Classification results (F-measure) for summaries using root (lead = first sentences; final = last sentences;  tf = term frequency; te = textual entailment; cqp = code quantity principle with noun-phrases; qf = query-focused  summaries; and sent = opinion-oriented summaries)", "labels": [], "entities": [{"text": "F-measure", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9835091233253479}]}, {"text": " Table 5: Classification results (F-measure) for summaries using root, category and SentiWordNet (lead = first sen- tences; final = last sentences; tf = term frequency; te = textual entailment; cqp = code quantity principle with  noun-phrases; qf = query-focused summaries; and sent = opinion-oriented summaries)", "labels": [], "entities": [{"text": "F-measure", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9659468531608582}]}]}