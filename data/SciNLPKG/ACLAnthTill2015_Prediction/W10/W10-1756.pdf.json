{"title": [{"text": "A Unified Approach to Minimum Risk Training and Decoding", "labels": [], "entities": [{"text": "Minimum Risk Training", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.9488258560498556}]}], "abstractContent": [{"text": "We present a unified approach to performing minimum risk training and minimum Bayes risk (MBR) decoding with BLEU in a phrase-based model.", "labels": [], "entities": [{"text": "minimum Bayes risk (MBR", "start_pos": 70, "end_pos": 93, "type": "METRIC", "confidence": 0.7513071715831756}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9953566193580627}]}, {"text": "Key to our approach is the use of a Gibbs sampler that allows us to explore the entire probability distribution and maintain a strict prob-abilistic formulation across the pipeline.", "labels": [], "entities": []}, {"text": "We also describe anew sampling algorithm called corpus sampling which allows us at training time to use BLEU instead of an approximation thereof.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9982115030288696}]}, {"text": "Our approach is theoretically sound and gives better (up to +0.6%BLEU) and more stable results than the standard MERT optimization algorithm.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9994868040084839}, {"text": "MERT optimization", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.8002544045448303}]}, {"text": "By comparing our approach to lattice MBR, we are also able to gain crucial insights about both methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "According to statistical decision theory, the optimal decision rule for any statistical model is the solution that minimizes its risk (expected loss).", "labels": [], "entities": [{"text": "statistical decision theory", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.73192431529363}]}, {"text": "This solution is often referred to as the Minimum Bayes Risk (MBR) solution ().", "labels": [], "entities": [{"text": "Minimum Bayes Risk (MBR)", "start_pos": 42, "end_pos": 66, "type": "METRIC", "confidence": 0.7803098658720652}]}, {"text": "Since machine translation (MT) models are typically evaluated by BLEU (), a loss function which rewards partial matches, the MBR solution is to be preferred to the Maximum A Posteriori (MAP) solution.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 6, "end_pos": 30, "type": "TASK", "confidence": 0.8657391309738159}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9988963603973389}, {"text": "MBR", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.6694214344024658}, {"text": "Maximum A Posteriori (MAP)", "start_pos": 164, "end_pos": 190, "type": "METRIC", "confidence": 0.802473376194636}]}, {"text": "In most statistical MT (SMT) systems, MBR is implemented as a reranker of a list 1 of translations generated by a first-pass decoder.", "labels": [], "entities": [{"text": "MT (SMT)", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.8519082516431808}, {"text": "MBR", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.6734709143638611}]}, {"text": "This decoder typically assigns unnormalised log probabilities (known as scores) to each translation hypoth-esis, so these scores must be converted to probabilities in order to apply MBR.", "labels": [], "entities": [{"text": "MBR", "start_pos": 182, "end_pos": 185, "type": "DATASET", "confidence": 0.5589026212692261}]}, {"text": "In order to perform this conversion, it is first necessary to compute the normalization function Z.", "labels": [], "entities": []}, {"text": "Since Z is defined as an intractable sum overall possible translations, it is approximated by summing over the translations in the list.", "labels": [], "entities": []}, {"text": "The second step is to find the correct scale factor for the scores using a hyper-parameter search over held-out data.", "labels": [], "entities": []}, {"text": "This is needed because the model parameters for the first-pass decoder are normally learnt using MERT, which is invariant under scaling of the scores.", "labels": [], "entities": [{"text": "MERT", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9705348014831543}]}, {"text": "Both these steps are theoretically unsatisfactory methods of estimating the posterior probability distribution since the approximation to Z is an unbounded term and the scaling factor is an artificial way of inducing a probability distribution.", "labels": [], "entities": []}, {"text": "Recently, ( have shown that using a search lattice to improve the estimation of the true probability distribution can lead to improved MBR performance.", "labels": [], "entities": [{"text": "MBR", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.5657852292060852}]}, {"text": "However, these approaches still rely on MERT for training the base model, and in fact introduce several extra parameters which must also be estimated using either grid search or a second MERT run.", "labels": [], "entities": []}, {"text": "The lattice pruning required to make these techniques tractable is quite drastic, and is in addition to the pruning already performed during the search.", "labels": [], "entities": []}, {"text": "Such extensive pruning is liable to render any probability estimates heavily biased).", "labels": [], "entities": []}, {"text": "Here, we present a unified approach to training and decoding in a phrase-based translation model () which keeps the objective constant across the translation pipeline and so obviates the need for any extra hyper-parameter fitting.", "labels": [], "entities": []}, {"text": "We use the phrase-based Gibbs sampler of at training time to compute the gradient of our minimum risk training objective in order to apply first-order optimization techniques, and attest time we use it to estimate the posterior distribution required by MBR (Section 3).", "labels": [], "entities": [{"text": "MBR", "start_pos": 253, "end_pos": 256, "type": "DATASET", "confidence": 0.7646846175193787}]}, {"text": "We experimented with two different objective functions for training (Section 4).", "labels": [], "entities": []}, {"text": "First, following (, we define our objective at the sentence-level using a sentence-level variant of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9960512518882751}]}, {"text": "Then, in order to reduce the mismatch between training and test loss functions, we also tried directly optimising the expected corpus level BLEU, where we introduce a novel sampling technique, which we call corpus sampling to calculate the required expectations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9100466370582581}]}, {"text": "The methods presented in this paper are theoretically sound.", "labels": [], "entities": []}, {"text": "Moreover, experimental evidence on three language pairs shows that our training regime is more stable than MERT, able to generalize better and generally leads to improvement in translation when used with sampling based MBR (Section 5).", "labels": [], "entities": [{"text": "MERT", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.7769937515258789}, {"text": "translation", "start_pos": 177, "end_pos": 188, "type": "TASK", "confidence": 0.9214524030685425}]}, {"text": "An added benefit is that the trained weights also lead to better performance when used with a beam-search based decoder.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Baseline results -MERT trained models decoded using Viterbi, nbest MBR (nMBR) and lattice  MBR (lMBR). MERT was run 10 times for each language pair. We report minimum, maximum, mean  and standard deviation of test set BLEU scores across the 10 runs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 228, "end_pos": 232, "type": "METRIC", "confidence": 0.7227665781974792}]}, {"text": " Table 2: Final results comparing MERT/Moses  pipeline with unified sampler pipeline. Sampler  uses corpus sampling during training and MBR  decoding at test time. Moses results are aver- aged across decoding runs using weights from  10 MERT runs and sampler results are averaged  across 10 decoding runs for each of 5 different  training runs. We report BLEU scores and standard  deviation (\u03c3).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 355, "end_pos": 359, "type": "METRIC", "confidence": 0.9996015429496765}, {"text": "standard  deviation (\u03c3)", "start_pos": 371, "end_pos": 394, "type": "METRIC", "confidence": 0.9644320845603943}]}]}