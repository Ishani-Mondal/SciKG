{"title": [{"text": "The DCU Dependency-Based Metric in WMT-MetricsMATR 2010", "labels": [], "entities": [{"text": "DCU Dependency-Based Metric in WMT-MetricsMATR 2010", "start_pos": 4, "end_pos": 55, "type": "DATASET", "confidence": 0.8009388546148936}]}], "abstractContent": [{"text": "We describe DCU's LFG dependency-based metric submitted to the shared evaluation task of WMT-MetricsMATR 2010.", "labels": [], "entities": [{"text": "DCU", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.9377679228782654}, {"text": "WMT-MetricsMATR 2010", "start_pos": 89, "end_pos": 109, "type": "DATASET", "confidence": 0.8706805109977722}]}, {"text": "The metric is built on the LFG F-structure-based approach presented in (Owczarzak et al., 2007).", "labels": [], "entities": []}, {"text": "We explore the following improvements on the original metric: 1) we replace the in-house LFG parser with an open source dependency parser that directly parses strings into LFG dependencies ; 2) we add a stemming module and unigram paraphrases to strengthen the aligner; 3) we introduce a chunk penalty following the practice of METEOR to reward continuous matches; and 4) we introduce and tune parameters to maximize the correlation with human judgement.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 328, "end_pos": 334, "type": "METRIC", "confidence": 0.6931929588317871}]}, {"text": "Experiments show that these enhancements improve the dependency-based metric's correlation with human judgement.", "labels": [], "entities": []}], "introductionContent": [{"text": "String-based automatic evaluation metrics such as BLEU () have led directly to quality improvements in machine translation (MT).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9976688027381897}, {"text": "machine translation (MT)", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.8218259990215302}]}, {"text": "These metrics provide an alternative to expensive human evaluations, and enable tuning of MT systems based on automatic evaluation results.", "labels": [], "entities": [{"text": "MT", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9898411631584167}]}, {"text": "However, there is widespread recognition in the MT community that string-based metrics are not discriminative enough to reflect the translation quality of today's MT systems, many of which have gone beyond pure string-based approaches (cf.).", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9678171277046204}, {"text": "MT", "start_pos": 163, "end_pos": 165, "type": "TASK", "confidence": 0.9684985876083374}]}, {"text": "With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and linguistically motivated resources.", "labels": [], "entities": []}, {"text": "Examples include METEOR () and TERP , both of which now utilize stemming, WordNet and paraphrase information.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.8448516726493835}, {"text": "TERP", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9628890752792358}, {"text": "WordNet", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9229357242584229}]}, {"text": "Experimental and evaluation campaign results have shown that these metrics can obtain better correlation with human judgements than metrics that only use surface-level information.", "labels": [], "entities": []}, {"text": "Given that many of today's MT systems incorporate some kind of syntactic information, it was perhaps natural to use syntax in automatic MT evaluation as well.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9730246663093567}, {"text": "MT evaluation", "start_pos": 136, "end_pos": 149, "type": "TASK", "confidence": 0.9657949209213257}]}, {"text": "This direction was first explored by, who used syntactic structure and dependency information to go beyond the surface level matching.", "labels": [], "entities": []}, {"text": "extended this line of research with the use of a term-based encoding of Lexical Functional Grammar (LFG:) labelled dependency graphs into unordered sets of dependency triples, and calculating precision, recall, and F-score on the triple sets corresponding to the translation and reference sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 192, "end_pos": 201, "type": "METRIC", "confidence": 0.9992921352386475}, {"text": "recall", "start_pos": 203, "end_pos": 209, "type": "METRIC", "confidence": 0.9985594153404236}, {"text": "F-score", "start_pos": 215, "end_pos": 222, "type": "METRIC", "confidence": 0.9991230368614197}]}, {"text": "With the addition of partial matching and n-best parses,'s method considerably outperforms w.r.t. correlation with human judgement.", "labels": [], "entities": []}, {"text": "The EDPM metric () improves this line of research by using arc labels derived from a Probabilistic Context-Free Grammar (PCFG) parse to replace the LFG labels, showing that a PCFG parser is sufficient for preprocessing, compared to a dependency parser in (Liu and) and (.", "labels": [], "entities": []}, {"text": "EDPM also incorporates more information sources: e.g. the parser confidence, the Porter stemmer, WordNet synonyms and paraphrases.", "labels": [], "entities": [{"text": "EDPM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9495130777359009}]}, {"text": "Besides the metrics that rely solely on the dependency structures, information from the dependency parser is a component of some other metrics that use more diverse resources, such as the textual entailment-based metric of (.", "labels": [], "entities": []}, {"text": "In this paper we extend the work of) in a different manner: we use an adapted version of the Malt parser () to produce 1-best LFG dependencies and allow triple matches where the dependency labels are different.", "labels": [], "entities": []}, {"text": "We incorporate stemming, synonym and paraphrase information as in (), and at the same time introduce a chunk penalty in the spirit of METEOR to penalize discontinuous matches.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.936870276927948}]}, {"text": "We sort the matches according to the match level and the dependency type, and weight the matches to maximize correlation with human judgement.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews the dependency-based metric.", "labels": [], "entities": []}, {"text": "Sections 3, 4, 5 and 6 introduce our improvements on this metric.", "labels": [], "entities": []}, {"text": "We report experimental results in Section 7 and conclude in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "The basic method of can be illustrated by the example in.", "labels": [], "entities": []}, {"text": "The metric in) performs triple matching over the Hyp-and Ref-Triples and calculates the metric score using the F-score of matching precision and recall.", "labels": [], "entities": [{"text": "triple matching", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.6642288863658905}, {"text": "F-score", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9936888217926025}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9265490174293518}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9978224039077759}]}, {"text": "Let m be the number of matches, h be the number of triples in the hypothesis and e be the number of triples in the reference.", "labels": [], "entities": []}, {"text": "Then we have the matching precision P = m/h and recall R = m/e.", "labels": [], "entities": [{"text": "matching precision P", "start_pos": 17, "end_pos": 37, "type": "METRIC", "confidence": 0.7898711959520975}, {"text": "recall R", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9594871997833252}]}, {"text": "The score of the hypothesis in () is the Fscore based on the precision and recall of matching as in:  We experiment with four settings of the metric: HARD, SOFT, SOFTALL and WEIGHTED in order to validate our enhancements.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9990108013153076}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9989519119262695}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9978680610656738}, {"text": "HARD", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9973740577697754}, {"text": "SOFT", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.8682066202163696}, {"text": "WEIGHTED", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.8503211140632629}]}, {"text": "The first two settings compare the effect of allowing/not allowing soft matches, but only uses WordNet as in.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9695990085601807}]}, {"text": "The third setting applies our additional linguistic features and the final setting tunes parameter weights for higher correlation with human judgement.", "labels": [], "entities": []}, {"text": "We report Pearson's r, Spearman's \u03c1 and Kendall's \u03c4 on segment and system levels on the NIST MetricsMATR 2010 development set using Snover's scoring tool.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9069268504778544}, {"text": "Kendall's \u03c4", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.7317726810773214}, {"text": "NIST MetricsMATR 2010 development set", "start_pos": 88, "end_pos": 125, "type": "DATASET", "confidence": 0.9428009152412414}]}, {"text": "7 shows that allowing soft triple matches and using more linguistic features all lead to higher correlation with human judgement.", "labels": [], "entities": []}, {"text": "Though the parameters might somehow overfit on the data set even if we apply cross validation, this certainly confirms the necessity of weighing dependency matches according to their types.", "labels": [], "entities": []}, {"text": "When considering the system-level correlation in, the trend is very similar to that of the segment level.", "labels": [], "entities": []}, {"text": "The improvements we introduce all lead to improvements in correlation with human judgement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Correlation on the Segment Level  r  \u03c1  \u03c4  HARD  0.557 0.586 0.176  SOFT  0.600 0.634 0.213  SOFTALL  0.633 0.662 0.235  WEIGHTED 0.673 0.709 0.277", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9631205797195435}, {"text": "HARD", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.7672481536865234}, {"text": "WEIGHTED", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.8454802632331848}]}, {"text": " Table 3: Correlation on the System Level  r  \u03c1  \u03c4  HARD  0.948 0.905 0.786  SOFT  0.964 0.905 0.786  SOFTALL  0.975 0.976 0.929  WEIGHTED 0.989 1.000 1.000", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9556673169136047}, {"text": "HARD", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9726734161376953}, {"text": "WEIGHTED 0.989 1.000 1.000", "start_pos": 130, "end_pos": 156, "type": "METRIC", "confidence": 0.8266338557004929}]}]}