{"title": [{"text": "Applying morphological decomposition to statistical machine translation", "labels": [], "entities": [{"text": "Applying morphological decomposition", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7760891715685526}, {"text": "statistical machine translation", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.6815509001413981}]}], "abstractContent": [{"text": "This paper describes the Aalto submission for the German-to-English and the Czech-to-English translation tasks of the ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR.", "labels": [], "entities": [{"text": "Czech-to-English translation tasks of the ACL 2010 Joint Fifth Workshop on Statistical Machine Translation", "start_pos": 76, "end_pos": 182, "type": "TASK", "confidence": 0.7893114941460746}]}, {"text": "Statistical machine translation has focused on using words, and longer phrases constructed from words, as tokens in the system.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6553682486216227}]}, {"text": "In contrast, we apply different morphological decompositions of words using the unsupervised Morfessor algorithms.", "labels": [], "entities": []}, {"text": "While translation models trained using the morphological decompositions did not improve the BLEU scores, we show that the Minimum Bayes Risk combination with a word-based translation model produces significant improvements for the German-to-English translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9990676045417786}, {"text": "Minimum Bayes Risk", "start_pos": 122, "end_pos": 140, "type": "METRIC", "confidence": 0.6588007112344106}]}, {"text": "However, we did not see improvements for the Czech-to-English translations.", "labels": [], "entities": []}], "introductionContent": [{"text": "The effect of morphological variation in languages can be alleviated by using word analysis schemes, which may include morpheme discovery, part-ofspeech tagging, or other linguistic information.", "labels": [], "entities": [{"text": "morpheme discovery", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7734727561473846}, {"text": "part-ofspeech tagging", "start_pos": 139, "end_pos": 160, "type": "TASK", "confidence": 0.6884692311286926}]}, {"text": "Words are very convenient and even efficient representation in statistical natural language processing, especially with English, but morphologically rich languages can benefit from more fine-grained information.", "labels": [], "entities": [{"text": "statistical natural language processing", "start_pos": 63, "end_pos": 102, "type": "TASK", "confidence": 0.6207299083471298}]}, {"text": "For instance, statistical morphs discovered with unsupervised methods result in better performance in automatic speech recognition for highly-inflecting and agglutinative languages ; ).", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 102, "end_pos": 130, "type": "TASK", "confidence": 0.6458345850308737}]}, {"text": "applied morph-based models in statistical machine translation (SMT) between several language pairs without gaining improvement in BLEU score, but obtaining reductions in out-of-vocabulary rates.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.7809191644191742}, {"text": "BLEU score", "start_pos": 130, "end_pos": 140, "type": "METRIC", "confidence": 0.9840295016765594}]}, {"text": "They utilized morphs both in the source and in the target language.", "labels": [], "entities": []}, {"text": "Later, de showed that Minimum Bayes Risk (MBR) combination of word-based and morph-based translation models improves translation with Arabicto-English and Finnish-to-English language pairs, where only the source language utilized morphbased models.", "labels": [], "entities": [{"text": "Minimum Bayes Risk (MBR)", "start_pos": 22, "end_pos": 46, "type": "METRIC", "confidence": 0.8318518499533335}, {"text": "translation", "start_pos": 117, "end_pos": 128, "type": "TASK", "confidence": 0.9741525650024414}]}, {"text": "Similar results have been shown for Finnish-to-English and Finnish-to-German in performance evaluation of various unsupervised morpheme analysis algorithms in Morpho Challenge 2009 competition (.", "labels": [], "entities": [{"text": "Morpho Challenge 2009 competition", "start_pos": 159, "end_pos": 192, "type": "DATASET", "confidence": 0.826498419046402}]}, {"text": "We continue the research described above and examine how the level of decomposition affects both the individual morph-based systems and MBR combinations with the baseline word-based model.", "labels": [], "entities": []}, {"text": "Experiments are conducted with the WMT10 shared task data for German-to-English and Czech-to-English language pairs.", "labels": [], "entities": [{"text": "WMT10 shared task data", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.8116790056228638}]}], "datasetContent": [{"text": "The German-to-English and Czech-to-English parts of the ACL WMT10 shared task data were investigated.", "labels": [], "entities": [{"text": "ACL WMT10 shared task data", "start_pos": 56, "end_pos": 82, "type": "DATASET", "confidence": 0.896430230140686}]}, {"text": "Vanilla SMT models were trained with Moses using word tokens for MBR combination and comparison purposes.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.8547292947769165}, {"text": "MBR combination", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.80742347240448}]}, {"text": "Several different morphological segmentation models for German and Czech were trained with Morfessor.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.6397843807935715}]}, {"text": "Each segmentation model corresponds to a morph-based SMT model trained with Moses.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9643334150314331}]}, {"text": "The word-based vanilla Moses model is compared to each morphbased model as well as to several MBR com-binations between word-based translation models and morph-based translation models.", "labels": [], "entities": [{"text": "word-based vanilla Moses", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.5836245119571686}]}, {"text": "Quantitative evaluation is carried out using the BLEU score with re-cased and re-tokenized translations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9512576162815094}]}], "tableCaptions": [{"text": " Table 1: Data sets for the Czech-to-English and German-to-English SMT experiments, including the  number of aligned sentences and the average number of words per sentence in each language. The data  sets used for model training, development and evaluation are marked. Training is divided into German  (DE) and Czech (CZ) segmentation model (SM) training, English (EN) language model (LM) training  and German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation model (TM) training.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.73464035987854}]}]}