{"title": [{"text": "HMM Word-to-Phrase Alignment with Dependency Constraints", "labels": [], "entities": [{"text": "HMM Word-to-Phrase Alignment", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7843901117642721}]}], "abstractContent": [{"text": "In this paper, we extend the HMM word-to-phrase alignment model with syntactic dependency constraints.", "labels": [], "entities": [{"text": "HMM word-to-phrase alignment", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.8521422346433004}]}, {"text": "The syntactic dependencies between multiple words in one language are introduced into the model in a bid to produce coherent alignments.", "labels": [], "entities": []}, {"text": "Our experimental results on a variety of Chinese-English data show that our syntactically constrained model can lead to as much as a 3.24% relative improvement in BLEU score over current HMM word-to-phrase alignment models on a Phrase-Based Statistical Machine Translation system when the training data is small, and a comparable performance compared to IBM model 4 on a Hiero-style system with larger training data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 163, "end_pos": 173, "type": "METRIC", "confidence": 0.9798400700092316}, {"text": "HMM word-to-phrase alignment", "start_pos": 187, "end_pos": 215, "type": "TASK", "confidence": 0.7010651131470998}, {"text": "Phrase-Based Statistical Machine Translation", "start_pos": 228, "end_pos": 272, "type": "TASK", "confidence": 0.5512934848666191}]}, {"text": "An intrinsic alignment quality evaluation shows that our alignment model with dependency constraints leads to improvements in both precision (by 1.74% relative) and recall (by 1.75% relative) over the model without dependency information.", "labels": [], "entities": [{"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9993910789489746}, {"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9993627667427063}]}], "introductionContent": [{"text": "Generative word alignment models including IBM models and HMM word alignment models () have been widely used in various types of Statistical Machine Translation (SMT) systems.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7049918472766876}, {"text": "HMM word alignment", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.5938667754332224}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 129, "end_pos": 166, "type": "TASK", "confidence": 0.8170098960399628}]}, {"text": "This widespread use can be attributed to their robustness and high performance particularly on largescale translation tasks.", "labels": [], "entities": [{"text": "largescale translation tasks", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.6508141358693441}]}, {"text": "However, the quality of the alignment yielded from these models is still far from satisfactory even with significant amounts of training data; this is particularly true for radically different languages such as Chinese and English.", "labels": [], "entities": []}, {"text": "The weakness of most generative models often lies in the incapability of addressing one to many (1-to-n), many to one (n-to-1) and many to many (m-to-n) alignments.", "labels": [], "entities": []}, {"text": "Some research directly addresses m-to-n alignment with phrase alignment models ().", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.7629674673080444}]}, {"text": "However, these models are unsuccessful largely due to intractable estimation.", "labels": [], "entities": []}, {"text": "Recent progress in better parameterisation and approximate inference can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. ().", "labels": [], "entities": [{"text": "approximate inference", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.8670307993888855}]}, {"text": "The most widely used word alignment models, such as IBM models 3 and 4, can only model 1-to-n alignment; these models are often called \"asymmetric\" models.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.7419085204601288}]}, {"text": "IBM models 3 and 4 model 1-to-n alignments using the notion of \"fertility\", which is associated with a \"deficiency\" problem despite its high performance in practice.", "labels": [], "entities": []}, {"text": "On the other hand, the HMM word-to-phrase alignment model tackles 1-to-n alignment problems with simultaneous segmentation and alignment while maintaining the efficiency of the models.", "labels": [], "entities": [{"text": "HMM word-to-phrase alignment", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.7815153002738953}]}, {"text": "Therefore, this model sets a good example of addressing the tradeoffs between modelling power and modelling complexity.", "labels": [], "entities": []}, {"text": "This model can also be seen as a more generalised case of the HMM word-to-word model (, since this model can be reduced to an HMM word-to-word model by restricting the generated target phrase length to one.", "labels": [], "entities": []}, {"text": "One can further refine existing word alignment models with syntactic constraints (e.g. ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7465261220932007}]}, {"text": "However, most research focuses on the incorporation of syntactic constraints into discriminative alignment models.", "labels": [], "entities": []}, {"text": "Introducing syntactic information into generative alignment models is shown to be more challenging mainly due to the absence of appropriate modelling of syntactic constraints and the \"inflexibility\" of these generative models.", "labels": [], "entities": [{"text": "generative alignment", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.9471069276332855}]}, {"text": "In this paper, we extend the HMM word-tophrase alignment model with syntactic dependencies by presenting a model that can incorporate syntactic information while maintaining the efficiency of the model.", "labels": [], "entities": [{"text": "HMM word-tophrase alignment", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.7772940993309021}]}, {"text": "This model is based on the observation that in 1-to-n alignments, then words bear some syntactic dependencies.", "labels": [], "entities": []}, {"text": "Leveraging such information in the model can potentially further aid the model in producing more fine-grained word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 110, "end_pos": 125, "type": "TASK", "confidence": 0.7193203270435333}]}, {"text": "The syntactic constraints are specifically imposed on then words involved in 1-to-n alignments, which is different from the cohesion constraints) as explored by, where knowledge of cross-lingual syntactic projection is used.", "labels": [], "entities": [{"text": "cross-lingual syntactic projection", "start_pos": 181, "end_pos": 215, "type": "TASK", "confidence": 0.6243060330549876}]}, {"text": "As a syntactic extension of the open-source MTTK implementation) of the HMM word-to-phrase alignment model, its source code will also be released as open source in the near future.", "labels": [], "entities": [{"text": "HMM word-to-phrase alignment", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.7624508937199911}]}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the HMM word-tophrase alignment model.", "labels": [], "entities": [{"text": "HMM word-tophrase alignment", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.8339126904805502}]}, {"text": "In section 3, we present the details of the incorporation of syntactic dependencies.", "labels": [], "entities": []}, {"text": "Section 4 presents the experimental setup, and section 5 reports the experimental results.", "labels": [], "entities": []}, {"text": "In section 6, we draw our conclusions and point out some avenues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to further investigate the intrinsic quality of the word alignment, we compute the Precision (P), Recall (R) and F-score (F) of the alignments obtained using different alignment models.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 92, "end_pos": 105, "type": "METRIC", "confidence": 0.964307889342308}, {"text": "Recall (R)", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.9592715352773666}, {"text": "F-score (F)", "start_pos": 122, "end_pos": 133, "type": "METRIC", "confidence": 0.9594286531209946}]}, {"text": "As the models investigated here are asymmetric models, we conducted intrinsic evaluation for both alignment directions, i.e. ZH-EN word alignment where one Chinese word can be aligned to multiple English words, and EN-ZH word alignment where one English word can be aligned to multiple Chinese words.", "labels": [], "entities": [{"text": "EN-ZH word alignment", "start_pos": 215, "end_pos": 235, "type": "TASK", "confidence": 0.5817537705103556}]}, {"text": "shows the results of the intrinsic evaluation of ZH-EN and EN-ZH word alignment on a small data set (results on the medium data set follow the same trend but are left out due to space limitations).", "labels": [], "entities": [{"text": "EN-ZH word alignment", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.5912239750226339}]}, {"text": "Note that the P and R are all quite low, demonstrating the difficulty of Chinese-English word alignment in the news domain.", "labels": [], "entities": [{"text": "Chinese-English word alignment", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.6109884977340698}]}, {"text": "For the ZH-EN direction, using the SSH model does not lead to significant gains over SH in P or R.", "labels": [], "entities": []}, {"text": "For the EN-ZH direction, the SSH model leads to a 1.74% relative improvement in P, and a 1.75% relative improvement in Rover the SH model.", "labels": [], "entities": [{"text": "P", "start_pos": 80, "end_pos": 81, "type": "METRIC", "confidence": 0.9980608820915222}]}, {"text": "Both SH and SSH lead to gains over H for both ZH-EN and EN-ZH directions, while gains in the EN-ZH direction appear to be more pronounced.", "labels": [], "entities": []}, {"text": "IBM model 4 achieves significantly higher P over other models while the gap in R is narrow.", "labels": [], "entities": [{"text": "IBM model 4", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9283338983853658}, {"text": "P", "start_pos": 42, "end_pos": 43, "type": "METRIC", "confidence": 0.9986193180084229}, {"text": "R", "start_pos": 79, "end_pos": 80, "type": "METRIC", "confidence": 0.9786252379417419}]}, {"text": "Relating to, we observe that the HMM word-to-word alignment model (H) can still achieve good MT performance despite the lower P and R compared to other models.", "labels": [], "entities": [{"text": "HMM word-to-word alignment", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.7000406781832377}, {"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9870175719261169}, {"text": "R", "start_pos": 132, "end_pos": 133, "type": "METRIC", "confidence": 0.510520875453949}]}, {"text": "This provides additional support to previous findings) that the intrinsic quality of word alignment does not necessarily correlate with the performance of the resulted MT system.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.7283769398927689}, {"text": "MT", "start_pos": 168, "end_pos": 170, "type": "TASK", "confidence": 0.9827128648757935}]}], "tableCaptions": [{"text": " Table 2: Intrinsic evaluation of the alignment us- ing different alignment models", "labels": [], "entities": []}, {"text": " Table 3: Alignment types using different alignment models", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9710587859153748}]}]}