{"title": [{"text": "Acquiring Human-like Feature-Based Conceptual Representations from Corpora", "labels": [], "entities": [{"text": "Acquiring Human-like Feature-Based Conceptual Representations", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.7909055471420288}]}], "abstractContent": [{"text": "The automatic acquisition of feature-based conceptual representations from text corpora can be challenging, given the unconstrained nature of human-generated features.", "labels": [], "entities": []}, {"text": "We examine large-scale extraction of concept-relation-feature triples and the utility of syntactic , semantic, and encyclopedic information in guiding this complex task.", "labels": [], "entities": []}, {"text": "Methods traditionally employed do not investigate the full range of triples occurring in human-generated norms (e.g. flute produce sound), rather targeting concept-feature pairs (e.g. flute-sound) or triples involving specific relations (e.g. is-a, part-of).", "labels": [], "entities": []}, {"text": "We introduce a novel method that extracts candidate triples (e.g. deer have antlers, flute produce sound) from parsed data and re-ranks them using semantic information.", "labels": [], "entities": []}, {"text": "We apply this technique to Wikipedia and the British National Corpus and assess its accuracy in a variety of ways.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 45, "end_pos": 68, "type": "DATASET", "confidence": 0.9380692640940348}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9983193278312683}]}, {"text": "Our work demonstrates the utility of external knowledge in guiding feature extraction, and suggests a number of avenues for future work.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.722233459353447}]}], "introductionContent": [{"text": "In the cognitive sciences, theories about how concrete concepts such as ELEPHANT are represented in the mind have often adopted a distributed, featurebased model of conceptual knowledge (e.g.,).", "labels": [], "entities": []}, {"text": "According to such accounts, conceptual representations consist of patterns of activation over sets of interconnected semantic feature nodes (e.g. has eyes, has ears, is large).", "labels": [], "entities": []}, {"text": "To test these theories empirically, cognitive psychologists require an accurate estimate of the kinds of knowledge that people are likely to represent in such a system.", "labels": [], "entities": []}, {"text": "To date, the most important sources of such knowledge are property-norming studies, where a large number of participants write down lists of features for concepts.", "labels": [], "entities": []}, {"text": "For example, collected a set of norms listing features for 541 concrete concepts.", "labels": [], "entities": []}, {"text": "In that study, the features listed by different participants were normalised by mapping different feature descriptions with identical meanings to the same feature label.", "labels": [], "entities": []}, {"text": "1 gives the ten most frequent normed features for two concepts in the norms.", "labels": [], "entities": []}, {"text": "However, property norm data have certain weaknesses (these have been widely discussed; e.g.,).", "labels": [], "entities": []}, {"text": "One issue is that participants tend to under-report features that are present in many of the concepts in a given category ().", "labels": [], "entities": []}, {"text": "For example, for the concept ELEPHANT, participants list salient features like has trunk, but not less salient features such as breathes air, even though presumably all McRae et al.'s participants knew that elephants breathe air.", "labels": [], "entities": [{"text": "ELEPHANT", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.8045229315757751}]}, {"text": "Although the largest collection of norms lists features for over 500 concepts, the relatively small size of property norm sets still gives cause for concern.", "labels": [], "entities": []}, {"text": "Larger sets of norms would be useful to psycholinguists; however, large-scale property norming studies are time-consuming and costly.", "labels": [], "entities": []}, {"text": "In NLP, researchers have developed methods for extracting and classifying generic relationships from data, e.g.,.", "labels": [], "entities": []}, {"text": "In recent years, researchers have also begun to develop methods which can automatically extract feature norm-like representations from corpora, e.g.,,.", "labels": [], "entities": []}, {"text": "The automatic approach is capable of gathering large-scale distributional data, and furthermore it is cost-effective.", "labels": [], "entities": []}, {"text": "Corpora contain natural-language instances of words denoting concepts and their features, and therefore serve as ideal material for feature generation tasks.", "labels": [], "entities": [{"text": "feature generation tasks", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.8040205041567484}]}, {"text": "However, current methods are restricted to specific relations between concepts and their features, or target concept-feature pairs only.", "labels": [], "entities": []}, {"text": "For example, proposed a method based on manually developed lexico-syntactic patterns that extracts information about attributes and values of concepts.", "labels": [], "entities": []}, {"text": "They used these syntactic patterns and two grammatical relations to create descriptions of nouns consisting of vector entries and evaluated their approach based on how well their vector descriptions clustered concepts.", "labels": [], "entities": []}, {"text": "This method performed well, but targeted is-a and part-of relations only.", "labels": [], "entities": []}, {"text": "combined manually defined linguistic patterns with a cooccurrence based method to extract features involving six classes of relations.", "labels": [], "entities": []}, {"text": "He then split learning for the property classes into two distinct paradigms.", "labels": [], "entities": []}, {"text": "One used a pattern-based approach (four classes) with a seeded pattern-learning algorithm.", "labels": [], "entities": []}, {"text": "The other measured strength of association between the concept and referring adjectives and verbs (two classes).", "labels": [], "entities": []}, {"text": "His pattern-based approach worked well for properties in the superordinate class, had reasonable recall for stuff and location classes, but zero recall for part class.", "labels": [], "entities": [{"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9960159659385681}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.989153265953064}]}, {"text": "His approach for the other two classes used various association measures which he summed to establish an overall score for potential properties.", "labels": [], "entities": []}, {"text": "The recent Strudel model () relies on more general linguistic patterns, \"connector patterns\", consisting of sequences of part-of-speech (POS) tags to look for candidate feature terms near a target concept.", "labels": [], "entities": []}, {"text": "The method assumes that \"the variety of patterns connecting a concept and a potential property is a good indicator of the presence of a true semantic link\".", "labels": [], "entities": []}, {"text": "Thus, properties are scored based on the count of distinct patterns connecting them to a concept.", "labels": [], "entities": []}, {"text": "When evaluated against the ESS-LLI dataset ( ; see section 3.1), Strudel yields a precision of 23.9% -this figure is the best state-of-the-art result for unconstrained acquisition of concept-feature pairs.", "labels": [], "entities": [{"text": "ESS-LLI dataset", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.9585472643375397}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9979103207588196}]}, {"text": "It seems unlikely that further development of the shallow connector patterns will significantly improve accuracy, as these already broadly cover most POS sequences that are concept-feature connectors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9991169571876526}]}, {"text": "Because of the difficult nature of the task, we believe that extraction of more accurate representations necessitates additional linguistic and world knowledge.", "labels": [], "entities": []}, {"text": "Furthermore, the utility of Strudel is limited because it only produces concept-feature pairs, and not concept-relation-feature triples similar to those inhuman generated norms (although the distribution of the connector patterns fora extracted pair does offer clues about the broad class of semantic relation that holds between concept and feature).", "labels": [], "entities": []}, {"text": "In this paper, we explore issues of both methodology and evaluation that arise when attempting unconstrained, large-scale extraction of conceptrelation-feature triples in corpus data.", "labels": [], "entities": []}, {"text": "Extracting such human-like features is difficult, and we do not anticipate a high level of accuracy in these early experiments.", "labels": [], "entities": [{"text": "Extracting", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9595030546188354}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9993258714675903}]}, {"text": "We examine the utility of three types of external knowledge in guiding feature extraction: syntactic, semantic and encyclopedic.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7523061335086823}]}, {"text": "We build three automatically parsed corpora, two from Wikipedia and one from the British National Corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 81, "end_pos": 104, "type": "DATASET", "confidence": 0.9399888515472412}]}, {"text": "We introduce a method that (i) extracts conceptrelation-feature triples from grammatical dependency paths produced by a parser and (ii) uses probabilistic information about semantic classes of features and concepts to re-rank the candidate triples before filtering them.", "labels": [], "entities": []}, {"text": "We then assess the accuracy of our model using several different methods, and demonstrate that external knowledge can help guide the extraction of human-like features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9993090629577637}]}, {"text": "Finally, we highlight issues in both methodology and evaluation that are important for further progress in this area of research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We considered a number of methods for evaluating the quality of the extracted feature triples.", "labels": [], "entities": []}, {"text": "One possibility would be to calculate precision and recall for the extracted triples with respect to the McRae norms \"gold standard\".", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9993677735328674}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.999519944190979}, {"text": "McRae norms \"gold standard", "start_pos": 105, "end_pos": 131, "type": "DATASET", "confidence": 0.9243177771568298}]}, {"text": "However, direct comparison with the recoded norms is problematic, since there maybe extracted features which are semantically equivalent to a triple in the norms but possessing a different lexical form.", "labels": [], "entities": []}, {"text": "Since semantically identical features can be lexically different, we followed the approach taken in the ESSLLI 2008 Workshop on semantic models ( ).", "labels": [], "entities": [{"text": "ESSLLI 2008 Workshop", "start_pos": 104, "end_pos": 124, "type": "DATASET", "confidence": 0.8215562502543131}]}, {"text": "The gold standard for the ESS-LLI task was the top 10 features for 44 of the McRae concepts.", "labels": [], "entities": [{"text": "ESS-LLI task", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.6469683051109314}]}, {"text": "For each concept-feature pair an expansion set was generated containing synonyms of the feature terms appearing in the norms.", "labels": [], "entities": []}, {"text": "For example, the feature lives on water was expanded to the set {aquatic, lake, ocean, river, sea, water}.", "labels": [], "entities": []}, {"text": "We would expect to find in corpus data correct features that do not appear in our \"gold standard\" (e.g. breathes air is listed for WHALE but for no other animal).", "labels": [], "entities": [{"text": "WHALE", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.4870396852493286}]}, {"text": "We therefore aim to attain high recall when evaluating against the ESSLLI set (since ideally all features in the norms should be extracted) but we are somewhat less concerned about achieving high precision (since extracted features that are not in the norms may still be correct, e.g. breathes air for TIGER).", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9991349577903748}, {"text": "ESSLLI", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9635699987411499}, {"text": "precision", "start_pos": 196, "end_pos": 205, "type": "METRIC", "confidence": 0.9978827834129333}]}, {"text": "To evaluate the ability of our model to generate such novel features, we also conducted a manual evaluation of the highest-ranked extracted features that did not appear in the norms.: Results when matching on features only.", "labels": [], "entities": []}, {"text": "Previous large-scale models of feature extraction have been evaluated on pairs rather than triples e.g..", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7227643579244614}]}, {"text": "presents the results of our method when we evaluate using the featurehead term alone (i.e. in calculating precision and recall we disregard the relation verb and require only a match between the feature-head terms in the extracted triples and the recoded norms).", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9981526732444763}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9951849579811096}]}, {"text": "Results for six sets of extractions are presented.", "labels": [], "entities": []}, {"text": "The first set is the set of features extracted by the SVD baseline.", "labels": [], "entities": []}, {"text": "The second set of extracted triples consists of the full set of triples extracted by our method, prior to the reweighting stage.", "labels": [], "entities": []}, {"text": "\"Top 20 unweighted\" gives the results when all but the top 20 most frequently extracted triples for each concept are filtered out.", "labels": [], "entities": []}, {"text": "Note that the filtering criteria here is raw extraction frequency, without reweighting by conditional probabilities.", "labels": [], "entities": []}, {"text": "\"Top 20 (clustering type)\" are the corresponding results when the features are weighted by the conditional probability factors (derived from our three clustering methods) prior to filtering; that is, using the top 20 reranked features.", "labels": [], "entities": []}, {"text": "The effectiveness of using the semantic class-based analysis data in our method can be assessed by comparing the filtered results with and without feature weighting.", "labels": [], "entities": []}, {"text": "For the baseline implementation, the results are better when we use the smaller Wiki500 corpus compared to the larger Wiki110K corpus.", "labels": [], "entities": [{"text": "Wiki500 corpus", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.94764044880867}, {"text": "Wiki110K corpus", "start_pos": 118, "end_pos": 133, "type": "DATASET", "confidence": 0.8932132720947266}]}, {"text": "This is not surprising, since the smaller corpus contains only those articles which correspond to the concepts found in the norms.", "labels": [], "entities": []}, {"text": "This smaller corpus thus minimises noise due to phenomena such as word polysemy which are more apparent in the larger corpus.", "labels": [], "entities": []}, {"text": "The results for the baseline model and the unfiltered method are quite similar for the Wiki500 corpus, whilst the results for the unfiltered method using the Wiki110K corpus give the maximum recall achieved by our method; 89.4% of the features are extracted, although this figure is closely followed by that of the BNC at 88.1%.", "labels": [], "entities": [{"text": "Wiki500 corpus", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.9453184604644775}, {"text": "Wiki110K corpus", "start_pos": 158, "end_pos": 173, "type": "DATASET", "confidence": 0.9212256669998169}, {"text": "recall", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.9982252717018127}, {"text": "BNC", "start_pos": 315, "end_pos": 318, "type": "DATASET", "confidence": 0.8671196103096008}]}, {"text": "As the unfiltered method is deliberately greedy, a large number of features are being extracted and therefore precision is low.: Results for our best method when matching on features and relations.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9995697140693665}]}, {"text": "For the results of the filtered method, where all but the top 20 of features were discarded, we seethe benefit of reranking, with the reranked frequencies for all three clustering types yielding much higher precision and recall scores than the unweighted method.", "labels": [], "entities": [{"text": "precision", "start_pos": 207, "end_pos": 216, "type": "METRIC", "confidence": 0.9986119270324707}, {"text": "recall", "start_pos": 221, "end_pos": 227, "type": "METRIC", "confidence": 0.9973549842834473}]}, {"text": "Our best performance is achieved using the BNC and hierarchical clustering, where we obtain 19.4% precision and 38.9% recall.", "labels": [], "entities": [{"text": "BNC", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.6212769150733948}, {"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9995225667953491}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9991794228553772}]}, {"text": "Thus both general and encyclopedic corpus data prove useful for the task.", "labels": [], "entities": []}, {"text": "An interesting question is whether these two data types offer different, complementary feature types for the task.", "labels": [], "entities": []}, {"text": "We discuss this point further in section 3.3.", "labels": [], "entities": []}, {"text": "Using exactly the same gold standard, obtained precision of 23.9%.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9997852444648743}]}, {"text": "However, this result is not directly comparable with ours, since we define precision over the whole set of extracted features while Baroni et al. considered the top 10 extracted features only.", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9991438388824463}]}, {"text": "The innovation of our method is that it uses information about the GR-graph of the sentence to also extract the relation which appears in the path linking the concept and feature terms in the sentence, which is not possible in a purely co-occurrencebased model.", "labels": [], "entities": []}, {"text": "We therefore also evaluated the extracted triples using the full relation + feature-head pair (i.e. both the feature and the relation verb have to be correct).", "labels": [], "entities": []}, {"text": "The results for our best method are shown in.", "labels": [], "entities": []}, {"text": "Unsurprisingly, because this task is more difficult, precision and recall are reduced.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9996923208236694}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9996902942657471}]}, {"text": "However, since we enforce no constraints on what the relation maybe and since we do not have expanded synonym sets for our relations (as we do for our features) it is actually impressive to have both the exact relation verb and feature matching with the recoded norms almost one in every five times.", "labels": [], "entities": []}, {"text": "To our knowledge, our work is the first to try to compare extracted features to the full relation and feature norm parts of the triple.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table  3. The hierarchical clustering seems to be producing Hierarchical Clustering  Plant Parts  Materials  Activities  berry  cotton  annoying  bush  fibre  listening  core  nylon  music  plant  silk  showing  seed  spandex  looking", "labels": [], "entities": []}, {"text": " Table 4: Example members of feature clusters for hierar- chical clustering.", "labels": [], "entities": [{"text": "hierar- chical clustering", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6172437220811844}]}, {"text": " Table 5: P(F|C) for C \u2208 {Fruit/Veg, Apparel, Instru- ments} and F \u2208 {Plant Parts, Materials, Activities}", "labels": [], "entities": []}, {"text": " Table 6: Results when matching on features only.", "labels": [], "entities": []}, {"text": " Table 7: Results for our best method when matching on  features and relations.", "labels": [], "entities": []}]}