{"title": [{"text": "Phrase Based Decoding using a Discriminative Model", "labels": [], "entities": [{"text": "Phrase Based Decoding", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9230921069780985}]}], "abstractContent": [{"text": "In this paper, we present an approach to statistical machine translation that combines the power of a discriminative model (for training a model for Machine Translation), and the standard beam-search based decoding technique (for the translation of an input sentence).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.6776729822158813}, {"text": "Machine Translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.8379888236522675}]}, {"text": "A discriminative approach for learning lexical selection and reordering utilizes a large set of feature functions (thereby providing the power to incorporate greater contextual and linguistic information), which leads to an effective training of these models.", "labels": [], "entities": [{"text": "learning lexical selection and reordering", "start_pos": 30, "end_pos": 71, "type": "TASK", "confidence": 0.7625689625740051}]}, {"text": "This model is then used by the standard state-of-art Moses decoder (Koehn et al., 2007) for the translation of an input sentence.", "labels": [], "entities": [{"text": "translation of an input sentence", "start_pos": 96, "end_pos": 128, "type": "TASK", "confidence": 0.8583900094032287}]}, {"text": "We conducted our experiments on Spanish-English language pair.", "labels": [], "entities": []}, {"text": "We used maximum entropy model in our experiments.", "labels": [], "entities": []}, {"text": "We show that the performance of our approach (using simple lexical features) is comparable to that of the state-of-art statistical MT system (Koehn et al., 2007).", "labels": [], "entities": [{"text": "MT", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.9175704717636108}]}, {"text": "When additional syntactic features (POS tags in this paper) are used, there is a boost in the performance which is likely to improve when richer syntactic features are incorporated in the model.", "labels": [], "entities": []}], "introductionContent": [{"text": "The popular approaches to machine translation use the generative IBM models for training).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8156678378582001}, {"text": "generative IBM", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.7511586844921112}]}, {"text": "The parameters for these models are learnt using the standard EM Algorithm.", "labels": [], "entities": [{"text": "EM Algorithm", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.8483410179615021}]}, {"text": "The parameters used in these models are extremely restrictive, that is, a simple, small and closed set of feature functions is used to represent the translation process.", "labels": [], "entities": []}, {"text": "Also, these feature functions are local and are word based.", "labels": [], "entities": []}, {"text": "In spite of these limitations, these models perform very well for the task of word-alignment because of the restricted search space.", "labels": [], "entities": []}, {"text": "However, they perform poorly during decoding (or translation) because of their limitations in the context of a much larger search space.", "labels": [], "entities": [{"text": "decoding (or translation)", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.6576011002063751}]}, {"text": "To handle the contextual information, phrasebased models were introduced (.", "labels": [], "entities": []}, {"text": "The phrase-based models use the word alignment information from the IBM models and train source-target phrase pairs for lexical selection (phrase-table) and distortions of source phrases (reordering-table).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7014100402593613}]}, {"text": "These models are still relatively local, as the target phrases are tightly associated with their corresponding source phrases.", "labels": [], "entities": []}, {"text": "In contrast to a phrase-based model, a discriminative model has the power to integrate much richer contextual information into the training model.", "labels": [], "entities": []}, {"text": "Contextual information is extremely useful in making lexical selections of higher quality, as illustrated by the models for Global Lexical Selection (.", "labels": [], "entities": [{"text": "Global Lexical Selection", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.7169245282808939}]}, {"text": "However, the limitation of global lexical selection models has been sentence construction.", "labels": [], "entities": [{"text": "sentence construction", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.8384475708007812}]}, {"text": "In global lexical selection models, lattice construction and scoring (LCS) is used for the purpose of sentence construction (.", "labels": [], "entities": [{"text": "lattice construction", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7731530070304871}, {"text": "sentence construction", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.721505880355835}]}, {"text": "In our work, we address this limitation of global lexical selection models by using an existing state-ofart decoder (  for the purpose of sentence construction.", "labels": [], "entities": [{"text": "sentence construction", "start_pos": 138, "end_pos": 159, "type": "TASK", "confidence": 0.7338889539241791}]}, {"text": "The translation model used by this decoder is derived from a discriminative model, instead of the usual phrase-table and reordering-table construction algorithms.", "labels": [], "entities": []}, {"text": "This allows us to use the effectiveness of an existing phrase-based decoder while retaining the advantages of the discriminative model.", "labels": [], "entities": []}, {"text": "In this paper, we compare the sentence construction accuracies of lattice construction and scoring approach (see section 4.1 for LCS Decoding) and the phrasebased decoding approach (see section 4.2).", "labels": [], "entities": [{"text": "lattice construction", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.726060301065445}]}, {"text": "Another advantage of using a discriminative approach to construct the phrase table and the reordering table is the flexibility it provides to incorporate linguistic knowledge in the form of additional feature functions.", "labels": [], "entities": []}, {"text": "In the past, factored phrase-based approaches for Machine Translation have allowed the use of linguistic feature functions.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8135631084442139}]}, {"text": "But, they are still bound by the locality of context, and definition of a fixed structure of dependencies between the factors ( . Furthermore, factored phrasebased approaches place constraints both on the type and number of factors that can be incorporated into the training.", "labels": [], "entities": []}, {"text": "In this paper, though we do not extensively test this aspect, we show that using syntactic feature functions does improve the performance of our approach, which is likely to improve when much richer syntactic feature functions (such as information about the parse structure) are incorporated in the model.", "labels": [], "entities": []}, {"text": "As the training model in a standard phrasebased system is relatively impoverished with respect to contextual/linguistic information, integration of the discriminative model in the form of phrase-table and reordering-table with the phrasebased decoder is highly desirable.", "labels": [], "entities": []}, {"text": "We propose to do this by defining sentence specific tables.", "labels": [], "entities": []}, {"text": "For example, given a source sentence s, the phrasetable contains all the possible phrase-pairs conditioned on the context of the source sentence s.", "labels": [], "entities": []}, {"text": "In this paper, the key contributions are, 1.", "labels": [], "entities": []}, {"text": "We combine a discriminative training model with a phrase-based decoder.", "labels": [], "entities": []}, {"text": "We obtained comparable results with the state-ofart phrase-based decoder.", "labels": [], "entities": []}, {"text": "2. We evaluate the performance of the lattice construction and scoring (LCS) approach to decoding.", "labels": [], "entities": []}, {"text": "We observed that even though the lexical accuracy obtained using LCS is high, the performance in terms of sentence construction is low when compared to phrasebased decoder.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9450138211250305}, {"text": "sentence construction", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.7398637533187866}]}, {"text": "3. We show that the incorporation of syntactic information (POS tags) in our discriminative model boosts the performance of translation.", "labels": [], "entities": []}, {"text": "In future, we plan to use richer syntactic feature functions (which the discriminative approach allows us to incorporate) to evaluate the approach.", "labels": [], "entities": []}, {"text": "The paper is organized in the following sections.", "labels": [], "entities": []}, {"text": "Section 2 presents the related work.", "labels": [], "entities": []}, {"text": "In section 3, we describe the training of our model.", "labels": [], "entities": []}, {"text": "In section 4, we present the decoding approaches (both LCS and phrase-based decoder).", "labels": [], "entities": []}, {"text": "We describe the data used in our experiments in section 5.", "labels": [], "entities": []}, {"text": "Section 6 consists of the experiments and results.", "labels": [], "entities": []}, {"text": "Finally we conclude the paper in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments were conducted on the SpanishEnglish language pair.", "labels": [], "entities": [{"text": "SpanishEnglish language pair", "start_pos": 38, "end_pos": 66, "type": "DATASET", "confidence": 0.8921773036321005}]}, {"text": "The latest version of the Europarl corpus(version-5) was used in this work.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.9924813210964203}]}, {"text": "A small set of 200K sentences was selected from the training set to conduct the experiments.", "labels": [], "entities": []}, {"text": "The test and development sets containing 2525 sentences and 2051 sentences respectively were used, without making any changes.", "labels": [], "entities": []}, {"text": "The output of our experiments was evaluated using two metrics, (1) BLEU (), and (2) Lexical Accuracy (LexAcc).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9990007281303406}, {"text": "Accuracy (LexAcc)", "start_pos": 92, "end_pos": 109, "type": "METRIC", "confidence": 0.8575854599475861}]}, {"text": "Lexical accuracy measures the similarity between the unordered bag of words in the reference sentence against the unordered bag of words in the hypothesized translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9122397303581238}]}, {"text": "Lexical accuracy is a measure of the fidelity of lexical transfer from the source to the target sentence, independent of the syntax of the target language).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9063090682029724}]}, {"text": "We report lexical accuracies to show the performance of LCS decoding in comparison with the baseline system.", "labels": [], "entities": []}, {"text": "We first present the results of the state-of-art phrase-based model (Moses) trained on a parallel corpus.", "labels": [], "entities": []}, {"text": "We treat this as our baseline.", "labels": [], "entities": []}, {"text": "The reordering feature used is msd-bidirectional, which allows for all possible reorderings over a specified distortion limit.", "labels": [], "entities": []}, {"text": "The baseline accuracies are shown in  We conduct two types of experiments to test our approach.", "labels": [], "entities": []}, {"text": "1. Experiments using lexical features (see section 6.1), and 2.", "labels": [], "entities": []}, {"text": "Experiments using syntactic features (see section 6.2).", "labels": [], "entities": []}, {"text": "In this section, we present results of our experiments that use only lexical features.", "labels": [], "entities": []}, {"text": "First, we measure the translation accuracy using LCS decoding.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9590015411376953}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.8633394241333008}]}, {"text": "On the development set, we explored the set of decoding parameters (as described in section 4.1) to compute the optimal parameter values.", "labels": [], "entities": []}, {"text": "The best lexical accuracy obtained on the development set is 0.4321 and the best BLEU score obtained is 0.0923 at a threshold of 0.17 and a permutation window size of value 3.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9780457019805908}, {"text": "BLEU score", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9828352928161621}]}, {"text": "The accuracies corresponding to a few other parameter values are shown in.", "labels": [], "entities": []}, {"text": "On the test data, we obtained a lexical accuracy of 0.4721 and a BLEU score of 0.1023.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9463819265365601}, {"text": "BLEU score", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9846934676170349}]}, {"text": "As we can observe, the BLEU score obtained using the LCS decoding technique is low when compared to the BLEU score of the state-of-art system.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9780877232551575}, {"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9993327260017395}]}, {"text": "However, the lexical accuracy is comparable   On the test set, we obtained a BLEU score of 0.1771.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9829210638999939}, {"text": "BLEU score", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.980731189250946}]}, {"text": "We observe that both the lexical accuracy and the BLEU scores obtained using the discriminative training model combined with the Moses decoder are comparable to the state-of-art results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9818890690803528}, {"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.999461829662323}]}, {"text": "The summary of the results obtained using three approaches and lexical feature functions is presented in.", "labels": [], "entities": []}, {"text": "In this section, we present the effect of incorporating syntactic features using our model on the: BLEU for different weight values using syntactic features shows the comparative performance of the model using syntactic as well as lexical features against the one with lexical features functions only.: Comparison between translation accuracies from models using syntactic and lexical features", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9993385672569275}]}], "tableCaptions": [{"text": " Table 3: Lexical Accuracies of Lattice-Output us- ing lexical features alone for various parameter  values", "labels": [], "entities": []}, {"text": " Table 4: BLEU for different weight values using  lexical features only", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999061644077301}]}, {"text": " Table 5: Translation accuracies using lexical fea- tures for different approaches", "labels": [], "entities": [{"text": "Translation accuracies", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8731486797332764}]}, {"text": " Table 6: BLEU for different weight values using  syntactic features", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991593360900879}]}, {"text": " Table 7: Comparison between translation accura- cies from models using syntactic and lexical fea- tures", "labels": [], "entities": []}]}