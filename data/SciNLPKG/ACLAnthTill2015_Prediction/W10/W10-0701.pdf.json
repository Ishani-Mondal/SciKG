{"title": [{"text": "Creating Speech and Language Data With Amazon's Mechanical Turk", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.7988602072000504}]}], "abstractContent": [{"text": "In this paper we give an introduction to using Amazon's Mechanical Turk crowdsourc-ing platform for the purpose of collecting data for human language technologies.", "labels": [], "entities": []}, {"text": "We survey the papers published in the NAACL-2010 Workshop.", "labels": [], "entities": [{"text": "NAACL-2010 Workshop", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.8875663876533508}]}, {"text": "24 researchers participated in the workshop's shared task to create data for speech and language applications with $100.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper gives an overview of the NAACL-2010 Workshop on Creating Speech and Language Data With Amazon's Mechanical Turk.", "labels": [], "entities": []}, {"text": "A number of recent papers have evaluated the effectiveness of using Mechanical Turk to create annotated data for natural language processing applications.", "labels": [], "entities": []}, {"text": "The low cost, scalable workforce available through Mechanical Turk (MTurk) and other crowdsourcing sites opens new possibilities for annotating speech and text, and has the potential to dramatically change how we create data for human language technologies.", "labels": [], "entities": []}, {"text": "Open questions include: What kind of research is possible when the cost of creating annotated training data is dramatically reduced?", "labels": [], "entities": []}, {"text": "What new tasks should we try to solve if we do not limit ourselves to reusing existing training and test sets?", "labels": [], "entities": []}, {"text": "Can complex annotation be done by untrained annotators?", "labels": [], "entities": []}, {"text": "How can we ensure high quality annotations from crowdsourced contributors?", "labels": [], "entities": []}, {"text": "To begin addressing these questions, we organized an open-ended $100 shared task.", "labels": [], "entities": []}, {"text": "Researchers were given $100 of credit on Amazon Mechanical Turk to spend on an annotation task of their choosing.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.9537846644719442}]}, {"text": "They were required to write a short paper describing their experience, and to distribute the data that they created.", "labels": [], "entities": []}, {"text": "They were encouraged to address the following questions: How did you convey the task in terms that were simple enough for nonexperts to understand?", "labels": [], "entities": []}, {"text": "Were non-experts as good as experts?", "labels": [], "entities": []}, {"text": "What did you do to ensure quality?", "labels": [], "entities": [{"text": "quality", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9545979499816895}]}, {"text": "How quickly did the data get annotated?", "labels": [], "entities": []}, {"text": "What is the cost per label?", "labels": [], "entities": []}, {"text": "Researchers submitted a 1 page proposal to the workshop organizers that described their intended experiments and expected outcomes.", "labels": [], "entities": []}, {"text": "The organizers selected proposals based on merit, and awarded $100 credits that were generously provided by Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 108, "end_pos": 130, "type": "DATASET", "confidence": 0.9438216884930929}]}, {"text": "In total, 35 credits were awarded to researchers.", "labels": [], "entities": []}, {"text": "Shared task participants were given 10 days to run experiments between the distribution of the credit and the initial submission deadline.", "labels": [], "entities": []}, {"text": "30 papers were submitted to the shared task track, of which 24 were accepted.", "labels": [], "entities": []}, {"text": "14 papers were submitted to the general track of which 10 were accepted, giving a 77% acceptance rate and a total of 34 papers.", "labels": [], "entities": [{"text": "acceptance rate", "start_pos": 86, "end_pos": 101, "type": "METRIC", "confidence": 0.984330028295517}]}, {"text": "Shared task participants were required to provide the data collected as part of their experiments.", "labels": [], "entities": []}, {"text": "All of the shared task data is available on the workshop website.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}