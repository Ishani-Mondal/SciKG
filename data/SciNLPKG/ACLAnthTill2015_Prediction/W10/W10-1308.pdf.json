{"title": [{"text": "A Multimodal Vocabulary for Augmentative and Alternative Communi- cation from Sound/Image Label Datasets", "labels": [], "entities": [{"text": "Sound/Image Label Datasets", "start_pos": 78, "end_pos": 104, "type": "DATASET", "confidence": 0.5616648495197296}]}], "abstractContent": [{"text": "Existing Augmentative and Alternative Communication vocabularies assign multimodal stimuli to words with multiple meanings.", "labels": [], "entities": []}, {"text": "The ambiguity hampers the vocabulary effectiveness when used by people with language disabilities.", "labels": [], "entities": []}, {"text": "For example, the noun \"a missing letter\" may refer to a character or a written message, and each corresponds to a different picture.", "labels": [], "entities": []}, {"text": "A vocabulary with images and sounds unambiguously linked to words can better eliminate misunderstanding and assist communication for people with language disorders.", "labels": [], "entities": []}, {"text": "We explore anew approach of creating such a vocabulary via automatically assigning semantically unambiguous groups of synonyms to sound and image labels.", "labels": [], "entities": []}, {"text": "We propose an un-supervised word sense disambiguation (WSD) voting algorithm, which combines different semantic relatedness measures.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD) voting", "start_pos": 28, "end_pos": 66, "type": "TASK", "confidence": 0.7948591836861202}]}, {"text": "Our voting algorithm achieved over 80% accuracy with a sound label dataset, which significantly out-performs WSD with individual measures.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9995421171188354}, {"text": "WSD", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.5719119310379028}]}, {"text": "We also explore the use of human judgments of evocation between members of concept pairs, in the label disambiguation task.", "labels": [], "entities": [{"text": "label disambiguation task", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.8133584062258402}]}, {"text": "Results show that evocation achieves similar performance to most of the existing relatedness measures.", "labels": [], "entities": []}], "introductionContent": [{"text": "In natural languages, a word form may refer to different meanings.", "labels": [], "entities": []}, {"text": "For instance, the word \"fly\" means \"travel through the air\" in context like \"fly to New York,\" while it refers to an insect in the phrase \"a fly on the trashcan.\"", "labels": [], "entities": []}, {"text": "Speakers determine the appropriate sense of a polysemous word based on the context.", "labels": [], "entities": []}, {"text": "However, people with language disorders and access/retrieval problems, may have great difficulty in understanding words individually or in a context.", "labels": [], "entities": []}, {"text": "To overcome such language barriers, visual and auditory representations are introduced to help illustrate concepts ()(.", "labels": [], "entities": []}, {"text": "For example, a person with a language disability can tell the word \"fly\" refers to \"travel through the air\" when he sees a plane in the image (rather than an insect); likewise he can distinguish the meaning of \"fly\" given the plane engine sound vs. the insect buzzing sound.", "labels": [], "entities": []}, {"text": "This approach has been employed in Augmentative and Alternative Communication (AAC), in the form of multimodal vocabularies in assistive devices ()(.", "labels": [], "entities": [{"text": "Augmentative and Alternative Communication (AAC)", "start_pos": 35, "end_pos": 83, "type": "TASK", "confidence": 0.7784553340503148}]}, {"text": "However, current AAC vocabularies assign visual stimuli to words instead of specific meanings, and thus bring in ambiguity when a user with language disability tries to comprehend and communicate a concept.", "labels": [], "entities": [{"text": "AAC vocabularies assign visual stimuli to words", "start_pos": 17, "end_pos": 64, "type": "TASK", "confidence": 0.8198571971484593}]}, {"text": "For example, for the word \"fly,\" Lingraphica only has an icon showing a plane and a flock of birds flying.", "labels": [], "entities": []}, {"text": "Confusion arises when a sentence like \"I want to kill the fly (the insect)\" is explained using the airplane/bird icon.", "labels": [], "entities": []}, {"text": "Similarly, it will lead to miscommunication if the sound of keys jingling is used to express \"a key is missing\" when the person intends to refer to a key on the keyboard.", "labels": [], "entities": []}, {"text": "People with language impairment are relying on the AAC vocabularies for language access, and any ambiguity may result in communication failure.", "labels": [], "entities": [{"text": "AAC vocabularies", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.8712108135223389}, {"text": "language access", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.6836384683847427}]}, {"text": "To address this problem, we propose building a semantic multimodal AAC vocabulary with visual and auditory representations expressing concepts rather than words), as the backbone of the language assistant system for people with aphasia ().", "labels": [], "entities": []}, {"text": "Our work is exploratory with the following innovations: 1) we target the insufficiency of current assistive vocabularies by resolving ambiguity; 2) we enrich concept inventory and connect concepts through language, environmental sounds, and images (little research has looked into conveying concepts through natural nonspeech sounds); and 3) our vocabulary has a dynamic scalable semantic network structure rather than simply grouping words into categories as conventional assistive devices do.", "labels": [], "entities": []}, {"text": "One intuitive way to build a disambiguated multimodal vocabulary is to manually assign meanings to each word in the existing vocabulary.", "labels": [], "entities": []}, {"text": "However, the task is time consuming with poor scalabilityno new multimedia representations are generated for concepts that are missing in the vocabulary.", "labels": [], "entities": []}, {"text": "ImageNet () was constructed by people verifying the assignment of web images to given synonym sets (synsets).", "labels": [], "entities": []}, {"text": "ImageNet has over nine million images linked to about 15 thousands noun synsets in WordNet.", "labels": [], "entities": [{"text": "ImageNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9199538230895996}, {"text": "WordNet", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9742115139961243}]}, {"text": "Despite the huge human effort, ImageNet, with the goal of creating a computer vision database, does not yet include all the most commonly used words across different parts of speech.", "labels": [], "entities": []}, {"text": "It is not yet suitable fora language support application.", "labels": [], "entities": []}, {"text": "We explore anew approach for generating a vocabulary with concept to sound/image associations, that is, conducting word sense disambiguation (WSD) techniques used in Natural Language Processing on sound/image label datasets.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 115, "end_pos": 146, "type": "TASK", "confidence": 0.7510865976413091}]}, {"text": "For example, the labels \"car, drive, fast\" for the sound \"car -passing.wav\" are assigned to synsets \"car: a motor vehicle,\" \"drive: operate or control a vehicle,\" and \"fast: quickly or rapidly\" via WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 198, "end_pos": 201, "type": "DATASET", "confidence": 0.881208062171936}]}, {"text": "It means the sound \"car -passing.wav\" can be used to depict those concepts.", "labels": [], "entities": []}, {"text": "This approach is viable because the words in the sound/image labels were shown to evoke one another based on the auditory/visual content, and their meanings can be identified by considering all the tags generated fora given sound or image as a context.", "labels": [], "entities": []}, {"text": "With the availability of large sound/image label datasets, the vocabulary created from WSD can be easily expanded.", "labels": [], "entities": []}, {"text": "A variety of WSD methods (e.g. knowledgebased methods, unsupervised methods (Lin, 1997), semi-supervised methods, and supervised methods () were developed and evaluated with corpus data and other text documents like webpages.", "labels": [], "entities": [{"text": "WSD", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9678014516830444}]}, {"text": "Compared to the text data that WSD methods work with, labels for sounds and images have unique characteristics.", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9422889947891235}]}, {"text": "The labels area bag of words related to the visual/auditory content; there is no syntactic or part of speech information, nor are the words necessarily contextual neighbors.", "labels": [], "entities": []}, {"text": "For example, contexts suggest landscape senses for the word pair \"bank\" and \"water\", whereas in an image, a person may drink water inside a bank building.", "labels": [], "entities": []}, {"text": "Furthermore, few annotated image or sound label datasets are available, making it hard to apply supervised or semi-supervised WSD methods.", "labels": [], "entities": [{"text": "WSD", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.9262075424194336}]}, {"text": "To efficiently and effectively create a disambiguated multimodal vocabulary, we need to achieve two goals.", "labels": [], "entities": []}, {"text": "First, optimize the accuracy of the WSD algorithm to minimize the work required for manual checking and correction afterwards.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9994841814041138}, {"text": "WSD", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.8584607243537903}]}, {"text": "Second, construct a semantic network across different parts of speech, and thus explore linking semantic relatedness measures that can capture aspects different from existing ones.", "labels": [], "entities": []}, {"text": "In this paper, we target the first goal by proposing an unsupervised sense disam- biguation algorithm combining a variety of semantic relatedness measures.", "labels": [], "entities": []}, {"text": "We chose an unsupervised method because of the lack of a large manually annotated gold standard.", "labels": [], "entities": []}, {"text": "The measurecombined voting algorithm presented here draws advantages from different semantic relatedness measures and has them vote for the best-fitting sense to assign to a label.", "labels": [], "entities": []}, {"text": "Evaluation shows that the voting algorithm significantly exceeds WSD with each individual measure.", "labels": [], "entities": [{"text": "WSD", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9870336055755615}]}, {"text": "To approach the second goal, we proposed and tested a semantic relatedness measure called evocation) in disambiguation of sound/image labels.", "labels": [], "entities": [{"text": "disambiguation of sound/image labels", "start_pos": 104, "end_pos": 140, "type": "TASK", "confidence": 0.8263507088025411}]}, {"text": "Evocation measures human judgements of relatedness between a directed concepts pair.", "labels": [], "entities": []}, {"text": "It provides cross parts of speech evocativeness information which supplements most of the knowledge-based semantic relatedness measures.", "labels": [], "entities": []}, {"text": "Evaluation results showed that the performance of WSD with evocation is no worse than most of the relatedness measures that we applied, despite the relatively small size of the current evocation dataset.", "labels": [], "entities": [{"text": "WSD", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9409259557723999}]}], "datasetContent": [{"text": "Our ultimate goal is to create an AAC vocabulary of associations between environmental sounds and images and groups of synonymous words that are relevant to the content.", "labels": [], "entities": [{"text": "AAC vocabulary of associations between environmental sounds and images", "start_pos": 34, "end_pos": 104, "type": "TASK", "confidence": 0.7591349416308932}]}, {"text": "We are working with two datasets of human labels for multimedia data, SoundNet and the Peekaboom dataset.", "labels": [], "entities": [{"text": "Peekaboom dataset", "start_pos": 87, "end_pos": 104, "type": "DATASET", "confidence": 0.9716857671737671}]}, {"text": "The SoundNet Dataset consists of 327 environmental \"soundnails\" (5-second audio clips) each with semantic labels collected from participants via a large scale Amazon Mechanical Turk (AMT) study.", "labels": [], "entities": [{"text": "SoundNet Dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.7753142714500427}]}, {"text": "The soundnails cover a wide range of auditory scenes, from vehicle (e.g. car starting), mechanical tools (e.g. handsaw) and electrical devices (e.g. TV), to natural phenomena (e.g. rain), animals (e.g. a dog barking), and human sounds (e.g. a baby crying).", "labels": [], "entities": []}, {"text": "In the AMT study, participants were asked to generate tags for each soundnail labeling its source, possible location, and actions involved in making the sound.", "labels": [], "entities": [{"text": "AMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.8583658933639526}]}, {"text": "Each soundnail was labeled by over 100 people.", "labels": [], "entities": []}, {"text": "The tags were clustered into meaning units that SoundNet refers to as \"sense sets.\"", "labels": [], "entities": []}, {"text": "A sense set includes a set of words with similar meanings.", "labels": [], "entities": []}, {"text": "The word in bold is was judged by SoundNet to be the best representative of the sense set, and other words, possibly belonging to different parts of speech are included in the curly brackets enclosing the sense sets.", "labels": [], "entities": []}, {"text": "In this experiment, only sense sets (labels) that were generated by at least 25% of the labelers were used.", "labels": [], "entities": []}, {"text": "In our disambiguation experiment, two kinds of contexts were explored.", "labels": [], "entities": []}, {"text": "In the Context 1 scheme, each label is treated separately: all its members plus the representatives of the other sense sets are considered.", "labels": [], "entities": []}, {"text": "Take the soundnail \"bag, zipOpen\" as an example.", "labels": [], "entities": []}, {"text": "The context for disambiguating label (a) \"zipper\" {zipper, zip up, zip, unzip} is: zipper, zip up, zip, unzip, bag, house, clothes.", "labels": [], "entities": []}, {"text": "The context for label (d) \"clothes\" {clothes, jacket, coat, pants, jeans, dress, garment} is: clothes, jacket, coat, pants, jeans, dress, garment, zipper, bag, house.", "labels": [], "entities": []}, {"text": "In the Context 1 scheme, all representative words will be disambiguated multiple times.", "labels": [], "entities": []}, {"text": "The final result will be the synset that gets the most votes.", "labels": [], "entities": []}, {"text": "In the Context 2 scheme, as for the image dataset described below, all members from each sense set are put together to create the context, and each word is disambiguated only once.", "labels": [], "entities": []}, {"text": "The ESP Game Dataset) contains a large number of web images and human labels produced via an online game.", "labels": [], "entities": [{"text": "ESP Game Dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9112773935000101}]}, {"text": "For example, an image of a glass of hard liquor is labeled \"full, shot, alcohol, clear, drink, glass, beverage.\"", "labels": [], "entities": []}, {"text": "The Peekaboom Game) is the successor of the ESP Game.", "labels": [], "entities": [{"text": "ESP Game", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9020010530948639}]}, {"text": "In our experiment, part of the Peekaboom Dataset (3,086 images) was used.", "labels": [], "entities": [{"text": "Peekaboom Dataset", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9592967629432678}]}, {"text": "For each image, all the labels together form the context for sense disambiguation.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.7569036483764648}]}, {"text": "The Peekaboom labels are noisier than the SoundNet labels for several reasons.", "labels": [], "entities": [{"text": "Peekaboom labels", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.916523814201355}]}, {"text": "First, random objects may appear in a picture and thus be included in the labels.", "labels": [], "entities": []}, {"text": "For example, an image is labeled \"computer, shark\" because there is a shark picture on the computer screen.", "labels": [], "entities": []}, {"text": "Second, texts in the images are often included in the labels.", "labels": [], "entities": []}, {"text": "For example, the word \"green\" is one of the labels for an image with a street sign \"Green St.\" Third, the Peekaboom labels are not stemmed, which adds another layer of ambiguity.", "labels": [], "entities": [{"text": "Peekaboom labels", "start_pos": 106, "end_pos": 122, "type": "DATASET", "confidence": 0.9029131531715393}]}, {"text": "For example, the labels \"bridge, building\" could refer to a building event or to a built entity.", "labels": [], "entities": []}, {"text": "In the experiment, all labels for an image are used in their unstemmed form to construct the context for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.8249887228012085}]}, {"text": "The evaluation of WSD with evocation and the measure-combined voting algorithm was carried out primarily on the SoundNet label dataset because of the availability of ground truth data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.8881769776344299}, {"text": "SoundNet label dataset", "start_pos": 112, "end_pos": 134, "type": "DATASET", "confidence": 0.9391273061434428}]}, {"text": "SoundNet provides manual annotation for 1,553 different words for 327 soundnails (e.g. the word \"road\" appears in 41 sounds).", "labels": [], "entities": [{"text": "SoundNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9287139773368835}]}, {"text": "The accuracy rate (precision) was computed for each WSD method.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.989188015460968}, {"text": "precision)", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9716045260429382}, {"text": "WSD", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8080130219459534}]}, {"text": "The sound level accuracy of a WSD k is the average percentage of correct sense assignments over the 327 sounds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.7393772602081299}]}, {"text": "The word level accuracy is the mean over 1553 distinctive words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9023803472518921}]}, {"text": "Accuracy rates of different measures at both level accepted the null hypothesis in homogeneity test.", "labels": [], "entities": []}], "tableCaptions": []}