{"title": [{"text": "Improved Unsupervised POS Induction Using Intrinsic Clustering Quality and a Zipfian Constraint", "labels": [], "entities": [{"text": "POS Induction", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.7317992448806763}]}], "abstractContent": [{"text": "Modern unsupervised POS taggers usually apply an optimization procedure to a non-convex function, and tend to converge to local maxima that are sensitive to starting conditions.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.8431519865989685}]}, {"text": "The quality of the tagging induced by such algorithms is thus highly variable, and researchers report average results over several random initial-izations.", "labels": [], "entities": []}, {"text": "Consequently, applications are not guaranteed to use an induced tagging of the quality reported for the algorithm.", "labels": [], "entities": []}, {"text": "In this paper we address this issue using an unsupervised test for intrinsic clustering quality.", "labels": [], "entities": []}, {"text": "We run abase tagger with different random initializations, and select the best tagging using the quality test.", "labels": [], "entities": []}, {"text": "As abase tagger, we modify a leading un-supervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test.", "labels": [], "entities": []}, {"text": "We show that the correlation between our quality test and gold standard-based tagging quality measures is high.", "labels": [], "entities": []}, {"text": "Our results are better inmost evaluation measures than all results reported in the literature for this task, and are always better than the Clark average results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised part-of-speech (POS) induction is of major theoretical and practical importance.", "labels": [], "entities": [{"text": "Unsupervised part-of-speech (POS) induction", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.5788741707801819}]}, {"text": "It counters the arbitrary nature of manually designed tag sets, and avoids manual corpus annotation costs.", "labels": [], "entities": []}, {"text": "The task enjoys considerable current interest in the research community (see Section 3).", "labels": [], "entities": []}, {"text": "Most unsupervised POS tagging algorithms apply an optimization procedure to a non-convex function, and tend to converge to local maxima that strongly depend on the algorithm's (usually random) initialization.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.8550013303756714}]}, {"text": "The quality of the taggings produced by different initializations varies substantially.", "labels": [], "entities": []}, {"text": "demonstrates this phenomenon fora leading POS induction algorithm.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.7345641851425171}]}, {"text": "The absolute variability of the induced tagging quality is 10-15%, which is around 20% of the mean.", "labels": [], "entities": []}, {"text": "Strong variability has also been reported by other authors (Section 3).", "labels": [], "entities": []}, {"text": "The common practice in the literature is to report mean results over several random initializations of the algorithm (e.g.).", "labels": [], "entities": []}, {"text": "This means that applications using the induced tagging are not guaranteed to use a tagging of the reported quality.", "labels": [], "entities": []}, {"text": "In this paper we address this issue using an unsupervised test for intrinsic clustering quality.", "labels": [], "entities": []}, {"text": "We present a quality-based algorithmic family Q.", "labels": [], "entities": []}, {"text": "Each of its concrete member algorithms Q(B) runs abase tagger B with different random initializations, and selects the best tagging according the quality test.", "labels": [], "entities": []}, {"text": "If the testis highly positively correlated with external tagging quality measures (e.g., those based on gold standard tagging), Q(B) will produce better results than B with high probability.", "labels": [], "entities": []}, {"text": "We experiment with two base taggers, Clark's original tagger (CT) and Zipf Constrained Clark (ZCC).", "labels": [], "entities": [{"text": "Clark's original tagger (CT", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.5886706560850143}]}, {"text": "ZCC is a novel algorithm of interest in its own right, which is especially suitable as abase tagger in the family Q.", "labels": [], "entities": []}, {"text": "ZCC is a modification of Clark's algorithm in which the distribution of the number of word types in a cluster (cluster type size) is constrained to be Zipfian.", "labels": [], "entities": []}, {"text": "This property holds for natural languages, hence we can expect a higher correlation between ZCC and an accepted unsupervised quality measure, perplexity.", "labels": [], "entities": []}, {"text": "We show that for both base taggers, the correlation between our unsupervised quality test and gold standard based tagging quality measures is high.", "labels": [], "entities": []}, {"text": "For the English WSJ corpus, the Q(ZCC) Figure 1: Distribution of the quality of the taggings produced in 100 runs of the Clark POS induction algorithm (with different random initializations) for sections 2-21 of the WSJ corpus.", "labels": [], "entities": [{"text": "English WSJ corpus", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.8864946762720743}, {"text": "WSJ corpus", "start_pos": 216, "end_pos": 226, "type": "DATASET", "confidence": 0.937654972076416}]}, {"text": "All graphs are 10-bin histograms presenting the number of runs (y-axis) with the corresponding quality (x-axis).", "labels": [], "entities": []}, {"text": "Quality is evaluated with 4 clustering evaluation measures: V, NVI, greedy m-1 mapping and greedy 1-1 mapping.", "labels": [], "entities": []}, {"text": "The quality of the induced tagging varies considerably.", "labels": [], "entities": []}, {"text": "algorithm gives better results than CT with probability 82-100% (depending on the external quality measure used).", "labels": [], "entities": []}, {"text": "Q(CT) is shown to be better than the original CT algorithm as well.", "labels": [], "entities": [{"text": "Q(CT)", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.798304095864296}]}, {"text": "Our results are better inmost evaluation measures than all previous results reported in the literature for this task, and are always better than Clark's average results.", "labels": [], "entities": []}, {"text": "Section 2 describes the ZCC algorithm and our quality measure.", "labels": [], "entities": []}, {"text": "Section 3 discusses previous work.", "labels": [], "entities": []}, {"text": "Section 4 presents the experimental setup and Section 5 reports our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the English WSJ PennTreebank corpus in our experiments.", "labels": [], "entities": [{"text": "English WSJ PennTreebank corpus", "start_pos": 12, "end_pos": 43, "type": "DATASET", "confidence": 0.7417749613523483}]}, {"text": "We induced POS tags for sections 2-21 (43K word types, 950K word instances of which 832K (87.6%) are not punctuation marks), using Q(ZCC), Q(CT), and CT.", "labels": [], "entities": [{"text": "CT", "start_pos": 150, "end_pos": 152, "type": "METRIC", "confidence": 0.9788485765457153}]}, {"text": "For the unsupervised quality test, we trained the bigram class-based language model on sections 2-21 with the induced clusters, and computed its perplexity on section 23.", "labels": [], "entities": []}, {"text": "In Q(ZCC) and Q(CT), the base taggers were run a 100 times each, using different random initializations.", "labels": [], "entities": []}, {"text": "In each run we induce 13 clusters, since this is the number of unique POS tags required to cover 98% of the word types in WSJ . Some previous work (e.g.,) also induced 13 non-punctuation tags.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 122, "end_pos": 125, "type": "DATASET", "confidence": 0.8810069561004639}]}, {"text": "We compare the results of our algorithm to those of the original Clark algorithm . The induced clusters are evaluated against two POS tag sets: one is the full set of WSJ POS tags, and the other consists of the non-punctuation tags of the first set.", "labels": [], "entities": [{"text": "WSJ POS tags", "start_pos": 167, "end_pos": 179, "type": "DATASET", "confidence": 0.8078850309054056}]}, {"text": "Punctuation marks constitute a sizeable volume of corpus tokens and are easy to cluster correctly.", "labels": [], "entities": []}, {"text": "Hence, evaluting against the full tag set that includes punctuation artificially increases the quality of the reported results, which is why we report results for the non-punctuation tag set.", "labels": [], "entities": []}, {"text": "However, to be able to directly compare with previous work, we also report results for the full WSJ POS tag set.", "labels": [], "entities": [{"text": "WSJ POS tag set", "start_pos": 96, "end_pos": 111, "type": "DATASET", "confidence": 0.8794426918029785}]}, {"text": "We do so by assigning a singleton cluster to each punctuation mark (in addition to the 13 clusters).", "labels": [], "entities": []}, {"text": "This simple heuristic yields very high performance on punctuation, scoring (when all other terminals are assumed perfect tagging) 99.6% in 1-to-1 accuracy.", "labels": [], "entities": [{"text": "scoring", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9685419797897339}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9825814962387085}]}, {"text": "In addition to comparing the different algorithms, we compare the correlation between our tagging quality test and external clustering quality for both the original CT algorithm and our ZCC algorithm.", "labels": [], "entities": []}, {"text": "The induced POS tags have arbitrary names.", "labels": [], "entities": []}, {"text": "To evaluate them against a manually annotated corpus, a proper correspondence with the gold standard POS tags should be established.", "labels": [], "entities": [{"text": "POS tags", "start_pos": 101, "end_pos": 109, "type": "DATASET", "confidence": 0.7991807758808136}]}, {"text": "Many evaluation measures for unsupervised clustering against gold standard exist.", "labels": [], "entities": []}, {"text": "Here we use measures from two well accepted families: mapping based and information theoretic (IT) based.", "labels": [], "entities": []}, {"text": "For a recent discussion on this subject see.", "labels": [], "entities": []}, {"text": "The mapping based measures are accuracy with greedy many-to-1 (M-1) and with greedy 1-to-1 (1-1) mappings of the induced to the gold labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9994563460350037}]}, {"text": "In the former mapping, two induced clusters can be mapped to the same gold standard cluster, while in the latter mapping each and every induced cluster is assigned a unique gold cluster.", "labels": [], "entities": []}, {"text": "After each induced label is mapped to a gold label, tagging accuracy is computed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9853672981262207}]}, {"text": "Accuracy is defined to be the number of correctly tagged words in the corpus divided by the total number of words in the corpus.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.988654613494873}]}, {"text": "The IT based measures we use are V and NVI.", "labels": [], "entities": [{"text": "V", "start_pos": 33, "end_pos": 34, "type": "METRIC", "confidence": 0.9846523404121399}]}, {"text": "The latter is a normalization of the VI measure.", "labels": [], "entities": [{"text": "VI measure", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9633440971374512}]}, {"text": "VI and NVI induce the same order over clusterings but NVI values for good clusterings lie in.", "labels": [], "entities": []}, {"text": "For V, the higher the score, the better the clustering.", "labels": [], "entities": []}, {"text": "For NVI lower scores imply improved clustering quality.", "labels": [], "entities": []}, {"text": "We use e as the base of the logarithm.", "labels": [], "entities": []}, {"text": "Evaluation of the Quality Test.", "labels": [], "entities": [{"text": "Quality", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9260388612747192}]}, {"text": "To measure the correlation between the score produced by the tagging quality test and the external quality of a tagging, we use two well accepted measures: Spearman's rank correlation coefficient and Kendall Tau ().", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 156, "end_pos": 195, "type": "METRIC", "confidence": 0.6074432671070099}, {"text": "Kendall Tau ()", "start_pos": 200, "end_pos": 214, "type": "METRIC", "confidence": 0.9165758093198141}]}, {"text": "These measure the correlation between two sorted lists.", "labels": [], "entities": []}, {"text": "For the computation of these measures, we rank the clusterings once according to the identification criterion and once according to the external quality measure.", "labels": [], "entities": []}, {"text": "The measures are given by the equations: (6) kendall \u2212 tau = 2(nc\u2212n d ) r(r\u22121) where r is the number of runs (100 in our case), n c and n dare the numbers of concordant and discordant pairs respectively 6 and d i is the absolute value of the difference between the ranks of item i.", "labels": [], "entities": []}, {"text": "The two measures have the properties that a perfect agreement between rankings results in a score of 1, a perfect disagreement results in a score of \u22121, completely independent rankings have the value of 0 on the average, the range of values is between \u22121 and 1, and increasing values imply increasing agreement between the rankings.", "labels": [], "entities": []}, {"text": "For a discussion see).", "labels": [], "entities": []}, {"text": "presents the results of the Q(ZCC) and Q(CT) algorithms, which are both better than those of the original Clark tagger CT.", "labels": [], "entities": [{"text": "Clark tagger CT", "start_pos": 106, "end_pos": 121, "type": "DATASET", "confidence": 0.6803956627845764}]}, {"text": "The Q algorithms provide a tagging that is better than that produced by CT in 82-100% (Q(ZCC)) and 75-100% (Q(CT)) of the cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Average performance of ZCC compared  with CT (results presented without punctuation).  Presented are mean, mode (see text for its calcu- lation), and standard deviation (std). CT mean re- sults are slightly better, and both algorithms have  about the same standard deviation. ZCC sacrifices  a small amount of mean quality for a good corre- lation with our quality test, which allows Q(ZCC)  to be much better than the mean of CT and most  of its runs.", "labels": [], "entities": [{"text": "mode", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9505793452262878}, {"text": "standard", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9706335067749023}, {"text": "CT mean re- sults", "start_pos": 186, "end_pos": 203, "type": "METRIC", "confidence": 0.8074320673942565}]}]}