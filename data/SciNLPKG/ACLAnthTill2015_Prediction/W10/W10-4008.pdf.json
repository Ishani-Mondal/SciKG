{"title": [{"text": "A Voting Mechanism for Named Entity Translation in English- Chinese Question Answering", "labels": [], "entities": [{"text": "Named Entity Translation in English- Chinese Question Answering", "start_pos": 23, "end_pos": 86, "type": "TASK", "confidence": 0.6265290147728391}]}], "abstractContent": [{"text": "In this paper, we describe a voting mechanism for accurate named entity (NE) translation in English-Chinese question answering (QA).", "labels": [], "entities": [{"text": "accurate named entity (NE) translation", "start_pos": 50, "end_pos": 88, "type": "TASK", "confidence": 0.5829153997557504}, {"text": "question answering (QA)", "start_pos": 108, "end_pos": 131, "type": "TASK", "confidence": 0.8361074090003967}]}, {"text": "This mechanism involves translations from three different sources: machine translation, online encyclopaedia, and web documents.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7063123732805252}]}, {"text": "The translation with the highest number of votes is selected.", "labels": [], "entities": []}, {"text": "We evaluated this approach using test collection, topics and assessment results from the NTCIR-8 evaluation forum.", "labels": [], "entities": [{"text": "NTCIR-8 evaluation forum", "start_pos": 89, "end_pos": 113, "type": "DATASET", "confidence": 0.9618743062019348}]}, {"text": "This mechanism achieved 95% accuracy in NEs translation and 0.3756 MAP in English-Chinese cross-lingual information retrieval of QA.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9996028542518616}, {"text": "NEs translation", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.9693814516067505}, {"text": "MAP", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9955691695213318}]}], "introductionContent": [{"text": "Nowadays, it is easy for people to access multi-lingual information on the Internet.", "labels": [], "entities": []}, {"text": "Key term searching on an information retrieval (IR) system is common for information lookup.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.8076270580291748}]}, {"text": "However, when people try to look for answers in a different language, it is more natural and comfortable for them to provide the IR system with questions in their own natural languages (e.g. looking fora Chinese answer with an English question: \"what is Taiji\"?).", "labels": [], "entities": []}, {"text": "Crosslingual question answering (CLQA) tries to satisfy such needs by directly finding the correct answer for the question in a different language.", "labels": [], "entities": [{"text": "Crosslingual question answering (CLQA)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8027593245108923}]}, {"text": "In order to return a cross-lingual answer, a CLQA system needs to understand the question, choose proper query terms, and then extract correct answers.", "labels": [], "entities": []}, {"text": "Cross-lingual information retrieval (CLIR) plays a very important role in this process because the relevancy of retrieved documents (or passages) affects the accuracy of the answers.", "labels": [], "entities": [{"text": "Cross-lingual information retrieval (CLIR)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7774115353822708}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9945017099380493}]}, {"text": "A simple approach to achieving CLIR is to translate the query into the language of the target documents and then to use a monolingual IR system to locate the relevant ones.", "labels": [], "entities": []}, {"text": "However, it is essential but difficult to translate the question correctly.", "labels": [], "entities": []}, {"text": "Currently, machine translation (MT) can achieve very high accuracy when translating general text.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.8399401783943177}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9962002635002136}]}, {"text": "However, the complex phrases and possible ambiguities present in a question challenge general purpose MT approaches.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9864659905433655}]}, {"text": "Out-of-vocabulary (OOV) terms are particularly problematic.", "labels": [], "entities": []}, {"text": "So the key for successful CLQA is being able to correctly translate all terms in the question, especially the OOV phrases.", "labels": [], "entities": []}, {"text": "In this paper, we discuss an approach for accurate question translation that targets the OOV phrases and uses a translation voting mechanism.", "labels": [], "entities": [{"text": "accurate question translation", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.6075016558170319}]}, {"text": "This mechanism involves translations from three different sources: machine translation, online encyclopaedia, and web documents.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7063123732805252}]}, {"text": "The translation with the highest number of votes is selected.", "labels": [], "entities": []}, {"text": "To demonstrate this mechanism, we use Google Translate (GT) as the MT source, Wikipedia as the encyclopaedia source, and Google web search engine to retrieve Wikipedia links and relevant Web document snippets.", "labels": [], "entities": [{"text": "MT source", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.7091360688209534}]}, {"text": "English questions on the Chinese corpus for CLQA are used to illustrate of this approach.", "labels": [], "entities": [{"text": "Chinese corpus for CLQA", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.7386157363653183}]}, {"text": "Finally, the approach is examined and evaluated in terms of translation accuracy and resulting CLIR performance using the test collection, topics and assessment results from NTCIR-8 2 .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.7918491959571838}, {"text": "NTCIR-8", "start_pos": 174, "end_pos": 181, "type": "DATASET", "confidence": 0.9193398952484131}]}], "datasetContent": [{"text": "The evaluation of VMNET performance covers two main aspects: translation accuracy and CLIR performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.9360275864601135}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.881565511226654}]}, {"text": "As we focus on named entity translation, the translation accuracy is measured using the precision of translated named entities at the topic level.", "labels": [], "entities": [{"text": "named entity translation", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.6968385974566141}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9198977947235107}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9682913422584534}]}, {"text": "So the translation precision -P is defined as: where c is the number of topics in which all the named entities are correctly translated; N is the number of topics evaluated.", "labels": [], "entities": [{"text": "precision -P", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.897953987121582}]}, {"text": "The effectiveness of different translation methods can be further measured by the resulting CLIR performance.", "labels": [], "entities": []}, {"text": "In NTCIR-8, CLIR performance is measured using the mean average precision.", "labels": [], "entities": [{"text": "NTCIR-8", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.9192204475402832}, {"text": "mean average precision", "start_pos": 51, "end_pos": 73, "type": "METRIC", "confidence": 0.744529128074646}]}, {"text": "The MAP values are obtained by running the ir4qa_eval2 toolkit with the assessment results 3 on experimental run s.", "labels": [], "entities": [{"text": "MAP", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.864801824092865}]}, {"text": "MAP is computed using only 73 topics due to an insufficient number of relevant document found for the other 27 topics (.", "labels": [], "entities": [{"text": "MAP", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5517712831497192}]}, {"text": "This is the case for all NTCIR-8 ACLIA submissions and not our decision.", "labels": [], "entities": [{"text": "NTCIR-8 ACLIA submissions", "start_pos": 25, "end_pos": 50, "type": "DATASET", "confidence": 0.7923388878504435}]}, {"text": "It also must be noted that there are five topics that have misspelled terms in their English questions.", "labels": [], "entities": []}, {"text": "The misspelled terms in those 5 topics are given in.", "labels": [], "entities": []}, {"text": "It is interesting to see how different translations cope with misspelled terms and how this affects the CLIR result.", "labels": [], "entities": []}, {"text": "A few experimental runs were created for VMNET and CLIR system performance evaluation.", "labels": [], "entities": [{"text": "VMNET", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.6198711395263672}, {"text": "CLIR system performance evaluation", "start_pos": 51, "end_pos": 85, "type": "TASK", "confidence": 0.6325253695249557}]}, {"text": "Their details are listed in.", "labels": [], "entities": []}, {"text": "Those with name *CS-CS* are the Chinese monolingual IR runs; and those with the name *EN-CS* are the English-to-Chinese CLIR runs.", "labels": [], "entities": []}, {"text": "Mono-lingual IR runs are used for benchmarking our CLIR system performance.", "labels": [], "entities": []}, {"text": "The translations in our experiments using Google Translate reflect only the results retrieved at the time of the experiments because Google Translate is believed to be improved overtime.", "labels": [], "entities": []}, {"text": "The result of the final translation evaluation on the 100 topics is given in  There are in total 14 topics in which Google Translate or VMNET failed to correctly translate all named entities.", "labels": [], "entities": []}, {"text": "These topics are listed in.", "labels": [], "entities": []}, {"text": "Interestingly, for topic (ACLIA2-CS-0066) with the misspelled term \"Kasianov\", VMNET still managed to find a correct translation ( \u7c73\u54c8\u4f0a\u5c14 \u00b7 \u7c73 \u54c8 \u4f0a \u6d1b \u7ef4 \u5947\u00b7\u5361\u897f\u4e9a\u8bfa\u592b).", "labels": [], "entities": [{"text": "VMNET", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.8516205549240112}]}, {"text": "This has to be attributed to the search engine's capability in handling misspellings.", "labels": [], "entities": []}, {"text": "On the other hand, Google Translate was correct in its translation of \"Northern Territories\" of Japan, but VMNET incorrectly chose \"Northern Territory\" (of Australia).", "labels": [], "entities": []}, {"text": "For the rest of the misspelled phrases, neither Google Translate nor VMNET could pick the correct translation.", "labels": [], "entities": [{"text": "VMNET", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.9204126000404358}]}, {"text": "The MAP values of all experimental runs corresponding to each query processing technique and Chinese indexing strategy are given in Table 5.", "labels": [], "entities": [{"text": "MAP", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.976678192615509}]}, {"text": "The results of mono-lingual runs give benchmarking scores for CLIR runs.", "labels": [], "entities": []}, {"text": "As expected, the highest MAP 0.4681 is achieved by the monolingual run VMNET-CS-CS-01-T, in which the questions were manually segmented and all the noise words were removed.", "labels": [], "entities": [{"text": "MAP 0.4681", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9746798872947693}, {"text": "VMNET-CS-CS-01-T", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.9252383708953857}]}, {"text": "It is encouraging to see that the automatic run VMNET-CS-CS-02-T with only question template phrase removal has a slightly lower MAP 0.4419 than that (0.4488) of the best performance CS-CS run in the NTCIR-8 evaluation forum.", "labels": [], "entities": [{"text": "question template phrase removal", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.5853715240955353}, {"text": "MAP", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.9977499842643738}, {"text": "NTCIR-8 evaluation forum", "start_pos": 200, "end_pos": 224, "type": "DATASET", "confidence": 0.85851122935613}]}, {"text": "If unigrams were used as the only indexing units, the MAP of VMNET-CS-CS-04-T dropped from 0.4681 to 0.3406.", "labels": [], "entities": [{"text": "MAP", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9994986057281494}, {"text": "VMNET-CS-CS-04-T", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.9346901178359985}]}, {"text": "On the other hand, all runs using bigrams as indexing units either exclusively or jointly performed very well.", "labels": [], "entities": []}, {"text": "The MAP of run VMNET-CS-CS-05-T using bigrams only is 0.4653, which is slightly lower than that of the top performer run VMNET-CS-CS-01-T, which used two forms of indexing units.", "labels": [], "entities": [{"text": "MAP", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9995726943016052}, {"text": "VMNET-CS-CS-05-T", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.8503162860870361}, {"text": "VMNET-CS-CS-01-T", "start_pos": 121, "end_pos": 137, "type": "DATASET", "confidence": 0.9009875655174255}]}, {"text": "However, retrieval performance could be maximised by using both unigrams and bigrams as indexing units.", "labels": [], "entities": []}, {"text": "The highest MAP (0.3756) of a CLIR run is achieved by run VMNET-EN-CS-03-T, which used VMNET for translation.", "labels": [], "entities": [{"text": "MAP", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9990045428276062}, {"text": "VMNET-EN-CS-03-T", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.9023427963256836}]}, {"text": "Comparing it to our manual run VMNET-CS-CS-01-T, there is around 9% performance degradation as a result of the influence of noise words in the questions, and the possible information loss or added noise due to English-to-Chinese translation, even though the named entities translation precision is relatively high.", "labels": [], "entities": [{"text": "VMNET-CS-CS-01-T", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.900341808795929}, {"text": "precision", "start_pos": 285, "end_pos": 294, "type": "METRIC", "confidence": 0.9152539372444153}]}, {"text": "The best EN-CS CLIR run (MAP 0.4209) in all submissions to the NTCIR-8 ACLIA task used the same indexing technique (bigrams and unigrams) and ranking function (BM25) as run VMNET-EN-CS-03-T but with \"query expansion based on RSV\").", "labels": [], "entities": [{"text": "NTCIR-8 ACLIA task", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.8180718223253886}, {"text": "BM25", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.7394530773162842}]}, {"text": "The MAP difference 4.5% between the forum best run and our CLIR best run could suggest that using query expansion is an effective way to improve the CLIR system performance.", "labels": [], "entities": [{"text": "MAP", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9964565634727478}]}, {"text": "Runs VMNET-EN-CS-01-T and VMNET-EN-CS-04-T, that both used Google Translate provide direct comparisons with runs VMNET-EN-CS-02-T and VMNET-EN-CS-03-T, respectively, which employed VMNET for translation.", "labels": [], "entities": [{"text": "VMNET-EN-CS-01-T", "start_pos": 5, "end_pos": 21, "type": "DATASET", "confidence": 0.9316166639328003}, {"text": "VMNET-EN-CS-04-T", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.858452558517456}]}, {"text": "All runs using VMNET performed better than the runs using Google Translate.", "labels": [], "entities": []}, {"text": "The different performances between CLIR runs using Google Translate and VMENT is the joint result of the translation improvement and other translation differences.", "labels": [], "entities": [{"text": "VMENT", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.852321445941925}]}, {"text": "As shown in, VMNET found the correct translations for 8 more topics than Google Translate.", "labels": [], "entities": [{"text": "VMNET", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.8696706891059875}]}, {"text": "It should be noted that there are two topics (ACLIA2-CS-0008 and ACLIA2-CS-0088) not included in the final CLIR evaluation.", "labels": [], "entities": [{"text": "ACLIA2-CS-0008", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.8580489158630371}]}, {"text": "Also, there is one phrase, \"Kenneth Yen (K. T. Yen) (\u4e25\u51ef\u6cf0)\", which VMNET couldn't find the correct translation for, but it detected a highly associated term \"Yulon -\u88d5\u9686\u6c7d\u8f66\", an automaker company in Taiwan; Kenneth Yen is the CEO of Yulon.", "labels": [], "entities": [{"text": "VMNET", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.950011670589447}]}, {"text": "Although Yulon is not a correct translation, it is still a good query term because it is then possible to find the correct answer for the question: \"Who is Kenneth Yen?\".", "labels": [], "entities": []}, {"text": "However, this topic was not included in the NTCIR-8 IR4QA evaluation.", "labels": [], "entities": [{"text": "NTCIR-8 IR4QA evaluation", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.7917556762695312}]}, {"text": "Moreover, it is possible to have multiple explanations fora term.", "labels": [], "entities": []}, {"text": "In order to discover as many question-related documents as possible, alternative translations found by VMNET are also used as additional query terms.", "labels": [], "entities": [{"text": "VMNET", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.903809130191803}]}, {"text": "For example, \u4e01\u514b is the Chinese term for DINK in Mainland China, but \u9876 \u5ba2\u65cf is used in Taiwan.", "labels": [], "entities": []}, {"text": "Furthermore, because VMNET gives the Wikipedia translation the highest priority if only one entry is found, a person's full name is used in person name translation rather than the short commonly used name.", "labels": [], "entities": [{"text": "VMNET", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.8983756899833679}, {"text": "person name translation", "start_pos": 140, "end_pos": 163, "type": "TASK", "confidence": 0.6569091975688934}]}, {"text": "For example, Cheney (former vice president of U.S.) is translated into \u8fea \u514b\u00b7\u5207\u5c3c rather than just \u5207\u5c3c..", "labels": [], "entities": []}, {"text": "Alternative translations The biggest difference, 3.07%, between runs that used different translation is from runs VMNET-EN-CS-03-T and VMNET-EN-CS-04-T, which both pruned the question template phrase for simple query processing.", "labels": [], "entities": [{"text": "VMNET-EN-CS-03-T", "start_pos": 114, "end_pos": 130, "type": "DATASET", "confidence": 0.9310895800590515}, {"text": "VMNET-EN-CS-04-T", "start_pos": 135, "end_pos": 151, "type": "DATASET", "confidence": 0.9131109714508057}]}, {"text": "Although the performance improvement is not obvious, the correct translations and the additional query terms found by VMNET are still very valuable.", "labels": [], "entities": [{"text": "VMNET", "start_pos": 118, "end_pos": 123, "type": "DATASET", "confidence": 0.9142304062843323}]}], "tableCaptions": []}