{"title": [{"text": "Identifying Patterns for Unsupervised Grammar Induction", "labels": [], "entities": [{"text": "Identifying Patterns", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9287514090538025}, {"text": "Unsupervised Grammar Induction", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.5830024977525076}]}], "abstractContent": [{"text": "This paper describes anew method for un-supervised grammar induction based on the automatic extraction of certain patterns in the texts.", "labels": [], "entities": [{"text": "un-supervised grammar induction", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6813459495703379}]}, {"text": "Our starting hypothesis is that there exist some classes of words that function as separators, marking the beginning or the end of new constituents.", "labels": [], "entities": []}, {"text": "Among these separators we distinguish those which trigger new levels in the parse tree.", "labels": [], "entities": []}, {"text": "If we are able to detect these separators we can follow a very simple procedure to identify the constituents of a sentence by taking the classes of words between separators.", "labels": [], "entities": []}, {"text": "This paper is devoted to describe the process that we have followed to automatically identify the set of sepa-rators from a corpus only annotated with Part-of-Speech (POS) tags.", "labels": [], "entities": []}, {"text": "The proposed approach has allowed us to improve the results of previous proposals when parsing sentences from the Wall Street Journal corpus .", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 114, "end_pos": 140, "type": "DATASET", "confidence": 0.8693398386240005}]}], "introductionContent": [{"text": "Most works dealing with Grammar Induction (GI) are focused on Supervised Grammar Induction, using a corpus of syntactically annotated sentences, or treebank, as a reference to extract the grammar.", "labels": [], "entities": [{"text": "Grammar Induction (GI)", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.8798310399055481}, {"text": "Supervised Grammar Induction", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.6615905364354452}]}, {"text": "The existence of a treebank for the language and fora particular type of texts from which we want to extract the grammar is a great help to GI, even taking into account the theoretical limitations of GI, such as the fact that grammars cannot be correctly identified from positive examples alone.", "labels": [], "entities": []}, {"text": "But the manual annotation of thousands of sentences is a very expensive task and thus there are many languages for which there are not treebanks available.", "labels": [], "entities": []}, {"text": "Even in languages for which there is a treebank, it is usually composed of a particular kind of texts (newspaper articles, for example) and may not be appropriate for other kind of texts, such as tales or poetry.", "labels": [], "entities": []}, {"text": "These reasons have led to the appearance of several works focused on unsupervised GI.", "labels": [], "entities": []}, {"text": "Thanks to our knowledge of the language we know that some classes of words are particularly influential to determine the structure of a sentence.", "labels": [], "entities": []}, {"text": "For example, let us consider the tree in, for which the meaning of the POS tags appears in.", "labels": [], "entities": []}, {"text": "We can observe that the tag MD (Modal) breaks the sentence into two parts.", "labels": [], "entities": []}, {"text": "Analogously, in the tree appearing in the POS tag VBZ breaks the sentence.", "labels": [], "entities": [{"text": "POS tag VBZ", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.7551450928052267}]}, {"text": "In both cases, we can see that after the breaking tag, anew level appears in the parse tree.", "labels": [], "entities": []}, {"text": "A similar effect is observed for other POS tags, such as VB in the tree of and IN in the tree of.", "labels": [], "entities": [{"text": "IN", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9900397658348083}]}, {"text": "We call these kind of POS tags separators.", "labels": [], "entities": []}, {"text": "There are also other POS tags which are frequently the beginning or the end of a constituent . For example in the tree in we can find the sequences (DT NN) and (DT JJ NN), which according to the parse tree are constituents.", "labels": [], "entities": []}, {"text": "In the tree in we find the sequence (DT NNP VBG NN).", "labels": [], "entities": [{"text": "DT NNP VBG NN", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.5747847110033035}]}, {"text": "In both trees we can also find sequences beginning with the tag NNP: (NNP NNP) and (NNP CD) in the tree in and (NNP NNP), which appears twice, in the tree in.", "labels": [], "entities": []}, {"text": "This suggests that there are classes of words with a trend to be the beginning or the end of constituents without giving rise to new levels in the parse tree.", "labels": [], "entities": []}, {"text": "We call these POS tags subseparators.", "labels": [], "entities": []}, {"text": "These observations reflect some of our intuitions, such as the fact that most sentences are composed of a noun phrase and a verb phrase, being frequently the verb the beginning of the verbal phrase, which usually leads to anew level of the parse tree.", "labels": [], "entities": []}, {"text": "We also know that determiners (DT) are frequently the beginning of the noun phrases.", "labels": [], "entities": [{"text": "determiners", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9404123425483704}]}, {"text": "At this point we could either try to figure out what is the set of tags which work as separators, or to compute them from a parsed corpus for the considered language, provided it is available.", "labels": [], "entities": []}, {"text": "However, because we do not want to rely on the existence of a treebank for the corresponding language and type of texts we have done something different: we have devised a statistical procedure to automatically capture the word classes which function as separators.", "labels": [], "entities": []}, {"text": "In this way our approach can be applied to most languages, and apart from providing a tool for extracting grammars and parsing sentences, it can be useful to study the different classes of words that work as separators in different languages.", "labels": [], "entities": []}, {"text": "Our statistical mechanism to detect separators is applied to a corpus of sentences annotated with POS tags.", "labels": [], "entities": []}, {"text": "This is not a strong requirement since there are very accurate POS taggers (about 97%) for many languages.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.788134902715683}]}, {"text": "The grammar that we obtain does not specify the left-hand-side of the rules, but only sequences of POS tags that are constituents.: Alphabetical list of part-of-speech tags used in the Penn Treebank, the corpus used in our experiments At this point we have followed the Klein and Manning (2005) setting for the problem, which allows us to compare our results to theirs.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 185, "end_pos": 198, "type": "DATASET", "confidence": 0.9936570823192596}]}, {"text": "As far as we know these are the best results obtained so far for unsupervised GI using a monolingual corpus.", "labels": [], "entities": []}, {"text": "As they do, we have used the Penn treebank for our experiments, employing the syntactic annotations that it provides for evaluation purposes only.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9935798346996307}]}, {"text": "Specifically, we have used WSJ10, composed of 6842 sentences, which is the subset of the Wall Street Journal section of the Penn Treebank, containing only those sentences of 10 words or less after removing punctuation and null elements, such as $, \", etc.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.8988410234451294}, {"text": "Wall Street Journal section of the Penn Treebank", "start_pos": 89, "end_pos": 137, "type": "DATASET", "confidence": 0.9533399939537048}]}, {"text": "The rest of the paper is organized as follows: section 2 reviews some related works; section 3 describes the details of the proposal to automatically extract the separators from a POS tagged corpus; section 4 is devoted to describe the procedure to find a parse tree using the separators; section 5 presents and discusses the experimental results, and section 6 draws the main conclusions of this work.", "labels": [], "entities": [{"text": "POS tagged corpus", "start_pos": 180, "end_pos": 197, "type": "DATASET", "confidence": 0.7449050347010294}]}], "datasetContent": [{"text": "Our proposal has been evaluated by comparing the tree structures produced by the system to the goldstandard trees produced by linguists, which can be found in the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 163, "end_pos": 176, "type": "DATASET", "confidence": 0.9968482553958893}]}, {"text": "Because we do not assign class name to our constituents, i.e. a left hand side symbol for the grammar rules, as the linguists do in treebanks, the comparison ignores the class labels, considering only groups of tags.", "labels": [], "entities": []}, {"text": "The results presented in the work by have been our reference, since as far we know they are the best ones obtained so far for unsupervised GI.", "labels": [], "entities": [{"text": "GI", "start_pos": 139, "end_pos": 141, "type": "TASK", "confidence": 0.848476767539978}]}, {"text": "For the sake of comparison, we have considered the same corpus and the same measures.", "labels": [], "entities": []}, {"text": "Accordingly, we performed the experiments on the 6842 sentences 3 of the WSJ10 selection from the Penn treebank Wall Street Journal section.", "labels": [], "entities": [{"text": "WSJ10 selection from the Penn treebank Wall Street Journal section", "start_pos": 73, "end_pos": 139, "type": "DATASET", "confidence": 0.9445314645767212}]}, {"text": "In order to evaluate the quality of the obtained grammar we have used the most common measures for parsing and grammar induction evaluation: recall, precision, and their harmonic mean (F-measure).", "labels": [], "entities": [{"text": "parsing", "start_pos": 99, "end_pos": 106, "type": "TASK", "confidence": 0.9718727469444275}, {"text": "grammar induction evaluation", "start_pos": 111, "end_pos": 139, "type": "TASK", "confidence": 0.7449348171552023}, {"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9991581439971924}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9989457726478577}, {"text": "F-measure", "start_pos": 185, "end_pos": 194, "type": "METRIC", "confidence": 0.9253263473510742}]}, {"text": "They are defined assuming a bracket representation of a parse tree.", "labels": [], "entities": []}, {"text": "Precision is given by the number of brackets in the parse to evaluate which match those in the correct tree and recall measures how many of the brackets in the correct tree are in the parse.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9901844263076782}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9993295669555664}]}, {"text": "These measures have counterparts for unlabeled trees, the ones considered in this work -in which the label assigned to each constituent is not checked.", "labels": [], "entities": []}, {"text": "Constituents which could not be wrong (those of size one and those spanning the whole sentence) have not been included in the measures.", "labels": [], "entities": []}, {"text": "The definitions of Unlabeled Precision (UP) and Recall (UR) of a proposed corpus Finally, U F (Unlabeled F-measure) is given by: 3 More precisely sequences of POS tags shows the results of unlabeled recall, precision and F-measure obtained per constituent size.", "labels": [], "entities": [{"text": "Recall (UR)", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9502181559801102}, {"text": "U F (Unlabeled F-measure)", "start_pos": 90, "end_pos": 115, "type": "METRIC", "confidence": 0.6831770539283752}, {"text": "recall", "start_pos": 199, "end_pos": 205, "type": "METRIC", "confidence": 0.9417293071746826}, {"text": "precision", "start_pos": 207, "end_pos": 216, "type": "METRIC", "confidence": 0.998950183391571}, {"text": "F-measure", "start_pos": 221, "end_pos": 230, "type": "METRIC", "confidence": 0.9938295483589172}]}, {"text": "We can observe that recall and precision, and thus the corresponding F-measure, are quite similar for every constituent size.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9991631507873535}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9981347322463989}, {"text": "F-measure", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9958401918411255}]}, {"text": "This is important, because obtaining a high F-measure thanks to a very high recall but with a poor precision, is not so useful.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9988340735435486}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9991279244422913}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9985740184783936}]}, {"text": "We can also observe that the best results are obtained for short and long constituents, with lower values for middle lengths, such as 5 and 6.", "labels": [], "entities": []}, {"text": "We believe that this is because intermediate size constituents present more variability.", "labels": [], "entities": []}, {"text": "Moreover, for intermediate sizes, the composition of the constituents is more dependent on sub-separators, for which the statistical differences are less significant than for separators.", "labels": [], "entities": []}, {"text": "We have compared our results to those obtained by for the same corpus.", "labels": [], "entities": []}, {"text": "shows the obtained results for WSJ10.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.8461886048316956}]}, {"text": "We can observe that we have obtained more balanced values of recall and precision, as well as a better value for the F-measure.", "labels": [], "entities": [{"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9996764659881592}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.999334990978241}, {"text": "F-measure", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9833773374557495}]}, {"text": "Thus the method proposed in this work, that we expect to refine by assigning different probabilities to separators and sub-separators, depending on the context they appear in, provides a very promising approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: F-measure results obtained for different  values of the threshold used to classify the set of  POS-tags.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9972869157791138}]}]}