{"title": [{"text": "Domain Adaptation to Summarize Human Conversations", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7009394764900208}, {"text": "Summarize Human Conversations", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.889086922009786}]}], "abstractContent": [{"text": "We are interested in improving the sum-marization of conversations by using domain adaptation.", "labels": [], "entities": []}, {"text": "Since very few email corpora have been annotated for summa-rization purposes, we attempt to leverage the labeled data available in the multi-party meetings domain for the summari-zation of email threads.", "labels": [], "entities": []}, {"text": "In this paper, we compare several approaches to supervised domain adaptation using out-of-domain labeled data, and also try to use unlabeled data in the target domain through semi-supervised domain adaptation.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.7443258762359619}]}, {"text": "From the results of our experiments, we conclude that with some in-domain", "labels": [], "entities": []}], "introductionContent": [{"text": "On a given day, many people engage in conversations via several modalities, including face-toface speech, telephone, email, SMS, chat, and blogs.", "labels": [], "entities": []}, {"text": "Being able to produce automatic summaries of multi-party conversations occurring in one or several of these modalities would enable the parties involved to keep track of and make sense of this diverse data.", "labels": [], "entities": []}, {"text": "However, summarizing spoken dialogue is more challenging than summarizing written monologues such as books and articles, as speech tends to be more fragmented and disfluent.", "labels": [], "entities": [{"text": "summarizing spoken dialogue", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.9295527338981628}, {"text": "summarizing written monologues such as books and articles", "start_pos": 62, "end_pos": 119, "type": "TASK", "confidence": 0.8879218623042107}]}, {"text": "We are interested in using both fully and semisupervised techniques to produce extractive summaries for conversations, where each sentence of a text is labeled with its informativeness, and a subset of sentences are concatenated into an extractive summary of the text.", "labels": [], "entities": []}, {"text": "In previous work, it has been shown that conversations in different modalities can be effectively characterized by a set of \"conversational\" features that are useful in detecting informativeness for the task of extractive summarization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 211, "end_pos": 235, "type": "TASK", "confidence": 0.5573931336402893}]}, {"text": "However, because of privacy concerns, annotated corpora are rarely publicly available for conversational data, including for the email domain.", "labels": [], "entities": []}, {"text": "One promising solution to this problem is domain adaptation, which aims to use labeled data in a well-studied source domain and a limited amount of labeled data from a different target domain to train a model that performs well in that target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7808671295642853}]}, {"text": "In this work, we investigate using domain adaptation that leverages labeled data in the domain of meetings along with labeled and unlabeled email data for summarizing email threads.", "labels": [], "entities": [{"text": "summarizing email threads", "start_pos": 155, "end_pos": 180, "type": "TASK", "confidence": 0.9037220676740011}]}, {"text": "We evaluate several domain adaptation algorithms, using both a small set of conversational features and a large set of simple lexical features to determine what settings will yield the best results for summarizing email conversations.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.749500960111618}, {"text": "summarizing email conversations", "start_pos": 202, "end_pos": 233, "type": "TASK", "confidence": 0.9148370027542114}]}, {"text": "In our experiments, we do not get a significant improvement from using out-ofdomain data in addition to in-domain data in supervised domain adaptation, though in the setting where only unlabeled in-domain data is available, we gain from using it through structural correspondence learning.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 122, "end_pos": 150, "type": "TASK", "confidence": 0.737929105758667}]}, {"text": "We also observe that conversational features are more useful in supervised methods, whereas lexical features are better leveraged in semi-supervised adaptation.", "labels": [], "entities": []}, {"text": "The next section surveys past research in domain adaptation and in summarizing conversational data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7362520396709442}, {"text": "summarizing conversational", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.9184720516204834}]}, {"text": "In section 3 we present the corpora and feature sets we used, and we describe our experimental setting in section 4.", "labels": [], "entities": []}, {"text": "We then compare the performance of different methods in section 5 and draw conclusions in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The AMI meeting corpus: We use the scenario portion of the AMI corpus (), for which groups of four participants take part in a series of four meetings and play roles within a fictitious company.", "labels": [], "entities": [{"text": "AMI meeting corpus", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8641970554987589}, {"text": "AMI corpus", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.940714031457901}]}, {"text": "While the scenario given to them is artificial, the speech and the actions are completely spontaneous and natural.", "labels": [], "entities": []}, {"text": "The dataset contains approximately 115000 dialogue act (DA) segments.", "labels": [], "entities": [{"text": "dialogue act (DA) segments", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.6593200912078222}]}, {"text": "For the annotation, annotators wrote abstract summaries of each meeting and extracted transcript DA segments that best conveyed or supported the information in the abstracts.", "labels": [], "entities": []}, {"text": "A many-to-many mapping between transcript DAs and sentences from the human abstract was obtained for each annotator, with three annotators assigned to each meeting.", "labels": [], "entities": []}, {"text": "We consider a dialogue act to be a positive example if it is linked to a given human summary, and a negative example otherwise.", "labels": [], "entities": []}, {"text": "Approximately 13% of the total DAs are ultimately labeled as positive.", "labels": [], "entities": []}, {"text": "The BC3 email corpus 1 : composed of 40 email threads from the World Wide Web Consortium (W3C) mailing list which feature a variety of topics such as web accessibility and planning face-to-face meetings.", "labels": [], "entities": [{"text": "BC3 email corpus 1", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8707340806722641}, {"text": "World Wide Web Consortium (W3C) mailing list", "start_pos": 63, "end_pos": 107, "type": "DATASET", "confidence": 0.5282418562306298}]}, {"text": "Each thread is annotated similarly to the AMI corpus, with three an-1 http://www.cs.ubc.ca/labs/lci/bc3.html notators authoring abstracts and linking email thread sentences to the abstract sentences.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.9033389687538147}]}, {"text": "The Enron email corpus 2 : a collection of emails released as part of the investigation into the Enron corporation, it has become a popular corpus for NLP research due to being realistic, naturally-occurring data from a corporate environment.", "labels": [], "entities": [{"text": "Enron email corpus 2", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7777589708566666}]}, {"text": "We use 39 threads from this corpus to supplement the BC3 email data.", "labels": [], "entities": [{"text": "BC3 email data", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9196049968401591}]}, {"text": "Given the predicted labels on a test set and the existing gold-standard labels of the test set data, in each of our experiments we compute the area under the receiver operator curve as a measure of performance.", "labels": [], "entities": []}, {"text": "The area under the ROC (auROC) is a common summary statistic used to measure the quality of binary classification, where a perfect classifier would achieve an auROC of 1.0, and a random classifier, near 0.5.", "labels": [], "entities": [{"text": "ROC (auROC)", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.7585241794586182}, {"text": "auROC", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.983111560344696}]}, {"text": "The available labeled BC3 data totals about 3000 sentences, and the available labeled AMI data totals over 100,000 sentences, so for both efficiency and to not overwhelm the in-domain data, in each of our runs we subsample 10,000 sentences from the AMI data to use for training.", "labels": [], "entities": [{"text": "BC3 data", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.7581804692745209}, {"text": "AMI data", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.9013786911964417}, {"text": "AMI data", "start_pos": 249, "end_pos": 257, "type": "DATASET", "confidence": 0.9365262389183044}]}, {"text": "After some initial experiments, where increasing the amount of target data beyond this did not improve accuracy, we decided not to incur the runtime cost of training on larger amounts of source data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9990561604499817}]}, {"text": "Similarly, given that we extracted about 200,000 lexical features from our corpora, from our initial experiments trading off auROC and runtime, we decided to select a subset of 10,000 lexical features chosen by having the top mutual information with respect to the summarization labels.", "labels": [], "entities": [{"text": "auROC", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.7009328007698059}]}, {"text": "We did 5-fold cross-validation to split the target set into training and testing portions, and ran all the domain adaptation methods using the same split.", "labels": [], "entities": []}, {"text": "We report the auROC performance of each method averaged over three runs of the 5-fold cross-validation.", "labels": [], "entities": [{"text": "auROC", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.8897047638893127}]}, {"text": "To test for significant differences between the performances of the various methods, we compute pairwise t-tests between the auROC values obtained on the same run.", "labels": [], "entities": [{"text": "auROC", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.838961660861969}]}, {"text": "To account for an increased chance of false positives in reporting results of several pairwise t-tests, we report significance for p-values < 0.005 rather than at the customary 0.05 level.", "labels": [], "entities": [{"text": "significance", "start_pos": 114, "end_pos": 126, "type": "METRIC", "confidence": 0.9922903180122375}]}], "tableCaptions": [{"text": " Table 1. Performance and time of domain adaptation methods with the two feature sets", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.6993915289640427}]}]}