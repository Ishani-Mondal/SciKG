{"title": [{"text": "Retrieving Correct Semantic Boundaries in Dependency Structure", "labels": [], "entities": [{"text": "Retrieving Correct Semantic Boundaries", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8074404746294022}]}], "abstractContent": [{"text": "This paper describes the retrieval of correct semantic boundaries for predicate-argument structures annotated by dependency structure.", "labels": [], "entities": []}, {"text": "Unlike phrase structure, in which arguments are annotated at the phrase level, dependency structure does not have phrases so the argument labels are associated with head words instead: the subtree of each headword is assumed to include the same set of words as the annotated phrase does in phrase structure.", "labels": [], "entities": []}, {"text": "However, at least in English, retrieving such subtrees does not always guarantee retrieval of the correct phrase boundaries.", "labels": [], "entities": []}, {"text": "In this paper, we present heuristics that retrieve correct phrase boundaries for semantic arguments, called semantic boundaries , from dependency trees.", "labels": [], "entities": []}, {"text": "By applying heuristics, we achieved an F1-score of 99.54% for correct representation of semantic boundaries.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9996676445007324}]}, {"text": "Furthermore, error analysis showed that some of the errors could also be considered correct, depending on the interpretation of the annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency structure has recently gained wide interest because it is simple yet provides useful information for many NLP tasks such as sentiment analysis) or machine translation.", "labels": [], "entities": [{"text": "Dependency structure", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8805320262908936}, {"text": "sentiment analysis", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.9440806210041046}, {"text": "machine translation", "start_pos": 158, "end_pos": 177, "type": "TASK", "confidence": 0.8292673528194427}]}, {"text": "Although dependency structure is a kind of syntactic structure, it is quite different from phrase structure: phrase structure gives phrase information by grouping constituents whereas dependency structure gives dependency relations between pairs of words.", "labels": [], "entities": []}, {"text": "Many dependency relations (e.g., subject, object) have high correlations with semantic roles (e.g., agent, patient), which makes dependency structure suitable for representing semantic information such as predicate-argument structure.", "labels": [], "entities": []}, {"text": "In 2009, the Conference on Computational Natural Language Learning (CoNLL) opened a shared task: the participants were supposed to take dependency trees as input and produce semantic role labels as output).", "labels": [], "entities": []}, {"text": "The dependency trees were automatically converted from the Penn Treebank (), which consists of phrase structure trees, using some heuristics (cf. Section 3).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.994367778301239}]}, {"text": "The semantic roles were extracted from the Propbank ().", "labels": [], "entities": [{"text": "Propbank", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.9269659519195557}]}, {"text": "Since Propbank arguments were originally annotated at the phrase level using the Penn Treebank and the phrase information got lost during the conversion to the dependency trees, arguments are annotated on head words instead of phrases in dependency trees; the subtree of each headword is assumed to include the same set of words as the annotated phrase does in phrase structure.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9959917962551117}]}, {"text": "shows a dependency tree that has been converted from the corresponding phrase structure tree.", "labels": [], "entities": []}, {"text": "In the phrase structure tree, arguments of the verb predicate appear are annotated on the phrases: NP 1 as ARG 0 and PP 1 as ARGM-LOC.", "labels": [], "entities": [{"text": "ARG 0", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.7395060062408447}, {"text": "ARGM-LOC", "start_pos": 125, "end_pos": 133, "type": "DATASET", "confidence": 0.931381344795227}]}, {"text": "In the dependency tree, the arguments are annotated on the head words instead: results as the ARG 0 and in as the In this example, both PP 1 and the subtree of in consist of the same set of words {in, today, 's, news} (as is the case for NP 1 and the subtree of results); therefore, the phrase boundaries for the semantic arguments, called semantic boundaries, are retrieved correctly from the dependency tree.", "labels": [], "entities": []}, {"text": "Retrieving the subtrees of head words usually gives correct semantic boundaries; however, there are cases where the strategy does notwork.", "labels": [], "entities": []}, {"text": "For example, if the verb predicate is a gerund or a pastparticiple, it is possible that the predicate becomes a syntactic child of the headword annotated as a semantic argument of the predicate.", "labels": [], "entities": []}, {"text": "In, the headword plant is annotated as ARG 1 of the verb predicate owned, where owned is a child of plant in the dependency tree.", "labels": [], "entities": [{"text": "ARG", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.6636337041854858}]}, {"text": "Thus, retrieving the subtree of plant would include the predicate itself, which is not the correct semantic boundary for the argument (the correct boundary would be only {The, plant}).", "labels": [], "entities": []}, {"text": "For such cases, we need some alternative for retrieving the correct semantic boundaries.", "labels": [], "entities": []}, {"text": "This is an important issue that has not yet been thoroughly addressed.", "labels": [], "entities": []}, {"text": "In this paper, we first show how to convert the Penn Treebank style phrase structure to dependency structure.", "labels": [], "entities": [{"text": "Penn Treebank style phrase structure", "start_pos": 48, "end_pos": 84, "type": "DATASET", "confidence": 0.9643690466880799}]}, {"text": "We then describe how to annotate the Propbank arguments, already annotated in the phrase structure, on head words in the dependency structure.", "labels": [], "entities": []}, {"text": "Finally, we present heuristics that correctly retrieve semantic boundaries inmost cases.", "labels": [], "entities": []}, {"text": "For our experiments, we used the entire Penn Treebank (Wall Street Journal).", "labels": [], "entities": [{"text": "Penn Treebank (Wall Street Journal)", "start_pos": 40, "end_pos": 75, "type": "DATASET", "confidence": 0.9453825524875096}]}, {"text": "Our experiments show that it is possible to achieve an F1-score of 99.54% for correct representation of the semantic boundaries.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9996709823608398}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Model comparisons (in percentage)", "labels": [], "entities": []}]}