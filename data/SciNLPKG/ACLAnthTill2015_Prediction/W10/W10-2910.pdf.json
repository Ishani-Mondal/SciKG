{"title": [{"text": "Syntactic and Semantic Structure for Opinion Expression Detection", "labels": [], "entities": [{"text": "Opinion Expression Detection", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.7294161121050516}]}], "abstractContent": [{"text": "We demonstrate that relational features derived from dependency-syntactic and semantic role structures are useful for the task of detecting opinionated expressions in natural-language text, significantly improving over conventional models based on sequence labeling with local features.", "labels": [], "entities": [{"text": "detecting opinionated expressions in natural-language text", "start_pos": 130, "end_pos": 188, "type": "TASK", "confidence": 0.7990402579307556}]}, {"text": "These features allow us to model the way opinionated expressions interact in a sentence over arbitrary distances.", "labels": [], "entities": []}, {"text": "While the relational features make the prediction task more computationally expensive , we show that it can be tackled effectively by using a reranker.", "labels": [], "entities": [{"text": "prediction task", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.8951539695262909}]}, {"text": "We evaluate a number of machine learning approaches for the reranker, and the best model results in a 10-point absolute improvement in soft recall on the MPQA corpus, while decreasing precision only slightly.", "labels": [], "entities": [{"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9682657122612}, {"text": "MPQA corpus", "start_pos": 154, "end_pos": 165, "type": "DATASET", "confidence": 0.9653482139110565}, {"text": "precision", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9992035031318665}]}], "introductionContent": [{"text": "The automatic detection and analysis of opinionated text -subjectivity analysis -is potentially useful fora number of natural language processing tasks.", "labels": [], "entities": [{"text": "automatic detection and analysis of opinionated text -subjectivity analysis", "start_pos": 4, "end_pos": 79, "type": "TASK", "confidence": 0.7120093643665314}, {"text": "natural language processing", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.6636578043301901}]}, {"text": "Examples include retrieval systems answering queries about how a particular person feels about a product or political question, and various types of market analysis tools such as review mining systems.", "labels": [], "entities": [{"text": "answering queries about how a particular person feels about a product or political question", "start_pos": 35, "end_pos": 126, "type": "TASK", "confidence": 0.6287187337875366}]}, {"text": "A primary task in subjectivity analysis is to markup the opinionated expressions, i.e. the text snippets signaling the subjective content of the text.", "labels": [], "entities": [{"text": "subjectivity analysis", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.7349059879779816}]}, {"text": "This is necessary for further analysis, such as the determination of opinion holder and the polarity of the opinion.", "labels": [], "entities": []}, {"text": "The MPQA corpus ( ), a widely used corpus annotated with subjectivity information, defines two types of subjective expressions: direct subjective expressions (DSEs), which are explicit mentions of opinion, and expressive subjective elements (ESEs), which signal the attitude of the speaker by the choice of words.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9284253716468811}]}, {"text": "DSEs are often verbs of statement and categorization, where the opinion and its holder tend to be direct semantic arguments of the verb.", "labels": [], "entities": []}, {"text": "ESEs, on the other hand, are less easy to categorize syntactically; prototypical examples would include value-expressing adjectives such as beautiful, biased, etc.", "labels": [], "entities": []}, {"text": "In addition to DSEs and ESEs, the MPQA corpus also contains annotation for non-subjective statements, which are referred to as objective speech events (OSEs).", "labels": [], "entities": [{"text": "ESEs", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.741930365562439}, {"text": "MPQA corpus", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.9683531224727631}]}, {"text": "Examples (1) and show two sentences from the MPQA corpus where DSEs and ESEs have been manually annotated.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9765795767307281}, {"text": "ESEs", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.7479606866836548}]}, {"text": "The task of marking up these expressions has usually been approached using straightforward sequence labeling techniques using simple features in a small contextual window ().", "labels": [], "entities": []}, {"text": "However, due to the simplicity of the feature sets, this approach fails to take into account the fact that the semantic and pragmatic interpretation of sentences is not only determined by words but also by syntactic and shallow-semantic relations.", "labels": [], "entities": []}, {"text": "Crucially, taking grammatical relations into account allows us to model how expressions interact in various ways that influence their interpretation as subjective or not.", "labels": [], "entities": []}, {"text": "Consider, for instance, the word said in examples (3) and (4) below, where the interpretation as a DSE or an OSE is influenced by the subjective content of the enclosed statement.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate how syntactic and semantic structural information can be used to improve opinion detection.", "labels": [], "entities": [{"text": "opinion detection", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.9210061132907867}]}, {"text": "While this feature model makes it impossible to use the standard sequence labeling method, we show that with a simple strategy based on reranking, incorporating structural features results in a significant improvement.", "labels": [], "entities": []}, {"text": "We investigate two different reranking strategies: the Preference Kernel approach and an approach based on structure learning.", "labels": [], "entities": []}, {"text": "In an evaluation on the MPQA corpus, the best system we evaluated, a structure learning-based reranker using the Passive-Aggressive learning algorithm, achieved a 10-point absolute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler .", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9215469658374786}, {"text": "recall", "start_pos": 201, "end_pos": 207, "type": "METRIC", "confidence": 0.9587951302528381}, {"text": "F-measure", "start_pos": 238, "end_pos": 247, "type": "METRIC", "confidence": 0.9976487755775452}]}], "datasetContent": [{"text": "We carried out the experiments on version 2 of the MPQA corpus ( ), which we split into a test set (150 documents, 3,743 sentences) and a training set (541 documents, 12,010 sentences).", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.9457998275756836}]}, {"text": "Since expression boundaries are hard to define exactly in annotation guidelines ( ), we used soft precision and recall measures to score the quality of the system output.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.8975264430046082}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9985301494598389}]}, {"text": "To derive the soft precision and recall, we first define the span coverage c of a span s with respect to another span s \ud97b\udf59 , which measures how well s \ud97b\udf59 is covered by s: In this formula, the operator | \u00b7 | counts tokens, and the intersection \u2229 gives the set of tokens that two spans have in common.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9611352682113647}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.999358594417572}]}, {"text": "Since our evaluation takes span labels (DSE, ESE, OSE) into account, we set c(s, s \ud97b\udf59 ) to zero if the labels associated with sand s \ud97b\udf59 are different.", "labels": [], "entities": [{"text": "OSE", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.7270849943161011}]}, {"text": "Using the span coverage, we define the span set coverage C of a set of spans S with respect to a set S \ud97b\udf59 : We now define the soft precision P and recall R of a proposed set of spans\u02c6Sspans\u02c6 spans\u02c6S with respect to a gold standard set S as follows:", "labels": [], "entities": [{"text": "soft precision P", "start_pos": 125, "end_pos": 141, "type": "METRIC", "confidence": 0.772138794263204}]}], "tableCaptions": [{"text": " Table 1: Evaluation of reranking architectures and  learning methods.", "labels": [], "entities": []}, {"text": " Table 2: Oracle and reranker performance as a  function of candidate set size.", "labels": [], "entities": []}, {"text": " Table 3: Effect of syntactic features.", "labels": [], "entities": []}, {"text": " Table 4: Effect of semantic features.", "labels": [], "entities": []}, {"text": " Table 5: Structural features compared to label  pairs.", "labels": [], "entities": []}, {"text": " Table 6: Results using the Breck et al. (2007) eval- uation setting.", "labels": [], "entities": []}]}