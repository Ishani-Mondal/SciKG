{"title": [{"text": "Building a Korean Web Corpus for Analyzing Learner Language", "labels": [], "entities": [{"text": "Korean Web Corpus", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.9148537913958231}, {"text": "Analyzing Learner Language", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.6229311724503835}]}], "abstractContent": [{"text": "Post-positional particles area significant source of errors for learners of Korean.", "labels": [], "entities": []}, {"text": "Following methodology that has proven effective in handling English preposition errors, we are beginning the process of building a machine learner for particle error detection in L2 Korean writing.", "labels": [], "entities": [{"text": "particle error detection", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.6864921450614929}]}, {"text": "As a first step, however, we must acquire data, and thus we present a methodology for constructing large-scale corpora of Korean from the Web, exploring the feasibility of building corpora appropriate fora given topic and grammatical construction.", "labels": [], "entities": []}], "introductionContent": [{"text": "Applications for assisting second language learners can be extremely useful when they make learners more aware of the non-native characteristics in their writing ().", "labels": [], "entities": []}, {"text": "Certain constructions, such as English prepositions, are difficult to characterize by grammar rules and thus are wellsuited for machine learning approaches.", "labels": [], "entities": []}, {"text": "Machine learning techniques are relatively portable to new languages, but new languages bring issues in terms of defining the language learning problem and in terms of acquiring appropriate data for training a machine learner.", "labels": [], "entities": []}, {"text": "We focus in this paper mainly on acquiring data for training a machine learning system.", "labels": [], "entities": []}, {"text": "In particular, we are interested in situations where the task is constant-e.g., detecting grammatical errors in particles-but the domain might fluctuate.", "labels": [], "entities": [{"text": "detecting grammatical errors in particles-but", "start_pos": 80, "end_pos": 125, "type": "TASK", "confidence": 0.824647867679596}]}, {"text": "This is the case when a learner is asked to write an essay on a prompt (e.g., \"What do you hope to do in life?\"), and the prompts may vary by student, by semester, by instructor, etc.", "labels": [], "entities": []}, {"text": "By isolating a particular domain, we can hope for greater degrees of accuracy; see, for example, the high accuracies for domain-specific grammar correction in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9979385733604431}, {"text": "domain-specific grammar correction", "start_pos": 121, "end_pos": 155, "type": "TASK", "confidence": 0.6657137274742126}]}, {"text": "In this situation, we face the challenge of obtaining data which is appropriate both for: a) the topic the learners are writing about, and b) the linguistic construction of interest, i.e., containing enough relevant instances.", "labels": [], "entities": []}, {"text": "In the ideal case, one could build a corpus directly for the types of learner data to analyze.", "labels": [], "entities": []}, {"text": "Luckily, using the web as a data source can provide such specialized corpora (), in addition to larger, more general corpora.", "labels": [], "entities": []}, {"text": "A crucial question, though, is how one goes about designing the right web corpus for analyzing learner language (see, e.g.,, for other contexts) The area of difficulty for language learners which we focus on is that of Korean post-positional particles, akin to English prepositions ().", "labels": [], "entities": []}, {"text": "Korean is an important language to develop NLP techniques for (see, e.g., discussion in, presenting a variety of features which are less prevalent in many Western languages, such as agglutinative morphology, a rich system of case marking, and relatively free word order.", "labels": [], "entities": []}, {"text": "Obtaining data is important in the general case, as non-English languages tend to lack resources.", "labels": [], "entities": []}, {"text": "The correct usage of Korean particles relies on knowing lexical, syntactic, semantic, and discourse information (), which makes this challenging for both learners and machines (cf. En-glish determiners in).", "labels": [], "entities": []}, {"text": "The only other approach we know of, a parser-based one, had very low precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9972188472747803}]}, {"text": "A secondary contribution of this work is thus defining the particle error detection problem fora machine learner.", "labels": [], "entities": [{"text": "particle error detection", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.7344165047009786}]}, {"text": "It is important that the data represent the relationships between specific lexical items: in the comparable English case, for example, interest is usually found with in: interest in/*with learning.", "labels": [], "entities": []}, {"text": "The basic framework we employ is to train a machine learner on correct Korean data and then apply this system to learner text, to predict correct particle usage, which may differ from the learner's (cf..", "labels": [], "entities": []}, {"text": "After describing the grammatical properties of particles in section 2, we turn to the general approach for obtaining relevant web data in section 3, reporting basic statistics for our corpora in section 4.", "labels": [], "entities": []}, {"text": "We outline the machine learing set-up in section 5 and present initial results in section 6.", "labels": [], "entities": []}, {"text": "These results help evaluate the best way to build specialized corpora for learner language.", "labels": [], "entities": []}], "datasetContent": [{"text": "In tandem with the basic statistics, it is also important to gauge the quality of the Korean data from a more qualitative perspective.", "labels": [], "entities": [{"text": "Korean data", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.8184773921966553}]}, {"text": "Thus, we examined the 120 3-tuple F 1 corpus and discovered a number of problems with the data.", "labels": [], "entities": [{"text": "120 3-tuple F 1 corpus", "start_pos": 22, "end_pos": 44, "type": "DATASET", "confidence": 0.6103191554546357}]}, {"text": "First, there are issues concerning collecting data which is not pure Korean.", "labels": [], "entities": []}, {"text": "We find data extracted from Chinese travel sites, where there is a mixture of non-standard foreign words and unnatural-sounding translated words in Korean.", "labels": [], "entities": []}, {"text": "Ironically, we also find learner data of Korean in our search for correct Korean data.", "labels": [], "entities": []}, {"text": "Secondly, there are topics which, while exhibiting valid forms of Korean, are too far afield from what we expect learners to know, including religious sites with rare expressions; poems, which commonly drop particles; gambling sites; and so forth.", "labels": [], "entities": []}, {"text": "Finally, there are cases of ungrammatical uses of Korean, which are used in specific contexts not appropriate for our purposes.", "labels": [], "entities": []}, {"text": "These include newspaper titles, lists of personal names and addresses, and incomplete phrases from advertisements and chats.", "labels": [], "entities": []}, {"text": "In these cases, we tend to find less particles.", "labels": [], "entities": []}, {"text": "Based on these properties, we developed the aforementioned second focused corpus with more advanced Korean words and examined the 240 3-tuple F 2 corpus.", "labels": [], "entities": []}, {"text": "The F 2 seeds allow us to capture a greater percentage of well-formed data, namely data from news articles, encyclopedic texts, and blogs about more serious topics such as politics, literature, and economics.", "labels": [], "entities": [{"text": "F 2 seeds", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.7832744121551514}]}, {"text": "While some of this data might be above learners' heads, it is, for the most part, wellformed native-like Korean.", "labels": [], "entities": []}, {"text": "Also, the inclusion of learner data has been dramatically reduced.", "labels": [], "entities": []}, {"text": "However, some of the same problems from the F 1 corpus persist, namely the inclusion of poetry, newspaper titles, religious text, and non-Korean data.", "labels": [], "entities": [{"text": "F 1 corpus", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.8692258993784586}]}, {"text": "Based on this qualitative analysis, it is clear that we need to filter out more data than is currently being filtered, in order to obtain valid Korean of a type which uses a sufficient number of particles in grammatical ways.", "labels": [], "entities": []}, {"text": "In the future, we plan on restricting the genre, filtering based on the number of rare words (e.g., religious words), and using a trigram language model to check the validity.", "labels": [], "entities": []}, {"text": "Note that one might consider building even larger corpora from the start and using the filtering step to winnow down the corpus fora particular application, such as particle error detection.", "labels": [], "entities": [{"text": "particle error detection", "start_pos": 165, "end_pos": 189, "type": "TASK", "confidence": 0.7656795183817545}]}, {"text": "However, while removing ungrammatical Korean is a process of removing noise, identifying whether a corpus is about traveling, for example, is a content-based decision.", "labels": [], "entities": []}, {"text": "Given that this is what a search engine is designed to do, we prefer filtering based only on grammatical and genre properties.", "labels": [], "entities": []}, {"text": "We evaluate the web corpora for the task of predicting particle usage, after describing the test corpus.", "labels": [], "entities": [{"text": "predicting particle usage", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.8994210561116537}]}], "tableCaptions": [{"text": " Table 1: Number of corpora based on parameters", "labels": [], "entities": []}, {"text": " Table 2: Basic statistics of different web corpora", "labels": [], "entities": []}, {"text": " Table 3: Results of guessing particle existence, training  with different corpora", "labels": [], "entities": [{"text": "guessing particle existence", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.8112615545590719}]}]}