{"title": [{"text": "Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.6801333725452423}, {"text": "Machine Translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7209109514951706}]}], "abstractContent": [{"text": "This paper presents the results of the WMT10 and MetricsMATR10 shared tasks, 1 which included a translation task, a system combination task, and an evaluation task.", "labels": [], "entities": [{"text": "WMT10", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.7889377474784851}, {"text": "translation task", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.890030562877655}]}, {"text": "We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7425522804260254}]}, {"text": "We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics.", "labels": [], "entities": []}, {"text": "This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon's Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 112, "end_pos": 136, "type": "DATASET", "confidence": 0.883187860250473}]}], "introductionContent": [{"text": "This paper presents the results of the shared tasks of the joint Workshop on statistical Machine Translation (WMT) and Metrics for MAchine TRanslation (MetricsMATR), which was held at ACL 2010.", "labels": [], "entities": [{"text": "statistical Machine Translation (WMT)", "start_pos": 77, "end_pos": 114, "type": "TASK", "confidence": 0.7817548364400864}, {"text": "ACL 2010", "start_pos": 184, "end_pos": 192, "type": "DATASET", "confidence": 0.9400504529476166}]}, {"text": "This builds on four previous WMT workshops (, and one previous MetricsMATR meeting (.", "labels": [], "entities": [{"text": "WMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.6379322409629822}]}, {"text": "There were three shared tasks this year: a translation task between English and four other European languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics.", "labels": [], "entities": [{"text": "translation task", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.9011934101581573}]}, {"text": "The The MetricsMATR analysis was not complete in time for the publication deadline.", "labels": [], "entities": [{"text": "The MetricsMATR analysis", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.7271990279356638}]}, {"text": "An updated version of paper will be made available on http://statmt.org/wmt10/ prior to performance on each of these shared task was determined after a comprehensive human evaluation.", "labels": [], "entities": []}, {"text": "There were a number of differences between this year's workshop and last year's workshop: \u2022 Non-expert judgments -In addition to having shared task participants judge translation quality, we also collected judgments from non-expert annotators hired through Amazon's Mechanical Turk.", "labels": [], "entities": []}, {"text": "By collecting a large number of judgments we hope to reduce the burden on shared task participants, and to increase the statistical significance of our findings.", "labels": [], "entities": []}, {"text": "We discuss the feasibility of using nonexperts evaluators, by analyzing the cost, volume and quality of non-expert annotations.", "labels": [], "entities": []}, {"text": "\u2022 Clearer results for system combinationThis year we excluded Google translations from the systems used in system combination.", "labels": [], "entities": [{"text": "Clearer", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.6448000073432922}]}, {"text": "In last year's evaluation, the large margin between Google and many of the other systems meant that it was hard to improve on when combining systems.", "labels": [], "entities": []}, {"text": "This year, the system combinations perform better than their component systems more often than last year.", "labels": [], "entities": []}, {"text": "\u2022 Fewer rule-based systems -This year there were fewer rule-based systems submitted.", "labels": [], "entities": []}, {"text": "In past years, University of Saarland compiled a large set of outputs from rule-based machine translation (RBMT) systems.", "labels": [], "entities": [{"text": "rule-based machine translation (RBMT)", "start_pos": 75, "end_pos": 112, "type": "TASK", "confidence": 0.7940416932106018}]}, {"text": "The RBMT systems were not submitted this year.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.6356501579284668}]}, {"text": "This is unfortunate, because they tended to outperform the statistical systems for German, and they were often difficult to rank properly using automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "The primary objectives of this workshop are to evaluate the state of the art in machine transla-tion, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation.", "labels": [], "entities": [{"text": "machine transla-tion", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.7677147090435028}, {"text": "machine translation", "start_pos": 238, "end_pos": 257, "type": "TASK", "confidence": 0.8269742727279663}]}, {"text": "As with past years, all of the data, translations, and human judgments produced for our workshop are publicly available.", "labels": [], "entities": []}, {"text": "We hope they form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.690511037906011}, {"text": "system combination", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7302986085414886}]}], "datasetContent": [{"text": "In addition to allowing the analysis of subjective translation quality measures for different systems, the judgments gathered during the manual evaluation maybe used to evaluate how well the automatic evaluation metrics serve as a surrogate to the manual evaluation processes.", "labels": [], "entities": []}, {"text": "NIST began running a \"Metrics for MAchine TRanslation\" challenge (MetricsMATR), and presented their findings at a workshop at AMTA (.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9702113270759583}, {"text": "MAchine TRanslation\" challenge (MetricsMATR)", "start_pos": 34, "end_pos": 78, "type": "TASK", "confidence": 0.4900730081966945}, {"text": "AMTA", "start_pos": 126, "end_pos": 130, "type": "DATASET", "confidence": 0.9238697290420532}]}, {"text": "This year we conducted a joint Metrics-MATR and WMT workshop, with NIST running the shared evaluation task and analyzing the results.", "labels": [], "entities": [{"text": "WMT", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.5623873472213745}, {"text": "NIST", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.8877108097076416}]}, {"text": "In this year's shared evaluation task 14 different research groups submitted a total of 26 different automatic metrics for evaluation: Aalto University of \u2022 MT-NCD -A machine translation metric based on normalized compression distance (NCD), a general information-theoretic measure of string similarity.", "labels": [], "entities": [{"text": "Aalto University of \u2022 MT-NCD", "start_pos": 135, "end_pos": 163, "type": "DATASET", "confidence": 0.8550965785980225}, {"text": "machine translation", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.6892224848270416}, {"text": "normalized compression distance (NCD)", "start_pos": 203, "end_pos": 240, "type": "METRIC", "confidence": 0.7437261492013931}]}, {"text": "MT-NCD measures the surface level similarity between two strings with a general compression algorithm.", "labels": [], "entities": []}, {"text": "More similar strings can be represented with Systems are listed in the order of how often their translations were ranked higher than or equal to any other system.", "labels": [], "entities": []}, {"text": "Ties are broken by direct comparison.", "labels": [], "entities": [{"text": "Ties", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7912004590034485}]}, {"text": "indicates constrained condition, meaning only using the supplied training data, standard monolingual linguistic tools, and optionally the LDC's GigaWord, which was allowed this year (entries that used the GigaWord are marked +GW).", "labels": [], "entities": [{"text": "LDC", "start_pos": 138, "end_pos": 141, "type": "DATASET", "confidence": 0.9083827137947083}]}, {"text": "\u2022 indicates a win in the category, meaning that no other system is statistically significantly better at p-level\u22640.1 in pairwise comparison.", "labels": [], "entities": []}, {"text": "indicates a constrained win, no other constrained system is statistically better.", "labels": [], "entities": []}, {"text": "For all pairwise comparisons between systems, please check the appendix.", "labels": [], "entities": []}, {"text": "System combinations are listed in the order of how often their translations were ranked higher than or equal to any other system.", "labels": [], "entities": []}, {"text": "Ties are broken by direct comparison.", "labels": [], "entities": [{"text": "Ties", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7912004590034485}]}, {"text": "We show the best individual systems alongside the system combinations, since the goal of combination is to produce better quality translation than the component systems.", "labels": [], "entities": []}, {"text": "\u2022 indicates a win for the system combination meaning that no other system or system combination is statistically significantly better at p-level\u22640.1 in pairwise comparison.", "labels": [], "entities": []}, {"text": "indicates an individual system that none of the system combinations beat by a statistically significant margin at plevel\u22640.1.", "labels": [], "entities": []}, {"text": "For all pairwise comparisons between systems, please check the appendix.", "labels": [], "entities": []}, {"text": "Note: ONLINEA and ONLINEB were not included among the systems being combined in the system combination shared tasks, except in the Czech-English and English-Czech conditions, where ONLINEB was included.", "labels": [], "entities": [{"text": "ONLINEA", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.9891275763511658}, {"text": "ONLINEB", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9695799350738525}, {"text": "ONLINEB", "start_pos": 181, "end_pos": 188, "type": "METRIC", "confidence": 0.9499149918556213}]}, {"text": "English-German  The results reported here are preliminary; a final release of results will be published on the WMT10 website before July 15, 2010.", "labels": [], "entities": [{"text": "WMT10 website", "start_pos": 111, "end_pos": 124, "type": "DATASET", "confidence": 0.9716410636901855}]}, {"text": "Metric developers submitted metrics for installation at NIST; they were also asked to submit metric scores on the WMT10 test set along with their metrics.", "labels": [], "entities": [{"text": "NIST", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9639569520950317}, {"text": "WMT10 test set", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.9765229821205139}]}, {"text": "Not all developers submitted scores, and not all metrics were verified to produce the same scores as submitted at NIST in time for publication.", "labels": [], "entities": [{"text": "NIST", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.9559672474861145}]}, {"text": "Any such caveats are reported with the description of the metrics above.", "labels": [], "entities": []}, {"text": "The results reported here are limited to a comparison of metric scores on the full WMT10 test set with human assessments on the humanassessed subset.", "labels": [], "entities": [{"text": "WMT10 test set", "start_pos": 83, "end_pos": 97, "type": "DATASET", "confidence": 0.9032451510429382}]}, {"text": "An analysis comparing the human assessments with the automatic metrics run only on the human-assessed subset will follow at a later date.", "labels": [], "entities": []}, {"text": "The WMT10 system output used to generate the reported metric scores was found to have improperly escaped characters fora small number of segments.", "labels": [], "entities": [{"text": "WMT10 system output", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9299316803614298}]}, {"text": "While we plan to regenerate the metric scores with this issue resolved, we do not expect this to significantly alter the results, given the small number of segments affected.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Inter-and intra-annotator agreement for  the sentence ranking task. In this task, P (E) is  0.333.", "labels": [], "entities": [{"text": "sentence ranking task", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.7954412698745728}, {"text": "P (E)", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.9636989086866379}]}, {"text": " Table 9: The segment-level performance for metrics for the into-English direction.", "labels": [], "entities": []}, {"text": " Table 11: Inter-and intra-annotator agreement for  the MTurk workers on the sentence ranking task.  (As before, P (E) is 0.333.) For comparison, we  repeat here the kappa coefficients of the experts  (K  *  ), taken from", "labels": [], "entities": [{"text": "sentence ranking task", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.80669105052948}, {"text": "P (E)", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.9423540383577347}]}, {"text": " Table 13: Sentence-level ranking for the WMT10 French-English News Task", "labels": [], "entities": [{"text": "WMT10 French-English News Task", "start_pos": 42, "end_pos": 72, "type": "DATASET", "confidence": 0.9254535138607025}]}, {"text": " Table 14: Sentence-level ranking for the WMT10 English-French News Task", "labels": [], "entities": [{"text": "WMT10 English-French News Task", "start_pos": 42, "end_pos": 72, "type": "DATASET", "confidence": 0.9398060441017151}]}, {"text": " Table 15: Sentence-level ranking for the WMT10 German-English News Task", "labels": [], "entities": [{"text": "Sentence-level", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9389399886131287}, {"text": "WMT10 German-English News Task", "start_pos": 42, "end_pos": 72, "type": "DATASET", "confidence": 0.915041908621788}]}, {"text": " Table 16: Sentence-level ranking for the WMT10 English-German News Task", "labels": [], "entities": [{"text": "WMT10 English-German News Task", "start_pos": 42, "end_pos": 72, "type": "DATASET", "confidence": 0.9274075925350189}]}, {"text": " Table 17: Sentence-level ranking for the WMT10 Spanish-English News Task", "labels": [], "entities": [{"text": "WMT10 Spanish-English News Task", "start_pos": 42, "end_pos": 73, "type": "DATASET", "confidence": 0.939674898982048}]}, {"text": " Table 18: Sentence-level ranking for the WMT10 English-Spanish News Task", "labels": [], "entities": [{"text": "WMT10 English-Spanish News Task", "start_pos": 42, "end_pos": 73, "type": "DATASET", "confidence": 0.9479122459888458}]}, {"text": " Table 19: Sentence-level ranking for the WMT10 Czech-English News Task", "labels": [], "entities": [{"text": "Sentence-level", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9333522915840149}, {"text": "WMT10 Czech-English News Task", "start_pos": 42, "end_pos": 71, "type": "DATASET", "confidence": 0.9276218861341476}]}, {"text": " Table 20: Sentence-level ranking for the WMT10 English-Czech News Task", "labels": [], "entities": [{"text": "WMT10 English-Czech News Task", "start_pos": 42, "end_pos": 71, "type": "DATASET", "confidence": 0.9492293447256088}]}, {"text": " Table 22: Sentence-level ranking for the WMT10 German-English News Task (Combining expert and  non-expert Mechanical Turk judgments)", "labels": [], "entities": [{"text": "WMT10 German-English News Task", "start_pos": 42, "end_pos": 72, "type": "DATASET", "confidence": 0.8499232977628708}]}, {"text": " Table 23: Sentence-level ranking for the WMT10 Spanish-English News Task (Combining expert and  non-expert Mechanical Turk judgments)", "labels": [], "entities": [{"text": "WMT10 Spanish-English News Task", "start_pos": 42, "end_pos": 73, "type": "DATASET", "confidence": 0.8871770352125168}]}]}