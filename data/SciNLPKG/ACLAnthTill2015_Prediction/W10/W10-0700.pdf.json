{"title": [], "abstractContent": [{"text": "Preface The NAACL-2010 Workshop on Creating Speech and Language Data With Amazon's Mechanical Turk explores applications of crowdsourcing technologies for the creation and study of language data.", "labels": [], "entities": []}, {"text": "Recent work has evaluated the effectiveness of using crowdsourcing platforms, such as Amazon's Mechanical Turk, to create annotated data for natural language processing applications.", "labels": [], "entities": []}, {"text": "This workshop further explores this area and these proceedings contain 34 papers and an overview paper that each experiment with applications of Mechanical Turk.", "labels": [], "entities": []}, {"text": "The diversity of applications showcases the new possibilities for annotating speech and text, and has the potential to dramatically change how we create data for human language technologies.", "labels": [], "entities": []}, {"text": "Papers in the workshop also looked at best practices in creating data using Mechanical Turk.", "labels": [], "entities": []}, {"text": "Experiments evaluated how to design Human Intelligence Tasks (HITs), how to attract users to the task, how to price annotation tasks, and how to ensure data quality.", "labels": [], "entities": []}, {"text": "Applications include the creation of data sets for standard NLP tasks, developing entirely new tasks, and investigating new ways of integrating user feedback in the learning process.", "labels": [], "entities": []}, {"text": "The workshop featured an open-ended shared task in which 35 teams were awarded $100 of credit on Amazon Mechanical Turk to spend on an annotation task of their choosing.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.9111362099647522}]}, {"text": "Results of the shared task are described in short papers and all collected data is publicly available.", "labels": [], "entities": []}, {"text": "Shared task participants focused on data collection questions, such as how to convey complex tasks to non-experts, how to evaluate and ensure quality and annotation cost and speed.", "labels": [], "entities": []}, {"text": "The organizers thank the workshop participants who contributed to an incredibly strong workshop program.", "labels": [], "entities": []}, {"text": "We also thank the program committee for quickly reviewing the large number of submissions.", "labels": [], "entities": []}, {"text": "Special thanks go to Sharon Chiarella, vice president of Amazon Mechanical Turk, for funding the shared task, Ted Sandler of Amazon for assistance in organizing the shared task, Stephanie Geerlings and Lukas Biewald of CrowdFlower for making their service available to shared task participants, and to Jonny Weese for editing and compiling the final proceedings.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}