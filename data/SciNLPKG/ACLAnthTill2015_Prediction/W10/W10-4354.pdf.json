{"title": [{"text": "F 2 -New Technique for Recognition of User Emotional States in Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Recognition of User Emotional States", "start_pos": 23, "end_pos": 59, "type": "TASK", "confidence": 0.8065336108207702}]}], "abstractContent": [{"text": "In this paper we propose anew technique to enhance emotion recognition by combining in different ways what we call emotion predictions.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7693105340003967}]}, {"text": "The technique is called F 2 as the combination is based on a double fusion process.", "labels": [], "entities": [{"text": "F 2", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9160121381282806}]}, {"text": "The input to the first fusion phase is the output of a number of classifiers which deal with different types of information regarding each sentence uttered by the user.", "labels": [], "entities": []}, {"text": "The output of this process is the input to the second fusion stage, which provides as output the most likely emotional category.", "labels": [], "entities": []}, {"text": "Experiments have been carried out using a previously developed spoken dialogue system designed for the fast food domain.", "labels": [], "entities": []}, {"text": "Results obtained considering three and two emotional categories show that our technique outperforms the standard single fusion technique by 2.25% and 3.35% absolute, respectively .", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic recognition of user emotional states is a very challenging task that has attracted the attention of the research community for several decades.", "labels": [], "entities": [{"text": "Automatic recognition of user emotional states", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8228072623411814}]}, {"text": "The goal is to design methods to make computers interact more naturally with human beings.", "labels": [], "entities": []}, {"text": "This is a very complex task due to a variety of reasons.", "labels": [], "entities": []}, {"text": "One is the absence of a generally agreed definition of emotion and of qualitatively different types of emotion.", "labels": [], "entities": []}, {"text": "Another is that we still have an incomplete understanding of how humans process emotions, as even people have difficulty in distinguishing between them.", "labels": [], "entities": []}, {"text": "Thus, in many cases a given emotion is perceived differently by different people.", "labels": [], "entities": []}, {"text": "Studies in emotion recognition made by the research community have been applied to enhance the quality or efficiency of several services provided by computers.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7765076160430908}]}, {"text": "For example, these have been applied to spoken dialogue systems (SDSs) used in automated call-centres, where the goal is to detect problems in the interaction and, if appropriate, transfer the call automatically to a human operator.", "labels": [], "entities": [{"text": "spoken dialogue systems (SDSs)", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6883198320865631}]}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 addresses related work on the application of emotion recognition to SDSs.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.751932680606842}, {"text": "SDSs", "start_pos": 78, "end_pos": 82, "type": "TASK", "confidence": 0.8037572503089905}]}, {"text": "Section 3 focuses on the proposed technique, describing the classifiers and fusion methods employed in the current implementation.", "labels": [], "entities": []}, {"text": "Section 4 discusses our speech database and its emotional annotation.", "labels": [], "entities": []}, {"text": "Section 5 presents the experiments, comparing results obtained using the standard single fusion technique with the proposed double fusion.", "labels": [], "entities": []}, {"text": "Finally, Section 6 presents the conclusions and outlines possibilities for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main goal of the experiments has been to test the proposed technique using our emotional speech database, and employing: \u2022 Three emotional categories (Neutral, Angry and Tired) on the one hand, and two emotional categories (Non-negative and Negative) on the other.", "labels": [], "entities": []}, {"text": "The experiments employing the former category set will be called 3-emotion experiments, whereas those employing the latter category will be called 2-emotion experiments.", "labels": [], "entities": []}, {"text": "\u2022 The four classifiers described in Section 3.1, and the three fusion methods discussed in Section 3.2.", "labels": [], "entities": []}, {"text": "In the 3-emotion experiments we consider that an input utterance is correctly classified if the emotional category deduced by the technique matches the label assigned to the utterance.", "labels": [], "entities": []}, {"text": "In the 2-emotion experiments, the utterance is considered to be correctly classified if either the deduced emotional category is Nonnegative and the label is Neutral, or the category is Negative and the label is Tired or Angry.", "labels": [], "entities": []}, {"text": "To carryout training and testing we have used a script that takes as its input a set of labelled dialogues in a corpus, and processes each dialogue by locating within it, from the beginning to the end, each prompt of the Saplen system, the voice samples file that contains the user's response to the prompt, and the result provided by the system's speech recogniser (sentence in text format).", "labels": [], "entities": []}, {"text": "The type of each prompt is used to create a sequence of dialogue acts of length L, which is the input to the dialogue acts classifier.", "labels": [], "entities": []}, {"text": "The voice samples file is the input to the prosodic and acoustic classifiers, and the speech recognition result is the input to the lexical classifier.", "labels": [], "entities": []}, {"text": "This procedure is repeated for all the dialogues in the corpus.", "labels": [], "entities": []}, {"text": "Experimental results have been obtained using 5-fold cross-validation, with each partition containing the utterances corresponding to 88 different dialogues in the corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of Fusion-0 (results in %).", "labels": [], "entities": []}, {"text": " Table 2: Performance of Fusion-1 (results in %).", "labels": [], "entities": []}]}