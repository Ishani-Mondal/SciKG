{"title": [{"text": "Online Entropy-based Model of Lexical Category Acquisition", "labels": [], "entities": [{"text": "Lexical Category Acquisition", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.6439719100793203}]}], "abstractContent": [{"text": "Children learn a robust representation of lexical categories at a young age.", "labels": [], "entities": []}, {"text": "We propose an incremental model of this process which efficiently groups words into lexical categories based on their local context using an information-theoretic criterion.", "labels": [], "entities": []}, {"text": "We train our model on a corpus of child-directed speech from CHILDES and show that the model learns a fine-grained set of intuitive word categories.", "labels": [], "entities": []}, {"text": "Furthermore, we propose a novel evaluation approach by comparing the efficiency of our induced categories against other category sets (in-cluding traditional part of speech tags) in a variety of language tasks.", "labels": [], "entities": []}, {"text": "We show the categories induced by our model typically outperform the other category sets.", "labels": [], "entities": []}, {"text": "1 The Acquisition of Lexical Categories Psycholinguistic studies suggest that early on children acquire robust knowledge of the abstract lexical categories such as nouns, verbs and deter-miners (e.g., Gelman & Taylor, 1984; Kemp et al., 2005).", "labels": [], "entities": []}, {"text": "Children's grouping of words into categories might be based on various cues, including phonological and morphological properties of a word, the distributional information about its surrounding context, and its semantic features.", "labels": [], "entities": [{"text": "grouping of words into categories", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.7940620899200439}]}, {"text": "Among these, the distributional properties of the local context of a word have been thoroughly studied.", "labels": [], "entities": []}, {"text": "It has been shown that child-directed speech provides informative co-occurrence cues, which can be reliably used to form lexical categories (Redington et al., 1998; Mintz, 2003).", "labels": [], "entities": []}, {"text": "The process of learning lexical categories by children is necessarily incremental.", "labels": [], "entities": []}, {"text": "Human language acquisition is bounded by memory and processing limitations, and it is implausible that humans process large volumes of text at once and induce an optimum set of categories.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.758523553609848}]}, {"text": "Efficient on-line computational models are needed to investigate whether distributional information is equally useful in an online process of word categoriza-tion.", "labels": [], "entities": []}, {"text": "However, the few incremental models of category acquisition which have been proposed so far are generally inefficient and over-sensitive to the properties of the input data (Cartwright & Brent, 1997; Parisien et al., 2008).", "labels": [], "entities": [{"text": "category acquisition", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8452939689159393}]}, {"text": "Moreover, the unsupervised nature of these models makes their assessment a challenge, and the evaluation techniques proposed in the literature are limited.", "labels": [], "entities": []}, {"text": "The main contributions of our research are twofold.", "labels": [], "entities": []}, {"text": "First, we propose an incremental en-tropy model for efficiently clustering words into categories given their local context.", "labels": [], "entities": []}, {"text": "We train our model on a corpus of child-directed speech from CHILDES (MacWhinney, 2000) and show that the model learns a fine-grained set of intuitive word categories.", "labels": [], "entities": [{"text": "CHILDES (MacWhinney, 2000)", "start_pos": 61, "end_pos": 87, "type": "DATASET", "confidence": 0.7925601005554199}]}, {"text": "Second, we propose a novel evaluation approach by comparing the efficiency of our induced categories against other category sets, including the traditional part of speech tags, in a variety of language tasks.", "labels": [], "entities": []}, {"text": "We evaluate our model on word prediction (where a missing word is guessed based on its sentential context), semantic inference (where the semantic properties of a novel word are predicted based on the context), and grammatical-ity judgment (where the syntactic well-formedness of a sentence is assessed based on the category labels assigned to its words).", "labels": [], "entities": [{"text": "word prediction", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.775825560092926}]}, {"text": "The results show that the categories induced by our model can be successfully used in a variety of tasks and typically perform better than other category sets.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "There is no standard and straightforward method for evaluating the unsupervised models of category learning (see, for discussion).", "labels": [], "entities": []}, {"text": "Many unsupervised models of lexical category acquisition treat the traditional part of speech (PoS) tags as the gold standard, and measure the accuracy and completeness of their induced categories based on how closely they resemble the PoS categories (e.g..", "labels": [], "entities": [{"text": "lexical category acquisition", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.7047757903734843}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.997992753982544}]}, {"text": "However, it is not at all clear whether humans form the same types of categories.", "labels": [], "entities": []}, {"text": "In fact, many language tasks might benefit from finer-grained categories than the traditional PoS tags used for corpus annotation.", "labels": [], "entities": []}, {"text": "propose a different, automatically generated set of gold standard categories for evaluating an unsupervised categorization model.", "labels": [], "entities": []}, {"text": "The gold-standard categories are formed according to \"substitutability\": if one word can be replaced by another and the resulting sentence is still grammatical, then there is a good chance that the two words belong to the same category.", "labels": [], "entities": []}, {"text": "They extract 3-word frames from the training data, and form the gold standard categories based on the words that appear in the same frame.", "labels": [], "entities": []}, {"text": "They emphasize that in order to provide some degree of generalization, different data sets must be used for forming the gold-standard categories and performing the evaluation.", "labels": [], "entities": []}, {"text": "However, the resulting categories are bound to be incomplete, and using them as gold standard inevitably favors categorization models which use a similar frame-based principle.", "labels": [], "entities": []}, {"text": "All in all, using any set of gold standard categories for evaluating an unsupervised categorization model has the disadvantage of favoring one set of principles and intuitions over another; that is, assuming that there is a correct set of categories which the model should converge to.", "labels": [], "entities": []}, {"text": "Alternatively, automatically induced categories can be evaluated based on how useful they are in performing different tasks.", "labels": [], "entities": []}, {"text": "This approach is taken by, where the perplexity of a finite-state model is used to compare different category sets.", "labels": [], "entities": []}, {"text": "We build on this idea and propose a more general usage-based approach to evaluating the automatically induced categories from a data set, emphasizing that the ultimate goal of a category induction model is to form categories that can be efficiently used in a variety of language tasks.", "labels": [], "entities": []}, {"text": "We argue that for such tasks, a finer-grained set of cat-egories might be more appropriate than the coarsegrained PoS categories.", "labels": [], "entities": []}, {"text": "Therefore, we propose a number of tasks for which we compare the performance based on various category sets, including those induced by our model.", "labels": [], "entities": []}, {"text": "We evaluate the categories formed by our model through three different tasks.", "labels": [], "entities": []}, {"text": "The first task is word prediction, where a target word is predicted based on the sentential context it appears in.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.840019702911377}]}, {"text": "The second task is to infer the semantic properties of a novel word based on its context.", "labels": [], "entities": []}, {"text": "The third task is to assess the grammaticality of a sentence tagged with category labels.", "labels": [], "entities": []}, {"text": "We run our model on a corpus of child-directed speech, and use the categories that it induces from that corpus in the above-mentioned tasks.", "labels": [], "entities": []}, {"text": "For each task, we compare the performance using our induced categories against the performance using other category sets.", "labels": [], "entities": []}, {"text": "In the following sections, we describe the properties of the data sets used for training and testing the model, and the formation of other category sets against which we compare our model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison against gold PoS tags using  Variation of Information (VI) and Adjusted Rand  Index (ARI).", "labels": [], "entities": [{"text": "Adjusted Rand  Index (ARI)", "start_pos": 84, "end_pos": 110, "type": "METRIC", "confidence": 0.918902983268102}]}, {"text": " Table 4: The performance in each of the three tasks using different category sets.", "labels": [], "entities": []}]}