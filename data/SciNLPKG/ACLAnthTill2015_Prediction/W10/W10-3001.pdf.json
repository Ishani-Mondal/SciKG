{"title": [{"text": "The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text", "labels": [], "entities": [{"text": "Learning to Detect Hedges and their Scope in Natural Language Text", "start_pos": 28, "end_pos": 94, "type": "TASK", "confidence": 0.7681173438375647}]}], "abstractContent": [{"text": "The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.", "labels": [], "entities": []}, {"text": "The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction.", "labels": [], "entities": [{"text": "distinguishing factual and uncertain information in texts", "start_pos": 41, "end_pos": 98, "type": "TASK", "confidence": 0.74727806023189}, {"text": "information extraction", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.8670553863048553}]}, {"text": "This paper provides a general overview of the shared task, including the annotation protocols of the training and evaluation datasets, the exact task definitions, the evaluation metrics employed and the overall results.", "labels": [], "entities": []}, {"text": "The paper concludes with an analysis of the prominent approaches and an overview of the systems submitted to the shared task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Every year since 1999, the Conference on Computational Natural Language Learning (CoNLL) provides a competitive shared task for the Computational Linguistics community.", "labels": [], "entities": [{"text": "Conference on Computational Natural Language Learning (CoNLL)", "start_pos": 27, "end_pos": 88, "type": "TASK", "confidence": 0.6652191446887122}]}, {"text": "After a fiveyear period of multi-language semantic role labeling and syntactic dependency parsing tasks, anew task was introduced in 2010, namely the detection of uncertainty and its linguistic scope in natural language sentences.", "labels": [], "entities": [{"text": "multi-language semantic role labeling", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.618003286421299}, {"text": "syntactic dependency parsing", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.6822313169638315}, {"text": "detection of uncertainty", "start_pos": 150, "end_pos": 174, "type": "TASK", "confidence": 0.8510862588882446}]}, {"text": "In natural language processing (NLP) -and in particular, in information extraction (IE) -many applications seek to extract factual information from text.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.8318593899408976}, {"text": "information extraction (IE)", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.8879133820533752}]}, {"text": "In order to distinguish facts from unreliable or uncertain information, linguistic devices such as hedges (indicating that authors do not or cannot backup their opinions/statements with facts) have to be identified.", "labels": [], "entities": []}, {"text": "Applications should handle detected speculative parts in a different manner.", "labels": [], "entities": []}, {"text": "A typical example is protein-protein interaction extraction from biological texts, where the aim is to mine text evidence for biological entities that are in a particular relation with each other.", "labels": [], "entities": [{"text": "protein-protein interaction extraction from biological texts", "start_pos": 21, "end_pos": 81, "type": "TASK", "confidence": 0.7936281661192576}]}, {"text": "Here, while an uncertain relation might be of some interest for an end-user as well, such information must not be confused with factual textual evidence (reliable information).", "labels": [], "entities": []}, {"text": "Uncertainty detection has two levels.", "labels": [], "entities": [{"text": "Uncertainty detection", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8739643692970276}]}, {"text": "Automatic hedge detectors might attempt to identify sentences which contain uncertain information and handle whole sentences in a different manner or they might attempt to recognize in-sentence spans which are speculative.", "labels": [], "entities": []}, {"text": "In-sentence uncertainty detection is a more complicated task compared to the sentence-level one, but it has benefits for NLP applications as there maybe spans containing useful factual information in a sentence that otherwise contains uncertain parts.", "labels": [], "entities": [{"text": "In-sentence uncertainty detection", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7652702530225118}]}, {"text": "For example, in the following sentence the subordinated clause starting with although contains factual information while uncertain information is included in the main clause and the embedded question.", "labels": [], "entities": []}, {"text": "Although IL-1 has been reported to contribute to Th17 differentiation in mouse and man, it remains to be determined {whether therapeutic targeting of IL-1 will substantially affect IL-17 in RA}.", "labels": [], "entities": [{"text": "Th17 differentiation", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.9158253967761993}, {"text": "RA", "start_pos": 190, "end_pos": 192, "type": "TASK", "confidence": 0.9349086284637451}]}, {"text": "Both tasks were addressed in the CoNLL-2010 Shared Task, in order to provide uniform manually annotated benchmark datasets for both and to compare their difficulties and state-of-the-art solutions for them.", "labels": [], "entities": [{"text": "CoNLL-2010 Shared Task", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.7610670526822408}]}, {"text": "The uncertainty detection problem consists of two stages.", "labels": [], "entities": [{"text": "uncertainty detection problem", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.875394364198049}]}, {"text": "First, keywords/cues indicating uncertainty should be recognized then either a sentence-level decision is made or the linguistic scope of the cue words has to be identified.", "labels": [], "entities": []}, {"text": "The latter task falls within the scope of semantic analysis of sentences exploiting syntactic patterns, as hedge spans can usually be determined on the basis of syntactic patterns dependent on the keyword.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation for Task1 was carried out at the sentence level, i.e. the cue annotations in the sentence were not taken into account.", "labels": [], "entities": []}, {"text": "The F \u03b2=1 measure (the harmonic mean of precision and recall) of the uncertain class was employed as the chief evaluation metric.", "labels": [], "entities": [{"text": "F \u03b2=1 measure", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9761314511299133}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9922911524772644}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9930518865585327}]}, {"text": "The Task2 systems were expected to mark cueand corresponding scope begin/end tags linked together by using some unique IDs.", "labels": [], "entities": []}, {"text": "A scope-level F \u03b2=1 measure was used as the chief evaluation metric where true positives were scopes which exactly matched the gold standard cue phrases and gold standard scope boundaries assigned to the cue word.", "labels": [], "entities": [{"text": "scope-level F \u03b2=1 measure", "start_pos": 2, "end_pos": 27, "type": "METRIC", "confidence": 0.847128838300705}]}, {"text": "That is, correct scope boundaries with incorrect cue annotation and correct cue words with bad scope boundaries were both treated as errors.", "labels": [], "entities": []}, {"text": "This scope-level metric is very strict.", "labels": [], "entities": []}, {"text": "For instance, the requirement of the precise match of the cue phrase is questionable as -from an application point of view -the goal is to find uncertain text spans and the evidence for this is not so important.", "labels": [], "entities": [{"text": "precise", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9602875113487244}]}, {"text": "However, the annotation of cues in datasets is essential for training scope detectors since locating the cues usually precedes the identification of their scope.", "labels": [], "entities": []}, {"text": "Hence we decided to incorporate cue matches into the evaluation metric.", "labels": [], "entities": []}, {"text": "Another questionable issue is the strict boundary matching requirement.", "labels": [], "entities": [{"text": "boundary matching", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7260158061981201}]}, {"text": "For example, including or excluding punctuations, citations or some bracketed expressions, like (see) from a scope is not crucial for an otherwise accurate scope detector.", "labels": [], "entities": []}, {"text": "On the other hand, the list of such ignorable phenomena is arguable, especially across domains.", "labels": [], "entities": []}, {"text": "Thus, we considered the strict boundary matching to be a straightforward and unambiguous evaluation criterion.", "labels": [], "entities": []}, {"text": "Minor issues like those mentioned above could be handled by simple post-processing rules.", "labels": [], "entities": []}, {"text": "In conclusion we think that the uncertainty detection community may find more flexible evaluation criteria in the future but the strict scope-level metric is definitely a good starting point for evaluation.", "labels": [], "entities": [{"text": "uncertainty detection community", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.8073475956916809}]}, {"text": "Training and evaluation corpora were annotated manually for hedge/weasel cues and their scope by two independent linguist annotators.", "labels": [], "entities": []}, {"text": "Any differences between the two annotations were later resolved by the chief annotator, who was also responsible for creating the annotation guidelines and training the two annotators.", "labels": [], "entities": []}, {"text": "The datasets are freely available 2 for further benchmark experiments at http://www.inf.u-szeged.hu/ rgai/conll2010st.", "labels": [], "entities": []}, {"text": "Since uncertainty cues play an important role in detecting sentences containing uncertainty, they are tagged in the Task1 datasets as well to enhance training and evaluation of systems.", "labels": [], "entities": [{"text": "Task1 datasets", "start_pos": 116, "end_pos": 130, "type": "DATASET", "confidence": 0.8816452920436859}]}, {"text": "2186 paragraphs collected from Wikipedia archives were also offered as Task1 training data (11111 sentences containing 2484 uncertain ones).", "labels": [], "entities": [{"text": "Task1 training data", "start_pos": 71, "end_pos": 90, "type": "DATASET", "confidence": 0.6536646286646525}]}, {"text": "The evaluation dataset contained 2346 Wikipedia paragraphs with 9634 sentences, out of which 2234 were uncertain.", "labels": [], "entities": []}, {"text": "For the selection of the Wikipedia paragraphs used to construct the training and evaluation datasets, we exploited the weasel tags added by the editors of the encyclopedia (marking unsupported opinions or expressions of a non-neutral point of view).", "labels": [], "entities": []}, {"text": "Each paragraph containing weasel tags (5874 different ones) was extracted from the history dump of English Wikipedia.", "labels": [], "entities": [{"text": "history dump of English Wikipedia", "start_pos": 83, "end_pos": 116, "type": "DATASET", "confidence": 0.9150851130485534}]}, {"text": "First, 438 randomly selected paragraphs were manually annotated from this pool then the most frequent cue phrases were collected.", "labels": [], "entities": []}, {"text": "Later on, two other sets of Wikipedia paragraphs were gathered on the basis of whether they contained such cue phrases or not.", "labels": [], "entities": []}, {"text": "The aim of this sampling procedure was to provide large enough training and evaluation samples containing weasel words and also occurrences of typical weasel words in non-weasel contexts.", "labels": [], "entities": []}, {"text": "Each sentence was annotated manually for weasel cues.", "labels": [], "entities": []}, {"text": "Sentences were treated as uncertain if they contained at least one weasel cue, i.e. the scope of weasel words was the entire sentence (which is supposed to be rewritten by Wikipedia editors).", "labels": [], "entities": []}], "tableCaptions": []}