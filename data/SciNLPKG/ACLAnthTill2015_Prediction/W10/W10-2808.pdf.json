{"title": [{"text": "Sketch Techniques for Scaling Distributional Similarity to the Web", "labels": [], "entities": [{"text": "Scaling Distributional Similarity", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.8718507687250773}]}], "abstractContent": [{"text": "In this paper, we propose a memory, space, and time efficient framework to scale dis-tributional similarity to the web.", "labels": [], "entities": []}, {"text": "We exploit sketch techniques, especially the Count-Min sketch, which approximates the frequency of an item in the corpus without explicitly storing the item itself.", "labels": [], "entities": []}, {"text": "These methods use hashing to deal with massive amounts of the streaming text.", "labels": [], "entities": []}, {"text": "We store all item counts computed from 90 GB of web data in just 2 billion counters (8 GB main memory) of CM sketch.", "labels": [], "entities": []}, {"text": "Our method returns semantic similarity between word pairs in O(K) time and can compute similarity between any word pairs that are stored in the sketch.", "labels": [], "entities": []}, {"text": "In our experiments, we show that our framework is as effective as using the exact counts.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many NLP problems, researchers have shown that having large amounts of data is beneficial.", "labels": [], "entities": []}, {"text": "It has also been shown that) having large amounts of data helps capturing the semantic similarity between pairs of words.", "labels": [], "entities": []}, {"text": "However, computing distributional similarity (Sec. 2.1) between word pairs from large text collections is a computationally expensive task.", "labels": [], "entities": [{"text": "Sec. 2.1) between word pairs from large text collections", "start_pos": 46, "end_pos": 102, "type": "TASK", "confidence": 0.7535816431045532}]}, {"text": "In this work, we consider scaling distributional similarity methods for computing semantic similarity between words to Web-scale.", "labels": [], "entities": []}, {"text": "The major difficulty in computing pairwise similarities stems from the rapid increase in the number of unique word-context pairs with the size of text corpus (number of tokens).", "labels": [], "entities": []}, {"text": "shows that  the number of unique word-context pairs increase rapidly compared to the number words when plotted against the number of tokens . For example, a 57 million word corpus 2 generates 224 thousand unique words and 15 million unique word-context pairs.", "labels": [], "entities": []}, {"text": "As a result, it is computationally hard to compute counts of all word-context pairs with a giant corpora using conventional machines (say with main memory of 8 GB).", "labels": [], "entities": []}, {"text": "To overcome this, used MapReduce infrastructure (with 2, 000 cores) to compute pairwise similarities of words on a corpus of roughly 1.6 Terawords.", "labels": [], "entities": []}, {"text": "Ina different direction, our earlier work (Goyal et al., 2010) developed techniques to make the computations feasible on a conventional machines by willing to accept some error in the counts.", "labels": [], "entities": []}, {"text": "Similar to that work, this work exploits the idea of Count-Min (CM) sketch) to approximate the frequency of word pairs in the corpus without explicitly storing the word pairs themselves.", "labels": [], "entities": []}, {"text": "In their, we stored counts of all words/word pairs in fixed amount of main memory.", "labels": [], "entities": []}, {"text": "We used conservative update with CM sketch (referred as CU sketch) and showed that it reduces the average relative error of its approximate counts by a factor of two.", "labels": [], "entities": [{"text": "relative error", "start_pos": 106, "end_pos": 120, "type": "METRIC", "confidence": 0.8771536648273468}]}, {"text": "The approximate counts returned by CU Sketch were used to compute approximate PMI between word pairs.", "labels": [], "entities": []}, {"text": "We found their that the approximate PMI values are as useful as the exact PMI values for computing semantic orientation) of words.", "labels": [], "entities": []}, {"text": "In addition, our intrinsic evaluations in their showed that the quality of approximate counts and approximate PMI is good.", "labels": [], "entities": [{"text": "approximate PMI", "start_pos": 98, "end_pos": 113, "type": "METRIC", "confidence": 0.7496935129165649}]}, {"text": "In this work, we use CU-sketch to store counts of items (words, contexts, and word-context pairs) using fixed amount of memory of 8 GB by using only 2B counters.", "labels": [], "entities": []}, {"text": "These approximate counts returned by CU Sketch are converted into approximate PMI between word-context pairs.", "labels": [], "entities": []}, {"text": "The top K contexts (based on PMI score) for each word are used to construct distributional profile (DP) for each word.", "labels": [], "entities": [{"text": "PMI score)", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.8499535918235779}, {"text": "distributional profile (DP)", "start_pos": 76, "end_pos": 103, "type": "METRIC", "confidence": 0.7991791605949402}]}, {"text": "The similarity between a pair of words is computed based on the cosine similarity of their respective DPs.", "labels": [], "entities": []}, {"text": "The above framework of using CU sketch to compute semantic similarity between words has five good properties.", "labels": [], "entities": []}, {"text": "First, this framework can return semantic similarity between any word pairs that are stored in the CU sketch.", "labels": [], "entities": []}, {"text": "Second, it can return the similarity between word pairs in time O(K).", "labels": [], "entities": [{"text": "similarity", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9525973796844482}, {"text": "O", "start_pos": 64, "end_pos": 65, "type": "METRIC", "confidence": 0.8879411220550537}]}, {"text": "Third, because we do not store items explicitly, the overall space required is significantly smaller.", "labels": [], "entities": []}, {"text": "Fourth, the additive property of CU sketch (Sec. 3.2) enables us to parallelize most of the steps in the algorithm.", "labels": [], "entities": []}, {"text": "Thus it can be easily extended to very large amounts of text data.", "labels": [], "entities": []}, {"text": "Fifth, this easily generalizes to any kind of association measure and semantic similarity measure.", "labels": [], "entities": []}], "datasetContent": [{"text": "As discussed earlier, the DPs of words are used to compute similarity between a pair of words.", "labels": [], "entities": []}, {"text": "We used the following four test sets and their corresponding human judgements to evaluate the word pair rankings.", "labels": [], "entities": []}, {"text": "1. WS-353: WordSimilarity-353) is a set of 353 word pairs.", "labels": [], "entities": [{"text": "WordSimilarity-353", "start_pos": 11, "end_pos": 29, "type": "DATASET", "confidence": 0.7045387029647827}]}, {"text": "Each of these data sets come with human ranking of the word pairs.", "labels": [], "entities": []}, {"text": "We rank the word pairs based on the similarity computed using DPs and evaluate this ranking against the human ranking.", "labels": [], "entities": []}, {"text": "We report the spearman's rank correlation coefficient (\u03c1) between these two rankings.", "labels": [], "entities": [{"text": "rank correlation coefficient (\u03c1)", "start_pos": 25, "end_pos": 57, "type": "METRIC", "confidence": 0.8519710948069891}]}], "tableCaptions": [{"text": " Table 2: Evaluating word pairs ranking with Exact and CU counts. Scores are evaluated using \u03c1 metric.", "labels": [], "entities": []}]}