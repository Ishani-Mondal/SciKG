{"title": [{"text": "Improving Word Alignment by Semi-supervised Ensemble", "labels": [], "entities": [{"text": "Improving Word Alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9066709081331888}]}], "abstractContent": [{"text": "Supervised learning has been recently used to improve the performance of word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.793025940656662}]}, {"text": "However, due to the limited amount of labeled data, the performance of \"pure\" supervised learning, which only used labeled data, is limited.", "labels": [], "entities": []}, {"text": "As a result , many existing methods employ features learnt from a large amount of unla-beled data to assist the task.", "labels": [], "entities": []}, {"text": "In this paper , we propose a semi-supervised ensemble method to better incorporate both labeled and unlabeled data during learning.", "labels": [], "entities": []}, {"text": "Firstly, we employ an ensemble learning framework, which effectively uses alignment results from different unsupervised alignment models.", "labels": [], "entities": []}, {"text": "We then propose to use a semi-supervised learning method, namely Tri-training, to train classifiers using both labeled and unlabeled data col-laboratively and further improve the result.", "labels": [], "entities": []}, {"text": "Experimental results show that our methods can substantially improve the quality of word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.755882978439331}]}, {"text": "The final translation quality of a phrase-based translation system is slightly improved, as well.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.765874594449997}]}], "introductionContent": [{"text": "Word alignment is the process of learning bilingual word correspondences.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.735139325261116}]}, {"text": "Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (.", "labels": [], "entities": [{"text": "Conventional word alignment", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6186740597089132}]}, {"text": "Recently, supervised learning methods have been used to improve the performance.", "labels": [], "entities": []}, {"text": "They firstly re-formalize word alignment as some kind of classification task.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7822122573852539}]}, {"text": "Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (.", "labels": [], "entities": []}, {"text": "It is well understood that the performance of supervised learning relies heavily on the feature set.", "labels": [], "entities": []}, {"text": "As more and more features are added into the model, more data is needed for training.", "labels": [], "entities": []}, {"text": "However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want.", "labels": [], "entities": [{"text": "labeling", "start_pos": 38, "end_pos": 46, "type": "TASK", "confidence": 0.9611015915870667}, {"text": "word alignment", "start_pos": 86, "end_pos": 100, "type": "TASK", "confidence": 0.7125885337591171}]}, {"text": "This may limit the performance of supervised methods ().", "labels": [], "entities": []}, {"text": "One possible alternative is to use features learnt in some unsupervised manner to help the task.", "labels": [], "entities": []}, {"text": "For example, uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; and use results from IBM Model 3 and Model 4, respectively.", "labels": [], "entities": []}, {"text": "propose another way of incorporating unlabeled data.", "labels": [], "entities": []}, {"text": "They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data.", "labels": [], "entities": [{"text": "IBM Model4", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.9157780706882477}]}, {"text": "The results of these models are then combined using a maximum entropy classifier, which is trained using labeled data.", "labels": [], "entities": []}, {"text": "This method is highly efficient in training because it only makes decisions on alignment links from existing models and avoids searching the entire alignment space.", "labels": [], "entities": []}, {"text": "In this paper, we follow's idea of combining multiple alignment results.", "labels": [], "entities": []}, {"text": "And we use more features, such as bi-lexical features, which help capture more information from unlabeled data.", "labels": [], "entities": []}, {"text": "To further improve the decision making during combination, we propose to use a semisupervised strategy, namely Tri-training (Zhou and), which ensembles three classifiers using both labeled and unlabeled data.", "labels": [], "entities": [{"text": "decision making", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7659512162208557}]}, {"text": "More specifically, Tri-training iteratively trains three classifiers and labels all the unlabeled instances.", "labels": [], "entities": []}, {"text": "It then uses some instances among the unlabeled ones to expand the labeled training set of each in-dividual classifier.", "labels": [], "entities": []}, {"text": "As word alignment task usually faces a huge parallel corpus, which contains millions of unlabeled instances, we develop specific algorithms to adapt Tri-training for this large scale task.", "labels": [], "entities": [{"text": "word alignment task", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8743554552396139}]}, {"text": "The next section introduces the supervised alignment combination framework; Section 3 presents our semi-supervised learning algorithm.", "labels": [], "entities": [{"text": "supervised alignment combination", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.7033186157544454}]}, {"text": "We show the experiments and results in Section 4; briefly overview related work in Section 5 and conclude in the last section.", "labels": [], "entities": []}], "datasetContent": [{"text": "All our experiments are conducted on the language pair of Chinese and English.", "labels": [], "entities": []}, {"text": "For training alignment systems, a parallel corpus coming from LDC2005T10 and LDC2005T14 is used as unlabeled training data.", "labels": [], "entities": [{"text": "training alignment", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7853493988513947}, {"text": "LDC2005T14", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.8801236152648926}]}, {"text": "Labeled data comes from NIST Open MT Eval'02, which has 491 labeled sentence pairs.", "labels": [], "entities": [{"text": "NIST Open MT Eval'02", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.8378355354070663}]}, {"text": "The first 200 labeled sentence pairs are used as labeled training data and the rest are used for evaluation ().", "labels": [], "entities": []}, {"text": "The number of candidate alignment links in each data set is also listed in.", "labels": [], "entities": []}, {"text": "These candidate alignment links are generated using the three sub-models described in Section 4.2.", "labels": [], "entities": []}, {"text": "The quality of word alignment is evaluated in terms of alignment error rate (AER)), classifier's accuracy and recall of correct decisions.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.7270723432302475}, {"text": "alignment error rate (AER))", "start_pos": 55, "end_pos": 82, "type": "METRIC", "confidence": 0.9433205723762512}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9968987703323364}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9986186027526855}]}, {"text": "Formula 3 shows the definition of AER, where P and S refer to the set of possible and sure alignment links, respectively.", "labels": [], "entities": [{"text": "AER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.996832549571991}]}, {"text": "In our experiments, We also define a F 1 score to be the harmonic mean of classifier's accuracy and recall of correct decisions (Formula 4).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.983944853146871}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.992771327495575}, {"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9932113885879517}]}, {"text": "We also evaluate the machine translation quality using unlabeled data (in) and these alignment results as aligned training data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7599488496780396}]}, {"text": "We use multi-references data sets from NIST Open MT Evaluation as development and test data.", "labels": [], "entities": [{"text": "NIST Open MT Evaluation", "start_pos": 39, "end_pos": 62, "type": "DATASET", "confidence": 0.835792750120163}]}, {"text": "The English side of the parallel corpus is trained into a language model using SRILM).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.7037606239318848}]}, {"text": "Moses () is used for decoding.", "labels": [], "entities": []}, {"text": "Translation quality is measured by BLEU4 score ignoring the case.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9620001912117004}, {"text": "BLEU4", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9978944659233093}]}, {"text": "We use the following three sub-models: bidirectional results of Giza++ (Och and Ney, 2003) Model4, namely Model4C2E and Model4E2C, and the joint training result of BerkeleyAligner () (BerkeleyAl.).", "labels": [], "entities": [{"text": "BerkeleyAligner", "start_pos": 164, "end_pos": 179, "type": "DATASET", "confidence": 0.9267557859420776}]}, {"text": "To evaluate AER, all three data sets listed in are combined and used for the unsupervised training of each sub-model.", "labels": [], "entities": [{"text": "AER", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.8269607424736023}]}, {"text": "presents the alignment quality of those sub-models, as well as a supervised ensemble of them, as described in Section 2.1.", "labels": [], "entities": []}, {"text": "We use the symmetrized IBM Model4 results by the grow-diagfinal-and heuristic as our baseline (Model4GDF).", "labels": [], "entities": []}, {"text": "Scores in show the great improvement of supervised learning, which reduce the alignment error rate significantly (more than 5% AER points from the best sub-model, i.e. BerkeleyAligner).", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 78, "end_pos": 98, "type": "METRIC", "confidence": 0.7935642004013062}, {"text": "AER", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.9982885718345642}, {"text": "BerkeleyAligner", "start_pos": 168, "end_pos": 183, "type": "DATASET", "confidence": 0.9489029049873352}]}, {"text": "This result is consistent with's experiments.", "labels": [], "entities": []}, {"text": "It is quite reasonable that supervised model achieves a much higher classification accuracy of 0.8124 than any unsupervised sub-model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8870290517807007}]}, {"text": "Besides, it also achieves the highest recall of correct alignment links (0.7027).", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9996325969696045}, {"text": "correct alignment links", "start_pos": 48, "end_pos": 71, "type": "METRIC", "confidence": 0.8430787126223246}]}, {"text": "We present our experiment results on semisupervised models in.", "labels": [], "entities": []}, {"text": "The two strategies of generating initial classifiers are compared.", "labels": [], "entities": []}, {"text": "TriBootstrap is the model using the original bootstrap sampling initialization; and Tri-Divide is the model using the dividing initialization as described in Section 3.2.", "labels": [], "entities": []}, {"text": "Items with superscripts 0 indicate models before the first iteration, i.e. initial models.", "labels": [], "entities": []}, {"text": "The scores of BerkeleyAligner and the supervised model are also included for comparison.", "labels": [], "entities": [{"text": "BerkeleyAligner", "start_pos": 14, "end_pos": 29, "type": "DATASET", "confidence": 0.9677836298942566}]}, {"text": "In general, all supervised and semi-supervised models achieve better results than the best submodel, which proves the effectiveness of labeled training data.", "labels": [], "entities": []}, {"text": "It is also reasonable that initial models are not as good as the supervised model, because they only use part of the labeled data for training.", "labels": [], "entities": []}, {"text": "After the iterative training, both the two Tri-training models get a significant increase in recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9994550347328186}]}, {"text": "We attribute this to the use of bi-lexical features described in Section 2.2.", "labels": [], "entities": []}, {"text": "Analysis of the resulting model shows that the number of bi-lexical features increases from around 300 to nearly 7,800 after Tri-training.", "labels": [], "entities": [{"text": "Tri-training", "start_pos": 125, "end_pos": 137, "type": "DATASET", "confidence": 0.7973146438598633}]}, {"text": "It demonstrates that semi-supervised algorithms are able to learn more bi-lexical features automatically from the unlabeled data, which may help recognize more translation equivalences.", "labels": [], "entities": []}, {"text": "However, we also notice that the accuracy drops a little after Tri-training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9997817873954773}, {"text": "Tri-training", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.8247960209846497}]}, {"text": "This might also be caused by the large set of bilexical features, which may contain some noises.", "labels": [], "entities": []}, {"text": "In the comparison of initialization strategies, the dividing strategy achieves a much higher recall of 0.7605, which is also the highest among all models.", "labels": [], "entities": [{"text": "initialization", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.9664567708969116}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9995768666267395}]}, {"text": "It also achieves the best F 1 score of 0.7717, higher than the bootstrap sampling strategy (0.7684).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9891392191251119}]}, {"text": "This result confirms that diversity of initial classifiers is important for Co-training style algorithms.", "labels": [], "entities": []}, {"text": "We compare the machine translation results of each sub-models, supervised models and semisupervised models in.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7060774862766266}]}, {"text": "Among sub-models, BerkeleyAligner gets better BLEU4 scores in almost all the data sets except TEST06, which agrees with its highest F 1 score among all submodels.", "labels": [], "entities": [{"text": "BerkeleyAligner", "start_pos": 18, "end_pos": 33, "type": "DATASET", "confidence": 0.9075793623924255}, {"text": "BLEU4", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9991095662117004}, {"text": "TEST06", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.6229365468025208}, {"text": "F 1 score", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9840227961540222}]}, {"text": "The supervised method gets the highest BLEU score of 27.07 on the dev set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9863085746765137}]}, {"text": "However, its performance on the test sets is a bit lower than that of BerkeleyAligner.", "labels": [], "entities": [{"text": "BerkeleyAligner", "start_pos": 70, "end_pos": 85, "type": "DATASET", "confidence": 0.9739438891410828}]}, {"text": "As we expect, our two semi-supervised models achieve highest scores on almost all the data sets, which are also higher than the commonly used grow-diag-final-and symmetrization of IBM Model 4.", "labels": [], "entities": []}, {"text": "More specifically, Tri-Divide is the best of all systems.", "labels": [], "entities": []}, {"text": "It gets a dev score of 27.04, which is comparable with the highest one (27.07).", "labels": [], "entities": []}, {"text": "Tri-Divide also gets the highest BLEU scores on, which are nearly 1 point higher than all sub-models.", "labels": [], "entities": [{"text": "Tri-Divide", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8874188661575317}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9992008805274963}]}, {"text": "The other Tri-training model, TriBootstrap, gets the highest score on Test08, which is also significantly better than those sub-models.", "labels": [], "entities": [{"text": "TriBootstrap", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.8833314180374146}, {"text": "Test08", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.9546425342559814}]}, {"text": "Despite the large improvement in F 1 score, our two Tri-training models only get slightly better score than the well-known Model4GDF.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9886107444763184}]}, {"text": "This kind of inconsistence between AER or F 1 scores and BLEU scores is a known issue in machine translation community.", "labels": [], "entities": [{"text": "AER or F 1 scores", "start_pos": 35, "end_pos": 52, "type": "METRIC", "confidence": 0.8082789421081543}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9907974004745483}, {"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7761600911617279}]}, {"text": "One possible explanation is that both AER or F 1 are 0-1 loss functions, which means missing one link and adding one redundant link will get the same penalty.", "labels": [], "entities": [{"text": "AER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9317365884780884}, {"text": "F 1", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.8376805782318115}]}, {"text": "And more importantly, every wrong link receives the same penalty under these metrics.", "labels": [], "entities": []}, {"text": "However, these different errors may have different effects on the machine translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7122653722763062}]}, {"text": "Thus, improving alignment quality according to AER or F 1 may not directly lead to an increase of BLEU scores.", "labels": [], "entities": [{"text": "alignment", "start_pos": 16, "end_pos": 25, "type": "TASK", "confidence": 0.9059417843818665}, {"text": "AER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9979477524757385}, {"text": "F 1", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9797216355800629}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9995348453521729}]}, {"text": "The relationship among these metrics are still under investigation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data used in the experiment", "labels": [], "entities": []}, {"text": " Table 2: Experiments of Sub-models", "labels": [], "entities": []}, {"text": " Table 3: Experiments of Semi-supervised Models", "labels": [], "entities": []}, {"text": " Table 4: Experiments on machine translation (BLEU4 scores in percentage)", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8135693669319153}, {"text": "BLEU4", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9982472658157349}]}]}