{"title": [{"text": "Chained Machine Translation Using Morphemes as Pivot Language", "labels": [], "entities": [{"text": "Chained Machine Translation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6697893341382345}]}], "abstractContent": [{"text": "As the smallest meaning-bearing elements of the languages which have rich morphology information, morphemes are often integrated into state-of-the-art statistical machine translation to improve translation quality.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 151, "end_pos": 182, "type": "TASK", "confidence": 0.6358747879664103}]}, {"text": "The paper proposes an approach which novelly uses morphemes as pivot language in a chained machine translation system.", "labels": [], "entities": []}, {"text": "A machine translation based method is used therein to find the mapping relations between morphemes and words.", "labels": [], "entities": []}, {"text": "Experiments show the effectiveness of our approach, achieving 18.6 percent increase in BLEU score over the baseline phrase-based machine translation system.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9744523763656616}, {"text": "phrase-based machine translation", "start_pos": 116, "end_pos": 148, "type": "TASK", "confidence": 0.634921153386434}]}], "introductionContent": [{"text": "Recently, most evaluations of machine translation systems) indicate that the performance of corpus-based statistical machine translation (SMT) has come up to the traditional rule-based method.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.710725337266922}, {"text": "corpus-based statistical machine translation (SMT)", "start_pos": 92, "end_pos": 142, "type": "TASK", "confidence": 0.7716115372521537}]}, {"text": "In the corpusbased SMT, it is difficult to exactly select the correct inflections (word-endings) if the target language is highly inflected.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8363078832626343}]}, {"text": "This problem will be more severe if the source language is an isolated language with non-morphology (eg. Chinese) and the target language is an agglutinative language with productive derivational and inflectional morphology (eg. Mongolian: a minority language of China).", "labels": [], "entities": []}, {"text": "In addition, the lack of large-scale parallel corpus may cause the sparse data problem, which will be more severe if one of the source language and the target language is highly inflected.", "labels": [], "entities": []}, {"text": "As the smallest meaningbearing elements of the languages which have rich morphology information, morphemes are the compact representation of words.", "labels": [], "entities": []}, {"text": "Using morphemes as the semantic units in the parallel corpus cannot only help choose the correct inflections, but also alleviate the data sparseness problem partially.", "labels": [], "entities": []}, {"text": "Many strategies of integrating morphology information into state-of-the-art SMT systems in different stages have been proposed.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9887329339981079}]}, {"text": "() proposed a preprocessing approach for incorporating syntactic and Morphological information within a phrase-based English-Hindi SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.6439588665962219}]}, {"text": "() proposed a method which uses Porter stems and even 4-letter prefixes for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7845130562782288}]}, {"text": "( proposed the factored translation models which combine feature functions to handle syntactic, morphological, and other linguistic information in a log-linear model during training.", "labels": [], "entities": []}, {"text": "() made use of the information of morphological structure and source language in postprocessing to improve SMT quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9961531758308411}]}, {"text": "(de) adopted the Minimum Bayes Risk decoding strategy to combine output from identical SMT system, which is trained on alternative morphological decompositions of the source language.", "labels": [], "entities": [{"text": "Minimum Bayes Risk decoding", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.5742747262120247}, {"text": "SMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.9794551134109497}]}, {"text": "Meanwhile, the SMT-based methods are widely used in the area of natural language processing.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 15, "end_pos": 24, "type": "TASK", "confidence": 0.9908444285392761}, {"text": "natural language processing", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.6381270984808604}]}, {"text": "() applied SMT to generate novel paraphrases.", "labels": [], "entities": [{"text": "SMT", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9832354784011841}]}, {"text": "() adopted an SMT-based method to query expansion in answer retrieval.) used SMT to generate the second sentence of the Chinese couplets.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 14, "end_pos": 23, "type": "TASK", "confidence": 0.9843524098396301}, {"text": "query expansion", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.7348244190216064}, {"text": "answer retrieval.", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7575419545173645}, {"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.8921410441398621}]}, {"text": "As opposed to the above strategies, the paper proposes an approach that uses morphemes as pivot language in a chained SMT system, for translating Chinese into Mongolian, which consists of two SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.9648373126983643}, {"text": "SMT", "start_pos": 192, "end_pos": 195, "type": "TASK", "confidence": 0.962391197681427}]}, {"text": "First, Chinese sentences are translated into Mongolian morphemes instead of Mongolian words in the ChineseMorphemes SMT (SMT 1 ).", "labels": [], "entities": [{"text": "ChineseMorphemes SMT (SMT 1 )", "start_pos": 99, "end_pos": 128, "type": "DATASET", "confidence": 0.8655147155125936}]}, {"text": "Then Mongolian words are generated from morphemes in the Morphemes-Mongolian SMT (SMT 2 ).", "labels": [], "entities": [{"text": "Morphemes-Mongolian SMT (SMT", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.531128853559494}]}, {"text": "The essential part of the chained SMT system is how to find the mapping relations between the morphemes and words, which is considered as a procedure of machine translation in our approach.", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9459468126296997}, {"text": "machine translation", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.7256020754575729}]}, {"text": "More concretely, the first challenge of this approach is to investigate some effective strategies to segment the Mongolian corpus in the ChineseMongolian parallel corpus.", "labels": [], "entities": [{"text": "Mongolian corpus", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.6764778941869736}, {"text": "ChineseMongolian parallel corpus", "start_pos": 137, "end_pos": 169, "type": "DATASET", "confidence": 0.858752171198527}]}, {"text": "And the second challenge is how to efficiently generate Mongolian words from morphemes.", "labels": [], "entities": []}, {"text": "Additionally, on the one hand Mongolian words may have multiple kinds of morphological segmentations.", "labels": [], "entities": []}, {"text": "On the other hand there is also the ambiguity of word boundaries in the processing of generating Mongolian words from morphemes.", "labels": [], "entities": []}, {"text": "In order to solve these ambiguities, a SMT-based method is applied in that word context and morphemes context can betaken into account in this method.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.9905134439468384}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces two methods of morphological segmentation.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.793059229850769}]}, {"text": "Section 3 presents the details of chained SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9413543343544006}]}, {"text": "Section 4 describes the experiment results and evaluation.", "labels": [], "entities": []}, {"text": "Section 5 gives concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiments, first we preprocess the corpus, such as converting Mongolian into Latin Mongolian and filtering the apparent noisy segmentation of the gold standard morphological segmentation corpus.", "labels": [], "entities": []}, {"text": "And then we evaluate the effectiveness of the SMTs which find the mapping relations between the morphemes and their corresponding word forms.", "labels": [], "entities": [{"text": "SMTs", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.9813063740730286}]}, {"text": "Namely, SMT-MS and SMT 2 . As mentioned above, SMT 1 is the core part of the chained SMT system, which decides the final quality of translation results.", "labels": [], "entities": [{"text": "SMT-MS", "start_pos": 8, "end_pos": 14, "type": "TASK", "confidence": 0.9296178221702576}, {"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9750875234603882}, {"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9575645327568054}, {"text": "SMT", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9468214511871338}]}, {"text": "So the evaluation of SMT 1 can be reflected by the evaluation of translation results of whole chained SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9843554496765137}, {"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9542551636695862}]}, {"text": "Finally, we evaluate and analyze the performance of the chained SMT system by using the automatic evaluation tools.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9599786400794983}]}, {"text": "The translation model consists of a standard phrase-table with lexicalized reordering.", "labels": [], "entities": []}, {"text": "Bidirectional word alignments obtained with GIZA++ are intersected using the grow-diagfinal heuristic ().", "labels": [], "entities": []}, {"text": "Translations of phrases of up to 7 words long are collected and scored with translation probabilities and lexical weighting.", "labels": [], "entities": []}, {"text": "The language model of morphemes is a 5-gram model with Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "The language model of Mongolian word is 3-gram model with Kneser-Ney smoothing too.", "labels": [], "entities": []}, {"text": "All the language models are built with the SRI language modeling toolkit).", "labels": [], "entities": [{"text": "SRI language modeling", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.6931001543998718}]}, {"text": "The log-linear model feature weights are learned by using minimum error rate training (MERT) with BLEU score () as the objective function.", "labels": [], "entities": [{"text": "minimum error rate training (MERT)", "start_pos": 58, "end_pos": 92, "type": "METRIC", "confidence": 0.8353641160896846}, {"text": "BLEU score", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9865010678768158}]}, {"text": "The tasks of SMT-MS and SMT 2 are to find the mapping relations between the morphemes and their corresponding word forms.", "labels": [], "entities": [{"text": "SMT-MS", "start_pos": 13, "end_pos": 19, "type": "TASK", "confidence": 0.9494946002960205}, {"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9593305587768555}]}, {"text": "Morphological segmentation is done by SMT-MS.", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9319899380207062}, {"text": "SMT-MS", "start_pos": 38, "end_pos": 44, "type": "TASK", "confidence": 0.9212349653244019}]}, {"text": "Contrarily, SMT 2 is used to generate the words from morphemes.", "labels": [], "entities": [{"text": "SMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9835004806518555}]}, {"text": "To evaluate the effectiveness of SMT-MS and SMT 2 , we divide the filtered gold standard corpus into two sets for training (90%) and testing (10%) respectively.", "labels": [], "entities": [{"text": "SMT-MS", "start_pos": 33, "end_pos": 39, "type": "TASK", "confidence": 0.9812860488891602}, {"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9846158623695374}, {"text": "filtered gold standard corpus", "start_pos": 66, "end_pos": 95, "type": "DATASET", "confidence": 0.8059644401073456}]}, {"text": "The correct morpheme boundaries are counted for SMT-MS evaluation, while the correct word boundaries are counted for SMT 2 evaluation.", "labels": [], "entities": [{"text": "SMT-MS evaluation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.8944671154022217}, {"text": "SMT 2 evaluation", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.850615938504537}]}, {"text": "We use the two measures precision and recall on discovered word boundaries to evaluate the effectiveness of SMT-MS and SMT 2 , where precision is the proportion of correctly discovered boundaries among all discovered boundaries by the algorithm, and recall is the proportion of correctly discovered boundaries among all correct boundaries.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9989733695983887}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9963273406028748}, {"text": "SMT-MS", "start_pos": 108, "end_pos": 114, "type": "TASK", "confidence": 0.876329779624939}, {"text": "SMT", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9183468222618103}, {"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9987896084785461}, {"text": "recall", "start_pos": 250, "end_pos": 256, "type": "METRIC", "confidence": 0.9990919828414917}]}, {"text": "A high precision indicates that a morpheme boundary is probably correct when it is suggested.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.996483564376831}]}, {"text": "However the proportion of missed boundaries cannot be obtained from it.", "labels": [], "entities": []}, {"text": "A high recall indicates that most of the desired boundaries were indeed discovered.", "labels": [], "entities": [{"text": "recall", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9978177547454834}]}, {"text": "However it cannot point out how many incorrect boundaries were suggested either.", "labels": [], "entities": []}, {"text": "In order to get a comprehensive idea, we also make use of the evaluation method: F-measure as a compromise.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9525794386863708}]}, {"text": "These measures assume values between zero and 100%, where high values reflect good performance.", "labels": [], "entities": []}, {"text": "Therefore, we evaluate the SMT-based methods by incrementally evaluating the features used in our phrase-based SMT model. are corresponding to the evaluations of SMT-MS and SMT 2 respectively, where P , Rand F denote the three measures, namely precision, recall and F-measure.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 27, "end_pos": 36, "type": "TASK", "confidence": 0.9885329008102417}, {"text": "SMT", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.8138497471809387}, {"text": "SMT-MS", "start_pos": 162, "end_pos": 168, "type": "TASK", "confidence": 0.8934445381164551}, {"text": "SMT", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.9645893573760986}, {"text": "Rand F", "start_pos": 203, "end_pos": 209, "type": "METRIC", "confidence": 0.9324173927307129}, {"text": "precision", "start_pos": 244, "end_pos": 253, "type": "METRIC", "confidence": 0.9991076588630676}, {"text": "recall", "start_pos": 255, "end_pos": 261, "type": "METRIC", "confidence": 0.9935294985771179}, {"text": "F-measure", "start_pos": 266, "end_pos": 275, "type": "METRIC", "confidence": 0.9604541063308716}]}, {"text": "The results show that when we add more features incrementally, the precision, recall and Fmeasure are improved consistently.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9997988343238831}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9997734427452087}, {"text": "Fmeasure", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9995180368423462}]}, {"text": "These indicate that the features are helpful for finding the mapping relations between morphemes and Mongolian words.", "labels": [], "entities": []}, {"text": "We use NIST score) and BLEU score () to evaluate chained SMT system.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.6938450932502747}, {"text": "BLEU score", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9888317286968231}, {"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9767308235168457}]}, {"text": "The training set contains 67288 Chinese-Mongolian parallel sentences.", "labels": [], "entities": []}, {"text": "The test set contains 400 sentences, where each sentence has four reference sentences which are translated by native experts.", "labels": [], "entities": []}, {"text": "In the training phase, we convert Mongolian into Latin Mongolian.", "labels": [], "entities": []}, {"text": "And while in the test phase, we convert the Latin Mongolian back into the traditional Mongolian words.", "labels": [], "entities": []}, {"text": "We compare the chained SMT system with the standard phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9518681168556213}, {"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.6909916996955872}]}, {"text": "The BLEU score is improved by 18.6 percent, from 20.71 (Baseline) to 24.57 (Chain 2 ).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9559291005134583}]}, {"text": "In addition, Chain 2 is better than Chain 1 . We believe that it is essentially related to the different morphemes corpus of Chain 1 and Chain 2 . The morphemes corpus of Chain 1 takes lemmatization into account, while the morphemes corpus of Chain 2 changes all morphemes to in- flected forms which are identical to the original word forms.", "labels": [], "entities": []}, {"text": "As the example in Section 2, the word \"BAYIG A\" is segmented into \"BAI+G A\" in Chain 1 and \"BAYI+G A\" in Chain 2 . Meanwhile, \"BAI\" is an independent Mongolian word in the corpus.", "labels": [], "entities": [{"text": "BAYIG A", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9249733686447144}, {"text": "BAI+G A", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.804487481713295}]}, {"text": "So Chain 1 cannot discriminate the word \"BAI\" from the morpheme \"BAI\".", "labels": [], "entities": [{"text": "BAI", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.8922019600868225}, {"text": "BAI", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.8470062613487244}]}, {"text": "As well known, the translation quality of SMT relies on the performance of morphological segmentation.", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9662249088287354}, {"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9936059713363647}]}, {"text": "We give the following example to intuitively show the quality of translation of the chained SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9503211379051208}]}, {"text": "gives four examples of translating Chinese into Mongolian.", "labels": [], "entities": []}, {"text": "In each example, four reference sentences translated by native experts are also given.", "labels": [], "entities": []}, {"text": "These examples indicate that the chained SMT system can help choose the correct inflections, and partly alleviate the data sparseness problem.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9680824875831604}]}], "tableCaptions": [{"text": " Table 1: Evaluation of SMT-MS and SMT 2", "labels": [], "entities": [{"text": "SMT-MS", "start_pos": 24, "end_pos": 30, "type": "TASK", "confidence": 0.9725952744483948}, {"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.984744131565094}]}, {"text": " Table 2: Evaluation of systems", "labels": [], "entities": []}, {"text": " Table 3: Examples of translating Chinese into Mongolian", "labels": [], "entities": []}]}