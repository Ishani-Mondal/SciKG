{"title": [{"text": "Capturing Nonlinear Structure in Word Spaces through Dimensionality Reduction", "labels": [], "entities": [{"text": "Capturing Nonlinear Structure", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8808518648147583}]}], "abstractContent": [{"text": "Dimensionality reduction has been shown to improve processing and information extraction from high dimensional data.", "labels": [], "entities": [{"text": "Dimensionality reduction", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8782720565795898}, {"text": "information extraction from high dimensional data", "start_pos": 66, "end_pos": 115, "type": "TASK", "confidence": 0.837661067644755}]}, {"text": "Word space algorithms typically employ linear reduction techniques that assume the space is Euclidean.", "labels": [], "entities": []}, {"text": "We investigate the effects of extracting nonlinear structure in the word space using Locality Preserving Projections, a reduction algorithm that performs manifold learning.", "labels": [], "entities": [{"text": "manifold learning", "start_pos": 154, "end_pos": 171, "type": "TASK", "confidence": 0.7591461539268494}]}, {"text": "We apply this reduction to two common word space models and show improved performance over the original models on benchmarks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector space models of semantics frequently employ some form of dimensionality reduction for improvement in representations or computational overhead.", "labels": [], "entities": []}, {"text": "Many of the dimensionality reduction algorithms assume that the unreduced word space is linear.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.7317202836275101}]}, {"text": "However, word similarities have been shown to exhibit many non-metric properties: asymmetry, e.g North Korea is more similar to Red China than Red China is to North Korea, and non-transitivity, e.g. Cuba is similar the former USSR, Jamaica is similar to Cuba, but Jamaica is not similar to the USSR.", "labels": [], "entities": []}, {"text": "We hypothesize that a non-linear word space model might more accurately preserve these non-metric relationships.", "labels": [], "entities": []}, {"text": "To test our hypothesis, we capture the nonlinear structure with dimensionality reduction by using Locality Preserving Projection (LPP), an efficient, linear approximation of Eigenmaps ().", "labels": [], "entities": [{"text": "Locality Preserving Projection (LPP)", "start_pos": 98, "end_pos": 134, "type": "METRIC", "confidence": 0.63620425760746}]}, {"text": "With this reduction, the word space vectors are assumed to exist on a nonlinear manifold that LPP learns in order to project the vectors into a Euclidean space.", "labels": [], "entities": []}, {"text": "We measure the effects of using LPP on two basic word space models: the Vector Space Model and a Word Co-occurrence model.", "labels": [], "entities": []}, {"text": "We begin with a brief overview of these word spaces and common dimensionality reduction techniques.", "labels": [], "entities": [{"text": "common dimensionality reduction", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.6110370059808096}]}, {"text": "We then formally introduce LPP.", "labels": [], "entities": [{"text": "LPP", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7590191960334778}]}, {"text": "Following, we use two experiments to demonstrate LPP's capacity to accurately dimensionally reduce word spaces.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two experiments measures the effects of nonlinear dimensionality reduction for word spaces.", "labels": [], "entities": []}, {"text": "For both, we apply LPP to two basic word space models, the VSM and WC.", "labels": [], "entities": [{"text": "VSM", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.7984393239021301}]}, {"text": "In the first experiment, we measure the word spaces' abilities to model semantic relations, as determined by priming experiments.", "labels": [], "entities": []}, {"text": "In the second experiment, we evaluate the representation capabilities of the LPP-reduced models on standard word space benchmarks.", "labels": [], "entities": []}, {"text": "Semantic priming measures word association based on human responses to a provided cue.", "labels": [], "entities": [{"text": "Semantic priming", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8184536099433899}]}, {"text": "Priming studies have been used to evaluate word spaces by equating vector similarity with an increased priming response.", "labels": [], "entities": []}, {"text": "We use data from two types of priming experiments to measure whether LPP models better correlate with human performance than non-LPP word spaces.", "labels": [], "entities": []}, {"text": "collected free association responses to 5,019 prime words.", "labels": [], "entities": []}, {"text": "An average of 149 participants responded to each prime with the first word that came to mind.", "labels": [], "entities": []}, {"text": "We use six standard word space benchmarks to test our hypothesis that LPP can accurately capture general semantic knowledge and association based relations.", "labels": [], "entities": []}, {"text": "The benchmarks come in two forms: word association and word choice tests.", "labels": [], "entities": [{"text": "word association", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7461922466754913}]}, {"text": "Word choice tests provide a target word and a list of options, one of which has the desired relation to the target.", "labels": [], "entities": []}, {"text": "To answer these questions, we select the option with the highest cosine similarity with the target.", "labels": [], "entities": []}, {"text": "Three word choice synonymy benchmarks are used: the Test of English as a Foreign Language (TOEFL) test set from, the English as a Second Language (ESL) test set from, and the Canadian Reader's Digest Word Power (RDWP) from  Word association tests measure the semantic relatedness of two words by comparing their similarity in the word space with human judgements.", "labels": [], "entities": []}, {"text": "These tests are more precise than word choice tests because they take into account the specific value of the word similarity.", "labels": [], "entities": []}, {"text": "Three word association benchmarks are used: the word similarity data set of, the wordrelatedness data set of, and the antonymy data set of, which measures the degree to which high similarity captures the antonymy relationship.", "labels": [], "entities": [{"text": "wordrelatedness data set", "start_pos": 81, "end_pos": 105, "type": "DATASET", "confidence": 0.7406629025936127}]}, {"text": "The Finkelstein et al. testis notable in that the human judges were free to score based on any word relationship.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experiment 1 priming results for the six relation categories from Hodgson (1991)", "labels": [], "entities": []}, {"text": " Table 2: Results from Experiment 2 on six word space benchmarks", "labels": [], "entities": []}, {"text": " Table 3: Experiment 1 results for normed priming.", "labels": [], "entities": []}]}