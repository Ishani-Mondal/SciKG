{"title": [], "abstractContent": [{"text": "In this paper we investigate methods for computing similarity of two phrases based on their relatedness scores across all ranks kin a SVD approximation of a phrase/term co-occurrence matrix.", "labels": [], "entities": []}, {"text": "We confirm the major observations made in previous work and our preliminary experiments indicate that these methods can lead to reliable similarity scores which in turn can be used for the task of paraphrasing.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 137, "end_pos": 154, "type": "METRIC", "confidence": 0.9594201743602753}, {"text": "paraphrasing", "start_pos": 197, "end_pos": 209, "type": "TASK", "confidence": 0.9721346497535706}]}], "introductionContent": [{"text": "Distributional methods for word similarity use large amounts of text to acquire similarity judgments based solely on co-occurrence statistics.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7836894690990448}]}, {"text": "Typically each word is assigned a representation as a point in a high dimensional space, where the dimensions represent contextual features; following this, vector similarity measures are used to judge the meaning relatedness of words.", "labels": [], "entities": []}, {"text": "One way to make these computations more reliable is to use Singular Value Decomposition (SVD) in order to obtain a lower rank approximation of an original co-occurrence matrix.", "labels": [], "entities": []}, {"text": "SVD is a matrix factorization method which has applications in a large number of fields such as signal processing or statistics.", "labels": [], "entities": []}, {"text": "In natural language processing methods such as Latent Semantic Analysis (LSA) use SVD to obtain a factorization of a (typically) word/document co-occurrence matrix.", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.7344940423965454}]}, {"text": "The underlying idea in these models is that the dimensionality reduction will produce meaningful dimensions which represent concepts rather than just terms, rendering similarity measures on these vectors more accurate.", "labels": [], "entities": []}, {"text": "Over the years, it has been shown that these methods can closely match human similarity judgments and that they can be used in various applications such as information retrieval, document classification, essay grading etc.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 156, "end_pos": 177, "type": "TASK", "confidence": 0.8314519226551056}, {"text": "document classification", "start_pos": 179, "end_pos": 202, "type": "TASK", "confidence": 0.8187068104743958}, {"text": "essay grading", "start_pos": 204, "end_pos": 217, "type": "TASK", "confidence": 0.7247601449489594}]}, {"text": "However it has been noted that the success of these methods is drastically determined by the choice of dimension k to which the original space is reduced.", "labels": [], "entities": []}, {"text": "() investigates exactly this aspect and proves that no fixed choice of dimension is appropriate.", "labels": [], "entities": []}, {"text": "The authors show that two terms can be reliably compared only by investigating the curve of their relatedness scores overall dimensions k.", "labels": [], "entities": []}, {"text": "The authors use a term/document matrix and analyze relatedness curves for inducing a hard related/not-related decision and show that their algorithms significantly improve over previous methods for information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 198, "end_pos": 219, "type": "TASK", "confidence": 0.7679487764835358}]}, {"text": "In this paper we investigate: 1) how the findings of) carryover to acquiring paraphrases using SVD on a phrase/term co-occurrence matrix and 2) if reliable similarity scores can be obtained from the analysis of relatedness curves.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to compute the similarity score between two phrases, we follow () and compute two similarity scores, corresponding to the X-fillers and Y-fillers, and multiply them.", "labels": [], "entities": []}, {"text": "Given a similarity function, any pattern encountered in the corpus can be paraphrased by returning its most similar patterns.", "labels": [], "entities": []}, {"text": "We implement five similarity functions on the data we have described in the previous section.", "labels": [], "entities": []}, {"text": "The first one is the DIRT algorithm and it is the only method using the original co-occurrence matrix in which raw counts are replaced by pointwise mutual information scores.", "labels": [], "entities": []}, {"text": "DIRT method The similarity function for two vectors pi and p j is: where values in pi and p j are point-wise mutual information, and I(\u00b7) gives the indices of nonnegative values in a vector.", "labels": [], "entities": [{"text": "I", "start_pos": 133, "end_pos": 134, "type": "METRIC", "confidence": 0.9573350548744202}]}, {"text": "Methods on SVD factorization All these methods perform computations the (85000, 800) U matrix in the SVD factorization.", "labels": [], "entities": [{"text": "SVD factorization", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7682751417160034}]}, {"text": "On this we implement two methods which do an arbitrary dimension cut of k = 600: 1) SP-600 (scalar product) and 2) COS-600 (cosine similarity).", "labels": [], "entities": []}, {"text": "The other two algorithms: CurveS1 and CurveS2 use the two curve smoothness functions in Section 3.2; the curves plot the scalar product corresponding to the two patterns, from dimension 1 to 800.", "labels": [], "entities": []}, {"text": "Data In these preliminary experiments we limit ourselves to paraphrasing a set of patterns extracted from a subset of the TREC02-TREC06 question answering tracks.", "labels": [], "entities": [{"text": "TREC02-TREC06 question answering", "start_pos": 122, "end_pos": 154, "type": "TASK", "confidence": 0.7335850199063619}]}, {"text": "From these questions we extracted and paraphrased the most frequently occurring 20 patterns.", "labels": [], "entities": []}, {"text": "Since judging the correctness of these paraphrases \"out-of-context\" is rather difficult we limit ourselves to giving examples and analyzing errors made on this data; important observations can be clearly made this way, however in future work we plan to build a proper evaluation setting (e.g. task-based or instancebased in the sense of) fora more detailed analysis of the performance on the methods discussed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: DIRT-like vector representation in the Y-filler", "labels": [], "entities": []}]}