{"title": [{"text": "Type Level Clustering Evaluation: New Measures and a POS Induction Case Study", "labels": [], "entities": []}], "abstractContent": [{"text": "Clustering is a central technique in NLP.", "labels": [], "entities": []}, {"text": "Consequently, clustering evaluation is of great importance.", "labels": [], "entities": [{"text": "clustering evaluation", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.9680614769458771}]}, {"text": "Many clustering algorithms are evaluated by their success in tagging corpus tokens.", "labels": [], "entities": [{"text": "tagging corpus tokens", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.9114798903465271}]}, {"text": "In this paper we discuss type level evaluation, which reflects class membership only and is independent of the token statistics of a particular reference corpus.", "labels": [], "entities": [{"text": "type level evaluation", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7803960641225179}]}, {"text": "Type level evaluation casts light on the merits of algorithms, and for some applications is a more natural measure of the algorithm's quality.", "labels": [], "entities": [{"text": "Type level evaluation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7368300358454386}]}, {"text": "We propose new type level evaluation measures that, contrary to existing measures , are applicable when items are pol-ysemous, the common casein NLP.", "labels": [], "entities": []}, {"text": "We demonstrate the benefits of our measures using a detailed case study, POS induction.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.8520364165306091}]}, {"text": "We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate , which underscores the fact that these two approaches reveal different aspects of clustering quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Clustering is a central machine learning technique.", "labels": [], "entities": []}, {"text": "In NLP, clustering has been used for virtually every semi-and unsupervised task, including POS tagging, labeled parse tree induction, verb-type classification (Schulte im), lexical acquisition (, multilingual document clustering (), coreference resolution () and named entity recognition.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.8379424512386322}, {"text": "labeled parse tree induction", "start_pos": 104, "end_pos": 132, "type": "TASK", "confidence": 0.7556109875440598}, {"text": "verb-type classification", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.7291964739561081}, {"text": "lexical acquisition", "start_pos": 173, "end_pos": 192, "type": "TASK", "confidence": 0.7227525562047958}, {"text": "multilingual document clustering", "start_pos": 196, "end_pos": 228, "type": "TASK", "confidence": 0.6138216455777487}, {"text": "coreference resolution", "start_pos": 233, "end_pos": 255, "type": "TASK", "confidence": 0.9430626630783081}, {"text": "named entity recognition", "start_pos": 263, "end_pos": 287, "type": "TASK", "confidence": 0.7053067088127136}]}, {"text": "Consequently, the methodology of clustering evaluation is of great importance.", "labels": [], "entities": [{"text": "clustering evaluation", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.9687225222587585}]}, {"text": "In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks.", "labels": [], "entities": []}, {"text": "External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. ().", "labels": [], "entities": [{"text": "External evaluation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7489394247531891}, {"text": "clustering evaluation", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.9418773651123047}]}, {"text": "In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus.", "labels": [], "entities": [{"text": "type level evaluation", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.74834144115448}, {"text": "gold standard corpus", "start_pos": 165, "end_pos": 185, "type": "DATASET", "confidence": 0.7325247923533121}]}, {"text": "Many clustering algorithms are evaluated by their success in tagging corpus tokens.", "labels": [], "entities": [{"text": "tagging corpus tokens", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.9114798903465271}]}, {"text": "However, in many cases a type level evaluation is the natural one.", "labels": [], "entities": []}, {"text": "This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word.", "labels": [], "entities": []}, {"text": "In addition, even when the goal is corpus tagging, a type level evaluation is highly valuable, since it may cast light on the relative or absolute merits of different algorithms (as we show in this paper).", "labels": [], "entities": [{"text": "corpus tagging", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7172008752822876}]}, {"text": "Clustering evaluation has been extensively investigated (Section 3).", "labels": [], "entities": [{"text": "Clustering evaluation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8833787143230438}]}, {"text": "However, the discussion centers around the monosemous case, where each item belongs to exactly one cluster, although polysemy is the common casein NLP.", "labels": [], "entities": []}, {"text": "The contribution of the present paper is as follows.", "labels": [], "entities": []}, {"text": "First, we discuss the issue of type level evaluation and explain why even in the monosemous case a token level evaluation presents a skewed picture (Section 2).", "labels": [], "entities": [{"text": "type level evaluation", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.6871974070866903}]}, {"text": "Second, we show for the common polysemous case why adapting existing information-theoretic measures to type level evaluation is not natural (Section 3).", "labels": [], "entities": []}, {"text": "Third, we propose new mapping-based measures and algorithms to compute them (Section 4).", "labels": [], "entities": []}, {"text": "Finally, we perform a detailed case study with part-of-speech (POS) induction (Section 5).", "labels": [], "entities": [{"text": "part-of-speech (POS) induction", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6026938796043396}]}, {"text": "We compare seven leading algorithms, showing that token and type level measures can weakly or even negatively correlate.", "labels": [], "entities": []}, {"text": "This shows that type level evaluation indeed reveals aspects of a clustering solution that are not revealed by the common tagging-based evaluation.", "labels": [], "entities": []}, {"text": "Clustering is avast research area.", "labels": [], "entities": []}, {"text": "As far as we know, this is the first NLP paper to propose type level measures for the polysemous case.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section motivates why both type and token level external evaluations should be done, even in the monosemous case.", "labels": [], "entities": []}, {"text": "Clustering algorithms compute a set of induced clusters (a clustering).", "labels": [], "entities": []}, {"text": "Some algorithms directly compute a clustering, while some others produce a tagging of corpus tokens from which a clustering can be easily derived.", "labels": [], "entities": []}, {"text": "A clustering is monosemous if each item is allowed to belong to a single cluster only, and polysemous otherwise.", "labels": [], "entities": []}, {"text": "An external evaluation is one which is based on a comparison of an algorithm's result to a gold standard.", "labels": [], "entities": []}, {"text": "In this paper we focus solely on external evaluation, which is the most common evaluation approach in NLP.", "labels": [], "entities": []}, {"text": "Token and type level evaluations reflect different aspects of a clustering.", "labels": [], "entities": []}, {"text": "External token level evaluation assesses clustering quality according to the clustering's accuracy on a given manually annotated corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9962594509124756}]}, {"text": "This is certainly a useful evaluation measure, e.g. when the purpose of the clustering algorithm is to annotate a corpus to serve as input to another application.", "labels": [], "entities": []}, {"text": "External type level evaluation views the computed clustering as a set membership structure and evalutes it independently of the token statistics in the gold standard corpus.", "labels": [], "entities": [{"text": "gold standard corpus", "start_pos": 152, "end_pos": 172, "type": "DATASET", "confidence": 0.7316643993059794}]}, {"text": "There are two main cases in which this is useful.", "labels": [], "entities": []}, {"text": "First, a type level evaluation can be the natural one in light of the problem itself.", "labels": [], "entities": []}, {"text": "For example, if the purpose of the clustering algorithm is to automatically build a lexicon (e.g., VerbNet ()), then the lexicon structure itself should be evaluated.", "labels": [], "entities": []}, {"text": "Second, it maybe valuable to decouple corpus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different statistics.", "labels": [], "entities": []}, {"text": "In other words, if we evaluate an algorithm that will be invoked on a diverse set of corpora having different token statistics, a type level evaluation might provide a better picture (or at least a complementary one) on the quality of the clustering algorithm.", "labels": [], "entities": []}, {"text": "To motivate type level evaluation, consider POS induction, which exemplifies both cases above.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.6481007486581802}]}, {"text": "Clearly, a word form may belong to several parts of speech (e.g., 'contrast' is both a noun and a verb, 'fast' is both an adjective and an adverb, 'that' can be a determiner, conjunction and adverb, etc.).", "labels": [], "entities": []}, {"text": "As an evaluation of a POS induction algorithm, it is natural to evaluate the lexicon it generates, even if the main goal is to annotate a corpus.", "labels": [], "entities": [{"text": "POS induction algorithm", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.8235230843226115}]}, {"text": "The lexicon lists the possible POS tags for each word, and thus its evaluation is a polysemous type level one.", "labels": [], "entities": []}, {"text": "Even if we ignore polysemy, type level evaluation is useful fora POS induction algorithm used to tag a corpus.", "labels": [], "entities": []}, {"text": "There are POS classes whose members are very frequent, e.g., determiners and prepositions.", "labels": [], "entities": []}, {"text": "Here, a very small number of word types usually accounts fora large portion of corpus tokens.", "labels": [], "entities": []}, {"text": "For example, in the WSJ Penn Treebank (, there are 43,740 word types and over 1M word tokens.", "labels": [], "entities": [{"text": "WSJ Penn Treebank (", "start_pos": 20, "end_pos": 39, "type": "DATASET", "confidence": 0.9000912755727768}]}, {"text": "Of the types, 88 are tagged as prepositions.", "labels": [], "entities": []}, {"text": "These types account for only 0.2% of the types, but for as many as 11.9% of the tokens.", "labels": [], "entities": []}, {"text": "An algorithm which is accurate only on prepositions would do much better in a token level evaluation than in a type level one.", "labels": [], "entities": []}, {"text": "This phenomenon is not restricted to prepositions or English.", "labels": [], "entities": []}, {"text": "In the WSJ corpus, determiners account for 0.05% of the types but for 9.8% of the tokens.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.8793251216411591}, {"text": "determiners", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9085142016410828}]}, {"text": "In the German NEGRA corpus, the article class (both definite and indefinite) accounts for 0.04% of the word types and for 12.5% of the word tokens, and the coordinating conjunctions class accounts for 0.05% of the word types but for 3% of the tokens.", "labels": [], "entities": [{"text": "German NEGRA corpus", "start_pos": 7, "end_pos": 26, "type": "DATASET", "confidence": 0.8021543224652609}]}, {"text": "The type and token behavior differences result from the Zipfian distribution of word tokens to word types).", "labels": [], "entities": []}, {"text": "Since the word frequency distribution is Zipfian, any clustering algorithm that is accurate only on a small number of frequent words (not necessarily members of a particular class) would perform well in a token level evaluation but not in a type one.", "labels": [], "entities": []}, {"text": "For example, the most frequent 100 word types (regardless of POS class) in WSJ (NEGRA) account for 43.9% (41.3%) of the tokens in the corpus.", "labels": [], "entities": [{"text": "WSJ (NEGRA)", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.8049957156181335}]}, {"text": "These words appear in 32 out of the 34 non-punctuation POS classes in WSJ and in 38 out of the 51 classes in NEGRA.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8537470698356628}, {"text": "NEGRA", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.9468384385108948}]}, {"text": "Other natural language entities also demonstrate Zipfian distribution of tokens to types.", "labels": [], "entities": []}, {"text": "For example, the distribution of syntactic categories in parse tree constituents is Zipfian, as shown in for English, German and Chinese corpora.", "labels": [], "entities": []}, {"text": "Thus, the distinction between token and type level evaluation is important also for grammar induction algorithms.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.7221521586179733}]}, {"text": "It maybe argued that a token level evaluation is sufficient since it already reflects type information.", "labels": [], "entities": []}, {"text": "In this paper we demonstrate that this is not the case, by showing that they correlate weakly or even negatively in an important NLP task.", "labels": [], "entities": []}, {"text": "Many measures have been proposed in the past decades.", "labels": [], "entities": []}, {"text": "In this section, we briefly survey the three main types: mapping based, counting pairs, and information theoretic measures, and motivate our decision to focus on the first in this paper.", "labels": [], "entities": []}, {"text": "Mapping based measures are based on a postprocessing step in which each induced cluster is mapped to a gold class (or vice versa).", "labels": [], "entities": []}, {"text": "The standard mappings are greedy many-to-one (M-1) and greedy one-to-one (1-1).", "labels": [], "entities": []}, {"text": "Several measures which rely on these mappings were proposed.", "labels": [], "entities": []}, {"text": "The most common and perhaps the simplest one is accuracy, which computes the fraction of items correctly clustered under the mapping.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9996412992477417}]}, {"text": "Other measures include: L (Larsen, 1999), D), misclassification index (MI) (), H (), clustering F-measure () and micro-averaged precision and recall (.", "labels": [], "entities": [{"text": "misclassification index (MI)", "start_pos": 46, "end_pos": 74, "type": "METRIC", "confidence": 0.9493703484535218}, {"text": "F-measure", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.6244380474090576}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9151025414466858}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9906430840492249}]}, {"text": "In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case.", "labels": [], "entities": []}, {"text": "Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering.", "labels": [], "entities": []}, {"text": "Among these are Rand Index, Adjusted Rand Index (, \u0393 statistic (Hubert and Schultz, 1976), Jaccard (, Fowlkes-Mallows ( and Mirkin.", "labels": [], "entities": [{"text": "Rand Index", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.6284804940223694}, {"text": "Adjusted Rand Index", "start_pos": 28, "end_pos": 47, "type": "METRIC", "confidence": 0.8693512678146362}, {"text": "Jaccard", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9368870854377747}]}, {"text": "Schulte im used such a measure for type level evaluation of monosemous verb type clustering.", "labels": [], "entities": [{"text": "monosemous verb type clustering", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.6154335662722588}]}, {"text": "Meila described a few problems with such measures.", "labels": [], "entities": [{"text": "Meila", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9101345539093018}]}, {"text": "A serious one is that their values are unbounded, making it hard to interpret their results.", "labels": [], "entities": []}, {"text": "This can be solved by adjusting their values to lie in, but even adjusted measures suffer from severe distributional problems, limiting their usability in practice.", "labels": [], "entities": []}, {"text": "We thus do not address counting pairs measures in this paper.", "labels": [], "entities": []}, {"text": "Information-theoretic (IT) measures.", "labels": [], "entities": []}, {"text": "IT measures assume that the items in the dataset are taken from a known distribution (usually the uniform distribution), and thus the gold and induced clusters can be treated as random variables.", "labels": [], "entities": []}, {"text": "These measures utilize a co-occurrence matrix I between the gold and induced clusters.", "labels": [], "entities": []}, {"text": "We denote the induced clustering by K and the gold clustering by C. I ij contains the number of items in the intersection of the i-th gold class and the j-th induced cluster.", "labels": [], "entities": []}, {"text": "When assuming the uniform distribution, the probability of an event (a gold class cor an induced cluster k) is its relative size, so Under this assumption we define the entropies and the conditional entropies: In Section 5 we use two IT measures for token level evaluation, V ( and NVI) (a normalized version of VI).", "labels": [], "entities": []}, {"text": "The appealing properties of these measures have been extensively discussed in these references; see also).", "labels": [], "entities": []}, {"text": "V and NVI are defined as follows: In the monosemous case (type or token), the application of the measures described in this section to type level evaluation is straightforward.", "labels": [], "entities": []}, {"text": "In the polysemous case, however, they suffer from serious shortcomings.", "labels": [], "entities": []}, {"text": "Consider a casein which each item is assigned exactly r gold clusters and each gold cluster has the exact same number of items (i.e., each has a size of l\u00b7r |C| , where l is the number of items).", "labels": [], "entities": []}, {"text": "Now, consider an induced clustering where there are |C| induced clusters (|K| = |C|) and each item is assigned to all induced clusters.", "labels": [], "entities": []}, {"text": "The co-occurrence matrix in this case should have identical values in all its entries.", "labels": [], "entities": []}, {"text": "Even if we allow the weight each item contributes to the matrix to depend on its gold and induced entry sizes, the situation will remain the same.", "labels": [], "entities": []}, {"text": "This is because all items have the exact same entry size and both gold and induced clusterings have uniform cluster sizes.", "labels": [], "entities": []}, {"text": "In this case, the random variables defined by the induced and gold clustering assignments are independent (this easily follows from the definition of independent events, since the joint probability is the multiplication of the marginals).", "labels": [], "entities": []}, {"text": "Hence, H(K|C) = H(K) and H(C|K) = H(C), and both V and NVI obtain their worst possible values . However, the score should surely depend on r (the size of each word's gold entry).", "labels": [], "entities": []}, {"text": "Specifically, when r = |C| we get that the induced and gold clusterings are identical.", "labels": [], "entities": []}, {"text": "This case should not get the worst score, and it should definitely score higher than the casein which r = 1, where K is dramatically different from C.", "labels": [], "entities": []}, {"text": "The problem can in theory be solved by providing the number of clusters per item as an input to the algorithm.", "labels": [], "entities": []}, {"text": "However, in NLP this is unrealistic (even if the total number of clusters can be provided) and the number should be determined by the algorithm.", "labels": [], "entities": []}, {"text": "We therefore do not consider IT-based measures in this paper, deferring them to future work.", "labels": [], "entities": []}, {"text": "In this section we present new type level evaluation measures for the polysemous case.", "labels": [], "entities": []}, {"text": "As we show below, these measures do not suffer from the problems discussed for IT measures in Section 3.", "labels": [], "entities": []}, {"text": "All measures are mapping-based: first, a mapping between the induced and gold clusters is performed, and then a measure E is computed.", "labels": [], "entities": []}, {"text": "As is common in the clustering evaluation literature (Section 3), we use M-1 and 1-1 greedy mappings, defined to be those that maximize the corresponding measure E.", "labels": [], "entities": []}, {"text": "Let C = {c 1 , ..., c n } be the set of gold classes and K = {k 1 , ..., km } be the set of induced clusters.", "labels": [], "entities": []}, {"text": "Denote the number of words types by l.", "labels": [], "entities": []}, {"text": "Let A i \u2282 C, Bi \u2282 K, i = 1...l be the set of gold classes and set of induced clusters for each word.", "labels": [], "entities": []}, {"text": "The polysemous nature of task is reflected by the fact that A i and Bi are subsets, rather than members, of C and K respectively.", "labels": [], "entities": []}, {"text": "Our measures address quality from two persectives, that of the individual items clustered (Section 4.1) and that of the clusters (Section 4.2).", "labels": [], "entities": []}, {"text": "Item-based measures especially suit evaluation of clustering quality for the purpose of lexicon induction, and have no counterpart in the monosemous case.", "labels": [], "entities": [{"text": "lexicon induction", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7455386519432068}]}, {"text": "Cluster-based measures area direct generalization of existing mapping based measures to the polysemous case.", "labels": [], "entities": []}, {"text": "The difficulty in designing item-based and cluster-based measures is that the number of clusters assigned to each item is determined by the clustering algorithm.", "labels": [], "entities": []}, {"text": "Below we show how to overcome this.", "labels": [], "entities": []}, {"text": "For a given mapping h : K \u2192 C, denote h(B i ) = {h(x) : x \u2208 Bi }.", "labels": [], "entities": []}, {"text": "A fundamental quantity for item-based evaluation is the number of correct clusters for each item (word type) under this mapping, denoted by IM i (IM stands for 'item match'): The total item match IM is defined to be: In the monosemous case, IM is normalized by the number of items, yielding an accuracy score.", "labels": [], "entities": [{"text": "IM", "start_pos": 241, "end_pos": 243, "type": "METRIC", "confidence": 0.9635887742042542}, {"text": "accuracy score", "start_pos": 294, "end_pos": 308, "type": "METRIC", "confidence": 0.9759840369224548}]}, {"text": "Applying a similar definition in the polysemous case, normalizing instead by the total number of gold clusters assigned to the items, can be easily manipulated.", "labels": [], "entities": []}, {"text": "Even a clustering which has the correct number of induced clusters (equal to the number of gold classes) but which assigns each item to all induced clusters, receives a perfect score under both greedy M-1 and 1-1 mappings.", "labels": [], "entities": []}, {"text": "This holds for any induced clustering for which \u2200i, A i \u2282 h(B i ).", "labels": [], "entities": []}, {"text": "Note that using a mapping from C to K (or a combination of both directions) would exhibit the same problem.", "labels": [], "entities": []}, {"text": "To overcome the problem, we use the harmonic average of two normalized terms (F-score).", "labels": [], "entities": [{"text": "F-score", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9944251775741577}]}, {"text": "We use two average variants, micro and macro.", "labels": [], "entities": []}, {"text": "Macro average computes the total number of matches overall words and normalizes in the end.", "labels": [], "entities": []}, {"text": "Recall (R), Precision (P) and their harmonic average (Fscore) are accordingly defined: is a constant depending on h.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9454235881567001}, {"text": "Precision (P)", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9634808301925659}, {"text": "harmonic average (Fscore)", "start_pos": 36, "end_pos": 61, "type": "METRIC", "confidence": 0.810344660282135}]}, {"text": "As all items are equally weighted, those with larger gold and induced entries have more impact on the measure.", "labels": [], "entities": []}, {"text": "The micro average, aiming to give all items an equal status, first computes an F-score for each item and then averages over them.", "labels": [], "entities": [{"text": "F-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9978777170181274}]}, {"text": "Hence, each item contributes at most 1 to the measure.", "labels": [], "entities": []}, {"text": "This MicroI measure is given by: Where w i (h) is a weight depending on h but also on i.", "labels": [], "entities": [{"text": "MicroI measure", "start_pos": 5, "end_pos": 19, "type": "METRIC", "confidence": 0.5003284513950348}]}, {"text": "For both measures, the maximum score is 1.", "labels": [], "entities": []}, {"text": "It is obtained if and only if A i = h(B i ) for every i.", "labels": [], "entities": []}, {"text": "In 1-1 mapping, when the number of induced clusters is larger than the number of gold clusters, some of the induced clusters are not mapped.", "labels": [], "entities": []}, {"text": "To preserve the nature of 1-1 mapping that punishes for excessive clusters 2 , we define |h(B i )| to be equal to |B i | even for these unmapped clusters.", "labels": [], "entities": []}, {"text": "Recall that any induced clustering in which \u2200i, A i \u2282 h(B i ) gets the best score under a greedy mapping with the accuracy measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9993209838867188}]}, {"text": "In MacroI and MicroI the obtained recalls are perfect, but the precision terms reflect deviation from the correct solution.", "labels": [], "entities": [{"text": "MicroI", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.9417330622673035}, {"text": "recalls", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9841222763061523}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9990504384040833}]}, {"text": "And to allow us to compute it accurately, see below.", "labels": [], "entities": []}, {"text": "In the example in Section 3 showing an unreasonable behavior of IT-based measures, the score depends on r for both MacroI and MicroI.", "labels": [], "entities": [{"text": "MicroI", "start_pos": 126, "end_pos": 132, "type": "DATASET", "confidence": 0.9442809820175171}]}, {"text": "With our new measures, recall is always 1, but precision is r n . This is true both for 1-1 and M-1 mappings.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9996144771575928}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9994485974311829}]}, {"text": "Hence, the new measures show reasonable behavior in this example for all r values.", "labels": [], "entities": []}, {"text": "MicroI was used in (Dasgupta and Ng, 2007) with a manually compiled mapping.", "labels": [], "entities": [{"text": "MicroI", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9392919540405273}]}, {"text": "Their mapping was not based on a well-defined scheme but on a heuristic.", "labels": [], "entities": []}, {"text": "Moreover, providing a manual mapping might be impractical when the number of clusters is large, and can be inaccurate, especially when the clustering is not of very high quality.", "labels": [], "entities": []}, {"text": "In the following we discuss how to compute the 1-1 and M-1 greedy mappings for each measure.", "labels": [], "entities": []}, {"text": "The cluster-based evaluation measures we propose area direct generalization of existing monosemous mapping based measures to the polysemous type case.", "labels": [], "entities": []}, {"text": "For a given mapping h : K \u2192 C, we define \u00af h : K h \u2192 C.", "labels": [], "entities": []}, {"text": "K h is defined to be a clustering which is obtained by performing set union between every two clusters in K that are mapped to the same gold cluster.", "labels": [], "entities": []}, {"text": "The resulting \u00af h is always 1-1.", "labels": [], "entities": []}, {"text": "We denote |K h | = m h . Our motivation for using \u00af h in the definition of the measures instead of h is to stay as close as possible to accuracy, the most common mappingbased measure in the monosemous case.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9992687106132507}]}, {"text": "M-1 (monosemous) accuracy does not punish for spliting classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9915822148323059}, {"text": "spliting classes", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.876349538564682}]}, {"text": "For instance, in a case where there is a gold cluster c i and two induced clusters k 1 and k 2 such that c i = k 1 \u222a k 2 , the M-1 accuracy is the same as in the case where there is one cluster k 1 such that c i = k 1 . M-1 accuracy, despite its indifference to splitting, was shown to reflect better than 1-1 accuracy the clustering's applicability for subsequent applications (at least in some contexts).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9572305679321289}, {"text": "accuracy", "start_pos": 224, "end_pos": 232, "type": "METRIC", "confidence": 0.9872763752937317}, {"text": "accuracy", "start_pos": 310, "end_pos": 318, "type": "METRIC", "confidence": 0.9423089623451233}]}, {"text": "Recall that in item-based evaluation, IM i measures the intersection between the induced and gold entries of each item.", "labels": [], "entities": [{"text": "IM", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9149349927902222}]}, {"text": "Therefore, the set union operation is not needed for that case, since when an item appears in two induced clusters that are mapped to the same gold cluster, its IM i is increased only by 1.", "labels": [], "entities": [{"text": "IM i", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9714345335960388}]}, {"text": "A fundamental quantity for cluster-based evaluation is the intersection between each induced cluster and the gold class to which it is mapped.", "labels": [], "entities": []}, {"text": "We denote this value by CM j (CM stands for 'cluster match'): The total intersection (CM ) is accordingly defined to be: As with the item-based evaluation (Section 4.1), using CM or a derived accuracy as a measure is problematic.", "labels": [], "entities": [{"text": "total intersection (CM )", "start_pos": 66, "end_pos": 90, "type": "METRIC", "confidence": 0.7259589195251465}, {"text": "accuracy", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9625074863433838}]}, {"text": "A clustering that assigns n induced classes to each word (n is the number of gold classes) will get the highest possible score under every greedy mapping (1-1 or M-1), as will any clustering in which \u2200i, A i \u2282 h(B i ).", "labels": [], "entities": []}, {"text": "As in the item-based evaluation, a possible solution is based on defining recall, precision and Fscore measures, computed either in the micro or in the macro level.", "labels": [], "entities": [{"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.999373733997345}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9991098046302795}, {"text": "Fscore", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9996861219406128}]}, {"text": "The macro cluster-based measure turns out to be identical to the macro item-based measure MacroI . The following derivation shows the equivalence for the 1-1 case.", "labels": [], "entities": []}, {"text": "The M-1 case is similar.", "labels": [], "entities": []}, {"text": "We note that h = \u00af h in the 1-1 case and we therefore exchange them in the definition of CM . It is enough to show that CM = IM , since the denominator is the same in both cases: The micro cluster-based measures are defined: The micro cluster measure MicroC is obtained by taking a weighted average over the F j 's: z\u2208K h |z| is the number of clustered items after performing the set union and including repetitions.", "labels": [], "entities": []}, {"text": "If, in the 1-1 case where m > n, an induced cluster is not mapped, we define F k = 0.", "labels": [], "entities": [{"text": "F", "start_pos": 77, "end_pos": 78, "type": "METRIC", "confidence": 0.9745146036148071}]}, {"text": "A definition of the measure using a reverse mapping (i.e., from C to K) would have used a weighted average with weights proportional to the gold classes' sizes.", "labels": [], "entities": []}, {"text": "The definition of \u00af h causes a similar computational difficulty as in the M-1 item-based measures.", "labels": [], "entities": []}, {"text": "Consequently, we apply a hill climbing algorithm similar to the one described in Section 4.1.", "labels": [], "entities": []}, {"text": "The 1-1 mapping is computed using the same bipartite graph method described in Section 4.1.", "labels": [], "entities": []}, {"text": "The graph's vertices correspond to gold and induced clusters and an edge's weight is the F-score between the class and cluster corresponding to its vertices times the cluster's weight (|k|/N * ).", "labels": [], "entities": [{"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.997305154800415}]}, {"text": "As a detailed case study for the ideas presented in this paper, we apply the various measures for the POS induction task, using seven leading POS induction algorithms.", "labels": [], "entities": [{"text": "POS induction task", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.8501028815905253}]}, {"text": "We experimented with the following models: ARR10 (), Clark03, GG07 (Goldwater and Griffiths, 2007), GJ08 (, and GVG09 (Van Gael et al., 2009) (three models).", "labels": [], "entities": [{"text": "ARR10", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.9191752076148987}, {"text": "Clark03", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.6989958882331848}, {"text": "GG07 (Goldwater and Griffiths, 2007)", "start_pos": 62, "end_pos": 98, "type": "DATASET", "confidence": 0.826813705265522}, {"text": "GVG09 (Van Gael et al., 2009)", "start_pos": 112, "end_pos": 141, "type": "DATASET", "confidence": 0.8436701695124308}]}, {"text": "Additional recent good results for various variants of the POS induction problem are described in e.g.,).", "labels": [], "entities": [{"text": "POS induction problem", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.873596986134847}]}, {"text": "Clark03 and ARR10 are monosemous algorithms, allowing a single cluster for each word type.", "labels": [], "entities": [{"text": "Clark03", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9751920700073242}, {"text": "ARR10", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.9159907102584839}]}, {"text": "The other algorithms are polysemous.", "labels": [], "entities": []}, {"text": "They perform sequence labeling where each token is tagged in its context, and different tokens (instances) of the same type (word form) may receive different tags.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.6669581830501556}]}, {"text": "All models were tested on sections 2-21 of the PTB-WSJ, which consists of 39832 sentences, 950028 tokens and 39546 unique types.", "labels": [], "entities": [{"text": "PTB-WSJ", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.5324481725692749}]}, {"text": "Of the tokens, 832629 (87.6%) are not punctuation marks.", "labels": [], "entities": []}, {"text": "Type level evaluation used the measures MacroI (which is equal to MacroC), MicroI and MicroC both with greedy 1-1 and M-1 mappings as described in Section 4.", "labels": [], "entities": [{"text": "MicroI", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.6915856003761292}, {"text": "MicroC", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.8083580732345581}]}, {"text": "The type level gold (induced) entry is defined to be the set of all gold (induced) clusters with which it appears.", "labels": [], "entities": []}, {"text": "For the token level evaluation, six measures are used (see Section 3): accuracy with M-1 and 1-1 mappings, NVI, V, H(C|K) and H(K|C), using e as the logarithm's base.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9994741082191467}]}, {"text": "We use the full WSJ POS tags set excluding punctuation . Punctuation.", "labels": [], "entities": [{"text": "WSJ POS tags set", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.8953052163124084}]}, {"text": "Punctuation marks occupy a large volume of the corpus tokens (12.4% in our experimental corpus), and are easy to cluster.", "labels": [], "entities": []}, {"text": "Clustering punctuation marks thus greatly inflates token level results.", "labels": [], "entities": []}, {"text": "To study the relationship between type and token level evaluations in a focused manner, we excluded punctuation from the evaluation (they are still used during training, so algorithms that rely on them are not harmed).", "labels": [], "entities": []}, {"text": "The number of gold POS tags in WSJ is 45, of which 11 are punctuation marks.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.7169245481491089}]}, {"text": "Therefore, for the ARR10 and Clark03 models, 34 clusters were induced.", "labels": [], "entities": [{"text": "ARR10", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.965284526348114}, {"text": "Clark03", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.7171268463134766}]}, {"text": "For GJ08 we received the output with 45 clusters.", "labels": [], "entities": [{"text": "GJ08", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9269251823425293}]}, {"text": "The iHMM models of GVG09 determine the number of clusters automatically (resulting in 47, 91 and 192 clusters, see below).", "labels": [], "entities": []}, {"text": "For GG07, our computing resources did not enable us to induce 45 clusters and we therefore used 17 . Our focus in this paper is to study the type vs. token distinction rather than to provide a full scope comparison between algorithms, for which more clustering sizes would need to be examined.", "labels": [], "entities": [{"text": "GG07", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8600305318832397}]}, {"text": "We ran the ARR10 tagger with the configuration detailed in ().", "labels": [], "entities": [{"text": "ARR10 tagger", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.9481950402259827}]}, {"text": "For Clark03, we ran his neyessenmorph model 7 10 times (using an unknown words threshold of 5) and report the average score for each measure.", "labels": [], "entities": [{"text": "Clark03", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.938624382019043}]}, {"text": "The models of GVG09 were run in the three configurations reported in their paper: one with a Dirichlet process prior and fixed parameters, another with a Pittman-Yore prior with fixed parameters, and a third with a Dirichlet process prior with parameters learnt from the data.", "labels": [], "entities": [{"text": "GVG09", "start_pos": 14, "end_pos": 19, "type": "DATASET", "confidence": 0.8328052163124084}]}, {"text": "All five models were run in an optimal configuration.", "labels": [], "entities": []}, {"text": "We obtained the code of Goldwater and Griffiths' BHMM model and ran it for 10K iterations with an annealing technique for parameter estimation.", "labels": [], "entities": [{"text": "Goldwater and Griffiths' BHMM model", "start_pos": 24, "end_pos": 59, "type": "DATASET", "confidence": 0.6856146931648255}, {"text": "parameter estimation", "start_pos": 122, "end_pos": 142, "type": "TASK", "confidence": 0.6389720588922501}]}, {"text": "That was the best parameter estimation technique available to us.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.6611928939819336}]}, {"text": "This is the first time that this model is evaluated on such a large experimental corpus, and it performed well under these conditions.", "labels": [], "entities": []}, {"text": "The output of the model of GJ08 was sent to us by the authors.", "labels": [], "entities": [{"text": "GJ08", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.9451811909675598}]}, {"text": "The model was run on sec-tions 2-21 of the WSJ-PTB using significantly inferior computing resources compared to those used for producing the results reported in their paper.", "labels": [], "entities": [{"text": "WSJ-PTB", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9785707592964172}]}, {"text": "While this model cannot be compared to the aforementioned six models due to the suboptimal configuration, we evaluate its output using our measures to get a broader variety of experimental results 8 . presents the scores of the compared models under all evaluation measures (six token level, six type level).", "labels": [], "entities": []}, {"text": "What is important hereto note are the differences between type and token level evaluations for the algorithms.", "labels": [], "entities": []}, {"text": "We are mainly interested in two things: (1) seeing how relative rankings change in the two evaluation types, thus showing that the two types are not highly correlated and are both useful; and (2) insights gained by using a type level evaluation in addition to the usual token level one.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Token level (left columns) and type level (right columns) results for seven POS induction  algorithms (rows) (see text for details). Token and type level performance are weakly correlated and  complement each other as evaluation measures. ARR10, a monosemous algorithm, yields the best results  in most measures. (GJ08* results are different from those reported in the original paper because it was  run with weaker computing resources than those used there.)", "labels": [], "entities": []}]}