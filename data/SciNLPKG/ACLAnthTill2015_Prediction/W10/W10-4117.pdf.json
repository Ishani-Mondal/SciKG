{"title": [{"text": "Exploiting Social Q&A Collection in Answering Complex Questions", "labels": [], "entities": [{"text": "Exploiting Social Q&A Collection", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7392059316237768}, {"text": "Answering Complex Questions", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.9409349163373312}]}], "abstractContent": [{"text": "This paper investigates techniques to automatically construct training data from social Q&A collections such as Yahoo!", "labels": [], "entities": []}, {"text": "Answer to support a machine learning-based complex QA system 1.", "labels": [], "entities": []}, {"text": "We extract cue expressions for each type of question from collected training data and build question-type-specific classifiers to improve complex QA system.", "labels": [], "entities": []}, {"text": "Experiments on 10 types of complex Chinese questions verify that it is effective to mine knowledge from social Q&A collections for answering complex questions, for instance , the F 3 improvement of our system over the baseline and translation-based model reaches 7.9% and 5.1%, respectively .", "labels": [], "entities": [{"text": "F 3", "start_pos": 179, "end_pos": 182, "type": "METRIC", "confidence": 0.9628553688526154}]}], "introductionContent": [{"text": "Research on the topic of QA systems has mainly concentrated on answering factoid, definitional, reason and opinion questions.", "labels": [], "entities": [{"text": "QA systems", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.9663510322570801}, {"text": "answering factoid, definitional, reason and opinion", "start_pos": 63, "end_pos": 114, "type": "TASK", "confidence": 0.7643800005316734}]}, {"text": "Among the approaches proposed to answer these questions, machine learning techniques have been found more effective in constructing QA components from scratch.", "labels": [], "entities": []}, {"text": "Yet these supervised techniques require a certain scale of (question, answer), short for Q&A, pairs as training data.", "labels": [], "entities": []}, {"text": "For example, ( and) constructed 90,000 English Q&A pairs and 2,000 Japanese Q&A pairs, respectively for their factoid QA systems.", "labels": [], "entities": []}, {"text": "() constructed 76 term-definition pairs for their definitional QA systems.", "labels": [], "entities": []}, {"text": "() required a known subjective vocabulary for their opinion QA.", "labels": [], "entities": []}, {"text": "() used 4,849 positive and 521,177 negative examples in their reason QA system.", "labels": [], "entities": []}, {"text": "Among complex QA systems, many other types of questions have not been well studied, apart from reason and definitional questions.", "labels": [], "entities": [{"text": "definitional questions", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.8980951607227325}]}, {"text": "Appendix A lists 10 types of complex Chinese questions and their examples we discussed in this paper.", "labels": [], "entities": []}, {"text": "According to the related studies on QA, supervised machine-learning technique maybe effective for answering these questions.", "labels": [], "entities": [{"text": "QA", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.8502904176712036}]}, {"text": "To employ the supervised approach, we need to reconstruct training Q&A pairs for each type of question, though this is an extremely expensive and labor-intensive task.", "labels": [], "entities": []}, {"text": "To deal with the acquisition problem of training Q&A pairs, we investigate techniques to automatically construct training data by utilizing social Q&A collections crawled from the Web, which contains millions of user-generated Q&A pairs.", "labels": [], "entities": []}, {"text": "Many studies () () have been done on retrieving similar Q&A pairs from social QA websites as answers to test questions.", "labels": [], "entities": []}, {"text": "Our study, however, regards social Q&A websites as a knowledge repository and aims to mine knowledge from them for synthesizing answers to questions from multiple documents.", "labels": [], "entities": []}, {"text": "There is very little literature on this aspect.", "labels": [], "entities": []}, {"text": "Our work can be seen as a kind of query-based summarization) () (, and can also be employed to answer questions that have not been answered in social Q&A websites.", "labels": [], "entities": []}, {"text": "This paper mainly focuses on the following three steps: (1) automatically constructing questiontype-specific training Q&A pairs from the social Q&A collection; (2) extracting cue expressions for each type of question from the collected training data, and (3) building questiontype-specific classifiers to filer out noise sentences before using a state-of-the-art IR formula to select answers.", "labels": [], "entities": []}, {"text": "We evaluate our system on 10 types of Chinese questions by using the Pourpre evaluation tool).", "labels": [], "entities": []}, {"text": "The experimental results show the effectiveness of our system, for instance, the F 3 /N R improvement of our system over the baseline and translation-based model reaches 7.9%/11.1%, and 5.1%/5.6%, respectively.", "labels": [], "entities": [{"text": "F 3 /N R", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9475159049034119}]}], "datasetContent": [{"text": "The NTCIR 2008 test data set () contains 30 complex questions 4 we discussed here.", "labels": [], "entities": [{"text": "NTCIR 2008 test data set", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9796283125877381}]}, {"text": "However, a small number of test questions are included for some question types, e.g.; it contains only 1 hazard-type, 1 scale-type, and 3 significance-type questions.", "labels": [], "entities": []}, {"text": "To form a more complete test set, we create another 65 test questions . Therefore, the test data used in this paper includes 95 complex questions.", "labels": [], "entities": []}, {"text": "For each test question we also provide a list of weighted nuggets, which are used as the gold standard answers for evaluation.", "labels": [], "entities": []}, {"text": "The evaluation is conducted by employing Pourpre v1.0c), which uses the standard scoring methodology for TREC other questions, i.e., answer nugget recall NR, nugget precision NP , and a combination score F 3 of NR and NP . For better understanding, we evaluate the systems when outputting the top N sentences as answers.", "labels": [], "entities": [{"text": "Pourpre", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9555448889732361}, {"text": "TREC", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.5377631783485413}, {"text": "answer nugget recall NR", "start_pos": 133, "end_pos": 156, "type": "METRIC", "confidence": 0.5445679128170013}, {"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.830975353717804}]}, {"text": "where, V q and V s are the vectors of the question and candidate answer.", "labels": [], "entities": []}, {"text": "The TransM denotes a translation model for QA, which uses Q&A pairs as the parallel corpus, with questions to the \"source\" language and answers corresponding to the \"target\" language.", "labels": [], "entities": []}, {"text": "This model can be expressed by: where, q is the question, S the sentence, P (w|t) the probability of translating a sentence term t to the question term w, which is obtained by using the GIZA++ toolkit.", "labels": [], "entities": [{"text": "GIZA++ toolkit", "start_pos": 186, "end_pos": 200, "type": "DATASET", "confidence": 0.8755926887194315}]}, {"text": "We use six million Q&A pairs to train IBM model 1 for obtaining word-to-word probability P (w|t).", "labels": [], "entities": []}, {"text": "Ours errorrate and Ours pre@k denote our models that are based on classifiers optimizing performance measure error-rate and prec@k, respectively.", "labels": [], "entities": [{"text": "errorrate", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.8887889385223389}]}, {"text": "Ours lin , a linear interpolation model, that combines scores of classifiers and the baseline, which is similar to ) and can be expressed by the equation: where, \u03c6(s) is the score calculated by classifiers) and \u03b1 denotes the weight of the score.", "labels": [], "entities": []}, {"text": "This experiment shows that: (1) Questiontype-specific classifiers can greatly outperform the baseline; for example, the F 3 improvements of Ours errorrate and Ours pre@k over the baseline in terms of N =10 are 5.8% and 7.9%, respectively.", "labels": [], "entities": [{"text": "F 3", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9685118496417999}, {"text": "errorrate", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.5227304697036743}]}, {"text": "(2) Ours errorrate is better than Ours pre@k when N < 10.", "labels": [], "entities": [{"text": "Ours errorrate", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.7308800518512726}]}, {"text": "The average numbers of sentences retained in Ours errorrate and Ours pre@k are 130, and 217, respectively.", "labels": [], "entities": [{"text": "errorrate", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.7231735587120056}]}, {"text": "That means the precision of the classifier optimizing errorrate is superior to the classifier optimizing prec@k, while the recall is relatively inferior.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9994716048240662}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9993956089019775}]}, {"text": "(3) Ours lin is worse than Ours errorrate and Ours pre@k , which indicates that using questiontype-specific classifiers by classification is better than using it by interpolation like . (4) Our models also outperform TransM, e.g.; the F 3 improvement is 5.1% when N is set to 10.", "labels": [], "entities": [{"text": "errorrate", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.75773686170578}, {"text": "F 3", "start_pos": 235, "end_pos": 238, "type": "METRIC", "confidence": 0.966643899679184}]}, {"text": "TransM exploits the social Q&A collection without consideration of question types, while our models select and exploit the social Q&A pairs of the same question types.", "labels": [], "entities": []}, {"text": "Thereby, this experiment also indicates that it is better to exploit social Q&A pairs by type of question.", "labels": [], "entities": []}, {"text": "The performance ranking of these models when N =10 is: Ours prec@k > Ours errorrate > Ours lin > TransM > Baseline.", "labels": [], "entities": []}, {"text": "Pourpre v1.0c evaluation is based on n-gram overlap between the automatically produced answers and the human generated reference answers.", "labels": [], "entities": []}, {"text": "Thus, it is notable to measure conceptual equivalent.", "labels": [], "entities": []}, {"text": "In subjective evaluation, the answer sentences returned by systems are labeled by a native Chinese assessor.", "labels": [], "entities": []}, {"text": "shows the distribution of the ranks of the first correct answers for all questions.", "labels": [], "entities": []}, {"text": "This figure demonstrates that the Ours pre@k answers 57 questions which first answers are ranked in top 3, which is larger than that of the baseline, i.e., 49.", "labels": [], "entities": []}, {"text": "Moreover, the Ours pre@k contains only 11.5% of questions which answers are ranked after top 10, while this number of the baseline is 20.7%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Impact of features on Our prec@k .", "labels": [], "entities": [{"text": "Our prec@k", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.8985657840967178}]}, {"text": " Table 5: Performance of Ours pre@k after remov- ing noises in the training Q&A pairs.", "labels": [], "entities": [{"text": "Performance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9383381605148315}]}]}