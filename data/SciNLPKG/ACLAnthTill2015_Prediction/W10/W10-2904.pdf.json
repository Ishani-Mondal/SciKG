{"title": [{"text": "Efficient, correct, unsupervised learning of context-sensitive languages", "labels": [], "entities": []}], "abstractContent": [{"text": "A central problem for NLP is grammar induction: the development of unsupervised learning algorithms for syntax.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7735487222671509}]}, {"text": "In this paper we present a lattice-theoretic representation for natural language syntax, called Distributional Lattice Grammars.", "labels": [], "entities": []}, {"text": "These representations are objective or empiri-cist, based on a generalisation of distribu-tional learning, and are capable of representing all regular languages, some but not all context-free languages and some non-context-free languages.", "labels": [], "entities": []}, {"text": "We present a simple algorithm for learning these grammars together with a complete self-contained proof of the correctness and efficiency of the algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammar induction, or unsupervised learning of syntax, no longer requires extensive justification and motivation.", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8132260739803314}]}, {"text": "Both from engineering and cognitive/linguistic angles, it is a central challenge for computational linguistics.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.7866457104682922}]}, {"text": "However good algorithms for this task are thin on the ground.", "labels": [], "entities": []}, {"text": "There are numerous heuristic algorithms, some of which have had significant success in inducing constituent structure ().", "labels": [], "entities": []}, {"text": "There are algorithms with theoretical guarantees as to their correctness -such as for example Bayesian algorithms for inducing PCFGs), but such algorithms are inefficient: an exponential search algorithm is hidden in the convergence of the MCMC samplers.", "labels": [], "entities": [{"text": "MCMC samplers", "start_pos": 240, "end_pos": 253, "type": "DATASET", "confidence": 0.9490827918052673}]}, {"text": "The efficient algorithms that are actually used are heuristic approximations to the true posteriors.", "labels": [], "entities": []}, {"text": "There are algorithms like the Inside-Outside algorithm) which are guaranteed to converge efficiently, but not necessarily to the right answer: they converge to a local optimum that maybe, and in practice nearly always is very far from the optimum.", "labels": [], "entities": []}, {"text": "There are naive enumerative algorithms that are correct, but involve exhaustively enumerating all representations below a certain size.", "labels": [], "entities": []}, {"text": "There are no correct and efficient algorithms, as there are for parsing, for example.", "labels": [], "entities": [{"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.9806244969367981}]}, {"text": "There is a reason for this: from a formal point of view, the problem is intractably hard for the standard representations in the Chomsky hierarchy.", "labels": [], "entities": []}, {"text": "showed that training stochastic regular grammars is hard; showed that regular grammars cannot be learned even using queries; these results obviously apply also to PCFGs and CFGs as well as to the more complex representations built by extending CFGs, such as TAGs and soon.", "labels": [], "entities": []}, {"text": "However, these results do not necessarily apply to other representations.", "labels": [], "entities": []}, {"text": "Regular grammars are not learnable, but deterministic finite automata are learnable under various paradigms.", "labels": [], "entities": []}, {"text": "Thus it is possible to learn by changing to representations that have better properties: in particular DFAs are learnable because they are \"objective\"; there is a correspondence between the structure of the language, (the residual languages) and the representational primitives of the formalism (the states) which is expressed by the MyhillNerode theorem.", "labels": [], "entities": []}, {"text": "In this paper we study the learnability of a class of representations that we call distributional lattice grammars (DLGs).", "labels": [], "entities": []}, {"text": "Lattice-based formalisms were introduced by and as context sensitive formalisms that are potentially learnable.", "labels": [], "entities": []}, {"text": "established a similar learnability result fora limited class of context free languages.", "labels": [], "entities": []}, {"text": "In, the approach was extended to a significantly larger class but without an explicit learning algorithm.", "labels": [], "entities": []}, {"text": "Most of the building blocks are however in place, though we need to make several modifications and ex-tensions to get a clean result.", "labels": [], "entities": []}, {"text": "Most importantly, we need to replace the representation used there, which naively could be exponential, with a lazy, exemplar based model.", "labels": [], "entities": []}, {"text": "In this paper we present a simple algorithm for the inference of these representations and prove its correctness under the following learning paradigm: we assume that as normal there is a supply of positive examples, and additionally that the learner can query whether a string is in the language or not (an oracle for membership queries).", "labels": [], "entities": []}, {"text": "We also prove that the algorithm is efficient in the sense that it will use a polynomial amount of computation and makes a polynomial number of queries at each step.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows: after some basic discussion of distributional learning in Section 2, we define in Section 3 an exemplar-based grammatical formalism which we call Distributional Lattice Grammars.", "labels": [], "entities": []}, {"text": "We then give a learning algorithm under a reasonable learning paradigm, together with a self contained proof in elementary terms (not presupposing any extensive knowledge of lattice theory), of the correctness of this algorithm.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}