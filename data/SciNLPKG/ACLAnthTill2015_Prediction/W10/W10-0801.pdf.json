{"title": [{"text": "Towards a Domain Independent Semantics: Enhancing Semantic Representation with Construction Grammar", "labels": [], "entities": []}], "abstractContent": [{"text": "In Construction Grammar, structurally patterned units called constructions are assigned meaning in the same way that words are-via convention rather than composition.", "labels": [], "entities": [{"text": "Construction Grammar", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.8457558751106262}]}, {"text": "That is, rather than piecing semantics together from individual lexical items, Construction Grammar proposes that semantics can be assigned at the construction level.", "labels": [], "entities": [{"text": "Construction Grammar", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.8470152318477631}]}, {"text": "In this paper, we investigate whether a classifier can be taught to identify these constructions and consider the hypothesis that identifying construction types can improve the semantic interpretation of previously unseen predicate uses.", "labels": [], "entities": []}, {"text": "Our results show that not only can the constructions be automatically identified with high accuracy, but the classifier also performs just as well with out-of-vocabulary predicates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.99647456407547}]}], "introductionContent": [{"text": "The root of many challenges in natural language processing applications is the fact that humans can convey a single piece of information in numerous and creative ways.", "labels": [], "entities": []}, {"text": "Syntactic variations (e.g. I gave him my book. vs. I gave my book to him.), the use of synonyms (e.g. She bought a used car. vs. She purchased a pre-owned automobile.) and numerous other variations can complicate the semantic analysis and the automatic understanding of a text.", "labels": [], "entities": []}, {"text": "(1) They hissed him out of the university While (1) is clearly understandable for humans, to automatically discern the meaning of hissed in this instance would take more than learning that the verb hiss is defined as \"make a sharp hissing sound\" (WordNet 3.0).", "labels": [], "entities": []}, {"text": "Knowing that hiss can also mean \"a show of contempt\" is helpful.", "labels": [], "entities": []}, {"text": "However, it would also require the understanding that the sentence describes a causative event if we are to interpret this sentence as meaning something like \"They caused him to leave the university by means of hissing or contempt\".", "labels": [], "entities": []}, {"text": "The problem of novel words, expressions and usages are especially significant because discriminative learning methods used for automatic text classification do not perform as well when tested on text with a feature distribution that is different from what was seen in the training data.", "labels": [], "entities": [{"text": "automatic text classification", "start_pos": 127, "end_pos": 156, "type": "TASK", "confidence": 0.6179945270220438}]}, {"text": "This is recognized to be a critical issue in domain adaptation).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7714401185512543}]}, {"text": "Whether we seek to account for words or usages that are infrequent in the training data or to adapt a trained classifier to anew domain of text that includes new vocabulary or new forms of expressions, success in overcoming these challenges partly lies in the successful identification and the use of features that generalize over linguistic variation.", "labels": [], "entities": []}, {"text": "In this paper we borrow from the theories presented by Construction Grammar (CxG) to explore the development of general features that may help account for the linguistic variability and creativity we see in the data.", "labels": [], "entities": [{"text": "Construction Grammar (CxG)", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.731607460975647}]}, {"text": "Specifically, we investigate whether a classifier can be taught to identify constructions as described by CxG and gauge their value in interpreting novel words.", "labels": [], "entities": [{"text": "interpreting novel words", "start_pos": 135, "end_pos": 159, "type": "TASK", "confidence": 0.8477267622947693}]}, {"text": "The development of approaches to effectively capture such novel semantics will enhance applications requiring richer representations of language understanding such as machine translation, information retrieval, and text summarization.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.7834201455116272}, {"text": "information retrieval", "start_pos": 188, "end_pos": 209, "type": "TASK", "confidence": 0.7850979268550873}, {"text": "text summarization", "start_pos": 215, "end_pos": 233, "type": "TASK", "confidence": 0.7665200233459473}]}, {"text": "Consider, for instance, the following machine translation into Spanish by the Google translate (http://translate.google.com/): They hissed him out of the university.", "labels": [], "entities": [{"text": "Google translate", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.8536730408668518}]}, {"text": "\uf0e0 Silbaban fuera de la universidad.", "labels": [], "entities": []}, {"text": "They were whistling outside the university.", "labels": [], "entities": []}, {"text": "The translation has absolutely no implication that a group of people did something to cause another person to leave the university.", "labels": [], "entities": []}, {"text": "However, when the verb is changed to a verb that is seen to frequently appear in a caused motion interpretation (e.g. throw), the results are correct: They threw him out of the university.", "labels": [], "entities": []}, {"text": "\uf0e0 Lo sacaron de la universidad.", "labels": [], "entities": []}, {"text": "They took him out of the university.", "labels": [], "entities": []}, {"text": "Thus, if we could facilitate a caused motion interpretation by bootstrapping semantics from constructions (e.g. \"X ___ Y out of Z\" implies caused motion), we could enable accurate translations that otherwise would not be possible.", "labels": [], "entities": [{"text": "caused motion interpretation", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.7317282954851786}]}], "datasetContent": [{"text": "The data for this study was pulled from the WSJ part of Penn Treebank II (.", "labels": [], "entities": [{"text": "WSJ part of Penn Treebank II", "start_pos": 44, "end_pos": 72, "type": "DATASET", "confidence": 0.9356399973233541}]}, {"text": "From this corpus, all sentences with the syntactic form (NP-SBJ (V NP PP)) were selected.", "labels": [], "entities": []}, {"text": "The selection allowed for intervening adverbial phrases (e.g. \"Sally threw a ball quickly to him\") and additional prepositional phrases (e.g. \"Sally threw a ball to him on Tuesday\" or \"Sally threw a ball in anger into the scorer's table\").", "labels": [], "entities": []}, {"text": "A total of 14.7k instances were identified in this manner.", "labels": [], "entities": []}, {"text": "To reduce the size of the corpus to be labeled to a target of 1800 instances, we removed, firstly, instances containing traces as parsed by the TreeBank.", "labels": [], "entities": []}, {"text": "These included passive usages (e.g. \"Coffee was shipped from Colombia by Gracie\") and instances with traces in the object NP or PP including questions and relative clauses (e.g. \"What did Gracie ship from Colombia?\").", "labels": [], "entities": []}, {"text": "In construction grammar, however, traces do not exist, since grammar is a set of patterns of varying degrees of complexity.", "labels": [], "entities": [{"text": "construction grammar", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.856400340795517}]}, {"text": "Thus CxG would characterize passives, questions structures, and relative clauses as having their own respective phrasal constructions, which combine with the caused-motion construction.", "labels": [], "entities": []}, {"text": "In order to ensure sufficient training data with the standard form of the caused-motion construction as defined in Goldberg 1995 and 2006 (see Section 3.2), we chose to remove these usages.", "labels": [], "entities": []}, {"text": "Secondly, we removed the instances of sentences that can be deterministically categorized as non-caused motion constructions: instances containing ADV, EXT, PRD, VOC, or TMP type object NPs (e.g.\"Cindy drove five hours from Dallas\", \"You listen, boy, to what I say!\").", "labels": [], "entities": []}, {"text": "Because we can automatically identify this category, keeping these examples in our data would have resulted in even higher performance.", "labels": [], "entities": []}, {"text": "We also considered the possibility of reducing the size by removing certain classes of verbs such as verbs of communication (e.g. reply, bark), psychological state (e.g. amuse, admire), or existence (e.g. be, exist).", "labels": [], "entities": []}, {"text": "While it is reasonable to say that these verb types are highly unlikely to appear in a caused-motion construction, if we were to remove sets of verbs based on their likely behavior, we would also be excluding interesting usages such as \"The stand-up comedian amused me into a state of total enjoyment.\" or \"The leader barked a command into a radio.\"", "labels": [], "entities": []}, {"text": "After filtering these sentences, 8700 remained.", "labels": [], "entities": []}, {"text": "From the remaining instances, we selected 1800 instances at random for the experiments presented.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: System Performance (*verb feature baseline)", "labels": [], "entities": []}, {"text": " Table 2: Effect of each feature on the performance in  classification of the caused-motion construction, in the order of  decreasing F-score. Features that performed statistically higher  than the majority class baseline are marked with an * in the last  column.", "labels": [], "entities": [{"text": "F-score", "start_pos": 134, "end_pos": 141, "type": "METRIC", "confidence": 0.9974156618118286}]}, {"text": " Table 3: System performance when the specified feature is  removed from the full set of features, in the order of  increasing F-score. Significant performance degradation,  when compared against the full feature set performance", "labels": [], "entities": [{"text": "F-score", "start_pos": 127, "end_pos": 134, "type": "METRIC", "confidence": 0.9983084201812744}]}]}