{"title": [], "abstractContent": [{"text": "Fluency rankers are used in modern sentence generation systems to pick sentences that are not just grammatical, but also fluent.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7580009698867798}]}, {"text": "It has been shown that feature-based models, such as maximum entropy models, work well for this task.", "labels": [], "entities": []}, {"text": "Since maximum entropy models allow for incorporation of arbitrary real-valued features, it is often attractive to create very general feature templates, that create a huge number of features.", "labels": [], "entities": []}, {"text": "To select the most discrim-inative features, feature selection can be applied.", "labels": [], "entities": []}, {"text": "In this paper we compare three feature selection methods: frequency-based selection , a generalization of maximum entropy feature selection for ranking tasks with real-valued features, and anew selection method based on feature value correlation.", "labels": [], "entities": []}, {"text": "We show that the often-used frequency-based selection performs badly compared to maximum en-tropy feature selection, and that models with a few hundred well-picked features are competitive to models with no feature selection applied.", "labels": [], "entities": []}, {"text": "In the experiments described in this paper , we compressed a model of approximately 490.000 features to 1.000 features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Feature selection is a process that tries to extract S \u2282 F from a set of features F , such that the model using S performs comparably to the model using F . Such a compression of a feature set can be obtained if there are features: that occur sporadically; that correlate strongly with other features (features that show the same behavior within events and contexts); or have values with little or no correlation to the classification or ranking.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7886883318424225}]}, {"text": "Features that do have no correlation to the classification can be removed from the model.", "labels": [], "entities": []}, {"text": "For a set of highly-correlating features, one feature can be selected to represent the whole group.", "labels": [], "entities": []}, {"text": "Initially it may seem attractive to perform fluency selection by training a model on all features, selecting features with relatively high weights.", "labels": [], "entities": [{"text": "fluency selection", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8889540433883667}]}, {"text": "However, if features overlap, weight mass will usually be divided over these features.", "labels": [], "entities": []}, {"text": "For instance, suppose that f 1 alone has a weight of 0.5 in a given model.", "labels": [], "entities": []}, {"text": "If we retrain the model, after adding the features f 2 ..f 5 that behave identically to f 1 , the weight maybe distributed evenly between f 1 ..f 5 , giving each feature the weight 0.1.", "labels": [], "entities": []}, {"text": "In the following sections, we will give a short overview of previous research in feature selection, and will then proceed to give a more detailed description of three feature selection methods.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.8596425950527191}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Peak accuracies for the maximum entropy,  correlation-based, and frequency-based selection meth- ods when selecting up to 5000 features. Accuracies for  random, n-gram and full models are included for com- parison.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 147, "end_pos": 157, "type": "METRIC", "confidence": 0.9685041904449463}]}, {"text": " Table 2: The first 10 features returned by maximum en- tropy feature selection, including the weights estimated  by this feature selection method.", "labels": [], "entities": []}]}