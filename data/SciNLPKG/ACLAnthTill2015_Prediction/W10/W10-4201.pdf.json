{"title": [{"text": "Comparing Rating Scales and Preference Judgements in Language Evaluation", "labels": [], "entities": [{"text": "Language Evaluation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.6875357031822205}]}], "abstractContent": [{"text": "Rating-scale evaluations are common in NLP, but are problematic fora range of reasons, e.g. they can be unintuitive for evaluators, inter-evaluator agreement and self-consistency tend to below, and the parametric statistics commonly applied to the results are not generally considered appropriate for ordinal data.", "labels": [], "entities": []}, {"text": "In this paper , we compare rating scales with an alternative evaluation paradigm, preference-strength judgement experiments (PJEs), where evaluators have the simpler task of deciding which of two texts is better in terms of a given quality criterion.", "labels": [], "entities": []}, {"text": "We present three pairs of evaluation experiments assessing text fluency and clarity for different data sets, where one of each pair of experiments is a rating-scale experiment , and the other is a PJE.", "labels": [], "entities": [{"text": "clarity", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9318176507949829}, {"text": "PJE", "start_pos": 197, "end_pos": 200, "type": "METRIC", "confidence": 0.6009514331817627}]}, {"text": "We find the PJE versions of the experiments have better evaluator self-consistency and inter-evaluator agreement, and a larger proportion of variation accounted for by system differences, resulting in a larger number of significant differences being found.", "labels": [], "entities": []}], "introductionContent": [{"text": "Rating-scale evaluations, where human evaluators assess system outputs by selecting a score on a discrete scale, are the most common form of humanassessed evaluation in NLP.", "labels": [], "entities": []}, {"text": "Results are typically presented in rank tables of means for each system accompanied by means-based measures of statistical significance of the differences between system scores.", "labels": [], "entities": []}, {"text": "NLP system evaluation tends to involve sets of systems, rather than single ones (evaluations tend to at least incorporate a baseline or, more rarely, a topline system).", "labels": [], "entities": [{"text": "NLP system evaluation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.667604923248291}]}, {"text": "The aim of system evaluation is to gain some insight into which systems are better than which others, in other words, the aim is inherently relative.", "labels": [], "entities": []}, {"text": "Yet NLP system evaluation experiments have generally preferred rating scale experiments where evaluators assess each system's quality in isolation, in absolute terms.", "labels": [], "entities": []}, {"text": "Such rating scales are not very intuitive to use; deciding whether a text deserves a 5, a 4 or a 3 etc.", "labels": [], "entities": []}, {"text": "Furthermore, evaluators may ascribe different meanings to scores and the distances between them.", "labels": [], "entities": []}, {"text": "Individual evaluators have different tendencies in using rating scales, e.g. what is known as 'end-aversion' tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g..", "labels": [], "entities": []}, {"text": "It is not surprising then that stable averages of quality judgements, let alone high levels of agreement, are hard to achieve, as has been observed for MT), text summarisation), and language generation ().", "labels": [], "entities": [{"text": "MT", "start_pos": 152, "end_pos": 154, "type": "TASK", "confidence": 0.9733273386955261}, {"text": "text summarisation", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.7360707819461823}, {"text": "language generation", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.7513818442821503}]}, {"text": "It has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (DUC literature).", "labels": [], "entities": [{"text": "DUC literature", "start_pos": 131, "end_pos": 145, "type": "DATASET", "confidence": 0.8904024660587311}]}, {"text": "The result of a rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale).", "labels": [], "entities": []}, {"text": "The means-based ranks and statistical significance tests that are commonly presented with the results of RSEs are not generally considered appropriate for ordinal data in the statistics literature.", "labels": [], "entities": [{"text": "RSEs", "start_pos": 105, "end_pos": 109, "type": "TASK", "confidence": 0.8634452223777771}]}, {"text": "At the least, \"a test on the means imposes the requirement that the measures must be additive, i.e. numerical\".", "labels": [], "entities": []}, {"text": "Parametric statistics are more powerful than non-parametric alternatives, because they make a number of strong assumptions (including that the data is numerical).", "labels": [], "entities": []}, {"text": "If the assumptions are violated then the risks is that the significance of results is overestimated.", "labels": [], "entities": []}, {"text": "In this paper we explore an alternative evaluation paradigm, Preference-strength Judgement Experiments (PJEs).", "labels": [], "entities": [{"text": "Preference-strength Judgement Experiments (PJEs)", "start_pos": 61, "end_pos": 109, "type": "TASK", "confidence": 0.665394738316536}]}, {"text": "Binary preference judgements have been used in NLP system evaluation), but to our knowledge this is the first systematic investigation of preference-strength judgements where evaluators express, in addition to their preference (which system do you prefer?), also the strength of their preference (how strongly do you prefer the system you prefer?).", "labels": [], "entities": [{"text": "NLP system evaluation", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.6742997964223226}]}, {"text": "It seems intuitively convincing that it should be easier to decide which of two texts is clearer than to decide whether a text's clarity deserves a 1, 2, 3, 4 or 5.", "labels": [], "entities": [{"text": "clarity", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.978968620300293}]}, {"text": "However, it is less clear whether evaluators are also able to express the strength of their preference in a consistent fashion, resulting not only in good self-consistency, but also in good agreement with other evaluators.", "labels": [], "entities": []}, {"text": "We present three pairs of directly comparable RSE and PJE evaluations, and investigate how they compare in terms of (i) the amount of variation accounted for by differences between systems (the more the better), relative to the amount of variation accounted for by other factors such as evaluator and arbitrary text properties (the less the better); (ii) inter-evaluator agreement, (iii) evaluator self-consistency, (iv) the number of significant differences identified, and (v) experimental cost.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following three sections we present the design and results of three pairs of evaluations.", "labels": [], "entities": []}, {"text": "Each pair consists of a rating-scale experiment (RSE) and a preference-strength judgement experiment (PJE) that differ only in the rating method they employ (relative ratings in the PJE and absolute ratings in the RSE).", "labels": [], "entities": []}, {"text": "1 In other words, they involve the same set of system outputs, the same instructions and method of presentating system outputs.", "labels": [], "entities": []}, {"text": "Each pair is fora different data domain and system task, the first for generating chains of references to people in Wikipedia articles (Section 3); the second for weather forecast text generation (Section 4); and the third for generating descriptions of images of furniture and faces (Section 5).", "labels": [], "entities": [{"text": "weather forecast text generation", "start_pos": 163, "end_pos": 195, "type": "TASK", "confidence": 0.6199926882982254}]}, {"text": "All experiments use a Repeated Latin Squares design which ensures that each subject sees the same number of outputs from each system and for each test set item.", "labels": [], "entities": []}, {"text": "Following detailed instructions, subjects first do 2 or 3 practice examples, followed by the texts to be evaluated, in an order randomised for each subject.", "labels": [], "entities": []}, {"text": "Subjects carryout the evaluation over the internet, at a time and place of their choosing.", "labels": [], "entities": []}, {"text": "They are allowed to interrupt and resume (but are discouraged from doing so).", "labels": [], "entities": []}, {"text": "There are subtle differences between the three experiment pairs, and for ease of comparison we provide an overview of the six experiments we investigate in this paper in.", "labels": [], "entities": []}, {"text": "Each of the aspects of experimental design and execution shown in this table is explained and described in more detail in the relevant subsection below, but some of the important differences are highlighted here.", "labels": [], "entities": []}, {"text": "In GREC-NEG PJE, each system is compared with only one other comparisor system (a humanauthored topline), whereas in the other two PJE experiments, each system is compared with all other systems for each test data set item.", "labels": [], "entities": [{"text": "GREC-NEG PJE", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.7397488355636597}]}, {"text": "In the two versions of the METEO evaluation, evaluators were not drawn from the same cohort of people, whereas in the other two evaluation pairs they were drawn from the same cohort.", "labels": [], "entities": [{"text": "METEO evaluation", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.6716240048408508}]}, {"text": "GREC-NEG RSE and METEO RSE used radio buttons (as shown in) as the rating-scale evaluation mechanism whereas in TUNA RSE it was an unmarked slider bar.", "labels": [], "entities": [{"text": "GREC-NEG RSE", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8649939596652985}, {"text": "METEO RSE", "start_pos": 17, "end_pos": 26, "type": "DATASET", "confidence": 0.8008721768856049}, {"text": "TUNA RSE", "start_pos": 112, "end_pos": 120, "type": "DATASET", "confidence": 0.7939886450767517}]}, {"text": "While slightly different names were used for the evaluation criteria in two of the evaluation pairs, Fluency/Readability were explained in very similar terms (does it read well?), and Adequacy in TUNA was explained in terms of clarity of reference (is it clear which entity the description refers to?), so there are in fact just two evaluation criteria (albeit with different names).", "labels": [], "entities": [{"text": "TUNA", "start_pos": 196, "end_pos": 200, "type": "DATASET", "confidence": 0.8180844187736511}, {"text": "clarity", "start_pos": 227, "end_pos": 234, "type": "METRIC", "confidence": 0.9643464684486389}]}, {"text": "Where we use preference-strength judgements,: Overview of experiments with details of design and execution.", "labels": [], "entities": []}, {"text": "(Comparisor(s) = the other systems against which each system is evaluated.) the evaluation mechanism is implemented using slider bars as shown at the bottom of which map to a scale \u2212X..", "labels": [], "entities": []}, {"text": "The evaluator's task is to express their preference in terms of each quality criterion by moving the pointers on the sliders.", "labels": [], "entities": []}, {"text": "Moving the pointer to the left means expressing a preference for the text on the left, moving it to the right means preferring the text on the right; the further to the left/right the slider is moved, the stronger the preference.", "labels": [], "entities": []}, {"text": "It was not evident to the evaluators that sliders were associated with numerical values.", "labels": [], "entities": []}, {"text": "Slider pointers started out in the middle of the scale (the position corresponding to no preference).", "labels": [], "entities": [{"text": "Slider pointers", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8092515766620636}]}, {"text": "If they wanted to leave the pointer in the middle (i.e. if they had no preference for either of the two texts), evaluators had to check a box to confirm their rating (to avoid evaluators accidentally not rating a text and leaving the pointer in the default position).", "labels": [], "entities": []}, {"text": "Our new experiment used our standardised preference strength sliders (bottom of).", "labels": [], "entities": []}, {"text": "To accommodate all pairwise comparisons as well as all test set items, we used a design of four 28 \u00d7 28 Latin Squares, and recruited 28 evaluators from among students currently completing, or recently having completed, a degree in a linguisticsrelated subject at Oxford, KCL, UCL, Sussex and Brighton universities.", "labels": [], "entities": []}, {"text": "There were 3,136 trials in this version of the experiment.", "labels": [], "entities": []}, {"text": "shows the same measures as we reported for the other two experiment pairs above.", "labels": [], "entities": []}, {"text": "The picture is somewhat similar in that the measures have better values for PJE version except for the inter-evaluator agreement (Kendall's W) for Fluency which is slightly higher for the RSE version.", "labels": [], "entities": [{"text": "inter-evaluator agreement (Kendall's W)", "start_pos": 103, "end_pos": 142, "type": "METRIC", "confidence": 0.8440299885613578}, {"text": "Fluency", "start_pos": 147, "end_pos": 154, "type": "METRIC", "confidence": 0.9853329062461853}]}, {"text": "The rating scale experiment that was part of the TUNA'09 evaluations had a design of fourteen 8 \u00d7 8 squares, and a total of 896 trials.", "labels": [], "entities": []}, {"text": "Subjects were asked to give their judgments for Clarity and Fluency for each item by manipulating a slider.", "labels": [], "entities": [{"text": "Clarity", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9907978773117065}, {"text": "Fluency", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9829378724098206}]}, {"text": "The slider pointer was placed in the center at the beginning of each trial.", "labels": [], "entities": [{"text": "slider pointer", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.8931602239608765}]}, {"text": "The position of the slider selected by the subject mapped to an integer value between 1 and 100.", "labels": [], "entities": []}, {"text": "However, the scale was not visible to participants who knew only that one end of the scale corresponded to the worst possible score and the opposite end to the best.", "labels": [], "entities": []}, {"text": "Eight native speakers of English were recruited for this experiment from among post-graduate students currently doing a Masters degree in a linguistics-related subject at UCL, Sussex and Brighton universities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of experiments with details of design and execution. (Comparisor(s) = the other  systems against which each system is evaluated.)", "labels": [], "entities": []}, {"text": " Table 2: GREC-NEG RSE/PJE: Results of analy- ses looking at effect of System.", "labels": [], "entities": [{"text": "GREC-NEG RSE", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.8099566996097565}]}, {"text": " Table 3: METEO RSE/PJE: Results of analyses  looking at effect of System.", "labels": [], "entities": [{"text": "METEO RSE/PJE", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.7364031970500946}]}, {"text": " Table 4: TUNA RSE/PJE: Results of analyses  looking at effect of System.", "labels": [], "entities": [{"text": "TUNA RSE/PJE", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.7569747269153595}]}]}