{"title": [{"text": "Sketching Techniques for Large Scale NLP", "labels": [], "entities": [{"text": "NLP", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.6279380321502686}]}], "abstractContent": [{"text": "In this paper, we address the challenges posed by large amounts of text data by exploiting the power of hashing in the context of streaming data.", "labels": [], "entities": []}, {"text": "We explore sketch techniques, especially the Count-Min Sketch, which approximates the frequency of a word pair in the corpus without explicitly storing the word pairs themselves.", "labels": [], "entities": []}, {"text": "We use the idea of a conservative update with the Count-Min Sketch to reduce the average relative error of its approximate counts by a factor of two.", "labels": [], "entities": [{"text": "relative error", "start_pos": 89, "end_pos": 103, "type": "METRIC", "confidence": 0.8818912208080292}]}, {"text": "We show that it is possible to store all words and word pairs counts computed from 37 GB of web data in just 2 billion counters (8 GB RAM).", "labels": [], "entities": []}, {"text": "The number of these counters is up to 30 times less than the stream size which is a big memory and space gain.", "labels": [], "entities": []}, {"text": "In Semantic Orientation experiments, the PMI scores computed from 2 billion counters are as effective as exact PMI scores.", "labels": [], "entities": [{"text": "Semantic Orientation", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.8957136571407318}]}], "introductionContent": [{"text": "Approaches to solve NLP problems () always benefited from having large amounts of data.", "labels": [], "entities": []}, {"text": "In some cases (), researchers attempted to use the evidence gathered from web via search engines to solve the problems.", "labels": [], "entities": []}, {"text": "But the commercial search engines limit the number of automatic requests on a daily basis for various reasons such as to avoid fraud and computational overhead.", "labels": [], "entities": []}, {"text": "Though we can crawl the data and save it on disk, most of the current approaches employ data structures that reside in main memory and thus do not scale well to huge corpora.", "labels": [], "entities": []}, {"text": "a corpus of size 577 MB.", "labels": [], "entities": []}, {"text": "Note that the plot is in log-log scale.", "labels": [], "entities": []}, {"text": "This 78 million word corpus generates 63 thousand unique words and 118 million unique word pairs.", "labels": [], "entities": []}, {"text": "As expected, the rapid increase in number of unique word pairs is much larger than the increase in number of words.", "labels": [], "entities": []}, {"text": "Hence, it shows that it is computationally infeasible to compute counts of all word pairs with a giant corpora using conventional main memory of 8 GB.", "labels": [], "entities": []}, {"text": "Storing only the 118 million unique word pairs in this corpus require 1.9 GB of disk space.", "labels": [], "entities": []}, {"text": "This space can be saved by avoiding storing the word pair itself.", "labels": [], "entities": []}, {"text": "As a trade-off we are willing to tolerate a small amount of error in the frequency of each word pair.", "labels": [], "entities": []}, {"text": "In this paper, we explore sketch techniques, especially the Count-Min Sketch, which approximates the frequency of a word pair in the corpus without explicitly storing the word pairs themselves.", "labels": [], "entities": []}, {"text": "It turns out that, in this technique, both updating (adding anew word pair or increasing the frequency of existing word pair) and querying (finding the frequency of a given word pair) are very efficient and can be done inconstant time . Counts stored in the CM Sketch can be used to compute various word-association measures like 1 depend only on one of the user chosen parameters Pointwise Mutual Information (PMI), and LogLikelihood ratio.", "labels": [], "entities": [{"text": "Pointwise Mutual Information (PMI)", "start_pos": 381, "end_pos": 415, "type": "METRIC", "confidence": 0.6368532876173655}]}, {"text": "These association scores are useful for other NLP applications like word sense disambiguation, speech and character recognition, and computing semantic orientation of a word.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.7125826478004456}, {"text": "speech and character recognition", "start_pos": 95, "end_pos": 127, "type": "TASK", "confidence": 0.6185333281755447}, {"text": "computing semantic orientation of a word", "start_pos": 133, "end_pos": 173, "type": "TASK", "confidence": 0.8037640005350113}]}, {"text": "In our work, we use computing semantic orientation of a word using PMI as a canonical task to show the effectiveness of CM Sketch for computing association scores.", "labels": [], "entities": [{"text": "computing semantic orientation of a word", "start_pos": 20, "end_pos": 60, "type": "TASK", "confidence": 0.7606230775515238}]}, {"text": "In our attempt to advocate the Count-Min sketch to store the frequency of keys (words or word pairs) for NLP applications, we perform both intrinsic and extrinsic evaluations.", "labels": [], "entities": []}, {"text": "In our intrinsic evaluation, first we show that low-frequent items are more prone to errors.", "labels": [], "entities": []}, {"text": "Second, we show that computing approximate PMI scores from these counts can give the same ranking as Exact PMI.", "labels": [], "entities": [{"text": "Exact PMI", "start_pos": 101, "end_pos": 110, "type": "DATASET", "confidence": 0.874723881483078}]}, {"text": "However, we need counters linear in size of stream to achieve that.", "labels": [], "entities": []}, {"text": "We use these approximate PMI scores in our extrinsic evaluation of computing semantic orientation.", "labels": [], "entities": [{"text": "computing semantic orientation", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.676742285490036}]}, {"text": "Here, we show that we do not need counters linear in size of stream to perform as good as Exact PMI.", "labels": [], "entities": [{"text": "Exact PMI", "start_pos": 90, "end_pos": 99, "type": "DATASET", "confidence": 0.677078366279602}]}, {"text": "In our experiments, by using only 2 billion counters (8GB RAM) we get the same accuracy as for exact PMI scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9994868040084839}]}, {"text": "The number of these counters is up to 30 times less than the stream size which is a big memory and space gain without any loss of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9965956807136536}]}], "datasetContent": [{"text": "To show the effectiveness of the Count-Min sketch in the context of NLP, we perform intrinsic evaluations.", "labels": [], "entities": []}, {"text": "The intrinsic evaluations are designed to measure the error in the approximate counts returned by CMS compared to their true counts.", "labels": [], "entities": []}, {"text": "By keeping the total size of the data structure fixed, we study the error by varying the width and the depth of the data structure to find the best setting of the parameters for textual data sets.", "labels": [], "entities": []}, {"text": "We show that using conservative update (CU) further improves the quality of counts over CM sketch.", "labels": [], "entities": [{"text": "conservative update (CU", "start_pos": 19, "end_pos": 42, "type": "METRIC", "confidence": 0.8298846632242203}]}, {"text": "To evaluate the effectiveness of CU-PMI word association scores, we infer semantic orientation (S0) of a word from CU-PMI and Exact-PMI scores.", "labels": [], "entities": [{"text": "semantic orientation (S0)", "start_pos": 74, "end_pos": 99, "type": "METRIC", "confidence": 0.7262243866920471}]}, {"text": "Given a word, the task of finding the SO () of the word is to identify if the word is more likely to be used in positive or negative sense.", "labels": [], "entities": [{"text": "SO", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.960378885269165}]}, {"text": "We use a similar framework as used by the authors 6 to infer the SO.", "labels": [], "entities": [{"text": "SO", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9000921845436096}]}, {"text": "We take the seven positive words (good, nice, excellent, positive, fortunate, correct, and superior) and the negative words (bad, nasty, poor, negative, unfortunate, wrong, and inferior) used in () work.", "labels": [], "entities": []}, {"text": "The SO of a given word is calculated based on the strength of its association with the seven positive words, and the strength of its association with the seven negative words.", "labels": [], "entities": [{"text": "SO", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.7692259550094604}]}, {"text": "We compute the SO of a word \"w\" as follows: Where, Pwords and Nwords denote the seven positive and negative prototype words respectively.", "labels": [], "entities": []}, {"text": "We compute SO score from different sized corpora (Section 4.1).", "labels": [], "entities": [{"text": "SO score", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.8194291293621063}]}, {"text": "We use the General Inquirer lexicon as a benchmark to evaluate the semantic orientation scores similar to) work.", "labels": [], "entities": [{"text": "General Inquirer lexicon", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.7844099601109823}]}, {"text": "Words with multiple senses have multiple entries in the lexicon, we merge these entries for our experiment.", "labels": [], "entities": []}, {"text": "Our test set consists of 1619 positive and 1989 negative words.", "labels": [], "entities": []}, {"text": "Accuracy is used as an evaluation metric and is defined as the fraction of number of correctly identified SO words.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9907671809196472}]}], "tableCaptions": [{"text": " Table 2: Evaluating the PMI rankings obtained using CM", "labels": [], "entities": [{"text": "PMI", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.879979133605957}, {"text": "CM", "start_pos": 53, "end_pos": 55, "type": "DATASET", "confidence": 0.6898492574691772}]}, {"text": " Table 3: Evaluating Semantic Orientation of words with different # of counters of CU sketch with increasing amount of data", "labels": [], "entities": [{"text": "Evaluating Semantic Orientation", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.6871111889680227}]}]}