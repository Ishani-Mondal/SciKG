{"title": [{"text": "Seeding Statistical Machine Translation with Translation Memory Output through Tree-Based Structural Alignment", "labels": [], "entities": [{"text": "Seeding Statistical Machine Translation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7927587628364563}]}], "abstractContent": [{"text": "With the steadily increasing demand for high-quality translation, the localisation industry is constantly searching for technologies that would increase translator throughput, with the current focus on the use of high-quality Statistical Machine Translation (SMT) as a supplement to the established Translation Memory (TM) technology.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 226, "end_pos": 263, "type": "TASK", "confidence": 0.7895188679297765}, {"text": "Translation Memory (TM)", "start_pos": 299, "end_pos": 322, "type": "TASK", "confidence": 0.8404379844665527}]}, {"text": "In this paper we present a novel modular approach that utilises state-of-the-art sub-tree alignment to pick out pre-translated segments from a TM match and seed with them an SMT system to produce a final translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 174, "end_pos": 177, "type": "TASK", "confidence": 0.9835905432701111}]}, {"text": "We show that the presented system can out-perform pure SMT when a good TM match is found.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9925314784049988}]}, {"text": "It can also be used in a Computer-Aided Translation (CAT) environment to present almost perfect translations to the human user with markup highlighting the segments of the translation that need to be checked manually for correctness.", "labels": [], "entities": [{"text": "Computer-Aided Translation (CAT)", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.8408864855766296}]}], "introductionContent": [{"text": "As the world becomes increasingly interconnected, the major trend is to try to deliver ideas and products to the widest audience possible.", "labels": [], "entities": []}, {"text": "This requires the localisation of products for as many countries and cultures as possible, with translation being one of the main parts of the localisation process.", "labels": [], "entities": []}, {"text": "Because of this, the amount of data that needs professional high-quality translation is continuing to increase well beyond the capacity of the world's human translators.", "labels": [], "entities": []}, {"text": "Thus, current efforts in the localisation industry are mostly directed at the reduction of the amount of data that needs to be translated from scratch by hand.", "labels": [], "entities": []}, {"text": "Such efforts mainly include the use of Translation Memory (TM) systems, where earlier translations are stored in a database and offered as suggestions when new data needs to be translated.", "labels": [], "entities": [{"text": "Translation Memory (TM)", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.9015993714332581}]}, {"text": "As TM systems were originally limited to providing translations only for (almost) exact matches of the new data, the integration of Machine Translation (MT) techniques is seen as the only feasible development that has the potential to significantly reduce the amount of manual translation required.", "labels": [], "entities": [{"text": "TM", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9590198397636414}, {"text": "Machine Translation (MT)", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.8564332842826843}]}, {"text": "At the same time, the use of SMT is frowned upon by the users of CAT tools as they still do not trust the quality of the SMT output.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9846072793006897}, {"text": "SMT output", "start_pos": 121, "end_pos": 131, "type": "TASK", "confidence": 0.873372346162796}]}, {"text": "There are two main reasons for that.", "labels": [], "entities": []}, {"text": "First, currently there is no reliable way to automatically ascertain the quality of SMT-generated translations, so that the user could at a glance make a judgement as to the amount of effort that might be needed to postedit the suggested translation).", "labels": [], "entities": [{"text": "SMT-generated translations", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.9270689785480499}]}, {"text": "Not having such automatic quality metrics also has the side effect of it being impossible fora Translation-Services Provider (TSP) company to reliably determine in advance the increase in translator productivity due to the use of MT and to adjust their resources-allocation and cost models correspondingly.", "labels": [], "entities": []}, {"text": "The second major problem for users is that SMTgenerated translations are as a rule only obtained for cases where the TM system could not produce a good-enough translation (cf..", "labels": [], "entities": [{"text": "SMTgenerated translations", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.9788898527622223}]}, {"text": "Given that the SMT system used is usually trained only on the data available in the TM, expectedly it also has few examples from which to construct the translation, thus producing low quality output.", "labels": [], "entities": [{"text": "SMT", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9910141825675964}]}, {"text": "In this paper, we combine a TM, SMT and an automatic Sub-Tree Alignment (STA) backends in a single integrated tool.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9642122387886047}]}, {"text": "When anew sentence that needs to be translated is supplied, first a Fuzzy-Match Score (FMS -see Section 2.2) is obtained from the TM backend, together with the suggested matching sentence and its translation.", "labels": [], "entities": [{"text": "Fuzzy-Match Score (FMS -", "start_pos": 68, "end_pos": 92, "type": "METRIC", "confidence": 0.8464017987251282}]}, {"text": "For sentences that receive a reasonably high FMS, the STA backend is used to find the correspondences between the input sentence and the TM-suggested translation, marking up the parts of the input that are correctly translated by the TM.", "labels": [], "entities": [{"text": "FMS", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.8564847707748413}, {"text": "STA", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.907365620136261}]}, {"text": "The SMT backend is then employed to obtain the final translation from the marked-up input sentence.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9770545959472656}]}, {"text": "In this way we expect to achieve a better result compared to using pure SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.985508382320404}]}, {"text": "In Section 2, we present the technical details of the design of our system, together with motivation for the particular design choices.", "labels": [], "entities": []}, {"text": "Section 3 details the experimental setup and the data set used for the evaluation results in Section 4.", "labels": [], "entities": []}, {"text": "We present improvements that we plan to investigate in further work in Section 5, and provide concluding remarks in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use real-life TM data from an industrial partner.", "labels": [], "entities": []}, {"text": "The TM was generated during the translation of RTF-formatted customer support documentation.", "labels": [], "entities": [{"text": "translation of RTF-formatted customer support documentation", "start_pos": 32, "end_pos": 91, "type": "TASK", "confidence": 0.7820019026597341}]}, {"text": "The data is in TMX format and originally contains 108 967 English-French translation segments, out of which 14 segments either have an empty language side or have an extreme discrepancy in the number of tokens for each language side and were therefore discarded.", "labels": [], "entities": []}, {"text": "A particular real-life trait of the data is the presence of a large number of XML tags.", "labels": [], "entities": []}, {"text": "Running the tag-mapping tool described in Section 2.6, we gathered 2 049 distinct tags for the English side of the data and 2 653 for the French side.", "labels": [], "entities": []}, {"text": "Still, there were certain XML tags that included a label argument whose value was translated from one language to the other.", "labels": [], "entities": []}, {"text": "These XML tags were left intact so that our system could handle the translation correctly.", "labels": [], "entities": []}, {"text": "The TM data also contain a large number of file paths, e-mail addresses, URLs and others, which makes bespoke tokenisation of the data necessary.", "labels": [], "entities": []}, {"text": "Our tokenisation tool ensures that none of these elements are tokenised, keeps RTF formatting sequences non-tokenised and properly handles non-masked XML tags, minimising their fragmentation.", "labels": [], "entities": [{"text": "RTF formatting sequences", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.7333645621935526}]}, {"text": "As translation segments rarely occur more than once in a TM, we observe a high number of unique tokens (measured after pre-processing) - 41 379 for English and 49 971 for French - out of 108 953 segment pairs.", "labels": [], "entities": []}, {"text": "The average sentence length is 13.2 for English and 15.0 for French.", "labels": [], "entities": []}, {"text": "For evaluation, we use a data set of 4977 English-French segments from the domain of the TM.", "labels": [], "entities": []}, {"text": "The sentences in the test set are significantly shorter on average, compared to the TM - 9.2 tokens for English and 10.9 for French.", "labels": [], "entities": [{"text": "TM - 9.2 tokens", "start_pos": 84, "end_pos": 99, "type": "METRIC", "confidence": 0.9509256333112717}]}, {"text": "It must be noted that we used SMT models with maximum phrase length of 3 tokens, rather than the standard 5 tokens, and for decoding we used a 3-gram language model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9872542023658752}]}, {"text": "This results in much smaller models than the ones usually used in mainstream SMT applications.", "labels": [], "entities": [{"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9943861961364746}]}, {"text": "(The standard for some tools goes as far as 7-token phaselength limit and 7-gram language models)  For the evaluation of our system, we used a number of widely accepted automatic metrics, namely BLEU (), METEOR (Banerjee and), TER) and inverse F-Score based on token-level precision and recall.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 195, "end_pos": 199, "type": "METRIC", "confidence": 0.9990110397338867}, {"text": "METEOR", "start_pos": 204, "end_pos": 210, "type": "METRIC", "confidence": 0.9955490827560425}, {"text": "TER", "start_pos": 227, "end_pos": 230, "type": "METRIC", "confidence": 0.995983362197876}, {"text": "inverse", "start_pos": 236, "end_pos": 243, "type": "METRIC", "confidence": 0.955285370349884}, {"text": "F-Score", "start_pos": 244, "end_pos": 251, "type": "METRIC", "confidence": 0.6882803440093994}, {"text": "precision", "start_pos": 273, "end_pos": 282, "type": "METRIC", "confidence": 0.9427233934402466}, {"text": "recall", "start_pos": 287, "end_pos": 293, "type": "METRIC", "confidence": 0.9965176582336426}]}, {"text": "We setup our system to only fully process input sentences for which a TM match with an FMS over 50% was found, although all sentences were translated directly using the SMT backend to check the overall pure SMT performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 169, "end_pos": 172, "type": "TASK", "confidence": 0.9499515295028687}, {"text": "SMT", "start_pos": 207, "end_pos": 210, "type": "TASK", "confidence": 0.9879531860351562}]}, {"text": "The TM-suggested translations were also output for all input sentences.", "labels": [], "entities": [{"text": "TM-suggested translations", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6022529602050781}]}, {"text": "The results of the evaluation are given in, where the tm and direct scores are also given for the FMS range [0%; 50%)\u222a{100%}.", "labels": [], "entities": [{"text": "FMS", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.6455733180046082}]}, {"text": "Across all metrics we see a uniform drop in the quality of TM-suggested translations, which is what we expected, given that these translations contain one or more wrong words.", "labels": [], "entities": []}, {"text": "We believe that the relatively high scores recorded for the TM-suggested translations at the high end of the FMS scale area result of the otherwise perfect word order and lexical choice.", "labels": [], "entities": [{"text": "TM-suggested translations", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.754333108663559}, {"text": "FMS scale area", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.775744765996933}]}, {"text": "For n-grammatch-based metrics like the ones we used such a result is expected and predictable.", "labels": [], "entities": []}, {"text": "Although the inverse F-score results show the potential of our setup to translate the outstanding tokens in a 90%-100% TM match, it appears that the SMT system produces word order that does not correspond to the reference translation and because of this receives lower scores on the other metrics.", "labels": [], "entities": [{"text": "F-score", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.9559623599052429}, {"text": "SMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.9822012782096863}]}, {"text": "The unexpected drop in scores for perfect TM matches is due to discrepancies between the reference translations in our test set and the translations stored in the TM.", "labels": [], "entities": []}, {"text": "We believe that this issue affects all FMS ranges, albeit to a lower extent for non-perfect matches.", "labels": [], "entities": []}, {"text": "Unfortunately, the exact impact cannot be ascertained without human evaluation.", "labels": [], "entities": []}, {"text": "We observe a significant drop-off in translation quality for the direct output below FMS 50%.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9160845279693604}]}, {"text": "This suggests that sentences with such low FMS should be translated either by a human translator from scratch, or by an SMT system trained on different/more data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.971094012260437}]}, {"text": "Our system (i.e. the xml setup) clearly outperforms the direct SMT translation for FMS between 80 and 100 and has comparable performance between FMS 70 and 80.", "labels": [], "entities": [{"text": "SMT translation", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.9578779637813568}]}, {"text": "Below FMS 70, the SMT backend has the best performance.", "labels": [], "entities": [{"text": "FMS 70", "start_pos": 6, "end_pos": 12, "type": "DATASET", "confidence": 0.8703844547271729}, {"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9331519603729248}]}, {"text": "Although these results are positive, we still need to investigate why our system has poor performance at lower FMS ranges.", "labels": [], "entities": []}, {"text": "Theoretically, it should outperform the SMT backend across all ranges, as its output is generated by supplying the SMT backend with good pre-translated fragments.", "labels": [], "entities": [{"text": "SMT backend", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.8660100996494293}]}, {"text": "The Inverse F-Score graph suggest that this is due to worse lexical choice, but only manual evaluation can provide us with clues for solving the issue.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.6243603825569153}]}, {"text": "The discrepancy in the results in the Inverse FScore graph with the other metrics suggest that the biggest problem for our system is producing output in the expected word-order.", "labels": [], "entities": []}], "tableCaptions": []}