{"title": [{"text": "Google Web 1T 5-Grams Made Easy (but not for the computer)", "labels": [], "entities": [{"text": "Google Web 1T 5-Grams", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.9132397472858429}]}], "abstractContent": [{"text": "This paper introduces Web1T5-Easy, a simple indexing solution that allows interactive searches of the Web 1T 5-gram database and a derived database of quasi-collocations.", "labels": [], "entities": [{"text": "Web 1T 5-gram database", "start_pos": 102, "end_pos": 124, "type": "DATASET", "confidence": 0.7881733179092407}]}, {"text": "The latter is validated against co-occurrence data from the BNC and ukWaC on the automatic identification of non-compositional VPC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.970373809337616}, {"text": "ukWaC", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.8895309567451477}]}], "introductionContent": [{"text": "The Google Web 1T 5-gram (Web1T5) database) consists of frequency counts for bigram, trigrams, 4-grams and 5-grams extracted from 1 trillion words of English Web text, i.e. from a corpus 10,000 times the size of the British National Corpus ().", "labels": [], "entities": [{"text": "Google Web 1T 5-gram (Web1T5) database", "start_pos": 4, "end_pos": 42, "type": "DATASET", "confidence": 0.7622849568724632}, {"text": "British National Corpus", "start_pos": 216, "end_pos": 239, "type": "DATASET", "confidence": 0.9276793400446574}]}, {"text": "While primarily designed as a resource to build better language models for machine translation and other NLP applications, its public release in 2006 was greeted with great enthusiasm by many researchers in computational linguistics.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.8061405718326569}]}, {"text": "As one example, used the Web1T5 data successfully to predict fMRI neural activation associated with concrete noun concepts.", "labels": [], "entities": [{"text": "Web1T5 data", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.9565265774726868}]}, {"text": "For linguistic applications, though, the Web1T5 database presents three major obstacles: (i) The lack of linguistic annotation: Google's tokenisation splits hyphenated compounds (e.g., parttime is split into a three-token sequence part|-|time) and differs in many other ways from the rules used in liguistic corpora.", "labels": [], "entities": []}, {"text": "The n-grams are neither annotated with part-of-speech tags nor lemmatised, and there are separate entries for sentence-initial uppercase and the corresponding lowercase forms.", "labels": [], "entities": []}, {"text": "(ii) The application of frequency thresholds: Despite the enormous size of the database, its compilers found it necessary to omit low-frequency ngrams with fewer than 40 occurrences.", "labels": [], "entities": []}, {"text": "This means that non-adjacent word combinations are listed only if the occur in a relatively frequent pattern.", "labels": [], "entities": []}, {"text": "As a consequence, it is impossible to obtain reliable frequency estimates for latent phenomena by pooling data (e.g. the co-occurrence frequency of a particular verb with nouns denoting animals).", "labels": [], "entities": []}, {"text": "(iii) The difficulty of interactive search: The complete Web1T5 database consists of 24.4 GiB of binary-sorted, compressed text files.", "labels": [], "entities": [{"text": "Web1T5 database", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.932102620601654}]}, {"text": "While this format is suitable for building n-gram language models and other offline processing, searching the database is not efficient enough for interactive use.", "labels": [], "entities": []}, {"text": "Except for simple, case-sensitive prefix searches -which can be restricted to a single file containing 50-90 MiB of compressed text -every query requires a linear scan of the full database.", "labels": [], "entities": []}, {"text": "This paper presents a simple open-source software solution to the third problem, called Web1T5-Easy.", "labels": [], "entities": []}, {"text": "The n-gram data are encoded and indexed in a relational database.", "labels": [], "entities": []}, {"text": "Building on convenient open-source tools such as SQLite and Perl, the software aims to strike a good balance between search efficiency and ease of use and implementation.", "labels": [], "entities": []}, {"text": "With its focus on interactive, but accurate search it complements the approximate indexing and batch processing approaches of The rest of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the general system architecture in more detail.", "labels": [], "entities": []}, {"text": "Section 3 explains how collocations (with a maximal span size of four tokens) and distributional semantic models (DSM) can be approximated on the basis of Web1T5 frequency data.", "labels": [], "entities": [{"text": "Web1T5 frequency data", "start_pos": 155, "end_pos": 176, "type": "DATASET", "confidence": 0.8308132489522299}]}, {"text": "Some technical aspects are summarised in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 addresses the consequences of problems (i) and (ii).", "labels": [], "entities": []}, {"text": "The linguistic usefulness of Web1T5 collocation data is validated on a multiword extraction task from the MWE 2008 workshop.", "labels": [], "entities": [{"text": "Web1T5 collocation data", "start_pos": 29, "end_pos": 52, "type": "DATASET", "confidence": 0.7906462748845419}, {"text": "multiword extraction task", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.7818243702252706}, {"text": "MWE 2008 workshop", "start_pos": 106, "end_pos": 123, "type": "DATASET", "confidence": 0.8951555887858073}]}, {"text": "2 Section 6 concludes with a brief outlook on the future development of Web1T5-Easy.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Example of Web1T5 3-gram frequency data (ex- cerpt from file 3gm-0088.gz).", "labels": [], "entities": [{"text": "Web1T5 3-gram frequency data", "start_pos": 21, "end_pos": 49, "type": "DATASET", "confidence": 0.7965401411056519}]}, {"text": " Table 2: Size of the fully indexed Web1T5 database, in- cluding quasi-collocations.", "labels": [], "entities": [{"text": "Web1T5 database", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9670185446739197}]}, {"text": " Table 4: Evaluation results for English non-compositional VPC (Baldwin, 2008): average precision (AP) as a global  indicator. The baseline AP for random candidate ranking is 14.29%. The best result in each row is highlighted in bold.", "labels": [], "entities": [{"text": "English non-compositional VPC", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.4838368892669678}, {"text": "average precision (AP)", "start_pos": 80, "end_pos": 102, "type": "METRIC", "confidence": 0.8303652286529541}, {"text": "AP", "start_pos": 140, "end_pos": 142, "type": "METRIC", "confidence": 0.9797249436378479}]}]}