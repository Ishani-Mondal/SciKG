{"title": [{"text": "TESLA: Translation Evaluation of Sentences with Linear-programming-based Analysis", "labels": [], "entities": [{"text": "TESLA", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.4309162199497223}, {"text": "Translation Evaluation of Sentences", "start_pos": 7, "end_pos": 42, "type": "TASK", "confidence": 0.9084647297859192}]}], "abstractContent": [{"text": "We present TESLA-M and TESLA, two novel automatic machine translation evaluation metrics with state-of-the-art performances.", "labels": [], "entities": [{"text": "TESLA", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.7939912676811218}, {"text": "machine translation evaluation", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.7659631371498108}]}, {"text": "TESLA-M builds on the success of METEOR and MaxSim, but employs a more expressive linear programming framework.", "labels": [], "entities": [{"text": "TESLA-M", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6202936768531799}]}, {"text": "TESLA further exploits parallel texts to build a shallow semantic representation.", "labels": [], "entities": [{"text": "TESLA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7583248615264893}]}, {"text": "We evaluate both on the WMT 2009 shared evaluation task and show that they outperform all participating systems inmost tasks.", "labels": [], "entities": [{"text": "WMT 2009 shared evaluation task", "start_pos": 24, "end_pos": 55, "type": "DATASET", "confidence": 0.8907144069671631}]}], "introductionContent": [{"text": "In recent years, many machine translation (MT) evaluation metrics have been proposed, exploiting varying amounts of linguistic resources.", "labels": [], "entities": [{"text": "machine translation (MT) evaluation", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.8727389375368754}]}, {"text": "Heavyweight linguistic approaches including RTE () and ULC () performed the best in the WMT 2009 shared evaluation task.", "labels": [], "entities": [{"text": "WMT 2009 shared evaluation task", "start_pos": 88, "end_pos": 119, "type": "DATASET", "confidence": 0.6788070201873779}]}, {"text": "They exploit an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.6133605539798737}, {"text": "discourse representation", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.7272963672876358}]}, {"text": "Lightweight linguistic approaches such as ME-TEOR (), MaxSim, wpF and wpBleu) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and synonym dictionaries.", "labels": [], "entities": [{"text": "part-ofspeech (POS) tagging", "start_pos": 205, "end_pos": 232, "type": "TASK", "confidence": 0.5879094481468201}]}, {"text": "Non-linguistic approaches include BLEU) and its variants, TER), among others.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9974784255027771}, {"text": "TER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9909167885780334}]}, {"text": "They operate purely at the surface word level and no linguistic resources are required.", "labels": [], "entities": []}, {"text": "Although still very popular with MT researchers, they have generally shown inferior performances than the linguistic approaches.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9908077120780945}]}, {"text": "We believe that the lightweight linguistic approaches area good compromise given the current state of computational linguistics research and resources.", "labels": [], "entities": []}, {"text": "In this paper, we devise TESLA-M and TESLA, two lightweight approaches to MT evaluation.", "labels": [], "entities": [{"text": "TESLA-M", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.8590778112411499}, {"text": "TESLA", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.8921958208084106}, {"text": "MT evaluation", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.9743333160877228}]}, {"text": "Specifically: (1) the core features are Fmeasures derived by matching bags of N-grams; (2) both recall and precision are considered, with more emphasis on recall; and (3) WordNet synonyms feature prominently.", "labels": [], "entities": [{"text": "Fmeasures", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9613668918609619}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9988208413124084}, {"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9960848093032837}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9980833530426025}]}, {"text": "The main novelty of TESLA-M compared to METEOR and MaxSim is that we match the Ngrams under a very expressive linear programming framework, which allows us to assign weights to the N-grams.", "labels": [], "entities": []}, {"text": "This is in contrast to the greedy approach of METEOR, and the more restrictive maximum bipartite matching formulation of MaxSim.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.6796188950538635}]}, {"text": "In addition, we present a heavier version TESLA, which combines the features using a linear model trained on development data, making it easy to exploit features not on the same scale, and leaving open the possibility of domain adaptation.", "labels": [], "entities": [{"text": "TESLA", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9565276503562927}, {"text": "domain adaptation", "start_pos": 221, "end_pos": 238, "type": "TASK", "confidence": 0.7208888381719589}]}, {"text": "It also exploits parallel texts of the target language with other languages as a shallow semantic representation, which allows us to model phrase synonyms and idioms.", "labels": [], "entities": []}, {"text": "In contrast, METEOR and MaxSim are capable of processing only word synonyms from WordNet.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.5558906197547913}, {"text": "WordNet", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9420099258422852}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a high level overview of the evaluation task.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 describe TESLA-M and TESLA, respectively.", "labels": [], "entities": [{"text": "TESLA-M", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.6029051542282104}, {"text": "TESLA", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.7934256792068481}]}, {"text": "Section 5 presents experimental results in the setting of the WMT 2009 shared evaluation task.", "labels": [], "entities": [{"text": "WMT 2009 shared evaluation task", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.5925428330898285}]}, {"text": "Finally, Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Into-English task on WMT 2009 data", "labels": [], "entities": [{"text": "WMT 2009 data", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9451902508735657}]}, {"text": " Table 2: Out-of-English task sentence-level con- sistency on WMT 2009 data", "labels": [], "entities": [{"text": "WMT 2009 data", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.9498090147972107}]}, {"text": " Table 3: Out-of-English task system-level correla- tion on WMT 2009 data", "labels": [], "entities": [{"text": "WMT 2009 data", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.9435866276423136}]}]}