{"title": [{"text": "CONE: Metrics for Automatic Evaluation of Named Entity Co-reference Resolution", "labels": [], "entities": [{"text": "Automatic Evaluation of Named Entity Co-reference Resolution", "start_pos": 18, "end_pos": 78, "type": "TASK", "confidence": 0.5622070772307259}]}], "abstractContent": [{"text": "Human annotation for Co-reference Resolution (CRR) is labor intensive and costly, and only a handful of annotated corpora are currently available.", "labels": [], "entities": [{"text": "Co-reference Resolution (CRR)", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.8504408180713654}]}, {"text": "However, corpora with Named Entity (NE) annotations are widely available.", "labels": [], "entities": []}, {"text": "Also, unlike current CRR systems, state-of-the-art NER systems have very high accuracy and can generate NE labels that are very close to the gold standard for unlabeled corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9975810050964355}]}, {"text": "We propose anew set of metrics collectively called CONE for Named Entity Co-reference Resolution (NE-CRR) that use a subset of gold standard annotations, with the advantage that this subset can be easily approximated using NE labels when gold standard CRR annotations are absent.", "labels": [], "entities": [{"text": "Named Entity Co-reference Resolution", "start_pos": 60, "end_pos": 96, "type": "TASK", "confidence": 0.6517537236213684}]}, {"text": "We define CONE B 3 and CONE CEAF metrics based on the traditional B 3 and CEAF metrics and show that CONE B 3 and CONE CEAF scores of any CRR system on any dataset are highly correlated with its B 3 and CEAF scores respectively.", "labels": [], "entities": [{"text": "CONE CEAF metrics", "start_pos": 23, "end_pos": 40, "type": "DATASET", "confidence": 0.7556848526000977}]}, {"text": "We obtain correlation factors greater than 0.6 for all CRR systems across all datasets, and a best-case correlation factor of 0.8.", "labels": [], "entities": [{"text": "correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9705103039741516}, {"text": "correlation", "start_pos": 104, "end_pos": 115, "type": "METRIC", "confidence": 0.9579396843910217}]}, {"text": "We also present a baseline method to estimate the gold standard required by CONE metrics, and show that CONE B 3 and CONE CEAF scores using this estimated gold standard are also correlated with B 3 and CEAF scores respectively.", "labels": [], "entities": [{"text": "CONE metrics", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.7971953749656677}, {"text": "CONE B 3", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.6527297894159952}, {"text": "CONE CEAF", "start_pos": 117, "end_pos": 126, "type": "DATASET", "confidence": 0.7114322185516357}, {"text": "B 3", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.9754575490951538}]}, {"text": "We thus demonstrate the suitability of CONE B 3 and CONE CEAF for automatic evaluation of NE-CRR.", "labels": [], "entities": [{"text": "CONE B 3", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.6080170273780823}, {"text": "CONE CEAF", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.8991595208644867}, {"text": "NE-CRR", "start_pos": 90, "end_pos": 96, "type": "DATASET", "confidence": 0.7733136415481567}]}], "introductionContent": [{"text": "Co-reference resolution (CRR) is the problem of determining whether two entity mentions in a text refer to the same entity in real world or not.", "labels": [], "entities": [{"text": "Co-reference resolution (CRR)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8571597278118134}]}, {"text": "Noun Phrase CRR (NP-CRR) considers all noun phrases as entities, while Named Entity CRR restricts itself to noun phrases that describe a Named Entity.", "labels": [], "entities": [{"text": "Noun Phrase CRR (NP-CRR)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7720630367596945}]}, {"text": "In this paper, we consider the task of Named Entity CRR (NE-CRR) only.", "labels": [], "entities": [{"text": "Named Entity CRR", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.5502233505249023}]}, {"text": "Most, if not all, recent efforts in the field of CRR have concentrated on machine-learning based approaches.", "labels": [], "entities": [{"text": "CRR", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.941078782081604}]}, {"text": "Many of them formulate the problem as a pair-wise binary classification task, in which possible co-reference between every pair of mentions is considered, and produce chains of coreferring mentions for each entity as their output.", "labels": [], "entities": [{"text": "pair-wise binary classification task", "start_pos": 40, "end_pos": 76, "type": "TASK", "confidence": 0.7280287593603134}]}, {"text": "One of the most important problems in CRR is the evaluation of CRR results.", "labels": [], "entities": [{"text": "CRR", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9850026965141296}, {"text": "CRR", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.633543848991394}]}, {"text": "Different evaluation metrics have been proposed for this task.", "labels": [], "entities": []}, {"text": "Bcubed () and CEAF () are the two most popular metrics; they compute Precision, Recall and F1 measure between matched equivalent classes and use weighted sums of Precision, Recall and F1 to produce a global score.", "labels": [], "entities": [{"text": "Bcubed", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9418898820877075}, {"text": "CEAF", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9390537142753601}, {"text": "F1 measure", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.926056832075119}, {"text": "F1", "start_pos": 184, "end_pos": 186, "type": "METRIC", "confidence": 0.9962806105613708}]}, {"text": "Like all metrics, B and CEAF require gold standard annotations; however, gold standard CRR annotations are scarce, because producing such annotations involves a substantial amount of human effort since it requires an in-depth knowledge of linguistics and a high level of understanding of the particular text.", "labels": [], "entities": [{"text": "B", "start_pos": 18, "end_pos": 19, "type": "METRIC", "confidence": 0.981980562210083}]}, {"text": "Consequently, very few corpora with gold standard CRR annotations are available.", "labels": [], "entities": []}, {"text": "By contrast, gold standard Named Entity (NE) annotations are easy to produce; indeed, there are many NE annotated corpora of different sizes and genres.", "labels": [], "entities": []}, {"text": "Similarly, there are few CRR systems and even the best scores obtained by them are only in the region of F1 = 0.5 -0.6.", "labels": [], "entities": [{"text": "F1", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9996021389961243}]}, {"text": "There are only four such CRR systems freely available, to the best of our knowledge (.", "labels": [], "entities": []}, {"text": "In comparison, there are numerous Named Entity recognition (NER) systems, both general-purpose and specialized, and many of them achieve scores better than F1 = 0.95).", "labels": [], "entities": [{"text": "Named Entity recognition (NER)", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.8250196576118469}, {"text": "F1", "start_pos": 156, "end_pos": 158, "type": "METRIC", "confidence": 0.9993568062782288}]}, {"text": "Although these facts can be partly attributed to the 'hardness' of CRR compared to NER, they also reflect the substantial gap between NER and CRR research.", "labels": [], "entities": []}, {"text": "In this paper, we present a set of metrics, collectively called CONE, that leverage widely available NER systems and resources and tools for the task of evaluating co-reference resolution systems.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 164, "end_pos": 187, "type": "TASK", "confidence": 0.6923774778842926}]}, {"text": "The basic idea behind CONE is to predict a CRR system's performance for the task of full NE-CRR on some dataset using its performance for the subtask of named mentions extraction and grouping (NMEG) on that dataset.", "labels": [], "entities": [{"text": "named mentions extraction and grouping (NMEG)", "start_pos": 153, "end_pos": 198, "type": "TASK", "confidence": 0.7812076359987259}]}, {"text": "The advantage of doing so is that measuring NE-CRR performance requires the co-reference information of all mentions of a Named Entity, including named mentions, nominal and pronominal references, while measuring the NMEG performance only requires co-reference information of named mentions of a NE, and this information is relatively easy to obtain automatically even in the absence of gold standard annotations.", "labels": [], "entities": [{"text": "NMEG", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.8235015273094177}]}, {"text": "We compute correlation between CONE B 3 , B 3 , CONE CEAF and CEAF scores for various CRR systems on various goldstandard annotated datasets and show that the CONE B 3 and B 3 scores are highly correlated for all such combinations of CRR systems and datasets, as are CONE CEAF and CEAF scores, with a best-case correlation of 0.8.", "labels": [], "entities": [{"text": "CONE CEAF", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.8692609369754791}, {"text": "CONE CEAF", "start_pos": 267, "end_pos": 276, "type": "DATASET", "confidence": 0.8577353358268738}]}, {"text": "We produce estimated gold standard annotations for the Enron email corpus, since no actual gold standard CRR annotations exist for it, and then use CONE B and CONE CEAF with these estimated gold standard annotations to compare the performance of various NE-CRR systems on this corpus.", "labels": [], "entities": [{"text": "Enron email corpus", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.8793640534083048}, {"text": "CONE B", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.6131122708320618}, {"text": "CONE CEAF", "start_pos": 159, "end_pos": 168, "type": "DATASET", "confidence": 0.6661373376846313}]}, {"text": "No such comparison has been previously performed for the Enron corpus.", "labels": [], "entities": [{"text": "Enron corpus", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9236768186092377}]}, {"text": "We adopt the same terminology as in: a mention refers to each individual phrase and an entity refers to the equivalence class or co-reference chain with several mentions.", "labels": [], "entities": []}, {"text": "This allows us to note some differences between NE-CRR and NP-CRR.", "labels": [], "entities": []}, {"text": "NE-CRR involves indentifying named entities and extracting their coreferring mentions; equivalences classes without any NEs are not considered.", "labels": [], "entities": []}, {"text": "NE-CRR is thus clearly a subset of NP-CRR, where all coreferring mentions and equivalence classes are considered.", "labels": [], "entities": []}, {"text": "However, we focus on NE-CRR because it is currently a more active research area than NP-CRR and a better fit for target applications such as text forensics and web mining, and also because it is more amenable to the automatic evaluation approach that we propose.", "labels": [], "entities": [{"text": "text forensics", "start_pos": 141, "end_pos": 155, "type": "TASK", "confidence": 0.825815737247467}, {"text": "web mining", "start_pos": 160, "end_pos": 170, "type": "TASK", "confidence": 0.6743173003196716}]}, {"text": "The research questions that motivate our work are: (1) Is it possible to use only NER resources to evaluate NE-CRR systems?", "labels": [], "entities": []}, {"text": "If so, how is this problem formulated?", "labels": [], "entities": []}, {"text": "(2) How does one perform evaluation in away that is accurate and automatic with least human intervention?", "labels": [], "entities": []}, {"text": "(3) How does one perform evaluation on large unlabeled datasets?", "labels": [], "entities": []}, {"text": "We show that our CONE metrics achieve good results and represent a promising first step toward answering these questions.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We present related work in the field of automatic evaluation methods for natural language processing tasks in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we give an overview of the standard metrics currently used for evaluating co-reference resolution.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.7190833389759064}]}, {"text": "We define our new metrics CONE B and CONE CEAF in Section 4.", "labels": [], "entities": [{"text": "CONE B", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.5403483211994171}, {"text": "CONE CEAF", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.7538112699985504}]}, {"text": "In section 5, we provide experimental results that illustrate the performance of CONE B and CONE CEAF compared to B and CEAF respectively.", "labels": [], "entities": [{"text": "CONE", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.7255682349205017}, {"text": "CONE CEAF", "start_pos": 92, "end_pos": 101, "type": "DATASET", "confidence": 0.8719192743301392}, {"text": "B", "start_pos": 114, "end_pos": 115, "type": "METRIC", "confidence": 0.9672846794128418}]}, {"text": "In Section 6, we give an example of the application of CONE metrics by evaluating NE-CRR systems on an unlabeled dataset, and discuss possible drawbacks and extensions of these metrics.", "labels": [], "entities": []}, {"text": "Finally, in section 7 we present our conclusions and ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present experimental results in support of the validity and effectiveness of CONE metrics.", "labels": [], "entities": []}, {"text": "As mentioned earlier, we used the following four publicly available CRR systems: UIUC's LBJ system (L), BART from JHU Summer Workshop (B), LingPipe from Alias-i (LP), and OpenNLP (OP).", "labels": [], "entities": [{"text": "UIUC's LBJ system", "start_pos": 81, "end_pos": 98, "type": "DATASET", "confidence": 0.9078702330589294}, {"text": "BART", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9864938259124756}, {"text": "JHU Summer Workshop (B)", "start_pos": 114, "end_pos": 137, "type": "DATASET", "confidence": 0.9201360543568929}]}, {"text": "All these CRR systems perform Noun Phrase co-reference resolution (NP-CRR), not NE-CRR.", "labels": [], "entities": [{"text": "Noun Phrase co-reference resolution", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.6396710127592087}]}, {"text": "So, we must first eliminate all equivalences classes that do not contain any named mentions.", "labels": [], "entities": []}, {"text": "We do so using the SYNERGY NER system to separate named mentions from unnamed ones.", "labels": [], "entities": [{"text": "SYNERGY NER system", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.5923954943815867}]}, {"text": "Note that this must not be confused with the use of SYNERGY to produce G NM and O NM from G and O respectively.", "labels": [], "entities": []}, {"text": "For that task, all equivalence classes in G and O already contain at least one named mention and we remove all unnamed mentions from each class.", "labels": [], "entities": []}, {"text": "This process effectively converts the NP-CRR results of these systems into NE-CRR ones.", "labels": [], "entities": []}, {"text": "We use the ACE Phase 2 NWIRE and ACE 2005 English datasets.", "labels": [], "entities": [{"text": "ACE Phase 2 NWIRE", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.8638570010662079}, {"text": "ACE 2005 English datasets", "start_pos": 33, "end_pos": 58, "type": "DATASET", "confidence": 0.9397450685501099}]}, {"text": "We avoid using the ACE 2004 and MUC6 datasets because the UIUC LBJ system was trained on ACE 2004, while BART and LingPipe were trained on MUC6.", "labels": [], "entities": [{"text": "ACE 2004", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.9534567296504974}, {"text": "MUC6 datasets", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.8722837865352631}, {"text": "UIUC LBJ system", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.964289923508962}, {"text": "ACE 2004", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.9571928083896637}, {"text": "BART", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.6484397053718567}, {"text": "LingPipe", "start_pos": 114, "end_pos": 122, "type": "DATASET", "confidence": 0.9318444132804871}, {"text": "MUC6", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.9447724223136902}]}, {"text": "There are 29 files in the test set of ACE Phrase 2 and 81 files in ACE 2005, summing up to 120 files with around 50,000 tokens with 5000 valid co-reference mentions.", "labels": [], "entities": [{"text": "ACE Phrase 2", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.8958269755045573}, {"text": "ACE 2005", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.9441949129104614}]}, {"text": "show the Pearson's correlation coefficients between CONE metric scores of the type Score(G NM , O NM ) and standard metric scores of the type Score(G, O) for combinations of various CRR systems and datasets.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 9, "end_pos": 30, "type": "METRIC", "confidence": 0.6810154517491659}]}, {"text": "show the correlation coefficients between the new CONE scores and the standard metric scores.", "labels": [], "entities": [{"text": "CONE scores", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.5491436272859573}]}, {"text": "We note from the above results that correlations scores are very similar across different systems and datasets.", "labels": [], "entities": [{"text": "correlations", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.9502376914024353}]}, {"text": "In order to formalize this assertion, we calculate correlation scores in a systemindependent and data-independent manner.", "labels": [], "entities": [{"text": "correlation scores", "start_pos": 51, "end_pos": 69, "type": "METRIC", "confidence": 0.9531829357147217}]}, {"text": "We combine all the data points across all four different systems and plot them in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. G NM : Correlation on ACE Phase 2", "labels": [], "entities": []}, {"text": " Table 2. G NM : Correlation on ACE 2005", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.8663381040096283}]}, {"text": " Table 3. G NM-approx : Correlation on ACE Phase 2", "labels": [], "entities": []}, {"text": " Table 5. G NM-approx Scores on Enron corpus", "labels": [], "entities": []}]}