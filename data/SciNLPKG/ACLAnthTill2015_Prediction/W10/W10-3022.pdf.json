{"title": [{"text": "A Baseline Approach for Detecting Sentences Containing Uncertainty", "labels": [], "entities": [{"text": "Detecting Sentences Containing Uncertainty", "start_pos": 24, "end_pos": 66, "type": "TASK", "confidence": 0.881899282336235}]}], "abstractContent": [{"text": "We apply a baseline approach to the CoNLL-2010 shared task data sets on hedge detection.", "labels": [], "entities": [{"text": "CoNLL-2010 shared task data sets", "start_pos": 36, "end_pos": 68, "type": "DATASET", "confidence": 0.9024387121200561}, {"text": "hedge detection", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.820351630449295}]}, {"text": "Weights have been assigned to cue words marked in the training data based on their occurrences in certain and uncertain sentences.", "labels": [], "entities": []}, {"text": "New sentences received scores that correspond with those of their best scoring cue word, if present.", "labels": [], "entities": []}, {"text": "The best acceptance scores for uncertain sentences were determined using 10-fold cross validation on the training data.", "labels": [], "entities": [{"text": "acceptance", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.8215505480766296}]}, {"text": "This approach performed reasonably on the shared task's biological (F=82.0) and Wikipedia (F=62.8) data sets.", "labels": [], "entities": [{"text": "F", "start_pos": 68, "end_pos": 69, "type": "METRIC", "confidence": 0.9840112328529358}, {"text": "Wikipedia (F=62.8) data sets", "start_pos": 80, "end_pos": 108, "type": "DATASET", "confidence": 0.7585920244455338}]}], "introductionContent": [{"text": "CoNLL-2010 offered two shared tasks which involve finding text parts which express uncertainty or unreliability.", "labels": [], "entities": [{"text": "CoNLL-2010", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.850688099861145}]}, {"text": "We focus on Task 1, identifying sentences which contain statements which can be considered uncertain or unreliable.", "labels": [], "entities": []}, {"text": "We train a basic statistical model on the training data supplied for the task, apply the trained model to the test data and discuss the results.", "labels": [], "entities": []}, {"text": "The next section describes the format of the data and introduces the model that was used.", "labels": [], "entities": []}, {"text": "Section three discusses the experiments with the model and their results.", "labels": [], "entities": []}, {"text": "Section four concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Apart from the word probabilities, we needed to obtain a good threshold score for deciding whether to classify a sentence ascertain or uncertain.", "labels": [], "entities": []}, {"text": "For this purpose, we performed a 10-fold crossvalidation experiment on each of the two training data files (biological and Wikipedia) and measured the effect of different threshold values.", "labels": [], "entities": []}, {"text": "The results can be found in.", "labels": [], "entities": []}, {"text": "The model performed well on the biological training data, with F scores above 80 fora large range of threshold values (0.15-0.85).", "labels": [], "entities": [{"text": "F", "start_pos": 63, "end_pos": 64, "type": "METRIC", "confidence": 0.9991101622581482}]}, {"text": "It performed less well on the Wikipedia training data, with a maximum F score of less than 60 and 50+ scores being limited to the threshold range 0.45-0.85.", "labels": [], "entities": [{"text": "Wikipedia training data", "start_pos": 30, "end_pos": 53, "type": "DATASET", "confidence": 0.8374456167221069}, {"text": "F score", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9914179742336273}]}, {"text": "The maximum F scores were reached for threshold values 0.55 and 0.65 for biological data (F=88.8) and Wikipedia data (F=59.4), respectively.", "labels": [], "entities": [{"text": "F", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.999468982219696}, {"text": "F=88.8)", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9572381228208542}, {"text": "Wikipedia data", "start_pos": 102, "end_pos": 116, "type": "DATASET", "confidence": 0.9173313975334167}, {"text": "F=59.4)", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.8626484274864197}]}, {"text": "We selected the threshold value 0.55 for our further work because the associated precision and recall values were closer to each other than for value 0.65.", "labels": [], "entities": [{"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9985321760177612}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9941954016685486}]}, {"text": "We build domain-specific models with the biological data (14,541 sentences) and the Wikipedia data (11,111 sentences) and applied the models to the related training data.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 84, "end_pos": 98, "type": "DATASET", "confidence": 0.9355483651161194}]}, {"text": "We obtained an F score of 80.2 on the biological data (13th of 20 participants) and a score of 54.4 on the Wikipedia data (9th of 15 participants).", "labels": [], "entities": [{"text": "F score", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9881539046764374}, {"text": "Wikipedia data", "start_pos": 107, "end_pos": 121, "type": "DATASET", "confidence": 0.9539884030818939}]}, {"text": "The balance between precision and recall scores that we strived for when processing the training data, was not visible in the test results.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9995369911193848}, {"text": "recall scores", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.9805260002613068}]}, {"text": "On the biological test data the system's recall score was 13 points higher than the precision score while on the Wikipedia test data precision outperformed recall by 31 points (see).", "labels": [], "entities": [{"text": "recall score", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9892389178276062}, {"text": "precision score", "start_pos": 84, "end_pos": 99, "type": "METRIC", "confidence": 0.9773841202259064}, {"text": "Wikipedia test data", "start_pos": 113, "end_pos": 132, "type": "DATASET", "confidence": 0.963594655195872}, {"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.8468257188796997}, {"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.8853815793991089}]}, {"text": "Next formance for the biological data dropped to F = 84.2 (threshold 0.60) while the top score for the Wikipedia data dropped to F = 56.5 (0.70).", "labels": [], "entities": [{"text": "F", "start_pos": 49, "end_pos": 50, "type": "METRIC", "confidence": 0.9993910789489746}, {"text": "Wikipedia data", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.946877509355545}, {"text": "F", "start_pos": 129, "end_pos": 130, "type": "METRIC", "confidence": 0.9980948567390442}]}, {"text": "We kept the threshold value of 0.55, built a model from all available training data and tested its performance on the two test sets.", "labels": [], "entities": []}, {"text": "In both cases the performances were lower than the ones obtained with domain dependent training data: F = 71.8 for biological data and F = 54.2 for Wikipedia data (see).", "labels": [], "entities": [{"text": "F", "start_pos": 102, "end_pos": 103, "type": "METRIC", "confidence": 0.998852014541626}, {"text": "F", "start_pos": 135, "end_pos": 136, "type": "METRIC", "confidence": 0.9926950931549072}, {"text": "Wikipedia data", "start_pos": 148, "end_pos": 162, "type": "DATASET", "confidence": 0.9081915318965912}]}, {"text": "As post-deadline work, we added statistics for word bigrams to the model, following up work by, who showed that considering word bigrams had a positive effect on hedge detection.", "labels": [], "entities": [{"text": "hedge detection", "start_pos": 162, "end_pos": 177, "type": "TASK", "confidence": 0.8800080418586731}]}, {"text": "We changed the probability estimation score of words appearing in a hedge cue to where w i\u22121 w i is a bigram of successive words in a sentence.", "labels": [], "entities": []}, {"text": "Bigrams were considered to be part of a hedge cue when either or both words were inside the hedge cue.", "labels": [], "entities": []}, {"text": "Unigram probabilities were used as backoff for known words that appeared outside known bigrams while unknown words received the most common score for known words (0).", "labels": [], "entities": []}, {"text": "Sentences received a score which is equal to one minus the highest score of their word bigrams: P (s is certain) = 1\u2212 argmax We repeated the threshold estimation experiments and found that new bigram scores enabled the models to perform slightly better on the training data.", "labels": [], "entities": []}, {"text": "The maximum F score for biological training data improved from 88.8 to 90.1 (threshold value 0.35) while the best F score for the Wikipedia training data moved up slightly to 59.8 (0.65).", "labels": [], "entities": [{"text": "F score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9862355291843414}, {"text": "F score", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.984252542257309}, {"text": "Wikipedia training data", "start_pos": 130, "end_pos": 153, "type": "DATASET", "confidence": 0.8597816030184428}]}, {"text": "We applied the bigram models with the two optimal threshold values for the training data to the test data sets.", "labels": [], "entities": []}, {"text": "For the biological data, we obtained an F score of 82.0, a borderline significant improvement over the unigram model score.", "labels": [], "entities": [{"text": "F score", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9901897609233856}]}, {"text": "The performance on the Wikipedia data improved significantly, by eight points, to F = 62.8 (see).", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.9725527763366699}, {"text": "F", "start_pos": 82, "end_pos": 83, "type": "METRIC", "confidence": 0.9996826648712158}]}, {"text": "This is also an improvement of the official best score for this data set (60.2).", "labels": [], "entities": []}, {"text": "We believe that the improvement originates from using the bigram model as well as applying a threshold value that is better suitable for the Wikipedia data set (note that in our unigram experiments we used the same threshold value for all data sets).", "labels": [], "entities": [{"text": "Wikipedia data set", "start_pos": 141, "end_pos": 159, "type": "DATASET", "confidence": 0.9645630915959676}]}], "tableCaptions": [{"text": " Table 1: Performances of the models for different  combinations of training and test data sets with the  associated acceptance threshold values. Training  and testing with data from the same domain pro- duces the best scores. Higher recall scores were  obtained for biological data than for Wikipedia  data. Standard deviations for F scores were esti- mated with bootstrap resampling (Yeh, 2000).", "labels": [], "entities": [{"text": "recall", "start_pos": 234, "end_pos": 240, "type": "METRIC", "confidence": 0.998497486114502}, {"text": "F scores", "start_pos": 333, "end_pos": 341, "type": "METRIC", "confidence": 0.9707920849323273}]}, {"text": " Table 2: Performances of bigram models for dif- ferent combinations of training and test data sets.  The bigram models performed better than the uni- gram models (compare with Table 1).", "labels": [], "entities": []}]}