{"title": [{"text": "Head Finalization: A Simple Reordering Rule for SOV Languages", "labels": [], "entities": [{"text": "Head Finalization", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8229204714298248}, {"text": "SOV Languages", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.9324950575828552}]}], "abstractContent": [{"text": "English is atypical SVO (Subject-Verb-Object) language, while Japanese is atypical SOV language.", "labels": [], "entities": []}, {"text": "Conventional Statistical Machine Translation (SMT) systems work well within each of these language families.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.7803543458382288}]}, {"text": "However, SMT-based translation from an SVO language to an SOV language does notwork well because their word orders are completely different.", "labels": [], "entities": [{"text": "SMT-based translation", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.9699754118919373}]}, {"text": "Recently , a few groups have proposed rule-based preprocessing methods to mitigate this problem (Xu et al., 2009; Hong et al., 2009).", "labels": [], "entities": []}, {"text": "These methods rewrite SVO sentences to derive more SOV-like sentences by using a set of handcrafted rules.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative single reordering rule: Head Finalization.", "labels": [], "entities": [{"text": "Head Finalization", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.8244971036911011}]}, {"text": "This is a syntax-based preprocessing approach that offers the advantage of simplicity.", "labels": [], "entities": []}, {"text": "We do not have to be concerned about part-of-speech tags or rule weights because the powerful Enju parser allows us to implement the rule at a general level.", "labels": [], "entities": [{"text": "Enju parser", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.715552031993866}]}, {"text": "Our experiments show that its result, Head Final English (HFE), follows almost the same order as Japanese.", "labels": [], "entities": [{"text": "Head Final English (HFE)", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.5265853603680929}]}, {"text": "We also show that this rule improves automatic evaluation scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical Machine Translation (SMT) is useful for building a machine translator between a pair of languages that follow similar word orders.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8350082834561666}]}, {"text": "However, SMT does notwork well for distant language pairs such as English and Japanese, since English is an SVO language and Japanese is an SOV language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9868564009666443}]}, {"text": "Some existing methods try to solve this wordorder problem in language-independent ways.", "labels": [], "entities": []}, {"text": "They usually parse input sentences and learn a reordering decision at each node of the parse trees.", "labels": [], "entities": []}, {"text": "For example,,,, and proposed such methods.", "labels": [], "entities": []}, {"text": "Other methods tackle this problem in languagedependent ways).", "labels": [], "entities": []}, {"text": "Recently, and proposed rule-based preprocessing methods for SOV languages.", "labels": [], "entities": [{"text": "SOV languages", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.9007265865802765}]}, {"text": "These methods parse input sentences and reorder the words using a set of handcrafted rules to get SOV-like sentences.", "labels": [], "entities": [{"text": "SOV-like sentences", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.876118004322052}]}, {"text": "If we could completely reorder the words in input sentences by preprocessing to match the word order of the target language, we would be able to greatly reduce the computational cost of SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 186, "end_pos": 189, "type": "TASK", "confidence": 0.9954600930213928}]}, {"text": "In this paper, we introduce a single reordering rule: Head Finalization.", "labels": [], "entities": [{"text": "Head Finalization", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.815734326839447}]}, {"text": "We simply move syntactic heads to the end of the corresponding syntactic constituents (e.g., phrases and clauses).", "labels": [], "entities": []}, {"text": "We use only this reordering rule, and we do not have to consider part-of-speech tags or rule weights because the powerful Enju parser allows us to implement the rule at a general level.", "labels": [], "entities": []}, {"text": "Why do we think this works?", "labels": [], "entities": []}, {"text": "The reason is simple: Japanese is atypical head-final language.", "labels": [], "entities": []}, {"text": "That is, a syntactic headword comes after nonhead (dependent) words.", "labels": [], "entities": []}, {"text": "SOV is just one aspect of head-final languages.", "labels": [], "entities": [{"text": "SOV", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9185943007469177}]}, {"text": "In order to implement this idea, we need a parser that outputs syntactic heads.", "labels": [], "entities": []}, {"text": "Enju is such a parser from the University of Tokyo (http://www-tsujii.is.s. u-tokyo.ac.jp/enju).", "labels": [], "entities": []}, {"text": "We discuss other parsers in section 5.", "labels": [], "entities": []}, {"text": "There is another kind of head: semantic heads.", "labels": [], "entities": []}, {"text": "used Stanford parser (), which outputs semantic headbased dependencies; also used the same representation.", "labels": [], "entities": []}, {"text": "The use of syntactic heads and the number of dependents are essential for the simplicity of Head Finalization (See Discussion).", "labels": [], "entities": [{"text": "Head Finalization", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.86432746052742}]}, {"text": "Our method simply checks whether a tree node is a syntactic head.", "labels": [], "entities": []}, {"text": "We do not have to consider what we are moving and how to move it.", "labels": [], "entities": []}, {"text": "On the other hand, Xu et al. had to introduce dozens of weighted rules, probably because they used the semantic headbased dependency representation without restriction on the number of dependents.", "labels": [], "entities": []}, {"text": "The major difference between our method and the above conventional methods, other than its simplicity, is that our method moves not only verbs and adjectives but also functional words such as prepositions.", "labels": [], "entities": []}, {"text": "shows Enju's XML output for the simple sentence: \"John hit a ball.\"", "labels": [], "entities": []}, {"text": "The tag <cons> indicates a nonterminal node and <tok> indicates a terminal node or a word (token).", "labels": [], "entities": []}, {"text": "Each node has a unique id.", "labels": [], "entities": []}, {"text": "Head information is given by the node's head attribute.", "labels": [], "entities": []}, {"text": "For instance, node c0's head is node c3, and c3 is a VP, or verb phrase.", "labels": [], "entities": []}, {"text": "Thus, Enju treats not only words but also non-terminal nodes as heads.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to show how closely our Head Finalization makes English follow Japanese word order, we measured Kendall's \u03c4 , a rank correlation coefficient.", "labels": [], "entities": [{"text": "Head Finalization", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.780634343624115}, {"text": "Kendall's \u03c4", "start_pos": 105, "end_pos": 116, "type": "METRIC", "confidence": 0.68693408370018}]}, {"text": "We also measured BLEU () and other automatic evaluation scores to show that Head Finalization can actually improve the translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9989885687828064}, {"text": "Head Finalization", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.8062692284584045}]}, {"text": "We used NTCIR7 PAT-MT's Patent corpus (.", "labels": [], "entities": [{"text": "NTCIR7 PAT-MT's Patent corpus", "start_pos": 8, "end_pos": 37, "type": "DATASET", "confidence": 0.8482531309127808}]}, {"text": "Its training corpus has 1.8 million sentence pairs.", "labels": [], "entities": []}, {"text": "We used MeCab (http:// mecab.sourceforge.net/) to segment Japanese sentences.", "labels": [], "entities": [{"text": "MeCab", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.9033406376838684}]}, {"text": "First, we examined rank correlation between Head Final English sentences produced by the Head Finalization rule and Japanese reference sentences.", "labels": [], "entities": [{"text": "Head Finalization", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.677389919757843}]}, {"text": "Since we do not have handcrafted word alignment data for an English-to-Japanese bilingual corpus, we used GIZA++ ( to get automatic word alignment.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.8407995700836182}, {"text": "word alignment", "start_pos": 132, "end_pos": 146, "type": "TASK", "confidence": 0.7285852134227753}]}, {"text": "Based on this automatic word alignment, we measured Kendall's \u03c4 for the word order between HFE sentences and Japanese sentences.", "labels": [], "entities": [{"text": "Kendall's \u03c4", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.735034704208374}]}, {"text": "In this case, we get \u03c4 = 5/6 \u00d7 2 \u2212 1 = 0.667.", "labels": [], "entities": []}, {"text": "For each sentence in the training data, we calculate \u03c4 based on a GIZA++ alignment file, en-ja.A3.final.", "labels": [], "entities": [{"text": "GIZA++ alignment file", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.8384542912244797}]}, {"text": "(We also tried ja-en.A3.final, but we got similar results.)", "labels": [], "entities": []}, {"text": "It looks something like this: Numbers in ({ }) indicate corresponding English words.", "labels": [], "entities": []}, {"text": "The article 'a' has no corresponding word in Japanese, and such words are listed in NULL ({ }).", "labels": [], "entities": [{"text": "NULL", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.5933359265327454}]}, {"text": "From this alignment information, we get an integer list.", "labels": [], "entities": []}, {"text": "Then, we get \u03c4 = 5/ 4 C 2 \u00d7 2 \u2212 1 = 0.667.", "labels": [], "entities": []}, {"text": "For HFE in, we will get the following alignment.", "labels": [], "entities": []}, {"text": "Then, we get and \u03c4 = 1.0.", "labels": [], "entities": []}, {"text": "We use \u03c4 or the average of \u03c4 overall training sentences to observe the tendency.", "labels": [], "entities": []}, {"text": "Sometimes, one Japanese word corresponds to an English phrase: We get [1, 4, 5, 3, 2, 6] from this alignment.", "labels": [], "entities": []}, {"text": "When the same word (or derivative words) appears twice or more in a single English sentence, two or more non-consecutive words in the English sentence are aligned to a single Japanese word: We excluded the ambiguously aligned words from the calculation of \u03c4 . We use only and get \u03c4 = \u22121.0.", "labels": [], "entities": []}, {"text": "The exclusion of these words will be criticized by statisticians, but even this rough calculation of \u03c4 sheds light on the weak points of Head Finalization.", "labels": [], "entities": [{"text": "Head Finalization", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.8236339390277863}]}, {"text": "Because of this exclusion, the best value \u03c4 = 1.0 does not mean that we obtained the perfect word ordering, but low \u03c4 values imply failures.", "labels": [], "entities": []}, {"text": "In section 4, we use \u03c4 to analyze failures.", "labels": [], "entities": []}, {"text": "By examining low \u03c4 sentences, we found that patent documents have a lot of expressions such as \"motor 2.\"", "labels": [], "entities": []}, {"text": "These are reordered (2 motor) and slightly degrade \u03c4 . We did not notice this problem until we handled the patent corpus because these expressions are rare in other documents such as news articles.", "labels": [], "entities": []}, {"text": "Here, we added a rule to keep these expressions.", "labels": [], "entities": []}, {"text": "We did not use any dictionary in our experiment, but if we add dictionary entries to the training data, it raises \u03c4 because most entries are short.", "labels": [], "entities": []}, {"text": "One-word entries do not affect \u03c4 because we cannot calculate \u03c4 . Most multi-word entries are short noun phrases that are not reordered (\u03c4 = 1.0).", "labels": [], "entities": []}, {"text": "Therefore, we should exclude dictionary entries from the calculation of \u03c4 .  In general, it is believed that translation between English and Japanese requires a large distortion limit (dl), which restricts how far a phrase can move.", "labels": [], "entities": [{"text": "translation between English and Japanese", "start_pos": 109, "end_pos": 149, "type": "TASK", "confidence": 0.8853587865829468}, {"text": "distortion limit (dl)", "start_pos": 167, "end_pos": 188, "type": "METRIC", "confidence": 0.9039613008499146}]}, {"text": "SMT reasearchers working on E-J or J-E translation often use dl=\u22121 (unlimited) as a default value, and this takes along translation time.", "labels": [], "entities": [{"text": "SMT reasearchers", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9040459096431732}]}, {"text": "For PATMT J-E translation, showed that dl=unlimited is the best and it requires a very long translation time.", "labels": [], "entities": [{"text": "PATMT J-E translation", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.6524864435195923}]}, {"text": "For PATMT E-J translation, claimed that they achieved the best result \"when the distortion limit was 20 instead of \u22121.\" compares the single-reference BLEU score of the proposed method and that of the Moses-based system by the NTCIR-7 PATMT organizers.", "labels": [], "entities": [{"text": "PATMT E-J translation", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.6928092837333679}, {"text": "BLEU", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9766002893447876}, {"text": "NTCIR-7 PATMT organizers", "start_pos": 226, "end_pos": 250, "type": "DATASET", "confidence": 0.9065120816230774}]}, {"text": "This organizers' system was better than all participants () in terms of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9955594539642334}]}, {"text": "Here, we used Bleu Kit (http:// www.mibel.cs.tsukuba.ac.jp/norimatsu/ bleu kit/) following the PATMT's overview paper ().", "labels": [], "entities": [{"text": "PATMT's overview paper", "start_pos": 95, "end_pos": 117, "type": "DATASET", "confidence": 0.8940166383981705}]}, {"text": "The table shows that dl=6 gives the best result, and even dl=0 (no reordering in Moses) gives better scores than the organizers' Moses.: Improvement in word order sures global word order.", "labels": [], "entities": []}, {"text": "Another line 'no va' stands for our method without vas or particle seeds.", "labels": [], "entities": []}, {"text": "Without particle seeds, all scores slightly drop.", "labels": [], "entities": []}, {"text": "Echizen-ya et al. showed that IMPACT and ROUGE-L are highly correlated to human evaluation in evaluating J-E patent translation.", "labels": [], "entities": [{"text": "IMPACT", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.988061785697937}, {"text": "ROUGE-L", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9971407651901245}, {"text": "J-E patent translation", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.7310553590456644}]}, {"text": "Therefore, we also used these evaluation methods here for E-J translation.", "labels": [], "entities": [{"text": "E-J translation", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.6952962279319763}]}, {"text": "shows that the proposed method is also much better than the organizers' Moses in terms of these measures.", "labels": [], "entities": []}, {"text": "Without particle seeds, these scores also drop slightly.", "labels": [], "entities": []}, {"text": "On the other hand, Position-independent Word Error Rate (PER), which completely disregards word order, does not change very much.", "labels": [], "entities": [{"text": "Position-independent Word Error Rate (PER)", "start_pos": 19, "end_pos": 61, "type": "METRIC", "confidence": 0.793018285717283}]}, {"text": "These facts indicate that our method improves word order, which is the most important problem in E-J translation.", "labels": [], "entities": [{"text": "E-J translation", "start_pos": 97, "end_pos": 112, "type": "TASK", "confidence": 0.7839990556240082}]}, {"text": "The organizers' Moses uses dl=unlimited, and it has been reported that its MERT training took two weeks.", "labels": [], "entities": [{"text": "MERT", "start_pos": 75, "end_pos": 79, "type": "TASK", "confidence": 0.6021007895469666}]}, {"text": "On the other hand, our MERT training with dl=6 took only eight hours on a PC: Xeon X5570 2.93 GHz.", "labels": [], "entities": [{"text": "MERT", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.7001394629478455}]}, {"text": "Our method takes extra time to parse sentences by Enju, but it is easy to run the parser in parallel.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Main causes of 20 worst sentences", "labels": [], "entities": []}, {"text": " Table 2: Automatic Evaluation of Translation  Quality (Numbers in parentheses indicate distor- tion limits).", "labels": [], "entities": [{"text": "Automatic Evaluation of Translation", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.5129596590995789}, {"text": "distor- tion limits", "start_pos": 88, "end_pos": 107, "type": "METRIC", "confidence": 0.8933684080839157}]}]}