{"title": [{"text": "ISCAS\uff1aA System for Chinese Word Sense Induction Based on K-means Algorithm", "labels": [], "entities": [{"text": "Chinese Word Sense Induction", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.7074180766940117}]}], "abstractContent": [{"text": "This paper presents an unsupervised method for automatic Chinese word sense induction.", "labels": [], "entities": [{"text": "automatic Chinese word sense induction", "start_pos": 47, "end_pos": 85, "type": "TASK", "confidence": 0.6062100052833557}]}, {"text": "The algorithm is based on clustering the similar words according to the contexts in which they occur.", "labels": [], "entities": []}, {"text": "First, the target word which needs to be disambiguated is represented as the vector of its contexts.", "labels": [], "entities": []}, {"text": "Then, reconstruct the matrix constituted by the vectors of target words through singular value decomposition (SVD) method, and use the vectors to cluster the similar words.", "labels": [], "entities": []}, {"text": "Our system participants in CLP2010 back off task4-Chinese word sense induction.", "labels": [], "entities": [{"text": "CLP2010", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9526340961456299}, {"text": "task4-Chinese word sense induction", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.5918080136179924}]}], "introductionContent": [{"text": "It has been shown that using word senses instead of surface word forms could improve performance on many nature language processing tasks such as information extraction), information retrieval () and machine translation ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 146, "end_pos": 168, "type": "TASK", "confidence": 0.7820058166980743}, {"text": "information retrieval", "start_pos": 171, "end_pos": 192, "type": "TASK", "confidence": 0.8215640485286713}, {"text": "machine translation", "start_pos": 200, "end_pos": 219, "type": "TASK", "confidence": 0.8262598514556885}]}, {"text": "Historically, word senses are represented as a fixed-list of definitions coming from a manually complied dictionary.", "labels": [], "entities": []}, {"text": "However, there seem to be some disadvantages associated with such fixed-list of senses paradigm.", "labels": [], "entities": []}, {"text": "Since dictionaries usually contain general definitions and lack explicit semantic, they can't reflect the exact content of the context where the target word appears.", "labels": [], "entities": []}, {"text": "Another disadvantage is that the granularity of sense distinctions is fixed, so it may not be entirely suitable for different applications.", "labels": [], "entities": []}, {"text": "In order to overcome these limitations, some techniques like word sense induction (WSI) have been proposed for discovering words' senses automatically from the unannotated corpus.", "labels": [], "entities": [{"text": "word sense induction (WSI)", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.7884286592404047}]}, {"text": "The word sense induction algorithms are usually base on the Distributional Hypothesis, proposed by, which showed that words with similar meanings appear in similar contexts.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.813937266667684}]}, {"text": "And the hypothesis is also popularized with the phrase \"a word characterized by the company it keeps\".", "labels": [], "entities": []}, {"text": "This concept shows us a method to automatically discover senses of words by clustering the target words with similar contexts.", "labels": [], "entities": []}, {"text": "The word sense induction can be regarded as an unsupervised clustering problem.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.6488110919793447}]}, {"text": "First, select some features to be used when comparing similarity between words.", "labels": [], "entities": []}, {"text": "Second, represent disambiguated words as vectors of selected features according to target words' contexts.", "labels": [], "entities": []}, {"text": "Third, cluster the similar words using the vectors.", "labels": [], "entities": []}, {"text": "But compared with European languages such as English, Chinese language has its own characteristics.", "labels": [], "entities": []}, {"text": "For example, Chinese ideographs have senses while the English alphabets don't have.", "labels": [], "entities": []}, {"text": "So the methods which work well in English may not be entirely suitable for Chinese.", "labels": [], "entities": []}, {"text": "This paper proposes a method for Chinese word sense induction, which contains two stage processes: features selecting and context clustering.", "labels": [], "entities": [{"text": "Chinese word sense induction", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.6742223352193832}, {"text": "context clustering", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.6824271380901337}]}, {"text": "Chinese ideographs and Chinese words which have two or more Chinese ideographs are used different strategies when selecting features.", "labels": [], "entities": []}, {"text": "The vectors of target word's instances are put together to constitute a matrix, whose row is instances and column is features.", "labels": [], "entities": []}, {"text": "Reconstruct the matrix through singular value decomposition to get anew vector for each instance.", "labels": [], "entities": []}, {"text": "Then, Kmeans clustering algorithm is employed to cluster the vectors of disambiguated words' contexts.", "labels": [], "entities": [{"text": "Kmeans clustering", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.6761735677719116}]}, {"text": "Each cluster to which some instances belong to identifies a sense of corresponding target word.", "labels": [], "entities": []}, {"text": "Our system participants in CLP2010 back off task4 -Chinese word sense induction.", "labels": [], "entities": [{"text": "CLP2010", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9526633024215698}, {"text": "Chinese word sense induction", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.5726203769445419}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the Chinese word senses induction algorithm.", "labels": [], "entities": [{"text": "Chinese word senses induction", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.579171821475029}]}, {"text": "Section 3 presents the evaluation sheme and the results of our system.", "labels": [], "entities": []}, {"text": "Section 4 gives some discussions and conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents the evaluation scheme, set of parameters and the result of our system.", "labels": [], "entities": []}, {"text": "We use standard cluster evaluation methods to measure the performance of our WSI system.", "labels": [], "entities": []}, {"text": "Following the former practice (), we consider the FScore measure for assessing WSI methods.", "labels": [], "entities": [{"text": "FScore", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9886921644210815}, {"text": "WSI", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.954179048538208}]}, {"text": "The FScore is used in a similar fashion to Information Retrieval exercises.", "labels": [], "entities": [{"text": "FScore", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.5163523554801941}, {"text": "Information Retrieval", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.8650630712509155}]}, {"text": "Let we assume that the size of a particular class s r is n r , the size of a particular cluster h j is n j and the size of their common instances set is n r,j.", "labels": [], "entities": []}, {"text": "The precision can be calculated as follow:  Where q is the number of classes and n is the total number of the instances where target word appears.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9993622899055481}]}], "tableCaptions": [{"text": " Table 2: The results of K-means", "labels": [], "entities": [{"text": "K-means", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.923393726348877}]}, {"text": " Table 3: The results of CW.  The result shows that the K-means algorithm  has a better performance than CW. That may", "labels": [], "entities": []}]}