{"title": [{"text": "Close = Relevant? The Role of Context in Efficient Language Production", "labels": [], "entities": []}], "abstractContent": [{"text": "We formally derive a mathematical model for evaluating the effect of context relevance in language production.", "labels": [], "entities": []}, {"text": "The model is based on the principle that distant con-textual cues tend to gradually lose their relevance for predicting upcoming linguistic signals.", "labels": [], "entities": [{"text": "predicting upcoming linguistic signals", "start_pos": 109, "end_pos": 147, "type": "TASK", "confidence": 0.8316930681467056}]}, {"text": "We evaluate our model against a hypothesis of efficient communication (Genzel and Charniak's Constant Entropy Rate hypothesis).", "labels": [], "entities": []}, {"text": "We show that the development of entropy throughout discourses is described significantly better by a model with cue relevance decay than by previous models that do not consider context effects .", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we present a study on the effect of context relevance decay on the entropy of linguistic signals in natural discourses.", "labels": [], "entities": []}, {"text": "Context relevance decay refers to the phenomenon that contextual cues that are distant from an upcoming event (e.g. production of anew linguistic signal) are less likely to be relevant to the event, as discourse contents that are close to one another are likely to be semantically related.", "labels": [], "entities": [{"text": "Context relevance decay", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8556715647379557}]}, {"text": "One can also view the words and sentences in a discourse as time steps, where distant context becomes less relevant simply due to normal forgetting overtime (e.g. activation decay in memory).", "labels": [], "entities": []}, {"text": "The present study investigates how this decaying property of discourse context might affect the development of entropy of linguistic signals in discourses.", "labels": [], "entities": []}, {"text": "We first introduce the background on efficient language production and then propose our hypothesis.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of words and sentences in the training and test data for each of the twelve languages.  The last column gives the number of sentences at each sentence position (which is identical to the number  of documents contained in the corpora).", "labels": [], "entities": []}, {"text": " Table 2: In the power model, relevance of a con- textual cue decays rather quickly for each lan- guage.", "labels": [], "entities": [{"text": "relevance", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.968462347984314}]}, {"text": " Table 3: In the exponential model, relevance of a  contextual cue decays more slowly.", "labels": [], "entities": []}]}