{"title": [{"text": "Generating Shifting Sentiment fora Conversational Agent", "labels": [], "entities": [{"text": "Generating Shifting Sentiment", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7153970003128052}]}], "abstractContent": [{"text": "We investigate techniques for generating alternative output sentences with varying sentiment , using (an approximation to) the Valentino method, based on SentiWordNet, of Guerini et al.", "labels": [], "entities": []}, {"text": "We extend this method by filtering out unacceptable candidate sentences, using bigrams sourced from different corpora to determine whether lexical substitutions are appropriate in the given context.", "labels": [], "entities": []}, {"text": "We also compare the generated candidates against human judgements of whether the desired sentiment shift has occurred: our results suggest limitations with the overall knowledge-based approach , and we propose potential directions for improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "The design of more natural or believable conversational agents requires the need for such agents to communicate affectively, by the display of emotion or attitude towards objects, other agents, or states of affairs.", "labels": [], "entities": []}, {"text": "More engaging or influential agents may seek to actually affect their conversational partner at a deeper level, for example, by influencing their emotional state (van der.", "labels": [], "entities": []}, {"text": "Previous work in this area has explored the use of gestures and facial expression) and rhythm and prosody of speech () for expressing affect; however there has been little work on generation of affective language in dialogue.", "labels": [], "entities": []}, {"text": "Our general approach is inspired by)'s work on generating different surface-level versions of utterance content, depending on an agent's appraisals towards objects, characters and events in its environment.", "labels": [], "entities": []}, {"text": "While their approach is effective, it relies on manual creation of lexical alternatives, customized to the application domain.", "labels": [], "entities": []}, {"text": "We are interested in approaches that will scale, and can be applied domain-independently.", "labels": [], "entities": []}, {"text": "While our ultimate aim is generation of language that relects emotional state, in this work we investigate the automatic generation of varying \"sentiment\" in output utterances; we focus on sentiment mainly due to the recent development of useful resources for this task.", "labels": [], "entities": []}, {"text": "()'s Valentino system is an approach to automatically generating candidate output utterances with different sentiment from an original; the authors suggest ECAs as a possible application scenario for their techniques.", "labels": [], "entities": []}, {"text": "We explore this suggestion, implementing a lexical substitution approach to dialogue generation with sentiment, using the Valentino approach and associated resources.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7702524960041046}]}, {"text": "Lexical substitution approaches raise well-known challenges, and we investigate a number of techniques to address these in Section 4; for example, using bigrams and grammatical relations to determine which substitutions are acceptable based on their context in a sentence.", "labels": [], "entities": [{"text": "Lexical substitution", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8699430823326111}]}, {"text": "Our techniques show improvement over naive lexical substitution; however, an evaluation with human subjects suggests that a deeper problem is that even \"acceptable\" candidate sentences generated by the method do not match human judgements with respect to sentiment shift: i.e., alternatives labeled as more positive (resp., negative) than the original by the system are often seen as a sentiment shift in the opposite direction by human judges (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "The data set we used for this evaluation consisted of 25 sentences, randomly extracted from the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.9723470211029053}]}, {"text": "The sentences were sourced from the BNC to avoid any bias which may have been introduced had the test sentences been created manually.", "labels": [], "entities": [{"text": "BNC", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9434279799461365}]}, {"text": "We required that each test sentence satisfy the following conditions 11 : 1.", "labels": [], "entities": []}, {"text": "The sentence must contain between 6 and 10 words (to reflect length of atypical dialogue utterance); 2.", "labels": [], "entities": []}, {"text": "The sentence must contain at least one term which is found in the OVVTs (otherwise it would be pointless for evaluation purposes); the term may have any valence.", "labels": [], "entities": [{"text": "OVVTs", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.7919124960899353}]}, {"text": "12 Our second filtering technique requires information about the grammatical relations between terms in a sentence (illustrated in).", "labels": [], "entities": []}, {"text": "For this, we used aversion of the BNC which was pre-processed with the RASP parser ().", "labels": [], "entities": [{"text": "BNC", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.8484960794448853}]}, {"text": "Our gold standard for candidate acceptability was created using the first author's judgements.", "labels": [], "entities": []}, {"text": "In or-der to be judged as an ACCEPT by the annotator, a generated sentence needed to satisfy the following criteria (otherwise it was labelled REJECT):  To evaluate each candidate selection method, we performed the following procedure for each of our 25 test sentences: 1.", "labels": [], "entities": [{"text": "REJECT", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9862486720085144}]}, {"text": "Find all matching 15 terms and retrieve the valence score of each; 2.", "labels": [], "entities": [{"text": "valence score", "start_pos": 44, "end_pos": 57, "type": "METRIC", "confidence": 0.9353609085083008}]}, {"text": "For each matching term: (a) Retrieve the corresponding list of alternative terms from the OVVTs; (b) Generate several different candidate sentences by substituting each alternative term into the original sentence; (c) Apply the chosen candidate selection technique to each generated sentence, and label each as ACCEPT or REJECT (for step 3); 3.", "labels": [], "entities": [{"text": "ACCEPT", "start_pos": 311, "end_pos": 317, "type": "METRIC", "confidence": 0.6725088357925415}, {"text": "REJECT", "start_pos": 321, "end_pos": 327, "type": "METRIC", "confidence": 0.9312489628791809}]}, {"text": "Compare all system classifications to our gold standard (automatically), and mark each as either a true positive (TP), false positive (FP), true negative (TN), or false negative (FN).", "labels": [], "entities": [{"text": "true positive (TP), false positive (FP), true negative (TN)", "start_pos": 99, "end_pos": 158, "type": "METRIC", "confidence": 0.6910878349752987}, {"text": "false negative (FN)", "start_pos": 163, "end_pos": 182, "type": "METRIC", "confidence": 0.7070750176906586}]}, {"text": "We then used the TP, FP, TN and FN counts to compute the accuracy, precision, recall and F-score enough for us to believe it to be reliable.", "labels": [], "entities": [{"text": "TP", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.9430420398712158}, {"text": "FP", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9282565712928772}, {"text": "FN counts", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9531253576278687}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9997215867042542}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9993001222610474}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9995436072349548}, {"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9985859394073486}]}, {"text": "A fairly liberal view of \"semantic equivalence\" was taken; for example, for our purposes we consider all sentences in to be more-or-less semantically equivalent.", "labels": [], "entities": []}, {"text": "A matching term is defined as a term which has a corresponding entry in the OVVTs.", "labels": [], "entities": [{"text": "OVVTs", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.9168736338615417}]}, {"text": "These metrics are used to compare the relative performance between each of our candidate selection methods.", "labels": [], "entities": []}, {"text": "We describe each of our techniques and the results; we present all the measurements in a single table).", "labels": [], "entities": []}, {"text": "The technqiues described above attempt to create acceptable candidates to shift sentiment.", "labels": [], "entities": []}, {"text": "However, this) describes a successful approach to lexical substitution that combines multiple knowledge sources.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.7222881019115448}]}, {"text": "leaves open the question as to whether the technique has its desired effect: i.e. appropriately shifting sentiment.", "labels": [], "entities": []}, {"text": "We designed an experiment which aims to measure correlation between human judgements of the sentiment shift in our generated candidates, and our system's representation of sentiment shift.", "labels": [], "entities": []}, {"text": "We presented subjects with an original sentence, along with one of the generated candidates.", "labels": [], "entities": []}, {"text": "Our six subjects had no specialised knowledge of the task and were all native English speakers.", "labels": [], "entities": []}, {"text": "Subjects were asked to judge the modified sentence for change in sentiment relative to the original according to the five shift categories described earlier (i.e., major/minor positive/negative/no shift).", "labels": [], "entities": []}, {"text": "In order to avoid bias and to clarify the task, we explained that sentiment should be separated from changes in meaning, or the reader's opinions about the sentences.", "labels": [], "entities": []}, {"text": "Instead, we urged subjects to ask themselves the question: \"Is the author of the second sentence saying what they're saying in a more positive or more negative way, compared to the first sentence?\"", "labels": [], "entities": []}, {"text": "The sentences used were extracted from the BNC at random, using the restrictions listed above.", "labels": [], "entities": [{"text": "BNC", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.9493208527565002}]}, {"text": "We extracted 250 sentences to be used as the originals, each of which was used as input to our sentiment shifting system.", "labels": [], "entities": [{"text": "sentiment shifting", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.9298377633094788}]}, {"text": "For each original sentence, we produced all possible candidates using our best performing candidate selection method, Web 1T Bigrams.", "labels": [], "entities": []}, {"text": "We also limited our generation to changing one term per sentence, as to not produce a combinatorial explosion in the number of candidates generated.", "labels": [], "entities": []}, {"text": "This produced approximately 3000 modified candidates, including several candidates with no sentiment shift.", "labels": [], "entities": []}, {"text": "Upon inspection, we found many generated candidates contained the types of errors described above.", "labels": [], "entities": []}, {"text": "Hence, we manually extracted original and modified sentences until we had a total of 50 originals, and 100 shifted sentences.", "labels": [], "entities": []}, {"text": "In selecting which sentences to keep, we chose ones which sounded the most natural, or had the least amount of semantic change from the original.", "labels": [], "entities": []}, {"text": "Manual selection was performed in order to prevent introducing any bias into judgements when a subject is confronted with a grammatically incorrect or unnatural sentence.", "labels": [], "entities": []}, {"text": "We also aimed fora fairly even distribution of the shifted 95 sentences into the five sentiment shift intervals.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: (Abridged) example of an OVVT", "labels": [], "entities": [{"text": "OVVT", "start_pos": 35, "end_pos": 39, "type": "TASK", "confidence": 0.5355088710784912}]}, {"text": " Table 5: Collated results for all experiments", "labels": [], "entities": []}, {"text": " Table 6: Distribution of classification errors", "labels": [], "entities": [{"text": "Distribution of classification", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.8416891694068909}]}, {"text": " Table 7: Kendall's Tau rank correlation between system  (sys) and human (hi) judgement polarities", "labels": [], "entities": [{"text": "Tau rank correlation", "start_pos": 20, "end_pos": 40, "type": "METRIC", "confidence": 0.7996559143066406}]}]}