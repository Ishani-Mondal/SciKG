{"title": [{"text": "Measuring Conceptual Similarity by Spreading Activation over Wikipedia's Hyperlink Structure", "labels": [], "entities": []}], "abstractContent": [{"text": "Keyword-matching systems based on simple models of semantic relatedness are inadequate at modelling the ambiguities in natural language text, and cannot reliably address the increasingly complex information needs of users.", "labels": [], "entities": []}, {"text": "In this paper we propose novel methods for computing semantic relatedness by spreading activation energy over the hy-perlink structure of Wikipedia.", "labels": [], "entities": [{"text": "computing semantic relatedness", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.5983589986960093}]}, {"text": "We demonstrate that our techniques can approach state-of-the-art performance, while requiring only a fraction of the background data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The volume of information available to users on the World Wide Web is growing at an exponential rate.", "labels": [], "entities": []}, {"text": "Current keyword-matching information retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears).", "labels": [], "entities": [{"text": "keyword-matching information retrieval (IR)", "start_pos": 8, "end_pos": 51, "type": "TASK", "confidence": 0.8114900787671407}]}, {"text": "In recent years, much research attention has therefore been given to semantic techniques of information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.7979617416858673}]}, {"text": "Such systems allow for sophisticated semantic search, however, require the use of a more difficult-to-understand querysyntax (.", "labels": [], "entities": []}, {"text": "Furthermore, these methods require specially encoded (and thus costly) ontologies to describe the particular domain knowledge in which the system operates, and the specific interrelations of concepts within that domain.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the problem of computationally estimating similarity or relatedness between two natural-language documents.", "labels": [], "entities": [{"text": "computationally estimating similarity or relatedness between two natural-language documents", "start_pos": 42, "end_pos": 133, "type": "TASK", "confidence": 0.7126114269097646}]}, {"text": "A novel technique is proposed for computing semantic similarity by spreading activation over the hyperlink structure of Wikipedia, the largest free online encyclopaedia.", "labels": [], "entities": [{"text": "computing semantic similarity", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.7017494936784109}]}, {"text": "New measures for computing similarity between individual concepts (inter-concept similarity, such as \"France\" and \"Great Britain\"), as well as between documents (inter-document similarity) are proposed and tested.", "labels": [], "entities": []}, {"text": "It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM)  and Explicit Semantic Analysis (ESA) () methods respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.6779980659484863}]}, {"text": "Our methods outperform WLM in computing inter-concept similarity, and match ESA for inter-document similarity.", "labels": [], "entities": []}, {"text": "Furthermore, we use the same background data as for WLM, which is less than 10% of the data required for ESA.", "labels": [], "entities": [{"text": "WLM", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8286936283111572}, {"text": "ESA", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.8118601441383362}]}, {"text": "In the following sections we introduce work related to our work and an overview of our approach and the problems that have to be solved.", "labels": [], "entities": []}, {"text": "We then discuss our method in detail and present several experiments to test and compare it against other state-of-the-art methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to compare our method with results reported by and Witten and Milne (2008), we followed the same approach by randomly selecting Algorithm 1 Pseudo code to spread activation depth-first from node vi up to level L p,max , using global decay d, and threshold T , given an adjacency list graph structure G and a weighting scheme W such that 0 < w ij \u2208 W < 1. 50 word-pairs from the WordSimilarity-353 dataset) and correlating our method's scores with the human-assigned scores.", "labels": [], "entities": [{"text": "WordSimilarity-353 dataset", "start_pos": 387, "end_pos": 413, "type": "DATASET", "confidence": 0.987587034702301}]}, {"text": "To reduce the possibility of overestimating the performance of our technique on a sample set that happens to be favourable to our technique, we furthermore implemented a technique of repeated holdout): Given a sample test set of N pairs of words with human-assigned ratings of relatedness, randomly divide this set into k parts of roughly equal size 3 . Hold out one part of the data and iteratively evaluate the performance of the algorithm on the remaining k\u22121 parts until all k parts have been held out once.", "labels": [], "entities": []}, {"text": "Finally, average the algorithm's performance overall k runs into one score resembling the performance for that set of parameters.", "labels": [], "entities": []}, {"text": "Since there are five parameters (spreading strategy, weighting scheme, path length, network decay, and threshold), a grid search was implemented by holding three of the five parameters constant, and evaluating combinations of decay and threshold by stepping over the possible parameter space using some step size.", "labels": [], "entities": []}, {"text": "A coarsegrained grid search was first conducted with step AA-cos 0.70 ILF, Lp,max=3, d=0.5, T=0.1 size of 0.1 over d and a logarithmic scale over T , thus T = {0, 0.1, 0.01, 0.001, ..., 10 \u22129 }.", "labels": [], "entities": [{"text": "AA-cos 0.70 ILF", "start_pos": 58, "end_pos": 73, "type": "METRIC", "confidence": 0.8608168562253317}]}, {"text": "The best values ford and T were then chosen to conduct a finer-grained grid search.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Spreading results by maximum path  length L p,max . Best results in bold.", "labels": [], "entities": [{"text": "Spreading", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8918293118476868}]}, {"text": " Table 3: Spreading results by weighting scheme  w. Best results in bold.", "labels": [], "entities": [{"text": "Spreading", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.6740196347236633}]}]}