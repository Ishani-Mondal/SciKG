{"title": [], "abstractContent": [{"text": "In this work, we propose a semi-supervised extension to a well-known supervised domain adaptation approach (EA) (Daum\u00e9 III, 2007).", "labels": [], "entities": [{"text": "supervised domain adaptation approach (EA)", "start_pos": 69, "end_pos": 111, "type": "TASK", "confidence": 0.7192059925624302}, {"text": "Daum\u00e9 III, 2007)", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.8893025159835816}]}, {"text": "Our proposed approach (EA++) builds on the notion of augmented space (introduced in EA) and harnesses unlabeled data in target domain to ameliorate the transfer of information from source to target.", "labels": [], "entities": []}, {"text": "This semi-supervised approach to domain adaptation is extremely simple to implement, and can be applied as a pre-processing step to any supervised learner.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7578369975090027}]}, {"text": "Experimental results on sequential labeling tasks demonstrate the efficacy of the proposed method.", "labels": [], "entities": [{"text": "sequential labeling tasks", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.8164713382720947}]}], "introductionContent": [{"text": "A domain adaptation approach for sequential labeling tasks in NLP was proposed in.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.7206743657588959}, {"text": "sequential labeling tasks", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.7063392996788025}]}, {"text": "The proposed approach, termed EASYADAPT (EA), augments the source domain feature space using features from labeled data in target domain.", "labels": [], "entities": [{"text": "EASYADAPT (EA)", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.8319629579782486}]}, {"text": "EA is simple, easy to extend and implement as a preprocessing step and most importantly is agnostic of the underlying classifier.", "labels": [], "entities": []}, {"text": "However, EA requires labeled data in the target and hence applies to fully supervised (labeled data in source and target) domain adaptation settings only.", "labels": [], "entities": [{"text": "EA", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.5620349049568176}]}, {"text": "In this paper, we propose a semi-supervised 1 (labeled data in source, and both labeled and unlabeled data in target) approach to leverage unlabeled data for EASYADAPT (which we call EA++) and empirically demonstrate its superior performance over EA as well as few other existing approaches.", "labels": [], "entities": [{"text": "EASYADAPT", "start_pos": 158, "end_pos": 167, "type": "DATASET", "confidence": 0.7605707049369812}]}, {"text": "There exists prior work on supervised domain adaptation (or multi-task learning) that can be related to EASYADAPT.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.6744173566500345}, {"text": "EASYADAPT", "start_pos": 104, "end_pos": 113, "type": "DATASET", "confidence": 0.6513853073120117}]}, {"text": "An algorithm for multitask learning using shared parameters was proposed () for multi-task regularization where each task parameter was represented as sum of a mean parameter (that stays same for all tasks) and its deviation from this mean.", "labels": [], "entities": [{"text": "multi-task regularization", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.7417380213737488}]}, {"text": "SVM was used as the base classifier and the algorithm was formulated in the standard SVM dual optimization setting.", "labels": [], "entities": [{"text": "SVM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9033648371696472}, {"text": "SVM dual optimization", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.6849313577016195}]}, {"text": "Subsequently, this framework () was extended () to online multidomain setting.", "labels": [], "entities": []}, {"text": "Prior work on semi-supervised approaches to domain adaptation also exists in literature.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7635341584682465}]}, {"text": "Extraction of specific features from the available dataset was proposed) to facilitate the task of domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7656138837337494}]}, {"text": "Co-adaptation, a combination of co-training and domain adaptation, can also be considered as a semisupervised approach to domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.713622123003006}]}, {"text": "A semi-supervised EM algorithm for domain adaptation was proposed in . Similar to graph based semi-supervised approaches, a label propagation method was proposed () to facilitate domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7756845057010651}, {"text": "label propagation", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7121010571718216}, {"text": "domain adaptation", "start_pos": 179, "end_pos": 196, "type": "TASK", "confidence": 0.7738083004951477}]}, {"text": "The recently proposed Domain Adaptation Machine (DAM) () is a semi-supervised extension of SVMs for domain adaptation and presents extensive empirical results.", "labels": [], "entities": [{"text": "Domain Adaptation Machine (DAM)", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.7338891426722208}, {"text": "domain adaptation", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.7296731770038605}]}, {"text": "However, in almost all of the above cases, the proposed methods either use specifics of the datasets or are customized for some particular base classifier and hence it is not clear how the proposed methods can be extended to other existing classifiers.", "labels": [], "entities": []}, {"text": "EA, on the other hand, is remarkably general in the sense that it can be used as a pre-processing step in conjunction with any base classifier.", "labels": [], "entities": [{"text": "EA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.6338112950325012}]}, {"text": "However, one of the prime limitations of EA is its incapability to leverage unlabeled data.", "labels": [], "entities": [{"text": "EA", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.7319532632827759}]}, {"text": "Given its simplicity and generality, it would be interesting to extend EA to semi-supervised settings.", "labels": [], "entities": [{"text": "EA", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.8452827334403992}]}, {"text": "In this paper we propose EA++, a co-regularization based semi-supervised extension to EA.", "labels": [], "entities": []}, {"text": "We present our approach and results fora single pair of source and target domain.", "labels": [], "entities": []}, {"text": "However, we note that EA++ can also be extended to multiple source settings.", "labels": [], "entities": []}, {"text": "If we have k sources and a single target domain then we can introduce a co-regularizer for each source-target pair.", "labels": [], "entities": []}, {"text": "Due to space constraints, we defer details to a full version.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we demonstrate the empirical performance of EA augmented with unlabeled data.", "labels": [], "entities": [{"text": "EA", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.8941532969474792}]}], "tableCaptions": [{"text": " Table 1: Summary of Datasets. The columns de- note task, domain, size of training, development  and test data sets, and the number of unique fea- tures in the training data.", "labels": [], "entities": []}]}