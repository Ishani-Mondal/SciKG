{"title": [{"text": "Anveshan: A Framework for Analysis of Multiple Annotators' Labeling Behavior", "labels": [], "entities": [{"text": "Analysis of Multiple Annotators' Labeling Behavior", "start_pos": 26, "end_pos": 76, "type": "TASK", "confidence": 0.6506921152273814}]}], "abstractContent": [{"text": "Manual annotation of natural language to capture linguistic information is essential for NLP tasks involving supervised machine learning of semantic knowledge.", "labels": [], "entities": []}, {"text": "Judgements of meaning can be more or less subjective, in which case instead of a single correct label, the labels assigned might vary among annotators based on the annotators' knowledge, age, gender, intuitions , background, and soon.", "labels": [], "entities": [{"text": "Judgements of meaning", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7831881642341614}]}, {"text": "We introduce a framework \"Anveshan,\" where we investigate annotator behavior to find out-liers, cluster annotators by behavior, and identify confusable labels.", "labels": [], "entities": []}, {"text": "We also investigate the effectiveness of using trained annotators versus a larger number of untrained annotators on a word sense annotation task.", "labels": [], "entities": [{"text": "word sense annotation task", "start_pos": 118, "end_pos": 144, "type": "TASK", "confidence": 0.7484167516231537}]}, {"text": "The annotation data comes from a word sense disambiguation task for pol-ysemous words, annotated by both trained annotators and untrained annotators from Amazon's Mechanical turk.", "labels": [], "entities": [{"text": "word sense disambiguation task", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.7192386239767075}, {"text": "Amazon's Mechanical turk", "start_pos": 154, "end_pos": 178, "type": "DATASET", "confidence": 0.7190379947423935}]}, {"text": "Our results show that Anveshan is effective in uncovering patterns in annotator behavior, and we also show that trained annotators are superior to a larger number of untrained annotators for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Manual annotation of language data in order to capture linguistic knowledge has become increasingly important for semantic and pragmatic annotation tasks.", "labels": [], "entities": []}, {"text": "Avery shortlist of a few such tasks illustrates the range of types of annotation, in varying stages of development: predicate argument structure), dialogue acts (), discourse structure (), opinion), emotion).", "labels": [], "entities": []}, {"text": "The number of efforts to create corpus resources that include manual annotations has also been growing.", "labels": [], "entities": []}, {"text": "A common approach in assessing the resulting manual annotations is to report a single quantitative measure reflecting the quality of the annotations, either a summary statistic such as percent agreement, or an agreement coefficient from the family of metrics that include Krippendorff's alpha) and Cohen's kappa.", "labels": [], "entities": []}, {"text": "We present some new assessment methods to use in combination with an agreement coefficient for understanding annotator behavior when there are multiple annotators and many annotation values.", "labels": [], "entities": []}, {"text": "Anveshan (Annotation Variance Estimation) 1 is a suite of procedures for analyzing patterns of agreement and disagreement among annotators, as well as the distributions of annotation values across annotators.", "labels": [], "entities": []}, {"text": "Anveshan thus makes it possible to explore annotator behavior in more detail.", "labels": [], "entities": []}, {"text": "Currently, it includes three types of analysis: interannotator agreement (IA) among all subsets of annotators, leverage of annotation values for outlier detection, and metrics for comparing annotators' distributions of annotation values (e.g.,.", "labels": [], "entities": [{"text": "interannotator agreement (IA)", "start_pos": 48, "end_pos": 77, "type": "METRIC", "confidence": 0.894735586643219}, {"text": "outlier detection", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.7530509531497955}]}, {"text": "As an illustration of the utility of Anveshan, we compare two groups of annotators on the same annotation word sense annotation tasks: a half dozen trained annotators and fourteen Mechanical Turkers.", "labels": [], "entities": []}, {"text": "Previous work has argued that it can be cost effective to collect multiple labels from untrained labelers at a low cost per label, and to combine the multiple labels through a voting method, rather than to collect single labels from highly trained la-belers (.", "labels": [], "entities": []}, {"text": "The tasks included in, for example, include word sense annotation; in contrast to our case, where the average number of senses per word is 9.5, the one word sense annotation task had three senses.", "labels": [], "entities": [{"text": "word sense annotation", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.7049248218536377}]}, {"text": "We find that the same half dozen trained annotators can agree well or not on sense labels for polysemous words.", "labels": [], "entities": []}, {"text": "When they agree less well, we find that it is possible to distinguish between problems in the labels (e.g., confusable senses) and systematic differences of interpretation among annotators.", "labels": [], "entities": []}, {"text": "When we use twice the number of Mechanical Turkers as trained annotators for three of our ten polysemous words, we find inconsistent results.", "labels": [], "entities": []}, {"text": "The next section of the paper presents the motivation for Anveshan and its relevance to the word sense annotation task, followed by a section on related work.", "labels": [], "entities": [{"text": "word sense annotation task", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.8152847290039062}]}, {"text": "The word sense annotation data is given in section 5.", "labels": [], "entities": []}, {"text": "Anveshan is described in the subsequent section, followed by the results of its application to the two data sets.", "labels": [], "entities": [{"text": "Anveshan", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.5973117351531982}]}, {"text": "We discuss the comparison of trained annotators and Mechanical Turkers, as well as differences among words, in section 7.", "labels": [], "entities": []}, {"text": "Section 7 concludes with a short recap of Anveshan in general, and its application to word sense annotations in particular.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Interannotator agreement on ten poly- semous words: three adjectives, three nouns and  four verbs among trained annotators", "labels": [], "entities": []}, {"text": " Table 2: Interannotator agreement on adjectives  among Mechanical Turk annotators", "labels": [], "entities": []}, {"text": " Table 3: Increase in IA score by dropping annota- tors (TA Data)", "labels": [], "entities": [{"text": "IA score", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.7073981463909149}, {"text": "annota- tors", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.9284923473993937}, {"text": "TA", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.8256492018699646}]}]}