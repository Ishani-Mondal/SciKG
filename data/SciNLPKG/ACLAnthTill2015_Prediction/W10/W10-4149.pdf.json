{"title": [{"text": "Treebank Conversion based Self-training Strategy for Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.9491450190544128}]}], "abstractContent": [{"text": "In this paper, we propose a novel self-training strategy for parsing which is based on Treebank conversion (SSPTC).", "labels": [], "entities": [{"text": "parsing", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.9800368547439575}]}, {"text": "In SSPTC, we make full use of the strong points of Treebank conversion and self-training, and offset their weaknesses with each other.", "labels": [], "entities": [{"text": "Treebank conversion", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.6951870918273926}]}, {"text": "To provide good parse selection strategies which are needed in self-training, we score the automatically generated parse trees with parse trees in source Treebank as a reference.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.95931077003479}, {"text": "source Treebank", "start_pos": 147, "end_pos": 162, "type": "DATASET", "confidence": 0.7589571177959442}]}, {"text": "To maintain the constituency between source Treebank and conversion Treebank which is needed in Treebank conversion, we get the conversion trees with the help of self-training.", "labels": [], "entities": [{"text": "conversion Treebank", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.6319273114204407}]}, {"text": "In our experiments, SSPTC strategy is utilized to parse Tsinghua Chinese Treebank with the help of Penn Chinese Treebank.", "labels": [], "entities": [{"text": "parse", "start_pos": 50, "end_pos": 55, "type": "TASK", "confidence": 0.9420864582061768}, {"text": "Tsinghua Chinese Treebank", "start_pos": 56, "end_pos": 81, "type": "DATASET", "confidence": 0.776896079381307}, {"text": "Penn Chinese Treebank", "start_pos": 99, "end_pos": 120, "type": "DATASET", "confidence": 0.981445829073588}]}, {"text": "The results significantly outperform the baseline parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntax parsing is one of the most fundamental tasks in natural language processing (NLP) and has attracted extensive attention during the past few decades.", "labels": [], "entities": [{"text": "Syntax parsing", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9190311431884766}, {"text": "natural language processing (NLP)", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.8056932985782623}]}, {"text": "In statistical area, according to the type of data used in training stage, the parsing approaches can be classified into three categories: supervised, semi-supervised and unsupervised.", "labels": [], "entities": [{"text": "parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.9713061451911926}]}, {"text": "In supervised parsing approach, a high-performance parser can be built when given sufficient labeled data).", "labels": [], "entities": []}, {"text": "The semi-supervised approach utilizes some labeled data to annotate unlabeled data, then uses the annotated data to improve original model, e.g., self-training () and co-training (.", "labels": [], "entities": []}, {"text": "In unsupervised parsing, the labeled data was not employed and all annotations and grammars are discovered automatically from unlabeled data.", "labels": [], "entities": []}, {"text": "State-of-the-art supervised parsers) require large amounts of manually annotated training data, such as the Penn Treebank (, to achieve high performance.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 108, "end_pos": 121, "type": "DATASET", "confidence": 0.9935263097286224}]}, {"text": "However, it is quite costly and time-consuming to create high quality labeled data.", "labels": [], "entities": []}, {"text": "So it becomes a key bottleneck for supervised approach to acquire sufficient labeled training data.", "labels": [], "entities": []}, {"text": "Self-training is an effective strategy to overcome this shortage.", "labels": [], "entities": []}, {"text": "It tries to enlarge the training set with automatically annotated unlabeled data and trains a parser with the enlarged training set.", "labels": [], "entities": []}, {"text": "During the last few decades, many Treebanks annotated with different grammar formalisms are released).", "labels": [], "entities": []}, {"text": "Although they are annotated with different schemes, they have some linguistic consistency in some extent.", "labels": [], "entities": []}, {"text": "Intuitively, we can convert Treebank annotated with one grammar formalisms into another Treebank annotated with grammar formalism that we are interested in.", "labels": [], "entities": []}, {"text": "For simplicity, we call the first source Treebank, and the second target Treebank.", "labels": [], "entities": []}, {"text": "And we call this strategy as Treebank conversion.", "labels": [], "entities": [{"text": "Treebank conversion", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.8214149177074432}]}, {"text": "Although both self-training and Treebank conversion can overcome the limitation of labeled data shortage for supervised parsing in some extent, they all have drawbacks.", "labels": [], "entities": [{"text": "Treebank conversion", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7324205636978149}]}, {"text": "For selftraining, the quality of automatically annotated unlabeled data will affect the performance of semi-supervised parsers highly.", "labels": [], "entities": []}, {"text": "For example, shows that when the parser-best list is used for self-training, the parsing performance isn't improved, but after using reranker-best list, the retrained parser achieves an absolute 1.1% improvement.", "labels": [], "entities": []}, {"text": "For Treebank conversion, different types among Treebanks make the converting procedure very complicated, and it is very hard to get a conversion Treebank constituent with target Treebank.", "labels": [], "entities": [{"text": "Treebank conversion", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.786270022392273}]}, {"text": "To overcome the limitations mentioned above, we propose a Treebank conversion based selftraining strategy for parsing, which tries to combine self-training and Treebank conversion together.", "labels": [], "entities": [{"text": "parsing", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.9879361391067505}]}, {"text": "Remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce some related work.", "labels": [], "entities": []}, {"text": "Section 3 describes details of our SSPTC strategy.", "labels": [], "entities": []}, {"text": "In Section 4, we propose ahead finding method for Task21 in CLP2010.", "labels": [], "entities": [{"text": "ahead finding", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.64755579829216}, {"text": "CLP2010", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.922040581703186}]}, {"text": "The experiments and analysis is given in Section 5.", "labels": [], "entities": []}, {"text": "The last section draws conclusions and describes the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to get a good final accuracy, we choose BerkeleyParser 3 , which is a state-of-the-art unlexicalized parser, and train a model with the training set as our baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9860765933990479}]}, {"text": "The F1 score of validating set parsed by baseline parser is 85.72%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9838458895683289}]}, {"text": "In the following of this subsection, we try to combine our strategies into the baseline parser and evaluate the effectiveness.", "labels": [], "entities": []}, {"text": "Because mult-time iterations can't improve parsing performance tremendously but cost much time during our experiments, we take Iter=1 here.", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9689825773239136}, {"text": "Iter", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9894263744354248}]}, {"text": "To evaluate the corpus weighting strategy, we take sentences (ignore the tree structure) in PCTB as unlabeled data, and train a parser with self-training strategy.", "labels": [], "entities": [{"text": "PCTB", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.929334282875061}]}, {"text": "In this subsection we evaluate our parse selection strategies with the help of PCTB.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.9298517107963562}, {"text": "PCTB", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.9438211917877197}]}, {"text": "According to Algorithm 1, we train an initial parser with training set and development set.", "labels": [], "entities": []}, {"text": "Then we generate 50-best parses list with the initial parser for each sentence in PCTB, and select a higher-score parse for each sentence through our parse selection strategies to build a conversion Treebank.", "labels": [], "entities": [{"text": "PCTB", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.9372072815895081}, {"text": "conversion Treebank", "start_pos": 188, "end_pos": 207, "type": "DATASET", "confidence": 0.6889974176883698}]}, {"text": "Finally, we retrain a parser with training set and the conversion Treebank with the help of corpus weighting strategy.", "labels": [], "entities": [{"text": "conversion Treebank", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.5228501260280609}]}, {"text": "Because the highest F1 score is at the point \ud97b\udf59 =0.3 in, we choose \ud97b\udf59 =0.3 to evaluating LAF strategy.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.985398143529892}]}, {"text": "shows F1 scores on validating set using LAF.", "labels": [], "entities": [{"text": "F1", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.9995013475418091}]}, {"text": "The highest F1 score is 87.44% at the point \ud97b\udf59 =6, and it gets 1.72 percentage points improvement over baseline.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9891595542430878}]}, {"text": "Comparing with UAF, LAF gets 0.52 more improvement.", "labels": [], "entities": [{"text": "UAF", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.9040626287460327}, {"text": "LAF", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.81505286693573}]}, {"text": "So we can conclude that the parse selection strategy with LAF is much more effective.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.9697607159614563}, {"text": "LAF", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.5588588118553162}]}, {"text": "some configurations which achieve higher performance on the development set(more details have been shown in section 5).", "labels": [], "entities": []}, {"text": "The final parameters and training data of our systems are shown in 4 . We also make use of the approach explained in section 4 for the head finding procedure.", "labels": [], "entities": [{"text": "head finding", "start_pos": 135, "end_pos": 147, "type": "TASK", "confidence": 0.9877226650714874}]}, {"text": "The parsing results of our systems on the test set can be found on the official ranking report.", "labels": [], "entities": []}, {"text": "Our systems training with SSPTC strategy bring us an amazing performance which outperforms other systems in both the two sub-tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Feature Templates for Head Finding", "labels": [], "entities": [{"text": "Head Finding", "start_pos": 32, "end_pos": 44, "type": "TASK", "confidence": 0.9869923293590546}]}, {"text": " Table 2: F1 scores of various strategies", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.962843656539917}]}]}