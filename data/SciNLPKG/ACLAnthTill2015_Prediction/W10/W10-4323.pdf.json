{"title": [{"text": "Parameter estimation for agenda-based user simulation", "labels": [], "entities": [{"text": "Parameter estimation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7878937721252441}]}], "abstractContent": [{"text": "This paper presents an agenda-based user simulator which has been extended to be trainable on real data with the aim of more closely modelling the complex rational behaviour exhibited by real users.", "labels": [], "entities": []}, {"text": "The train-able part is formed by a set of random decision points that maybe encountered during the process of receiving a system act and responding with a user act.", "labels": [], "entities": []}, {"text": "A sample-based method is presented for using real user data to estimate the parameters that control these decisions.", "labels": [], "entities": []}, {"text": "Evaluation results are given both in terms of statistics of generated user behaviour and the quality of policies trained with different simulators.", "labels": [], "entities": []}, {"text": "Compared to a handcrafted simulator, the trained system provides a much better fit to corpus data and evaluations suggest that this better fit should result in improved dialogue performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "In spoken dialogue systems research, modelling dialogue as a (Partially Observable) Markov Decision Process ((PO)MDP) and using reinforcement learning techniques for optimising dialogue policies has proven to bean effective method for developing robust systems (;).", "labels": [], "entities": []}, {"text": "However, since this kind of optimisation requires a simulated user to generate a sufficiently large number of interactions to learn from, this effectiveness depends largely on the quality of such a user simulator.", "labels": [], "entities": []}, {"text": "An important requirement fora simulator is for it to be realistic, i.e., it should generate behaviour that is similar to that of real users.", "labels": [], "entities": []}, {"text": "Trained policies are then more likely to perform better on real users, and evaluation results on simulated data are more likely to predict results on real data more accurately.", "labels": [], "entities": []}, {"text": "This is one of the reasons why learning user simulation models from data on real user behaviour has become an important direction of research ().", "labels": [], "entities": []}, {"text": "However, the data driven user models developed so far lack the complexity required for training high quality policies in task domains where user behaviour is relatively complex.", "labels": [], "entities": []}, {"text": "Handcrafted models are still the most effective in those cases.", "labels": [], "entities": []}, {"text": "This paper presents an agenda-based user simulator which is handcrafted fora large part, but additionally can be trained with data from real users (Section 2).", "labels": [], "entities": []}, {"text": "As a result, it generates behaviour that better reflects the statistics of real user behaviour, whilst preserving the complexity and rationality required to effectively train dialogue management policies.", "labels": [], "entities": []}, {"text": "The trainable part is formed by a set of random decision points, which, depending on the context, mayor may not be encountered during the process of receiving a system act and deciding on a response act.", "labels": [], "entities": []}, {"text": "If such a point is encountered, the simulator makes a random decision between a number of options which may directly or indirectly influence the resulting output.", "labels": [], "entities": []}, {"text": "The options for each random decision point are reasonable in the context in which it is encountered, but a uniform distribution of outcomes might not reflect real user behaviour.", "labels": [], "entities": []}, {"text": "We will describe a sample-based method for estimating the parameters that define the probabilities for each possible decision, using data on real users from a corpus of human-machine dialogues (Section 3).", "labels": [], "entities": []}, {"text": "Evaluation results will be presented both in terms of statistics on generated user behaviour and the quality of dialogue policies trained with different user simulations (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "The parameter estimation technique for training the user simulator was evaluated in two different ways.", "labels": [], "entities": []}, {"text": "The first evaluation involved comparing the statistics of simulated and real user behaviour.", "labels": [], "entities": []}, {"text": "The second evaluation involved comparing dialogue manager policies trained with different simulators.", "labels": [], "entities": []}, {"text": "Although the corpus-based evaluation results give a useful indication of how realistic the behaviour generated by a simulator is, what really should be evaluated is the dialogue management policy that is trained using that simulator.", "labels": [], "entities": []}, {"text": "Therefore, different parameter sets for the simulator were used to train and evaluate different policies for the Hidden Information State (HIS) dialogue manager (.", "labels": [], "entities": []}, {"text": "Four different policies were trained: one policy using handcrafted simulation parameters (POL-HDC); two policies using simulation parameters estimated (using the sequence-based matching approach) from two data sets that were obtained by randomly splitting the data into two parts of 358 dialogues each (POL-TRA1 and POL-TRA2); and finally, a policy using a deterministic simulator (POL-DET) constructed from the trained parameters as discussed in Section 4.2.2.", "labels": [], "entities": []}, {"text": "The policies were then each evaluated on the simulator using the four parameter settings at different semantic error rates.", "labels": [], "entities": []}, {"text": "The performance of a policy is measured in terms of a reward that is given for each dialogue, i.e. a reward of 20 fora successful dialogue, minus the number of turns.", "labels": [], "entities": []}, {"text": "A dialogue is considered successful if the system has offered avenue matching the predefined user goal constraints and has given the correct values of all requested slots for this venue.", "labels": [], "entities": []}, {"text": "During the policy optimisation, in which a reinforcement learning algorithm tries to optimise the expected long term reward, this dialogue scoring regime was also used.", "labels": [], "entities": []}, {"text": "In, and 4, evaluation results are given resulting from running 3000 dialogues at each of 11 different semantic error rates.", "labels": [], "entities": []}, {"text": "The curves show average rewards with 95% confidence intervals.", "labels": [], "entities": []}, {"text": "The error rate is controlled by a hand- crafted error model that converts the user act generated by the simulator into an n-best list of dialogue act hypotheses.", "labels": [], "entities": [{"text": "error rate", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9523032307624817}]}, {"text": "The policy that was trained using the handcrafted simulator (POL-HDC) outperforms the other policies when evaluated on that same simulator (see, and both policies trained using the trained simulators (POL-TRA1 and POL-TRA2) outperform the other policies when evaluated on either trained simulator (see for the evaluation on UM-TRA1; the evaluation on UM-TRA2 is very similar and therefore omitted).", "labels": [], "entities": [{"text": "UM-TRA1", "start_pos": 324, "end_pos": 331, "type": "DATASET", "confidence": 0.9016137719154358}, {"text": "UM-TRA2", "start_pos": 351, "end_pos": 358, "type": "DATASET", "confidence": 0.9099747538566589}]}, {"text": "There is little difference in performance between policies POL-TRA1 and POL-TRA2, which can be explained by the fact that the two trained parameter settings are quite similar, in contrast to the handcrafted parameters.", "labels": [], "entities": []}, {"text": "The policy that was trained on the deterministic parameters (POL-DET) is competitive with the other policies when evaluated on UM-DET (see), but performs significantly worse on the other parameter settings which generate the variation in behaviour that the dialogue manager did not encounter during training of POL-DET.", "labels": [], "entities": [{"text": "UM-DET", "start_pos": 127, "end_pos": 133, "type": "DATASET", "confidence": 0.9091519713401794}]}, {"text": "In addition to comparing the policies when evaluated on each simulator separately, another comparison was made in terms of the average performance across all simulators.", "labels": [], "entities": []}, {"text": "For each policy and each simulator, we first computed the difference between the policy's performance and the 'maximum' performance on that simulator as achieved by the policy that was also trained on that simulator, and then averaged overall simulators.", "labels": [], "entities": []}, {"text": "To avoid biased results, only one of the trained simulators was included.", "labels": [], "entities": []}, {"text": "The results in show that the POL-TRA2 policy is more robust than POL-DET, and has similar robustness as POL-HDC.", "labels": [], "entities": []}, {"text": "Similar results are obtained when including UM-TRA1 only.", "labels": [], "entities": []}, {"text": "Given that the results of Section 4.2 show that the dialogues generated by the trained simulator more closely match real corpus data, and given that the above simulation results show that the POL-TRA policies are at least as robust as the other policies, it seems likely that policies trained using the trained user simulator will show improved performance when evaluated on real users.", "labels": [], "entities": []}, {"text": "However, this claim can only be properly demonstrated in areal user evaluation of the dialogue system containing different dialogue management policies.", "labels": [], "entities": []}, {"text": "Such a user trial would also be able to confirm whether the results from evaluations on the trained simulator can more accurately predict the actual performance expected with real users.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of the sample-based user simulator evaluation on the Mar'09 training  corpus (the corpus coverage was 59% for the turn-based and 33% for the sequence-based  matching approach).", "labels": [], "entities": [{"text": "Mar'09 training  corpus", "start_pos": 71, "end_pos": 94, "type": "DATASET", "confidence": 0.9022659262021383}]}, {"text": " Table 2: Results of the sample-based user simulator evaluation on the Mar'09 test corpus  (corpus coverage 59% for the turn-based, and 36% for sequence-based matching).", "labels": [], "entities": [{"text": "Mar'09 test corpus", "start_pos": 71, "end_pos": 89, "type": "DATASET", "confidence": 0.9731552402178446}]}]}