{"title": [{"text": "The GREC Challenges 2010: Overview and Evaluation Results", "labels": [], "entities": [{"text": "GREC Challenges 2010", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7244521776835123}]}], "abstractContent": [{"text": "There were three GREC Tasks at Generation Challenges 2010: GREC-NER required participating systems to identify all people references in texts; for GREC-NEG, systems selected coreference chains for all people entities in texts; and GREC-Full combined the NER and NEG tasks, i.e. systems identified and, if appropriate, replaced references to people in texts.", "labels": [], "entities": [{"text": "GREC-NER", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.7622674107551575}, {"text": "GREC-Full", "start_pos": 231, "end_pos": 240, "type": "DATASET", "confidence": 0.821880578994751}]}, {"text": "Five teams submitted 10 systems in total, and we additionally created baseline systems for each task.", "labels": [], "entities": []}, {"text": "Systems were evaluated automatically using a range of intrinsic met-rics.", "labels": [], "entities": []}, {"text": "In addition, systems were assessed by human judges using preference strength judgements.", "labels": [], "entities": []}, {"text": "This report presents the evaluation results, along with descriptions of the three GREC tasks, the evaluation methods , and the participating systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Until recently, referring expression generation (REG) research focused on the task of selecting the semantic content of one-off mentions of listenerfamiliar discourse entities.", "labels": [], "entities": [{"text": "referring expression generation (REG)", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.8586960136890411}, {"text": "selecting the semantic content of one-off mentions of listenerfamiliar discourse entities", "start_pos": 86, "end_pos": 175, "type": "TASK", "confidence": 0.5666263970461759}]}, {"text": "In the GREC research programme we have been interested in REG as (i) grounded within discourse context, (ii) embedded within an application context, and (iii) informed by naturally occurring data.", "labels": [], "entities": [{"text": "GREC research programme", "start_pos": 7, "end_pos": 30, "type": "DATASET", "confidence": 0.8984570105870565}, {"text": "REG", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9736571311950684}]}, {"text": "In general terms, the GREC tasks are about how to select appropriate references to an entity in the context of apiece of discourse longer than a sentence.", "labels": [], "entities": [{"text": "GREC", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.7015455365180969}]}, {"text": "In GREC'10, there were three subtasks: identification of references to people in free text (GREC-NER); selection of references to people in text (GREC-NEG); and regeneration of references to people in text (GREC-Full) which can bethought of as combining the NER and NEG tasks.", "labels": [], "entities": [{"text": "GREC'10", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.8866837620735168}, {"text": "identification of references to people in free text", "start_pos": 39, "end_pos": 90, "type": "TASK", "confidence": 0.8417692556977272}]}, {"text": "The immediate motivating application context for the GREC Tasks is the improvement of referential clarity and coherence in extractive summaries and multiply edited texts (such as Wikipedia articles) by regenerating referring expressions contained in them.", "labels": [], "entities": []}, {"text": "The motivating theoretical interest for the GREC Tasks is to discover what kind of information is useful for making choices between different kinds of referring expressions in context.", "labels": [], "entities": []}, {"text": "The tasks used the GREC-People corpus which consists of 1,100 Wikipedia texts about people within which we have annotated all references to people.", "labels": [], "entities": [{"text": "GREC-People corpus", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.8933190107345581}]}, {"text": "Five teams participated in the GREC'10 tasks (see), submitting 10 systems in total.", "labels": [], "entities": [{"text": "GREC'10 tasks", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.47244715690612793}]}, {"text": "Two of these were created by combining the NER system of one of the teams with the NEG systems of two different teams, producing two 'combined' systems for the Full Task.", "labels": [], "entities": []}, {"text": "We also used the corpus texts themselves as 'system' outputs, and created baseline systems for all three tasks.", "labels": [], "entities": []}, {"text": "We evaluated systems using a range of intrinsic automatically computed and human-assessed evaluation methods.", "labels": [], "entities": []}, {"text": "This report describes the data (Section 2) and evaluation methods (Section 3) used in the three GREC'10 tasks, and then presents task definition, participating systems, evaluation methods, and evaluation results for each of the three tasks separately (Sections 4-6).", "labels": [], "entities": []}], "datasetContent": [{"text": "REG08-Type Precision is defined as the proportion of REFEXs selected by a participating system which match the reference REFEXs.", "labels": [], "entities": [{"text": "REG08-Type Precision", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.3970698416233063}]}, {"text": "REG08-Type Recall is defined as the proportion of reference REFEXs for which a participating system has produced a match.", "labels": [], "entities": [{"text": "REG08-Type Recall", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.4443197101354599}]}, {"text": "String Accuracy is defined as the proportion of word strings selected by a participating system that match those in the reference texts.", "labels": [], "entities": []}, {"text": "This was computed on complete, 'flattened' word strings contained in the outermost REFEX i.e. embedded REFEX word strings were not considered separately.", "labels": [], "entities": []}, {"text": "We also computed BLEU-3, NIST, string-edit distance and length-normalised string-edit distance, all on word strings defined as for String Accuracy.", "labels": [], "entities": [{"text": "BLEU-3", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9969993829727173}, {"text": "NIST", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.7885259389877319}, {"text": "length-normalised string-edit distance", "start_pos": 56, "end_pos": 94, "type": "METRIC", "confidence": 0.8704400857289633}]}, {"text": "BLEU and NIST are designed for multiple output versions, and for the string-edit metrics we computed the mean of means over the three textlevel scores (computed against the three versions of a text).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.953423798084259}, {"text": "NIST", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.9273927807807922}]}, {"text": "To measure accuracy in the NER task, we applied three commonly used performance measures for coreference resolution: MUC-6 (, CEAF, and B-CUBED (Bagga and Baldwin, 1998).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9981650710105896}, {"text": "NER task", "start_pos": 27, "end_pos": 35, "type": "TASK", "confidence": 0.9287832677364349}, {"text": "coreference resolution", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.9629875421524048}, {"text": "MUC-6", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.6050434708595276}, {"text": "CEAF", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.8139923810958862}, {"text": "B-CUBED", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.9847273826599121}]}, {"text": "We designed the human-assessed intrinsic evaluation as a preference-judgement test where subjects expressed their preference, in terms of two criteria, for either the original Wikipedia text or the version of it with system-generated referring expressions in it.", "labels": [], "entities": []}, {"text": "For the GREC-NEG systems, the intrinsic human evaluation involved system outputs for 30 randomly selected items from the test set.", "labels": [], "entities": []}, {"text": "We used a Repeated Latin Squares design which ensures that each subject sees the same number of outputs from each system and for each test set item.", "labels": [], "entities": []}, {"text": "There were three 10 \u00d7 10 squares, and a total of 600 individual judgements in this evaluation (60 per system: 2 criteria \u00d7 3 articles \u00d7 10 evaluators).", "labels": [], "entities": []}, {"text": "We recruited 10 native speakers of English from among students currently completing a linguistics-related degree at Kings College London and University College London.", "labels": [], "entities": []}, {"text": "For the GREC-Full systems, we used 21 randomly selected test set items, a design analogous to that for the GREC-NEG experiment, and 7 evaluators from the same cohort.", "labels": [], "entities": []}, {"text": "This experiment had three 7 \u00d7 7 squares, and 294 individual judgements.", "labels": [], "entities": []}, {"text": "Following detailed instructions, subjects did two practice examples, followed by the texts to be evaluated, in random order.", "labels": [], "entities": []}, {"text": "Subjects carried out the evaluation over the internet, at a time and place of their choosing.", "labels": [], "entities": []}, {"text": "They were allowed to interrupt and resume the experiment (though discouraged from doing so).", "labels": [], "entities": []}, {"text": "shows what subjects saw during the evaluation of an individual text pair.", "labels": [], "entities": []}, {"text": "The place (left/right) of the original Wikipedia article was randomly determined for each individual evaluation of a text pair.", "labels": [], "entities": []}, {"text": "People references are highlighted in yellow/orange, those that are identical in both texts are yellow, those that are different are orange (in the GREC-Full version, there were only yellow highlights).", "labels": [], "entities": [{"text": "GREC-Full version", "start_pos": 147, "end_pos": 164, "type": "DATASET", "confidence": 0.8732323050498962}]}, {"text": "The evaluator's task is to express their preference in terms of each quality criterion by moving the slider pointers.", "labels": [], "entities": []}, {"text": "Moving the slider to the left means expressing a preference for  It was not evident to the evaluators that sliders were associated with numerical values.", "labels": [], "entities": []}, {"text": "Slider pointers started out in the middle of the scale (no preference).", "labels": [], "entities": [{"text": "Slider pointers", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.762305498123169}]}, {"text": "The values associated with the points on the slider ranged from -10.0 to +10.0.", "labels": [], "entities": []}, {"text": "Participants computed evaluation scores on the development set, using the geval code provided by us which computes Word String Accuracy,   the JU system.", "labels": [], "entities": []}, {"text": "As aside effect, the resulting variation led to fewer significant differences between systems being found in the results than would have been the case otherwise.", "labels": [], "entities": []}, {"text": "We carried out univariate ANOVAs with System as the fixed factor, and REG08-Type Recall as the dependent variable in one ANOVA, and REG08-Type Precision in the other.", "labels": [], "entities": [{"text": "ANOVAs", "start_pos": 26, "end_pos": 32, "type": "TASK", "confidence": 0.8627445697784424}, {"text": "REG08-Type Recall", "start_pos": 70, "end_pos": 87, "type": "METRIC", "confidence": 0.9086454212665558}, {"text": "REG08-Type Precision", "start_pos": 132, "end_pos": 152, "type": "METRIC", "confidence": 0.7463262975215912}]}, {"text": "The F-ratio for Recall was F (9,990) = 13.253, p < 0.001.", "labels": [], "entities": [{"text": "F-ratio", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.999437153339386}, {"text": "Recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.8663201332092285}, {"text": "F (9,990)", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9154521524906158}]}, {"text": "The F-ratio for Precision was F (9,990) = 12.670, p < 0.001.", "labels": [], "entities": [{"text": "F-ratio", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9995056390762329}, {"text": "Precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.598639965057373}, {"text": "F", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.9991624355316162}]}, {"text": "The columns containing single capital letters in show the homogeneous subsets of systems as determined by a post-hoc Tukey HSD analysis.", "labels": [], "entities": [{"text": "Tukey HSD analysis", "start_pos": 117, "end_pos": 135, "type": "DATASET", "confidence": 0.7484176754951477}]}, {"text": "Systems whose scores are not significantly different (at the .05 level) share a letter.", "labels": [], "entities": []}, {"text": "shows analogous results computed against Test Set NEG-b (which has three versions of each text).", "labels": [], "entities": [{"text": "Test Set NEG-b", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.7642265955607096}]}, {"text": "includes results for the corpus texts, also computed against the three versions of each text in test set GREC-NEG-b.", "labels": [], "entities": [{"text": "GREC-NEG-b", "start_pos": 105, "end_pos": 115, "type": "DATASET", "confidence": 0.7690998315811157}]}, {"text": "We performed univariate ANOVAs with System as the fixed factor, and Recall as the dependent variable in one, and Precision in the other.", "labels": [], "entities": [{"text": "ANOVAs", "start_pos": 24, "end_pos": 30, "type": "TASK", "confidence": 0.803468644618988}, {"text": "Recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9977189898490906}, {"text": "Precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9971452355384827}]}, {"text": "The result for Recall was F (9,990) = 5.248, p < .001), and for Precision F (9,990) = 5.038, p < .001.", "labels": [], "entities": [{"text": "Recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.7211170792579651}, {"text": "F", "start_pos": 26, "end_pos": 27, "type": "METRIC", "confidence": 0.9977213740348816}]}, {"text": "We again compared the mean scores with Tukey's HSD.", "labels": [], "entities": [{"text": "mean", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9565377235412598}, {"text": "Tukey's HSD", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.7622259060541788}]}, {"text": "One would generally expect results on test set NEG-b to be better than on NEG-a.", "labels": [], "entities": [{"text": "NEG-b", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.8382514715194702}, {"text": "NEG-a", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9493699073791504}]}, {"text": "This is the case for all baseline systems and some of the participating systems, but not all.", "labels": [], "entities": []}, {"text": "The JU system in particular drops in score (and rank).", "labels": [], "entities": [{"text": "JU", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.4750175178050995}, {"text": "score", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9718883037567139}]}, {"text": "We also computed Word String Accuracy and the other string similarity metrics described in Section 3 for the GREC-NEG Task.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.540687620639801}, {"text": "GREC-NEG Task", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.8222024440765381}]}, {"text": "The resulting scores for Test Set NEG-a are shown in Table 6.", "labels": [], "entities": [{"text": "Test Set NEG-a", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.8739678661028544}]}, {"text": "Ranks for peer systems relative to each other are very similar to the results for REG08-Type reported above.", "labels": [], "entities": [{"text": "REG08-Type", "start_pos": 82, "end_pos": 92, "type": "DATASET", "confidence": 0.7605999112129211}]}, {"text": "We performed a univariate ANOVA with System as the fixed factor, and Word String Accuracy as the dependent variable.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.7952790260314941}]}, {"text": "The F-ratio for System was F (9,990) = 41.308, p < 0.001; the homogeneous subsets resulting from the Tukey HSD posthoc analysis are shown in columns 3-7 of. shows analogous results for human topline Test Set NEG-b (which has three versions of each text).", "labels": [], "entities": [{"text": "F-ratio", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9988204836845398}, {"text": "F", "start_pos": 27, "end_pos": 28, "type": "METRIC", "confidence": 0.9972073435783386}, {"text": "Tukey HSD posthoc analysis", "start_pos": 101, "end_pos": 127, "type": "DATASET", "confidence": 0.8581084311008453}, {"text": "human topline Test Set NEG-b", "start_pos": 185, "end_pos": 213, "type": "DATASET", "confidence": 0.6583751916885376}]}, {"text": "We carried out the same kind of ANOVA as for Test Set NEG-a; the result for System on Word String Accuracy was F (9,990) = 35.123, p < 0.001.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.8547236323356628}, {"text": "F", "start_pos": 111, "end_pos": 112, "type": "METRIC", "confidence": 0.994712233543396}]}, {"text": "System rankings are the same as for Test Set NEG-a (the differences between JU and Base-freq, which swap ranks, are not significant); scores across the board (again, except for the JU system) are somewhat higher, because of the way scores are computed for version b test   sets: a score is the highest score a system achieves (at text-level) against any of the three versions of a test set text that is taken into account.", "labels": [], "entities": []}, {"text": "Results for BLEU-3, NIST and the two stringedit distance metrics are shown in the rightmost 4 columns of.", "labels": [], "entities": [{"text": "BLEU-3", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.8981966376304626}, {"text": "NIST", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.9365320801734924}]}, {"text": "With the exception of Base-freq/Basename on Test Set NEG-b, systems whose Word String Accuracy scores differ significantly are assigned the same relative ranks by all other string-similarity metrics as by Word String Accuracy.", "labels": [], "entities": []}, {"text": "In the human intrinsic evaluation, evaluators rated system outputs in terms of whether they preferred them over the original Wikipedia texts.", "labels": [], "entities": []}, {"text": "As a result of the experiment we had (for each system and each evaluation criterion) a set of scores ranging from -10.0 to +10.0, where 0 meant no preference, negative scores meant a preference for the Wikipedia text, and positive scores a preference for the system-produced text.", "labels": [], "entities": []}, {"text": "The second column of the left half of summarises the Clarity scores for each system in terms of their mean; if the mean is negative the evaluators overall preferred the Wikipedia texts, if it is positive evaluators overall preferred the system.", "labels": [], "entities": [{"text": "Clarity", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.8475498557090759}]}, {"text": "The more negative the score, the more strongly evaluators preferred the Wikipedia texts.", "labels": [], "entities": [{"text": "Wikipedia texts", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.8882206380367279}]}, {"text": "Columns 8-10 show corresponding counts of how many times each system was preferred (+), dispreferred (\u2212), and neither (0).", "labels": [], "entities": []}, {"text": "The other half of shows corresponding results for Fluency.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.6950867176055908}]}, {"text": "We ran a factorial multivariate ANOVA with Fluency and Clarity as the dependent variables.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9964141845703125}]}, {"text": "In the first version of the ANOVA, the fixed factors were System, Evaluator and Wikipedia Side (indicating whether the Wikipedia text was shown on the left or right during evaluation).", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 28, "end_pos": 33, "type": "TASK", "confidence": 0.4502990245819092}, {"text": "System", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.8869677782058716}, {"text": "Wikipedia Side", "start_pos": 80, "end_pos": 94, "type": "METRIC", "confidence": 0.6445883810520172}]}, {"text": "This showed no significant effect of Wikipedia Side on either Fluency or Clarity, and no significant interaction between any of the factors.", "labels": [], "entities": [{"text": "Wikipedia Side", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8021443486213684}]}, {"text": "There was also no significant effect of Evaluator on Fluency, and only a weakly significant effect of Evaluator on Clarity.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9906050562858582}]}, {"text": "We ran the ANOVA again, this time with just System as the fixed factor.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.7241773009300232}]}, {"text": "The F-ratio for System on Fluency was F (9,290) = 22.911, p < .001, and for System on Clarity it was F (9,290) = 13.051, p < .001.", "labels": [], "entities": [{"text": "F-ratio", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9984822869300842}, {"text": "F", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.9914423823356628}, {"text": "F", "start_pos": 101, "end_pos": 102, "type": "METRIC", "confidence": 0.9725740551948547}]}, {"text": "Post-hoc Tukey's HSD tests revealed the significant pairwise differences indicated by the letter columns in.", "labels": [], "entities": []}, {"text": "Correlation between individual Clarity and Fluency ratings as estimated with Pearson's coefficient was r = 0.66, p < 0.01, indicating that the two criteria covary to some extent.: GREC-NEG: Results for Clarity and Fluency preference judgement experiment.", "labels": [], "entities": [{"text": "Pearson's coefficient", "start_pos": 77, "end_pos": 98, "type": "METRIC", "confidence": 0.8683852752049764}, {"text": "GREC-NEG", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.632035493850708}, {"text": "Clarity and Fluency preference judgement", "start_pos": 202, "end_pos": 242, "type": "TASK", "confidence": 0.6325491607189179}]}, {"text": "Mean = mean of individual scores (where scores ranged from -10.0 to + 10.0); + = number of times system was preferred; \u2212 = number of times corpus text (Wikipedia) was preferred; 0 = number of times neither was preferred.", "labels": [], "entities": []}, {"text": "The relative ranks of the peer systems are the same in terms of both Fluency and Clarity.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9991480112075806}, {"text": "Clarity", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9510915875434875}]}, {"text": "However, there are interesting differences in the ranks of the baseline systems.", "labels": [], "entities": []}, {"text": "For Clarity, Base-name and Base-1st are scored fairly highly (presumably because both tend to pick named references which are clear if not always fluent), but both go back to not being significantly better than Base-rand in the Fluency rankings.", "labels": [], "entities": [{"text": "Base-name", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9106589555740356}, {"text": "Base-1st", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9362680912017822}]}, {"text": "Base-freq does badly in the Clarity scores, but is significantly better than the bottom three systems in terms of Fluency.", "labels": [], "entities": [{"text": "Clarity", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.8046709895133972}, {"text": "Fluency", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.9985164999961853}]}], "tableCaptions": [{"text": " Table 1: GREC-NEG'09 teams and systems (combined teams in last two rows). x = resubmitted after  fixing character encoding problems and/or software bugs; y = late submission.", "labels": [], "entities": []}, {"text": " Table 2: Overview of GREC'10 data sets.", "labels": [], "entities": [{"text": "GREC'10 data sets", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.9585180083910624}]}, {"text": " Table 4: REG08-Type Precision and Recall scores against corpus version of Test Set for complete set and  for subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.", "labels": [], "entities": [{"text": "REG08-Type Precision", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.6299126446247101}, {"text": "Recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9914922118186951}]}, {"text": " Table 5: REG08-Type Recall and Precision scores against human topline version of Test Set for complete  set and for subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.", "labels": [], "entities": [{"text": "REG08-Type Recall", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7581706047058105}, {"text": "Precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.967212975025177}]}, {"text": " Table 6: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set NEG-a (sys- tems in order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String  Accuracy only.", "labels": [], "entities": [{"text": "Word String Accuracy", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.6768941283226013}, {"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9967579245567322}, {"text": "NIST", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.6386839747428894}, {"text": "Tukey HSD", "start_pos": 162, "end_pos": 171, "type": "DATASET", "confidence": 0.8323836028575897}]}, {"text": " Table 7: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set NEG-b (sys- tems in order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String  Accuracy.", "labels": [], "entities": [{"text": "Word String Accuracy", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.658292273680369}, {"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9966397285461426}, {"text": "Test Set NEG-b", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.6716898183027903}, {"text": "Tukey HSD", "start_pos": 162, "end_pos": 171, "type": "DATASET", "confidence": 0.8246104717254639}]}, {"text": " Table 8: GREC-NEG: Results for Clarity and Fluency preference judgement experiment. Mean = mean of  individual scores (where scores ranged from -10.0 to + 10.0); + = number of times system was preferred;  \u2212 = number of times corpus text (Wikipedia) was preferred; 0 = number of times neither was preferred.", "labels": [], "entities": [{"text": "GREC-NEG", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.615138828754425}, {"text": "Clarity and Fluency preference judgement", "start_pos": 32, "end_pos": 72, "type": "TASK", "confidence": 0.5707649767398835}, {"text": "Mean", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9869934320449829}]}, {"text": " Table 9: MUC-6, CEAF and B-3 scores for GREC-NER systems. Systems shown in order of average  scores.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.4971048831939697}, {"text": "CEAF", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.6892417073249817}, {"text": "B-3", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.970001757144928}]}, {"text": " Table 10: GREC-FULL: Mean text-level BLEU-4 scores, system-level BLEU-4 and NIST scores.", "labels": [], "entities": [{"text": "GREC-FULL", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.993862509727478}, {"text": "Mean text-level BLEU-4 scores", "start_pos": 22, "end_pos": 51, "type": "METRIC", "confidence": 0.8378425687551498}, {"text": "BLEU-4", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.8477553725242615}, {"text": "NIST", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.68023681640625}]}]}