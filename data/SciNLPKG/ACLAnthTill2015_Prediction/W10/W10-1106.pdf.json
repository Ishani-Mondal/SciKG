{"title": [{"text": "Reliability and Type of Consumer Health Documents on the World Wide Web: an Annotation Study", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present a detailed scheme for annotating medical web pages designed for healthcare consumers.", "labels": [], "entities": []}, {"text": "The annotation is along two axes: first, by reliability or the extent to which the medical information on the page can be trusted, second, by the type of page (patient leaflet , commercial, link, medical article, testimonial , or support).", "labels": [], "entities": [{"text": "reliability", "start_pos": 44, "end_pos": 55, "type": "METRIC", "confidence": 0.9907006025314331}]}, {"text": "We analyze inter-rater agreement among three judges for each category.", "labels": [], "entities": []}, {"text": "Inter-rater agreement was moderate (0.77 accuracy, 0.62 F-measure, 0.49 Kappa) on the reliability axis and good (0.81 accuracy, 0.72 F-measure, 0.73 Kappa) along the type axis.", "labels": [], "entities": [{"text": "agreement", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9484837055206299}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9953474402427673}, {"text": "F-measure", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9902642965316772}, {"text": "reliability axis", "start_pos": 86, "end_pos": 102, "type": "METRIC", "confidence": 0.9750063717365265}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9937479496002197}, {"text": "F-measure", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9517834186553955}]}], "introductionContent": [{"text": "With the explosive growth of the World Wide Web has come, not just an explosion of information, but also the explosion of false, misleading and unsupported information.", "labels": [], "entities": []}, {"text": "At the same time, the web is increasingly used for tasks where information quality and reliability are vital, from legal and medical research by both professionals and laypeople, to fact checking by journalists and research by government policy makers.", "labels": [], "entities": [{"text": "fact checking", "start_pos": 182, "end_pos": 195, "type": "TASK", "confidence": 0.7904708385467529}]}, {"text": "In particular, there has been a proliferation of web pages in the medical domain for healthcare consumers.", "labels": [], "entities": []}, {"text": "At the first sign of illness or injury more and more people go to the web before consulting medical professionals.", "labels": [], "entities": []}, {"text": "The quality and reliability of the information on consumer medical web pages has been of concern for sometime to medical professionals and policy makers.", "labels": [], "entities": []}, {"text": "(For example see, Our goal is to create a system that can automatically measure the reliability of web pages in the medical domain).", "labels": [], "entities": []}, {"text": "More specifically, given a web page resulting from a user query on a medical topic, we would like to automatically provide an estimate of the extent to which the information on the page can be trusted.", "labels": [], "entities": []}, {"text": "In order to make use of supervised natural language processing and machine learning algorithms to create such a system, and to ultimately evaluate the performance of the system, it is necessary to have human annotated data.", "labels": [], "entities": []}, {"text": "It is important to note the varied uses of the term \"reliability\" in the computer and information sciences.", "labels": [], "entities": []}, {"text": "In the current context we use it to refer to an intrinsic property of a web page: essentially the trustworthiness of the information it contains.", "labels": [], "entities": []}, {"text": "This sense of reliability is distinct from its meaning in measurement theory as an indicator of repeatability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9814545512199402}]}, {"text": "It also excludes measures such as credibility that are based on user beliefs or understanding.", "labels": [], "entities": []}, {"text": "In this paper we report results of an annotation study of medical web pages designed for healthcare consumers.", "labels": [], "entities": []}, {"text": "Three humans annotated a corpus of web pages along two axes.", "labels": [], "entities": []}, {"text": "The first axis is the reliability of the information contained in the page.", "labels": [], "entities": [{"text": "reliability", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9916847944259644}]}, {"text": "The second axis is the type, or kind, of page.", "labels": [], "entities": []}, {"text": "Intercoder agreement was moderate (0.77 accuracy, 0.62 F-measure, 0.49 Kappa) on the reliability axis and good (0.81 accuracy, 0.72 F-measure, 0.73 Kappa) along the type axis.", "labels": [], "entities": [{"text": "Intercoder", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9647266864776611}, {"text": "agreement", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.5010451674461365}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9943248629570007}, {"text": "F-measure", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9888441562652588}, {"text": "reliability axis", "start_pos": 85, "end_pos": 101, "type": "METRIC", "confidence": 0.9740327596664429}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.99272620677948}, {"text": "F-measure", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9527851343154907}]}, {"text": "In our materials and methods section we discuss the data, definitions, annotation study and the results.", "labels": [], "entities": []}, {"text": "We follow with a discussion section and a conclusion.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Inter-rater agreement for 5-class reliability.", "labels": [], "entities": []}, {"text": " Table 2. Inter-rater agreement for 3-classs reliability.", "labels": [], "entities": []}, {"text": " Table 3. F-measure by class for 3-classs reliability.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9935321807861328}]}, {"text": " Table 4. Inter-rater reliability agreement for M-E by  query.", "labels": [], "entities": []}, {"text": " Table 5. Inter-rater agreement for type annotation.", "labels": [], "entities": []}, {"text": " Table 6. Of the  three most common types of pages (Patient Leaflet,  Link, Commercial), the Link type was the most  difficult for M-E to agree on.", "labels": [], "entities": []}, {"text": " Table 6. F-measure by class for page type.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9946922659873962}]}, {"text": " Table 7. Inter-rater type agreement for M-E by query.", "labels": [], "entities": []}]}