{"title": [{"text": "Bayesian Hidden Markov Models and Extensions", "labels": [], "entities": []}], "abstractContent": [{"text": "Hidden Markov models (HMMs) are one of the cornerstones of time-series modelling.", "labels": [], "entities": []}, {"text": "I will review HMMs, motivations for Bayesian approaches to inference in them, and our work on variational Bayesian learning.", "labels": [], "entities": []}, {"text": "I will then focus on recent nonparametric extensions to HMMs.", "labels": [], "entities": []}, {"text": "Traditionally, HMMs have a known structure with a fixed number of states and are trained using maximum likelihood techniques.", "labels": [], "entities": []}, {"text": "The infinite HMM (iHMM) allows a potentially unbounded number of hidden states, letting the model use as many states as it needs for the data.", "labels": [], "entities": []}, {"text": "The recent development of 'Beam Sampling'-an efficient inference algorithm for iHMMs based on dynamic programming-makes it possible to apply iHMMs to large problems.", "labels": [], "entities": [{"text": "Beam Sampling", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.7621413469314575}]}, {"text": "I will show some applications of iHMMs to unsupervised POS tagging and experiments with parallel and distributed implementations.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7505998611450195}]}, {"text": "I will also describe a factorial generalisation of the iHMM which makes it possible to have an unbounded number of binary state variables, and can bethought of as a time-series generalisation of the Indian buffet process.", "labels": [], "entities": [{"text": "iHMM", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9484682679176331}]}, {"text": "I will conclude with thoughts on future directions in Bayesian modelling of sequential data.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}