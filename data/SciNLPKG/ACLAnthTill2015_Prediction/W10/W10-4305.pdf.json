{"title": [{"text": "Evaluation Metrics For End-to-End Coreference Resolution Systems", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8733406066894531}]}], "abstractContent": [{"text": "Commonly used coreference resolution evaluation metrics can only be applied to key mentions, i.e. already annotated mentions.", "labels": [], "entities": [{"text": "coreference resolution evaluation", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.9205172061920166}]}, {"text": "We here propose two variants of the B 3 and CEAF coreference resolution evaluation algorithms which can be applied to coreference resolution systems dealing with system mentions, i.e. automatically determined mentions.", "labels": [], "entities": [{"text": "coreference resolution evaluation", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.7673216859499613}, {"text": "coreference resolution", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.8753969967365265}]}, {"text": "Our experiments show that our variants lead to intuitive and reliable results.", "labels": [], "entities": []}], "introductionContent": [{"text": "The coreference resolution problem can be divided into two steps: (1) determining mentions, i.e., whether an expression is referential and can take part in a coreferential relationship, and (2) deciding whether mentions are coreferent or not.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.946033239364624}]}, {"text": "Most recent research on coreference resolution simplifies the resolution task by providing the system with key mentions, i.e. already annotated mentions (,,,, inter alia; see also the task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only, inter alia).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.9436156749725342}, {"text": "coreference resolution", "start_pos": 231, "end_pos": 253, "type": "TASK", "confidence": 0.8473596572875977}]}, {"text": "We follow here) in arguing that such evaluations are \"an unrealistic surrogate for the original problem\" and ask researchers to evaluate end-toend coreference resolution systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 147, "end_pos": 169, "type": "TASK", "confidence": 0.6796810030937195}]}, {"text": "However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.904003918170929}]}, {"text": "evaluate using the MUC score ( and the CEAF algorithm () without modifications.", "labels": [], "entities": [{"text": "MUC score", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.6180100739002228}, {"text": "CEAF algorithm", "start_pos": 39, "end_pos": 53, "type": "METRIC", "confidence": 0.8954290449619293}]}, {"text": "use only the MUC score. and derive variants from the B 3 algorithm (.", "labels": [], "entities": [{"text": "MUC score.", "start_pos": 13, "end_pos": 23, "type": "DATASET", "confidence": 0.5829370319843292}]}, {"text": "propose their own variants of B and CEAF.", "labels": [], "entities": [{"text": "B", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.9961047172546387}, {"text": "CEAF", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.7731979489326477}]}, {"text": "Unfortunately, some of the metrics' descriptions are so concise that they leave too much room for interpretation.", "labels": [], "entities": []}, {"text": "Also, some of the metrics proposed are too lenient or are more sensitive to mention detection than to coreference resolution.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7225565761327744}, {"text": "coreference resolution", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.9536264836788177}]}, {"text": "Hence, though standard corpora are used, the results are not comparable.", "labels": [], "entities": []}, {"text": "This paper attempts to fill that desideratum by analysing several variants of the B 3 and CEAF algorithms.", "labels": [], "entities": []}, {"text": "We propose two new variants, namely B 3 sys and CEAF sys , and provide algorithmic details in Section 2.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.787926971912384}]}, {"text": "We describe two experiments in Section 3 showing that B 3 sys and CEAF sys lead to intuitive and reliable results.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.5222291350364685}]}, {"text": "Implementations of B 3 sys and CEAF sys are available open source along with extended examples 1 .", "labels": [], "entities": [{"text": "CEAF", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8649553060531616}]}], "datasetContent": [{"text": "We discuss the problems which arise when applying the most prevalent coreference resolution evaluation metrics to end-to-end systems and propose our variants which overcome those problems.", "labels": [], "entities": [{"text": "coreference resolution evaluation", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.92099662621816}]}, {"text": "We provide detailed analyses of illustrative examples.", "labels": [], "entities": []}, {"text": "While Section 2 used toy examples to motivate our metrics B 3 sys and CEAF sys , we here report results on two larger experiments using ACE2004 data.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.6185888051986694}, {"text": "ACE2004 data", "start_pos": 136, "end_pos": 148, "type": "DATASET", "confidence": 0.9685620069503784}]}, {"text": "For the realistic setting we compare SM1 and SM2 as preprocessing components for the BART) reimplementation of.", "labels": [], "entities": []}, {"text": "The coreference resolution system with the SM2 tagger performs better, because a better coreference model is achieved from system mentions with higher accuracy.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9189386963844299}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9922938346862793}]}, {"text": "The MUC, B 3 sys and CEAF sys metrics have the same tendency when applied to systems with different mention taggers (Table 8, 9 and 10 and the bold numbers are higher with a p-value of 0.05, by a paired-t test).", "labels": [], "entities": [{"text": "MUC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.895165741443634}, {"text": "CEAF", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.7727709412574768}]}, {"text": "Since the MUC scorer does not evaluate singleton entities, it produces too low numbers which are not informative anymore.", "labels": [], "entities": [{"text": "MUC scorer", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.7260438799858093}]}, {"text": "As shown in, B 3 all reports counterintuitive results when a system is fed with system mentions generated by different mention taggers.", "labels": [], "entities": []}, {"text": "B 3 all cannot be used to evaluate two different endto-end coreference resolution systems, because the mention tagger is likely to have bigger impact than the coreference resolution system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7202474027872086}, {"text": "coreference resolution", "start_pos": 159, "end_pos": 181, "type": "TASK", "confidence": 0.8323200941085815}]}, {"text": "B 3 0 fails to generate the right comparison too, because it is too illustrate the big influence the system mentions have on precision (e.g. the very low precision number for Soon (SM1)).", "labels": [], "entities": [{"text": "B 3 0", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.93598872423172}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9991542100906372}, {"text": "precision number", "start_pos": 154, "end_pos": 170, "type": "METRIC", "confidence": 0.9700290858745575}]}, {"text": "The big improvement for Soon (SM2) is largely due to the system mentions it uses, rather than to different coreference models.", "labels": [], "entities": []}, {"text": "Both B 3 r&n and CEAF r&n show no serious problems in the experimental results.", "labels": [], "entities": [{"text": "B 3 r&n", "start_pos": 5, "end_pos": 12, "type": "DATASET", "confidence": 0.6983540654182434}, {"text": "CEAF r&n", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.9390372037887573}]}, {"text": "However, as discussed before, they fail to penalize the spurious entities with twinless system mentions adequately.", "labels": [], "entities": []}, {"text": "We compare results of system with our Soon (SM2) system.", "labels": [], "entities": []}, {"text": "Bengtson & Roth's embedded mention tagger aims at high precision, generating half of the mentions SM1 generates (explicit statistics are not available to us).", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9940493106842041}]}, {"text": "Bengtson & Roth report a B 3 F-score for system mentions, which is very close to the one for true mentions.", "labels": [], "entities": [{"text": "B 3 F-score", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.8085297147432963}]}, {"text": "Their B 3 -variant does not impute errors of twinless mentions and is assumed to be quite similar to the B 3 0 strategy.", "labels": [], "entities": [{"text": "B 3 -variant", "start_pos": 6, "end_pos": 18, "type": "METRIC", "confidence": 0.8453399688005447}]}, {"text": "We integrate both the B 3 0 and B 3 sys variants into their system and show results in (we cannot report significance, because we do not have access to results for single documents in Bengtson & Roth's system).", "labels": [], "entities": []}, {"text": "It can be seen that, when different variants of evaluation metrics are applied, the performance of the systems vary wildly.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Problems of B 3", "labels": [], "entities": []}, {"text": " Table 2: Problems of B 3  all (1)", "labels": [], "entities": []}, {"text": " Table 3: Problems of B 3  all", "labels": [], "entities": []}, {"text": " Table 4: Problems of CEAF orig", "labels": [], "entities": [{"text": "CEAF", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.9106221795082092}]}, {"text": " Table 5: Problems of CEAF r&n", "labels": [], "entities": [{"text": "CEAF r&n", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8110534995794296}]}, {"text": " Table 6: Problems of \u03c6 4 (\u22c6, \u22c6)", "labels": [], "entities": []}, {"text": " Table 7: Mention Taggers on ACE2004 Data", "labels": [], "entities": [{"text": "Mention Taggers", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6600473821163177}, {"text": "ACE2004", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.8363856673240662}]}, {"text": " Table 8: Realistic Setting MUC", "labels": [], "entities": [{"text": "Realistic Setting MUC", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7992601792017618}]}, {"text": " Table 9: Realistic Setting B 3 Variants", "labels": [], "entities": []}, {"text": " Table 12: Analysis of B 3  sys 1", "labels": [], "entities": []}, {"text": " Table 13: Analysis of B 3  sys 2", "labels": [], "entities": []}, {"text": " Table 14: Analysis of B 3  sys 3", "labels": [], "entities": []}]}