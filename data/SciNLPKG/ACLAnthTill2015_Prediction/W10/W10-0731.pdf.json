{"title": [{"text": "Amazon Mechanical Turk for Subjectivity Word Sense Disambiguation", "labels": [], "entities": [{"text": "Subjectivity Word Sense Disambiguation", "start_pos": 27, "end_pos": 65, "type": "TASK", "confidence": 0.6733785271644592}]}], "abstractContent": [{"text": "Amazon Mechanical Turk (MTurk) is a marketplace for so-called \"human intelligence tasks\" (HITs), or tasks that are easy for humans but currently difficult for automated processes.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7190849979718527}]}, {"text": "Providers upload tasks to MTurk which workers then complete.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.8850296139717102}]}, {"text": "Natural language annotation is one such human intelligence task.", "labels": [], "entities": [{"text": "Natural language annotation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6612462898095449}]}, {"text": "In this paper, we investigate using MTurk to collect annotations for Subjec-tivity Word Sense Disambiguation (SWSD), a coarse-grained word sense disambiguation task.", "labels": [], "entities": [{"text": "Subjec-tivity Word Sense Disambiguation (SWSD)", "start_pos": 69, "end_pos": 115, "type": "TASK", "confidence": 0.7228439577988216}, {"text": "word sense disambiguation task", "start_pos": 134, "end_pos": 164, "type": "TASK", "confidence": 0.7501621022820473}]}, {"text": "We investigate whether we can use MTurk to acquire good annotations with respect to gold-standard data, whether we can filter out low-quality workers (spammers), and whether there is a learning effect associated with repeatedly completing the same kind of task.", "labels": [], "entities": []}, {"text": "While our results with respect to spam-mers are inconclusive, we are able to obtain high-quality annotations for the SWSD task.", "labels": [], "entities": [{"text": "SWSD task", "start_pos": 117, "end_pos": 126, "type": "TASK", "confidence": 0.9125752747058868}]}, {"text": "These results suggest a greater role for MTurk with respect to constructing a large scale SWSD system in the future, promising substantial improvement in subjectivity and sentiment analysis.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 41, "end_pos": 46, "type": "TASK", "confidence": 0.6206947565078735}, {"text": "SWSD", "start_pos": 90, "end_pos": 94, "type": "TASK", "confidence": 0.9360339641571045}, {"text": "sentiment analysis", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.9447053670883179}]}], "introductionContent": [{"text": "Many Natural Language Processing (NLP) systems rely on large amounts of manually annotated data that is collected from domain experts.", "labels": [], "entities": []}, {"text": "The annotation process to obtain this data is very laborious and expensive.", "labels": [], "entities": []}, {"text": "This makes supervised NLP systems subject to a so-called knowledge acquisition bottleneck.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.7951931059360504}]}, {"text": "For example,) estimates an effort of 16 person years to construct training data fora highaccuracy domain independent Word Sense Disambiguation (WSD) system.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 117, "end_pos": 148, "type": "TASK", "confidence": 0.7217913269996643}]}, {"text": "Recently researchers have been investigating Amazon Mechanical Turk (MTurk) as a source of non-expert natural language annotation, which is a cheap and quick alternative to expert annotations.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 45, "end_pos": 75, "type": "DATASET", "confidence": 0.7983969748020172}]}, {"text": "In this paper, we utilize MTurk to obtain training data for Subjectivity Word Sense Disambiguation (SWSD) as described in ().", "labels": [], "entities": [{"text": "Subjectivity Word Sense Disambiguation (SWSD)", "start_pos": 60, "end_pos": 105, "type": "TASK", "confidence": 0.7245271929672786}]}, {"text": "The goal of SWSD is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses.", "labels": [], "entities": [{"text": "SWSD", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.9756608605384827}]}, {"text": "SWSD is anew task which suffers from the absence of a substantial amount of annotated data and thus can only be applied on a small scale.", "labels": [], "entities": [{"text": "SWSD", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.7314322590827942}]}, {"text": "SWSD has strong connections to WSD.", "labels": [], "entities": [{"text": "SWSD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6134858131408691}, {"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.7028890252113342}]}, {"text": "Like supervised WSD, it requires training data where target word instances -words which need to be disambiguated by the system -are labeled as having an objective sense or a subjective sense.", "labels": [], "entities": [{"text": "WSD", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9236757755279541}]}, {"text": "( show that SWSD may bring substantial improvement in subjectivity and sentiment analysis, if it could be applied on a larger scale.", "labels": [], "entities": [{"text": "SWSD", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.922685980796814}, {"text": "sentiment analysis", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.9469897449016571}]}, {"text": "The good news is that training data for 80 selected keywords is enough to make a substantial difference ().", "labels": [], "entities": []}, {"text": "Thus, large scale SWSD is feasible.", "labels": [], "entities": [{"text": "SWSD", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.9759865999221802}]}, {"text": "We hypothesize that annotations for SWSD can be provided by non-experts reliably if the annotation task is presented in a simple way.", "labels": [], "entities": [{"text": "SWSD", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.9533450603485107}]}, {"text": "The annotations obtained from MTurk workers are noisy by nature, because MTurk workers are not trained for the underlying annotation task.", "labels": [], "entities": []}, {"text": "That is why previous work explored methods to assess annotation quality and to aggregate multiple noisy annotations for high reliability.", "labels": [], "entities": []}, {"text": "It is understandable that not every worker will provide high-quality annotations, depending on their background and interest.", "labels": [], "entities": []}, {"text": "Unfortunately, some MTurk workers do not follow the annotation guidelines and carelessly submit annotations in order to gain economic benefits with only minimal effort.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 20, "end_pos": 25, "type": "TASK", "confidence": 0.8078019618988037}]}, {"text": "We define this group of workers as spammers.", "labels": [], "entities": []}, {"text": "We believe it is essential to distinguish between workers as well-meaning annotators and workers as spammers who should be filtered out as a first step when utilizing MTurk.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 167, "end_pos": 172, "type": "DATASET", "confidence": 0.7972375154495239}]}, {"text": "In this work, we investigate how well the built-in qualifications in MTurk function as such a filter.", "labels": [], "entities": []}, {"text": "Another important question about MTurk workers is whether they learn to provide better annotations overtime in the absence of any interaction and feedback.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 33, "end_pos": 38, "type": "TASK", "confidence": 0.9396058917045593}]}, {"text": "The presence of a learning effect may support working with the same workers over along time and creating private groups of workers.", "labels": [], "entities": []}, {"text": "In this work, we also examine if there is a learning effect associated with MTurk workers.", "labels": [], "entities": []}, {"text": "To summarize, in this work we investigate the following questions: \u2022 Can MTurk be utilized to collect reliable training data for SWSD ? \u2022 Are the built-in methods provided by MTurk enough to avoid spammers ? \u2022 Is there a learning effect associated with MTurk workers ? The remainder of the paper is organized as follows.", "labels": [], "entities": [{"text": "SWSD", "start_pos": 129, "end_pos": 133, "type": "TASK", "confidence": 0.8419426679611206}]}, {"text": "In Section 2, we give general background information on the Amazon Mechanical Turk service.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk service", "start_pos": 60, "end_pos": 90, "type": "DATASET", "confidence": 0.9532479047775269}]}, {"text": "In Section 3, we discuss sense subjectivity.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the subjectivity word sense disambiguation task.", "labels": [], "entities": [{"text": "subjectivity word sense disambiguation task", "start_pos": 30, "end_pos": 73, "type": "TASK", "confidence": 0.7124470233917236}]}, {"text": "In Section 5, we discuss the design of our experiment and our filtering mechanisms for workers.", "labels": [], "entities": []}, {"text": "In Section 6, we evaluate MTurk annotations and relate results to our questions.", "labels": [], "entities": [{"text": "MTurk annotations", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.8007280230522156}]}, {"text": "In Section 7, we review related work.", "labels": [], "entities": []}, {"text": "In Section 8, we draw conclusions and discuss future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We chose randomly 8 target words that have a distribution of subjective and objective instances in subjSENSEVAL with less skew than 75%.", "labels": [], "entities": []}, {"text": "That is, no more than 75% of a word's senses are subjective or objective.", "labels": [], "entities": []}, {"text": "Our concern is that using skewed data might bias the workers to choose from the more frequent label without thinking much about the problem.", "labels": [], "entities": []}, {"text": "Another important fact is that these words with low skew are more ambiguous and responsible for more false hits.", "labels": [], "entities": []}, {"text": "Thus, these target words are the ones for which we really need subjectivity word sense disambiguation.", "labels": [], "entities": [{"text": "subjectivity word sense disambiguation", "start_pos": 63, "end_pos": 101, "type": "TASK", "confidence": 0.6291938200592995}]}, {"text": "For each of these 8 target words, we select 40 passages from subjSENSEVAL in which the target word appears, to include in our experiments. and their label distribution.", "labels": [], "entities": []}, {"text": "In this table, frequent label percentage (FLP) represents the skew for each word.", "labels": [], "entities": [{"text": "frequent label percentage (FLP)", "start_pos": 15, "end_pos": 46, "type": "METRIC", "confidence": 0.9510250786940256}]}, {"text": "A word's FLP is equal to the percent of the senses that are of the most frequently occurring type of sense (subjective or objective) for that word.", "labels": [], "entities": [{"text": "FLP", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9946984052658081}]}, {"text": "We believe this annotation task is a good candidate for attracting spammers.", "labels": [], "entities": [{"text": "attracting spammers", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7701623439788818}]}, {"text": "This task requires only binary annotations, where the worker just chooses from one of the two given sets, which is not a difficult task.", "labels": [], "entities": []}, {"text": "Since it is easy to provide labels, we believe that there will be a distinct line, with respect to quality of annotations, between spammers and mediocre annotators.", "labels": [], "entities": []}, {"text": "For our experiments, we created three different HIT groups each having different qualification requirements but sharing the same data.", "labels": [], "entities": []}, {"text": "To be concrete, each HIT group consists of the same 320 instances: 40 instances for each target word listed in.", "labels": [], "entities": []}, {"text": "Each HIT presents an MTurk worker with four instances of the same word in a text passage -this makes 80 HITs for each HIT group -and asks him to choose the set to which the activated sense belongs.", "labels": [], "entities": []}, {"text": "We know for each HIT the mapping between sense set numbers and subjectivity.", "labels": [], "entities": []}, {"text": "Thus, we can evaluate each HIT response on our goldstandard data, as discussed in Section 4.2.", "labels": [], "entities": [{"text": "goldstandard data", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.7082530558109283}]}, {"text": "We pay seven cents per HIT.", "labels": [], "entities": [{"text": "HIT", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.574685275554657}]}, {"text": "We consider this to be generous compensation for such a simple task.", "labels": [], "entities": []}, {"text": "There are many builtin qualifications in MTurk.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 41, "end_pos": 46, "type": "TASK", "confidence": 0.7269247174263}]}, {"text": "We concentrated only on three of them: location, HIT approval rate, and approved HITs, as discussed in Section 2.", "labels": [], "entities": [{"text": "HIT approval rate", "start_pos": 49, "end_pos": 66, "type": "METRIC", "confidence": 0.7323054273923238}]}, {"text": "In our experience, these qualifications are widely used for quality assurance.", "labels": [], "entities": []}, {"text": "As mentioned before, we created three different HIT groups in order to see how well different built-in qualification combinations do with respect to filtering spammers.", "labels": [], "entities": []}, {"text": "These groups -starting from the least constrained to the most constrained -are listed in.", "labels": [], "entities": []}, {"text": "We are interested in how accurate the MTurk annotations are with respect to gold-standard data.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.6843432188034058}]}, {"text": "We are also interested in how the accuracy of each group differs from the others.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9989463686943054}]}, {"text": "We evaluate each group itself separately on the gold-standard data.", "labels": [], "entities": [{"text": "gold-standard data", "start_pos": 48, "end_pos": 66, "type": "DATASET", "confidence": 0.7937616109848022}]}, {"text": "Additionally, we evaluate each worker's performance on the gold-standard data and inspect their distribution in various groups.", "labels": [], "entities": []}, {"text": "As mentioned in the previous section, we collect three annotations for each HIT.", "labels": [], "entities": [{"text": "HIT", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.8741236329078674}]}, {"text": "They are assigned to respective trials in the order submitted by the workers.", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "Trials are labeled as TX and MV is the majority vote annotation among the three trials.", "labels": [], "entities": [{"text": "MV", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9970276951789856}]}, {"text": "The final column contains the baseline agreement where a worker labels each instance of a word with the most frequent label of that word in the gold-standard data.", "labels": [], "entities": []}, {"text": "It is clear from this table that, since worker accuracy always exceeds the baseline agreement, subjectivity word sense annotation can be done reliably by MTurk workers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.964801013469696}, {"text": "subjectivity word sense annotation", "start_pos": 95, "end_pos": 129, "type": "TASK", "confidence": 0.6339433267712593}]}, {"text": "Considering the low cost and low time required to obtain MTurk annotations, a large scale SWSD is realistic.", "labels": [], "entities": [{"text": "SWSD", "start_pos": 90, "end_pos": 94, "type": "TASK", "confidence": 0.951752781867981}]}, {"text": "For example, ( shows that the most frequent 80 lexicon keywords are responsible for almost half of the false hits in the MPQA Corpus, a corpus annotated for subjective expressions.", "labels": [], "entities": [{"text": "MPQA Corpus", "start_pos": 121, "end_pos": 132, "type": "DATASET", "confidence": 0.9667802453041077}]}, {"text": "Utilizing MTurk to collect training data for these 80 lexicon keywords will be quick and cheap and most importantly reliable.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.8175768852233887}]}, {"text": "When we compare groups with each other, we see that the best trial result is achieved in Group3.", "labels": [], "entities": []}, {"text": "However, according to McNemar's test, there is no statistically significant difference between any trial of any group.", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.8086324532826742}]}, {"text": "On the other hand, the best majority vote annotation is achieved in Group2, but again there is no statistically significant difference between any majority vote annotation of any group.", "labels": [], "entities": []}, {"text": "These results are surprising to us, since we do not see any significant difference in the quality of the data throughout different groups.", "labels": [], "entities": []}, {"text": "In this section, we evaluate all 26 workers and group them as either spammers or well-meaning workers.", "labels": [], "entities": []}, {"text": "All workers who deviate from the gold-standard by a", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Frequent label percentages for target words.", "labels": [], "entities": [{"text": "Frequent label percentages", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.8508850932121277}]}, {"text": " Table 3: Accuracy and kappa scores for each group of workers.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994159936904907}, {"text": "kappa scores", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9441592395305634}]}]}