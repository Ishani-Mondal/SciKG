{"title": [{"text": "Ontology driven content extraction using interlingual annotation of texts in the OMNIA project", "labels": [], "entities": [{"text": "Ontology driven content extraction", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6818500310182571}]}], "abstractContent": [{"text": "OMNIA is an ongoing project that aims to retrieve images accompanied with multilingual texts.", "labels": [], "entities": [{"text": "OMNIA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7796541452407837}]}, {"text": "In this paper, we propose a generic method (language and domain independent) to extract conceptual information from such texts and spontaneous user requests.", "labels": [], "entities": []}, {"text": "First, texts are labelled with interlingual annotation, then a generic extractor taking a domain on-tology as a parameter extract relevant conceptual information.", "labels": [], "entities": []}, {"text": "Implementation is also presented with a first experiment and preliminary results.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7188003063201904}]}], "introductionContent": [{"text": "The OMNIA project) aims to retrieve images that are described with multilingual free companion texts) in large Web datasets.", "labels": [], "entities": []}, {"text": "Images are first classified with formal descriptors in a lightweight ontology using automatic textual and visual analysis.", "labels": [], "entities": []}, {"text": "Then, users may express spontaneous queries in their mother tongue to retrieve images.", "labels": [], "entities": []}, {"text": "In order to build both formal descriptors and queries for the ontology, a content extraction in multilingual texts is required.", "labels": [], "entities": []}, {"text": "Multilingual content extraction does not imply translation.", "labels": [], "entities": [{"text": "Multilingual content extraction", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7523345947265625}, {"text": "translation", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.9669784903526306}]}, {"text": "It has been shown in) that annotating words or chunks with interlingual lexemes is a valid approach to initiate a content extraction.", "labels": [], "entities": [{"text": "content extraction", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.7878047823905945}]}, {"text": "We thus skip syntactical analysis, an expensive and low quality process, and get language-independent data early in our flow, allowing further treatments to be languageindependent.", "labels": [], "entities": [{"text": "syntactical analysis", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7511340975761414}]}, {"text": "We use the lightweight ontology for image classifications as the formal knowledge representation tha determines relevant information to extract.", "labels": [], "entities": [{"text": "image classifications", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7324653565883636}]}, {"text": "This ontology is considered as a domain parameter for the content extractor.", "labels": [], "entities": [{"text": "content extractor", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.6841520369052887}]}, {"text": "We are testing this method on a database provided for the image retrieval challenge CLEF09 by the Belgium press agency Belga.", "labels": [], "entities": [{"text": "image retrieval challenge CLEF09", "start_pos": 58, "end_pos": 90, "type": "TASK", "confidence": 0.7407425194978714}, {"text": "Belgium press agency Belga", "start_pos": 98, "end_pos": 124, "type": "DATASET", "confidence": 0.7625294476747513}]}, {"text": "The database contains 500K images with free companion texts of about 50 words (about 25M words in total).", "labels": [], "entities": []}, {"text": "The texts in the database are in English only, and we \"simulate\" multilinguism with partially post-edited machine translation.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follow.", "labels": [], "entities": []}, {"text": "We first depict our general architecture deployed for CLIA and then detail the various processes involved : interlingual annotation, conceptual vector based disambiguation and ontology driven content extraction.", "labels": [], "entities": [{"text": "content extraction", "start_pos": 192, "end_pos": 210, "type": "TASK", "confidence": 0.7551724314689636}]}, {"text": "We conclude with the first results of experimentations on the CLEF09 data.", "labels": [], "entities": [{"text": "CLEF09 data", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.9762056171894073}]}], "datasetContent": [{"text": "For a first experiment, we used a small dataset, containing: \u2022 a sub-corpus of 1046 English companion texts from CLEF09 corpus (press pictures and captions of about 50 words), \u2022 a 159 concepts ontology, designed for picture and emotions depiction, \u2022 a UW-concept map comprising 3099 UW.", "labels": [], "entities": [{"text": "CLEF09 corpus", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.9466269612312317}, {"text": "picture and emotions depiction", "start_pos": 216, "end_pos": 246, "type": "TASK", "confidence": 0.6342346668243408}]}, {"text": "It appeared that, with this parameters, concepts where extracted for only 25% of the texts.", "labels": [], "entities": []}, {"text": "This preliminary result stressed the importance of recall for such short texts.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9956316947937012}]}, {"text": "However, there were many ways to improve recall in the system: \u2022 improve the ontology, in order to better cover the press domain; \u2022 significantly increase the quantity of UW linked to concepts (only 3099 obtained for this experiment), by considering synonyms during the linking process; \u2022 using UW restrictions during concept matching for UW that are not directly linked to a concept, as these restrictions area rich source of refined semantic information.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9963584542274475}]}, {"text": "A second experiment with an improved ontology, including 732 concepts, and the use of UW restrictions, showed very promising results.", "labels": [], "entities": []}, {"text": "Concepts were retrieved from 77% of texts.", "labels": [], "entities": []}, {"text": "The remaining texts were very short (less than 10 words, sometime just date or name).", "labels": [], "entities": []}, {"text": "For example, we extracted the following concepts from the picture and companion text reproduced in figure 5.", "labels": [], "entities": []}, {"text": "As this results were more consistent, we could have a preliminary survey about precision, on a 30 texts sample.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9993163347244263}]}, {"text": "While disambiguation implementation is still at an early stage, weights were not yet taken into account.", "labels": [], "entities": [{"text": "disambiguation implementation", "start_pos": 6, "end_pos": 35, "type": "TASK", "confidence": 0.9683520197868347}]}, {"text": "A concept match can be considered correct following two criterons : 1.", "labels": [], "entities": []}, {"text": "Visual relevance considers a concept as correct if carried by an element of the picture; for instance, the match of concept \"SPORT\" is regarded as correct fora picture containing a minister of sports, even if not actually performing any sport.", "labels": [], "entities": [{"text": "Visual relevance", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7001178115606308}, {"text": "SPORT", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.7896480560302734}]}, {"text": "2. Textual relevance considers a concept as correct if carried by a word of the text, as parts of texts may involve concepts that are not actually present in the picture, such as contextual information, previous events, etc.", "labels": [], "entities": [{"text": "Textual relevance", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7575719654560089}]}, {"text": "124 concepts were found in 23 texts (7 texts had no concept match): 1. 99 concepts were correct according to the visual relevance, 2. 110 were correct according to the textual relevance, 3. 14 were totally incorrect.", "labels": [], "entities": []}, {"text": "We thus have an overall precision score of 0.798 according to the visual relevance and 0.895 according to the textual relevance.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.999421238899231}]}, {"text": "Most of the errors where caused by ambiguity problems, and maybe addressed with disambiguation process that are not fully implemented yet.", "labels": [], "entities": []}], "tableCaptions": []}