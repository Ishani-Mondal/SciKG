{"title": [{"text": "Joshua 2.0: A Toolkit for Parsing-Based Machine Translation with Syntax, Semirings, Discriminative Training and Other Goodies", "labels": [], "entities": [{"text": "Parsing-Based Machine Translation", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.7485664089520773}]}], "abstractContent": [{"text": "We describe the progress we have made in the past year on Joshua (Li et al., 2009a), an open source toolkit for parsing based machine translation.", "labels": [], "entities": [{"text": "parsing based machine translation", "start_pos": 112, "end_pos": 145, "type": "TASK", "confidence": 0.8665115535259247}]}], "introductionContent": [{"text": "Joshua is an open-source toolkit for parsing-based machine translation that is written in Java.", "labels": [], "entities": [{"text": "parsing-based machine translation", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.8861302336057028}]}, {"text": "The initial release of Joshua ( ) was a re-implementation of the Hiero system and all its associated algorithms, including: chart parsing, n-gram language model integration, beam and cube pruning, and k-best extraction.", "labels": [], "entities": [{"text": "Hiero system", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.9253485798835754}, {"text": "chart parsing", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.7712434828281403}, {"text": "n-gram language model integration", "start_pos": 139, "end_pos": 172, "type": "TASK", "confidence": 0.6320368051528931}, {"text": "k-best extraction", "start_pos": 201, "end_pos": 218, "type": "TASK", "confidence": 0.7353558540344238}]}, {"text": "The Joshua 1.0 release also included re-implementations of suffix array grammar extraction () and minimum error rate training.", "labels": [], "entities": [{"text": "suffix array grammar extraction", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.6737412735819817}, {"text": "minimum error rate training", "start_pos": 98, "end_pos": 125, "type": "METRIC", "confidence": 0.7735953703522682}]}, {"text": "Additionally, it included parallel and distributed computing techniques for scalability (.", "labels": [], "entities": []}, {"text": "This paper describes the additions to the toolkit over the past year, which together form the 2.0 release.", "labels": [], "entities": []}, {"text": "The software has been heavily used by the authors and several other groups in their daily research, and has been substantially refined since the first release.", "labels": [], "entities": []}], "datasetContent": [{"text": "Reproducing other researchers' machine translation experiments is difficult because the pipeline is too complex to fully detail in short conference papers.", "labels": [], "entities": [{"text": "Reproducing other researchers' machine translation", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.6176826775074005}]}, {"text": "We have put together a workflow framework for designing and running reproducible machine translation experiments using Joshua (Schwartz, under review).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.6911419183015823}]}, {"text": "Each step in the machine translation workflow (data preprocessing, grammar training, MERT, decoding, etc) is modeled by a Make script that defines how to run the tools used in that step, and an auxiliary configuration file that defines the exact parameters to be used in that step fora particular experimental setup.", "labels": [], "entities": [{"text": "machine translation workflow", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.8095636566480001}, {"text": "MERT", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.6605046987533569}]}, {"text": "Workflows configured using this framework allow a complete experiment to be run -from downloading data and software through scoring the final translated results -by executing a single Makefile.", "labels": [], "entities": []}, {"text": "This framework encourages researchers to supplement research publications with links to the complete set of scripts and configurations that were actually used to run the experiment.", "labels": [], "entities": []}, {"text": "The Johns Hopkins University submission for the WMT10 shared translation task was implemented in this framework, so it can be easily and exactly reproduced.", "labels": [], "entities": [{"text": "Johns Hopkins University submission", "start_pos": 4, "end_pos": 39, "type": "DATASET", "confidence": 0.8984516561031342}, {"text": "WMT10 shared translation task", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.6950312107801437}]}], "tableCaptions": []}