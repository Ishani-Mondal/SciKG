{"title": [{"text": "Annotating ESL Errors: Challenges and Rewards", "labels": [], "entities": [{"text": "Annotating ESL Errors", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.719909648100535}]}], "abstractContent": [{"text": "In this paper, we present a corrected and error-tagged corpus of essays written by non-native speakers of English.", "labels": [], "entities": []}, {"text": "The corpus contains 63000 words and includes data by learners of English of nine first language backgrounds.", "labels": [], "entities": []}, {"text": "The annotation was performed at the sentence level and involved correcting all errors in the sentence.", "labels": [], "entities": []}, {"text": "Error classification includes mistakes in preposition and article usage, errors in grammar, word order, and word choice.", "labels": [], "entities": [{"text": "Error classification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7699211537837982}, {"text": "word choice", "start_pos": 108, "end_pos": 119, "type": "TASK", "confidence": 0.7224440574645996}]}, {"text": "We show an analysis of errors in the annotated corpus by error categories and first language backgrounds, as well as inter-annotator agreement on the task.", "labels": [], "entities": []}, {"text": "We also describe a computer program that was developed to facilitate and standardize the annotation procedure for the task.", "labels": [], "entities": []}, {"text": "The program allows for the annotation of various types of mistakes and was used in the annotation of the corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Work on automated methods for detecting and correcting context dependent mistakes (e.g.,) has taken an interesting turnover the last few years, and has focused on correcting mistakes made by non-native speakers of English.", "labels": [], "entities": [{"text": "detecting and correcting context dependent mistakes", "start_pos": 30, "end_pos": 81, "type": "TASK", "confidence": 0.7778412501017252}, {"text": "correcting mistakes made by non-native speakers of English", "start_pos": 163, "end_pos": 221, "type": "TASK", "confidence": 0.8494091853499413}]}, {"text": "Nonnative writers make a variety of errors in grammar and word usage.", "labels": [], "entities": []}, {"text": "Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage;).", "labels": [], "entities": []}, {"text": "consider several error types, including article and preposition mistakes, made by Japanese learners of English, and focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers.", "labels": [], "entities": []}, {"text": "Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds), but there is no systematic study of a whole range of errors non-native writers produce, nor is it clear what the distribution of different types of mistakes is in learner language.", "labels": [], "entities": []}, {"text": "In this paper, we describe a corpus of sentences written by English as a Second Language (ESL) speakers, annotated for the purposes of developing an automated system for correcting mistakes in text.", "labels": [], "entities": []}, {"text": "Although the focus of the annotation were errors in article and preposition usage, all mistakes in the sentence have been corrected.", "labels": [], "entities": []}, {"text": "The data for annotation were taken from two sources: The International Corpus of Learner English (ICLE, () and Chinese Learners of English Corpus).", "labels": [], "entities": [{"text": "International Corpus of Learner English (ICLE", "start_pos": 57, "end_pos": 102, "type": "DATASET", "confidence": 0.9598405872072492}, {"text": "Chinese Learners of English Corpus", "start_pos": 111, "end_pos": 145, "type": "DATASET", "confidence": 0.768058979511261}]}, {"text": "The annotated corpus includes data from speakers of nine first language backgrounds.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first corpus of non-native English text (learner corpus) of fully-corrected sentences from such a diverse group of learners . The size of the annotated corpus is 63000 words, or 2645 sentences.", "labels": [], "entities": []}, {"text": "While a corpus of this size may not seem significant in many natural language applications, this is in fact a large corpus for this field, especially considering the effort to correct all mistakes, as opposed to focusing on one language phenomenon.", "labels": [], "entities": []}, {"text": "This corpus was used in the experiments described in the companion paper.", "labels": [], "entities": []}, {"text": "The annotation schema that we developed was motivated by our special interest in errors in article and preposition usage, but also includes errors in verbs, morphology, and noun number.", "labels": [], "entities": []}, {"text": "The corpus contains 907 article corrections and 1309 preposition corrections, in addition to annotated mistakes of other types.", "labels": [], "entities": []}, {"text": "While the focus of the present paper is on annotating ESL mistakes, we have several goals in mind.", "labels": [], "entities": []}, {"text": "First, we present the annotation procedure for the task, including an error classification schema, annotation speed, and inter-annotator agreement.", "labels": [], "entities": [{"text": "annotation speed", "start_pos": 99, "end_pos": 115, "type": "METRIC", "confidence": 0.7966382801532745}]}, {"text": "Second, we describe a computer program that we developed to facilitate the annotation of mistakes in text.", "labels": [], "entities": []}, {"text": "Third, having such a diverse corpus allows us to analyze the annotated data with respect to the source language of the learner.", "labels": [], "entities": []}, {"text": "We show the analysis of the annotated data through an overall breakdown of error types by the writer's first language.", "labels": [], "entities": []}, {"text": "We also present a detailed analysis of errors in article and preposition usage.", "labels": [], "entities": []}, {"text": "Finally, it should be noted that there are currently very few annotated learner corpora available.", "labels": [], "entities": []}, {"text": "Consequently, systems are evaluated on different data sets, which makes performance comparison impossible.", "labels": [], "entities": []}, {"text": "The annotation of the data presented here is available 2 and, thus, can be used by researchers who obtain access to these respective corpora . The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, we describe previous work on the annotation of learner corpora and statistics on ESL mistakes.", "labels": [], "entities": []}, {"text": "Section 3 gives a description of the annotation procedure, Section 4 presents the annotation tool that was developed for the purpose of this project and used in the annotation.", "labels": [], "entities": []}, {"text": "We then present error statistics based on the annotated corpus across all error types and separately for errors in article and preposition usage.", "labels": [], "entities": []}, {"text": "Finally, in Section 6 we describe how we", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Error statistics on the annotated data by source language and error type", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9880405068397522}]}, {"text": " Table 4: Distribution of article mistakes by error type and source language of the writer. Confusion error type refers to  confusing articles a and the. Multiple labels denotes cases where the annotator specified more than one article choice,  one of which was used by the learner. Other refers to confusing articles with possessive and demonstrative pronouns.", "labels": [], "entities": []}, {"text": " Table 7: Agreement at the sentence level. Agreement  shows how many sentences in each agreement set were  assigned to the same category (\"correct\", \"incorrect\") for  each of the two raters.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9267845749855042}]}]}