{"title": [{"text": "The Dynamics of Action Corrections in Situated Interaction", "labels": [], "entities": []}], "abstractContent": [{"text": "In spoken communications, correction utterances , which are utterances correcting other participants utterances and behaviors , play crucial roles, and detecting them is one of the key issues.", "labels": [], "entities": [{"text": "correction utterances", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.6245755106210709}]}, {"text": "Previously , much work has been done on automatic detection of correction utterances in human-human and human-computer dialogs , but they mostly dealt with the correction of erroneous utterances.", "labels": [], "entities": [{"text": "automatic detection of correction utterances", "start_pos": 40, "end_pos": 84, "type": "TASK", "confidence": 0.8220313370227814}]}, {"text": "However , in many real situations, especially in communications between humans and mobile robots, the misunderstandings manifest themselves not only through utterances but also through physical actions performed by the participants.", "labels": [], "entities": []}, {"text": "In this paper , we focus on action corrections and propose a classification of such utterances into Omission, Commission, and Degree corrections.", "labels": [], "entities": [{"text": "action corrections", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.733819380402565}, {"text": "Commission", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.5108290314674377}, {"text": "Degree corrections", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.47769932448863983}]}, {"text": "We present the results of our analysis of correction utterances in dialogs between two humans who were engaging in a kind of on-line computer game, where one participant plays the role of the remote manager of a convenience store, and the other plays the role of a robot store clerk.", "labels": [], "entities": []}, {"text": "We analyze the linguistic content, prosody as well as the timing of correction utterances and found that all features were significantly correlated with action corrections .", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent progress in robot technology made it a reality to have robots work in offices and homes, and spoken dialog is considered to be one of the most desired interface for such robots.", "labels": [], "entities": []}, {"text": "Our goal is to build a spoken dialog interface for robots that can move around in an office or a house and execute tasks according to humans' requests.", "labels": [], "entities": []}, {"text": "Building such spoken dialog interface for robots raises new problems different from those of traditional spoken/multimodal dialog systems.", "labels": [], "entities": []}, {"text": "The intentions behind human utterances may vary depending on the situation where the robot is and the situation changes continuously not only because the robot moves but also because humans and objects move, and human requests change.", "labels": [], "entities": []}, {"text": "In this sense human-robot interaction is situated.", "labels": [], "entities": []}, {"text": "Of the many aspects of situated interaction, we focus on the timing structure of interaction.", "labels": [], "entities": []}, {"text": "Although traditional spoken dialog systems deal with some timing issues such as turn-taking and handling barge-ins, timing structure in human-robot interaction is far more complex because the robot can execute physical actions and those actions can occur in parallel with utterances.", "labels": [], "entities": []}, {"text": "In this work we are concerned specifically with corrections in situated interaction.", "labels": [], "entities": []}, {"text": "In joint physical tasks, human corrective behavior, which allows to repair discrepancies in participants' mutual understanding, is tightly tied to actions.", "labels": [], "entities": [{"text": "human corrective behavior", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.7475680112838745}]}, {"text": "While past work on non-situated spoken dialog systems has shown the necessity and feasibility of detecting and handling corrections (), most of these models assume that corrections target past utterances and rely on a strict turn-based structure which is frequently violated in situated interaction.", "labels": [], "entities": []}, {"text": "When dialog is interleaved with physical actions, the specific timing of an utterance relative to other utterances and actions is more relevant than the turn sequence.", "labels": [], "entities": []}, {"text": "In this paper, we propose a classification of errors and corrections in physical tasks and analyze the properties of different types of corrections in the context of human-human task-oriented interactions in a virtual environment.", "labels": [], "entities": []}, {"text": "The next section gives some characteristics of corrections in situated interaction.", "labels": [], "entities": []}, {"text": "Section 3 describes our experi-Alice : Put it right above mental setup and data collection effort.", "labels": [], "entities": [{"text": "data collection", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.7359702885150909}]}, {"text": "Section 4 presents the results of our analysis of corrections in terms of timing, prosodic, and lexical features.", "labels": [], "entities": [{"text": "timing", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9892197251319885}]}, {"text": "These results are discussed in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiment, two human participants collaborate in order to perform certain tasks pertaining to the management of a small convenience store in a virtual world.", "labels": [], "entities": []}, {"text": "The two participants sit in different rooms, both facing a computer that presents a view of the virtual store.", "labels": [], "entities": []}, {"text": "One of the participants, the Operator (O) controls a (simulated) humanoid robot whose role is to answer all customer requests.", "labels": [], "entities": []}, {"text": "The other participant plays the role of a remote Manager (M) who sees the whole store but can only interact with O through speech.", "labels": [], "entities": []}, {"text": "shows the Operator and Manager views.", "labels": [], "entities": []}, {"text": "M can seethe whole store at anytime, including how many customers there are and where they are.", "labels": [], "entities": []}, {"text": "In addition, M knows when a particular customer has a request because the customer's character starts blinking (initially green, then yellow, then red, as time passes).", "labels": [], "entities": []}, {"text": "M's role is then to guide O towards the customers needing attention.", "labels": [], "entities": []}, {"text": "On the other hand, O sees the world through the \"eyes\" of the robot, whose vision is limited both in terms of field of view (90 degrees) and depth (degradation of vision with depth is produced by adding a virtual \"fog\" to the view).", "labels": [], "entities": []}, {"text": "When approaching a customer who has a pending request, O's view display the customer's request in the form of a caption.", "labels": [], "entities": []}, {"text": "1 O can act upon the virtual world by clicking on certain object such as items on the counter (to check them out), machines in the store (to repair them when needed), and various objects littering the floor (to clean them up).", "labels": [], "entities": []}, {"text": "Each action takes a certain amount of time to perform (between 3 and 45 seconds), indicated by a progress bar that decreases as O keeps the pointer on the target object and the left mouse button down.", "labels": [], "entities": [{"text": "progress bar", "start_pos": 97, "end_pos": 109, "type": "METRIC", "confidence": 0.9338837265968323}]}, {"text": "Once the counter goes to zero the action is completed and the participants receive 50 (for partially fulfilling a customer request) or 100 points (for completely fulfilling a request).", "labels": [], "entities": []}, {"text": "When the session begins, customers start entering the store at random intervals, with a maximum of 4 customers in the store at anytime.", "labels": [], "entities": []}, {"text": "Each customer follows one of 14 predefined scenarios, each involving between 1 and 5 requests.", "labels": [], "entities": []}, {"text": "Scenarios represent the customer's moves in terms of fixed way points.", "labels": [], "entities": []}, {"text": "As a simplification, we did not implement any reactive path planning.", "labels": [], "entities": []}, {"text": "Rather, the experimenter, sitting in a different room than either subject has the ability to temporarily take control of any customer to make them avoid obstacles.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Mean Z-score of prosodic features for dif- ferent correction classes.", "labels": [], "entities": [{"text": "Mean Z-score", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.842860072851181}]}, {"text": " Table 4: Mean ASR confidence score using class- specific LMs.", "labels": [], "entities": [{"text": "Mean ASR confidence score", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8415067791938782}]}, {"text": " Table 3: Keywords with highest mutual information with correction category.", "labels": [], "entities": [{"text": "correction", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9652929306030273}]}]}