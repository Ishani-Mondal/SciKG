{"title": [], "abstractContent": [{"text": "Opinion Mining aims to automatically acquire useful opinioned information and knowledge in subjective texts.", "labels": [], "entities": [{"text": "Opinion Mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8202421963214874}]}, {"text": "Research of Chinese Opin-ioned Mining requires the support of annotated corpus for Chinese opinioned-subjective texts.", "labels": [], "entities": []}, {"text": "To facilitate the work of corpus annotators, this paper implements an active learning based annotation tool for Chinese opinioned elements which can identify topic, sentiment, and opinion holder in a sentence automatically.", "labels": [], "entities": []}], "introductionContent": [{"text": "Opinion Mining is a novel and important research topic, aiming to automatically acquire useful opinioned information and knowledge in subjective texts (.", "labels": [], "entities": [{"text": "Opinion Mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8532513678073883}]}, {"text": "This technique has wide and many real world applications, such as e-commerce, business intelligence, information monitoring, public opinion poll, e-learning, newspaper and publication compilation, and business management.", "labels": [], "entities": [{"text": "information monitoring", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.7449052333831787}, {"text": "newspaper and publication compilation", "start_pos": 158, "end_pos": 195, "type": "TASK", "confidence": 0.5499447658658028}, {"text": "business management", "start_pos": 201, "end_pos": 220, "type": "TASK", "confidence": 0.8143521249294281}]}, {"text": "For instance, atypical opinion mining system produces statistical results from online product reviews, which can be used by potential customers when deciding which model to choose, by manufacturers to find out the possible areas of improvement, and by dealers for sales plan evaluation ( ).", "labels": [], "entities": []}, {"text": "According to, an opinion is composed of four parts, namely, topic, holder, sentiment, and claim, in which the holder expresses the claim including positive or negative sentiment towards the topic.", "labels": [], "entities": []}, {"text": "For example, in the sentence I like this car, I is the holder, like is the positive sentiment, car is the topic, and the whole sentence is the claim.", "labels": [], "entities": []}, {"text": "Research on Chinese opinion mining technology requires the support of annotated corpus for Chinese opinioned-subjective text.", "labels": [], "entities": [{"text": "Chinese opinion mining", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.5996948977311453}]}, {"text": "Since the corpus includes deep level information related to word segmentation, part-of-speech, syntax, semantics, opinioned elements, and some other information, the finished annotation is very complicated.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7145856469869614}]}, {"text": "Hence, it is necessary to develop an automatic tool to facilitate the work of annotators so that the efficiency and accuracy of annotation can be improved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9989780187606812}]}, {"text": "When developing the automatic annotation tool, we find it is most difficult for the tool to annotate opinioned elements automatically.", "labels": [], "entities": []}, {"text": "Because unlike other elements such as part-of-speech, and dependency relationship that needed to be annotated in the corpus, there is no available tool that can identify opinioned elements automatically.", "labels": [], "entities": []}, {"text": "Special classifiers should be constructed to solve this problem.", "labels": [], "entities": []}, {"text": "In traditional supervised learning tasks, training process consumes all the available annotated training instances, so a classifier with high classification accuracy might be constructed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.8962407112121582}]}, {"text": "When training a classifier for opinioned elements, it is very expensive and time-consuming to get annotated instances.", "labels": [], "entities": []}, {"text": "On the other hand, unannotated instances are abundant in this case, because all the texts in the corpus can be regarded as unannotated instances before being annotated.", "labels": [], "entities": []}, {"text": "This scenario is very appropriate for active learning application.", "labels": [], "entities": []}, {"text": "An active learning algorithm picks up the instances which will improve the performance of the classifier to the largest extent into the training set, and often produce classifier with higher accuracy using less training instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9932396411895752}]}, {"text": "Active learning algorithm is featured with smaller training set size, less influence from unbalanced training data and better classification performance comparing to classical learning algorithm.", "labels": [], "entities": []}, {"text": "This paper experimentally demonstrates the validity of active learning algorithm when used for opinioned elements identification and proposes a computational method for overall system performance evaluation which consists of Fmeasure, training time, and number of training instances.", "labels": [], "entities": [{"text": "opinioned elements identification", "start_pos": 95, "end_pos": 128, "type": "TASK", "confidence": 0.7557303309440613}, {"text": "Fmeasure", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.9935013651847839}]}], "datasetContent": [{"text": "To prove the validity of active learning algorithm and find out the relations between the performance of the classifiers and the way the classifiers are trained, we carried out batches of experiments.", "labels": [], "entities": []}, {"text": "In most information extraction tasks, a word and its context are considered a learning sample, and encoded as feature vectors.", "labels": [], "entities": [{"text": "information extraction tasks", "start_pos": 8, "end_pos": 36, "type": "TASK", "confidence": 0.8141204516092936}]}, {"text": "In our experiments, context data includes the part-of-speech tag, dependency relation, word semantic meaning, and word disambiguation information of the word being classified, its neighboring words and its parent word in dependency grammar.", "labels": [], "entities": []}, {"text": "Part-ofspeech tag and dependency relation are common features for Chinese Natural Language Processing (NLP) tasks . We get word semantic meaning from HowNet, which is an online commonsense knowledge base unveiling inter-conceptual relations and inter-attribute relations of concepts as connoting in lexicons of the Chinese and the English equivalents.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 150, "end_pos": 156, "type": "DATASET", "confidence": 0.9394738674163818}]}, {"text": "Given an occurrence of a word in natural language text, word sense disambiguation is the process of identifying which sense of the word is intended if the word has a number of distinct senses.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.6725091338157654}]}, {"text": "According to , this information may help in Chinese NLP tasks such as topic identification.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.9189950525760651}]}, {"text": "\ud97b\udf59\ud97b\udf59\ud97b\udf59Lack of explicit boundary between training instances and testing instances is a great difference between common machine learning algorithm and learning algorithm designed for corpus annotation.", "labels": [], "entities": []}, {"text": "For common machine learning algorithm such as human face recognition, the quantity of training instances is limited while the testing instances could be infinite.", "labels": [], "entities": [{"text": "face recognition", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.6767153888940811}]}, {"text": "It is unnecessary and impossible to annotate all the testing instances.", "labels": [], "entities": []}, {"text": "However, when annotating a corpus, all the texts need to be annotated are decided beforehand.", "labels": [], "entities": []}, {"text": "Although tools automated part of the annotation process, the results still need to be reviewed for several times to ensure the quality of annotation.", "labels": [], "entities": []}, {"text": "That means in an annotation scenario, all the data to be processed are available during the training stage.", "labels": [], "entities": []}, {"text": "\ud97b\udf59\ud97b\udf59\ud97b\udf59The raw texts used in our experiments are taken from forums of chinacars.com.", "labels": [], "entities": []}, {"text": "These texts include explicit subjective opinion and informal network language, which are necessary for opinion mining research.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.7809593081474304}]}, {"text": "Most of them are comments composed of one or more sentences on certain type of vehicle.", "labels": [], "entities": []}, {"text": "The detailed opinion elements distributions are showed in table 3.", "labels": [], "entities": []}, {"text": "\ud97b\udf59\ud97b\udf59\ud97b\udf59We use all the texts as testing data set and a subset of it as a training data set.", "labels": [], "entities": []}, {"text": "First of all, we pickup 10 instances for each class, and train a simple classification model with them.", "labels": [], "entities": []}, {"text": "Then, the baseline system picks up k instances in sequence and adds them into the training data set to train anew classification model iteratively until the training data set is as large as the testing data set, while the active learning system picks up instances according to the strategy in Chapter 3.3.", "labels": [], "entities": []}, {"text": "For active learning algorithm based on membership query, its training process will probably take longer time by the time the optimum classier is found, since the training process consists of several rounds of iteration.", "labels": [], "entities": []}, {"text": "At the beginning of the iteration, the classification speed of the model is much faster due to less training instances are used and the model is simple.", "labels": [], "entities": []}, {"text": "With more and more training instances are added into the training data set, the model will become more complex and more time will be needed for classifica-\ud97b\udf59 \ud97b\udf59    tion.", "labels": [], "entities": [{"text": "classifica-\ud97b\udf59 \ud97b\udf59    tion", "start_pos": 144, "end_pos": 166, "type": "TASK", "confidence": 0.8728786259889603}]}, {"text": "On account of the features of active learning algorithm, we believe it is necessary to find away to balance the performance of the classifier and the time it take in training process fora thorough evaluation of the algorithm.", "labels": [], "entities": []}, {"text": "\ud97b\udf59\ud97b\udf59\ud97b\udf59We define the measurement for time as: where C is the number of all the possible training instances available, k is the number of training instances added into the training data set in each round of iteration.", "labels": [], "entities": []}, {"text": "T is the approximate value of the inverse ratio of the time it takes for training process.", "labels": [], "entities": [{"text": "T", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.993115246295929}, {"text": "inverse ratio", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.9676244258880615}]}, {"text": "T will have a greater value if the training process takes less time.", "labels": [], "entities": [{"text": "T", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9755605459213257}]}, {"text": "Its range is (0, 1] just similar to F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.8121844530105591}]}, {"text": "\ud97b\udf59\ud97b\udf59\ud97b\udf59We define the measurement for the training instances used as: where n is the number of the training instances actually used.", "labels": [], "entities": []}, {"text": "K will have a greater value if less training instances are used in the training process.", "labels": [], "entities": [{"text": "K", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9402124881744385}]}, {"text": "The range of K is [0, 1).", "labels": [], "entities": []}, {"text": "\ud97b\udf59\ud97b\udf59\ud97b\udf59To judge the overall performance of an active learning algorithm, we consider the F-measure (F) of the classifier, the time it takes during the training process, and the training instances used.", "labels": [], "entities": [{"text": "F-measure (F)", "start_pos": 85, "end_pos": 98, "type": "METRIC", "confidence": 0.9730993360280991}]}, {"text": "We define the Active Learning Performance (ALP) as the harmonic mean of the three aspects:", "labels": [], "entities": [{"text": "Active Learning Performance (ALP)", "start_pos": 14, "end_pos": 47, "type": "METRIC", "confidence": 0.6740062038103739}]}], "tableCaptions": [{"text": " Table 1. They are calculated from the  decision function of the SVM gained from the  last round of iteration.", "labels": [], "entities": [{"text": "SVM", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8498296141624451}]}, {"text": " Table 1 Example of 2-Classification SVM  Predict Value", "labels": [], "entities": [{"text": "SVM", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9280961155891418}, {"text": "Predict", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.7113102674484253}]}, {"text": " Table 2 Example of 4-Classification SVM Decision  Process", "labels": [], "entities": [{"text": "SVM Decision  Process", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.9182349046071371}]}, {"text": " Table 3 Detailed Information of the Data Set", "labels": [], "entities": [{"text": "Detailed", "start_pos": 9, "end_pos": 17, "type": "TASK", "confidence": 0.9479541182518005}]}, {"text": " Table 4 F-measure Trends when k=200", "labels": [], "entities": [{"text": "F-measure Trends", "start_pos": 9, "end_pos": 25, "type": "METRIC", "confidence": 0.7359265387058258}]}, {"text": " Table 5 F-measure Trends when k=500\ud97b\udf59", "labels": [], "entities": [{"text": "F-measure Trends", "start_pos": 9, "end_pos": 25, "type": "METRIC", "confidence": 0.7659898400306702}]}, {"text": " Table 6 F-measure Trends when k=1000\ud97b\udf59", "labels": [], "entities": [{"text": "F-measure Trends", "start_pos": 9, "end_pos": 25, "type": "METRIC", "confidence": 0.7839282751083374}]}]}