{"title": [{"text": "Using Deep Belief Nets for Chinese Named Entity Categorization", "labels": [], "entities": [{"text": "Chinese Named Entity Categorization", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.5265874117612839}]}], "abstractContent": [{"text": "Identifying named entities is essential in understanding plain texts.", "labels": [], "entities": [{"text": "Identifying named entities", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9134343862533569}]}, {"text": "Moreover, the categories of the named entities are indicative of their roles in the texts.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach, Deep Belief Nets (DBN), for the Chinese entity mention categorization problem.", "labels": [], "entities": []}, {"text": "DBN has very strong representation power and it is able to elaborately self-train for discovering complicated feature combinations.", "labels": [], "entities": [{"text": "DBN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9707655310630798}]}, {"text": "The experiments conducted on the Automatic Context Extraction (ACE) 2004 data set demonstrate the effectiveness of DBN.", "labels": [], "entities": [{"text": "Automatic Context Extraction (ACE) 2004 data set", "start_pos": 33, "end_pos": 81, "type": "DATASET", "confidence": 0.7902510431077745}]}, {"text": "It outperforms the state-of-the-art learning models such as SVM or BP neural network.", "labels": [], "entities": [{"text": "BP neural network", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.7956039508183798}]}], "introductionContent": [{"text": "Named entities (NE) are defined as the names of existing objects, such as persons, organizations and etc.", "labels": [], "entities": [{"text": "Named entities (NE)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.591698843240738}]}, {"text": "Identifying NEs in plain texts provides structured information for semantic analysis.", "labels": [], "entities": [{"text": "Identifying NEs in plain texts", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8248145461082459}, {"text": "semantic analysis", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.8538630604743958}]}, {"text": "Hence the named entity recognition (NER) task is a fundamental task fora wide variety of natural language processing applications, such as question answering, information retrieval and etc.", "labels": [], "entities": [{"text": "named entity recognition (NER) task", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.8040709027222225}, {"text": "question answering", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.9093391597270966}, {"text": "information retrieval", "start_pos": 159, "end_pos": 180, "type": "TASK", "confidence": 0.8051615357398987}]}, {"text": "Ina text, an entity may either be referred to by a common noun, a noun phrase, or a pronoun.", "labels": [], "entities": []}, {"text": "Each reference of the entity is called a mention.", "labels": [], "entities": []}, {"text": "NER indeed requires the systems to identify these entity mentions from plain texts.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5898319482803345}]}, {"text": "The task can be decomposed into two sub-tasks, i.e., the identification of the entities in the text and the classification of the entities into a set of predefined categories.", "labels": [], "entities": []}, {"text": "In the study of this paper, we focus on the second sub-task and assume that the boundaries of all the entity mentions to be categorized are already correctly identified.", "labels": [], "entities": []}, {"text": "In early times, NER systems are mainly based on handcrafted rule-based approaches.", "labels": [], "entities": [{"text": "NER", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9762011766433716}]}, {"text": "Although rule-based approaches achieved reasonably good results, they have some obvious flaws.", "labels": [], "entities": []}, {"text": "First, they require exhausted handcraft work to construct a proper and complete rule set, which partially expressing the meaning of entity.", "labels": [], "entities": []}, {"text": "Moreover, once the interest of task is transferred to a different domain or language, rules have to be revised or even rewritten.", "labels": [], "entities": []}, {"text": "The discovered rules are indeed heavily dependent on the task interests and the particular corpus.", "labels": [], "entities": []}, {"text": "Finally, the manually-formatted rules are usually incomplete and their qualities are not guaranteed.", "labels": [], "entities": []}, {"text": "Recently, more attentions are switched to the applications of machine learning models with statistic information.", "labels": [], "entities": []}, {"text": "In this camp, entity categorization is typically cast as a multi-class classification process, where the named entities are represented by feature vectors.", "labels": [], "entities": [{"text": "entity categorization", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.7059409618377686}]}, {"text": "Usually, the vectors are abstracted by some lexical and syntactic features instead of semantic feature.", "labels": [], "entities": []}, {"text": "Many learning models, such as Support Vector Machine (SVM) and Neural Network (NN), are then used to classify the entities by their feature vectors.", "labels": [], "entities": []}, {"text": "Entity categorization in Chinese attracted less attention when compared to English or other western languages.", "labels": [], "entities": [{"text": "Entity categorization", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8557115793228149}]}, {"text": "This is mainly because the unique characteristics of Chinese.", "labels": [], "entities": []}, {"text": "One of the most common problems is the lack of boundary information in Chinese texts.", "labels": [], "entities": []}, {"text": "For this problem, character-based methods are reported to be a possible substitution of word-based methods.", "labels": [], "entities": []}, {"text": "As to character-based methods, it is important to study the implicit combination of characters.", "labels": [], "entities": []}, {"text": "In our study, we explore the use of Deep Belief Net (DBN) in character-based entity categorization.", "labels": [], "entities": []}, {"text": "DBN is a neural network model which is developed under the deep learning architecture.", "labels": [], "entities": [{"text": "DBN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8254304528236389}]}, {"text": "It is claimed to be able to automatically learn a deep hierarchy of the input features with increasing levels of abstraction for the complex problem.", "labels": [], "entities": []}, {"text": "In our problem, DBN is used to automatically discover the complicated composite effects of the characters to the NE categories from the input data.", "labels": [], "entities": []}, {"text": "With DBN, we need not to manually construct the character combination features for expressing the semantic relationship among characters in entities.", "labels": [], "entities": []}, {"text": "Moreover, the deep structure of DBN enables the possibility of discovering very sophisticated combinations of the characters, which may even be hard to discover by human.", "labels": [], "entities": [{"text": "DBN", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.7256150841712952}]}, {"text": "The rest of this paper is organized as follow.", "labels": [], "entities": []}, {"text": "Section 2 reviews the related work on name entity categorization.", "labels": [], "entities": [{"text": "name entity categorization", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7241831620534261}]}, {"text": "Section 3 introduces the methodology of the proposed approach.", "labels": [], "entities": []}, {"text": "Section 4 provides the experimental results.", "labels": [], "entities": []}, {"text": "Finally, section 5 concludes the whole paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiment, we use the ACE 2004 corpus to evaluate our approach.", "labels": [], "entities": [{"text": "ACE 2004 corpus", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.9838523666063944}]}, {"text": "The objective of this study is that the correctly detected Chinese entity mentions categorization using DBN from the text and figure out the suitability of DBN on this task.", "labels": [], "entities": []}, {"text": "Moreover, an entity mention should belong to one and only one category.", "labels": [], "entities": []}, {"text": "According to the guideline of the ACE04 task, there are five categories for consideration in total, i.e., Person, Organization, Geo-political entity, Location, and Facility.", "labels": [], "entities": []}, {"text": "Moreover, each entity mention is expressed in two forms, i.e., the head and the extent.", "labels": [], "entities": [{"text": "extent", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9718651175498962}]}, {"text": "For example, \u7f8e\u56fd\u603b\u7edf\u514b\u6797\u987f \"President Clinton of USA\" is the extent of an entity mention and \u514b \u6797 \u987f \"Clinton\" is the corresponding head.", "labels": [], "entities": []}, {"text": "The two phrases both point to a named entity whose name is Clinton and he is the president of USA.", "labels": [], "entities": [{"text": "USA", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.9775664210319519}]}, {"text": "Here we make the \"breakdown\" strategy mentioned in that only the entity head is considered to generate the feature vector, considering that the information from the entity head refines the name entity.", "labels": [], "entities": []}, {"text": "Although the entity extent includes more information, it also brings many noises which may make the learning process much more difficult.", "labels": [], "entities": []}, {"text": "In our experiments, we test the machine learning models under a 4-flod cross-validation.", "labels": [], "entities": []}, {"text": "All entity mentions are divided into four parts randomly where three parts are used for training and one for test.", "labels": [], "entities": []}, {"text": "In total, 7746 mentions are used for training and 2482 mentions are used for testing at each round.", "labels": [], "entities": []}, {"text": "Precision is chosen as the evaluation criterion, calculated by the proportion of the number of correctly categorized instances and the number of total instances.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9605926871299744}]}, {"text": "Since all the instances should be classified, the recall value is equal to the precision value.", "labels": [], "entities": [{"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9994670748710632}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9993307590484619}]}, {"text": "First of all, we provide some statistics of the data set.", "labels": [], "entities": []}, {"text": "The distribution of entity mentions in each category is given in  In this experiment, the DBN has three RBM layers and one BP layer.", "labels": [], "entities": [{"text": "BP", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.9570578336715698}]}, {"text": "And the numbers of units in each RBM layer are 900, 600 and 300 respectively.", "labels": [], "entities": []}, {"text": "NN (BP) has the same structure as DBN.", "labels": [], "entities": [{"text": "DBN", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.9263556599617004}]}, {"text": "As for SVM, we choose the linear kernel with the penalty parameter C=1 and set the other parameters as default after comparing different kernels and parameters.", "labels": [], "entities": []}, {"text": "In the results, DBN achieved better performance than both SVM and BP neural network.", "labels": [], "entities": []}, {"text": "This clearly proved the advantages of DBN.", "labels": [], "entities": [{"text": "DBN", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.7722563147544861}]}, {"text": "The deep architecture of DBN yields stronger representation power which makes it able to detect more complicated and efficient features, thus better performance is achieved.", "labels": [], "entities": []}, {"text": "In the second experiment, we intend to examine the performance of DBN with different number of RBM layers, from one RBM layer plus one BP layer to three RBM layers plus one BP layer.", "labels": [], "entities": []}, {"text": "The amount of the units in the first RBM layer is set 900 and the amount in the second RBM layer is 600, if the second layer exists.", "labels": [], "entities": []}, {"text": "As for the third RBM layers, the amount of units is set to 300.", "labels": [], "entities": []}, {"text": "show that the performance tends to be better when more RBM layers are incorporated.", "labels": [], "entities": []}, {"text": "More RBM layers do enhance the representation power of DBN.", "labels": [], "entities": []}, {"text": "However, it is also noted that the improvement is not significant from two layers to three layers.", "labels": [], "entities": []}, {"text": "The reason maybe that two-RBM DBN already has enough representation power for modeling this data set and thus one more RBM layer brings insignificant improvement.", "labels": [], "entities": []}, {"text": "It is also mentioned in that more than three RBM layers are indeed not necessary.", "labels": [], "entities": []}, {"text": "Another important result in is that the DBN with One RBM and one BP performs much better than the neutral network with only BP in.", "labels": [], "entities": []}, {"text": "This clearly showed the effectiveness of feature combination by the RBM layer again.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Number of entity mentions in each  category", "labels": [], "entities": []}, {"text": " Table 2. Performances of the systems with  different classification models", "labels": [], "entities": []}, {"text": " Table 3. Performance of DBNs with different  number s of RBM layers", "labels": [], "entities": []}, {"text": " Table 4. Performance of One-RBM DBNs  with different number of units", "labels": [], "entities": []}]}