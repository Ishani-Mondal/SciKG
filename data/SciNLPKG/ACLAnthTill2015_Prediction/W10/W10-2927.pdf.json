{"title": [{"text": "Inspecting the Structural Biases of Dependency Parsing Algorithms *", "labels": [], "entities": [{"text": "Structural Biases of Dependency Parsing Algorithms", "start_pos": 15, "end_pos": 65, "type": "TASK", "confidence": 0.7283003528912863}]}], "abstractContent": [{"text": "We propose the notion of a structural bias inherent in a parsing system with respect to the language it is aiming to parse.", "labels": [], "entities": []}, {"text": "This structural bias characterizes the behaviour of a parsing system in terms of structures it tends to under-and over-produce.", "labels": [], "entities": []}, {"text": "We propose a Boosting-based method for uncovering some of the structural bias inherent in parsing systems.", "labels": [], "entities": [{"text": "parsing", "start_pos": 90, "end_pos": 97, "type": "TASK", "confidence": 0.976668655872345}]}, {"text": "We then apply our method to four English dependency parsers (an Arc-Eager and Arc-Standard transition-based parsers, and first-and second-order graph-based parsers).", "labels": [], "entities": []}, {"text": "We show that all four parsers are biased with respect to the kind of annotation they are trained to parse.", "labels": [], "entities": []}, {"text": "We present a detailed analysis of the biases that highlights specific differences and commonalities between the parsing systems, and improves our understanding of their strengths and weaknesses.", "labels": [], "entities": [{"text": "parsing", "start_pos": 112, "end_pos": 119, "type": "TASK", "confidence": 0.9645925164222717}]}], "introductionContent": [{"text": "Dependency Parsing, the task of inferring a dependency structure over an input sentence, has gained a lot of research attention in the last couple of years, due in part to to the two CoNLL shared tasks () in which various dependency parsing algorithms were compared on various data sets.", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8456404209136963}, {"text": "dependency parsing", "start_pos": 222, "end_pos": 240, "type": "TASK", "confidence": 0.7175037264823914}]}, {"text": "As a result of this research effort, we have a choice of several robust, efficient and accurate parsing algorithms.", "labels": [], "entities": [{"text": "parsing", "start_pos": 96, "end_pos": 103, "type": "TASK", "confidence": 0.9688807129859924}]}, {"text": "These different parsing systems achieve comparable scores, yet produce qualitatively different parses.", "labels": [], "entities": [{"text": "parsing", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.9588501453399658}]}, {"text": "demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9650735259056091}]}, {"text": "explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser.", "labels": [], "entities": [{"text": "parser stacking", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8506394922733307}]}, {"text": "The stacking approach also produces more accurate parses.", "labels": [], "entities": [{"text": "stacking", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.9801715612411499}]}, {"text": "However, while we know how to produce accurate parsers and how to blend and stack their outputs, little effort was directed toward understanding the behavior of different parsing systems in terms of structures they produce and errors they make.", "labels": [], "entities": []}, {"text": "Question such as which linguistic phenomena are hard for parser Y? and what kinds of errors are common for parser Z?, as well as the more ambitious which parsing approach is most suitable to parse language X?, remain largely unanswered.", "labels": [], "entities": []}, {"text": "The current work aims to fill this gap by proposing a methodology to identify systematic biases in various parsing models and proposing and initial analysis of such biases.", "labels": [], "entities": []}, {"text": "analyze the difference between graph-based and transition-based parsers (specifically the MALT and MST parsers) by comparing the different kinds of errors made by both parsers.", "labels": [], "entities": []}, {"text": "They focus on single edge errors, and learn that MST is better for longer dependency arcs while MALT is better on short dependency arcs, that MALT is better than MST in predicting edges further from the root and vice-versa, that MALT has a slight advantage when predicting the parents of nouns and pronouns, and that MST is better at all other word categories.", "labels": [], "entities": [{"text": "predicting the parents of nouns and pronouns", "start_pos": 262, "end_pos": 306, "type": "TASK", "confidence": 0.8326918227331979}]}, {"text": "They also conclude that the greedy MALT Parser suffer from error propagation more than the globally optimized MST Parser.", "labels": [], "entities": []}, {"text": "In what follows, we complement their work by suggesting a different methodology of analysis of parsers behaviour.", "labels": [], "entities": []}, {"text": "Our methodology is based on the notion of structural bias of parsers, further explained in Section 2.", "labels": [], "entities": []}, {"text": "Instead of comparing two parsing systems in terms of the errors they produce, our analysis compares the output of a parsing system with a collection of gold-parsed trees, and searches for common structures which are predicted by the parser more often than they appear in the gold-trees or vice-versa.", "labels": [], "entities": []}, {"text": "These kinds of structures represent the bias of the parsing systems, and by analyzing them we can gain important insights into the strengths, weaknesses and inner working of the parser.", "labels": [], "entities": []}, {"text": "In Section 2.2 we propose a Boosting-based algorithm for uncovering these structural biases.", "labels": [], "entities": []}, {"text": "Then, in Section 3 we goon to apply our analysis methodology to four parsing systems for English: two transition-based systems and two graph-based systems (Sections 4 and 5).", "labels": [], "entities": []}, {"text": "The analysis shows that the different parsing systems indeed possess different biases.", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.9663718342781067}]}, {"text": "Furthermore, the analysis highlights the differences and commonalities among the different parsers, and sheds some more light on the specific behaviours of each system.", "labels": [], "entities": []}, {"text": "Recent work by, published concurrently with this one, aims to identify dependency errors in automatically parsed corpora by inspecting grammatical rules which appear in the automatically parsed corpora and do not fit well with the grammar learned from a manually annotated treebank.", "labels": [], "entities": []}, {"text": "While Dickinson's main concern is with automatic identification of errors rather than characterizing parsers behaviour, we feel that his work shares many intuitions with this one: automatic parsers fail in predictable ways, those ways can be analyzed, and this analysis should be carried out on structures which are larger than single edges, and by inspecting trends rather than individual decisions.", "labels": [], "entities": [{"text": "automatic identification of errors", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.7139784693717957}]}], "datasetContent": [{"text": "In what follows, we analyze and compare the structural biases of 4 parsers, with respect to a dependency representation of English.", "labels": [], "entities": []}, {"text": "Syntactic representation The dependency treebank we use is a conversion of the English WSJ treebank () to dependency structure using the procedure described in.", "labels": [], "entities": [{"text": "WSJ treebank", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.8955622315406799}]}, {"text": "We use the Mel'\u02c7 cuk encoding of coordination structure, in which the first conjunct is the head of the coordination structure, the coordinating conjunction depends on the head, and the second conjunct depend on the coordinating conjunction.", "labels": [], "entities": []}, {"text": "Data Sections 15-18 were used for training the parsers 3 . The first 4,000 sentences from sections 10-11 were used to train the Boosting algorithm and find structural predictors candidates.", "labels": [], "entities": []}, {"text": "Sections 4-7 were used as a validation set for ranking the structural predictors.", "labels": [], "entities": []}, {"text": "In all experiments, we used the gold-standard POS tags.", "labels": [], "entities": [{"text": "POS tags", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.7382770478725433}]}, {"text": "We binned the distance-to-parent values to 1,2,3,4-5,6-8 and 9+.", "labels": [], "entities": []}, {"text": "Parsers For graph-based parsers, we used the projective first-order (MST1) and secondorder (MST2) variants of the freely available MST parser).", "labels": [], "entities": []}, {"text": "For the transition-based parsers, we used the arc-eager (ARCE) variant of the freely available MALT parser), and our own implementation of an arcstandard parser (ARCS) as described in).", "labels": [], "entities": []}, {"text": "The unlabeled attachment accuracies of the four parsers are presented in.", "labels": [], "entities": []}, {"text": "Procedure For each parser, we train a boosting classifier to distinguish between the gold-standard trees and the parses produced for them by the  parser.", "labels": [], "entities": []}, {"text": "We remove from the training and validation sets all the sentences which the parser got 100% correct.", "labels": [], "entities": []}, {"text": "We then apply the models to the validation set.", "labels": [], "entities": []}, {"text": "We rank the learned predictors based on their appearances in gold-and parserproduced trees in the train and validation sets, and inspect the highest ranking predictors.", "labels": [], "entities": []}, {"text": "Training the boosting algorithm was done using the bact 6 toolkit.", "labels": [], "entities": [{"text": "bact 6 toolkit", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.8817958037058512}]}, {"text": "We ran 400 iterations of boosting, resulting in between 100 and 250 distinct subtrees in each model.", "labels": [], "entities": []}, {"text": "Of these, the top 40 to 60 ranked subtrees in each model were good indicators of structural bias.", "labels": [], "entities": []}, {"text": "Our wrapping code is available online 7 in order to ease the application of the method to other parsers and languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Unlabeled accuracies of the analyzed parsers", "labels": [], "entities": []}, {"text": " Table 2: Distinguishing parser output from gold-trees based  on structural information", "labels": [], "entities": [{"text": "Distinguishing parser", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9751305878162384}]}]}