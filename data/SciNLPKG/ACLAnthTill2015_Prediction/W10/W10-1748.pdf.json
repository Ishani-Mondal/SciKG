{"title": [{"text": "BBN System Description for WMT10 System Combination Task", "labels": [], "entities": [{"text": "BBN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7686797380447388}, {"text": "WMT10", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.6089136004447937}]}], "abstractContent": [{"text": "BBN submitted system combination outputs for Czech-English, German-English, Spanish-English, French-English, and All-English language pairs.", "labels": [], "entities": [{"text": "BBN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9646980166435242}]}, {"text": "All combinations were based on confusion network decoding.", "labels": [], "entities": []}, {"text": "An incremental hypothesis alignment algorithm with flexible matching was used to build the networks.", "labels": [], "entities": []}, {"text": "The bi-gram decoding weights for the single source language translations were tuned directly to maximize the BLEU score of the decoding output.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9775611162185669}]}, {"text": "Approximate expected BLEU was used as the objective function in gradient based optimization of the combination weights fora 44 system multi-source language combination (All-English).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9960668683052063}]}, {"text": "The system combination gained around 0.4-2.0 BLEU points over the best individual systems on the single source conditions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9995601773262024}]}, {"text": "On the multi-source condition, the system combination gained 6.6 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9993917942047119}]}], "introductionContent": [{"text": "The BBN submissions to the WMT10 system combination task were based on confusion network decoding.", "labels": [], "entities": [{"text": "BBN", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7871072888374329}, {"text": "WMT10 system combination task", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.620705634355545}]}, {"text": "The confusion networks were built using the incremental hypothesis alignment algorithm with flexible matching introduced in the BBN submission for the WMT09 system combination task (.", "labels": [], "entities": [{"text": "BBN submission", "start_pos": 128, "end_pos": 142, "type": "DATASET", "confidence": 0.958058625459671}, {"text": "WMT09 system combination task", "start_pos": 151, "end_pos": 180, "type": "TASK", "confidence": 0.6941825747489929}]}, {"text": "This year, the system combination weights were tuned to maximize the BLEU score () of the 1-best decoding output (lattice based BLEU tuning) using downhill simplex method.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9853667616844177}, {"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9901162981987}]}, {"text": "A 44 system multi-source combination was also submitted.", "labels": [], "entities": []}, {"text": "Since the gradient-free optimization algorithms do not seem to be able to handle more than 20-30 weights, a gradient ascent to maximize an approximate expected BLEU objective was used to optimize the larger number of weights.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9979727864265442}]}, {"text": "The lattice based BLEU tuning maybe implemented using any optimization algorithm that does not require the gradient of the objective function.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9889655113220215}]}, {"text": "Due to the size of the lattices, the objective function evaluation may have to be distributed to multiple servers.", "labels": [], "entities": []}, {"text": "The optimizer client accumulates the BLEU statistics of the 1-best hypotheses from the servers forgiven search weights, computes the final BLEU score, and passes it to the optimization algorithm which returns anew set of search weights.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9978265166282654}, {"text": "BLEU score", "start_pos": 139, "end_pos": 149, "type": "METRIC", "confidence": 0.9752339422702789}]}, {"text": "The lattice based tuning explores the entire search space and does not require multiple decoding iterations with N -best list merging to approximate the search space as in the standard minimum error rate training.", "labels": [], "entities": []}, {"text": "This allows much faster turnaround in weight tuning.", "labels": [], "entities": [{"text": "weight tuning", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.7188458442687988}]}, {"text": "Differentiable approximations of BLEU have been proposed for consensus decoding.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9965317845344543}, {"text": "consensus decoding", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.886224776506424}]}, {"text": "used a linear approximation and used a closer approximation called CoBLEU.", "labels": [], "entities": [{"text": "CoBLEU", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9033744931221008}]}, {"text": "CoBLEU is based on the BLEU formula but the n-gram counts are replaced by expected counts over a translation forest.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9964878559112549}]}, {"text": "Due to the min-functions required in converting the n-gram counts to matches and a non-differentiable brevity penalty, a sub-gradient ascent must be used.", "labels": [], "entities": []}, {"text": "In this work, an approximate expected BLEU (Exp-BLEU) defined over N -best lists was used as a differentiable objective function.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9898844957351685}]}, {"text": "ExpBLEU uses expected BLEU statistics where the min-function is not needed as the statistics are computed offline and the brevity penalty is replaced by a differentiable approximation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9975528120994568}]}, {"text": "The ExpBLEU tuning yields comparable results to direct BLEU tuning using gradient-free algorithms on combinations of small number of systems (fewer than 20-30 weights).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9807345271110535}]}, {"text": "Results on a 44 system combination show that the gradient based optimization is more robust with larger number of weights.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews the incremental hypothesis alignment algorithm used to built the confusion networks.", "labels": [], "entities": []}, {"text": "Decoding weight optimization using direct lattice 1-best BLEU tuning and N -best list based Exp-BLEU tuning are presented in Section 3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9438797831535339}]}, {"text": "Experimental results on combining single source language to English outputs and all 44 English outputs are detailed in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes this paper with some ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "System outputs for all language pairs with English as the target were combined.", "labels": [], "entities": []}, {"text": "Unpruned English bi-gram and 5-gram language model components were trained using the WMT10 corpora: EuroParl, GigaFrEn, NewsCommentary, and News.", "labels": [], "entities": [{"text": "WMT10 corpora", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.9540807008743286}, {"text": "EuroParl", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.8875378370285034}, {"text": "GigaFrEn", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.8308139443397522}, {"text": "NewsCommentary", "start_pos": 120, "end_pos": 134, "type": "DATASET", "confidence": 0.9044859409332275}, {"text": "News", "start_pos": 140, "end_pos": 144, "type": "DATASET", "confidence": 0.9496351480484009}]}, {"text": "Additional six Gigaword v4 components were trained: AFP, APW, XIN+CNA, LTW, NYT, and Headlines+Datelines.", "labels": [], "entities": [{"text": "AFP", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.7677165865898132}, {"text": "NYT", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.5670442581176758}]}, {"text": "Interpolation weights for the ten components were tuned so as to minimize perplexity on the newstest2009-ref.en development set.", "labels": [], "entities": [{"text": "newstest2009-ref.en development set", "start_pos": 92, "end_pos": 127, "type": "DATASET", "confidence": 0.9252733588218689}]}, {"text": "The LMs used modified Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "On the multi-source condition (xx-en) another LM was trained from the system outputs and interpolated with the general LM using an interpolation weight 0.3 for the LM trained on the system outputs.", "labels": [], "entities": []}, {"text": "This LM is referred to as biasLM later.", "labels": [], "entities": [{"text": "biasLM later", "start_pos": 26, "end_pos": 38, "type": "METRIC", "confidence": 0.9489525854587555}]}, {"text": "A tri-gram true casing model was trained using all available English data.", "labels": [], "entities": []}, {"text": "This model was used to restore the case of the lower-case system combination output.", "labels": [], "entities": []}, {"text": "All six 1-best system outputs on cz-en, 16 outputs on de-en, 8 outputs on es-en, and 14 outputs on fr-en were combined.", "labels": [], "entities": []}, {"text": "The lattice based BLEU tuning was used to optimize the bi-gram decoding weights and N-best list based BLEU tuning was used to optimize the 5-gram rescoring weights.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.994683563709259}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9876766204833984}]}, {"text": "Results for these single source language experiments are shown in.", "labels": [], "entities": []}, {"text": "The gains on syscombtune were similar to those on syscombtest for all but French-English.", "labels": [], "entities": []}, {"text": "The tuning set contained only 455 segments but appeared to be well matched with the larger (2034 segments) test set.", "labels": [], "entities": []}, {"text": "The characteristics of the individual system outputs were probably different for the tuning and test sets on French-English translation.", "labels": [], "entities": []}, {"text": "In our experience, optimizing system combination weights using the ExpBLEU tuning fora small number of systems yields similar results to lattice based BLEU tuning.", "labels": [], "entities": [{"text": "ExpBLEU", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.622562050819397}, {"text": "BLEU", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9641095399856567}]}, {"text": "The lattice based BLEU tuning is faster as there is no need for multiple decoding and tuning iterations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9895437955856323}]}, {"text": "Using the biasLM on the single source combinations did not  yield any gains.", "labels": [], "entities": [{"text": "biasLM", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9863291382789612}]}, {"text": "The output for these conditions probably did not contain enough data for biasLM training given the small tuning set and small number of systems.", "labels": [], "entities": [{"text": "biasLM training", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.9005424976348877}]}, {"text": "Finally, experiments combining all 44 1-best system outputs were performed to produce a multi-source combination output.", "labels": [], "entities": []}, {"text": "The first experiment used the lattice based BLEU tuning and gave a 5.6 BLEU point gain on the tuning set as seen in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9979658126831055}, {"text": "BLEU point gain", "start_pos": 71, "end_pos": 86, "type": "METRIC", "confidence": 0.9759788711865743}]}, {"text": "The ExpBLEU tuning gave an additional 1.2 point gain which suggests that the direct lattice based BLEU tuning got stuck in a local optimum.", "labels": [], "entities": [{"text": "ExpBLEU", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.6594154238700867}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.997296154499054}]}, {"text": "Using the system output biased LM gave an additional 0.7 point gain.", "labels": [], "entities": []}, {"text": "The gains on the test set were similar and the best combination gave a 6.6 point gain over the best individual system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Case insensitive TER and BLEU scores on syscombtune (tune) and syscombtest (test)  for combinations of outputs from four source languages.", "labels": [], "entities": [{"text": "TER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.7701709270477295}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9956798553466797}]}, {"text": " Table 2: Case insensitive TER and BLEU scores  on syscombtune (tune) and syscombtest  (test) for xx-en combination. Combinations us- ing lattice BLEU tuning, expected BLEU tuning,  and after adding the system output biased LM are  shown.", "labels": [], "entities": [{"text": "TER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9862762689590454}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.99647456407547}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9876270890235901}, {"text": "BLEU tuning", "start_pos": 168, "end_pos": 179, "type": "METRIC", "confidence": 0.9644140899181366}]}]}