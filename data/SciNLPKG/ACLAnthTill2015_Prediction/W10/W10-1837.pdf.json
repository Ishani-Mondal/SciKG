{"title": [{"text": "PackPlay: Mining semantic data in collaborative games", "labels": [], "entities": []}], "abstractContent": [{"text": "Building training data is labor-intensive and presents a major obstacle to advancing machine learning technologies such as machine translators, named entity recog-nizers (NER), part-of-speech taggers, etc.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 177, "end_pos": 199, "type": "TASK", "confidence": 0.7481958866119385}]}, {"text": "Training data are often specialized fora particular language or Natural Language Processing (NLP) task.", "labels": [], "entities": []}, {"text": "Knowledge captured by a specific set of training data is not easily transferable, even to the same NLP task in another language.", "labels": [], "entities": []}, {"text": "Emerging technologies, such as social networks and serious games, offer a unique opportunity to change how we construct training data.", "labels": [], "entities": []}, {"text": "While collaborative games have been used in information retrieval, it is an open issue whether users can contribute accurate annotations in a collaborative game context fora problem that requires an exact answer, such as games that would create named entity recognition training data.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.7675058543682098}, {"text": "entity recognition training", "start_pos": 251, "end_pos": 278, "type": "TASK", "confidence": 0.7932433485984802}]}, {"text": "We present PackPlay, a collaborative game framework that empirically shows players' ability to mimic annotation accuracy and thoroughness seen in gold standard annotated corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9392345547676086}, {"text": "thoroughness", "start_pos": 125, "end_pos": 137, "type": "METRIC", "confidence": 0.9502343535423279}]}], "introductionContent": [{"text": "Annotated corpora are sets of structured text used in Natural Language Processing (NLP) that contain supplemental knowledge, such as tagged parts-of-speech, semantic concepts assigned to phrases, or semantic relationships between these concepts.", "labels": [], "entities": []}, {"text": "Machine Learning (ML) is a subfield of Artificial Intelligence that studies how computers can obtain knowledge and create predictive models.", "labels": [], "entities": [{"text": "Machine Learning (ML)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8406888365745544}]}, {"text": "These models require annotated corpora to learn rules and patterns.", "labels": [], "entities": []}, {"text": "However, these annotated corpora must be manually curated for each domain or task, which is labor intensive and tedious, thereby creating a bottleneck for advancing ML and NLP prediction tools.", "labels": [], "entities": [{"text": "ML and NLP prediction", "start_pos": 165, "end_pos": 186, "type": "TASK", "confidence": 0.7461935430765152}]}, {"text": "Furthermore, knowledge captured by a specific annotated corpus is often not transferable to another task, even to the same NLP task in another language.", "labels": [], "entities": []}, {"text": "Domain and language specific corpora are useful for many language technology applications, including named entity recognition (NER), machine translation, spelling correction, and machine-readable dictionaries.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 101, "end_pos": 131, "type": "TASK", "confidence": 0.764918585618337}, {"text": "machine translation", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7967094480991364}, {"text": "spelling correction", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.9346725642681122}]}, {"text": "The An Cr\u00fabad\u00e1n Project, for example, has succeeded in creating corpora for more than 400 of the world's 6000+ languages by Web crawling.", "labels": [], "entities": [{"text": "An Cr\u00fabad\u00e1n Project", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.917422334353129}]}, {"text": "With a few exceptions, most of the 400+ corpora, however, lack any linguistic annotations due to the limitations of annotation tools).", "labels": [], "entities": []}, {"text": "Despite the many documented advantages of annotated data over raw data, there is a dearth of annotated corpora in many domains.", "labels": [], "entities": []}, {"text": "The majority of previous corpus annotation efforts relied on manual annotation by domain experts, automated prediction tagging systems, and hybrid semi-automatic systems that used both approaches.", "labels": [], "entities": [{"text": "corpus annotation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7054384648799896}, {"text": "prediction tagging", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.6779177188873291}]}, {"text": "While yielding high quality and enormously valuable corpora, manually annotating corpora can be prohibitively costly and time consuming.", "labels": [], "entities": []}, {"text": "For example, the GENIA corpus contains 9,372 sentences, curated by five part-time annotators, one senior coordinator, and one junior coordinator over 1.5 years (.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9362892210483551}]}, {"text": "Semiautomatic approaches decrease human effort but often introduce significant error, while still requiring human interaction.", "labels": [], "entities": []}, {"text": "The Web can help facilitate semi-automatic approaches by connecting distributed human users at a previously unfathomable scale and presents an opportunity to expand annotation efforts to countless users using Human Computation, the concept of outsourcing certain computational processes to humans, generally to solve problems that are intractable or difficult for computers.", "labels": [], "entities": []}, {"text": "This concept is demonstrated in our previous work,) and BioDEAL ( , which allows users to annotate Web documents through a Web browser plugin for the purposes of creating linguistically and biologically tagged annotated corpora and with micro-tasking via Mechanical Turk, which allows fora low cost option for manual labor tasks.", "labels": [], "entities": [{"text": "BioDEAL", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.591882586479187}]}, {"text": "While the Web and Human Computation maybe a powerful tandem for generating data and solving difficult problems, in order to succeed, users must be motivated to participate.", "labels": [], "entities": []}, {"text": "Humans have been fascinated with games for centuries and play them for many reasons, including for entertainment, honing skills, and gaining knowledge).", "labels": [], "entities": []}, {"text": "Every year, a large amount of hours are spent playing online computer games.", "labels": [], "entities": []}, {"text": "The games range form simple card and word games to more complex 3-D world games.", "labels": [], "entities": []}, {"text": "One such site for word, puzzle, and card games is Pogo.com 1 . According to protrackr, 2 Pogo has almost 6 million unique visitors a day.", "labels": [], "entities": []}, {"text": "Alexa.com shows that the average user is on the site for 11 minutes at a time.", "labels": [], "entities": [{"text": "Alexa.com", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9612841010093689}]}, {"text": "When the average time spent on the site is propagated to each user, the combined time is equal to more than 45,000 days of human time.", "labels": [], "entities": []}, {"text": "Arguably if, the games on Pogo were used to harvest useful data, various fields of Computer Science research could be advanced.", "labels": [], "entities": [{"text": "Pogo", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.9213318824768066}]}, {"text": "There has been a recent trend to leverage human's fascination in game playing to solve difficult problems through Human Computation.", "labels": [], "entities": []}, {"text": "Two such games include ESP and Google's Image Labeler (), in which players annotate images in a cooperative environment to correctly match image tags with their partner.", "labels": [], "entities": []}, {"text": "Semantic annotation has also been addressed in the game Phrase Detectives (, which has the goal of creating large scale training data for anaphora resolution.", "labels": [], "entities": [{"text": "Semantic annotation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8429945111274719}, {"text": "Phrase Detectives", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7187530994415283}, {"text": "anaphora resolution", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.7222685217857361}]}, {"text": "These types of games are part of a larger, serious games, initiative how collaborative online gaming can affect annotation throughput and annotation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.8983186483383179}]}, {"text": "There are two main questions for such systems: first, will overall throughput increase compared to traditional methods of annotating, such as the manual construction of the Genia Corpus?", "labels": [], "entities": [{"text": "Genia Corpus", "start_pos": 173, "end_pos": 185, "type": "DATASET", "confidence": 0.6498591750860214}]}, {"text": "Second, how accurate are the collective annotations?", "labels": [], "entities": []}, {"text": "A successful human computation environment, such as PackPlay, would represent a paradigm shift in the way annotated corpora are created.", "labels": [], "entities": []}, {"text": "However, adoption of such a framework cannot be expected until these questions are answered.", "labels": [], "entities": []}, {"text": "We address both of these questions in multiple games in our PackPlay system through evaluation of the collective players' annotations with precision and recall to judge accuracy of players' annotations and the number of games played to judge throughput.", "labels": [], "entities": [{"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9986735582351685}, {"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9984862804412842}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9980131387710571}]}, {"text": "We show improvements in both areas over traditional annotation methods and show accuracy comparable to expert prediction systems that could be used for semi-supervised annotation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9994335770606995}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics returned from our user study for  the game Entity Discovery  Statistic  Total Mean  # of games  29  3.62  # of annotations 291 40.85", "labels": [], "entities": []}, {"text": " Table 2: Recall and precision for Entity Discovery  annotations of CoNLL data.  Per Loc Org Avg CoNLL  Avg  Recall  (All Data) 0.94 0.95 0.85 0.9  0.82  Precision  (All Data) 0.47 0.70 0.53 0.62  0.83", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9894183278083801}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.999584972858429}, {"text": "Entity Discovery  annotations", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.7911247412363688}, {"text": "CoNLL data", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.783571869134903}, {"text": "Per Loc Org Avg CoNLL  Avg  Recall", "start_pos": 81, "end_pos": 115, "type": "METRIC", "confidence": 0.5169117280415126}]}, {"text": " Table 3: Precision for Entity Discovery annota- tions of CoNLL data with filtering", "labels": [], "entities": [{"text": "CoNLL data", "start_pos": 58, "end_pos": 68, "type": "DATASET", "confidence": 0.9322847723960876}]}, {"text": " Table 4: Statistics returned from our user study for  the game Name That Entity  Statistic  Total Mean  # of games  20  2.85  # of annotations 195 27.85", "labels": [], "entities": []}, {"text": " Table 5: Types of annotations generated by Name  That Entity  Error  Count  Annotations  195  Unique Annotations  141  Conflicts  38  Unique Conflicts  35", "labels": [], "entities": [{"text": "Error  Count  Annotations", "start_pos": 63, "end_pos": 88, "type": "METRIC", "confidence": 0.8516978621482849}]}]}