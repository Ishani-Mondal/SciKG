{"title": [], "abstractContent": [{"text": "We present the MT-NCD and MT-mNCD machine translation evaluation metrics as submission to the machine translation evaluation shared task (MetricsMATR 2010).", "labels": [], "entities": [{"text": "MT-NCD", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.7898935079574585}, {"text": "MT-mNCD machine translation evaluation", "start_pos": 26, "end_pos": 64, "type": "TASK", "confidence": 0.7500776201486588}, {"text": "machine translation evaluation shared task", "start_pos": 94, "end_pos": 136, "type": "TASK", "confidence": 0.824392557144165}]}, {"text": "The metrics are based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and evaluated against human judgments from the WMT08 shared task.", "labels": [], "entities": [{"text": "normalized compression distance (NCD)", "start_pos": 25, "end_pos": 62, "type": "METRIC", "confidence": 0.7741830597321192}, {"text": "WMT08 shared task", "start_pos": 173, "end_pos": 190, "type": "DATASET", "confidence": 0.7418779532114664}]}, {"text": "The experiments show that 1) our metric improves correlation to human judgments by using flexible matching , 2) segment replication is effective, and 3) our NCD-inspired method for multiple references indicates improved results.", "labels": [], "entities": [{"text": "segment replication", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8629684150218964}]}, {"text": "Generally, the proposed MT-NCD and MT-mNCD methods correlate competitively with human judgments compared to commonly used machine translations evaluation metrics, for instance, BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 177, "end_pos": 181, "type": "METRIC", "confidence": 0.8660465478897095}]}], "introductionContent": [{"text": "The quality of automatic machine translation (MT) evaluation metrics plays an important role in the development of MT systems.", "labels": [], "entities": [{"text": "automatic machine translation (MT) evaluation", "start_pos": 15, "end_pos": 60, "type": "TASK", "confidence": 0.8380746671131679}, {"text": "MT", "start_pos": 115, "end_pos": 117, "type": "TASK", "confidence": 0.9945671558380127}]}, {"text": "Human evaluation would no longer be necessary if automatic MT metrics correlated perfectly with manual judgments.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9844282269477844}]}, {"text": "Besides high correlation with human judgments of translation quality, a good metric should be language independent, fast to compute and sensitive enough to reliably detect small improvements in MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 194, "end_pos": 196, "type": "TASK", "confidence": 0.9858161211013794}]}, {"text": "Recently there have been some experiments with normalized compression distance (NCD) as a method for automatic evaluation of machine translation.", "labels": [], "entities": [{"text": "normalized compression distance (NCD)", "start_pos": 47, "end_pos": 84, "type": "METRIC", "confidence": 0.7109843641519547}, {"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7875032126903534}]}, {"text": "NCD is a general string similarity measure that has been useful for clustering in various tasks).", "labels": [], "entities": []}, {"text": "Parker introduced BADGER, a machine translation evaluation metric that uses NCD together with a language independent word normalization method.", "labels": [], "entities": [{"text": "BADGER", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9776043891906738}, {"text": "machine translation evaluation", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.8512284755706787}, {"text": "language independent word normalization", "start_pos": 96, "end_pos": 135, "type": "TASK", "confidence": 0.6411858201026917}]}, {"text": "independently applied NCD to the direct evaluation of translations.", "labels": [], "entities": []}, {"text": "He showed with a small corpus of three language pairs that the scores of NCD and METEOR (v0.6) from translations of 10-12 MT systems were highly correlated.", "labels": [], "entities": [{"text": "NCD", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8684591054916382}, {"text": "METEOR", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9636784791946411}]}, {"text": "have extended the work by showing that NCD can be used to rank translations of different MT systems so that the ranking order correlates with human rankings at the same level as BLEU ().", "labels": [], "entities": [{"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9502297043800354}, {"text": "BLEU", "start_pos": 178, "end_pos": 182, "type": "METRIC", "confidence": 0.9976089000701904}]}, {"text": "For translations into English, NCD had an overall systemlevel correlation of 0.66 whereas the best method, ULC had an overall correlation of 0.76, and BLEU had an overall correlation of 0.65.", "labels": [], "entities": [{"text": "NCD", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.9125449657440186}, {"text": "BLEU", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9888193011283875}]}, {"text": "NCD presents a viable alternative to the de facto standard BLEU.", "labels": [], "entities": [{"text": "NCD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9616401195526123}, {"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9354153275489807}]}, {"text": "Both metrics are language independent, simple and efficient to compute.", "labels": [], "entities": []}, {"text": "However, NCD is a general measure of similarity that has been applied in many domains.", "labels": [], "entities": [{"text": "similarity", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9498751759529114}]}, {"text": "More advanced methods achieve better correlation with human judgments, but typically use additional language specific linguistic resources.", "labels": [], "entities": []}, {"text": "experimented with relaxed word matching, adding language specific resources to NCD.", "labels": [], "entities": [{"text": "word matching", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.794384628534317}]}, {"text": "The metric called mNCD, which works similarly to mBLEU, showed improved correlation to human judgments in English, the only language where a METEOR synonym module was used.", "labels": [], "entities": []}, {"text": "The motivation for this challenge submission is to evaluate the MT-NCD and MT-mNCD metric performance in an open competition with state-of-the-art MT evaluation metrics.", "labels": [], "entities": [{"text": "MT-NCD", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.7978721261024475}, {"text": "MT evaluation", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.9038814008235931}]}, {"text": "Our experiments and submission build on NCD and mNCD.", "labels": [], "entities": [{"text": "NCD", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.9661225080490112}, {"text": "mNCD", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.9124739766120911}]}, {"text": "We expand NCD to handle multiple references and report experimental results for replicating segments as a preprocessing step that improves the NCD as an MT evaluation metric.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 153, "end_pos": 166, "type": "TASK", "confidence": 0.8964561223983765}]}], "datasetContent": [{"text": "NCD-based MT evaluation metrics build on the idea that a string x is similar to another stringy, when both share common substrings.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.7933602333068848}]}, {"text": "When describing y, common substrings do not have to be repeated, but can be referenced to x.", "labels": [], "entities": []}, {"text": "This is done when compressing the concatenation of x and y, which results in smaller output when more information of y is already included in x.", "labels": [], "entities": []}, {"text": "We chose parameters and evaluated our metrics using the WMT08 part of the MetricsMATR 2010 development data, which contains human judgments of the 2008 ACL Workshop on Statistical Machine Translation for translations from a total of 30 MT systems between English and five other European languages.", "labels": [], "entities": [{"text": "WMT08 part of the MetricsMATR 2010 development data", "start_pos": 56, "end_pos": 107, "type": "DATASET", "confidence": 0.9078361392021179}, {"text": "ACL Workshop on Statistical Machine Translation", "start_pos": 152, "end_pos": 199, "type": "TASK", "confidence": 0.5029470721880595}]}, {"text": "There are human evaluations and several automatic evaluations for the translations, divided into several tasks defined by the language pair and the domain of the translated sentences.", "labels": [], "entities": []}, {"text": "For each of these tasks, the WMT08 data contains about 2 000 reference sentences (segments) plus their aligned translations for 12 to 17 different translation systems, depending on the language pair.", "labels": [], "entities": [{"text": "WMT08 data", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.912954717874527}]}, {"text": "The human judgments include three categories which contain evaluations for at most one segment at a time, not whole documents.", "labels": [], "entities": []}, {"text": "In the RANK category, humans had to rank the output of five MT systems according to quality.", "labels": [], "entities": [{"text": "RANK", "start_pos": 7, "end_pos": 11, "type": "TASK", "confidence": 0.755461573600769}, {"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9794705510139465}]}, {"text": "The CONST category contains rankings for short phrases (constituents), and the YES/NO category contains binary answers to judge if a short phrase is an acceptable translation or not.", "labels": [], "entities": [{"text": "YES/NO", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.8410595059394836}]}, {"text": "We report RANK, CONST and YES/NO system level correlations to human judgments as results of our metrics for French, Spanish and German both from and to English.", "labels": [], "entities": [{"text": "RANK", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9906051754951477}, {"text": "CONST", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.9905999898910522}, {"text": "YES/NO system level correlations", "start_pos": 26, "end_pos": 58, "type": "METRIC", "confidence": 0.8682921826839447}]}, {"text": "The English-Spanish news task was left out as most metrics had negative correlation with human judgments.", "labels": [], "entities": []}, {"text": "The evaluation methodology used in allows us to measure how each MT evaluation metric correlates with human judgments on the system level, in which all translations from each MT system are aggregated into a single score.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.8945719003677368}]}, {"text": "The system rankings based on the scores are compared to human judgments.", "labels": [], "entities": []}, {"text": "Spearman's rank correlation coefficient \u03c1 was calculated between each MT metric and human judgment category using the simplified equation: where for each system i, d i is the difference between the rank derived from annotators' input and the rank obtained from the metric.", "labels": [], "entities": [{"text": "rank correlation coefficient \u03c1", "start_pos": 11, "end_pos": 41, "type": "METRIC", "confidence": 0.8062200993299484}, {"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9038943648338318}]}, {"text": "From the annotators' input, then MT systems were ranked based on the number of times each system's output was selected as the best translation divided by the number of times each system was part of a judgment.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9795412421226501}]}], "tableCaptions": [{"text": " Table 1: Effect of the replication factor on  MT-NCD correlation scores for the bz2 compres- sor with block size one as average over all lan- guages.", "labels": [], "entities": [{"text": "replication", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.8837184309959412}, {"text": "MT-NCD correlation scores", "start_pos": 47, "end_pos": 72, "type": "METRIC", "confidence": 0.7167029976844788}]}]}