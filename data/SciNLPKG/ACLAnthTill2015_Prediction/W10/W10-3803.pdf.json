{"title": [{"text": "Source-side Syntactic Reordering Patterns with Functional Words for Improved Phrase-based SMT", "labels": [], "entities": [{"text": "Syntactic Reordering Patterns", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.8865486780802408}, {"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.6647400259971619}]}], "abstractContent": [{"text": "Inspired by previous source-side syntactic reordering methods for SMT, this paper focuses on using automatically learned syntactic reordering patterns with functional words which indicate structural re-orderings between the source and target language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9938212633132935}]}, {"text": "This approach takes advantage of phrase alignments and source-side parse trees for pattern extraction, and then filters out those patterns without functional words.", "labels": [], "entities": [{"text": "phrase alignments", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7098614126443863}, {"text": "pattern extraction", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7379903793334961}]}, {"text": "Word lattices transformed by the generated patterns are fed into PB-SMT systems to incorporate potential re-orderings from the inputs.", "labels": [], "entities": []}, {"text": "Experiments are carried out on a medium-sized corpus fora Chinese-English SMT task.", "labels": [], "entities": [{"text": "SMT task", "start_pos": 74, "end_pos": 82, "type": "TASK", "confidence": 0.8308948576450348}]}, {"text": "The proposed method outperforms the base-line system by 1.38% relative on a randomly selected testset and 10.45% relative on the NIST 2008 testset in terms of BLEU score.", "labels": [], "entities": [{"text": "NIST 2008 testset", "start_pos": 129, "end_pos": 146, "type": "DATASET", "confidence": 0.9751789768536886}, {"text": "BLEU score", "start_pos": 159, "end_pos": 169, "type": "METRIC", "confidence": 0.9810238778591156}]}, {"text": "Furthermore, a system with just 61.88% of the patterns filtered by functional words obtains a comparable performance with the unfiltered one on the randomly selected testset, and achieves 1.74% relative improvements on the NIST 2008 testset.", "labels": [], "entities": [{"text": "NIST 2008 testset", "start_pos": 223, "end_pos": 240, "type": "DATASET", "confidence": 0.9770093361536661}]}], "introductionContent": [{"text": "Previous work has shown that the problem of structural differences between language pairs in SMT can be alleviated by source-side syntactic reordering.", "labels": [], "entities": [{"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9794745445251465}]}, {"text": "Taking account for the integration with SMT systems, these methods can be divided into two different kinds of approaches: the deterministic reordering and the nondeterministic reordering approach.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9812078475952148}]}, {"text": "To carryout the deterministic approach, syntactic reordering is performed uniformly on the training, devset and testset before being fed into the SMT systems, so that only the reordered source sentences are dealt with while building during the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 146, "end_pos": 149, "type": "TASK", "confidence": 0.9606348276138306}, {"text": "SMT", "start_pos": 244, "end_pos": 247, "type": "TASK", "confidence": 0.9673101305961609}]}, {"text": "In this case, most work is focused on methods to extract and to apply syntactic reordering patterns which come from manually created rules (, or via an automatic extraction process taking advantage of parse trees.", "labels": [], "entities": []}, {"text": "Because reordered source sentence cannot be undone by the SMT decoders), which implies a systematic error for this approach, classifiers are utilized to obtain high-performance reordering for some specialized syntactic structures (e.g. DE construction in Chinese).", "labels": [], "entities": [{"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9095995426177979}, {"text": "DE construction", "start_pos": 236, "end_pos": 251, "type": "TASK", "confidence": 0.7868008315563202}]}, {"text": "On the other hand, the non-deterministic approach leaves the decisions to the decoders to choose appropriate source-side reorderings.", "labels": [], "entities": []}, {"text": "This is more flexible because both the original and reordered source sentences are presented in the inputs.", "labels": [], "entities": []}, {"text": "Word lattices generated from syntactic structures for N-gram-based SMT is presented in ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.824822723865509}]}, {"text": "In (), chunks and POS tags are used to extract reordering rules, while the generated word lattices are weighted by language models and reordering models.", "labels": [], "entities": []}, {"text": "Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (.", "labels": [], "entities": []}, {"text": "Furthermore,) uses syntactic rules to score the output word order, both on English-Danish and EnglishArabic tasks.", "labels": [], "entities": []}, {"text": "Syntactic reordering information is also considered as an extra feature to improve PB-SMT in) for the ChineseEnglish task.", "labels": [], "entities": [{"text": "PB-SMT", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.720100998878479}, {"text": "ChineseEnglish", "start_pos": 102, "end_pos": 116, "type": "DATASET", "confidence": 0.8973715901374817}]}, {"text": "These results confirmed the effectiveness of syntactic reorderings.", "labels": [], "entities": [{"text": "syntactic reorderings", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7652516961097717}]}, {"text": "However, for the particular case of Chinese source inputs, although the DE construction has been addressed for both PBSMT and HPBSMT systems in (, as indicated by (, there are still lots of unexamined structures that imply source-side reordering, especially in the nondeterministic approach.", "labels": [], "entities": []}, {"text": "As specified in, these include the bei-construction, baconstruction, three kinds of de-construction (including DE construction) and general preposition constructions.", "labels": [], "entities": [{"text": "baconstruction", "start_pos": 53, "end_pos": 67, "type": "METRIC", "confidence": 0.5439122319221497}]}, {"text": "Such structures are referred with functional words in this paper, and all the constructions can be identified by their corresponding tags in the Penn Chinese TreeBank.", "labels": [], "entities": [{"text": "Penn Chinese TreeBank", "start_pos": 145, "end_pos": 166, "type": "DATASET", "confidence": 0.9799915750821432}]}, {"text": "It is interesting to investigate these functional words for the syntactic reordering task since most of them tend to produce structural reordering between the source and target sentences.", "labels": [], "entities": [{"text": "syntactic reordering", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.7126140296459198}]}, {"text": "Another related work is to filter the bilingual phrase pairs with closed-class words.", "labels": [], "entities": []}, {"text": "By taking account of the word alignments and word types, the filtering process reduces the phrase tables by up to a third, but still provide a system with competitive performance compared to the baseline.", "labels": [], "entities": []}, {"text": "Similarly, our idea is to use special type of words for the filtering purpose on the syntactic reordering patterns.", "labels": [], "entities": []}, {"text": "In this paper, our objective is to exploit these functional words for source-side syntactic reordering of Chinese-English SMT in the non-deterministic approach.", "labels": [], "entities": [{"text": "SMT", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.6439899802207947}]}, {"text": "Our assumption is that syntactic reordering patterns with functional words are the most effective ones, and others can be pruned for both speed and performance.", "labels": [], "entities": [{"text": "speed", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9851558804512024}]}, {"text": "To validate this assumption, three systems are compared in this paper: a baseline PBSMT system, a syntactic reordering system with all patterns extracted from a corpus, and a syntactic reordering system with patterns filtered with functional words.", "labels": [], "entities": []}, {"text": "To accomplish this, firstly the lattice scoring approach () is utilized to discover non-monotonic phrase alignments, and then syntactic reordering patterns are extracted from source-side parse trees.", "labels": [], "entities": []}, {"text": "After that, functional word tags specified in) are adopted to perform pattern filtering.", "labels": [], "entities": [{"text": "pattern filtering", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7700664103031158}]}, {"text": "Finally, both the unfiltered pattern set and the filtered one are used to transform inputs into word lattices to present potential reorderings for improving PB-SMT system.", "labels": [], "entities": []}, {"text": "A comparison between the three systems is carried out to examine the performance of syntactic reordering as well as the usefulness of functional words for pattern filtering.", "labels": [], "entities": [{"text": "pattern filtering", "start_pos": 155, "end_pos": 172, "type": "TASK", "confidence": 0.7412426471710205}]}, {"text": "The rest of this paper is organized as follows: in section 2 we describe the extraction process of syntactic reordering patterns, including the lattice scoring approach and the extraction procedures.", "labels": [], "entities": []}, {"text": "Then section 3 presents the filtering process used to obtain patterns with functional words.", "labels": [], "entities": []}, {"text": "After that, section 4 shows the generation of word lattices with patterns, and experimental setup and results included related discussion are presented in section 5.", "labels": [], "entities": []}, {"text": "Finally, we give our conclusion and avenues for future work in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted our experiments on a medium-sized corpus FBIS (a multilingual paragraph-aligned corpus with LDC resource number LDC2003E14) for the Chinese-English SMT task.", "labels": [], "entities": [{"text": "SMT task", "start_pos": 161, "end_pos": 169, "type": "TASK", "confidence": 0.8761976957321167}]}, {"text": "The Champollion aligner) is utilized to perform sentence alignment.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7831980884075165}]}, {"text": "A total number of 256,911 sentence pairs are obtained, while 2,000 pairs for devset and 2,000 pairs for testset are randomly selected, which we call FBIS set.", "labels": [], "entities": []}, {"text": "The rest of the data is used as the training corpus.", "labels": [], "entities": []}, {"text": "The baseline system is Moses (, and GIZA++ 1 is used to perform word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.7714803218841553}]}, {"text": "Minimum error rate training (MERT)) is carried out for tuning.", "labels": [], "entities": [{"text": "Minimum error rate training (MERT))", "start_pos": 0, "end_pos": 35, "type": "METRIC", "confidence": 0.9067903586796352}, {"text": "tuning", "start_pos": 55, "end_pos": 61, "type": "TASK", "confidence": 0.9575129151344299}]}, {"text": "A 5-gram language model built via SRILM 2 is used for all the experiments in this paper.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.6792749166488647}]}, {"text": "Experiments results are reported on two different sets: the FBIS set and the NIST set.", "labels": [], "entities": [{"text": "FBIS set", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.8855798542499542}, {"text": "NIST set", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9676064252853394}]}, {"text": "For the NIST set, the NIST 2005 testset (1,082 sentences) is used as the devset, and the NIST 2008 testset (1,357 sentences) is used as the testset.", "labels": [], "entities": [{"text": "NIST set", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.9739991128444672}, {"text": "NIST 2005 testset", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.9805505673090616}, {"text": "NIST 2008 testset", "start_pos": 89, "end_pos": 106, "type": "DATASET", "confidence": 0.9735037485758463}]}, {"text": "The FBIS set contains only one reference translation for both devset and testset, while NIST set has four references.", "labels": [], "entities": [{"text": "FBIS set", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.8585483431816101}, {"text": "NIST set", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.9705301821231842}]}], "tableCaptions": [{"text": " Table 2: Statistics on the number of patterns for  each type of functional word", "labels": [], "entities": []}, {"text": " Table 3. For both the  FBIS and NIST sets, the average number of nodes  in word lattices are illustrated before and after pat- tern filtering. From the table, it is clear that the  pattern filtering procedure dramatically reduces  the input size for the PBSMT system. The reduc- tion is up to 37.99% for the NIST testset.", "labels": [], "entities": [{"text": "NIST sets", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.8868211209774017}, {"text": "NIST testset", "start_pos": 309, "end_pos": 321, "type": "DATASET", "confidence": 0.9480180740356445}]}, {"text": " Table 3: Comparison of the average number of  nodes in word lattices", "labels": [], "entities": []}, {"text": " Table 4: Results on FBIS testset (DL = distortion  limit, METE=METEOR)", "labels": [], "entities": [{"text": "FBIS testset", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.822545975446701}, {"text": "DL", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9821985363960266}, {"text": "distortion  limit", "start_pos": 40, "end_pos": 57, "type": "METRIC", "confidence": 0.9823097586631775}, {"text": "METE", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9952778816223145}, {"text": "METEOR", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9210319519042969}]}, {"text": " Table 5: Results on NIST testset (DL = distortion  limit, METE=METEOR)", "labels": [], "entities": [{"text": "NIST testset", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.9364330768585205}, {"text": "DL", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9713600277900696}, {"text": "distortion  limit", "start_pos": 40, "end_pos": 57, "type": "METRIC", "confidence": 0.9703466296195984}, {"text": "METE", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9940943121910095}, {"text": "METEOR", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.8864045143127441}]}]}