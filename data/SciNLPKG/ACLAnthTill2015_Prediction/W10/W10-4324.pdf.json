{"title": [{"text": "Adaptive Referring Expression Generation in Spoken Dialogue Systems: Evaluation with Real Users", "labels": [], "entities": [{"text": "Adaptive Referring Expression Generation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.920265719294548}]}], "abstractContent": [{"text": "We present new results from a real-user evaluation of a data-driven approach to learning user-adaptive referring expression generation (REG) policies for spoken dialogue systems.", "labels": [], "entities": [{"text": "learning user-adaptive referring expression generation (REG)", "start_pos": 80, "end_pos": 140, "type": "TASK", "confidence": 0.7449518516659737}]}, {"text": "Referring expressions can be difficult to understand in technical domains where users may not know the technical 'jargon' names of the domain entities.", "labels": [], "entities": [{"text": "Referring expressions", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.91013103723526}]}, {"text": "In such cases, dialogue systems must be able to model the user's (lex-ical) domain knowledge and use appropriate referring expressions.", "labels": [], "entities": []}, {"text": "We present a reinforcement learning (RL) framework in which the system learns REG policies which can adapt to unknown users on-line.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6683925569057465}]}, {"text": "For real users of such a system, we show that in comparison to an adaptive hand-coded baseline policy, the learned policy performs significantly better, with a 20.8% average increase in adaptation accuracy , 12.6% decrease in time taken, and a 15.1% increase in task completion rate.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9502801895141602}, {"text": "completion rate", "start_pos": 267, "end_pos": 282, "type": "METRIC", "confidence": 0.8691063225269318}]}, {"text": "The learned policy also has a significantly better subjective rating from users.", "labels": [], "entities": []}, {"text": "This is because the learned policies adapt online to changing evidence about the user's domain expertise.", "labels": [], "entities": []}, {"text": "We also discuss the issue of evaluation in simulation versus evaluation with real users.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present new results from an evaluation with real users, fora reinforcement learning) framework to learn user-adaptive referring expression generation policies from datadriven user simulations.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 121, "end_pos": 152, "type": "TASK", "confidence": 0.6257144014040629}]}, {"text": "Such a policy allows the system to choose appropriate expressions to refer to domain entities in a dialogue setting.", "labels": [], "entities": []}, {"text": "For instance, in a technical support conversation, the Jargon: Please plug one end of the broadband cable into the broadband filter.", "labels": [], "entities": []}, {"text": "Descriptive: Please plug one end of the thin white cable with grey ends into the small white box.).", "labels": [], "entities": []}, {"text": "In natural human-human conversations, dialogue partners learn about each other and adapt their language to suit their domain expertise.", "labels": [], "entities": []}, {"text": "This kind of adaptation is called Alignment through Audience Design.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 34, "end_pos": 43, "type": "TASK", "confidence": 0.9857989549636841}]}, {"text": "We assume that users are mostly unknown to the system and therefore that a spoken dialogue system (SDS) must be capable of observing the user's dialogue behaviour, modelling his/her domain knowledge, and adapting accordingly, just like human interlocutors.", "labels": [], "entities": []}, {"text": "Therefore unlike systems that use static user models, our system has to dynamically model the user's domain knowledge in order to adapt during the conversation.", "labels": [], "entities": []}, {"text": "We present a corpus-driven framework for learning a user-adaptive REG policy from a small corpus of non-adaptive human-machine interaction.", "labels": [], "entities": []}, {"text": "We show that the learned policy performs better than a simple hand-coded adaptive policy in terms of accuracy of adaptation, dialogue time and task completion rate when evaluated with real users in a wizarded study.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9991870522499084}]}, {"text": "In section 2, we present some of the related work.", "labels": [], "entities": []}, {"text": "Section 3 and section 4 describe the dialogue system framework and the user simulation model.", "labels": [], "entities": []}, {"text": "In section 5, we present the training and in section 6, we present the evaluation for different REG policies with real users.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the details of the evaluation process, the baseline policy, the metrics used, and the results.", "labels": [], "entities": []}, {"text": "Ina recent study, we evaluated the learned policy and several hand-coded baselines with simulated users and found that).", "labels": [], "entities": []}, {"text": "An interesting issue for research in this area is to what extent evaluation results obtained in simulated environments transfer to evaluations with real users ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation with real users", "labels": [], "entities": []}]}