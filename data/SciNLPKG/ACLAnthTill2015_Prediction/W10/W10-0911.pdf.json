{"title": [], "abstractContent": [{"text": "Machine reading is a long-standing goal of AI and NLP.", "labels": [], "entities": [{"text": "Machine reading", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8586123883724213}]}, {"text": "In recent years, tremendous progress has been made in developing machine learning approaches for many of its subtasks such as parsing, information extraction, and question answering.", "labels": [], "entities": [{"text": "parsing", "start_pos": 126, "end_pos": 133, "type": "TASK", "confidence": 0.970984160900116}, {"text": "information extraction", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.7789382338523865}, {"text": "question answering", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.9078565239906311}]}, {"text": "However, existing end-to-end solutions typically require substantial amount of human efforts (e.g., labeled data and/or manual engineering), and are not well poised for Web-scale knowledge acquisition.", "labels": [], "entities": [{"text": "Web-scale knowledge acquisition", "start_pos": 169, "end_pos": 200, "type": "TASK", "confidence": 0.5740861097971598}]}, {"text": "In this paper , we propose a unifying approach for machine reading by bootstrapping from the easiest extractable knowledge and conquering the long tail via a self-supervised learning process.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.8070890605449677}]}, {"text": "This self-supervision is powered by joint inference based on Markov logic, and is made scalable by leveraging hierarchical structures and coarse-to-fine inference.", "labels": [], "entities": []}, {"text": "Researchers at the University of Washington have taken the first steps in this direction.", "labels": [], "entities": []}, {"text": "Our existing work explores the wide spectrum of this vision and shows its promise.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine reading, or learning by reading, aims to extract knowledge automatically from unstructured text and apply the extracted knowledge to end tasks such as decision making and question answering.", "labels": [], "entities": [{"text": "Machine reading", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7478529512882233}, {"text": "decision making", "start_pos": 159, "end_pos": 174, "type": "TASK", "confidence": 0.8206131756305695}, {"text": "question answering", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.8370439410209656}]}, {"text": "It has been a major goal of AI and NLP since their early days.", "labels": [], "entities": []}, {"text": "With the advent of the Web, the billions of online text documents contain virtually unlimited amount of knowledge to extract, further increasing the importance and urgency of machine reading.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 175, "end_pos": 190, "type": "TASK", "confidence": 0.7338296473026276}]}, {"text": "In the past, there has been a lot of progress in automating many subtasks of machine reading by machine learning approaches (e.g., components in the traditional NLP pipeline such as POS tagging and syntactic parsing).", "labels": [], "entities": [{"text": "machine reading", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.6977241188287735}, {"text": "POS tagging", "start_pos": 182, "end_pos": 193, "type": "TASK", "confidence": 0.7388368546962738}, {"text": "syntactic parsing", "start_pos": 198, "end_pos": 215, "type": "TASK", "confidence": 0.6907520592212677}]}, {"text": "However, end-to-end solutions are still rare, and existing systems typically require substantial amount of human effort in manual engineering and/or labeling examples.", "labels": [], "entities": []}, {"text": "As a result, they often target restricted domains and only extract limited types of knowledge (e.g., a pre-specified relation).", "labels": [], "entities": []}, {"text": "Moreover, many machine reading systems train their knowledge extractors once and do not leverage further learning opportunities such as additional text and interaction with end users.", "labels": [], "entities": []}, {"text": "These desiderata raise many intriguing and challenging research questions.", "labels": [], "entities": []}, {"text": "Machine reading research at the University of Washington has exploreda wide spectrum of solutions to these challenges and has produced a large number of initial systems that demonstrated promising performance.", "labels": [], "entities": [{"text": "Machine reading", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8270769119262695}]}, {"text": "During this expedition, an underlying unifying vision starts to emerge.", "labels": [], "entities": []}, {"text": "It becomes apparent that the key to solving machine reading is to: 1.", "labels": [], "entities": [{"text": "solving machine reading", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7279678483804067}]}, {"text": "Conquer the long tail of textual knowledge via a self-supervised learning process that leverages data redundancy to bootstrap from the head and propagates information down the long tail by joint inference; 2.", "labels": [], "entities": []}, {"text": "Scale this process to billions of Web documents by identifying and leveraging ubiquitous structures that lead to sparsity.", "labels": [], "entities": []}, {"text": "In Section 2, we present this vision in detail, identify the major dimensions these initial systems have explored, and propose a unifying approach that satisfies all five desiderata.", "labels": [], "entities": []}, {"text": "In Section 3, we reivew machine reading research at the University of Washington and show how they form synergistic effort towards solving the machine reading problem.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7558793127536774}, {"text": "machine reading problem", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.8469692269961039}]}, {"text": "We conclude in Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}