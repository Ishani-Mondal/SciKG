{"title": [{"text": "Preliminary Experience with Amazon's Mechanical Turk for Annotating Medical Named Entities", "labels": [], "entities": [{"text": "Amazon", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.9348387122154236}]}], "abstractContent": [{"text": "Amazon's Mechanical Turk (MTurk) service is becoming increasingly popular in Natural Language Processing (NLP) research.", "labels": [], "entities": []}, {"text": "In this paper, we report our findings in using MTurk to annotate medical text extracted from clinical trial descriptions with three entity types: medical condition, medication, and laboratory test.", "labels": [], "entities": []}, {"text": "We compared MTurk annotations with a gold standard manually created by a domain expert.", "labels": [], "entities": []}, {"text": "Based on the good performance results , we conclude that MTurk is a very promising tool for annotating large-scale corpora for biomedical NLP tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "The manual construction of annotated corpora is extremely expensive both in terms of time and money.", "labels": [], "entities": []}, {"text": "demonstrated the potential power of Amazon's Mechanical Turk (MTurk) service in annotating large corpora for natural language tasks cheaply and quickly.", "labels": [], "entities": []}, {"text": "We are working on a Natural Language Processing (NLP) project to automate the clinical trial eligibility screening of patients.", "labels": [], "entities": []}, {"text": "This project involves building statistical models for medical named entity recognition which requires a large-scale annotated corpus for training.", "labels": [], "entities": [{"text": "medical named entity recognition", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.6050505116581917}]}, {"text": "As part of corpus development, we tested the feasibility of using MTurk for the annotation of medical named entities in biomedical text and we report our findings in this paper.", "labels": [], "entities": []}, {"text": "In the following sections we describe how we used MTurk to annotate the biomedical corpus created from publicly available clinical trial announcements.", "labels": [], "entities": []}, {"text": "The main goal of our study was to understand how well nonexperts perform compared to medical expert in annotating the biomedical text.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our annotation experiments, each of 100 documents in our corpus was annotated by four workers, resulting in 100\u00d74=400 files per experiment.", "labels": [], "entities": []}, {"text": "We experimented with different pay scales to understand how they affect the quality and speed of the annotations.", "labels": [], "entities": [{"text": "speed", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9702984094619751}]}], "tableCaptions": [{"text": " Table 1. Cost analysis of annotation experiments (\"File\" in this table means the annotation of a document.  There are 100 documents, and each document is annotated by four workers.)", "labels": [], "entities": []}, {"text": " Table 2. Quality measurement of MTurk annotations (k: Agreement level, P: Precision, R: Recall, F: F-measure;  the highest value for each column is in boldface)", "labels": [], "entities": [{"text": "MTurk annotations", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8236469626426697}, {"text": "Precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.8651733994483948}, {"text": "Recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.8442443013191223}, {"text": "F-measure", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9058992266654968}]}]}