{"title": [{"text": "Cross-caption coreference resolution for automatic image understanding", "labels": [], "entities": [{"text": "Cross-caption coreference resolution", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.724625696738561}, {"text": "automatic image understanding", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.6191301246484121}]}], "abstractContent": [{"text": "Recent work in computer vision has aimed to associate image regions with keywords describing the depicted entities, but actual image 'understanding' would also require identifying their attributes, relations and activities.", "labels": [], "entities": []}, {"text": "Since this information cannot be conveyed by simple keywords, we have collected a corpus of \"action\" photos each associated with five descriptive captions.", "labels": [], "entities": []}, {"text": "In order to obtain a consistent semantic representation for each image, we need to first identify which NPs refer to the same entities.", "labels": [], "entities": []}, {"text": "We present three hierarchical Bayesian models for cross-caption coreference resolution.", "labels": [], "entities": [{"text": "cross-caption coreference resolution", "start_pos": 50, "end_pos": 86, "type": "TASK", "confidence": 0.8777394493420919}]}, {"text": "We have also created a simple ontology of entity classes that appear in images and evaluate how well these can be recovered.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many photos capture a moment in time, telling a brief story of people, animals and objects, their attributes, and their relationship to each other.", "labels": [], "entities": []}, {"text": "Although different people may give different interpretations to the same picture, people can readily interpret photos and describe the entities and events they perceive in complex sentences.", "labels": [], "entities": []}, {"text": "This level of image understanding still remains an elusive goal for computer vision: although current methods maybe able to identify the overall scene or some specific classes of entities, they are only starting to be able to identify attributes of entities, and are far from recovering a complete semantic interpretation of the depicted situation.", "labels": [], "entities": [{"text": "image understanding", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.742851734161377}]}, {"text": "Like natural language processing, computer vision requires suitable training data, and there are currently no publicly available data sets that would enable the development of such systems.", "labels": [], "entities": [{"text": "computer vision", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.7456310391426086}]}, {"text": "Photo sharing sites such as Flickr allow users to annotate images with keywords and other descriptions, and vision researchers have access to large collections of images annotated with keywords (e.g. the Corel collection).", "labels": [], "entities": [{"text": "Flickr", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.9531253576278687}, {"text": "Corel collection", "start_pos": 204, "end_pos": 220, "type": "DATASET", "confidence": 0.9412442147731781}]}, {"text": "A lot of recent work in computer vision has been aimed at predicting these keywords (.", "labels": [], "entities": [{"text": "computer vision", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7401919364929199}]}, {"text": "But keywords alone are not expressive enough to capture relations between entities.", "labels": [], "entities": []}, {"text": "Some research has used the text that surrounds an image in a news article as a proxy.", "labels": [], "entities": []}, {"text": "However, in many cases, the surrounding text or a user-provided caption does not simply describe what is depicted in the image (since this is usually obvious to the human reader for which this text is intended), but provides additional information.", "labels": [], "entities": []}, {"text": "We have collected a corpus of 8108 images associated with several simple descriptive captions.", "labels": [], "entities": []}, {"text": "In contrast to the text near an image on the web, the captions in our corpus provide direct, if partial and slightly noisy, descriptions of the image content.", "labels": [], "entities": []}, {"text": "Our data set differs from paraphrase corpora () in that the different captions of an image are produced independently by different writers.", "labels": [], "entities": []}, {"text": "There are many ways of describing the same image, because it is often possible to focus on different aspects of the depicted situation, and because certain aspects of the situation maybe unclear to the human viewer.", "labels": [], "entities": []}, {"text": "One of our goals is to use these captions to obtain a semantic representation of each image that is consistent with all of its captions.", "labels": [], "entities": []}, {"text": "In order to obtain such a representation, it is necessary to identify the entities that appear in the image, and to perform cross-caption coreference resolution, i.e. to identify all mentions of the same entity in the five captions associated with an image.", "labels": [], "entities": [{"text": "cross-caption coreference resolution", "start_pos": 124, "end_pos": 160, "type": "TASK", "confidence": 0.7133689125378927}]}, {"text": "In this paper, we compare different meth-  ods of cross-caption coreference resolution on our corpus.", "labels": [], "entities": [{"text": "cross-caption coreference resolution", "start_pos": 50, "end_pos": 86, "type": "TASK", "confidence": 0.7646467188994089}]}, {"text": "In order to facilitate further computer vision research, we have also defined a set of coarsegrained ontological classes that we use to automatically categorize the entities in our data set.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate each of the generative models and the heuristic coreference algorithm on the annotated test subset of our corpus consisting of 100 images with both the OpenNLP chunking and the gold standard chunking.", "labels": [], "entities": [{"text": "OpenNLP", "start_pos": 164, "end_pos": 171, "type": "DATASET", "confidence": 0.907808780670166}]}, {"text": "We report our scores based on the MUC evaluation metric.", "labels": [], "entities": [{"text": "MUC evaluation metric", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.8434259096781412}]}, {"text": "The results are reported in as the average scores across all the samples of two independent runs of each model.", "labels": [], "entities": []}, {"text": "We also present results on Model 0 without using WordNet where every word can bean expression of one of 200 fake entity sets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.9640741944313049}]}, {"text": "The same table also shows the performance of a baseline model and the upper bound on performance imposed by WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9550406336784363}]}, {"text": "A baseline model: In our baseline model, two noun phrases in captions of the same image are coreferent if they share the same head noun.", "labels": [], "entities": []}, {"text": "Upper bound on performance: Although WordNet synsets provide a good indication of whether two mentions can refer to the same entity or not, they may also be overly restrictive in other cases.", "labels": [], "entities": []}, {"text": "We measure the upper bound on performance that our reliance on WordNet imposes by finding the best-scoring coreference assignment that is consistent with our lexicon. respectively.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9280993938446045}]}, {"text": "This achieves an F-score of 90.2 on the test data with gold chunks.", "labels": [], "entities": [{"text": "F-score", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9996839761734009}]}, {"text": "Performance increases in each subsequent model.", "labels": [], "entities": []}, {"text": "The heuristic beats each of the models, but in some sense it is an extreme version of Model 1.", "labels": [], "entities": []}, {"text": "Both it and Model 1 attempt to produce entity sets that cover as many captions as possible, while minimizing the number of distinct words involved.", "labels": [], "entities": []}, {"text": "The heuristic locally forces this case, at the expense of no longer being a generative model.", "labels": [], "entities": []}, {"text": "The naive baseline categorizes words by selecting the most frequent class of the word.", "labels": [], "entities": []}, {"text": "If no instances of the word have occurred, it uses the overall most frequent class.", "labels": [], "entities": []}, {"text": "The WordNet baseline works by finding the most frequent class amongst the most relevant synsets fora word.", "labels": [], "entities": [{"text": "WordNet baseline", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9403815865516663}]}, {"text": "It calculates the class frequency for each synset by assuming each word has the sense of its first synset and incrementing the frequency of the first synset and its hypernyms.", "labels": [], "entities": []}, {"text": "When categorizing a word, it finds the set of closest hypernyms of the word that have a non-zero frequency, and chooses the class with the greatest sum of frequency counts amongst those hypernyms.", "labels": [], "entities": []}, {"text": "We train the MaxEnt classifier using semisupervised learning.", "labels": [], "entities": []}, {"text": "Initially, we train a classifier using the 500 sentence gold standard development set.", "labels": [], "entities": [{"text": "500 sentence gold standard development set", "start_pos": 43, "end_pos": 85, "type": "DATASET", "confidence": 0.7164150178432465}]}, {"text": "For each class, we use the top 5% 15 of the labels to label the unlabeled data and provide additional training data.", "labels": [], "entities": []}, {"text": "We then retrain the classifier on the newly labeled examples and the development set, and run it on the test set.", "labels": [], "entities": []}, {"text": "For each coreference chain in the test set, we relabel all of the mentions in the chain to use the majority class, if a clear majority exists.", "labels": [], "entities": []}, {"text": "If no such majority exists, we leave the labels as is.", "labels": [], "entities": []}, {"text": "The MaxEnt classifier experiments were conducted by varying the source of the synset assigned to each word.", "labels": [], "entities": []}, {"text": "For each of our coreference systems, we report two scores (Table 3).", "labels": [], "entities": []}, {"text": "The first is the average accuracy when using the output from two runs of each model with about 20 samples per run, and the second uses the output that performs best on the coreference task when scored on the development data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9971020817756653}]}, {"text": "Discussion Although we use WordNet to classify our entity mentions, we designed our ontology by considering only the images and their captions, with no particular mapping to WordNet in mind.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9354081153869629}, {"text": "WordNet", "start_pos": 174, "end_pos": 181, "type": "DATASET", "confidence": 0.9630820155143738}]}, {"text": "This was tuned using 10-fold cross-validation of the development set.", "labels": [], "entities": []}, {"text": "Therefore, these experiments provide of a proof of concept for the semi-supervised labeling of a corpus using any semantic/visual ontology.", "labels": [], "entities": []}, {"text": "Overall, Model 2 had the best performance for this task.", "labels": [], "entities": []}, {"text": "This demonstrates that the additional features of Model 2 force synset selections that are consistent across the entire corpus, and are sensitive to the modifiers appearing with them.", "labels": [], "entities": []}, {"text": "The WordNet heuristic selects synsets in a fairly arbitrary manner -all other things being equal, the synsets are chosen without reference to what other synsets are chosen by similar clusters of nouns.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Coreference resolution results (MUC scores; Models 0-2 are averaged over all samples)", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.9535765051841736}, {"text": "MUC", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.6155573129653931}]}, {"text": " Table 3: Prediction of ontological classes", "labels": [], "entities": [{"text": "Prediction of ontological classes", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.7534538805484772}]}, {"text": " Table 4: Overall entity recovery. We measure  how many entities we identify correctly (requiring  complete recovery of their coreference chains and  correct prediction of their ontological class.", "labels": [], "entities": [{"text": "entity recovery", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.7413855940103531}]}]}