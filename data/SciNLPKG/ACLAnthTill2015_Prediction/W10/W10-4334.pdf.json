{"title": [{"text": "Gaussian Processes for Fast Policy Optimisation of POMDP-based Dialogue Managers", "labels": [], "entities": [{"text": "POMDP-based Dialogue Managers", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.573744386434555}]}], "abstractContent": [{"text": "Modelling dialogue as a Partially Observable Markov Decision Process (POMDP) enables a dialogue policy robust to speech understanding errors to be learnt.", "labels": [], "entities": []}, {"text": "However , a major challenge in POMDP policy learning is to maintain tractability, so the use of approximation is inevitable.", "labels": [], "entities": [{"text": "POMDP policy learning", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.8582413593928019}]}, {"text": "We propose applying Gaussian Processes in Reinforcement learning of optimal POMDP dialogue policies, in order (1) to make the learning process faster and (2) to obtain an estimate of the uncertainty of the approximation.", "labels": [], "entities": [{"text": "Reinforcement learning", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.8615694642066956}]}, {"text": "We first demonstrate the idea on a simple voicemail dialogue task and then apply this method to a real-world tourist information dialogue task.", "labels": [], "entities": [{"text": "tourist information dialogue task", "start_pos": 109, "end_pos": 142, "type": "TASK", "confidence": 0.7273680120706558}]}], "introductionContent": [{"text": "One of the main challenges in dialogue management is effective handling of speech understanding errors.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.89007967710495}, {"text": "handling of speech understanding errors", "start_pos": 63, "end_pos": 102, "type": "TASK", "confidence": 0.7597123622894287}]}, {"text": "Instead of hand-crafting the error handler for each dialogue step, statistical approaches allow the optimal dialogue manager behaviour to be learnt automatically.", "labels": [], "entities": []}, {"text": "Reinforcement learning (RL), in particular, enables the notion of planning to be embedded in the dialogue management criteria.", "labels": [], "entities": [{"text": "Reinforcement learning (RL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8514392256736756}]}, {"text": "The objective of the dialogue manager is for each dialogue state to choose such an action that leads to the highest expected long-term reward, which is defined in this framework by the Qfunction.", "labels": [], "entities": []}, {"text": "This is in contrast to Supervised learning, which estimates a dialogue strategy in such away as to make it resemble the behaviour from a given corpus, but without directly optimising overall dialogue success.", "labels": [], "entities": []}, {"text": "Modelling dialogue as a Partially Observable Markov Decision Process (POMDP) allows action selection to be based on the differing levels of uncertainty in each dialogue state as well as the overall reward.", "labels": [], "entities": []}, {"text": "This approach requires that a distribution of states (belief state) is maintained at each turn.", "labels": [], "entities": []}, {"text": "This explicit representation of uncertainty in the POMDP gives it the potential to produce more robust dialogue policies (.", "labels": [], "entities": [{"text": "POMDP", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.8632526993751526}]}, {"text": "The main challenge in the POMDP approach is the tractability of the learning process.", "labels": [], "entities": []}, {"text": "A discrete state space POMDP can be perceived as a continuous space MDP where the state space consists of the belief states of the original POMDP.", "labels": [], "entities": []}, {"text": "A grid-based approach to policy optimisation assumes discretisation of this space, allowing for discrete space MDP algorithms to be used for learning and thus approximating the optimal Q-function.", "labels": [], "entities": [{"text": "policy optimisation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7442573308944702}]}, {"text": "Such an approach takes the order of 100, 000 dialogues to train a realworld dialogue manager.", "labels": [], "entities": []}, {"text": "Therefore, the training normally takes place in interaction with a simulated user, rather than real users.", "labels": [], "entities": []}, {"text": "This raises questions regarding the quality of the approximation as well as the potential discrepancy between simulated and real user behaviour.", "labels": [], "entities": []}, {"text": "Gaussian Processes have been successfully used in Reinforcement learning for continuous space MDPs, for both model-free approaches) and model-based approaches).", "labels": [], "entities": [{"text": "Reinforcement learning", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.9716713428497314}]}, {"text": "We propose using GP Reinforcement learning in a POMDP dialogue manager to, firstly, speedup the learning process and, secondly, obtain the uncertainty of the approximation.", "labels": [], "entities": [{"text": "POMDP dialogue manager", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.7879751523335775}, {"text": "uncertainty", "start_pos": 139, "end_pos": 150, "type": "METRIC", "confidence": 0.9745848774909973}]}, {"text": "We opt for the model-free approach since it has the potential to allow the policy obtained in interaction with the simulated user to be further refined in interaction with real users.", "labels": [], "entities": []}, {"text": "In the next section, the core idea of the method is explained on a toy dialogue problem where different aspects of GP learning are examined.", "labels": [], "entities": [{"text": "GP learning", "start_pos": 115, "end_pos": 126, "type": "TASK", "confidence": 0.8574436604976654}]}, {"text": "Following that, in Section 3, it is demonstrated how this methodology can be effectively applied to areal world dialogue.", "labels": [], "entities": [{"text": "areal world dialogue", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.6417364180088043}]}, {"text": "We conclude with Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Policy optimisation is performed by interacting with a simulated user on the dialogue act level.", "labels": [], "entities": [{"text": "Policy optimisation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8725131452083588}]}, {"text": "The simulated user gives a reward at the final state of the dialogue, and that is 20 if the dialogue was successful, 0 otherwise, less the number of turns taken to fulfil the user goal.", "labels": [], "entities": []}, {"text": "The simulated user takes a maximum of 100 turns in each dialogue, terminating it when all the necessary information has been obtained or if it looses patience.", "labels": [], "entities": []}, {"text": "A grid-based MCC algorithm provides the baseline method.", "labels": [], "entities": []}, {"text": "The distance metric used ensures that the number of regions in the grid is small enough for the learning to be tractable (.", "labels": [], "entities": []}, {"text": "In order to measure how fast each algorithm learns, a similar training set-up to the one presented in Section 2.7 was adopted and the averaged results are plotted on the graph,.", "labels": [], "entities": []}, {"text": "The results show that in the very early stage of learning, i.e., during the first 400 dialogues, the GP-based method learns faster.", "labels": [], "entities": []}, {"text": "Also, the learning process can be accelerated by adopting the active learning framework where the actions are selected based on the estimated uncertainty.", "labels": [], "entities": []}, {"text": "After performing many iterations in an incremental noise learning set-up () both the GP-Sarsa and the grid-based MCC algorithms converge to the same performance.", "labels": [], "entities": [{"text": "GP-Sarsa", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.9653446078300476}]}], "tableCaptions": []}