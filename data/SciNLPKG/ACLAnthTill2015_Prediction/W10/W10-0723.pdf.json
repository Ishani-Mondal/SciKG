{"title": [{"text": "Shedding (a Thousand Points of) Light on Biased Language", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper considers the linguistic indicators of bias in political text.", "labels": [], "entities": []}, {"text": "We used Amazon Mechanical Turk judgments about sentences from American political blogs, asking annotators to indicate whether a sentence showed bias, and if so, in which political direction and through which word tokens.", "labels": [], "entities": []}, {"text": "We also asked annotators questions about their own political views.", "labels": [], "entities": []}, {"text": "We conducted a preliminary analysis of the data, exploring how different groups perceive bias in different blogs, and showing some lexical indicators strongly associated with perceived bias.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bias and framing are central topics in the study of communications, media, and political discourse), but they have received relatively little attention in computational linguistics.", "labels": [], "entities": [{"text": "Bias and framing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8511987725893656}]}, {"text": "What are the linguistic indicators of bias?", "labels": [], "entities": []}, {"text": "Are there lexical, syntactic, topical, or other clues that can be computationally modeled and automatically detected?", "labels": [], "entities": []}, {"text": "Here we use Amazon Mechanical Turk (MTurk) to engage in a systematic, empirical study of linguistic indicators of bias in the political domain, using text drawn from political blogs.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 12, "end_pos": 42, "type": "DATASET", "confidence": 0.7880714237689972}]}, {"text": "Using the MTurk framework, we collected judgments connected with the two dominant schools of thought in American politics, as exhibited in single sentences.", "labels": [], "entities": [{"text": "MTurk framework", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.9140510857105255}]}, {"text": "Since no one person can claim to bean unbiased judge of political bias in language, MTurk is an attractive framework that lets us measure perception of bias across a population.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extracted our sentences from the collection of blog posts in.", "labels": [], "entities": []}, {"text": "The corpus consists of 2008 blog posts gathered from six sites focused on American politics: \u2022 American Thinker (conservative), 3 \u2022 Digby (liberal), 4 \u2022 Hot Air (conservative), 5 \u2022 Michelle Malkin (conservative), \u2022 Think Progress (liberal), 7 and \u2022 Talking Points Memo (liberal).", "labels": [], "entities": [{"text": "American Thinker", "start_pos": 95, "end_pos": 111, "type": "DATASET", "confidence": 0.9204500913619995}, {"text": "Talking Points Memo", "start_pos": 249, "end_pos": 268, "type": "DATASET", "confidence": 0.8997598687807719}]}, {"text": "13,246 posts were gathered in total, and 261,073 sentences were extracted using WebHarvest  We prepared 1,100 Human Intelligence Tasks (HITs), each containing one sentence annotation task.", "labels": [], "entities": []}, {"text": "1,041 sentences were annotated five times each (5,205 judgements total).", "labels": [], "entities": []}, {"text": "One annotation task consists of three bias judgement questions plus four survey questions.", "labels": [], "entities": []}, {"text": "We priced each HIT between $0.02 and $0.04 (moving from less to more to encourage faster completion).", "labels": [], "entities": [{"text": "completion", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9587437510490417}]}, {"text": "The total cost was $212.", "labels": [], "entities": []}, {"text": "We restricted access to our tasks to those who resided in United States and who had above 90% approval history, to ensure quality and awareness of American political issues.", "labels": [], "entities": []}, {"text": "We also discarded HITs annotated by workers with particularly low agreement scores.", "labels": [], "entities": []}, {"text": "The time allowance for each HIT was set at 5 minutes.", "labels": [], "entities": [{"text": "time allowance", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9168921709060669}, {"text": "HIT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.3940028250217438}]}], "tableCaptions": [{"text": " Table 2: Strength of perceived bias per sentence, averaged over  the annotators (rounded to nearest half point). Annotators rate  bias on a scale of 1 (no bias), 2 (some bias), and 3 (very biased).", "labels": [], "entities": []}, {"text": " Table 3: Direction of perceived bias, per judgment (very lib- eral, moderately liberal, no bias, moderately conservative, very  conservative, biased but not sure which).", "labels": [], "entities": []}, {"text": " Table 4: Distribution of judgements by annotators' self- identification on social issues (row) and fiscal issue (column);  {L, C, M, NA} denote liberal, conservative, moderate, and de- cline to answer, respectively.", "labels": [], "entities": []}, {"text": " Table 5: Percentage representation of each site within bias label  pools from question 2 (direction of perceived bias): very liberal,  moderately liberal, no bias, moderately conservative, very con- servative, biased but not sure which. Rows sum to 100. Bold- face indicates rates higher than the site's overall representation  in the pool.", "labels": [], "entities": []}]}