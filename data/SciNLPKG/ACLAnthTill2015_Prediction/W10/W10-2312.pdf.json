{"title": [], "abstractContent": [{"text": "Recently, with the huge amount of growing information in the web and the little available time to read and process all this information, automatic summaries have become very important resources.", "labels": [], "entities": [{"text": "summaries", "start_pos": 147, "end_pos": 156, "type": "TASK", "confidence": 0.8286907076835632}]}, {"text": "In this work, we evaluate deep content selection methods for multidocument summarization based on the CST model (Cross-document Structure Theory).", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.6647162437438965}]}, {"text": "Our methods consider summarization preferences and focus on the overall main problems of multidocument treatment: redundancy, complementarity, and contradiction among different information sources.", "labels": [], "entities": [{"text": "summarization", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9660161733627319}]}, {"text": "We also evaluate the impact of the CST model over superficial summarization systems.", "labels": [], "entities": []}, {"text": "Our results show that the use of CST model helps to improve informativeness and quality in automatic summaries.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the last years there has been a considerable increase in the amount of online information and consequently the task of processing this information has become more difficult.", "labels": [], "entities": []}, {"text": "Just to have an idea, recent studies conducted by IDC showed that 800 exabytes of information were produced in 2009, and it is estimated that in 2012 it will be produced 3 times more.", "labels": [], "entities": [{"text": "IDC", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.9345135688781738}]}, {"text": "Among all of this information, there is a lot of related content that comes from different sources and that presents similarities and differences.", "labels": [], "entities": []}, {"text": "Reading and dealing with this is not straightforward.", "labels": [], "entities": []}, {"text": "In this scenario, multidocument summarization has become an important task.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.7545488774776459}]}, {"text": "Multidocument summarization consists in producing a unique summary from a set of documents on the same topics.", "labels": [], "entities": [{"text": "Multidocument summarization", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8338275253772736}]}, {"text": "A multidocument summary must contain the most relevant information from the documents.", "labels": [], "entities": []}, {"text": "For example, we may want to produce a multidocument summary from all the documents telling about the recent world economical crisis or the terrorism in some region.", "labels": [], "entities": []}, {"text": "As an example, reproduces a summary from, which contains the main facts from 4 news sources.", "labels": [], "entities": []}, {"text": "Multidocument summarization has to deal not only with the fact of showing relevant information but also with some multidocument phenomena such as redundancy, complementarity, contradiction, information ordering, source identification, temporal resolution, etc.", "labels": [], "entities": [{"text": "Multidocument summarization", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.812929779291153}, {"text": "information ordering", "start_pos": 190, "end_pos": 210, "type": "TASK", "confidence": 0.7493992447853088}, {"text": "source identification", "start_pos": 212, "end_pos": 233, "type": "TASK", "confidence": 0.7328043580055237}, {"text": "temporal resolution", "start_pos": 235, "end_pos": 254, "type": "TASK", "confidence": 0.6882772296667099}]}, {"text": "It is also interesting to notice that, instead of only generic summaries (as the one in the example), summaries maybe produced considering user preferences.", "labels": [], "entities": []}, {"text": "For example, one may prefer summaries including information attributed to particular sources (if one trusts more in some sources) or more context information (considering a reader that has not accompanied some recent important news), among other possibilities.", "labels": [], "entities": [{"text": "summaries", "start_pos": 28, "end_pos": 37, "type": "TASK", "confidence": 0.9767881631851196}]}, {"text": "Reuters reported that 18 people were killed in a Jerusalem bombing Sunday.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.947482705116272}]}, {"text": "The next day, a bomb in Tel Aviv killed at least 10 people and wounded 30 according to Israel radio.", "labels": [], "entities": []}, {"text": "Reuters reported that at least 12 people were killed and 105 wounded.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9395984411239624}]}, {"text": "Later the same day, Reuters reported that the radical Muslim group Hamas had claimed responsibility for the act.", "labels": [], "entities": []}, {"text": "There are two main approaches for multidocument summarization: the superficial and the deep approaches.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6996111273765564}]}, {"text": "Superficial approach uses little linguistic knowledge to produce summaries.", "labels": [], "entities": []}, {"text": "This approach usually has low cost and is more robust, but it produces poor results.", "labels": [], "entities": []}, {"text": "On the other hand, deep approaches use more linguistic knowledge to produce summaries.", "labels": [], "entities": []}, {"text": "In general terms, in this approach it is commonly used syntactical, semantic and discourse parsers to analyze the original documents.", "labels": [], "entities": []}, {"text": "Avery common way to analyze documents consists in establishing semantic relations among the documents parts, which helps identifying commonalities and differences in information.", "labels": [], "entities": []}, {"text": "Within this context, discourse models as CST (Cross-document Structure Theory)) are useful (see, e.g.,.", "labels": [], "entities": []}, {"text": "It was proposed in Mani and Maybury (1999) a general architecture for multidocument summarization, with analysis, transformation, and synthesis stages.", "labels": [], "entities": []}, {"text": "The first stage consists in analyzing and formally representing the content of the original documents.", "labels": [], "entities": []}, {"text": "The second stage consists mainly in transforming the represented content into a condensed content that will be included in the final summary.", "labels": [], "entities": []}, {"text": "One of the most important tasks in this stage is the content selection process, which consists in selecting the most relevant information.", "labels": [], "entities": [{"text": "content selection process", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7914719780286154}]}, {"text": "Finally, the third stage expresses the condensed content in natural language, producing the summary.", "labels": [], "entities": []}, {"text": "In this paper, we explore a CST-based summarization method and evaluate the corresponding prototype system for multidocument summarization.", "labels": [], "entities": [{"text": "CST-based summarization", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.6813746690750122}]}, {"text": "Our system, called CSTSumm (CST-based SUMMarizer), produces multidocument summaries from input CST-analyzed news documents.", "labels": [], "entities": []}, {"text": "We mainly investigate content selection methods for producing both generic and preference-based summaries.", "labels": [], "entities": []}, {"text": "Particularly, we formalize and codify our content selection strategies as operators that perform the previously cited transformation stage.", "labels": [], "entities": []}, {"text": "We run our experiments with Brazilian Portuguese news texts (previously analyzed according to CST by human experts) and show that we produce more informative summaries in comparison with some superficial summarizers ().", "labels": [], "entities": [{"text": "CST", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.9274007678031921}]}, {"text": "We also use CST to enrich these superficial summarizers, showing that the results also improve.", "labels": [], "entities": []}, {"text": "Our general hypothesis for this work is that the deep knowledge provided by CST helps to improve information and quality in summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 124, "end_pos": 133, "type": "TASK", "confidence": 0.9627164006233215}]}, {"text": "This work is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, the main concepts of the CST model are introduced and the works that have already used CST for multidocument summarization are reviewed.", "labels": [], "entities": []}, {"text": "In Section 3, we present CSTSumm, while its evaluation is reported in Section 4.", "labels": [], "entities": [{"text": "CSTSumm", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.7020713090896606}]}, {"text": "Some final remarks are presented in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our main research question in this work was how helpful CST would be for producing better summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 90, "end_pos": 99, "type": "TASK", "confidence": 0.9428477883338928}]}, {"text": "CSTSumm enables us to assess the summaries and content selection strategies, but a comparison of these summaries with summaries produced by superficial methods is still necessary.", "labels": [], "entities": [{"text": "CSTSumm", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8298614621162415}, {"text": "summaries", "start_pos": 33, "end_pos": 42, "type": "TASK", "confidence": 0.9653465747833252}, {"text": "content selection", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.6671092212200165}]}, {"text": "In fact, we not only proceeded to such Sentence 1: According to a spokesman from United Nations, the plane was trying to land at the airport in Bukavu in the middle of a storm.", "labels": [], "entities": [{"text": "Sentence 1", "start_pos": 39, "end_pos": 49, "type": "TASK", "confidence": 0.8983426988124847}]}, {"text": "Sentence 2: Everyone died when the plane, hampered by bad weather, failed to reach the runway and crashed in a forest 15 kilometers from the airport in Bukavu.", "labels": [], "entities": [{"text": "Sentence", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.756703794002533}]}, {"text": "Fusion: According to a spokesman for the United Nations, everyone died when a plane that was trying to land at Bukavu airport, hampered by bad weather, failed to reach the runway and crashed in a forest 15 kilometers from the airport.", "labels": [], "entities": []}, {"text": "procedure for application of content selection operators input data: initial rank, user summarization preference, operators output data: refined rank apply the redundancy operator select one operator according to the user summarization preference for i=sentence at the first position in the rank to the last but one sentence for j=sentence at position i+1 in the rank to the last sentence if the operator relations happen among sentences i and j, rearrange the rank appropriately comparison, but also improved the superficial methods with CST knowledge.", "labels": [], "entities": [{"text": "CST knowledge", "start_pos": 539, "end_pos": 552, "type": "DATASET", "confidence": 0.8796078264713287}]}, {"text": "As superficial summarizers, we selected MEAD () and GistSumm () summarizers.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9202073216438293}]}, {"text": "Initially, MEAD builds an initial rank of sentences according to a score based on three parameters: position of the sentence in the text, lexical distance of the sentence to the centroid of the text, and the size of the sentence.", "labels": [], "entities": []}, {"text": "These three elements are linearly combined for producing the score.", "labels": [], "entities": []}, {"text": "GistSumm, on the other side, is very simple: the system juxtaposes all the source texts and gives a score to each sentence according to the presence of frequent words (following the approach of or by using TF-ISF (Term FrequencyInverse Sentence Frequency, as proposed in).", "labels": [], "entities": []}, {"text": "Following the work of, we decided to use CST to rearrange (and supposedly improve) the sentence ranks produced by MEAD and GistSumm.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.880014955997467}]}, {"text": "We simply add to each sentence score the number of CST relations that the sentence presents: new sentence score = old sentence score + number of CST relations The number of sentences is retrieved from the CST graph.", "labels": [], "entities": []}, {"text": "This way, the sentence positions in the rank are changed.", "labels": [], "entities": []}, {"text": "For our experiments, we used the CSTNews corpus (, which is a corpus of news texts written in Brazilian Portuguese.", "labels": [], "entities": [{"text": "CSTNews corpus", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.9147831201553345}]}, {"text": "The corpus contains 50 clusters of texts.", "labels": [], "entities": []}, {"text": "Each group has from 2 to 4 texts on the same topic annotated according to CST by human experts, as well as a manual generic summary with 70% compression rate (in relation to the longest text).", "labels": [], "entities": [{"text": "CST", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.9351381063461304}, {"text": "compression rate", "start_pos": 141, "end_pos": 157, "type": "METRIC", "confidence": 0.9594078361988068}]}, {"text": "The annotation process was carried out by 4 humans, with satisfactory agreement, which demonstrated that the annotation task was well defined and performed.", "labels": [], "entities": []}, {"text": "More details about the corpus and its annotation process are presented by.", "labels": [], "entities": []}, {"text": "For each cluster of CSTNews corpus, it was produced a set of automatic summaries corresponding to each method that was explored in this work.", "labels": [], "entities": [{"text": "CSTNews corpus", "start_pos": 20, "end_pos": 34, "type": "DATASET", "confidence": 0.8983741998672485}]}, {"text": "To evaluate the informativity and quality of the summaries, we used two types of evaluation: automatic evaluation and human evaluation.", "labels": [], "entities": []}, {"text": "For the automatic evaluation we used ROUGE) informativity measure, which compares automatic summaries with human summaries in terms of the n-grams that they have in common, resulting in precision, recall and f-measure numbers between 0 (the worst) and 1 (the best), which indicate how much information the summary presents.", "labels": [], "entities": [{"text": "ROUGE) informativity measure", "start_pos": 37, "end_pos": 65, "type": "METRIC", "confidence": 0.8938917070627213}, {"text": "precision", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.9992403984069824}, {"text": "recall", "start_pos": 197, "end_pos": 203, "type": "METRIC", "confidence": 0.9990817308425903}]}, {"text": "Precision indicates the amount of relevant information that the automatic summary contains; recall indicates how much information from the human summary is reproduced in the automatic summary; fmeasure is a unique performance measure that combines precision and recall.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9823198914527893}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9992114305496216}, {"text": "precision", "start_pos": 248, "end_pos": 257, "type": "METRIC", "confidence": 0.9992178678512573}, {"text": "recall", "start_pos": 262, "end_pos": 268, "type": "METRIC", "confidence": 0.9940704703330994}]}, {"text": "Although it looks simple, ROUGE author has showed that it performs as well as humans in differentiating summary informativeness, which caused the measure to be widely used in the area.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9662192463874817}]}, {"text": "In particular, for this work, we considered only unigram comparison, since the author of the measure demonstrated that unigrams are enough for differentiating summary quality.", "labels": [], "entities": []}, {"text": "For computing ROUGE, we compared each automatic summary with the corresponding human summary in the corpus.", "labels": [], "entities": []}, {"text": "We computed ROUGE for every summary we produced through several strategies: using only the initial rank, only the redundancy operator, and the remaining preference operators (applied after the redundancy operator).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.981582760810852}]}, {"text": "Isis important to notice that it is only fair to use ROUGE to evaluate the summaries produced by the initial rank and by the redundancy operator, since the human summary (to which ROUGE compares the automatic summaries) are generic, produced with no preference in mind.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9698649048805237}]}, {"text": "We only computed ROUGE for the preference-biased summaries in order to have a measure of how informative they are.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.9979221224784851}]}, {"text": "Ideally, these preference-biased summaries should not only mirror the user preference, but also contain the main information from the source texts.", "labels": [], "entities": []}, {"text": "On the other hand, we used human evaluation to measure the quality of the summaries in terms of coherence, cohesion and redundancy, factors that ROUGE is not sensitive enough to capture.", "labels": [], "entities": []}, {"text": "By coherence, we mean the characteristic of a text having a meaning and being understandable.", "labels": [], "entities": []}, {"text": "By cohesion, we mean the superficial makers of coherence, i.e., the sequence of text elements that connect the ideas in the text, as punctuation, discourse markers, anaphors, etc.", "labels": [], "entities": []}, {"text": "For each one of the above evaluation factors, a human evaluator was asked to assign one of five values: very bad (score 0), bad (score 1), regular (score 2), good (score 3), and excellent (score 4).", "labels": [], "entities": []}, {"text": "We also asked humans to evaluate informativity in the preference-biased summaries produced by our system, which is a more fair evaluation than the automatic one described above.", "labels": [], "entities": []}, {"text": "The user should score each summary (using the same values above) according to how much he was satisfied with the actual content of the summary in face of the preference made.", "labels": [], "entities": []}, {"text": "The user had access to the source texts for performing the evaluation.", "labels": [], "entities": []}, {"text": "shows the ROUGE scores for the summaries produced by the initial rank, by the application of the operators, by the superficial summarizers, and by the CST-enriched superficial summarizers.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9925190210342407}, {"text": "summaries", "start_pos": 31, "end_pos": 40, "type": "TASK", "confidence": 0.9604924917221069}]}, {"text": "It is important to say that these results are the average results obtained for the automatic summaries generated for all the clusters in the CSTNews corpus.", "labels": [], "entities": [{"text": "CSTNews corpus", "start_pos": 141, "end_pos": 155, "type": "DATASET", "confidence": 0.9319219291210175}]}, {"text": "As expected, it maybe observed that the best results were achieved by the initial rank (since it produces generic summaries, as happens to the human summaries to which they are compared), which does not consider any summarization preference at all.", "labels": [], "entities": []}, {"text": "It is also possible to see that: (a) the superficial summarizers are outperformed by the CST-based methods and (b) CST-enriched superficial summarizers produced better results than the superficial summarizers.", "labels": [], "entities": [{"text": "CST-enriched superficial summarizers", "start_pos": 115, "end_pos": 151, "type": "TASK", "confidence": 0.5914350847403208}]}, {"text": "Results for human evaluation are shown in.", "labels": [], "entities": []}, {"text": "These results show the average value for each factor evaluated fora sample group of 48 texts randomly selected from the corpus.", "labels": [], "entities": []}, {"text": "We also associated to each value the closest concept in our evaluation.", "labels": [], "entities": []}, {"text": "We could not perform the evaluation for the whole corpus due to the high cost and time-demanding nature of the human evaluation.", "labels": [], "entities": []}, {"text": "Six humans carried out this evaluation.", "labels": [], "entities": []}, {"text": "Each human evaluated eight summaries, and each summary was evaluated by three humans.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: ROUGE results  Content selection method  Precision  Recall  F-measure  Initial rank  0.5564  0.5303  0.5356  Redundancy treatment (only)  0.5761  0.5065  0.5297  Context information  0.5196  0.4938  0.4994  Authorship information  0.5563  0.5224  0.5310  Contradiction information  0.5503  0.5379  0.5355  Evolving events information  0.5159  0.5222  0.5140  MEAD without CST  0.5242  0.4602  0.4869  MEAD with CST  0.5599  0.4988  0.5230  GistSumm without CST  0.3599  0.6643  0.4599  GistSumm with CST  0.4945  0.5089  0.4994", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.7817050218582153}, {"text": "Precision  Recall  F-measure  Initial rank  0.5564  0.5303  0.5356  Redundancy treatment", "start_pos": 51, "end_pos": 139, "type": "METRIC", "confidence": 0.746689859032631}, {"text": "MEAD", "start_pos": 411, "end_pos": 415, "type": "METRIC", "confidence": 0.9535420536994934}]}, {"text": " Table 3. These results show the average value for", "labels": [], "entities": []}]}