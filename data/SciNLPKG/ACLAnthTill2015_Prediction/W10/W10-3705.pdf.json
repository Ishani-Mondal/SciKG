{"title": [], "abstractContent": [{"text": "Identifying collocations in a sentence, in order to ensure their proper processing in subsequent applications, and performing the syntactic analysis of the sentence are interrelated processes.", "labels": [], "entities": []}, {"text": "Syntactic information is crucial for detecting collocations, and vice versa, collocational information is useful for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 117, "end_pos": 124, "type": "TASK", "confidence": 0.976889967918396}]}, {"text": "This article describes an original approach in which collocations are identified in a sentence as soon as possible during the analysis of that sentence, rather than at the end of the analysis, as in our previous work.", "labels": [], "entities": []}, {"text": "In this way, priority is given to parsing alternatives involving col-locations, and collocational information guide the parser through the maze of alternatives.", "labels": [], "entities": []}, {"text": "This solution was shown to lead to substantial improvements in the performance of both tasks (collocation identification and parsing), and in that of a subsequent task (machine translation).", "labels": [], "entities": [{"text": "collocation identification", "start_pos": 94, "end_pos": 120, "type": "TASK", "confidence": 0.7301653623580933}, {"text": "machine translation)", "start_pos": 169, "end_pos": 189, "type": "TASK", "confidence": 0.804186483224233}]}], "introductionContent": [{"text": "Collocations 1 constitute a central language phenomenon and an impressive amount of work has been devoted over the past decades to the automatic acquisition of collocational resources -as attested, among others, by initiatives like the MWE 2008 shared task aimed at creating a repository of reference data).", "labels": [], "entities": [{"text": "MWE 2008 shared task", "start_pos": 236, "end_pos": 256, "type": "TASK", "confidence": 0.69757080078125}]}, {"text": "However, little or no reference exist in the literature about the actual use made of these resources in other NLP applications.", "labels": [], "entities": []}, {"text": "In this paper, we consider the particular application of syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.8421403765678406}]}, {"text": "Just as other types of multi-word expressions (henceforth, MWEs), collocations are problematic for parsing because they have to be recognised and treated as a whole, rather than compositionally, i.e., in a word byword fashion ().", "labels": [], "entities": []}, {"text": "The standard approach in dealing with MWEs in parsing is to apply a \"words-with-spaces\" preprocessing step, which marks the MWEs in the input sentence as units which will later be integrated as single blocks in the parse tree built during analysis.", "labels": [], "entities": []}, {"text": "We argue that such an approach, albeit sufficiently appropriate for some subtypes of MWEs 2 , is not really adequate for processing collocations.", "labels": [], "entities": []}, {"text": "Unlike other expressions that are fixed or semi-fixed 3 , collocations do not allow a \"wordswith-spaces\" treatment because they have a high morpho-syntactic flexibility.", "labels": [], "entities": []}, {"text": "There is no systematic restriction, for instance, on the number of forms a lexical item (such as a verb) may have in a collocation, on the order of items in a collocation, or on the number of words that may intervene between these items.", "labels": [], "entities": []}, {"text": "Collocations are situated at the intersection of lexicon and grammar; therefore, they cannot be accounted for merely by the lexical component of a parsing system, but have to be integrated to the grammatical component as well, as the parser has to consi-der all the possible syntactic realisations of collocations.", "labels": [], "entities": []}, {"text": "Alternatively, a post-processing approach (such as the one we pursued previously in) would identify collocations after the syntactic analysis has been performed, and output a parse tree in which collocational relations are highlighted between the composing items, in order to inform the subsequent processing applications (e.g., a machine translation application).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 331, "end_pos": 350, "type": "TASK", "confidence": 0.7305998504161835}]}, {"text": "Again, this solution is not fully appropriate, and the reason lies with the important observation that prior collocational knowledge is highly relevant for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 156, "end_pos": 163, "type": "TASK", "confidence": 0.9703906774520874}]}, {"text": "Collocational restrictions are, along with other types of information like selectional preferences and subcategorization frames, a major means of structural disambiguation.", "labels": [], "entities": []}, {"text": "Collocational relations between the words in a sentence proved very helpful in selecting the most plausible among all the possible parse trees fora sentence.", "labels": [], "entities": []}, {"text": "Hence, the question whether collocations should be identified in a sentence before or after parsing is not an easy one.", "labels": [], "entities": []}, {"text": "The previous literature on parsing and collocations fails to provide insightful details on how this circular issue is (or can be) solved.", "labels": [], "entities": []}, {"text": "In this paper, we argue that the identification of collocations and the construction of a parse tree are interrelated processes, that must be accounted for simultaneously.", "labels": [], "entities": []}, {"text": "We present a processing model in which collocations, if present in a lexicon, are identified in the input sentence during the analysis of that sentence.", "labels": [], "entities": []}, {"text": "At the same time, they are used to rank competing parsing hypotheses.", "labels": [], "entities": []}, {"text": "The paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews the previous work on the interrelation between parsing and processing of collocations (or, more generally, MWEs).", "labels": [], "entities": []}, {"text": "Section 3 introduces our approach, and section 4 evaluates it by comparing it against the standard non-simultaneous approach.", "labels": [], "entities": []}, {"text": "Section 5 provides concluding remarks and presents directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the experiments we performed in order to evaluate the precision and recall of the method introduced in section 3, and to compare it against the previous method (fully described in).", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9994271993637085}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9988995790481567}]}, {"text": "We extend this comparison by performing a task-based evaluation, which investigates the impact that the new method has on the quality of translations produced by a machine translation system relying on our parser ().", "labels": [], "entities": []}, {"text": "The data considered in this experiment consist of a subpart of a corpus of newspaper articles collected from the on-line version of The Economist 8 , containing slightly more that 0.5 million words.", "labels": [], "entities": [{"text": "newspaper articles collected from the on-line version of The Economist 8", "start_pos": 75, "end_pos": 147, "type": "DATASET", "confidence": 0.7953736782073975}]}, {"text": "On these data, we run two versions of our parser: \u2022 V1: aversion implementing the previous method of collocation identification, \u2022 V2: aversion implementing the new method described in section 3.", "labels": [], "entities": [{"text": "collocation identification", "start_pos": 101, "end_pos": 127, "type": "TASK", "confidence": 0.7462922036647797}]}, {"text": "The lexicon of the parser was kept constant, which is to say that both versions used the same lexicon (which contains slightly more than 7500 English collocation entries), only the parsing module handling collocations was different.", "labels": [], "entities": []}, {"text": "From the output of each parser version, we collected statistics on the number of collocations (present in the lexicon) that were identified in the test corpus.", "labels": [], "entities": []}, {"text": "More precisely, we traversed the output trees and counted the items that were marked as collocation heads, each time this was the case (note that an item may participate in several collocations, not only one).", "labels": [], "entities": []}, {"text": "presents the number of collocations identified, both with respect to collocation instances and collocation types.", "labels": [], "entities": []}, {"text": "As the results show, the new method (column V2) is more efficient in retrieving collocation instances.", "labels": [], "entities": []}, {"text": "It detects 696 more instances, which correspond to an increase of 14.8% relative to the previous method (column V1).", "labels": [], "entities": []}, {"text": "As we lack the means to compare on a large scale the corresponding syntactic trees, we can only speculate that the increase is mainly due to the fact that more appropriate analyses are produced by the new method.", "labels": [], "entities": []}, {"text": "A large number of instances are found by both versions of the parser.", "labels": [], "entities": []}, {"text": "The difference between the two methods is more visible for some syntactic types than for others.", "labels": [], "entities": []}, {"text": "details the number of instances of each syntactic type which are retrieved exclusively by one method or by the other.", "labels": [], "entities": []}, {"text": "To measure the precision of the two methods, we randomly selected 20 collocation instances among those identified by each version of the parser, V1 and V2, and manually checked whether these instances are correct.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9986729621887207}]}, {"text": "Correctness means that in the given context (i.e., the sentence in which they were identified), the word combination marked as instance of a lexicalized collocation is indeed an instance of that collocation.", "labels": [], "entities": []}, {"text": "A counterexample would be, for instance, to mark the pair decision -make in the sentence in   an instance of the verb-object collocation to make a decision, which is an entry in our lexicon.", "labels": [], "entities": []}, {"text": "The decision to make an offer to buy or sell property at price is a management decision that cannot be delegated to staff.", "labels": [], "entities": []}, {"text": "Since judging the correctness of a collocation instance in context is a rather straightforward task, we do not require multiple judges for this evaluation.", "labels": [], "entities": []}, {"text": "The precision obtained is 90% for V1, and 100% for V2.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996498823165894}]}, {"text": "The small size of test set is motivated by the fact that the precision is expected to be very high, since the presence of both collocation components in a sentence in the relevant syntactic relation almost certainly means that the recognition of the corresponding collocation is justified.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9996389150619507}]}, {"text": "Exceptions would correspond to a minority of cases in which the parser either wrongly establishes a relation between two items which happen to belong to an entry in the lexicon, or the two items are related but the combination corresponds to a literal usage (examples are provided later in this section).", "labels": [], "entities": []}, {"text": "The errors of V1 correspond, in fact, to cases in which a combination of words used literally was wrongly attributed to a collocation: in example (8a), V1 assigned the words on and business to the lexical entry on business, and in example (8b), it assigned in and country to the entry in the country 9 . (8)a.", "labels": [], "entities": [{"text": "V1", "start_pos": 14, "end_pos": 16, "type": "DATASET", "confidence": 0.6396596431732178}]}, {"text": "It is not, by any means, specific to the countryside, but it falls especially heavily on small businesses. b. Industrial labour costs in western Germany are higher than in any other country.", "labels": [], "entities": []}, {"text": "To better pinpoint the difference between V1 and V2, we performed a similar evaluation on an additional set of 20 instances, randomly selected among the collocations identified exclusively by each method.", "labels": [], "entities": []}, {"text": "Thus, the precision of V1, when measured on the tokens in \"V1 only\", was 65%.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9997001886367798}, {"text": "V1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.723432719707489}]}, {"text": "The precision of V2 on \"V2 only\" was 90%.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9997072815895081}]}, {"text": "The 2 errors of V2 concern the pair in country, found in contexts similar to the one shown in example (8b).", "labels": [], "entities": []}, {"text": "The errors of V1 also concerned the same pair, with one exception -the identification of the collocation world trade from the context the destruction of the World Trade Centre.", "labels": [], "entities": [{"text": "World Trade Centre", "start_pos": 157, "end_pos": 175, "type": "DATASET", "confidence": 0.9692054390907288}]}, {"text": "Since World Trade Centre is not in the parser lexicon, V1 analysed it and assigned the first two words to the entry world trade.", "labels": [], "entities": [{"text": "World Trade Centre", "start_pos": 6, "end_pos": 24, "type": "DATASET", "confidence": 0.9787999391555786}]}, {"text": "World was wrongly attached to Trade, rather than to Centre.", "labels": [], "entities": [{"text": "World", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9165357351303101}, {"text": "Trade", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.9388200640678406}, {"text": "Centre", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.900966465473175}]}, {"text": "When reported on the totality of the instances tested, the precision of V1 is 77.5% and that of V2 is 95%.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9997795224189758}, {"text": "V1", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9819098711013794}]}, {"text": "Besides the increase in the precision of identified collocations, the new method also contributes to an increase in the parser coverage 10 , from 81.7% to 83.3%.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9987635612487793}, {"text": "parser coverage 10", "start_pos": 120, "end_pos": 138, "type": "METRIC", "confidence": 0.6549468537171682}]}, {"text": "The V1 parser version succeeds in building a complete parse tree for 23187 of the total 28375 sentences in the corpus, while V2 does so for 23629 sentences.", "labels": [], "entities": []}, {"text": "To compare the recall of two methods we performed a similar experiment, in which we run the two versions of the parser, V1 and V2, on a small collection of sentences containing annotated collocation instances.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9855082631111145}]}, {"text": "These sentences were randomly selected from the Europarl corpus ().", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9939764440059662}]}, {"text": "The collocations they contain are all verb-object collocations.", "labels": [], "entities": []}, {"text": "We limit our present investigation to this syntactic type for two reasons: a) annotating a corpus with all instances of collocation entries in the lexicon would be a time-consuming task; and b) verb-object collocations are among the most syntactically flexible and therefore difficult to detect in real texts.", "labels": [], "entities": []}, {"text": "Thus, this test set provides realistic information on recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9951915740966797}]}, {"text": "The test set is divided in two parts: 100 sentences are in English, and 100 other in Italian, which allows fora cross-linguistic evaluation of the two methods.", "labels": [], "entities": []}, {"text": "Each sentence contains one annotated collocation instance, and there are 10 instances fora collocation type.", "labels": [], "entities": []}, {"text": "lists the collocation types in the test set (the even rows in column 2 display the glosses for the words in the Italian collocations)..", "labels": [], "entities": []}, {"text": "Collocation types in the test set.", "labels": [], "entities": []}, {"text": "In addition to reporting the performance results by using the standard measures of precision and recall, we performed a task-based performance evaluation, in which we quantified the impact that the newly-proposed method has on the quality of the output of a machine translation system.", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9992566704750061}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9980229139328003}]}, {"text": "As the examples in table 3 suggest, a literal translation of collocations is rarely the most appropriate.", "labels": [], "entities": []}, {"text": "In fact, as stated by, knowledge of collocations is crucial for machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.802885502576828}]}, {"text": "An important purpose in identifying collocations with our parser is to enable their proper treatment in our translation system, a rule-based system that performs syntactic transfer by relying on the structures produced by the parser.", "labels": [], "entities": [{"text": "syntactic transfer", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.7263221591711044}]}, {"text": "In this system, the translation of a collocation takes place as follows.", "labels": [], "entities": []}, {"text": "When the parser identifies a collocation in the source sentence, its component words are marked as collocation members, in order to prevent their literal translation.", "labels": [], "entities": []}, {"text": "When the transfer module processes the collocation head, the system checks in the bilingual lexicon whether an entry exists for that collocation.", "labels": [], "entities": []}, {"text": "If not, the literal translation will apply; otherwise, the transfer module projects a targetlanguage structure as specified in the corresponding target lexical entry.", "labels": [], "entities": []}, {"text": "More precisely, the transfer yields a target language abstract representation, to which grammatical transformations and morphological generation will apply to create the target sentence.", "labels": [], "entities": []}, {"text": "The identification of collocations in the source text is a necessary, yet not a sufficient condition for their successful translation.", "labels": [], "entities": []}, {"text": "In this experiment, we considered the test set described in section 4.2 and we manually evaluated the translation obtained for each collocation instance.", "labels": [], "entities": []}, {"text": "Both subsets (100 English sentences and 100 Italian sentences) were translated into French.", "labels": [], "entities": []}, {"text": "We compared the translations obtai-  ned by relying on the versions V1 and V2 of our parser (recall that V2 corresponds to the newlyproposed method and V1 to the previous method).", "labels": [], "entities": []}, {"text": "The use of automatic metrics for evaluating the translation output was not considered appropriate in this context, since such n-gram based metrics underestimate the effect that the substitution of a single word (like in our case, the verb in a verbobject collocation) has on the fluency, adequacy, and even on the interpretability of the output sentence.", "labels": [], "entities": []}, {"text": "The comparison showed that, for both language pairs considered (English-French and ItalianFrench), the version of parser which integrates the new method is indeed more useful for the machine translation system than the previous version.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.7792751491069794}]}, {"text": "When V2 was used, 10 more collocation instances were correctly translated from English to French than when using V1.", "labels": [], "entities": []}, {"text": "For the Italian-French pair, V2 helped correctly translating 16 more collocation instances in comparison with V1.", "labels": [], "entities": []}, {"text": "This corresponds to an increase in precision of 13% on the whole test set of 200 sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9995002746582031}]}, {"text": "The increase in performance obtained in all the experiments described in this section is summarized in table 5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Collocation identification results.", "labels": [], "entities": [{"text": "Collocation identification", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.9500129222869873}]}, {"text": " Table 2. Differences between the two methods:  number of tokens retrieved exclusively by each  method.", "labels": [], "entities": []}, {"text": " Table 4. Recall evaluation results: number of cor- rect collocation instances identified.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9573728442192078}]}, {"text": " Table 5. Summary of evaluation results.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7899212837219238}]}]}