{"title": [{"text": "L 1 Regularized Regression for Reranking and System Combination in Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7576787769794464}]}], "abstractContent": [{"text": "We use L 1 regularized transductive regression to learn mappings between source and target features of the training sets derived for each test sentence and use these mappings to rerank translation outputs.", "labels": [], "entities": []}, {"text": "We compare the effectiveness of L 1 regularization techniques for regression to learn mappings between features given in a sparse feature matrix.", "labels": [], "entities": []}, {"text": "The results show the effectiveness of using L 1 regulariza-tion versus L 2 used in ridge regression.", "labels": [], "entities": []}, {"text": "We show that regression mapping is effective in reranking translation outputs and in selecting the best system combinations with encouraging results on different language pairs.", "labels": [], "entities": [{"text": "regression mapping", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.9440403878688812}]}], "introductionContent": [{"text": "Regression can be used to find mappings between the source and target feature sets derived from given parallel corpora.", "labels": [], "entities": []}, {"text": "Transduction learning uses a subset of the training examples that are closely related to the test set without using the model induced by the full training set.", "labels": [], "entities": [{"text": "Transduction learning", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9462049007415771}]}, {"text": "In the context of SMT, we select a few training instances for each test instance to guide the translation process.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9966905117034912}, {"text": "translation", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.970648467540741}]}, {"text": "This also gives us a computational advantage when considering the high dimensionality of the problem.", "labels": [], "entities": []}, {"text": "The goal in transductive regression based machine translation (TRegMT) is both reducing the computational burden of the regression approach by reducing the dimensionality of the training set and the feature set and also improving the translation quality by using transduction.", "labels": [], "entities": [{"text": "transductive regression based machine translation (TRegMT)", "start_pos": 12, "end_pos": 70, "type": "TASK", "confidence": 0.75361168384552}]}, {"text": "Transductive regression is shown to achieve higher accuracy than L 2 regularized ridge regression on some machine learning benchmark datasets (.", "labels": [], "entities": [{"text": "Transductive regression", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7814344763755798}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9988107681274414}]}, {"text": "In an idealized feature mapping matrix where features are word sequences, we would like to observe few target features for each source feature derived from a source sentence.", "labels": [], "entities": []}, {"text": "In this setting, we can think of feature mappings being close to permutation matrices with one nonzero item for each column.", "labels": [], "entities": []}, {"text": "L 1 regularization helps us achieve solutions close to the permutation matrices by increasing sparsity.", "labels": [], "entities": []}, {"text": "We show that L 1 regularized regression mapping is effective in reranking translation outputs and present encouraging results on different language pairs in the translation task of WMT10.", "labels": [], "entities": [{"text": "WMT10", "start_pos": 181, "end_pos": 186, "type": "DATASET", "confidence": 0.8992539644241333}]}, {"text": "In the system combination task, different translation outputs of different translation systems are combined to find a better translation.", "labels": [], "entities": []}, {"text": "We model system combination task as a reranking problem among the competing translation models and present encouraging results with the TRegMT system.", "labels": [], "entities": []}, {"text": "Related Work: Regression techniques can be used to model the relationship between strings (.", "labels": [], "entities": []}, {"text": "applies a string-to-string mapping approach to machine translation by using ordinary least squares regression and n-gram string kernels to a small dataset.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.801763266324997}]}, {"text": "Later they use L 2 regularized least squares regression (.", "labels": [], "entities": []}, {"text": "Although the translation quality they achieve is not better than Moses (, which is accepted to be the state-of-the-art, they show the feasibility of the approach.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9681199193000793}]}, {"text": "use kernel regression to find translation mappings from source to target feature vectors and experiment with translating hotel front desk requests.", "labels": [], "entities": [{"text": "translating hotel front desk requests", "start_pos": 109, "end_pos": 146, "type": "TASK", "confidence": 0.8981919646263122}]}, {"text": "approaches the transductive learning problem for SMT by bootstrapping the training using the translations produced by the SMT system that have a scoring performance above some threshold as estimated by the SMT system itself.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9936000108718872}]}, {"text": "Outline: Section 2 gives an overview of regression based machine translation, which is used to find the mappings between the source and target features of the training set.", "labels": [], "entities": [{"text": "regression based machine translation", "start_pos": 40, "end_pos": 76, "type": "TASK", "confidence": 0.6396385058760643}]}, {"text": "In section 3 we present L 1 regularized transductive regression for alignment learning.", "labels": [], "entities": [{"text": "alignment learning", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.9801090657711029}]}, {"text": "Section 4 presents our experiments, instance selection techniques, and results on the translation task for WMT10.", "labels": [], "entities": [{"text": "translation task", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.9252255856990814}, {"text": "WMT10", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.7885575890541077}]}, {"text": "In section 5, we present the results on the system combination task using reranking.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments on the translation task of the English-German, German-English, EnglishFrench, English-Spanish, and English-Czech language pairs using the training corpus provided in WMT10.", "labels": [], "entities": [{"text": "translation task", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.8924131989479065}, {"text": "WMT10", "start_pos": 189, "end_pos": 194, "type": "DATASET", "confidence": 0.9530535936355591}]}, {"text": "We developed separate SMT models using Moses () with default settings with maximum sentence length set to 80 using 5-gram language model and obtained distinct 100-best lists for the test sets.", "labels": [], "entities": [{"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9850029349327087}]}, {"text": "All systems were tuned with 2051 sentences and tested with 2525 sentences.", "labels": [], "entities": []}, {"text": "We have randomly picked 100 instances from the development set to be used in tuning the regression experiments (dev.100).", "labels": [], "entities": []}, {"text": "The translation challenge test set contains 2489 sentences.", "labels": [], "entities": [{"text": "translation challenge test set", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.8344383239746094}]}, {"text": "Number of sentences in the training set of each system and baseline performances for uncased output (test set BLEU, challenge test set BLEU) are given in.: Initial uncased performances of the translation systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.8946847319602966}, {"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.6092827916145325}]}, {"text": "Feature mappers used are 3-spectrum counting word kernels, which consider all N -grams up to order 3 weighted by the number of tokens in the feature.", "labels": [], "entities": []}, {"text": "We segment sentences using some of the punctuation for managing the feature set better and do not consider N -grams that cross segments.", "labels": [], "entities": []}, {"text": "We use BLEU () and NIST) evaluation metrics for measuring the performance of translations automatically.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9968829154968262}, {"text": "NIST", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.8888470530509949}]}, {"text": "We rerank N -best lists by using linear combinations of the following scoring functions: 1.", "labels": [], "entities": []}, {"text": "TRegMT: Transductive regression based machine translation scores as found by Equation 3. 2. TM: Translation model scores we obtain from the baseline SMT system that is used to generate the N -best lists.", "labels": [], "entities": [{"text": "TRegMT", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5488462448120117}, {"text": "Transductive regression based machine translation", "start_pos": 8, "end_pos": 57, "type": "TASK", "confidence": 0.5708046436309815}, {"text": "SMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.9672908186912537}]}, {"text": "3. LM: 5-gram language model scores that the baseline SMT system uses when calculating the translation model scores.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9813733696937561}]}, {"text": "The training set we obtain may not contain all of the features of the reference target due to low coverage.", "labels": [], "entities": []}, {"text": "Therefore, when performing reranking, we also add the cost coming from the features of \u03a6 Y (y) that are not represented in the training set to the squared loss as in: where \u03a6 Y (y) \\ FY represent the features of y not represented in the training set.", "labels": [], "entities": [{"text": "reranking", "start_pos": 27, "end_pos": 36, "type": "TASK", "confidence": 0.9725114703178406}]}, {"text": "We note that TRegMT score only contains ordering information as present in the bi/tri-gram features in the training set.", "labels": [], "entities": [{"text": "TRegMT score", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.8444388508796692}]}, {"text": "Therefore, the addition of a 5-gram LM score as well as the TM score, which also incorporates the LM score in itself, improves the performance.", "labels": [], "entities": [{"text": "TM score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.8691407144069672}]}, {"text": "We are notable to improve the BLEU score when we use TRegMT score by itself however we are able to achieve improvements in the NIST and 1-WER scores.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9811283648014069}, {"text": "TRegMT score", "start_pos": 53, "end_pos": 65, "type": "METRIC", "confidence": 0.821344256401062}, {"text": "NIST", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.8593191504478455}]}, {"text": "The performance increase is important for two reasons.", "labels": [], "entities": []}, {"text": "First of all, we are able to improve the performance using blended spectrum 3-gram features against translations obtained with 5-gram language model and higher order features.", "labels": [], "entities": []}, {"text": "Outperforming higher order n-gram models is known to be a difficult task (.", "labels": [], "entities": []}, {"text": "Secondly, increasing the performance with reranking itself is a hard task since possible translations are already constrained by the ones observed in Nbest lists.", "labels": [], "entities": []}, {"text": "Therefore, an increase in the N -best list size may increase the score gaps.", "labels": [], "entities": [{"text": "N -best list size", "start_pos": 30, "end_pos": 47, "type": "METRIC", "confidence": 0.8931712985038758}]}, {"text": "presents reranking results on all of the language pairs we considered, using TRegMT, TM, and LM scores with the combination weights learned in the development set.", "labels": [], "entities": [{"text": "TRegMT", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.8174708485603333}]}, {"text": "We are able to achieve better BLEU and NIST scores on all of the listed systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9985442161560059}, {"text": "NIST scores", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.5703465044498444}]}, {"text": "We are able to see up to .38 BLEU points increase for the en-es pair.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9986005425453186}]}, {"text": "Oracle reranking performances are obtained by using BLEU scoring metric.", "labels": [], "entities": [{"text": "BLEU scoring metric", "start_pos": 52, "end_pos": 71, "type": "METRIC", "confidence": 0.9377122720082601}]}, {"text": "If we used only the TM and LM scores when reranking with the en-de system, then we would obtain .1309 BLEU and 5.1472 NIST scores.", "labels": [], "entities": [{"text": "TM", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.8412549495697021}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9990222454071045}, {"text": "NIST", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.496831476688385}]}, {"text": "We only see a minor increase in the NIST score and no change in the BLEU score with this setting when compared with the baseline given in.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.554484486579895}, {"text": "BLEU score", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9693019688129425}]}, {"text": "Due to computational reasons, we do not use the same number of instances to train different models.", "labels": [], "entities": []}, {"text": "In our experiments, we used n = 3 for L2, n = 1.5 for FSR, and n = 1.2 for QP and LP solutions to select the number of instances in Equation 9.", "labels": [], "entities": [{"text": "FSR", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.7848063707351685}]}, {"text": "The average number of instances used per sentence in training corresponding to these choices are approximately 140, 74, and 61.", "labels": [], "entities": []}, {"text": "Even with these decreased number of training instances, L 1 regularized regression techniques are able to achieve comparable scores to L 2 regularized regression model in.", "labels": [], "entities": []}, {"text": "We perform experiments on the system combination task for the English-German, GermanEnglish, English-French, English-Spanish, and English-Czech language pairs using the training en-de: Reranking results using TRegMT, TM, and LM scores.", "labels": [], "entities": [{"text": "TRegMT", "start_pos": 209, "end_pos": 215, "type": "METRIC", "confidence": 0.9058215022087097}, {"text": "LM", "start_pos": 225, "end_pos": 227, "type": "METRIC", "confidence": 0.9348311424255371}]}, {"text": "bold correspond to the best score in each rectangle of scores.", "labels": [], "entities": []}, {"text": "We use the training set provided in WMT10 to index and select transductive instances from.", "labels": [], "entities": [{"text": "WMT10", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.9200701713562012}]}, {"text": "The challenge split the test set for the translation task of 2489 sentences into a tuning set of 455 sentences and a test set with the remaining 2034 sentences.", "labels": [], "entities": [{"text": "translation task", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.9271012246608734}]}, {"text": "Translation outputs for each system is given in a separate file and the number of system outputs per translation pair varies.", "labels": [], "entities": []}, {"text": "We have tokenized and lowercased each of the system outputs and combined these in a single N -best file per language pair.", "labels": [], "entities": []}, {"text": "We also segment sentences using some of the punctuation for managing the feature set better.", "labels": [], "entities": []}, {"text": "We use these N -best lists for TRegMT reranking to select the best translation model.", "labels": [], "entities": [{"text": "TRegMT reranking", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7124052345752716}]}, {"text": "Feature mappers used are 3-spectrum counting word kernels, which consider all n-grams up to order 3 weighted by the number of tokens in the feature.", "labels": [], "entities": []}, {"text": "We rerank N -best lists by using combinations of the following scoring functions: 1.", "labels": [], "entities": []}, {"text": "TRegMT: Transductive regression based machine translation scores as found by Equation 3. 2. TM': Translation model scores are obtained by measuring the average BLEU performance of each translation relative to the other translations in the N -best list.", "labels": [], "entities": [{"text": "TRegMT", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5654733180999756}, {"text": "Transductive regression based machine translation", "start_pos": 8, "end_pos": 57, "type": "TASK", "confidence": 0.56473388671875}, {"text": "BLEU", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9984270334243774}]}, {"text": "3. LM: We calculate 5-gram language model scores for each translation using the language model trained over the target corpus provided in the translation task.", "labels": [], "entities": []}, {"text": "Since we do not have access to the reference translations nor to the translation model scores each system obtained for each sentence, we estimate translation model performance (TM') by measuring the average BLEU performance of each translation relative to the other translations in the N -best list.", "labels": [], "entities": [{"text": "translation model performance (TM')", "start_pos": 146, "end_pos": 181, "type": "METRIC", "confidence": 0.6532923430204391}, {"text": "BLEU", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.9985561966896057}]}, {"text": "Thus, each possible translation in the N -best list is BLEU scored against other translations and the average of these scores is selected as the TM score for the sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9992672801017761}, {"text": "TM", "start_pos": 145, "end_pos": 147, "type": "METRIC", "confidence": 0.9907943606376648}]}, {"text": "Sentence level BLEU score calculation avoids singularities in ngram precisions by taking the maximum of the match count and 1 2|s i | for |s i | denoting the length of the source sentence s i as used in. presents reranking results on all of the language pairs we considered, using TRegMT, TM, and LM scores with the same combination weights as above.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9169823527336121}]}, {"text": "Random model score lists the random model performance selected among the competing translations randomly and it is used as a baseline.", "labels": [], "entities": []}, {"text": "Best model score lists the performance of the best model performance.", "labels": [], "entities": []}, {"text": "We are able to achieve better BLEU and NIST scores in all of the listed systems except for the de-en language pair when compared with the performance of the best competing translation system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9981151819229126}]}, {"text": "The lower performance in the de-en language pair maybe due to having a single best translation system that outperforms others significantly.", "labels": [], "entities": []}, {"text": "The difference between the best model performance and the mean as well as the variance of the scores in the de-en language pair is about twice their counterparts in en-de language pair.", "labels": [], "entities": []}, {"text": "Due to computational reasons, we do not use the same number of instances to train different models.", "labels": [], "entities": []}, {"text": "In our experiments, we used n = 4 for L2, n = 1.5 for FSR, and n = 1.2 for QP and LP solutions to select the number of instances in Equation 9.", "labels": [], "entities": [{"text": "FSR", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.7962164878845215}]}, {"text": "The average number of instances used per sentence in training corresponding to these choices are approximately 189, 78, and 64.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Initial uncased performances of the trans- lation systems.", "labels": [], "entities": []}, {"text": " Table 2: Reranking results using TRegMT, TM, and LM scores. We use approximate randomization  test", "labels": [], "entities": [{"text": "Reranking", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.7426274418830872}, {"text": "TRegMT", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9236137270927429}, {"text": "LM", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9383225440979004}]}]}