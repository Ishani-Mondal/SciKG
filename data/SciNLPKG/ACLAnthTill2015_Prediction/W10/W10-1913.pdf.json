{"title": [{"text": "Identifying the Information Structure of Scientific Abstracts: An Investigation of Three Different Schemes", "labels": [], "entities": [{"text": "Identifying the Information Structure of Scientific Abstracts", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.9054352981703622}]}], "abstractContent": [{"text": "Many practical tasks require accessing specific types of information in scientific literature; e.g. information about the objective , methods, results or conclusions of the study in question.", "labels": [], "entities": []}, {"text": "Several schemes have been developed to characterize such information in full journal papers.", "labels": [], "entities": []}, {"text": "Yet many tasks focus on abstracts instead.", "labels": [], "entities": []}, {"text": "We take three schemes of different type and granularity (those based on section names, argumentative zones and conceptual structure of documents) and investigate their applicability to biomedical abstracts.", "labels": [], "entities": []}, {"text": "We show that even for the finest-grained of these schemes, the majority of categories appear in abstracts and can be identified relatively reliably using machine learning.", "labels": [], "entities": []}, {"text": "We discuss the impact of our results and the need for subsequent task-based evaluation of the schemes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Scientific abstracts tend to be very similar in terms of their information structure.", "labels": [], "entities": []}, {"text": "For example, many abstracts provide some background information before defining the precise objective of the study, and the conclusions are typically preceded by the description of the results obtained.", "labels": [], "entities": []}, {"text": "Many readers of scientific abstracts are interested in specific types of information only, e.g. the general background of the study, the methods used in the study, or the results obtained.", "labels": [], "entities": []}, {"text": "Accordingly, many text mining tasks focus on the extraction of information from certain parts of abstracts only.", "labels": [], "entities": [{"text": "text mining", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.8503206670284271}]}, {"text": "Therefore classification of abstracts (or full articles) according to the categories of information structure can support both the manual study of scientific literature as well as its automatic analysis, e.g. information extraction, summarization and information retrieval.", "labels": [], "entities": [{"text": "classification of abstracts (or full articles)", "start_pos": 10, "end_pos": 56, "type": "TASK", "confidence": 0.7615547850728035}, {"text": "information extraction", "start_pos": 209, "end_pos": 231, "type": "TASK", "confidence": 0.7939624190330505}, {"text": "summarization", "start_pos": 233, "end_pos": 246, "type": "TASK", "confidence": 0.990620493888855}, {"text": "information retrieval", "start_pos": 251, "end_pos": 272, "type": "TASK", "confidence": 0.7886832058429718}]}, {"text": "To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. ().", "labels": [], "entities": [{"text": "sentence-based classification of scientific literature", "start_pos": 77, "end_pos": 131, "type": "TASK", "confidence": 0.7632305026054382}]}, {"text": "Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (.", "labels": [], "entities": []}, {"text": "Others are finer-grained and based e.g. on argumentative zones (, qualitative dimensions () or conceptual structure () of documents.", "labels": [], "entities": []}, {"text": "The majority of such schemes have been developed for full scientific journal articles which are richer in information and also considered to be more in need of the definition of information structure.", "labels": [], "entities": []}, {"text": "However, many practical tasks currently focus on abstracts.", "labels": [], "entities": []}, {"text": "As a distilled summary of key information in full articles, abstracts may exhibit an entirely different distribution of scheme categories than full articles.", "labels": [], "entities": []}, {"text": "For tasks involving abstracts, it would be useful to know which schemes are applicable to abstracts and which can be automatically identified in them with reasonable accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.987417459487915}]}, {"text": "In this paper, we will compare the applicability of three different schemes -those based on section names, argumentative zones and conceptual structure of documents -to a collection of biomedical abstracts used for cancer risk assessment (CRA).", "labels": [], "entities": [{"text": "cancer risk assessment (CRA)", "start_pos": 215, "end_pos": 243, "type": "TASK", "confidence": 0.7894668579101562}]}, {"text": "CRA is an example of a real-world task which could greatly benefit from knowledge about the information structure of abstracts since cancer risk assessors look fora variety of information in them ranging from specific methods to results concerning different chemicals ).", "labels": [], "entities": []}, {"text": "We report work on the annotation of CRA abstracts according to each scheme and investigate the schemes in terms of their distribution, mutual overlap, and the success of identifying them automatically using machine learning.", "labels": [], "entities": []}, {"text": "Our investigation provides an initial idea of the practical usefulness of the schemes for tasks involving abstracts.", "labels": [], "entities": []}, {"text": "We discuss the impact of our results and the further task-based evaluation which we intend to conduct in the context of CRA.", "labels": [], "entities": [{"text": "CRA", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9229832291603088}]}], "datasetContent": [{"text": "We used Weka for the classification, employing its NB and SVM linear kernel.", "labels": [], "entities": [{"text": "classification", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.9627906084060669}]}, {"text": "The results were measured in terms of accuracy (the percentage of correctly classified sentences), precision, recall, and F-Measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9994254112243652}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9997455477714539}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9997856020927429}, {"text": "F-Measure", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9987905621528625}]}, {"text": "We used 10-fold cross validation to avoid the possible bias introduced by relying on anyone particular split of the data.", "labels": [], "entities": []}, {"text": "The data were randomly divided into ten parts of approximately the same size.", "labels": [], "entities": []}, {"text": "Each individual part was retained as test data and the remaining nine parts were used as training data.", "labels": [], "entities": []}, {"text": "The process was repeated ten times with each part used once as the test data.", "labels": [], "entities": []}, {"text": "The resulting ten estimates were then combined to give a final score.", "labels": [], "entities": []}, {"text": "We compare our classifiers against a baseline method based on random sampling of category labels from training data and their assignment to sentences on the basis of their observed distribution.", "labels": [], "entities": []}, {"text": "shows F-measure results when using each individual feature alone, and when using all the features but the individual feature in question.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9973179697990417}]}, {"text": "In these two tables, we only report the results for SVM which performed considerably better than NB.", "labels": [], "entities": [{"text": "SVM", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.6286225914955139}, {"text": "NB", "start_pos": 97, "end_pos": 99, "type": "DATASET", "confidence": 0.8626521229743958}]}, {"text": "Although we have results for most scheme categories, the results for some are missing due to the lack of sufficient training data (see), or due to a small feature set (e.g. History alone).", "labels": [], "entities": [{"text": "History", "start_pos": 173, "end_pos": 180, "type": "DATASET", "confidence": 0.9227043390274048}]}, {"text": "Looking at individual features alone, Word, Bi-gram and Verb perform the best for all the schemes, and History and Voice perform the worst.", "labels": [], "entities": []}, {"text": "In fact History performs very well on the training data, but for the test data we can only use estimates rather than the actual labels.", "labels": [], "entities": []}, {"text": "The Voice feature works only for RES and METH for S1 and S2, and for OBS for S3.", "labels": [], "entities": [{"text": "RES", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.49078091979026794}, {"text": "METH", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.7929332852363586}, {"text": "OBS", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.9045207500457764}]}, {"text": "This feature is probably only meaningful for some of the categories.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Distribution of sentences in the scheme- annotated CRA corpus", "labels": [], "entities": []}, {"text": " Table 3: Association measures between schemes S1, S2, S3", "labels": [], "entities": [{"text": "Association", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9316009283065796}]}, {"text": " Table 4: F-Measure results when using each in- dividual feature alone", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9968363046646118}]}, {"text": " Table 5: F-Measure results using all the features and  all but one of the features", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9833407998085022}]}, {"text": " Table 6: Baseline and best NB and SVM results", "labels": [], "entities": [{"text": "Baseline", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8910863995552063}]}]}