{"title": [], "abstractContent": [{"text": "This paper explores ways to detect errors in aligned corpora, using very little technology.", "labels": [], "entities": []}, {"text": "In the first method, applicable to any aligned corpus, we consider alignment as a string-to-string mapping.", "labels": [], "entities": []}, {"text": "Treating the target string as a label, we examine each source string to find inconsistencies in alignment.", "labels": [], "entities": []}, {"text": "Despite setting up the problem on a par with grammatical annotation, we demonstrate crucial differences in sorting errors from legitimate variations.", "labels": [], "entities": []}, {"text": "The second method examines phrase nodes which are predicted to be aligned, based on the alignment of their yields.", "labels": [], "entities": []}, {"text": "Both methods are effective in complementary ways.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parallel corpora-texts and their translationshave become essential in the development of machine translation (MT) systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.8456208348274231}]}, {"text": "Alignment quality is crucial to these corpora; as Tiedemann (2003) states, \"he most important feature of texts and their translations is the correspondence between source and target segments\" (p. 2).", "labels": [], "entities": []}, {"text": "While being useful for translation studies and foreign language pedagogy (see, e.g.,, PARAL-LEL TREEBANKS-syntactically-annotated parallel corpora-offer additional useful information for machine translation, cross-language information retrieval, and word-sense disambiguation (see, e.g.,, While high-quality alignments are desirable, even gold standard annotation can contain annotation errors.", "labels": [], "entities": [{"text": "translation", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.9683629870414734}, {"text": "PARAL-LEL TREEBANKS-syntactically-annotated", "start_pos": 86, "end_pos": 129, "type": "METRIC", "confidence": 0.7782014012336731}, {"text": "machine translation", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.7905921339988708}, {"text": "cross-language information retrieval", "start_pos": 208, "end_pos": 244, "type": "TASK", "confidence": 0.7343836228052775}, {"text": "word-sense disambiguation", "start_pos": 250, "end_pos": 275, "type": "TASK", "confidence": 0.7532477676868439}]}, {"text": "For other forms of linguistic annotation, the presence of errors has been shown to create various problems, from unreliable training and evaluation of NLP technology (e.g.,) to low precision and recall of queries for already rare linguistic phenomena (e.g.,.", "labels": [], "entities": [{"text": "precision", "start_pos": 181, "end_pos": 190, "type": "METRIC", "confidence": 0.9980270266532898}, {"text": "recall", "start_pos": 195, "end_pos": 201, "type": "METRIC", "confidence": 0.9963805079460144}]}, {"text": "Even a small number of errors can have a significant impact on the uses of linguistic annotation, e.g., changing the assessment of parsers (e.g.,.", "labels": [], "entities": []}, {"text": "One could remove potentially unfavorable sentence pairs when training a statistical MT system, to avoid incorrect word alignments, but this removes all relevant data from those sentences and does not help evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.8662026524543762}]}, {"text": "We thus focus on detecting errors in the annotation of alignments.", "labels": [], "entities": []}, {"text": "Annotation error detection has been explored for part-of-speech (POS) annotation (e.g.,) and syntactic annotation (e.g.,), but there have been few, if any, attempts to develop general approaches to error detection for aligned corpora.", "labels": [], "entities": [{"text": "Annotation error detection", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7139925559361776}, {"text": "error detection", "start_pos": 198, "end_pos": 213, "type": "TASK", "confidence": 0.7502674162387848}]}, {"text": "Alignments are different in nature, as the annotation does not introduce abstract categories such as POS, but relies upon defining translation units with equivalent meanings.", "labels": [], "entities": []}, {"text": "We use the idea that variation in annotation can indicate errors (section 2), for consistency checking of alignments, as detailed in section 3.", "labels": [], "entities": [{"text": "consistency checking of alignments", "start_pos": 82, "end_pos": 116, "type": "TASK", "confidence": 0.7198563367128372}]}, {"text": "In section 4, we outline language-independent heuristics to sort true ambiguities from errors, and evaluate them on a parallel treebank in section 5.", "labels": [], "entities": []}, {"text": "In section 6 we turn to a complementary method, exploiting compositional properties of aligned treebanks, to align more nodes.", "labels": [], "entities": []}, {"text": "The methods are simple, effective, and applicable to any aligned treebank.", "labels": [], "entities": []}, {"text": "As far as we know, this is the first attempt to thoroughly investigate and empirically verify error detection methods for aligned corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "The method returns 318 cases, in addition to 135 cases with multiple source/target phrases and 104 predicted non-alignments.", "labels": [], "entities": []}, {"text": "To evaluate, we sampled 55 of the 318 flagged phrases and found that 25 should have been aligned as suggested.", "labels": [], "entities": []}, {"text": "21 of the phrases have zero difference in length between source and target, while 34 have differences of up to 9 tokens.", "labels": [], "entities": []}, {"text": "Of the phrases with zerolength difference, 18 should have been aligned (precision=85.7%), while only 7 with length differences should have been aligned.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9981661438941956}]}, {"text": "This is inline with previous findings that length difference can help predict alignment (cf., e.g.,.", "labels": [], "entities": []}, {"text": "About half of all phrase pairs that should be aligned should be EXACT, regardless of the length difference.", "labels": [], "entities": [{"text": "EXACT", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9901607036590576}]}, {"text": "The method is good at predicting the alignment of one-word phrases, e.g., pronouns, as in (3).", "labels": [], "entities": []}, {"text": "Of the 11 suggested alignments where both source and target have a length of 1, all were correct suggestions.", "labels": [], "entities": []}, {"text": "This is not surprising, since all words under the phrases are (trivially) aligned.", "labels": [], "entities": []}, {"text": "Although shorter phrases with short length differences generally means a higher rate of correct suggestions, we do not want to filter out items based on phrase length, since there are outliers that are correct suggestions, e.g., phrase pairs with lengths of 15 and 13 (difference=2) or 31 and 36 (difference=5).", "labels": [], "entities": []}, {"text": "It is worth noting that checking the suggestions took very little time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Error precision and recall", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9941293001174927}, {"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9557178616523743}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9990828037261963}]}]}