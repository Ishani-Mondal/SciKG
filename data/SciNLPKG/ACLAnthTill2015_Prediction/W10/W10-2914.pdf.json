{"title": [{"text": "Semi-Supervised Recognition of Sarcastic Sentences in Twitter and Amazon", "labels": [], "entities": [{"text": "Recognition of Sarcastic Sentences", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.7493834942579269}, {"text": "Amazon", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.5897883176803589}]}], "abstractContent": [{"text": "Sarcasm is a form of speech act in which the speakers convey their message in an implicit way.", "labels": [], "entities": []}, {"text": "The inherently ambiguous nature of sarcasm sometimes makes it hard even for humans to decide whether an utterance is sarcastic or not.", "labels": [], "entities": []}, {"text": "Recognition of sarcasm can benefit many sentiment analysis NLP applications, such as review sum-marization, dialogue systems and review ranking systems.", "labels": [], "entities": [{"text": "Recognition of sarcasm", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8556873599688212}, {"text": "sentiment analysis NLP", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.9343113700548807}]}, {"text": "In this paper we experiment with semi-supervised sarcasm identification on two very different data sets: a collection of 5.9 million tweets collected from Twit-ter, and a collection of 66000 product reviews from Amazon.", "labels": [], "entities": [{"text": "sarcasm identification", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.8503870069980621}]}, {"text": "Using the Mechanical Turk we created a gold standard sample in which each sentence was tagged by 3 annotators, obtaining F-scores of 0.78 on the product reviews dataset and 0.83 on the Twitter dataset.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9985449314117432}, {"text": "product reviews dataset", "start_pos": 145, "end_pos": 168, "type": "DATASET", "confidence": 0.6831866900126139}, {"text": "Twitter dataset", "start_pos": 185, "end_pos": 200, "type": "DATASET", "confidence": 0.9289364218711853}]}, {"text": "We discuss the differences between the datasets and how the algorithm uses them (e.g., for the Amazon dataset the algorithm makes use of struc-tured information).", "labels": [], "entities": [{"text": "Amazon dataset", "start_pos": 95, "end_pos": 109, "type": "DATASET", "confidence": 0.9428005218505859}]}, {"text": "We also discuss the utility of Twitter #sarcasm hashtags for the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sarcasm (also known as verbal irony) is a sophisticated form of speech act in which the speakers convey their message in an implicit way.", "labels": [], "entities": [{"text": "Sarcasm (also known as verbal irony)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.636119369417429}]}, {"text": "One inherent characteristic of the sarcastic speech act is that it is sometimes hard to recognize.", "labels": [], "entities": []}, {"text": "The difficulty in recognition of sarcasm causes misunderstanding in everyday communication and poses problems to many NLP systems such as online review summarization systems, dialogue systems or brand monitoring systems due to the failure of state of the art sentiment analysis systems to detect sarcastic comments.", "labels": [], "entities": [{"text": "recognition of sarcasm", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.8144433299700419}, {"text": "online review summarization", "start_pos": 138, "end_pos": 165, "type": "TASK", "confidence": 0.6006843447685242}]}, {"text": "In this paper we experiment with a semi-supervised framework for automatic identification of sarcastic sentences.", "labels": [], "entities": [{"text": "automatic identification of sarcastic sentences", "start_pos": 65, "end_pos": 112, "type": "TASK", "confidence": 0.7777389407157898}]}, {"text": "One definition for sarcasm is: the activity of saying or writing the opposite of what you mean, or of speaking in away intended to make someone else feel stupid or show them that you are angry).", "labels": [], "entities": []}, {"text": "Using the former definition, sarcastic utterances appear in many forms.", "labels": [], "entities": []}, {"text": "It is best to present a number of examples which show different facets of the phenomenon, followed by a brief review of different aspects of the sarcastic use.", "labels": [], "entities": []}, {"text": "The sentences are all taken from our experimental data sets: 1.", "labels": [], "entities": []}, {"text": "\"thank you Janet Jackson for yet another year of Super Bowl classic rock!\"", "labels": [], "entities": []}, {"text": "(Twitter) Example (1) refers to the supposedly lame music performance in super bowl 2010 and attributes it to the aftermath of the scandalous performance of Janet Jackson in the previous year.", "labels": [], "entities": []}, {"text": "Note that the previous year is not mentioned and the reader has to guess the context (use universal knowledge).", "labels": [], "entities": []}, {"text": "The words yet and another might hint at sarcasm.", "labels": [], "entities": []}, {"text": "Example (2) is composed of three short sentences, each of them sarcastic on its own.", "labels": [], "entities": []}, {"text": "However, combining them in one tweet brings the sarcasm to its extreme.", "labels": [], "entities": []}, {"text": "Example (3) is a factual statement without explicit opinion.", "labels": [], "entities": []}, {"text": "However, having a fast connection is a positive thing.", "labels": [], "entities": []}, {"text": "A possible sarcasm emerges from the over exaggeration ('wow', 'blazing-fast').", "labels": [], "entities": []}, {"text": "Example (4) from Amazon, might be a genuine compliment if it appears in the body of the review.", "labels": [], "entities": []}, {"text": "However, recalling the expression 'don't judge a book by its cover', choosing it as the title of the review reveals its sarcastic nature.", "labels": [], "entities": []}, {"text": "Although the negative sentiment is very explicit in the iPod review (5), the sarcastic effect emerges from the pun that assumes the knowledge that the design is one of the most celebrated features of Apple's products.", "labels": [], "entities": [{"text": "iPod review", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.9182213842868805}]}, {"text": "(None of the above reasoning was directly introduced to our algorithm.)", "labels": [], "entities": []}, {"text": "Modeling the underlying patterns of sarcastic utterances is interesting from the psychological and cognitive perspectives and can benefit various NLP systems such as review summarization () and dialogue systems.", "labels": [], "entities": [{"text": "review summarization", "start_pos": 166, "end_pos": 186, "type": "TASK", "confidence": 0.5986919701099396}]}, {"text": "Following the 'brilliant-butcruel' hypothesis (Danescu-Niculescu-, it can help improve ranking and recommendation systems).", "labels": [], "entities": []}, {"text": "All systems currently fail to correctly classify the sentiment of sarcastic sentences.", "labels": [], "entities": [{"text": "classify the sentiment of sarcastic sentences", "start_pos": 40, "end_pos": 85, "type": "TASK", "confidence": 0.7012481192747752}]}, {"text": "In this paper we utilize the semi-supervised sarcasm identification algorithm (SASI) of.", "labels": [], "entities": [{"text": "sarcasm identification algorithm (SASI)", "start_pos": 45, "end_pos": 84, "type": "TASK", "confidence": 0.6993843764066696}]}, {"text": "The algorithm employs two modules: semi supervised pattern acquisition for identifying sarcastic patterns that serve as features fora classifier, and a classification stage that classifies each sentence to a sarcastic class.", "labels": [], "entities": [{"text": "semi supervised pattern acquisition", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.6249238774180412}]}, {"text": "We experiment with two radically different datasets: 5.9 million tweets collected from Twitter, and 66000 Amazon product reviews.", "labels": [], "entities": []}, {"text": "Although for the Amazon dataset the algorithm utilizes structured information, results for the Twitter dataset are higher.", "labels": [], "entities": [{"text": "Amazon dataset", "start_pos": 17, "end_pos": 31, "type": "DATASET", "confidence": 0.9515132009983063}, {"text": "Twitter dataset", "start_pos": 95, "end_pos": 110, "type": "DATASET", "confidence": 0.9367232620716095}]}, {"text": "We discuss the possible reasons for this, and also the utility of Twitter #sarcasm hashtags for the task.", "labels": [], "entities": []}, {"text": "Our algorithm performed well in both domains, substantially outperforming a strong baseline based on semantic gap and user annotations.", "labels": [], "entities": []}, {"text": "To further test its robustness we also trained the algorithm in across domain manner, achieving good results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Seed and extended training sets (Amazon).", "labels": [], "entities": [{"text": "Amazon)", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.906637966632843}]}, {"text": "As described in the previous section, SASI is semi supervised, hence requires a small seed of annotated data.", "labels": [], "entities": [{"text": "SASI", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.9653041362762451}]}, {"text": "We used the same seed of 80 positive (sarcastic) examples and 505 negative examples described at (.", "labels": [], "entities": []}, {"text": "After automatically expanding the training set, our training data now contains 471 positive examples and 5020 negative examples.", "labels": [], "entities": []}, {"text": "These ratios are We used k = 5 for all experiments.", "labels": [], "entities": []}, {"text": "to be expected, since non-sarcastic sentences outnumber sarcastic ones, definitely when most online reviews are positive (.", "labels": [], "entities": []}, {"text": "This generally positive tendency is also reflected in our data -the average number of stars is 4.12.", "labels": [], "entities": []}, {"text": "Seed training set with #sarcasm (Twitter).", "labels": [], "entities": []}, {"text": "We used a sample of 1500 tweets marked with the #sarcasm hashtag as a positive set that represents sarcasm styles special to Twitter.", "labels": [], "entities": []}, {"text": "However, this set is very noisy (see discussion in Section 5).", "labels": [], "entities": []}, {"text": "Seed training set (cross domain).", "labels": [], "entities": []}, {"text": "Results obtained by training on the 1500 #sarcasm hashtagged tweets were not promising.", "labels": [], "entities": [{"text": "1500 #sarcasm hashtagged tweets", "start_pos": 36, "end_pos": 67, "type": "DATASET", "confidence": 0.7705227136611938}]}, {"text": "Examination of the #sarcasm tagged tweets shows that the annotation is biased and noisy as we discuss in length in Section 5.", "labels": [], "entities": []}, {"text": "A better annotated set was needed in order to properly train the algorithm.", "labels": [], "entities": []}, {"text": "Sarcastic tweets are sparse and hard to find and annotate manually.", "labels": [], "entities": []}, {"text": "In order to overcome sparsity we used the positive seed annotated on the Amazon dataset.", "labels": [], "entities": [{"text": "Amazon dataset", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.9228158295154572}]}, {"text": "The training set was completed by manually selected negative example from the Twitter dataset.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.9045833647251129}]}, {"text": "Note that in this setting our training set is thus of mixed domains.", "labels": [], "entities": []}, {"text": "We used two experimental frameworks to test SASI's accuracy.", "labels": [], "entities": [{"text": "SASI", "start_pos": 44, "end_pos": 48, "type": "TASK", "confidence": 0.9644737839698792}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9983208775520325}]}, {"text": "In the first experiment we evaluated the pattern acquisition process, how consistent it is and to what extent it contributes to correct classification.", "labels": [], "entities": [{"text": "pattern acquisition", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7940368354320526}, {"text": "correct classification", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.558706134557724}]}, {"text": "We did that by 5-fold cross validation over the seed data.", "labels": [], "entities": []}, {"text": "In the second experiment we evaluated SASI on a test set of unseen sentences, comparing its output to a gold standard annotated by a large number of human annotators (using the Mechanical Turk).", "labels": [], "entities": [{"text": "SASI", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.955164909362793}]}, {"text": "This way we verify that there is no over-fitting and that the algorithm is not biased by the notion of sarcasm of a single seed annotator.", "labels": [], "entities": []}, {"text": "5-fold cross validation (Amazon).", "labels": [], "entities": [{"text": "Amazon)", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.9586451649665833}]}, {"text": "In this experimental setting, the seed data was divided to 5 parts and a 5-fold cross validation testis executed.", "labels": [], "entities": []}, {"text": "Each time, we use 4 parts of the seed as the training data and only this part is used for the feature selection and data enrichment.", "labels": [], "entities": []}, {"text": "This 5-fold process was repeated ten times.", "labels": [], "entities": []}, {"text": "This procedure was repeated with different sets of optional features.", "labels": [], "entities": []}, {"text": "We used 5-fold cross validation and not the standard 10-fold since the number of seed examples (especially positive) is relatively small hence 10-fold is too sensitive to the broad range of possible sarcastic patterns (see the examples in Section 1).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results (F-Score for \"no enrichment\" mode) of", "labels": [], "entities": [{"text": "F-Score", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9982317090034485}]}, {"text": " Table 2: 5-fold cross validation results on the Amazon gold", "labels": [], "entities": [{"text": "Amazon gold", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.9895959794521332}]}, {"text": " Table 3: Evaluation on the Amazon (AM) and the Twitter", "labels": [], "entities": [{"text": "Amazon (AM)", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.7849671244621277}]}, {"text": " Table 4: 5-fold cross validation results on the Twitter hash-", "labels": [], "entities": []}]}