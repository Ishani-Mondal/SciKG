{"title": [{"text": "Co-occurrence Graph Based Iterative Bilingual Lexicon Extraction From Comparable Corpora", "labels": [], "entities": [{"text": "Graph Based Iterative Bilingual Lexicon Extraction", "start_pos": 14, "end_pos": 64, "type": "TASK", "confidence": 0.6125386705001196}]}], "abstractContent": [{"text": "This paper presents an iterative algorithm for bilingual lexicon extraction from comparable corpora.", "labels": [], "entities": [{"text": "bilingual lexicon extraction from comparable corpora", "start_pos": 47, "end_pos": 99, "type": "TASK", "confidence": 0.7531343797842661}]}, {"text": "It is based on a bag-of-words model generated at the level of sentences.", "labels": [], "entities": []}, {"text": "We present our results of experimentation on corpora of multiple degrees of comparability derived from the FIRE 2010 dataset.", "labels": [], "entities": [{"text": "FIRE 2010 dataset", "start_pos": 107, "end_pos": 124, "type": "DATASET", "confidence": 0.9654879172643026}]}, {"text": "Evaluation results on 100 nouns shows that this method outper-forms the standard context-vector based approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bilingual dictionaries play a pivotal role in a number of Natural Language Processing tasks like Machine Translation and Cross Lingual Information Retrieval(CLIR).", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.775061845779419}, {"text": "Cross Lingual Information Retrieval(CLIR)", "start_pos": 121, "end_pos": 162, "type": "TASK", "confidence": 0.7259524890354702}]}, {"text": "Machine Translation systems often use bilingual dictionaries in order to augment word and phrase alignment.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7751046419143677}, {"text": "word and phrase alignment", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.6412996649742126}]}, {"text": "CLIR systems use bilingual dictionaries in the query translation step.", "labels": [], "entities": [{"text": "query translation", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7323946356773376}]}, {"text": "However, high coverage electronic bilingual dictionaries are not available for all language pairs.", "labels": [], "entities": []}, {"text": "So a major research area in Machine Translation and CLIR is bilingual dictionary extraction.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8658261895179749}, {"text": "bilingual dictionary extraction", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.7242734432220459}]}, {"text": "The most common approach for extracting bilingual dictionary is applying some statistical alignment algorithm on a parallel corpus.", "labels": [], "entities": [{"text": "extracting bilingual dictionary", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.8358129064242045}]}, {"text": "However, parallel corpora are not readily available for most language pairs.", "labels": [], "entities": []}, {"text": "Also, it takes a lot of effort to actually get the accurate translations of sentences.", "labels": [], "entities": []}, {"text": "Hence, constructing parallel corpora involves a lot of effort and time.", "labels": [], "entities": []}, {"text": "So in recent years, extracting bilingual dictionaries from comparable corpora has become an important area of research.", "labels": [], "entities": []}, {"text": "Comparable corpora consist of documents on similar topics in different languages.", "labels": [], "entities": []}, {"text": "Unlike parallel corpora, they are not sentence aligned.", "labels": [], "entities": []}, {"text": "In fact, the sentences in one language do not have to be the exact translations of the sentence in the other language.", "labels": [], "entities": []}, {"text": "However, the two corpora must be on the same domain or topic.", "labels": [], "entities": []}, {"text": "Comparable corpora can be obtained more easily than parallel corpora.", "labels": [], "entities": []}, {"text": "For example, a collection of news articles from the same time period but in different languages can form a comparable corpora.", "labels": [], "entities": []}, {"text": "But after careful study of news articles in English and Hindi published on same days at the same city, we have observed that along with articles on similar topics, the corpora also contain a lot of articles which have no topical similarity.", "labels": [], "entities": []}, {"text": "Thus, the corpora are quite noisy, which makes it unsuitable for lexicon extraction.", "labels": [], "entities": [{"text": "lexicon extraction", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7611192762851715}]}, {"text": "Thus another important factor in comparable corpora construction is the degree of similarity of the corpora.", "labels": [], "entities": [{"text": "comparable corpora construction", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.6879269480705261}]}, {"text": "Approaches for lexicon extraction from comparable corpora have been proposed that use the bagof-words model to find words that occur in similar lexical contexts.", "labels": [], "entities": [{"text": "lexicon extraction from comparable corpora", "start_pos": 15, "end_pos": 57, "type": "TASK", "confidence": 0.8198067545890808}]}, {"text": "There have been approaches proposed which improve upon this model by using some linguistic information (.", "labels": [], "entities": []}, {"text": "However, these require some linguistic tool like dependency parsers which are not commonly obtainable for resource-poor languages.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7063281387090683}]}, {"text": "For example, in case of Indian languages like Hindi and Bengali, we still do not have good enough dependency parsers.", "labels": [], "entities": []}, {"text": "In this paper, we propose a word co-occurrence based approach for lexicon extraction from comparable corpora using English and Hindi as the source and target languages respectively.", "labels": [], "entities": [{"text": "lexicon extraction", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.753177285194397}]}, {"text": "We do not use any language-specific resource in our approach.", "labels": [], "entities": []}, {"text": "We did experiments with 100 words in English,and show that our approach performs significantly better than the the Context Heterogeneity approach.", "labels": [], "entities": []}, {"text": "We show the results over corpora with varying degrees of comparability.", "labels": [], "entities": []}, {"text": "The outline of the paper is as follows.", "labels": [], "entities": []}, {"text": "In section 2, we analyze the different approaches for lexicon extraction from comparable corpora.", "labels": [], "entities": [{"text": "lexicon extraction", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7545211315155029}]}, {"text": "In section 3, we present our algorithm and the experimental results.", "labels": [], "entities": []}, {"text": "In section 4, we present an analysis of the results followed by the conclusion and future research directions in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The languages we used for our experiments were English and Hindi.", "labels": [], "entities": []}, {"text": "English was the source language and Hindi was chosen as the target.", "labels": [], "entities": []}, {"text": "For our experiments, we used a collection frequency threshold of 400 to filter out the function words.", "labels": [], "entities": []}, {"text": "The words having a collection frequency more than 400 were discarded.", "labels": [], "entities": []}, {"text": "This threshold was obtained manually by \"Trial and Error\" method in order to perform an effective function word filtering.", "labels": [], "entities": [{"text": "word filtering", "start_pos": 107, "end_pos": 121, "type": "TASK", "confidence": 0.7169149965047836}]}, {"text": "For each corpora, we extracted the cooccurrence information and then clustered the cooccurrence graph into 20 clusters.", "labels": [], "entities": []}, {"text": "From each cluster we chose 15 words, thus giving us an overall initial seed dictionary size of 300.", "labels": [], "entities": []}, {"text": "We ran the algorithm for 3000 iterations.", "labels": [], "entities": []}, {"text": "For graph clustering, we used the Graclus system () which uses a weighted kernel k-means clustering algorithm at various levels of coarseness of the input graph.", "labels": [], "entities": [{"text": "graph clustering", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7272559553384781}]}, {"text": "For evaluation, we have used the Accuracy and MMR measure.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9990197420120239}, {"text": "MMR", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.7195627093315125}]}, {"text": "The measures are defined as follows: 1 if correct translation in top n 0 otherwise where, rank i = ( ri if ri \u2264 n 0 otherwise n means top n evaluation r i means rank of correct translation in top n ranking N means total number of words used for evaluation For our experiments, we have used: The 100 words used for evaluation were chosen randomly from the source language.", "labels": [], "entities": []}, {"text": "Two evaluation methods were followed -manual and automated.", "labels": [], "entities": []}, {"text": "In the manual evaluation, a person who knows both English and Hindi was asked to find the candidate translation in the target language for the words in the source language.", "labels": [], "entities": []}, {"text": "Using this gold standard map, the Accuracy and MMR values were computed.", "labels": [], "entities": [{"text": "gold standard map", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.766884962717692}, {"text": "Accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9995152950286865}, {"text": "MMR", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9067702889442444}]}, {"text": "In the second phase (automated), lexicon extracted is evaluated against English to Hindi wordnet 4 . The evaluation process proceeds as follows: 1.", "labels": [], "entities": []}, {"text": "Hashmap is created with English words as keys and Hindi meanings as values.", "labels": [], "entities": []}, {"text": "2. English words in the extracted lexicon are crudely stemmed so that inflected words match the root words in the dictionary.", "labels": [], "entities": []}, {"text": "Stemming is done by removing the last 4 characters, one at a time and checking if word found in dictionary.", "labels": [], "entities": [{"text": "Stemming", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9793004393577576}]}, {"text": "3. Accuracy and MMR are computed.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.999614953994751}, {"text": "MMR", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.5566938519477844}]}, {"text": "As \u2191 12.9% \u2191 16.84%.", "labels": [], "entities": []}, {"text": "For auto evaluation, We see that the proposed approach shows the maximum improvement (83.33% in Accuracy and 66.67% in MMR) in performance when the corpus size is medium.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9991790652275085}]}, {"text": "For very large (too general) corpora, both the approaches give identical result while for very small (too specific) corpora, the proposed approach gives slightly better results than the reference.", "labels": [], "entities": []}, {"text": "The trends are similar for manual evaluation.", "labels": [], "entities": []}, {"text": "Once again, the maximum improvement is observed for the medium sized corpus (> 20%).", "labels": [], "entities": []}, {"text": "However, in this evaluation system, the proposed approach performs much better than the reference even for the large (more general) corpora.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the main corpora used for extraction", "labels": [], "entities": [{"text": "extraction", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.7406049370765686}]}, {"text": " Table 2: Criteria used for thresholding in the two  corpora", "labels": [], "entities": [{"text": "thresholding", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.9663363099098206}]}, {"text": " Table 3: Statistics of extracted corpora", "labels": [], "entities": []}, {"text": " Table 4: Comparison of performance between  Context Heterogeneity and Co-occurrence Ap- proach for manual evaluation", "labels": [], "entities": [{"text": "Ap- proach", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.936815083026886}]}, {"text": " Table 5: Degree of improvement shown by Co- occurrence approach over Context Heterogeneity  for manual evaluation", "labels": [], "entities": [{"text": "Degree", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9830570816993713}]}, {"text": " Table 6: Comparison of performance between  Context Heterogeneity and Co-occurrence Ap- proach for auto-evaluation", "labels": [], "entities": [{"text": "Ap- proach", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9289830724398295}]}]}