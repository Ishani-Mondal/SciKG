{"title": [{"text": "Selecting Optimal Feature Template Subset for CRFs", "labels": [], "entities": [{"text": "CRFs", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.5246219038963318}]}], "abstractContent": [{"text": "Conditional Random Fields (CRFs) are the state-of-the-art models for sequential labeling problems.", "labels": [], "entities": [{"text": "sequential labeling", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7062397301197052}]}, {"text": "A critical step is to select optimal feature template subset before employing CRFs, which is a tedious task.", "labels": [], "entities": []}, {"text": "To improve the efficiency oft his step, we propose anew method that adopts the maximum entropy (ME) model and maximum entropy Markov models (MEMMs) instead of CRFs considering the homology between ME, MEMMs, and CRFs.", "labels": [], "entities": []}, {"text": "Moreover, empirical studies on the efficiency and effectiveness of the method are conducted in the field of Chinese text chunking, whose performance is ranked the first place in task two of CIPS-ParsEval-2009.", "labels": [], "entities": [{"text": "Chinese text chunking", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.5801360607147217}, {"text": "CIPS-ParsEval-2009", "start_pos": 190, "end_pos": 208, "type": "DATASET", "confidence": 0.9567525386810303}]}], "introductionContent": [{"text": "Conditional Random Fields (CRFs) are the stateof-the-art models for sequential labeling problem.", "labels": [], "entities": [{"text": "sequential labeling problem", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.8222519357999166}]}, {"text": "In natural language processing, two aspects of CRFs have been investigated sufficiently: one is to apply it to new tasks, such as named entity recognition, part-of-speech tagging), shallow parsing (, and language modeling (); the other is to exploit new training methods for CRFs, such as improved iterative scaling), L-BFGS and gradient tree boosting ().", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 130, "end_pos": 154, "type": "TASK", "confidence": 0.6368902226289114}, {"text": "part-of-speech tagging", "start_pos": 156, "end_pos": 178, "type": "TASK", "confidence": 0.716586634516716}, {"text": "shallow parsing", "start_pos": 181, "end_pos": 196, "type": "TASK", "confidence": 0.6838156282901764}, {"text": "language modeling", "start_pos": 204, "end_pos": 221, "type": "TASK", "confidence": 0.7372230440378189}]}, {"text": "One of the critical steps is to select optimal feature subset before employing CRFs.", "labels": [], "entities": []}, {"text": "suggested an efficient method of feature induction by iteratively increasing conditional loglikelihood for discrete features.", "labels": [], "entities": [{"text": "feature induction", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8230681121349335}]}, {"text": "However, since there are millions of features and feature selection is an NP problem, this is intractable when searching optimal feature subset.", "labels": [], "entities": []}, {"text": "Therefore, it is necessary that selects feature at feature template level, which reduces input scale from millions of features to tensor hundreds of candidate templates.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew method that adopts ME and MEMMs instead of CRFs to improve the efficiency of selecting optimal feature template subset considering the homology between ME, MEMMs, and CRFs, which reduces the training time from hours to minutes without loss of performance.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents an overview of previous work for feature template selection.", "labels": [], "entities": [{"text": "feature template selection", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.7529442509015402}]}, {"text": "We propose our optimal method for feature template selection in Section 3.", "labels": [], "entities": [{"text": "feature template selection", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.7712674339612325}]}, {"text": "Section 4 presents our experiments and results.", "labels": [], "entities": []}, {"text": "Finally, we end this paper with some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the effectiveness and efficiency of the new framework by the data set in the task two of CIPS-ParsEval-2009 (.", "labels": [], "entities": [{"text": "CIPS-ParsEval-2009", "start_pos": 101, "end_pos": 119, "type": "DATASET", "confidence": 0.9559627175331116}]}, {"text": "The effectiveness is supported by high F-1 measure in the task two of CIPS-ParsEval-2009 (see, which shows that optimal feature template subset driven by ME or MEMMs is also optimal for CRFs.", "labels": [], "entities": [{"text": "F-1", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9950864911079407}, {"text": "CIPS-ParsEval-2009", "start_pos": 70, "end_pos": 88, "type": "DATASET", "confidence": 0.9326944351196289}]}, {"text": "The efficiency is shown by significant decline in training time (see, where the baseline is CRFs, and comparative methods are ME or MEMMs.", "labels": [], "entities": [{"text": "ME", "start_pos": 126, "end_pos": 128, "type": "METRIC", "confidence": 0.9678866863250732}]}, {"text": "We design six subsets of feature template set and six experiments to show the effectiveness and efficiency of the new framework.", "labels": [], "entities": []}, {"text": "As shown in and, the 1~3 experiments shows the influence of the feature templates, which are unrelated to Y i-1 , for both ME and CRFs.", "labels": [], "entities": []}, {"text": "And the 4~6 experiments show the influence of the feature templates, which are related to Y i-1 , for both MEMMs_2 and CRFs.", "labels": [], "entities": []}, {"text": "In table 1, six template subsets can be divided into two sets by relevance of previous label: 1, 2, 3 and 4, 5, 6.", "labels": [], "entities": []}, {"text": "Moreover, the first set can be divided into 1, 2, and 3 by distances between features with headwords; the second set can be divided into 4, 5 and 6 by relevance of observed value.", "labels": [], "entities": []}, {"text": "In order to ensure the objectivity of comparative experiments, candidate label filtering algorithm is not adopted.", "labels": [], "entities": [{"text": "candidate label filtering", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.6464985807736715}]}], "tableCaptions": [{"text": " Table 1: six subsets of feature template set", "labels": [], "entities": []}]}