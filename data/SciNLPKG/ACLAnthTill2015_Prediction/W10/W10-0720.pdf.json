{"title": [{"text": "Not-So-Latent Dirichlet Allocation: Collapsed Gibbs Sampling Using Human Judgments", "labels": [], "entities": [{"text": "Collapsed Gibbs Sampling", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.511848380168279}]}], "abstractContent": [{"text": "Probabilistic topic models area popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus.", "labels": [], "entities": []}, {"text": "Recent studies have found that while there are suggestive connections between topic models and the way humans interpret data, these two often disagree.", "labels": [], "entities": []}, {"text": "In this paper, we explore this disagreement from the perspective of the learning process rather than the output.", "labels": [], "entities": []}, {"text": "We present a novel task, tag-and-cluster, which asks subjects to simultaneously annotate documents and cluster those annotations.", "labels": [], "entities": []}, {"text": "We use these annotations as a novel approach for constructing a topic model, grounded inhuman interpretations of documents.", "labels": [], "entities": []}, {"text": "We demonstrate that these topic models have features which distinguish them from traditional topic models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic topic models have become popular tools for the unsupervised analysis of large document collections.", "labels": [], "entities": []}, {"text": "These models posit a set of latent topics, multinomial distributions over words, and assume that each document can be described as a mixture of these topics.", "labels": [], "entities": []}, {"text": "With algorithms for fast approximate posterior inference, we can use topic models to discover both the topics and an assignment of topics to documents from a collection of documents.", "labels": [], "entities": []}, {"text": "(See) These modeling assumptions are useful in the sense that, empirically, they lead to good models of documents ().", "labels": [], "entities": []}, {"text": "However, recent work has explored how these assumptions correspond to humans' understanding of language (.", "labels": [], "entities": []}, {"text": "Focusing on the latent space, i.e., the inferred mappings between topics and words and between documents and topics, this work has discovered that although there are some suggestive correspondences between human semantics and topic models, they are often discordant.", "labels": [], "entities": []}, {"text": "In this paper we build on this work to further explore how humans relate to topic models.", "labels": [], "entities": []}, {"text": "But whereas previous work has focused on the results of topic models, here we focus on the process by which these models are learned.", "labels": [], "entities": []}, {"text": "Topic models lend themselves to sequential procedures through which the latent space is inferred; these procedures are in effect programmatic encodings of the modeling assumptions.", "labels": [], "entities": []}, {"text": "By substituting key steps in this program with human judgments, we obtain insights into the semantic model conceived by humans.", "labels": [], "entities": []}, {"text": "Here we present a novel task, tag-and-cluster, which asks subjects to simultaneously annotate a document and cluster that annotation.", "labels": [], "entities": []}, {"text": "This task simulates the sampling step of the collapsed Gibbs sampler (described in the next section), except that the posterior defined by the model has been replaced by human judgments.", "labels": [], "entities": []}, {"text": "The task is quick to complete and is robust against noise.", "labels": [], "entities": []}, {"text": "We report the results of a largescale human study of this task, and show that humans are indeed able to construct a topic model in this fashion, and that the learned topic model has semantic properties distinct from existing topic models.", "labels": [], "entities": []}, {"text": "We also demonstrate that the judgments can be used to guide computer-learned topic models towards models which are more concordant with human intuitions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted our experiments using Amazon Mechanical Turk, which allows workers (our pool of prospective subjects) to perform small jobs fora fee through a Web interface.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.8968958258628845}]}, {"text": "No specialized training or knowledge is typically expected of the workers.", "labels": [], "entities": []}, {"text": "Amazon Mechanical Turk has been successfully used in the past to develop gold-standard data for natural language processing (.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.927056630452474}]}, {"text": "We prepare two randomly-chosen, 100-document subsets of English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.8916072845458984}]}, {"text": "For convenience, we denote these two sets of documents as set1 and set2.", "labels": [], "entities": []}, {"text": "For each document, we keep only the first 150 words for our experiments.", "labels": [], "entities": []}, {"text": "Because of the encyclopedic nature of the corpus, the first 150 words typically provides abroad overview of the themes in the article.", "labels": [], "entities": []}, {"text": "We also removed from the corpus stop words and words which occur infrequently 2 , leading to a lexicon of 8263 words.", "labels": [], "entities": []}, {"text": "After this pruning set1 contained 11614 words and set2 contained 11318 words.", "labels": [], "entities": []}, {"text": "Workers were asked to perform twenty of the taggings described in Section 3 for each task; workers were paid $0.25 for each such task.", "labels": [], "entities": []}, {"text": "The number of latent topics, K, is a free parameter.", "labels": [], "entities": []}, {"text": "Here we explore two values of this parameter, K = 10 and K = 15, leading to a total of four experiments -two for each set of documents and two for each value of K.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The five words with the highest probability mass in each topic inferred by humans using the task described in  Section 3. Each subtable shows the results for a particular experimental setup. Each row is a topic; the most probable  words are ordered from left to right.", "labels": [], "entities": []}]}