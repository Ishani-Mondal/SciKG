{"title": [{"text": "Recession Segmentation: Simpler Online Word Segmentation Using Limited Resources *", "labels": [], "entities": [{"text": "Recession Segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9841400980949402}, {"text": "Simpler Online Word Segmentation", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.6762640327215195}]}], "abstractContent": [{"text": "In this paper we present a cognitively plausible approach to word segmentation that segments in an online fashion using only local information and a lexicon of previously segmented words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.740151897072792}]}, {"text": "Unlike popular statistical optimization techniques, the learner uses structural information of the input syllables rather than distributional cues to segment words.", "labels": [], "entities": []}, {"text": "We develop a memory model for the learner that like a child learner does not recall previously hypothesized words perfectly.", "labels": [], "entities": []}, {"text": "The learner attains an F-score of 86.69% in ideal conditions and 85.05% when word recall is unreliable and stress in the input is reduced.", "labels": [], "entities": [{"text": "F-score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9994669556617737}]}, {"text": "These results demonstrate the power that a simple learner can have when paired with appropriate structural constraints on its hypotheses .", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of word segmentation presents an important challenge in language acquisition.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7430433332920074}, {"text": "language acquisition", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7096814662218094}]}, {"text": "The child learner must segment a continuous stream of sounds into words without knowing what the individual words are until the stream has been segmented.", "labels": [], "entities": []}, {"text": "Computational models present an opportunity to test the potentially innate constraints, structures, and algorithms that a child maybe using to guide her acquisition.", "labels": [], "entities": []}, {"text": "In this work we develop a segmentation model from the constraints suggested by and evaluate it in idealized conditions and conditions that better approximate the environment of a child learner.", "labels": [], "entities": []}, {"text": "We seek to determine how these limitations in the learner's input and memory affect the learner's performance and to demonstrate that the presented learner is robust even under non-ideal conditions.", "labels": [], "entities": []}, {"text": "* Portions of this work were adapted from an earlier manuscript, Word Segmentation: Quick But Not Dirty.", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.6905346065759659}]}], "datasetContent": [{"text": "Our computational model is designed to process child-directed speech.", "labels": [], "entities": []}, {"text": "The corpus we use to evaluate it is the same corpus used by.", "labels": [], "entities": []}, {"text": "Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus), consisting of three children's data: Adam, Eve, and Sarah.", "labels": [], "entities": [{"text": "Brown (1973) data in the CHILDES corpus", "start_pos": 41, "end_pos": 80, "type": "DATASET", "confidence": 0.7853494551446702}]}, {"text": "We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6, using the first pronunciation of each word.", "labels": [], "entities": [{"text": "Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6", "start_pos": 58, "end_pos": 118, "type": "DATASET", "confidence": 0.7972633838653564}]}, {"text": "In CMUdict, lexical stress information is preserved by numbers: 0 for unstressed, 1 for primary stress, 2 for secondary stress.", "labels": [], "entities": [{"text": "CMUdict", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.8639323115348816}]}, {"text": "For instance, cat is represented as K.AE1.T, catalog is K.AE1.T.AH0.L.AO0.G, and catapult is K.AE1.T.AH0.P.AH2.L.T.", "labels": [], "entities": [{"text": "catalog", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.8807951211929321}]}, {"text": "We treat primary stress as \"strong\" and secondary or unstressed syllables as \"weak.\"", "labels": [], "entities": []}, {"text": "For each word, the phonetic segments were grouped into syllables.", "labels": [], "entities": []}, {"text": "This process is straightforward by the use of the principle \"Maximize Onset,\" which maximizes the length of the onset as long as it is valid consonant cluster of English, i.e., it conforms to the phonotactic constraints of English.", "labels": [], "entities": []}, {"text": "For example, Einstein is AY1.N.S.T.AY0.N as segments and parsed into AY1.N S.T.AY0.N as syllables: this is because /st/ is the longest valid onset for the second syllable containing AY0 while /nst/ is longer but violates English phonotactics.", "labels": [], "entities": []}, {"text": "While we performed syllabification as a preprocessing step outside of learning, a child learner would presumably learn the required phonotactics as apart of learning to segment words.", "labels": [], "entities": []}, {"text": "9-month old infants are believed to have learned some phonotactic constraints of their native language, and learning these constraints can be done with only minimal exposure ().", "labels": [], "entities": []}, {"text": "Finally, spaces and punctuation between words were removed, but the boundaries between utterances-as indicated byline breaks in CHILDES-are retained.", "labels": [], "entities": []}, {"text": "Altogether, there are 226,178 words, consisting of 263,660 syllables.", "labels": [], "entities": []}, {"text": "The learning material is a list of unsegmented syllable sequences grouped into utterances, and the learner's task is to find word boundaries that group substrings of syllables together, building a lexicon of words as it segments.", "labels": [], "entities": []}, {"text": "We evaluated the learner's performance to address these questions: \u2022 How does probabilistic memory affect learner performance?", "labels": [], "entities": []}, {"text": "\u2022 How much does degrading stress information relied on by USC segmentation reduce performance?", "labels": [], "entities": [{"text": "USC segmentation", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.5889473110437393}]}, {"text": "\u2022 What is the interaction between the probabilistic lexicon and non-idealized stress information?", "labels": [], "entities": []}, {"text": "To evaluate the learner, we tested configurations that used a probabilistic lexicon and ones with perfect memory in two scenarios: Dictionary Stress, and Reduced Stress.", "labels": [], "entities": []}, {"text": "We create the Reduced Stress condition in order to simulate that stress is often reduced in casual speech, and that languagespecific stress rules may cause reductions or shifts in stress that prevent two strong syllables from occurring in sequence.", "labels": [], "entities": []}, {"text": "The difference between the scenarios is defined as follows: Dictionary Stress.", "labels": [], "entities": []}, {"text": "The stress information is given to the learner as it was looked up in CMUdict.", "labels": [], "entities": [{"text": "CMUdict", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.9780842661857605}]}, {"text": "For example, the first utterance from the Adam corpus would be B.IH1.G D.R.AH1.M (big drum), an utterance with two stressed monosyllables (SS).", "labels": [], "entities": []}, {"text": "In most languages, however, conditions where two stressed syllables are in sequence are handled by reducing the stress of one syllable.", "labels": [], "entities": []}, {"text": "This is simulated in the reduced stress condition.", "labels": [], "entities": []}, {"text": "The stress information obtained from CMUdict is post-processed in the context of each utterance.", "labels": [], "entities": []}, {"text": "For any two adjacent primary stressed syllables, the first syllable is reduced from a strong syllable to a weak one.", "labels": [], "entities": []}, {"text": "This is applied iteratively from left to right, so for any sequence of n adjacent primary-stress syllables, only the nth syllable retains primary stress; all others are reduced.", "labels": [], "entities": []}, {"text": "This removes the most valuable clue as to where utterances can be segmented, as USC segmentation no longer applies.", "labels": [], "entities": [{"text": "USC segmentation", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.8212886154651642}]}, {"text": "This simulates the stress retraction effect found in real speech, which tries to avoid adjacent primary stresses.", "labels": [], "entities": []}, {"text": "Learners that use probabilistic memory were allowed to iterate over the input two times with access to the lexicon developed over previous iterations but no access to previous segmentations.", "labels": [], "entities": []}, {"text": "This simulates a child hearing many of the same words and utterances again, and reduces the effect of the small corpus size used on the learning process.", "labels": [], "entities": []}, {"text": "Because the probabilistic memory reduces the algorithm's ability to build a lexicon, performance in a single iteration is lower than perfect memory conditions.", "labels": [], "entities": []}, {"text": "In all other conditions, the learner is allowed only a single pass over the corpus.", "labels": [], "entities": []}, {"text": "The precision and recall metrics are calculated for the segmentation that the learner outputs and the lexicon itself.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9993036985397339}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9979715943336487}]}, {"text": "For an utterance, each word in the learner's segmentation that also appears in the gold standard segmentation is counted as correct, and each word in the learner's segmentation not present in the gold standard segmentation is a false alarm.", "labels": [], "entities": []}, {"text": "F-score is computed using equally balanced precision and recall (F 0 ).", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9734379053115845}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9992045760154724}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9994242191314697}, {"text": "F 0", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.957763135433197}]}, {"text": "The correct words, false words, and number of words in the gold standard are summed over the output in each iteration to produce performance measures for that iteration.", "labels": [], "entities": []}, {"text": "Precision, recall, and F-score are similarly computed for the lexicon; every word in the learner's lexicon present in the gold standard is counted as correct, and every word in the learner's lexicon not present in the gold standard is a false alarm.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9965084195137024}, {"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9971408843994141}, {"text": "F-score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9991766810417175}]}, {"text": "These computations are performed over word types in the lexicon, thus all words in the lexicon are of equal weight in computing performance regardless of their frequency.", "labels": [], "entities": []}, {"text": "In the probabilistic memory conditions, however, the memory function defines the probability of each word being recalled (and thus being considered apart of the lexicon) at evaluation time.", "labels": [], "entities": []}, {"text": "In addition to evaluating the learner, we also implemented three baseline approaches to compare the learner against.", "labels": [], "entities": []}, {"text": "The Utterance baseline segmenter assumes every utterance is a single word.", "labels": [], "entities": [{"text": "Utterance baseline segmenter", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.652651309967041}]}, {"text": "The Monosyllabic baseline segmenter assumes every syllable is a single word.", "labels": [], "entities": []}, {"text": "The USC segmenter inserts word boundaries between all adjacent syllables with primary stress in the corpus.", "labels": [], "entities": [{"text": "USC segmenter inserts word boundaries", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.7269371032714844}]}], "tableCaptions": [{"text": " Table 2: Number of segmentations performed by  each operation: USC Segmentation, Initial Sub- traction, and Final Subtraction.", "labels": [], "entities": [{"text": "segmentations", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.9477711915969849}, {"text": "USC Segmentation", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.6156345903873444}, {"text": "Initial Sub- traction", "start_pos": 82, "end_pos": 103, "type": "METRIC", "confidence": 0.862971767783165}]}, {"text": " Table 1: Baseline and Learner Performance. Performance is reported after two iterations over the corpus  for probabilistic memory learners and after a single iteration for all other learners.", "labels": [], "entities": []}]}