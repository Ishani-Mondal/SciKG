{"title": [{"text": "Direct Parsing of Discontinuous Constituents in German", "labels": [], "entities": []}], "abstractContent": [{"text": "Discontinuities occur especially frequently in languages with a relatively free word order, such as German.", "labels": [], "entities": []}, {"text": "Generally, due to the long-distance dependencies they induce, they lie beyond the expressivity of Probabilistic CFG, i.e., they cannot be directly reconstructed by a PCFG parser.", "labels": [], "entities": [{"text": "Probabilistic CFG", "start_pos": 98, "end_pos": 115, "type": "DATASET", "confidence": 0.7049938440322876}]}, {"text": "In this paper, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), a formalism with high expressivity, to directly parse the German NeGra and TIGER treebanks.", "labels": [], "entities": [{"text": "German NeGra and TIGER treebanks", "start_pos": 155, "end_pos": 187, "type": "DATASET", "confidence": 0.7166678071022033}]}, {"text": "In both treebanks, discontinuities are annotated with crossing branches.", "labels": [], "entities": []}, {"text": "Based on an evaluation using different metrics, we show that an output quality can be achieved which is comparable to the output quality of PCFG-based systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Languages with a rather free word order, like German, display discontinuous constituents particularly frequently.", "labels": [], "entities": []}, {"text": "In (1), the discontinuity is caused by an extraposed relative clause.", "labels": [], "entities": []}, {"text": "Another language with a rather free word order is Bulgarian.", "labels": [], "entities": []}, {"text": "In (2), the discontinuity is caused by topicalization.", "labels": [], "entities": [{"text": "discontinuity", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.973656415939331}]}, {"text": "In most constituency treebanks, sentence annotation is restricted to having the shape of trees without crossing branches, and the non-local dependencies induced by the discontinuities are modeled by an additional mechanism.", "labels": [], "entities": [{"text": "sentence annotation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7146606594324112}]}, {"text": "In the Penn Treebank (PTB), e.g., this mechanism is a combination of special labels and empty nodes, establishing implicit additional edges.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 7, "end_pos": 26, "type": "DATASET", "confidence": 0.9698764085769653}]}, {"text": "In the German T\u00fcBa-D/Z (), additional edges are established by a combination of topological field annotation and special edge labels.", "labels": [], "entities": []}, {"text": "As an example, shows a tree from T\u00fcBa-D/Z with the annotation of (1).", "labels": [], "entities": [{"text": "T\u00fcBa-D/Z", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.8942143122355143}]}, {"text": "Note here the edge label ON-MOD on the relative clause which indicates that the subject of the sentence (alle Attribute) is modified.", "labels": [], "entities": [{"text": "ON-MOD", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.8375116586685181}]}, {"text": "However, in a few other treebanks, such as the German NeGra and TIGER treebanks (), crossing branches are allowed.", "labels": [], "entities": [{"text": "German NeGra and TIGER treebanks", "start_pos": 47, "end_pos": 79, "type": "DATASET", "confidence": 0.7027913212776185}]}, {"text": "This way, all dependents of a long-distance dependency can be grouped under a single node.", "labels": [], "entities": []}, {"text": "Since in general, the annotation mechanisms for non-local dependencies lie beyond the expressivity of Context-Free Grammar, non-local information is inaccessible for PCFG parsing and therefore generally discarded.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 166, "end_pos": 178, "type": "TASK", "confidence": 0.6778049916028976}]}, {"text": "In NeGra/TIGER annotation, e.g., tree transformation algorithms are applied before parsing in order to resolve the crossing branches.", "labels": [], "entities": [{"text": "tree transformation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7353922426700592}]}, {"text": "See, e.g., and for details.", "labels": [], "entities": []}, {"text": "If one wants to avoid the loss of annotation information which is implied with such transformations, one possibility is to use a probabilistic parser fora formalism which is more expressive than CFG.", "labels": [], "entities": [{"text": "CFG", "start_pos": 195, "end_pos": 198, "type": "DATASET", "confidence": 0.9157361388206482}]}, {"text": "In this paper, we tackle the question if qualitatively good results can be achieved when parsing German with such a parser.", "labels": [], "entities": [{"text": "parsing German", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.8622056841850281}]}, {"text": "Concretely, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS).", "labels": [], "entities": []}, {"text": "LCFRS ( area natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals.", "labels": [], "entities": [{"text": "LCFRS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.551125168800354}, {"text": "CFG", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.8865583539009094}]}, {"text": "We can directly interpret NeGra-style trees as its derivation structures, i.e., we can extract grammars without making further linguistic assumptions) (see Sect.", "labels": [], "entities": []}, {"text": "2.3), as it is necessary for other formalisms such as Probabilistic Tree Adjoining Grammars.", "labels": [], "entities": [{"text": "Probabilistic Tree Adjoining Grammars", "start_pos": 54, "end_pos": 91, "type": "TASK", "confidence": 0.540216714143753}]}, {"text": "Since the non-local dependencies are immediately accessible in NeGra and TIGER, we choose these treebanks as our data source.", "labels": [], "entities": [{"text": "NeGra", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.8936071991920471}]}, {"text": "In order to judge parser output quality, we use four different evaluation types.", "labels": [], "entities": []}, {"text": "We use an EVALB-style measure, adapted for LCFRS, in order to compare our parser to previous work on parsing German treebanks.", "labels": [], "entities": [{"text": "parsing German treebanks", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.7946373025576273}]}, {"text": "In order to address the known shortcomings of EVALB, we perform an additional evaluation using the tree distance metric of, which works independently of the fact if there are crossing branches in the trees or not, and a dependency evaluation, which has also be applied before in the context of parsing German.", "labels": [], "entities": [{"text": "parsing German", "start_pos": 294, "end_pos": 308, "type": "TASK", "confidence": 0.8908276557922363}]}, {"text": "Last, we evaluate certain difficult phenomena by hand on, a set of sentences hand-picked from TIGER.", "labels": [], "entities": []}, {"text": "The evaluations show that with a PLCFRS parser, competitive results can be achieved.", "labels": [], "entities": []}, {"text": "The remainder of the article is structured as follows.", "labels": [], "entities": []}, {"text": "2, we present the formalism, the parser, and how we obtain our grammars.", "labels": [], "entities": []}, {"text": "3, we discuss the evaluation methods we employ.", "labels": [], "entities": []}, {"text": "4 contains our experimental results.", "labels": [], "entities": []}, {"text": "5 is dedicated to related work.", "labels": [], "entities": []}, {"text": "6 contains the conclusion and presents some possible future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We assess the quality of our parser output using different methods.", "labels": [], "entities": []}, {"text": "The first is an EVALB-style metric (henceforth EVALB), i.e., we compare phrase boundaries.", "labels": [], "entities": []}, {"text": "In spite of its shortcomings, it allows us to compare to previous work on parsing NeGra.", "labels": [], "entities": [{"text": "parsing NeGra", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.6890708953142166}]}, {"text": "In the context of LCFRS, we compare sets of tuples of the form one from the corresponding treebank trees.", "labels": [], "entities": []}, {"text": "Using these tuple sets, we compute labeled and unlabeled recall (LR/UR), precision (LP/UP), and the F 1 measure (LF 1 /UF 1 ) in the usual way.", "labels": [], "entities": [{"text": "recall (LR/UR)", "start_pos": 57, "end_pos": 71, "type": "METRIC", "confidence": 0.8934073050816854}, {"text": "precision (LP/UP)", "start_pos": 73, "end_pos": 90, "type": "METRIC", "confidence": 0.9204694032669067}, {"text": "F 1 measure (LF 1 /UF 1 )", "start_pos": 100, "end_pos": 125, "type": "METRIC", "confidence": 0.9209621369838714}]}, {"text": "Note that if k = 1, our metric is identical to its PCFG version.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.9623945355415344}]}, {"text": "EVALB does not necessarily reflect parser output quality).", "labels": [], "entities": [{"text": "EVALB", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5028587579727173}]}, {"text": "One of its major problems is that attachment errors are penalized too hard.", "labels": [], "entities": []}, {"text": "As the second evaluation method, we therefore choose the tree-distance measure (henceforth TDIST), which levitates this problem.", "labels": [], "entities": []}, {"text": "It has been proposed for parser evaluation by.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.9531570374965668}]}, {"text": "TDIST is an ideal candidate for evaluation of the output of a PLCFRS, since it the fact if trees have crossing branches or not is not relevant to it.", "labels": [], "entities": [{"text": "TDIST", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8823177814483643}]}, {"text": "Two trees \u03c4 k and \u03c4 A are compared on the basis of T -mappings from \u03c4 k to \u03c4 A . A T -mapping is a partial mapping \u03c3 of nodes of \u03c4 k to nodes of \u03c4 A where all node mappings preserve left-to-right order and ancestry.", "labels": [], "entities": [{"text": "ancestry", "start_pos": 206, "end_pos": 214, "type": "METRIC", "confidence": 0.9625127911567688}]}, {"text": "Within the mappings, node insertion, node deletion, and label swap operations are identified, represented resp. by the sets I, D and S.", "labels": [], "entities": [{"text": "node insertion", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.6502972692251205}]}, {"text": "Furthermore, we consider the set M representing the matched (i.e., unchanged) nodes.", "labels": [], "entities": []}, {"text": "The cost of a T -mapping is the total number of operations, i.e. |I|+ |D|+ |S|.", "labels": [], "entities": []}, {"text": "The tree distance between two trees \u03c4 K and \u03c4 A is the cost of the cheapest T -mapping., borrowed from Emms, shows an example fora T -mapping.", "labels": [], "entities": []}, {"text": "Inserted nodes are prefixed with >, deleted nodes are suffixed with <, and nodes with swapped labels are linked with arrows.", "labels": [], "entities": []}, {"text": "Since in total, four operations are involved, to this T -mapping, a cost of 4 is assigned.", "labels": [], "entities": [{"text": "T -mapping", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.5667306184768677}]}, {"text": "For more details, especially on algorithms which compute TDIST, refer to.", "labels": [], "entities": []}, {"text": "In order to convert the tree distance measure into a similarity measure like EVALB, we use the macro-averaged Dice and Jaccard normalizations as defined by Emms.", "labels": [], "entities": [{"text": "EVALB", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.7585962414741516}]}, {"text": "Let \u03c4 K and \u03c4 Abe two trees with |\u03c4 K | and |\u03c4 A | nodes, respectively.", "labels": [], "entities": []}, {"text": "For a T -mapping \u03c3 from \u03c4 K to \u03c4 A with the sets D, I, Sand M, we compute them as follows.", "labels": [], "entities": []}, {"text": "where, in order to achieve macro-averaging, we sum the numerators and denominators overall tree pairs before dividing.", "labels": [], "entities": []}, {"text": "See Emms (2008) for further details.", "labels": [], "entities": []}, {"text": "The third method is dependency evaluation (henceforth DEP), as described by.", "labels": [], "entities": [{"text": "dependency evaluation", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.8816544115543365}]}, {"text": "It consists of comparing dependency graphs extracted from the gold data and from the parser output.", "labels": [], "entities": []}, {"text": "The dependency extraction algorithm as given by Lin does also not rely on trees to be free of crossing branches.", "labels": [], "entities": [{"text": "dependency extraction", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8387115001678467}]}, {"text": "It only relies on a method to identify the head of each phrase.", "labels": [], "entities": []}, {"text": "We use our own implementation of the algorithm which is described in Sect.", "labels": [], "entities": []}, {"text": "4 of, combined with the head finding algorithm of the parser.", "labels": [], "entities": [{"text": "head finding", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.8191560804843903}]}, {"text": "Dependency evaluation abstracts away from another bias of EVALB.", "labels": [], "entities": [{"text": "Dependency evaluation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7944195568561554}, {"text": "EVALB", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.838747501373291}]}, {"text": "Concretely, it does not prefer trees with a high node/token ratio, since two dependency graphs to be compared necessarily have the same number of (terminal) nodes.", "labels": [], "entities": []}, {"text": "In the context of parsing German, this evaluation has been employed previously by.", "labels": [], "entities": [{"text": "parsing German", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.8889637291431427}]}, {"text": "Last, we evaluate on TePaCoC (Testing Parser Performance on Complex Grammatical Constructions), a set of particularly difficult sentences hand-picked from TIGER ().", "labels": [], "entities": []}, {"text": "Our data sources are the German NeGra () and TIGER () treebanks.", "labels": [], "entities": [{"text": "German", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.8746514320373535}, {"text": "NeGra", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.5093538165092468}, {"text": "TIGER () treebanks", "start_pos": 45, "end_pos": 63, "type": "DATASET", "confidence": 0.7351692716280619}]}, {"text": "Ina preprocessing step, following common practice, we attach all punctuation to nodes within the tree, since it is not included in the NeGra annotation.", "labels": [], "entities": []}, {"text": "Ina first pass, using heuristics, we attach all nodes to the in each case highest available phrasal node such that ideally, we do not introduce new crossing branches.", "labels": [], "entities": []}, {"text": "Ina second pass, parentheses and quotation marks are preferably attached to the same node.", "labels": [], "entities": []}, {"text": "Grammatical function labels are discarded.", "labels": [], "entities": []}, {"text": "After this preprocessing step, we create a separate version of the data set, in which we resolve the crossing branches in the trees, using the common approach of re-attaching nodes to higher constituents.", "labels": [], "entities": []}, {"text": "We use the first 90% of our data sets for training and the remaining 10% for testing.", "labels": [], "entities": []}, {"text": "Due to memory limitations, we restrict ourselves to sentences of a maximal length of 30 words.", "labels": [], "entities": []}, {"text": "Our TIGER data sets (TIGER and T-CF) have 31,568 sentences of an average length of 14.81, splitted into 31,568 sentences for training and 3,508 sentences for testing.", "labels": [], "entities": [{"text": "TIGER data sets", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8252626160780588}, {"text": "TIGER", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.5433764457702637}]}, {"text": "Our NeGra data sets (NeGra and N-CF) have 18,335 sentences, splitted into 16,501 sentences for training and 1,834 sentences for testing.", "labels": [], "entities": [{"text": "NeGra data sets", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9357940355936686}]}, {"text": "We parse the data sets described above with activated LR estimate.", "labels": [], "entities": [{"text": "activated LR estimate", "start_pos": 44, "end_pos": 65, "type": "METRIC", "confidence": 0.7334990700085958}]}, {"text": "For all our experiments, we use the markovization settings v = 2 and h = 1, which have proven to be successful in previous work on parsing NeGra (.", "labels": [], "entities": [{"text": "parsing NeGra", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.7256994247436523}]}, {"text": "We provide the parser with the gold tagging.", "labels": [], "entities": []}, {"text": "shows the average parsing times for all data sets on an AMD Opteron node with 8GB of RAM (pure Java implementation), Tab.", "labels": [], "entities": [{"text": "Tab", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.9359451532363892}]}, {"text": "1 shows the percentage of parsed sentences.", "labels": [], "entities": []}, {"text": "Not surprisingly, reconstructing discontinuities is hard.", "labels": [], "entities": [{"text": "reconstructing discontinuities", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.9420292973518372}]}, {"text": "Therefore, when parsing without crossing branches, the results are slightly better.", "labels": [], "entities": [{"text": "parsing", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.9896538853645325}]}, {"text": "In order to seethe influence of discontinuous structures during parsing on the underlying phrase structure, we resolve the crossing branches in the parser output of NeGra and TIGER and compare it to the respective gold test data of N-CF and T-CF.", "labels": [], "entities": []}, {"text": "The results deteriorate slightly in comparison with N-CF and T-CF, however, they are slightly higher than for than for NeGra and TIGER.", "labels": [], "entities": []}, {"text": "This is due to the fact that during the transformation, some errors in the LCFRS parses get \"corrected\": Wrongly attached phrasal nodes are re-attached to unique higher positions in the trees.", "labels": [], "entities": []}, {"text": "In order to give a point of comparison with previous work on parsing TIGER and NeGra, in Tab.", "labels": [], "entities": [{"text": "parsing TIGER", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7217861115932465}, {"text": "Tab.", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9316878616809845}]}, {"text": "4, we report some of the results from the literature.", "labels": [], "entities": []}, {"text": "All of them were obtained using PCFG parsers: Our results are slightly better than for the plain PCFG models.", "labels": [], "entities": []}, {"text": "We would expect the result for T-CF to be closer to the corresponding result for the Stanford parser, since we are using a comparable: PCFG parsing of NeGra, Labeled F 1 model.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 135, "end_pos": 147, "type": "TASK", "confidence": 0.6678238213062286}]}, {"text": "This difference is mostly likely due to losses induced by the LR estimate.", "labels": [], "entities": []}, {"text": "All items to which the estimate assigns an outside log probability estimate of \u2212\u221e get blocked and are not put on the agenda.", "labels": [], "entities": []}, {"text": "This blocking has an extremely beneficial effect on parser speed.", "labels": [], "entities": []}, {"text": "However, it is paid by a worse recall, as experiments with smaller data sets have shown.", "labels": [], "entities": [{"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9992263317108154}]}, {"text": "A complete discussion of the effects of estimates, as well as a discussion of other possible optimizations, is presented in.", "labels": [], "entities": []}, {"text": "Recall finally that LCFRS parses are more informative than PCFG parses -a lower score for LCFRS EVALB than for PCFG EVALB does not necessarily mean that the PCFG parse is \"better\".", "labels": [], "entities": []}, {"text": "5 shows the results of evaluating with TDIST, excluding unparsed sentences.", "labels": [], "entities": [{"text": "TDIST", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.5904584527015686}]}, {"text": "We report the dice and jaccard normalizations, as well as a summary of the distribution of the tree distances between gold trees and trees from the parser output (see  Again, we can observe that parsing LCFRS is harder than parsing PCFG.", "labels": [], "entities": [{"text": "parsing LCFRS", "start_pos": 195, "end_pos": 208, "type": "TASK", "confidence": 0.8029239177703857}, {"text": "parsing PCFG", "start_pos": 224, "end_pos": 236, "type": "TASK", "confidence": 0.6879410147666931}]}, {"text": "As for EVALB, the results for TIGER are slightly higher than the ones for NeGra.", "labels": [], "entities": [{"text": "EVALB", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.6843293905258179}, {"text": "TIGER", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9257298111915588}, {"text": "NeGra", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.902205765247345}]}, {"text": "The distribution of the tree distances shows that about a third of all sentences receive a completely correct parse.", "labels": [], "entities": []}, {"text": "More than a half, resp.", "labels": [], "entities": []}, {"text": "a third of all parser output trees require \u2264 3 operations to be mapped to the corresponding gold tree, and a only a small percentage requires \u2265 10 operations.", "labels": [], "entities": []}, {"text": "To our knowledge, TDIST has not been used to evaluate parser output for NeGra and TIGER.", "labels": [], "entities": [{"text": "TDIST", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.6386863589286804}]}, {"text": "However, reports results for the PTB using different parsers.", "labels": [], "entities": [{"text": "PTB", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.8320493698120117}]}, {"text": "Collins' Model 1), e.g., lies at 93.62 (Dice) and 87.87 (Jaccard).", "labels": [], "entities": [{"text": "Collins' Model 1", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9239488045374552}]}, {"text": "For the Berkeley Parser (, 94.72 and 89.87 is reported.", "labels": [], "entities": [{"text": "Berkeley Parser", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.9081319272518158}]}, {"text": "We see that our results lie in them same range.", "labels": [], "entities": []}, {"text": "However, Jaccard scores are lower since this normalization punishes a higher number of edit operations more severely than Dice.", "labels": [], "entities": [{"text": "Jaccard", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.8956984877586365}]}, {"text": "In order to meaningfully interpret which treebank properties are responsible for the fact that between the gold trees and the trees from the parser, the German data requires more tree edit operations than the English data, a TDIST evaluation of the output of an off-the-shelf PCFG parser would be necessary.", "labels": [], "entities": [{"text": "German data", "start_pos": 153, "end_pos": 164, "type": "DATASET", "confidence": 0.7896918654441833}]}, {"text": "This is left for future work.", "labels": [], "entities": []}, {"text": "For the dependency evaluation, we extract dependency graphs from both the gold data and the test data and compare the unlabeled accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.986588180065155}]}, {"text": "We report unlabeled attachment score (UAS).", "labels": [], "entities": [{"text": "attachment score (UAS)", "start_pos": 20, "end_pos": 42, "type": "METRIC", "confidence": 0.8900593161582947}]}, {"text": "The dependency results are consistent with the previous results in as much as the scores for PCFG parsing are again higher.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 93, "end_pos": 105, "type": "TASK", "confidence": 0.6925350427627563}]}, {"text": "The dependency results reported in however are much higher (85.6 UAS for the markovized Stanford parser).", "labels": [], "entities": [{"text": "UAS", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9947518706321716}]}, {"text": "While apart of the losses can again be attributed to the LR estimate, another reason lies undoubtedly in the different dependency conversion method which we employ, and in further treebank transformations which In order to get a more fine grained result, in future work, we will consider graph modifications as proposed by as well as including annotation-specific information from NeGra/TIGER in our conversion procedure.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: EVALB results (resolved crossing branches)", "labels": [], "entities": [{"text": "EVALB", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8485052585601807}]}, {"text": " Table 4: PCFG parsing of NeGra, Labeled F 1", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.7733606696128845}]}, {"text": " Table 5: Tree distance evaluation", "labels": [], "entities": [{"text": "Tree distance evaluation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8046354254086813}]}, {"text": " Table 7: EVALB scores for TePaCoC", "labels": [], "entities": [{"text": "EVALB", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8575186133384705}, {"text": "TePaCoC", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.8426375985145569}]}]}