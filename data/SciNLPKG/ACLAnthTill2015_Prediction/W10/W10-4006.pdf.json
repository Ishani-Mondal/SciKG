{"title": [], "abstractContent": [{"text": "This paper presents anew word alignment method which incorporates knowledge about Bilingual Multi-Word Expressions (BMWEs).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7557181715965271}, {"text": "Bilingual Multi-Word Expressions (BMWEs)", "start_pos": 82, "end_pos": 122, "type": "TASK", "confidence": 0.5965486268202463}]}, {"text": "Our method of word alignment first extracts such BMWEs in a bidirectional way fora given corpus and then starts conventional word alignment, considering the properties of BMWEs in their grouping as well as their alignment links.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7912536859512329}, {"text": "word alignment", "start_pos": 125, "end_pos": 139, "type": "TASK", "confidence": 0.7136949151754379}]}, {"text": "We give partial annotation of alignment links as prior knowledge to the word alignment process; by replacing the maximum likelihood estimate in the M-step of the IBM Models with the Maximum A Posteriori (MAP) estimate, prior knowledge about BMWEs is embedded in the prior in this MAP estimate.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.7334234416484833}, {"text": "maximum likelihood estimate", "start_pos": 113, "end_pos": 140, "type": "METRIC", "confidence": 0.7412644624710083}, {"text": "IBM Models", "start_pos": 162, "end_pos": 172, "type": "DATASET", "confidence": 0.9577092528343201}, {"text": "Maximum A Posteriori (MAP) estimate", "start_pos": 182, "end_pos": 217, "type": "METRIC", "confidence": 0.8617241638047355}, {"text": "BMWEs", "start_pos": 241, "end_pos": 246, "type": "DATASET", "confidence": 0.8713461756706238}]}, {"text": "In our experiments , we saw an improvement of 0.77 Bleu points absolute in JP-EN.", "labels": [], "entities": [{"text": "Bleu points absolute", "start_pos": 51, "end_pos": 71, "type": "METRIC", "confidence": 0.9594227274258932}, {"text": "JP-EN", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.8881667852401733}]}, {"text": "Except for one case, our method gave better results than the method using only BMWEs grouping.", "labels": [], "entities": [{"text": "BMWEs", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.8445550203323364}]}, {"text": "Even though this paper does not directly address the issues in Cross-Lingual Information Retrieval (CLIR), it discusses an approach of direct relevance to the field.", "labels": [], "entities": [{"text": "Cross-Lingual Information Retrieval (CLIR)", "start_pos": 63, "end_pos": 105, "type": "TASK", "confidence": 0.7771419237057368}]}, {"text": "This approach could be viewed as the opposite of current trends in CLIR on semantic space that incorporate a notion of order in the bag-of-words model (e.g. co-occurences).", "labels": [], "entities": []}], "introductionContent": [{"text": "Word alignment ( remains key to providing high-quality translations as all subsequent training stages rely on its performance.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7494527697563171}]}, {"text": "It alone does not effectively capture many-to-many word correspondences, but instead relies on the ability of subsequent heuristic phrase extraction algorithms, such as grow-diagfinal (, to resolve them.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.7320532351732254}]}, {"text": "Some aligned corpora include implicit partial alignment annotation, while for other corpora a partial alignment can be extracted by state-ofthe-art techniques.", "labels": [], "entities": []}, {"text": "For example, implicit tags such as reference number within the patent corpus of provide (often many-tomany) correspondences between source and target words, while statistical methods for extracting a partial annotation, like, extract terminology pairs using linguistically predefined POS patterns.", "labels": [], "entities": []}, {"text": "extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information.", "labels": [], "entities": []}, {"text": "In Machine Translation, extract BMWEs from a phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8019367754459381}, {"text": "word alignment", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.7091948986053467}, {"text": "phrase extraction", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7471810877323151}, {"text": "word alignment", "start_pos": 159, "end_pos": 173, "type": "TASK", "confidence": 0.7065793871879578}]}, {"text": "This paper introduces anew method of incorporating previously known many-to-many word correspondences into word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 107, "end_pos": 121, "type": "TASK", "confidence": 0.7660017907619476}]}, {"text": "A well-known method of incorporating such prior knowledge in Machine Learning is to replace the likelihood maximization in the M-step of the EM algorithm with either the MAP estimate or the Maximum Penalized Likelihood (MPL) estimate).", "labels": [], "entities": []}, {"text": "Then, the MAP estimate allows us to incorporate the prior, a probability used to reflect the degree of prior belief about the occurrences of the events.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.77999347448349}]}, {"text": "A small number of studies have been carried out that use partial alignment annotation for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.8211483955383301}]}, {"text": "Firstly, introduce a posterior regularization to employ the prior that cannot be easily expressed over model parameters such as stochastic constraints and agreement constraints.", "labels": [], "entities": []}, {"text": "These constraints are set in the E-step to discard intractable alignments contradicting these constraints.", "labels": [], "entities": []}, {"text": "This mechanism in the E-step is in a similar spirit to that in GIZA++ for IBM Model 3 and 4 which only searches around neighbouring alignments around the Viterbi alignment.", "labels": [], "entities": []}, {"text": "For this reason, this algorithm is not intended to be used combined with IBM Models 3 and 4.", "labels": [], "entities": []}, {"text": "Although theoretically it is possible to incorporate partial annotation with a small change in its code, Graca et al. do not mention it.", "labels": [], "entities": []}, {"text": "Secondly, Talbot (2005) introduces a constrained EM method which constrains the E-step to incorporate partial alignment into word alignment, 1 which is in a similar manner to.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 125, "end_pos": 139, "type": "TASK", "confidence": 0.6755487620830536}]}, {"text": "He conducted experiments using partial alignment annotation based on cognate relations, a bilingual dictionary, domain-specific bilingual semantic annotation, and numerical pattern matching.", "labels": [], "entities": [{"text": "partial alignment annotation", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.7762836813926697}, {"text": "numerical pattern matching", "start_pos": 163, "end_pos": 189, "type": "TASK", "confidence": 0.5933282176653544}]}, {"text": "He did not incorporate BMWEs.", "labels": [], "entities": [{"text": "BMWEs", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.9449107050895691}]}, {"text": "Thirdly, replace the likelihood maximization in the M-step with mixed likelihood maximization, which is a convex combination of negative log likelihood of known links and unknown links.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: in Section 2 we define the anchor word alignment problem.", "labels": [], "entities": [{"text": "anchor word alignment", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.66535551349322}]}, {"text": "In Section 3 we include a review of the EM algorithm with IBM Models 1-5, and the HMM Model.", "labels": [], "entities": [{"text": "HMM Model", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.7945598065853119}]}, {"text": "Section 4 describes our own algorithm based on the combination of BMWE extraction and the modified word alignment which incorporates the groupings of BMWEs and enforces their alignment links; we explain the EM algorithm with MAP estimation Although the code maybe similar in practice to our Prior Model I, his explanation to modify the E-step will not be applied to IBM Models 3 and 4.", "labels": [], "entities": [{"text": "BMWE extraction", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.8181098401546478}, {"text": "MAP estimation", "start_pos": 225, "end_pos": 239, "type": "METRIC", "confidence": 0.8824315071105957}]}, {"text": "Our view is to modify the M-step due to the same reason above, i.e. GIZA++ searches only over the alignment space around the Viterbi alignment. with three kinds of priors.", "labels": [], "entities": []}, {"text": "In Section 5 our experimental results are presented, and we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The baseline in our experiments is a standard log-linear phrase-based MT system based on Moses.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9193596839904785}]}, {"text": "The GIZA++ implementation of IBM Model 4 is used as the baseline for word alignment, which we compare to our modified GIZA++.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.816422700881958}]}, {"text": "Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 5 iterations of Model 3, and 5 iterations of Model 4.", "labels": [], "entities": []}, {"text": "For phrase extraction the grow-diag-final heuristics are used to derive the refined alignment from bidirectional alignments.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8642380237579346}]}, {"text": "We then perform MERT while a 5-gram language model is trained with SRILM.", "labels": [], "entities": [{"text": "MERT", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.4833170771598816}, {"text": "SRILM", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.5420383214950562}]}, {"text": "Our implementation is based on a modified version of GIZA++ ().", "labels": [], "entities": []}, {"text": "This modification is on the function that reads a bilingual terminology file, the function that calculates priors, the M-step in IBM Models 1-5, and the forward-backward algorithm in the HMM Model.", "labels": [], "entities": [{"text": "IBM Models 1-5", "start_pos": 129, "end_pos": 143, "type": "DATASET", "confidence": 0.8700870672861735}]}, {"text": "Other related software tools are written in Python and Perl: terminology concatenation, terminology numbering, and so forth.", "labels": [], "entities": [{"text": "terminology concatenation", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.8590064644813538}, {"text": "terminology numbering", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.864395260810852}]}, {"text": "We conduct an experimental evaluation on the NTCIR-8 corpus () and on Europarl (.", "labels": [], "entities": [{"text": "NTCIR-8 corpus", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.9825573861598969}, {"text": "Europarl", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.9886511564254761}]}, {"text": "Firstly, MWEs are extracted from both corpora, as shown in.", "labels": [], "entities": []}, {"text": "In the second step, we apply our modified version of GIZA++ in which we incorporate the results of 6 This is because it needs to maintain potentially an \u2113 \u00d7 m matrix, where \u2113 denotes the number of English tokens in the corpus and m denotes the number of foreign tokens, even if the matrix is sparse.", "labels": [], "entities": []}, {"text": "Prior Model I only requires an\u02c6\u2113an\u02c6 an\u02c6\u2113 \u00d7 \u02c6 m matrix wher\u00ea \u2113 is the number of English tokens in a sentence and\u02c6mand\u02c6 and\u02c6m is the number of foreign tokens in a sentence, which is only needed until this information is incorporated in a posterior probability during the iterative process.", "labels": [], "entities": []}, {"text": "Secondly, in order to incorporate the extracted MWEs, they are reformatted as shown in.", "labels": [], "entities": []}, {"text": "Thirdly, we convert all MWEs into a single token, i.e. we concatenate them with an underscore character.", "labels": [], "entities": []}, {"text": "We then run the modified version of GIZA++ and obtain a phrase and reordering table.", "labels": [], "entities": []}, {"text": "In the fourth step, we split the concatenated MWEs embedded in the third step.", "labels": [], "entities": []}, {"text": "Finally, in the fifth step, we run MERT, and proceed with decoding before automatically evaluating the translations.", "labels": [], "entities": [{"text": "MERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.903674304485321}]}, {"text": "shows the results where 'baseline' indicates no BMWE grouping nor prior, and 'baseline2' represents a BMWE grouping but without the prior.", "labels": [], "entities": [{"text": "BMWE", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.8641720414161682}]}, {"text": "Although 'baseline2' (BMWE grouping) shows a drop in performance in the JP-EN / EN-JP 50k sentence pair setting, Prior Model I results in an increase in performance in the same setting.", "labels": [], "entities": [{"text": "BMWE grouping", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.6737040579319}]}, {"text": "Except for EN-ES 200k, our Prior Model I was better than 'baseline2'.", "labels": [], "entities": [{"text": "EN-ES 200k", "start_pos": 11, "end_pos": 21, "type": "DATASET", "confidence": 0.9093186259269714}, {"text": "Prior Model I", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.925839344660441}]}, {"text": "For EN-JP NT-CIR using 200k sentence pairs, we obtained an absolute improvement of 0.77 Bleu points compared to the 'baseline'; for EN-JP using 50k sentence pairs, 0.75 Bleu points; and for ES-EN Europarl corpus using 200k sentence pairs, 0.63 Bleu points.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.978557288646698}, {"text": "ES-EN Europarl corpus", "start_pos": 190, "end_pos": 211, "type": "DATASET", "confidence": 0.8717817068099976}]}, {"text": "In contrast, Prior Model II did notwork well.", "labels": [], "entities": [{"text": "Prior Model II", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.9365370869636536}]}, {"text": "The possible reason for this is the misspecification, i.e. the modelling by IBM Model 4 was wrong in terms of the given data.", "labels": [], "entities": [{"text": "misspecification", "start_pos": 36, "end_pos": 52, "type": "METRIC", "confidence": 0.9902985692024231}, {"text": "IBM Model 4", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.8987619876861572}]}, {"text": "One piece of evidence for this is that most of the enforced alignments were found correct in a manual inspection.", "labels": [], "entities": []}, {"text": "For  is slightly better than that of the statistical method.", "labels": [], "entities": []}, {"text": "The possible reason for this is related to the way the heuristic method groups terms including reference numbers, while the statistical method does not.", "labels": [], "entities": []}, {"text": "As a result, the complexity of the alignment model simplifies slightly in the case of the heuristic method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The benefit of prior knowledge of anchor  words.", "labels": [], "entities": []}, {"text": " Table 2: Example of MWE pairs in Europarl cor- pus (FR-EN) and NTCIR patent corpus (JP-EN).  There are 5 columns for each term: sentence num- ber, source term, target term, source position, and  target position. The number appended to each  term from the patent corpus (lower half) is a ref- erence number. In this corpus, all the important  technical terms have been identified and annotated  with reference numbers.", "labels": [], "entities": [{"text": "Europarl cor- pus (FR-EN)", "start_pos": 34, "end_pos": 59, "type": "DATASET", "confidence": 0.7418197223118373}, {"text": "NTCIR patent corpus (JP-EN)", "start_pos": 64, "end_pos": 91, "type": "DATASET", "confidence": 0.9219881097475687}, {"text": "ref- erence number", "start_pos": 288, "end_pos": 306, "type": "METRIC", "confidence": 0.7549156099557877}]}, {"text": " Table 3: Statistics of our MWE extraction method.  The numbers of MWEs are from 0.08 to 0.6 MWE  / sentence pair in our statistical MWE extraction  methods.", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.9464194178581238}, {"text": "MWE extraction", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.8589735925197601}]}, {"text": " Table 4: Results. Baseline is plain GIZA++ /  Moses (without BMWE grouping / prior), base- line2 is with BMWE grouping, prior I / II are with  BMWE grouping and prior.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9654501676559448}, {"text": "GIZA", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9139673709869385}]}]}