{"title": [], "abstractContent": [{"text": "This paper describes the implementation of our system at CLP 2010 bake-off of Chinese word sense induction.", "labels": [], "entities": [{"text": "CLP 2010 bake-off", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.9014440973599752}, {"text": "Chinese word sense induction", "start_pos": 78, "end_pos": 106, "type": "TASK", "confidence": 0.7396146804094315}]}, {"text": "We first extract the triplets for the target word in each sentence, then use the intersection of all related words of these triplets from the Internet.", "labels": [], "entities": []}, {"text": "We use the related word to construct feature vectors for the sentence.", "labels": [], "entities": []}, {"text": "At last we discriminate the word senses by clustering the sentences.", "labels": [], "entities": []}, {"text": "Our system achieved 77.88% F-score under the official evaluation.", "labels": [], "entities": [{"text": "F-score", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9996753931045532}]}], "introductionContent": [{"text": "The goal of the CLP 2010 bake-off of Chinese word sense induction is to automatically discriminate the senses of Chinese target words by the use of only un-annotated data.", "labels": [], "entities": [{"text": "CLP 2010 bake-off", "start_pos": 16, "end_pos": 33, "type": "DATASET", "confidence": 0.8967158397038778}, {"text": "Chinese word sense induction", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.750564843416214}]}, {"text": "The use of word senses instead of word forms has been shown to improve performance in information retrieval, information extraction and machine translation.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.7955056130886078}, {"text": "information extraction", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.8275233209133148}, {"text": "machine translation", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.8120260536670685}]}, {"text": "Word Sense Disambiguation generally requires the use of large-scale manually annotated lexical resources.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7141280372937521}]}, {"text": "Word Sense Induction can overcome this limitation, and it has become one of the most important topics in current computational linguistics research.", "labels": [], "entities": [{"text": "Word Sense Induction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7456287542978922}]}, {"text": "In this paper we introduce a method to solve the problem of Chinese word sense induction.For this task, Firstly we constructed triplets containing the target word in every instance, then searched the intersection of all the three words from the Internet with web searching engine and constructed feature vectors.Then we clustered the vectors with the sIB clustering algorithm and at last discriminated the word senses.", "labels": [], "entities": [{"text": "Chinese word sense induction.For", "start_pos": 60, "end_pos": 92, "type": "TASK", "confidence": 0.6414727568626404}]}, {"text": "This paper is organized as following: firstly we introduce the related works.", "labels": [], "entities": []}, {"text": "Then we talk about the methods in features selection and clustering.", "labels": [], "entities": [{"text": "features selection", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.766525149345398}]}, {"text": "The method of evaluation and the result of our system is following.", "labels": [], "entities": []}, {"text": "At last we discuss the improvement and the weakness of our system.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation measure is described as following: We consider the gold standard as a solution to the clustering problem.", "labels": [], "entities": []}, {"text": "All examples tagged with a given sense in the gold standard form a class.", "labels": [], "entities": []}, {"text": "For the system output, the clusters are formed by instances assigned to the same sense tag (the sense tag with the highest weight for that instance).", "labels": [], "entities": []}, {"text": "We will compare clusters output by the system with the classes in the gold standard and compute F-score as usual.", "labels": [], "entities": [{"text": "F-score", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.99504154920578}]}, {"text": "F-score is computed with the formula below.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9911355972290039}]}, {"text": "Suppose Cr is a class of the gold standard, and Si is a cluster of the system generated, then 1.", "labels": [], "entities": []}, {"text": "F \u2212 score(C r , Si ) = 2 * P * RP +R 2.", "labels": [], "entities": []}, {"text": "P =the number of correctly labeled examples fora cluster/total cluster size 3.", "labels": [], "entities": []}, {"text": "R =the number of correctly labeled examples fora cluster/total class size Then fora given class Cr , Where c is total number of classes, n r is the size of class Cr , and n is the total size.", "labels": [], "entities": []}, {"text": "The data set includes 100 ambiguous Chinese words and for every word it provided 50 instances.", "labels": [], "entities": []}, {"text": "Besides that they also provided a sample test set of 2500 examples of 50 target words with the answers to illustrate the data format.", "labels": [], "entities": []}, {"text": "Besides the sIB algorithm we also apply the k-means and EM algorithm to cluster the feature vectors.", "labels": [], "entities": [{"text": "EM", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9746969938278198}]}, {"text": "These algorithms are separately using the simpleKMeans class and the EM class in the Weka 3.5.8 cluster package.", "labels": [], "entities": [{"text": "Weka 3.5.8 cluster package", "start_pos": 85, "end_pos": 111, "type": "DATASET", "confidence": 0.9519663006067276}]}, {"text": "Except the number of clusters set as the given number of senses and number of seeds set as zero, all other parameters are set as default.", "labels": [], "entities": []}, {"text": "For the given sample test set with answers the result is illustrated in the algorithm F-score k-means 0.7025 EM 0.7286 sIB 0.8132: Results of three clustering algorithms From we can seethe sIB clustering algorithm improves the result of the Chinese word sense induction evidently.", "labels": [], "entities": [{"text": "F-score k-means 0.7025 EM 0.7286 sIB 0.8132", "start_pos": 86, "end_pos": 129, "type": "METRIC", "confidence": 0.901614921433585}]}, {"text": "In the real test data test containing 100 ambiguous Chinese words, our system achieves a F-score 0.7788 ranking 6th among the 18 systems submitted.", "labels": [], "entities": [{"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9993902444839478}]}, {"text": "The best F-score of these 18 systems is 0.7933 and the average of them is 0.7128.", "labels": [], "entities": [{"text": "F-score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9985365867614746}]}], "tableCaptions": []}