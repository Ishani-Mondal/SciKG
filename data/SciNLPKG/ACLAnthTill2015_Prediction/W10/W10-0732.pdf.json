{"title": [{"text": "Non-Expert Correction of Automatically Generated Relation Annotations", "labels": [], "entities": [{"text": "Automatically Generated Relation Annotations", "start_pos": 25, "end_pos": 69, "type": "TASK", "confidence": 0.6820804700255394}]}], "abstractContent": [{"text": "We explore anew way to collect human annotated relations in text using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 71, "end_pos": 93, "type": "DATASET", "confidence": 0.9472475647926331}]}, {"text": "Given a knowledge base of relations and a corpus, we identify sentences which mention both an entity and an attribute that have some relation in the knowledge base.", "labels": [], "entities": []}, {"text": "Each noisy sentence/relation pair is presented to multiple turkers, who are asked whether the sentence expresses the relation.", "labels": [], "entities": []}, {"text": "We describe a design which encourages user efficiency and aids discovery of cheating.", "labels": [], "entities": []}, {"text": "We also present results on inter-annotator agreement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation extraction (RE) is the task of determining the existence and type of relation between two textual entity mentions.", "labels": [], "entities": [{"text": "Relation extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.886733603477478}]}, {"text": "Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death.", "labels": [], "entities": [{"text": "Slot filling", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9232462346553802}, {"text": "relation extraction", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.8049923181533813}]}, {"text": "RE annotated data, such as ACE, is expensive to produce so systems take different approaches to minimizing data needs.", "labels": [], "entities": [{"text": "RE annotated", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8122743666172028}]}, {"text": "For example, tree kernels can reduce feature sparsity and generalize across many examples (.", "labels": [], "entities": []}, {"text": "Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (.", "labels": [], "entities": []}, {"text": "While this method can quickly generate training data, it also generates many false examples.", "labels": [], "entities": []}, {"text": "We reduce the noise in such examples by using Amazon, which has been shown to produce high quality annotations fora variety of natural language processing tasks (.", "labels": [], "entities": []}, {"text": "We use MTurk for annotation of textual relations to establish an inexpensive and rapid method of creating data for slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 115, "end_pos": 127, "type": "TASK", "confidence": 0.8865774571895599}]}, {"text": "We present a two step annotation process: (1) automatic creation of noisy examples, and (2) human validation of examples.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Size, cost and time to complete each HITs batch.", "labels": [], "entities": []}, {"text": " Table 3: Inter-annotator agreement across relation type.  # Ex. is the number of noisy examples. Exact-\u03ba and Pair- wise agreement are among the five workers.", "labels": [], "entities": [{"text": "Pair- wise agreement", "start_pos": 110, "end_pos": 130, "type": "METRIC", "confidence": 0.8971379846334457}]}, {"text": " Table 4: Confusion matrix of expert-1 and user's anno- tations on the sample of noisy examples, for the choices  Expressed (E), Not Expressed (NE), and Nonsense", "labels": [], "entities": []}, {"text": " Table 5: Exact-\u03ba scores for three levels of quality control  and a baseline, between each expert and the majority vote  on 231 sampled examples. For a fair comparison, we re- duced the sample size to include only examples for which  each level of quality control had at least one worker an- notation remaining.", "labels": [], "entities": []}]}