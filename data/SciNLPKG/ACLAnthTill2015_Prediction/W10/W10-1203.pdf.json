{"title": [{"text": "Query-based Text Normalization Selection Models for Enhanced Retrieval Accuracy", "labels": [], "entities": [{"text": "Query-based Text Normalization Selection", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.645574152469635}, {"text": "Accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.7974512577056885}]}], "abstractContent": [{"text": "Text normalization transforms words into abase form so that terms from common equivalent classes match.", "labels": [], "entities": [{"text": "Text normalization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7300575077533722}]}, {"text": "Traditionally, information retrieval systems employ stemming techniques to remove derivational affixes.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.7888844907283783}]}, {"text": "Deplu-ralization, the transformation of plurals into singular forms, is also used as a low-level text normalization technique to preserve more precise lexical semantics of text.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.7489867806434631}]}, {"text": "Experiment results suggest that the choice of text normalization technique should be made individually on each topic to enhance information retrieval accuracy.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7721317708492279}, {"text": "information retrieval", "start_pos": 128, "end_pos": 149, "type": "TASK", "confidence": 0.7375725507736206}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.8422983884811401}]}, {"text": "This paper proposes a hybrid approach, constructing a query-based selection model to select the appropriate text normalization technique (stemming, deplural-ization, or not doing any text normalization).", "labels": [], "entities": [{"text": "text normalization", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.724223181605339}, {"text": "text normalization", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.7099234908819199}]}, {"text": "The selection model utilized ambiguity properties extracted from queries to train a composite of Support Vector Regression (SVR) models to predict a text normalization technique that yields the highest Mean Average Precision (MAP).", "labels": [], "entities": [{"text": "text normalization", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.710584968328476}, {"text": "Mean Average Precision (MAP)", "start_pos": 202, "end_pos": 230, "type": "METRIC", "confidence": 0.9680842359860738}]}, {"text": "Based on our study, such a selection model holds promise in improving retrieval accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9473142623901367}]}], "introductionContent": [{"text": "Stemming removes derivational affixes of terms therefore allowing terms from common equivalence classes to be clustered.", "labels": [], "entities": []}, {"text": "However, stemming also introduces noise by mapping words of different concepts or meanings into one base form, thus impeding word-sense disambiguation.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.6996060758829117}]}, {"text": "Depluralization, the conversion of plural word forms to singular form, preserves more precise semantics of text than stemming).", "labels": [], "entities": []}, {"text": "Empirical research has demonstrated the ambivalent effect of stemming on text retrieval performance.", "labels": [], "entities": [{"text": "stemming", "start_pos": 61, "end_pos": 69, "type": "TASK", "confidence": 0.9627733826637268}, {"text": "text retrieval", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.798206239938736}]}, {"text": "conducted a comprehensive case study on the effects of four stemmer techniques and the removal of plural \"s\" 1 on retrieval performance.", "labels": [], "entities": []}, {"text": "Hull suggested that the adoption of stemming is beneficial but plural removal is as well competitive when the size of documents is small.", "labels": [], "entities": [{"text": "plural removal", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.9120673537254333}]}, {"text": "Prior research indicated that traditional stemming, though still benefiting some queries, would not necessarily enhance the average retrieval performance.", "labels": [], "entities": []}, {"text": "In addition, stemming was considered one of the technique failures undermining retrieval performance in the).", "labels": [], "entities": [{"text": "stemming", "start_pos": 13, "end_pos": 21, "type": "TASK", "confidence": 0.9860963225364685}]}, {"text": "Prior research also noted the semantic differences between plurals and singulars.", "labels": [], "entities": []}, {"text": "indicated that plural and singular nouns are distinct because plural nouns usually pertain to the \"general types of incidents,\" while singular nouns often pertain to \"a specific incident.\"", "labels": [], "entities": []}, {"text": "Nevertheless, prior research has not closely examined the effect of the change of the semantics caused by different level of text normalization techniques.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.7109336853027344}]}, {"text": "In our work, we conducted extensive experiments on the TREC 2004 Robust track collection to evaluate the effect of stemming and depluralization on information retrieval.", "labels": [], "entities": [{"text": "TREC 2004 Robust track collection", "start_pos": 55, "end_pos": 88, "type": "DATASET", "confidence": 0.9244369506835938}, {"text": "information retrieval", "start_pos": 147, "end_pos": 168, "type": "TASK", "confidence": 0.80524080991745}]}, {"text": "In addition, we quantify the ambiguity of a query, extracting five ambiguity properties from queries.", "labels": [], "entities": []}, {"text": "The extracted ambiguity properties are used to construct query-based selection model, a composite of multiple Support Vector Regression models, to determine the most appropriate text normalization technique fora given query.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.7013866156339645}]}, {"text": "To our knowledge, our work is the first study to construct a query-based selection model, using ambiguity properties extracted from provided queries to select an optimal text normalization technique for each query.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2 we describe our experimental setups and dataset.", "labels": [], "entities": []}, {"text": "Section 3 describes and analyzes experiment results of different text normalization techniques on the dataset.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7251199334859848}]}, {"text": "We discuss five ambiguity properties and validate each property in section 4.", "labels": [], "entities": []}, {"text": "In section 5 we describe the framework and the prediction results of the proposed query-based selection model.", "labels": [], "entities": []}, {"text": "Finally, we summarize our findings and discuss future work in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiment utilizes the queries and relevance judgment results from the TREC 2004 Robust Track to evaluate the effect of three text normalization techniques -raw text, depluralized text, and stemmed text.", "labels": [], "entities": [{"text": "TREC 2004 Robust Track", "start_pos": 76, "end_pos": 98, "type": "DATASET", "confidence": 0.8576948195695877}]}, {"text": "The TREC 2004 Robust Track used a document set of approximately 528,000 documents comprising 1,904 MB of text.", "labels": [], "entities": [{"text": "TREC 2004 Robust Track", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8780178129673004}]}, {"text": "In total, 249 query topics were used in TREC Robust 2004.", "labels": [], "entities": [{"text": "TREC Robust 2004", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.7490067283312479}]}, {"text": "illustrates the setup of the experiment.", "labels": [], "entities": []}, {"text": "The collection is parsed with a SAX parser and stored in a Postgres database.", "labels": [], "entities": [{"text": "Postgres database", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.9547417461872101}]}, {"text": "Lucene is then used to generate three indices: indices of raw text, depluralized text, and stemmed text.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8725690245628357}]}, {"text": "The Postgres database stores each document of the collection, the query topics of the TREC 2004 Robust Track, and results of experiments.", "labels": [], "entities": [{"text": "Postgres database", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9547402858734131}, {"text": "TREC 2004 Robust Track", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.7800157517194748}]}, {"text": "The ambiguity properties for each query is also computed in the Postgres system.", "labels": [], "entities": []}, {"text": "We query Lucene indices to obtain the top 1,000 relevant results and compute Mean Average Precision (MAP) with the trec eval program to evaluate performance.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 77, "end_pos": 105, "type": "METRIC", "confidence": 0.9750826954841614}]}, {"text": "We use R to analyze performance scores, generate descriptive charts, conduct non-parametric statistical tests, and perform a paired t-test.", "labels": [], "entities": []}, {"text": "We use Weka () to construct query-based selection model that incorporates multiple Support Vector Regression (SVR) models.", "labels": [], "entities": []}, {"text": "For each query model and text normalization technique, we present the MAP value computed across all relevant topics.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7463582158088684}, {"text": "MAP", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8894216418266296}]}, {"text": "We also provide the pvalue for comparing MAP between each normalization technique and the baseline (i.e. non-normalized (raw) queries).", "labels": [], "entities": [{"text": "MAP", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.6924342513084412}]}, {"text": "The p-value is generated from the pairwise Wilcoxon signed rank sum test.", "labels": [], "entities": []}, {"text": "describes the distribution of MAP across the three text normalization techniques and three query models.", "labels": [], "entities": [{"text": "MAP", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9145166873931885}, {"text": "text normalization", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7195737212896347}]}, {"text": "The distributions are skewed and many outliers are observed.", "labels": [], "entities": []}, {"text": "In general, boolean OR and MLT query models perform similarly and stemming has the highest median MAP value across all three query models.", "labels": [], "entities": [{"text": "MAP", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9874602556228638}]}, {"text": "The results from for the combined topic set show that depluralization and stemming perform significantly better than the raw baseline.", "labels": [], "entities": [{"text": "stemming", "start_pos": 74, "end_pos": 82, "type": "TASK", "confidence": 0.9667843580245972}]}, {"text": "However, the performance difference between depluralization and stemming is not significant except for the AND boolean query model.", "labels": [], "entities": []}, {"text": "In general, the differences of MAP among three text normalization 20 Figure 2: Using the query \"hydrogen fuel automobiles\" as an example, the depluralized query becomes \"hydrogen fuel automobile\" and the stemmed query becomes \"hydrogen fuel automobil.\"", "labels": [], "entities": [{"text": "MAP", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.6769920587539673}]}, {"text": "Final boolean queries for depluralized topic become \"hydrogen AND fuel AND automobile\" and \"hydrogen OR fuel OR automobil.\"", "labels": [], "entities": []}, {"text": "MoreLikeThis (MLT) is the Lucene class used for cosine similarity retrieval.", "labels": [], "entities": [{"text": "cosine similarity retrieval", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.6155726710955302}]}, {"text": "A term vector score appends each word in the topic.", "labels": [], "entities": []}, {"text": "techniques are within 2%.", "labels": [], "entities": []}, {"text": "To visualize the relative performances among three text normalization techniques, we standardized the three MAP values fora single topic (one from each text normalization technique) to have mean 0 and standard deviation of 1.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.6988245397806168}, {"text": "MAP", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9310818314552307}, {"text": "standard deviation", "start_pos": 201, "end_pos": 219, "type": "METRIC", "confidence": 0.940756618976593}]}, {"text": "The result provides a 3-value pattern emphasizing the ordering of the MAPs across the text normalization techniques, rather than the raw MAP values themselves.", "labels": [], "entities": []}, {"text": "We then used the K-medoids algorithm to cluster the transformed data, applying Euclidean distance as the distance measure. is an example of 9 constructed clusters based on the MAP scores of the MLT query model.", "labels": [], "entities": [{"text": "MAP", "start_pos": 176, "end_pos": 179, "type": "METRIC", "confidence": 0.7373961210250854}]}, {"text": "Ina cluster, a line represents the standardized MAP value of a topic on each text normalization technique.", "labels": [], "entities": []}, {"text": "Given the small differences in aggregate MAP performance, it is interesting to note that the clusters demonstrate variable patterns, indicating that some topics performed better as a depluralized query than a stemmed query.", "labels": [], "entities": [{"text": "MAP", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9157097339630127}]}, {"text": "The cluster analysis suggests that the choice of text normalization technique should be made individually on each topic.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7739565670490265}]}, {"text": "As we choose an appropriate text normalization technique fora given topic, we would further enhance retrieval performance.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7097226977348328}]}, {"text": "In 21  the next section, we address the issue of inconsistent performance by constructing regression models to predict the mean average precision of each query from the ambiguity measures, and choose an appropriate normalization method based on these predictions.", "labels": [], "entities": [{"text": "mean average precision", "start_pos": 123, "end_pos": 145, "type": "METRIC", "confidence": 0.6760956645011902}]}], "tableCaptions": [{"text": " Table 1: Paired Wilcoxon signed-ranked test on Mean  Average Precision (MAP), utilizing raw query as the  baseline. Significant differences between query models  are labeled with *. Results labeled with  \u2020 indicate sig- nificant differences between depluralized queries and a  stemmed queries.", "labels": [], "entities": [{"text": "Mean  Average Precision (MAP)", "start_pos": 48, "end_pos": 77, "type": "METRIC", "confidence": 0.9738862216472626}]}, {"text": " Table 2: Results of simple linear regression on the MAP of stemmed queries in MLT query model.", "labels": [], "entities": [{"text": "MAP", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.6612066626548767}]}, {"text": " Table 3: Paired T-test was performed to examine the differences of each text normalization techniques (raw, deplu- ralizer, and stemmer) and query-based selection model (hybrid model). Significant differences between models are  labeled with *.", "labels": [], "entities": []}]}