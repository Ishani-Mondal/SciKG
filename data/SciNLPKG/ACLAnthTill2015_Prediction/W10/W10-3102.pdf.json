{"title": [{"text": "Creating and Evaluating a Consensus for Negated and Speculative Words in a Swedish Clinical Corpus", "labels": [], "entities": [{"text": "Swedish Clinical Corpus", "start_pos": 75, "end_pos": 98, "type": "DATASET", "confidence": 0.6220411558945974}]}], "abstractContent": [{"text": "In this paper we describe the creation of a consensus corpus that was obtained through combining three individual annotations of the same clinical corpus in Swedish.", "labels": [], "entities": []}, {"text": "We used a few basic rules that were executed automatically to create the consensus.", "labels": [], "entities": []}, {"text": "The corpus contains negation words, speculative words, uncertain expressions and certain expressions.", "labels": [], "entities": []}, {"text": "We evaluated the consensus using it for negation and speculation cue detection.", "labels": [], "entities": [{"text": "negation and speculation cue detection", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.6427403748035431}]}, {"text": "We used Stanford NER, which is based on the machine learning algorithm Conditional Random Fields for the training and detection.", "labels": [], "entities": [{"text": "Stanford NER", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.9190772175788879}]}, {"text": "For comparison we also used the clinical part of the BioScope Corpus and trained it with Stanford NER.", "labels": [], "entities": [{"text": "BioScope Corpus", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.8528558611869812}, {"text": "Stanford NER", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.8993650674819946}]}, {"text": "For our clinical consensus corpus in Swedish we obtained a precision of 87.9 percent and a recall of 91.7 percent for negation cues, and for English with the Bioscope Corpus we obtained a precision of 97.6 percent and a recall of 96.7 percent for negation cues.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9976162910461426}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9982802867889404}, {"text": "Bioscope Corpus", "start_pos": 158, "end_pos": 173, "type": "DATASET", "confidence": 0.9296620190143585}, {"text": "precision", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.9824287295341492}, {"text": "recall", "start_pos": 220, "end_pos": 226, "type": "METRIC", "confidence": 0.9989784955978394}]}], "introductionContent": [{"text": "How we use language to express our thoughts, and how we interpret the language of others, varies between different speakers of a language.", "labels": [], "entities": []}, {"text": "This is true for various aspects of a language, and also for the topic of this article; negations and speculations.", "labels": [], "entities": [{"text": "negations", "start_pos": 88, "end_pos": 97, "type": "TASK", "confidence": 0.9637125730514526}]}, {"text": "The differences in interpretation are of course most relevant when a text is used for communication, but it also applies to the task of annotation.", "labels": [], "entities": []}, {"text": "When the same text is annotated by more than one annotator, given that the annotating task is non-trivial, the resulting annotated texts will not be identical.", "labels": [], "entities": []}, {"text": "This will be the result of differences in how the text is interpreted, but also of differences in how the instructions for annotation are interpreted.", "labels": [], "entities": []}, {"text": "In order to use the annotated texts, it must first be decided if the interpretations by the different annotators are similar enough for the purpose of the text, and if so, it must be decided how to handle the non-identical annotations.", "labels": [], "entities": []}, {"text": "In the study described in this article, we have used a Swedish clinical corpus that was annotated for certainty and uncertainty, as well as for negation and speculation cues by three Swedishspeaking annotators.", "labels": [], "entities": [{"text": "Swedish clinical corpus", "start_pos": 55, "end_pos": 78, "type": "DATASET", "confidence": 0.7742514212926229}, {"text": "certainty", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9774865508079529}]}, {"text": "The article describes an evaluation of a consensus annotation obtained through a few basic rules for combining the three different annotations into one annotated text.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of the number of occurrences  of each annotation class for the individual annota- tions and the consensus annotation. The figures  for the individual annotations are the mean of the  three annotators, normalised on the number of sen- tences in the consensus.", "labels": [], "entities": []}, {"text": " Table 3: The results for negation and speculation on consensus when executing Stanford NER CRF using  ten-fold cross validation.", "labels": [], "entities": [{"text": "negation", "start_pos": 26, "end_pos": 34, "type": "TASK", "confidence": 0.983347475528717}, {"text": "Stanford NER CRF", "start_pos": 79, "end_pos": 95, "type": "DATASET", "confidence": 0.8251147071520487}]}, {"text": " Table 4: The results for certain and uncertain on consensus when executing Stanford NER CRF using  ten-fold cross validation.", "labels": [], "entities": [{"text": "Stanford NER CRF", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.771621823310852}]}, {"text": " Table 5: The results for negations, speculation cues and scopes on the BioScope Corpus when executing  Stanford NER CRF using ten-fold cross validation.", "labels": [], "entities": [{"text": "negations", "start_pos": 26, "end_pos": 35, "type": "TASK", "confidence": 0.9761247038841248}, {"text": "BioScope Corpus", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.924252986907959}, {"text": "Stanford NER CRF", "start_pos": 104, "end_pos": 120, "type": "DATASET", "confidence": 0.9004063804944357}]}, {"text": " Table 6: The results for negations and speculation cues and scopes for annotator A, F and H respectively  when executing Stanford NER CRF using ten-fold cross validation.", "labels": [], "entities": [{"text": "negations", "start_pos": 26, "end_pos": 35, "type": "TASK", "confidence": 0.9726771712303162}, {"text": "Stanford NER CRF", "start_pos": 122, "end_pos": 138, "type": "DATASET", "confidence": 0.9101908604303995}]}, {"text": " Table 7: Number of unique words both in the Con- sensus and in the BioScope Corpus that were an- notated as Negation and as Speculative words, and  how many of these that occurred only once.", "labels": [], "entities": [{"text": "BioScope Corpus", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.9026451706886292}]}]}