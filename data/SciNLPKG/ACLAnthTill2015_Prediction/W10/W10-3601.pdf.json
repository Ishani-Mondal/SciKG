{"title": [{"text": "Boosting N-gram Coverage for Unsegmented Languages Using Multiple Text Segmentation Approach", "labels": [], "entities": [{"text": "Boosting N-gram Coverage", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6311467190583547}, {"text": "Multiple Text Segmentation Approach", "start_pos": 57, "end_pos": 92, "type": "TASK", "confidence": 0.6171838045120239}]}], "abstractContent": [{"text": "Automatic word segmentation errors, for languages having a writing system without word boundaries, negatively affect the performance of language models.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6928014755249023}]}, {"text": "As a solution, the use of multiple, instead of unique, segmentation has recently been proposed.", "labels": [], "entities": []}, {"text": "This approach boosts N-gram counts and generates new N-grams.", "labels": [], "entities": []}, {"text": "However, it also produces bad N-grams that affect the language models' performance.", "labels": [], "entities": []}, {"text": "In this paper , we study more deeply the contribution of our multiple segmentation approach and experiment on an efficient solution to minimize the effect of adding bad N-grams.", "labels": [], "entities": []}], "introductionContent": [{"text": "A language model is a probability assignment overall possible word sequences in a natural language.", "labels": [], "entities": []}, {"text": "It assigns a relatively large probability to meaningful, grammatical, or frequent word sequences and a low probability or a zero probability to nonsensical, ungrammatical or rare ones.", "labels": [], "entities": []}, {"text": "The statistical approach used in N-gram language modeling requires a large amount of text data in order to make an accurate estimation of probabilities.", "labels": [], "entities": [{"text": "N-gram language modeling", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.6264523168404897}]}, {"text": "These data are not available in large quantities for under-resourced languages and the lack of text data has a direct impact on the performance of language models.", "labels": [], "entities": []}, {"text": "While the word is usually a basic unit in statistical language modeling, word identification is not a simple task even for languages that separate words by a special character (a white space in general).", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.7147066990534464}, {"text": "word identification", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7618638277053833}]}, {"text": "For unsegmented languages, which have a writing system without obvious word delimiters, the N-grams of words are usually estimated from the text corpus segmented into words employing automatic methods.", "labels": [], "entities": []}, {"text": "Automatic segmentation of text is not a trivial task and introduces errors due to the ambiguities in natural language and the presence of out of vocabulary words in the text.", "labels": [], "entities": [{"text": "Automatic segmentation of text", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7782928124070168}]}, {"text": "While the lack of text resources has a negative impact on the performance of language models, the errors produced by the word segmentation make those data even less usable.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.7275844812393188}]}, {"text": "The word N-grams not found in the training corpus could be due not only to the errors introduced by the automatic segmentation but also to the fact that a sequence of characters could have more than one correct segmentation.", "labels": [], "entities": []}, {"text": "In previous article, we have proposed a method to estimate an N-gram language model from the training corpus on which each sentence is segmented into multiple ways instead of a unique segmentation.", "labels": [], "entities": []}, {"text": "The objective of multiple segmentation is to generate more N-grams from the training corpus to use in language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.6998675465583801}]}, {"text": "It was possible to show that this approach generates more N-grams (compared to the classical dictionary-based unique segmentation method) that are potentially useful and relevant in language modeling.", "labels": [], "entities": []}, {"text": "The application of multiple segmentation in language modeling for Khmer and Vietnamese showed improvement in terms of tri-gram hits and recognition error rate in Automatic Speech Recognition (ASR) systems.", "labels": [], "entities": [{"text": "tri-gram hits", "start_pos": 118, "end_pos": 131, "type": "METRIC", "confidence": 0.9423472881317139}, {"text": "recognition error rate", "start_pos": 136, "end_pos": 158, "type": "METRIC", "confidence": 0.7975109418233236}, {"text": "Automatic Speech Recognition (ASR)", "start_pos": 162, "end_pos": 196, "type": "TASK", "confidence": 0.8011328379313151}]}, {"text": "This work is a continuation of our previous work on the use of multiple segmentation.", "labels": [], "entities": []}, {"text": "It is conducted on Vietnamese only.", "labels": [], "entities": []}, {"text": "A close analysis of N-gram counts shows that the approach has in fact two contributions: boosting the N-gram counts that are generated with first best segmentation and generating new N-grams.", "labels": [], "entities": []}, {"text": "We have also identified that there are N-grams that negatively affect the performance of the language models.", "labels": [], "entities": []}, {"text": "In this paper, we study the contribution of boosting N-gram counts and of new N-grams to the performance of the language models and consequently to the recognition performance.", "labels": [], "entities": []}, {"text": "We also present experiments where rare or bad N-grams are cutoff in order to minimize their negative effect on the performance of the language models.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: section 2 presents the theoretical background of our multiple segmentation approach; in section 3 we point out the setup of our experiment; in section 4 we present the results of our detailed statistical analysis of N-grams generated by multiple segmentation systems.", "labels": [], "entities": []}, {"text": "Section 5 presents the evaluation results of our language models for ASR and finally, we give concluding remarks.", "labels": [], "entities": [{"text": "ASR", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9886825084686279}]}], "datasetContent": [{"text": "In this section we present the various language models we have developed and their performance in terms of perplexity, tri-gram hits and ASR performance (syllable error rate).", "labels": [], "entities": [{"text": "ASR performance (syllable error rate", "start_pos": 137, "end_pos": 173, "type": "METRIC", "confidence": 0.7517414391040802}]}, {"text": "We use the results obtained with the method presented in as baseline.", "labels": [], "entities": []}, {"text": "This method consists in re-estimating the N-gram counts using the multiple segmentation of the training data and add one to the count of a trigram that appears several times in multiple segmentations of a single sentence.", "labels": [], "entities": []}, {"text": "These baseline results are presented in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. boosting tri-gram counts", "labels": [], "entities": []}, {"text": " Table 2. tri-gram contribution of multiple seg- mentation", "labels": [], "entities": []}, {"text": " Table 3. The results  show an increase of the tri-gram coverage and  slight improvements of the ASR performance.", "labels": [], "entities": [{"text": "tri-gram coverage", "start_pos": 47, "end_pos": 64, "type": "METRIC", "confidence": 0.8172045350074768}, {"text": "ASR", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.7328872084617615}]}, {"text": " Table 3. Results of experiments using the base- line method presented in (Seng et al., 2009)", "labels": [], "entities": []}, {"text": " Table 4. Contributions of new tri-grams", "labels": [], "entities": []}, {"text": " Table 5. Performance with pooling", "labels": [], "entities": [{"text": "pooling", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.8832181096076965}]}, {"text": " Table 6. Performance with cut off.", "labels": [], "entities": []}, {"text": " Table 7. Performance with hybrid method", "labels": [], "entities": []}]}