{"title": [{"text": "How to Expand Dictionaries with Web-Mining Techniques", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents an approach to enrich conceptual classes based on the Web.", "labels": [], "entities": []}, {"text": "To test our approach, we first build conceptual classes using syntactic and semantic information provided by a corpus.", "labels": [], "entities": []}, {"text": "The concepts can be the input of a dictionary.", "labels": [], "entities": []}, {"text": "Our web-mining approach deals with a cognitive process which simulates human reasoning based on the enumeration principle.", "labels": [], "entities": []}, {"text": "The experiments reveal the interest of our approach by adding new relevant terms to existing conceptual classes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Concepts have several definitions; one of the most general describes a concept 'as the mind's representation of a thing or an item'.", "labels": [], "entities": []}, {"text": "Ina domain such as ours, i.e. ontology building, semantic webs, and computational linguistics, it seems appropriate to stick to the Aristotelian approach to a concept, and consider it as a set of knowledge (gathered information) on common semantic features.", "labels": [], "entities": [{"text": "ontology building", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7355513125658035}]}, {"text": "The choice of the features and how the knowledge is gathered depend on criteria we will explain below.", "labels": [], "entities": []}, {"text": "In this paper, we deal with the building of conceptual classes, which can be defined as gathering semantically close terms.", "labels": [], "entities": []}, {"text": "First, we suggest building specific conceptual classes by focusing on knowledge extracted from corpora.", "labels": [], "entities": []}, {"text": "Conceptual classes are shaped by the study of syntactic dependencies between corpus terms (as described in section 2).", "labels": [], "entities": []}, {"text": "Dependencies tackle relations such as Verb/Subject, Noun/Noun Phrase Complements, Verb/Object, Verb/Complements, and sometimes Sentence Head/Complements.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the Verb/Object dependency because it is representative of afield.", "labels": [], "entities": []}, {"text": "For instance, in computer science, the verb 'to load' takes as objects, nouns of the conceptual class software.", "labels": [], "entities": []}, {"text": "This feature also extends to 'download' or 'upload', which have the same verbal root.", "labels": [], "entities": []}, {"text": "Corpora are rich sources of terminological information that can be mined.", "labels": [], "entities": []}, {"text": "A terminology extraction of this kind is similar to a Harris-like distributional analysis) and many works in the literature have been the subject of distributional analysis to acquire terminological or ontological knowledge from textual data (e.g) for law,) for medicine).", "labels": [], "entities": [{"text": "terminology extraction", "start_pos": 2, "end_pos": 24, "type": "TASK", "confidence": 0.7477835118770599}]}, {"text": "After building conceptual classes (section 2), we describe an approach to expand concepts by using a Web search engine to discover new terms (section 3).", "labels": [], "entities": []}, {"text": "In section 4, experiments conducted on real data enable us to validate our approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used a French corpus from the Yahoo site (http://fr.news.yahoo.com/) composed of 8,948 news items (16.5 MB) from newspapers.", "labels": [], "entities": [{"text": "French corpus from the Yahoo site", "start_pos": 10, "end_pos": 43, "type": "DATASET", "confidence": 0.8655611972014109}]}, {"text": "Experiments were performed on 60,000 syntactic relations () to build original conceptual classes.", "labels": [], "entities": []}, {"text": "We manually selected five concepts (see.", "labels": [], "entities": []}, {"text": "Instances of these concepts are the common objects of verbs defining the concept (see section 2.2).", "labels": [], "entities": []}, {"text": "For our experiments, we use an API of the search engine Yahoo!", "labels": [], "entities": []}, {"text": "We apply the following post-treatments for each new candidate term.", "labels": [], "entities": []}, {"text": "Therefore, we only keep the nouns, after applying a PoS (Part of Speech) tagger, the TreeTagger.", "labels": [], "entities": []}, {"text": "After these post-treatments, we manually validate the new terms using three experts.", "labels": [], "entities": []}, {"text": "We compute the precision of our approach to each expert.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9993708729743958}]}, {"text": "The average is calculated to define the quality of the terms.", "labels": [], "entities": []}, {"text": "Precision is defined as follows.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.6190809011459351}]}, {"text": "We calculate the precision before the filtering step.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9997261166572571}]}, {"text": "After applying the automatic filtering by selecting k terms per class, we calculate the precision obtained.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9994396567344666}]}, {"text": "Note that the automatic filtering (see section 3.2) reduces the number of terms proposed, and thus reduces the recall 2 .: Results obtained with k=4 (i.e. automatic selection of the k first ranked terms by the filtering approach).", "labels": [], "entities": [{"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9979231953620911}]}], "tableCaptions": []}