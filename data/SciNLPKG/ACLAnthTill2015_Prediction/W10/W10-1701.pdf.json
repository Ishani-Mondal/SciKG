{"title": [{"text": "A Semi-supervised Word Alignment Algorithm with Partial Manual Alignments", "labels": [], "entities": [{"text": "Word Alignment Algorithm", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.7602290908495585}]}], "abstractContent": [{"text": "We present a word alignment framework that can incorporate partial manual alignments.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7434892356395721}]}, {"text": "The core of the approach is a novel semi-supervised algorithm extending the widely used IBM Models with a constrained EM algorithm.", "labels": [], "entities": []}, {"text": "The partial manual alignments can be obtained by human labelling or automatically by high-precision-low-recall heuristics.", "labels": [], "entities": []}, {"text": "We demonstrate the usages of both methods by selecting alignment links from manually aligned corpus and apply links generated from bilingual dictionary on unla-belled data.", "labels": [], "entities": []}, {"text": "For the first method, we conduct controlled experiments on Chinese-English and Arabic-English translation tasks to compare the quality of word alignment , and to measure effects of two different methods in selecting alignment links from manually aligned corpus.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 138, "end_pos": 152, "type": "TASK", "confidence": 0.7205199748277664}]}, {"text": "For the second method, we experimented with moderate-scale Chinese-English translation task.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6301143616437912}]}, {"text": "The experiment results show an average improvement of 0.33 BLEU point across 8 test sets.", "labels": [], "entities": [{"text": "BLEU point", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.972842276096344}]}], "introductionContent": [{"text": "Word alignment is used in various natural language processing applications, and most statistical machine translation systems rely on word alignment as a preprocessing step.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7623247504234314}, {"text": "statistical machine translation", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.6436719000339508}, {"text": "word alignment", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.7527893781661987}]}, {"text": "Traditionally the word alignment model is trained in an unsupervised manner, e.g. the most widely used tool GIZA++ (, which implements the IBM Models) and the HMM model (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7569819986820221}]}, {"text": "However, for language pairs such as Chinese-English, the word alignment quality is often unsatisfactory.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.6803580075502396}]}, {"text": "There has been increasing interest on using manual alignments in word alignment tasks.", "labels": [], "entities": [{"text": "word alignment tasks", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.8534210324287415}]}, {"text": "proposed to use only manual alignment links in a maximum entropy model.", "labels": [], "entities": []}, {"text": "A number of semi-supervised word aligners are proposed).", "labels": [], "entities": []}, {"text": "These approaches use held-out manual alignments to tune the weights for discriminative models, with the model parameters, model scores or alignment links from unsupervised word aligners as features.", "labels": [], "entities": []}, {"text": "Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 () and the model proposed by and its extension called LEAF aligner.", "labels": [], "entities": []}, {"text": "The approaches use labelled data to tune parameters to combine different components of the IBM Models.", "labels": [], "entities": [{"text": "IBM Models", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.963688313961029}]}], "datasetContent": [{"text": "We designed a set of controlled experiments to show that the algorithm acts as desired.", "labels": [], "entities": []}, {"text": "Particularly, with a number of manual alignment links fed into the aligner, we should be able to correct more misaligned alignment links than the manual alignment links through better alignment models.", "labels": [], "entities": []}, {"text": "Also, carefully selected alignment links should outperform randomly selected alignment links.", "labels": [], "entities": []}, {"text": "We used Chinese-English and Arabic-English manually aligned corpus in the experiments.", "labels": [], "entities": []}, {"text": "First the corpora is trained as unlabelled data to serve as baselines, and then we feed a portion of alignment links into the proposed aligner.", "labels": [], "entities": []}, {"text": "We experimented with different methods of choosing alignment links and adjust the number of links visible to the aligner.", "labels": [], "entities": []}, {"text": "Because of the limitations of the IBM Models, such as no N-to-1 alignments, the manual alignment is not reachable from either direction.", "labels": [], "entities": [{"text": "IBM Models", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.928349643945694}]}, {"text": "We then define the best alignment that the IBM Models can express \"oracle alignment\", which can be obtained by dropping all N-to-1 links from manual alignment.", "labels": [], "entities": [{"text": "IBM Models", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.9291700720787048}]}, {"text": "Also, to show the upper-bound performance, we feed all the manual alignment links to our aligner, and call the alignment \"force alignment\".", "labels": [], "entities": []}, {"text": "shows the alignment qualities of oracle alignments and force alignments of both systems.", "labels": [], "entities": []}, {"text": "For force alignments, we show the scores with and without implicit empty links derived from the manual alignment.", "labels": [], "entities": []}, {"text": "The oracle alignments are the performance upper-bounds of all aligners under IBM Model's 1-to-N assumption.", "labels": [], "entities": []}, {"text": "The result from shows that, if we include the derived empty links, the force alignments are close to the oracle results.", "labels": [], "entities": []}, {"text": "Then the question is how fast we can approach the upper-bound.", "labels": [], "entities": []}, {"text": "To answer the question, we gradually increase the number of links being fed into the aligner.", "labels": [], "entities": []}, {"text": "In these experiments the seeds for random number generator are fixed so that the links selected in later experiments are always superset of that of earlier experiments.", "labels": [], "entities": []}, {"text": "The comparison of the alignment quality is shown in and 4.", "labels": [], "entities": []}, {"text": "To show the actual improvement brought in by the algorithm instead of the manual alignment links themselves, we compare the alignment results of the proposed method with directly fixing the alignments from original GIZA++ training.", "labels": [], "entities": []}, {"text": "By fixing alignments we mean that first the conventional GIZA++ training is performed and then we add the manual alignment links to the resulting alignment.", "labels": [], "entities": [{"text": "GIZA++ training", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.6489839951197306}]}, {"text": "In case that the 1-to-N restriction of the IBM Models is violated, we keep the manual alignment links and remove the links from GIZA++.", "labels": [], "entities": [{"text": "IBM Models", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.9673066437244415}, {"text": "GIZA++", "start_pos": 128, "end_pos": 134, "type": "DATASET", "confidence": 0.9080108106136322}]}, {"text": "We show the results as FR (dashed curves with diamond markers) and FD (dashed curves with square markers) in the plots, corresponding to alignments selected from the random link selector and the disagreement-based link selector.", "labels": [], "entities": [{"text": "FR", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9972321391105652}, {"text": "FD", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9915335774421692}]}, {"text": "These two curves serve as baseline, and the gaps between the FR curves and the WN curves (dotted curves with cross markers) and the gaps between the FD curves and the DF curves (solid curves) show the amount of improvement we achieved using the method in addition to the manual alignment links.", "labels": [], "entities": [{"text": "FR curves", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.8033461272716522}]}, {"text": "Therefore, they represent the effectiveness of the proposed alignment approach.", "labels": [], "entities": [{"text": "alignment", "start_pos": 60, "end_pos": 69, "type": "TASK", "confidence": 0.9672595858573914}]}, {"text": "Also the gaps between DF and WN curves indicate the differences in the performance of two link selectors.", "labels": [], "entities": []}, {"text": "The plots illustrate that when the number of links is small, the WN and DF curves are always higher than the FR/FD curves.", "labels": [], "entities": [{"text": "FR", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9905645847320557}, {"text": "FD", "start_pos": 112, "end_pos": 114, "type": "METRIC", "confidence": 0.5305767059326172}]}, {"text": "It proves that our system does not just fix the links provided by manual alignments, instead the information propagates to other links.", "labels": [], "entities": []}, {"text": "The largest gap between FD and DF is 8% absolute in combined alignment of Chinese-English system with 200,000 manual alignment links.", "labels": [], "entities": [{"text": "FD", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.7008062601089478}, {"text": "absolute", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9644091129302979}]}, {"text": "Also, we can see that the disagreement-based link selector (DF) always outperform the random selector (WN).", "labels": [], "entities": []}, {"text": "It suggest that, if we want to harvest manual alignment links, it is possible to apply active learning method to minimize the user labelling effort while maximizing the improvement on word alignment qualities.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 184, "end_pos": 198, "type": "TASK", "confidence": 0.6837374567985535}]}, {"text": "Especially, notice that in the lower parts of the curves, with a small number of manual alignment links, we can already improve the alignment quality by a large gap.", "labels": [], "entities": []}, {"text": "This observation can benefit low-resource word alignment tasks.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.716920793056488}]}, {"text": "The previous experiment shows the potential of using the method on manual aligned corpus, here we demonstrate another possible usage of the proposed method that uses heuristics to generate highprecision-low-recall links.", "labels": [], "entities": []}, {"text": "We use LDC ChineseEnglish dictionary as an example.", "labels": [], "entities": [{"text": "LDC ChineseEnglish dictionary", "start_pos": 7, "end_pos": 36, "type": "DATASET", "confidence": 0.9106416503588358}]}, {"text": "The entries with single Chinese character and more than six English words are filtered out.", "labels": [], "entities": []}, {"text": "The heuristic-based aligner yields alignment that has 79.48% precision and 17.36% recall rate on the test set we used in 4.1.", "labels": [], "entities": [{"text": "alignment", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9415953755378723}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9992843270301819}, {"text": "recall rate", "start_pos": 82, "end_pos": 93, "type": "METRIC", "confidence": 0.9900911450386047}]}, {"text": "By applying the links as manual links, we run proposed method on the same ChineseEnglish test data presented in 4.1, and the results of alignment qualities are shown in 5.", "labels": [], "entities": [{"text": "ChineseEnglish test data", "start_pos": 74, "end_pos": 98, "type": "DATASET", "confidence": 0.9867810010910034}]}, {"text": "As we can see, the AER reduced by 1.64 from 37.23 to 35.61 on symmetrized alignment.", "labels": [], "entities": [{"text": "AER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9989951252937317}]}, {"text": "We also experimented with translation tasks with moderate-size corpus.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.9139946401119232}]}, {"text": "We used the corpus LDC2006G05 with 25 million words.", "labels": [], "entities": []}, {"text": "The training scheme is the same as previous experiments, where the filtered LDC dictionary is used.", "labels": [], "entities": []}, {"text": "After word alignment, standard Moses phrase extraction tool () is used to build the translation models and finally Moses) is used to tune and decode.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.7749846875667572}, {"text": "Moses phrase extraction", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.6263062457243601}]}, {"text": "We tune the system on the NIST MT06 test set (1664 sentences), and test on the MT08 (1357 sentences) and the DEV07 5 (1211 sentences) test sets, which are further divided into two sources (newswire and web data: Alignment quality of oracle alignment and force alignment, the rows with \"ORL\" in the second column are oracle alignments, \"F/NE\" and \"F/WE\" represent force alignments with empty links and without empty links correspondingly.", "labels": [], "entities": [{"text": "NIST MT06 test set", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.8757232427597046}, {"text": "MT08", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.967203676700592}, {"text": "DEV07 5 (1211 sentences) test sets", "start_pos": 109, "end_pos": 143, "type": "DATASET", "confidence": 0.8937874361872673}]}, {"text": "For \"F/NE\" and \"F/WE\" we also listed the scores of heuristically symmetrized alignment 4 . (\"Comb\") model trained from GigaWord V1 and V2 corpora is used.", "labels": [], "entities": []}, {"text": "shows the comparison of the performances on BLEU metric ().", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 44, "end_pos": 55, "type": "METRIC", "confidence": 0.9132448732852936}]}, {"text": "As we can observe from the results, the proposed method outperforms the baseline on all test sets except MT03, and has significant 6 improvement on MT02 (+0.72), MT04 (+0.93), and Dev07NW(+0.63).", "labels": [], "entities": [{"text": "MT03", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.9337278008460999}, {"text": "MT02", "start_pos": 148, "end_pos": 152, "type": "DATASET", "confidence": 0.8676651120185852}, {"text": "MT04", "start_pos": 162, "end_pos": 166, "type": "DATASET", "confidence": 0.903934121131897}]}, {"text": "The average improvement across all test sets is 0.35 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9992061257362366}]}, {"text": "As a summary, the purpose of the this experiment is to demonstrate an important characteristic of the proposed method.", "labels": [], "entities": []}, {"text": "Even with imperfect manual alignment links, we can get better alignment by applying our method.", "labels": [], "entities": []}, {"text": "This characteristic opens a possibility to integrate other more sophisticated aligners.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Corpus statistics of the corpora", "labels": [], "entities": []}, {"text": " Table 4: Comparison of the performance of baseline and the alignment generated by new aligner with  dictionary links in BLEU scores", "labels": [], "entities": [{"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9809939861297607}]}, {"text": " Table 3: Alignment quality of oracle alignment  and force alignment, the rows with \"ORL\" in the  second column are oracle alignments, \"F/NE\" and  \"F/WE\" represent force alignments with empty  links and without empty links correspondingly.  For \"F/NE\" and \"F/WE\" we also listed the  scores of heuristically symmetrized alignment 4 .", "labels": [], "entities": [{"text": "ORL", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9840368628501892}]}]}