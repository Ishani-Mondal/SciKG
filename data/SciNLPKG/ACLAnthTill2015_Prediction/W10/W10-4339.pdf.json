{"title": [{"text": "Modeling Spoken Decision Making Dialogue and Optimization of its Dialogue Strategy", "labels": [], "entities": [{"text": "Modeling Spoken Decision Making Dialogue", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.847136402130127}]}], "abstractContent": [{"text": "This paper presents a spoken dialogue framework that helps users in making decisions.", "labels": [], "entities": []}, {"text": "Users often do not have a definite goal or criteria for selecting from a list of alternatives.", "labels": [], "entities": []}, {"text": "Thus the system has to bridge this knowledge gap and also provide the users with an appropriate alternative together with the reason for this recommendation through dialogue.", "labels": [], "entities": []}, {"text": "We present a dialogue state model for such decision making dialogue.", "labels": [], "entities": []}, {"text": "To evaluate this model, we implement atrial sightseeing guidance system and collect dialogue data.", "labels": [], "entities": []}, {"text": "Then, we optimize the dialogue strategy based on the state model through reinforcement learning with a natural policy gradient approach using a user simulator trained on the collected dialogue corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many situations where spoken dialogue interfaces are used, information access by the user is not a goal in itself, but a means for decision making.", "labels": [], "entities": []}, {"text": "For example, in a restaurant retrieval system, the user's goal may not be the extraction of price information but to make a decision on candidate restaurants based on the retrieved information.", "labels": [], "entities": []}, {"text": "This work focuses on how to assist a user who is using the system for his/her decision making, when he/she does not have enough knowledge about the target domain.", "labels": [], "entities": [{"text": "decision making", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.7409752607345581}]}, {"text": "In such a situation, users are often unaware of not only what kind of information the system can provide but also their own preference or factors that they should emphasize.", "labels": [], "entities": []}, {"text": "The system, too, has little knowledge about the user, or where his/her interests lie.", "labels": [], "entities": []}, {"text": "Thus, the system has to bridge such gaps by sensing (potential) preferences of the user and recommend information that the user would be interested in, considering a trade-off with the length of the dialogue.", "labels": [], "entities": []}, {"text": "We propose a model of dialogue state that considers the user's preferences as well as his/her knowledge about the domain changing through a decision making dialogue.", "labels": [], "entities": []}, {"text": "A user simulator is trained on data collected with atrial sightseeing system.", "labels": [], "entities": []}, {"text": "Next, we optimize the dialogue strategy of the system via reinforcement learning (RL) with a natural policy gradient approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each simulated dialogue session, a simulated user (P user , K user , V user ) is sampled.", "labels": [], "entities": []}, {"text": "A preference vector P user of the user is generated so that he/she has four preferences.", "labels": [], "entities": []}, {"text": "As a result, four parameters in P user are \"1\" and the others are \"0\".", "labels": [], "entities": []}, {"text": "This vector is fixed throughout the dialogue episode.", "labels": [], "entities": []}, {"text": "This sampling is conducted based on the rate proportional to the percentage of users who emphasize it for making decisions (.", "labels": [], "entities": []}, {"text": "The user's knowledge K user is also set based on the statistics of the \"percentage of users who stated the determinants before system recommendation\".", "labels": [], "entities": []}, {"text": "For each determinant, we sample a random valuable r that ranges from \"0\" to \"1\", and km is set to \"1\" if r is smaller than the percentage.", "labels": [], "entities": []}, {"text": "All the parameters of local weights V user are initialized to \"0\", assuming that users have no prior knowledge about the candidate spots.", "labels": [], "entities": []}, {"text": "As for system parameters, the estimated user's preference P sys and knowledge K sys are initialized based on the statistics of our trial system (.", "labels": [], "entities": []}, {"text": "We assumed that the system does not misunderstand the user's action.", "labels": [], "entities": []}, {"text": "Users are assumed to continue a dialogue session for 20 turns 2 , and episodes are sampled using the policy \u03c0 at that time and the user simulator of subsection 4.1.", "labels": [], "entities": []}, {"text": "In each turn, the system is rewarded using the reward function of subsection 4.3.", "labels": [], "entities": []}, {"text": "The policy (parameter \u0398) is updated using NAC in every 2,000 dialogues.", "labels": [], "entities": [{"text": "NAC", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.7974530458450317}]}, {"text": "The policy was fixed at about 30,000 dialogue episodes.", "labels": [], "entities": []}, {"text": "We analyzed the learned dialogue policy by examining the value of weight parameter \u0398.", "labels": [], "entities": []}, {"text": "We compared the parameters of the trained policy between actions 3 . The weight of the parameters that represent the early stage of the dialogue was large in Methods 4 and 5.", "labels": [], "entities": []}, {"text": "On the other hand, the weight of the parameters that represent the latter stage of the dialogue was large in Methods 2 and 6.", "labels": [], "entities": []}, {"text": "This suggests that in the trained policy, the system first bridges the knowledge gap between the user, estimates the user's preference, and then, recommends specific information that would be useful to the user.", "labels": [], "entities": []}, {"text": "Next, we compared the trained policy with the following baseline methods.", "labels": [], "entities": []}, {"text": "1. No recommendation (B1) The system only provides the requested information and does not generate any recommendations.", "labels": [], "entities": []}, {"text": "2. Random recommendation (B2) The system randomly chooses a recommendation from six methods.", "labels": [], "entities": [{"text": "Random recommendation (B2)", "start_pos": 3, "end_pos": 29, "type": "METRIC", "confidence": 0.8638771891593933}]}, {"text": "The comparison of the average reward between the baseline methods is listed in Table 1.", "labels": [], "entities": []}, {"text": "Note that the oracle average reward that can be obtained only when the user knows all knowledge about the knowledge base (it requires at least 50 turns) was 1.45.", "labels": [], "entities": []}, {"text": "The reward by the strategy optimized by NAC was significantly better than that of baseline methods (n = 500, p < .01).", "labels": [], "entities": [{"text": "NAC", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.8240681290626526}]}, {"text": "We then compared the proposed method with the case where estimated user's knowledge and preference are represented as discrete binary parameters instead of probability distributions (PDs).", "labels": [], "entities": []}, {"text": "That is, the estimated user's preference pm of determinant m is set to \"1\" when the user requested the determinant, otherwise it is \"0\".", "labels": [], "entities": []}, {"text": "The estimated user's knowledge km is set to the following subsections.", "labels": [], "entities": [{"text": "knowledge km", "start_pos": 21, "end_pos": 33, "type": "METRIC", "confidence": 0.8517370522022247}]}, {"text": "In our trial system, the dialogue length was 16.3 turns with a standard deviation of 7.0 turns.", "labels": [], "entities": []}, {"text": "The parameters can be interpreted as the size of the contribution for selecting the action.", "labels": [], "entities": []}, {"text": "\"1\" when the system lets the user know the determinant, otherwise it is \"0\".", "labels": [], "entities": []}, {"text": "Another dialogue strategy was trained using this dialogue state expression.", "labels": [], "entities": []}, {"text": "This result is shown in.", "labels": [], "entities": []}, {"text": "The proposed method that represents the dialogue state as a probability distribution outperformed) the method using a discrete state expression.", "labels": [], "entities": []}, {"text": "We also compared the proposed method with the case where either one of estimated preference or knowledge was used as a feature for dialogue state in order to carefully investigate the effect of these factors.", "labels": [], "entities": []}, {"text": "In the proposed method, expectation of the probability that the user emphasizes the determinant (P r(k n = 1) \u00d7 P r(p n = 1)) was used as a feature of dialogue state.", "labels": [], "entities": []}, {"text": "We evaluated the performance of the cases where the estimated knowledge P r(k n = 1) or estimated preference P r(p n = 1) was used instead of the expectation of the probability that the user emphasizes the determinant.", "labels": [], "entities": []}, {"text": "We also compared with the case where no preference/knowledge feature was used.", "labels": [], "entities": []}, {"text": "This result is shown in.", "labels": [], "entities": []}, {"text": "We confirmed that significant improvement) was obtained by taking into account the estimated knowledge of the user.", "labels": [], "entities": []}], "tableCaptions": []}