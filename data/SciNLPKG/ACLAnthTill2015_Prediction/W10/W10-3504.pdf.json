{"title": [{"text": "Expanding textual entailment corpora from Wikipedia using co-training", "labels": [], "entities": [{"text": "Expanding textual entailment corpora", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7181329131126404}]}], "abstractContent": [{"text": "In this paper we propose a novel method to automatically extract large textual en-tailment datasets homogeneous to existing ones.", "labels": [], "entities": []}, {"text": "The key idea is the combination of two intuitions: (1) the use of Wikipedia to extract a large set of textual entail-ment pairs; (2) the application of semi-supervised machine learning methods to make the extracted dataset homogeneous to the existing ones.", "labels": [], "entities": []}, {"text": "We report empirical evidence that our method successfully expands existing textual entailment corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite the growing success of the Recognizing Textual Entailment (RTE) challenges ;;, the accuracy of most textual entailment recognition systems are still below 60%.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.7321674923102061}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9994567036628723}, {"text": "textual entailment recognition", "start_pos": 108, "end_pos": 138, "type": "TASK", "confidence": 0.6698187589645386}]}, {"text": "An intuitive way to improve performance is to provide systems with larger annotated datasets.", "labels": [], "entities": []}, {"text": "This is especially true for machine learning systems, where the size of the training corpus is an important factor.", "labels": [], "entities": []}, {"text": "As a consequence, several attempts have been made to train systems using larger datasets obtained by merging RTE corpora of different challenges.", "labels": [], "entities": []}, {"text": "Unfortunately, experimental results show a significant decrease inaccuracy ().", "labels": [], "entities": [{"text": "inaccuracy", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9862602353096008}]}, {"text": "There are two major reasons for this counter-intuitive result: Homogeneity.", "labels": [], "entities": []}, {"text": "As indicated by many studies (e.g.), homogeneity of the training corpus is an important factor for the applicability of supervised machine learning models, since examples with similar properties often imply more effective models.", "labels": [], "entities": []}, {"text": "Unfortunately, the corpora of the four RTE challenges are not homogenous.", "labels": [], "entities": [{"text": "RTE challenges", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.6594026982784271}]}, {"text": "Indeed, they model different properties of the textual entailment phenomenon, as they have been created using slightly (but significantly) different methodologies.", "labels": [], "entities": [{"text": "textual entailment phenomenon", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.7367343604564667}]}, {"text": "For example, part of the RTE-1 dataset ( ) was created using comparable documents, where positive entailments have a lexical overlap higher than negative ones).", "labels": [], "entities": [{"text": "RTE-1 dataset", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.938653826713562}]}, {"text": "Comparable documents have not been used as a source of later RTE corpora, making RTE-1 odd with respect to other datasets.", "labels": [], "entities": []}, {"text": "RTE corpora are relatively small in size (typically 800 pairs).", "labels": [], "entities": []}, {"text": "The increase in size obtained by merging corpora from different challenges is not a viable solution.", "labels": [], "entities": []}, {"text": "Much larger datasets, of one or more order of magnitude, are needed to capture the complex properties characterizing entailment.", "labels": [], "entities": []}, {"text": "A key issue for the future development of RTE is then the creation of datasets fulfilling two properties: (1) large size; (2) homogeneity wrt.", "labels": [], "entities": [{"text": "RTE", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9152852892875671}]}, {"text": "The task of creating large datasets is unfeasible for human annotators.", "labels": [], "entities": []}, {"text": "Collaborative annotation environments such as the Amazon Mechanical Turk 1 can help to annotate pairs of sentences in positive or negative entailment).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk 1", "start_pos": 50, "end_pos": 74, "type": "DATASET", "confidence": 0.9579249620437622}]}, {"text": "Yet, these environments can hardly solve the problem of finding relevant pairs of sentences.", "labels": [], "entities": []}, {"text": "Completely automatic processes of dataset creation have been proposed).", "labels": [], "entities": [{"text": "dataset creation", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7088325619697571}]}, {"text": "Unfortunately, these datasets are not homogeneous wrt.", "labels": [], "entities": []}, {"text": "to the RTE datasets, as they are created using different methodologies.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.8823039531707764}]}, {"text": "In this paper we propose a novel method to automatically extract entailment datasets which are guaranteed to be large and homogeneous to RTE ones.", "labels": [], "entities": []}, {"text": "The key idea is the combination of two factors: (1) the use of Wikipedia as source of a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods, namely cotraining, to make corpora homogeneous to RTE.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we report on previous attempts in automatically creating RTE corpora.", "labels": [], "entities": []}, {"text": "In Section 3 we outline important properties that these corpora should have, and introduce our methodology to extract an RTE corpus from Wikipedia (the WIKI corpus), conforming to these properties.", "labels": [], "entities": [{"text": "WIKI corpus)", "start_pos": 152, "end_pos": 164, "type": "DATASET", "confidence": 0.8789206544558207}]}, {"text": "In Section 4 we describe how co-training techniques can be leveraged to make the WIKI corpus homogeneous to existing RTE corpora.", "labels": [], "entities": [{"text": "WIKI corpus", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.8347302377223969}]}, {"text": "In Section 5 we report empirical evidence that the combination of the WIKI corpus and co-training is successful.", "labels": [], "entities": [{"text": "WIKI corpus", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.9124078750610352}]}, {"text": "Finally, in Section 6 we draw final conclusions and outline future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goals of our experiments are the following: (1) check the quality of the WIKI corpus, i.e. if positive and negative examples well represent the entailment phenomenon; (2) check if WIKI contains examples similar to those of the RTE challenges, i.e. if the corpus is homogeneous to RTE; (3) check if the WIKI corpus improves classification performance when used to expand the RTE datasets using the co-training technique described in Section 4.", "labels": [], "entities": [{"text": "WIKI corpus", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.9180325269699097}, {"text": "WIKI corpus", "start_pos": 306, "end_pos": 317, "type": "DATASET", "confidence": 0.9171955287456512}, {"text": "RTE datasets", "start_pos": 378, "end_pos": 390, "type": "DATASET", "confidence": 0.7653902173042297}]}, {"text": "In order to check the above claims, we need to experiment with both manually labelled and unlabelled corpora.", "labels": [], "entities": []}, {"text": "As unlabelled corpora we adopt: wiki unlabelled: An unlabelled WIKI corpus of about 3,000 examples.", "labels": [], "entities": [{"text": "WIKI corpus", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.8768309950828552}]}, {"text": "The corpus has been built by downloading 40,000 Wikipedia pages dealing with 800 entries about politics, scientific theories, and religion issues.", "labels": [], "entities": []}, {"text": "We extracted original entries and revisions from the XML and wiki code, collecting an overall corpus of 20,000 (S 1 , S 2 ) pairs.", "labels": [], "entities": []}, {"text": "We then randomly selected the final 3,000 pairs.", "labels": [], "entities": []}, {"text": "news: A corpus of 1,600 examples obtained using the methods adopted for the LCC corpus, both for negative and positive examples ().", "labels": [], "entities": [{"text": "LCC corpus", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.9682797789573669}]}, {"text": "We randomly divided the corpus in two parts: 800 training and 800 testing examples.", "labels": [], "entities": []}, {"text": "Each set contains an equal number of 400 positive and negative pairs.", "labels": [], "entities": []}, {"text": "As labelled corpora we use: We use the standard split between training and testing.", "labels": [], "entities": []}, {"text": "wiki: A manually annotated corpus of 2,000 examples from the WIKI corpus.", "labels": [], "entities": [{"text": "WIKI corpus", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.9725262224674225}]}, {"text": "Pairs have been annotated considering the original entry as the H and the revision as T . Noisy pairs containing vandalism or grammatical errors were removed (these accounts for about 19% of the examples).", "labels": [], "entities": []}, {"text": "In all, the annotation produced 945 positive examples (strict entailments and paraphrases) and 669 negative examples (reverse strict entailments and contradictions).", "labels": [], "entities": []}, {"text": "The annotation was carried out by two experienced researchers, each one annotating half of the corpus.", "labels": [], "entities": []}, {"text": "Annotation guidelines follow those used for the RTE challenges.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9478152990341187}, {"text": "RTE challenges", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.8004531860351562}]}, {"text": "The corpus has been randomly split in three equally numerous parts: development, training, and testing.", "labels": [], "entities": []}, {"text": "We kept aside the development to design the features, while we used training and testing for the experiments.", "labels": [], "entities": []}, {"text": "We use the Charniak Parser (Charniak, 2000) for parsing sentences, and SVM-light) extended with the syntactic first-order rule kernels described in ( for creating the FOSR feature space.", "labels": [], "entities": [{"text": "FOSR feature space", "start_pos": 167, "end_pos": 185, "type": "DATASET", "confidence": 0.8418957591056824}]}, {"text": "The first experiment aims at checking the quality of the WIKI corpus, by comparing the performance obtained by a standard RTE system over the corpus in exam with those obtained over any RTE challenge corpus.", "labels": [], "entities": [{"text": "WIKI corpus", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9019679427146912}, {"text": "RTE challenge corpus", "start_pos": 186, "end_pos": 206, "type": "DATASET", "confidence": 0.6169151961803436}]}, {"text": "The hypothesis is that if performance is comparable, then the corpus in exam has the same complexity (and quality) as the RTE challenge corpora.", "labels": [], "entities": [{"text": "complexity", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9535146355628967}, {"text": "RTE challenge corpora", "start_pos": 122, "end_pos": 143, "type": "DATASET", "confidence": 0.6366371214389801}]}, {"text": "We then independently experiment with the wiki and the news corpora with the training-test splits reported in Section 5.1.", "labels": [], "entities": []}, {"text": "As RTE system we adopt an SVM model learnt on the FOSR feature space described in Section 4.3.1.", "labels": [], "entities": [{"text": "FOSR feature space", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.869392991065979}]}, {"text": "The accuracies of the system on the wiki and news corpora are respectively 70.73% and 94.87%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.998467743396759}]}, {"text": "The performance of the system on the wiki corpus are inline with those obtained over the RTE-2 dataset (60.62%).", "labels": [], "entities": [{"text": "RTE-2 dataset", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.9627817571163177}]}, {"text": "This suggests that the WIKI corpus is at least as complex as the RTE corpora (i.e. positive and negatives are not trivially separable).", "labels": [], "entities": [{"text": "WIKI corpus", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.9376524388790131}, {"text": "RTE corpora", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.7850851714611053}]}, {"text": "On the contrary, the news corpus is much easier to separate.", "labels": [], "entities": []}, {"text": "Pilot experiments show that increasing the size of the news corpus, accuracy reaches nearly 100%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.999495267868042}]}, {"text": "This indicates that positive and negative examples in the news corpus are extremely different.", "labels": [], "entities": []}, {"text": "Indeed, as mentioned in Section 3.1, news is not consistent -i.e. the extraction methods for the positives and the negatives are so different that the examples can be easily recognized using evidence not representative of the entailment phenomenon (e.g. for negative examples, the lexical overlap is extremely low wrt. positives).", "labels": [], "entities": []}, {"text": "inline with the RTE challenge annotation efforts.", "labels": [], "entities": [{"text": "RTE challenge annotation", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.6883203784624735}]}, {"text": "Ina second experiment we aim at checking if WIKI is homogeneous to the RTE challenge corpora -i.e. if it contains (T, H) pairs similar to those of the RTE corpora.", "labels": [], "entities": [{"text": "WIKI", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.6220444440841675}]}, {"text": "If this holds, we would expect the performance of the RTE system to improve (or at least not decrease) when expanding a given RTE challenge corpus with WIKI.", "labels": [], "entities": [{"text": "WIKI", "start_pos": 152, "end_pos": 156, "type": "DATASET", "confidence": 0.944341778755188}]}, {"text": "already showed in their experiment that it is extremely difficult to obtain better performance by simply expanding an RTE challenge training corpus with corpora of other challenges, since different corpora are usually not homogeneous.", "labels": [], "entities": [{"text": "RTE challenge training", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.7297183871269226}]}], "tableCaptions": [{"text": " Table 1: Accuracy of different training corpora over RTE-2", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9959742426872253}, {"text": "RTE-2", "start_pos": 54, "end_pos": 59, "type": "TASK", "confidence": 0.48124662041664124}]}]}