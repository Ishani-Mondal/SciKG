{"title": [{"text": "Annotating Large Email Datasets for Named Entity Recognition with Mechanical Turk", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7707523107528687}]}], "abstractContent": [{"text": "Amazon's Mechanical Turk service has been successfully applied to many natural language processing tasks.", "labels": [], "entities": []}, {"text": "However, the task of named entity recognition presents unique challenges.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.6256517072518667}]}, {"text": "Ina large annotation task involving over 20,000 emails, we demonstrate that a compet\u00ad itive bonus system and inter\u00adannotator agree\u00ad ment can be used to improve the quality of named entity annotations from Mechanical Turk.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 205, "end_pos": 220, "type": "DATASET", "confidence": 0.8059684336185455}]}, {"text": "We also build several statistical named entity recognition models trained with these annotations, which compare favorably to sim\u00ad ilar models trained on expert annotations.", "labels": [], "entities": [{"text": "statistical named entity recognition", "start_pos": 22, "end_pos": 58, "type": "TASK", "confidence": 0.6643321663141251}]}], "introductionContent": [{"text": "It is well known that the performance of many machine learning systems is heavily determined by the size and quality of the data used as input to the training algorithms.", "labels": [], "entities": []}, {"text": "Additionally, for cer\u00ad tain applications in natural language processing (NLP), it has been noted that the particular al\u00ad gorithms or feature sets used tend to become ir\u00ad relevant as the size of the corpus increases (.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.7473752101262411}]}, {"text": "It is therefore not sur\u00ad prising that obtaining large annotated datasets is an issue of great practical importance for the working researcher.", "labels": [], "entities": []}, {"text": "Traditionally, annotated training data have been provided by experts in the field or the researchers themselves, often at great costs in terms of time and money.", "labels": [], "entities": []}, {"text": "Re\u00ad cently, however, attempts have been made to leverage non\u00adexpert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs).", "labels": [], "entities": []}, {"text": "The initial results seem promising, and anew avenue for enhancing existing sources of annotated data appears to have been opened.", "labels": [], "entities": []}, {"text": "Named entity recognition (NER) is one of the many fields of NLP that rely on machine learn\u00ad ing methods, and therefore large training cor\u00ad pora.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8133877217769623}]}, {"text": "Indeed, it is afield where more is almost always better, as indicated by the traditional use of named entity gazetteers (often culled from ex\u00ad ternal sources) to simulate data that would have been inferred from a larger training set).", "labels": [], "entities": []}, {"text": "Therefore, it appears to be afield that could profit from the enormous bargain\u00adprice workforce available through MTurk.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.910142719745636}]}, {"text": "It is not immediately obvious, though, that MTurk is well\u00adsuited for the task of NER annota\u00ad tion.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.783839225769043}, {"text": "NER annota\u00ad tion", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.8980960249900818}]}, {"text": "Commonly, MTurk has been used for the classification task) or for straightforward data entry.", "labels": [], "entities": [{"text": "classification task", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.9222392737865448}, {"text": "data entry", "start_pos": 82, "end_pos": 92, "type": "TASK", "confidence": 0.6650536358356476}]}, {"text": "However, NER does not fit well into either of these formats.", "labels": [], "entities": []}, {"text": "As poin\u00ad ted out by, NER can bethought of as a composition of two subtasks: 1) determin\u00ad ing the start and end boundaries of a textual en\u00ad tity, and 2) determining the label of the identified span.", "labels": [], "entities": []}, {"text": "The second task is the well\u00adunderstood classification task, but the first task presents subtler problems.", "labels": [], "entities": [{"text": "well\u00adunderstood classification task", "start_pos": 23, "end_pos": 58, "type": "TASK", "confidence": 0.6966897050539652}]}, {"text": "One is that MTurk's form\u00ad based user interface is inappropriate for the task of identifying textual spans.", "labels": [], "entities": []}, {"text": "Another problem is that MTurk's fixed\u00adfee payment system encour\u00ad ages low recall on the part of the annotators, since they receive the same pay no matter how many entities they identify.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.7569636702537537}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.998838484287262}]}, {"text": "This paper addresses both of these problems by describing a custom user interface and com\u00ad petitive payment system that together create a fluid user experience while encouraging high\u00ad quality annotations.", "labels": [], "entities": []}, {"text": "Further, we demonstrate that MTurk successfully scales to the task of an\u00ad notating a very large set of documents (over 20,000), with each document annotated by mul\u00ad tiple workers.", "labels": [], "entities": [{"text": "\u00ad notating", "start_pos": 72, "end_pos": 82, "type": "TASK", "confidence": 0.8749965131282806}]}, {"text": "We also present a system for resolving inter\u00adannotator conflicts to create the final training corpus, and determine the ideal agreement threshold to maximize precision and recall of a statistical named entity recognition model.", "labels": [], "entities": [{"text": "precision", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.9992152452468872}, {"text": "recall", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.998445451259613}, {"text": "statistical named entity recognition", "start_pos": 184, "end_pos": 220, "type": "TASK", "confidence": 0.679034061729908}]}, {"text": "Finally, we demonstrate that a model trained on our corpus is on par with one trained from expert annotations, when applied to a labeled test set.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran our Mechanical Turk tasks over a period of about three months, from August 2008 to November 2008.", "labels": [], "entities": [{"text": "Mechanical Turk tasks", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.5628846287727356}]}, {"text": "We typically processed 500\u00ad 1,500 documents per day.", "labels": [], "entities": []}, {"text": "In the end, the work\u00ad ers annotated 20,609 unique emails which totaled 7.9 megabytes, including both subject and body.", "labels": [], "entities": []}, {"text": "All in all, we were pleasantly surprised by the speed at which each HIT series was completed.", "labels": [], "entities": [{"text": "HIT series", "start_pos": 68, "end_pos": 78, "type": "TASK", "confidence": 0.7495169043540955}]}, {"text": "Out of 39 total HIT series, the average comple\u00ad tion time (i.e. from when the HITs were first pos\u00ad ted to MTurk.com until the last HIT was com\u00ad pleted) was 3.13 hours, with an average of 715.34 emails per HIT series.", "labels": [], "entities": [{"text": "MTurk.com", "start_pos": 106, "end_pos": 115, "type": "DATASET", "confidence": 0.9227330684661865}]}, {"text": "The fastest com\u00ad pletion time per number of emails was 1.9 hours fora 1,000\u00ademail task, and the slowest was 5.13 hours fora 100\u00ademail task.", "labels": [], "entities": []}, {"text": "We noticed, that, paradoxically, larger HIT series were often com\u00ad pleted more quickly -most likely because Amazon promotes the larger tasks to the front page.", "labels": [], "entities": []}, {"text": "To evaluate the quality of the worker annota\u00ad tions, one would ideally like to have at least a subset annotated by an expert, and then compare the expert's judgments with the Mechanical Turk workers'.", "labels": [], "entities": []}, {"text": "However, in our case we lacked expert annotations for any of the annotated emails.", "labels": [], "entities": []}, {"text": "Thus, we devised an alternative method to evalu\u00ad ate the annotation quality, using the NER system built into the open\u00adsource MinorThird toolkit.", "labels": [], "entities": [{"text": "open\u00adsource MinorThird toolkit", "start_pos": 113, "end_pos": 143, "type": "DATASET", "confidence": 0.6849267880121866}]}, {"text": "MinorThird is a popular machine learning and natural language processing library that has pre\u00ad viously been applied to the problem of NER with some success).", "labels": [], "entities": [{"text": "MinorThird", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9273110628128052}]}, {"text": "For our pur\u00ad poses, we wanted to minimize the irregularity in\u00ad troduced by deviating from the core features and algorithms available in MinorThird, and there\u00ad fore did not apply any feature selection or feature engineering in our experiments.", "labels": [], "entities": [{"text": "irregularity", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.954132616519928}, {"text": "MinorThird", "start_pos": 136, "end_pos": 146, "type": "DATASET", "confidence": 0.9146512746810913}]}, {"text": "We chose to use MinorThird's default \"CRFLearner,\" which is a module that learns feature weights using the IITB CRF library 8 and then applies them to a condi\u00ad tional Markov model\u00adbased extractor.", "labels": [], "entities": [{"text": "MinorThird", "start_pos": 16, "end_pos": 26, "type": "DATASET", "confidence": 0.8926094770431519}, {"text": "IITB CRF library 8", "start_pos": 107, "end_pos": 125, "type": "DATASET", "confidence": 0.9429619610309601}]}, {"text": "All of the parameters were set to their default value, includ\u00ad ing the built\u00adin \"TokenFE\" feature extractor, which extracts features for the lowercase value of each token and its capitalization pattern.", "labels": [], "entities": []}, {"text": "The version of MinorThird used was 13.7.10.8.", "labels": [], "entities": [{"text": "MinorThird", "start_pos": 15, "end_pos": 25, "type": "DATASET", "confidence": 0.9068943858146667}]}, {"text": "In order to convert the Mechanical Turk an\u00ad notations to a format that could be input as train\u00ad ing data to the NER system, we had to resolve the conflicting annotations of the multiple work\u00ad ers into a unified set of labeled documents.", "labels": [], "entities": [{"text": "NER system", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.9101017117500305}]}, {"text": "Sim\u00ad ilarly to the bonus system, we achieved this using a simple voting scheme.", "labels": [], "entities": []}, {"text": "In contrast to the bonus system, though, we experimented with multiple inter\u00adannotator agreement thresholds between 1 and 4.", "labels": [], "entities": []}, {"text": "For the PERSON corpora this meant a relat\u00ad ively stricter threshold than for the LOCATION or  Org.", "labels": [], "entities": [{"text": "LOCATION", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.837706446647644}]}], "tableCaptions": [{"text": " Table 1: Statistics by corpus and entity type (omitting rejected work).", "labels": [], "entities": []}, {"text": " Table 2: Cross\u00advalidation test results.", "labels": [], "entities": []}, {"text": " Table 3: Results from the second test.", "labels": [], "entities": []}, {"text": " Table 4: Results from the third test.", "labels": [], "entities": []}, {"text": " Table 5: Results from the fourth test.", "labels": [], "entities": []}]}