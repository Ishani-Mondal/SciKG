{"title": [{"text": "The RWTH Aachen Machine Translation System for WMT 2010", "labels": [], "entities": [{"text": "RWTH Aachen Machine Translation", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8082394301891327}, {"text": "WMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8635075688362122}]}], "abstractContent": [{"text": "In this paper we describe the statistical machine translation system of the RWTH Aachen University developed for the translation task of the Fifth Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.68981072306633}, {"text": "RWTH Aachen University", "start_pos": 76, "end_pos": 98, "type": "DATASET", "confidence": 0.8587182958920797}, {"text": "translation task of the Fifth Workshop on Statistical Machine Translation", "start_pos": 117, "end_pos": 190, "type": "TASK", "confidence": 0.6359230220317841}]}, {"text": "State-of-the-art phrase-based and hierarchical statistical MT systems are augmented with appropriate morpho-syntactic enhancements , as well as alternative phrase training methods and extended lexicon models.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9232975840568542}]}, {"text": "For some tasks, a system combination of the best systems was used to generate a final hypothesis.", "labels": [], "entities": []}, {"text": "We participated in the constrained condition of German-English and French-English in each translation direction.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the statistical MT system used for our participation in the WMT 2010 shared translation task.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9750604629516602}, {"text": "WMT 2010 shared translation task", "start_pos": 81, "end_pos": 113, "type": "TASK", "confidence": 0.7618865728378296}]}, {"text": "We used it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and which have proven to be successful in other evaluations.", "labels": [], "entities": [{"text": "RWTH", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.9672633409500122}]}, {"text": "For all tasks we used standard alignment and training tools as well as our in-house phrasebased and hierarchical statistical MT decoders.", "labels": [], "entities": []}, {"text": "When German was involved, morpho-syntactic preprocessing was applied.", "labels": [], "entities": []}, {"text": "An alternative phrasetraining method and additional models were tested and investigated with respect to their effect for the different language pairs.", "labels": [], "entities": []}, {"text": "For two of the language pairs we could improve performance by system combination.", "labels": [], "entities": []}, {"text": "An overview of the systems and models will follow in Section 2 and 3, which describe the baseline architecture, followed by descriptions of the additional system components.", "labels": [], "entities": []}, {"text": "Morpho-syntactic analysis and other preprocessing issues are covered by Section 4.", "labels": [], "entities": [{"text": "Morpho-syntactic analysis", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7105924338102341}]}, {"text": "Finally, translation results for the different languages and system variants are presented in Section 5.", "labels": [], "entities": [{"text": "translation", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.9584337472915649}]}], "datasetContent": [{"text": "For all translation directions, we used the provided parallel corpora (Europarl, news) to train the translation models and the monolingual corpora to train BLEU Dev Test phrase-based baseline 19.9 19.2 phrase-based (+POS+mero+giga) 21.0 20.3 hierarchical baseline 20.2 19.6 hierarchical (+giga) 20.5 20.1 system combination 21.4 20.4: Results for the German\u2192English task.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9512256383895874}, {"text": "BLEU", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9891902208328247}]}, {"text": "We improved the FrenchEnglish systems by enriching the data with parts of the large addional data, extracted with the method described in Section 4.1.", "labels": [], "entities": [{"text": "FrenchEnglish", "start_pos": 16, "end_pos": 29, "type": "DATASET", "confidence": 0.974970817565918}]}, {"text": "Depending on the system this gave an improvement of 0.2-0.7% BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9994685053825378}]}, {"text": "We also made use of the large giga-news as well as the LDC Gigaword corpora for the French and English language models.", "labels": [], "entities": [{"text": "LDC Gigaword corpora", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.9258158802986145}]}, {"text": "All systems were optimized for BLEU score on the development data, newstest2008.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9708468914031982}, {"text": "newstest2008", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.7137770652770996}]}, {"text": "The newstest2009 data is used as a blind test set.", "labels": [], "entities": [{"text": "newstest2009 data", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9661593437194824}]}, {"text": "In the following, we will give the BLEU scores for all language tasks of the baseline system and the best setup for both, the phrase-based and the hierarchical system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9995452761650085}]}, {"text": "We will use the following notations to indicate the several methods we used: (+POS) POS-based verb reordering (+mero) maximum entropy reordering (+giga) including giga-news and LDC Gigaword in LM (fa) trained by forced alignment (shallow) allow only shallow rules We applied system combination of up to 6 systems with several setups.", "labels": [], "entities": []}, {"text": "The submitted systems are marked in tables 4-7.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores on Test and phrase table sizes with and without forced alignment (FA). For  German\u2192English and English\u2192French phrase table interpolation was applied.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992501139640808}, {"text": "forced alignment (FA)", "start_pos": 70, "end_pos": 91, "type": "METRIC", "confidence": 0.695463091135025}]}, {"text": " Table 2: Overview on data for unsupervised train- ing.", "labels": [], "entities": [{"text": "train- ing", "start_pos": 44, "end_pos": 54, "type": "TASK", "confidence": 0.6210649311542511}]}, {"text": " Table 3: Results for unsupervised training method.", "labels": [], "entities": []}, {"text": " Table 5: Results for the English\u2192German task.", "labels": [], "entities": []}, {"text": " Table 6: Results for the French\u2192English task.", "labels": [], "entities": []}, {"text": " Table 7: Results for the English\u2192French task.", "labels": [], "entities": []}]}