{"title": [{"text": "The CUED HiFST System for the WMT10 Translation Shared Task", "labels": [], "entities": [{"text": "CUED HiFST", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7119304835796356}, {"text": "WMT10 Translation Shared Task", "start_pos": 30, "end_pos": 59, "type": "DATASET", "confidence": 0.6425408571958542}]}], "abstractContent": [{"text": "This paper describes the Cambridge University Engineering Department submission to the Fifth Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "Cambridge University Engineering Department", "start_pos": 25, "end_pos": 68, "type": "DATASET", "confidence": 0.8750016093254089}, {"text": "Statistical Machine Translation", "start_pos": 105, "end_pos": 136, "type": "TASK", "confidence": 0.8570144375165304}]}, {"text": "We report results for the French-English and Spanish-English shared translation tasks in both directions.", "labels": [], "entities": [{"text": "Spanish-English shared translation", "start_pos": 45, "end_pos": 79, "type": "TASK", "confidence": 0.5975458522637686}]}, {"text": "The CUED system is based on HiFST, a hierarchical phrase-based decoder implemented using weighted finite-state transducers.", "labels": [], "entities": []}, {"text": "In the French-English task, we investigate the use of context-dependent alignment models.", "labels": [], "entities": []}, {"text": "We also show that lattice minimum Bayes-risk decoding is an effective framework for multi-source translation, leading to large gains in BLEU score.", "labels": [], "entities": [{"text": "multi-source translation", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.6655347645282745}, {"text": "BLEU score", "start_pos": 136, "end_pos": 146, "type": "METRIC", "confidence": 0.9807436466217041}]}], "introductionContent": [{"text": "This paper describes the Cambridge University Engineering Department (CUED) system submission to the ACL 2010 Fifth Workshop on Statistical Machine Translation (WMT10).", "labels": [], "entities": [{"text": "Cambridge University Engineering Department (CUED) system submission to the ACL 2010 Fifth Workshop on", "start_pos": 25, "end_pos": 127, "type": "DATASET", "confidence": 0.8797726444900036}, {"text": "Statistical Machine Translation (WMT10)", "start_pos": 128, "end_pos": 167, "type": "TASK", "confidence": 0.8543706635634104}]}, {"text": "Our translation system is HiFST (), a hierarchical phrase-based decoder that generates translation lattices directly.", "labels": [], "entities": [{"text": "HiFST", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.8865663409233093}]}, {"text": "Decoding is guided by a CYK parser based on asynchronous contextfree grammar induced from automatic word alignments.", "labels": [], "entities": []}, {"text": "The decoder is implemented with Weighted Finite State Transducers (WFSTs) using standard operations available in the OpenFst libraries.", "labels": [], "entities": []}, {"text": "The use of WFSTs allows fast and efficient exploration of avast translation search space, avoiding search errors in decoding.", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.8514786958694458}]}, {"text": "It also allows better integration with other steps in our translation pipeline such as 5-gram language model (LM) rescoring and lattice minimum Bayes-risk (LMBR) decoding.", "labels": [], "entities": []}, {"text": "We participated in the French-English and Spanish-English translation shared tasks in each translation direction.", "labels": [], "entities": []}, {"text": "This paper describes the development of these systems.", "labels": [], "entities": []}, {"text": "Additionally, we report multi-source translation experiments that lead to very large gains in BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9792540073394775}]}, {"text": "The paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes each step in the development of our system for submission, from pre-processing to postprocessing.", "labels": [], "entities": [{"text": "submission", "start_pos": 67, "end_pos": 77, "type": "TASK", "confidence": 0.9704458713531494}]}, {"text": "Section 3 presents and discusses results and Section 4 describes an additional experiment on multi-source translation.", "labels": [], "entities": [{"text": "multi-source translation", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.7200896441936493}]}], "datasetContent": [{"text": "Multi-source translation) is possible whenever multiple translations of the source language input sentence are available.", "labels": [], "entities": [{"text": "Multi-source translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7162944376468658}]}, {"text": "The motivation for multisource translation is that some of the ambiguity that must be resolved in translating between one pair of languages may not be present in a different pair.", "labels": [], "entities": [{"text": "multisource translation", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.8143897354602814}]}, {"text": "In the following experiments, multiple LMBR is applied for the first time to the task of multi-source translation.", "labels": [], "entities": [{"text": "multi-source translation", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.7034082114696503}]}, {"text": "Separate second-pass 5-gram rescored lattices E FR and E ES are generated for each test set sentence using the French-to-English and Spanish-toEnglish HiFST translation systems.", "labels": [], "entities": [{"text": "FR", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.536412239074707}]}, {"text": "The MBR hypothesis space is formed as the union of these lattices.", "labels": [], "entities": [{"text": "MBR hypothesis space", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8762209018071493}]}, {"text": "Ina similar manner to MBR decoding over multiple k-best lists in de , the path posterior probability of each n-gram u required for linearised LMBR is computed as a linear interpolation of the posterior probabilities according to each individual lattice so that p(u|E) = \u03bb FR p(u|E FR ) + \u03bb ES p(u|E ES ), where p(u|E) is the sum of the posterior probabilities of all paths containing the n-gram u.", "labels": [], "entities": [{"text": "MBR decoding", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.733609139919281}, {"text": "FR", "start_pos": 272, "end_pos": 274, "type": "METRIC", "confidence": 0.9691652655601501}]}, {"text": "The interpolation weights \u03bb FR + \u03bb ES = 1 are optimised for BLEU score on the development set newstest2008.", "labels": [], "entities": [{"text": "FR + \u03bb ES", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.8036836981773376}, {"text": "BLEU score", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9747299253940582}, {"text": "development set newstest2008", "start_pos": 78, "end_pos": 106, "type": "DATASET", "confidence": 0.7295652826627096}]}, {"text": "The results of single-system and multi-source LMBR decoding are shown in.", "labels": [], "entities": []}, {"text": "The optimised interpolation weights were \u03bb FR = 0.55 and \u03bb ES = 0.45.", "labels": [], "entities": [{"text": "FR", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9971845746040344}, {"text": "ES", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9476204514503479}]}, {"text": "Single-system LMBR gives relatively small gains on these test sets.", "labels": [], "entities": []}, {"text": "Much larger gains are obtained through multi-source MBR combination.", "labels": [], "entities": []}, {"text": "Compared to the best of the single-system 5-gram rescored lattices, the BLEU score improves by +2.0 for newstest2008, +1.9 for newstest2009, and +1.9 for newstest2010.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9781941771507263}]}, {"text": "For scoring with respect to a single reference, these are very large gains indeed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parallel data sets used for French-to- English experiments.", "labels": [], "entities": []}, {"text": " Table 6: Translation Results for the French-English (FR-EN) language pair, shown in single-reference  lowercase IBM BLEU. Bold results correspond to submitted systems.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9085333943367004}, {"text": "FR-EN) language pair", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.8383283615112305}, {"text": "IBM", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.4808211624622345}, {"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.6803539991378784}]}, {"text": " Table 7: Translation Results for the Spanish-English (SP-EN) language pair, shown in lowercase IBM  BLEU. Bold results correspond to submitted systems.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9210293292999268}, {"text": "IBM", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.6970734000205994}, {"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.5733402967453003}]}, {"text": " Table 8: Lowercase IBM BLEU for single-system LMBR and multiple LMBR multi-source translation  of French (FR) and Spanish (ES) into English (EN).", "labels": [], "entities": [{"text": "IBM", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.7574242353439331}, {"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.6887981295585632}]}]}