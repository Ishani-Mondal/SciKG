{"title": [{"text": "Maximum Entropy Translation Model in Dependency-Based MT Framework", "labels": [], "entities": [{"text": "Maximum Entropy Translation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6452980637550354}]}], "abstractContent": [{"text": "Maximum Entropy Principle has been used successfully in various NLP tasks.", "labels": [], "entities": []}, {"text": "In this paper we propose a forward translation model consisting of a set of maximum entropy classifiers: a separate clas-sifier is trained for each (sufficiently frequent) source-side lemma.", "labels": [], "entities": []}, {"text": "In this way the estimates of translation probabilities can be sensitive to a large number of features derived from the source sentence (in-cluding non-local features, features making use of sentence syntactic structure, etc.).", "labels": [], "entities": []}, {"text": "When integrated into English-to-Czech dependency-based translation scenario implemented in the TectoMT framework , the new translation model significantly outperforms the baseline model (MLE) in terms of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 204, "end_pos": 208, "type": "METRIC", "confidence": 0.9966707825660706}]}, {"text": "The performance is further boosted in a configuration inspired by Hidden Tree Markov Models which combines the maximum entropy translation model with the target-language dependency tree model.", "labels": [], "entities": []}], "introductionContent": [{"text": "The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy.", "labels": [], "entities": []}, {"text": "Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging, parsing, and named entity recognition (.", "labels": [], "entities": [{"text": "tagging", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.9716193675994873}, {"text": "parsing", "start_pos": 119, "end_pos": 126, "type": "TASK", "confidence": 0.7339043617248535}, {"text": "named entity recognition", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.6242063244183859}]}, {"text": "Maximum entropy models have the following form where f i is a feature function, \u03bb i is its weight, and Z(x) is the normalizing factor In statistical machine translation (SMT), translation model (TM) p(t|s) is the probability that the string t from the target language is the translation of the string s from the source language.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 137, "end_pos": 174, "type": "TASK", "confidence": 0.7915533185005188}]}, {"text": "Typical approach in SMT is to use backward translation model p(s|t) according to Bayes' rule and noisychannel model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9950605034828186}]}, {"text": "However, in this paper we deal only with the forward (direct) model.", "labels": [], "entities": []}, {"text": "The idea of using maximum entropy for constructing forward translation models is not new.", "labels": [], "entities": []}, {"text": "It naturally allows to make use of various features potentially important for correct choice of targetlanguage expressions.", "labels": [], "entities": []}, {"text": "Let us adopt a motivating example of such a feature from) (which contains the first usage of maxent translation model we are aware of): \"If house appears within the next three words (e.g., the phrases in the house and in the red house), then dans might be a more likely translation.\"", "labels": [], "entities": []}, {"text": "Incorporating non-local features extracted from the source sentence into the standard noisychannel model in which only the backward translation model is available, is not possible.", "labels": [], "entities": []}, {"text": "This drawback of the noisy-channel approach is typically compensated by using large target-language n-gram models, which can -in a result -play a role similar to that of a more elaborate (more context sensitive) forward translation model.", "labels": [], "entities": []}, {"text": "However, we expect that it would be more beneficial to exploit both the parallel data and the monolingual data in a more balance fashion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side.", "labels": [], "entities": []}, {"text": "1 A backward translation model is used only for pruning training data in this paper.", "labels": [], "entities": []}, {"text": "A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in and, in which another successful applications of maxent translation models are shown.", "labels": [], "entities": [{"text": "maxent translation", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.5933095216751099}]}, {"text": "Log-linear translation models (instead of MLE) with rich feature sets are used also in and; the idea can be traced back to).", "labels": [], "entities": []}, {"text": "What makes our approach different from the previously published works is that 1.", "labels": [], "entities": []}, {"text": "we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency)) as the transfer layer, 2.", "labels": [], "entities": [{"text": "Prague Dependency", "start_pos": 147, "end_pos": 164, "type": "DATASET", "confidence": 0.977634847164154}]}, {"text": "we combine the maximum entropy translation model with target-language dependency tree model and use tree-modified Viterbi search for finding the optimal lemmas labeling of the target-tree nodes.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we give a brief overview of the translation framework TectoMT in which the experiments are implemented.", "labels": [], "entities": []}, {"text": "In Section 3 we describe how our translation models are constructed.", "labels": [], "entities": []}, {"text": "Section 4 summarizes the experimental results, and Section 5 contains a summary.", "labels": [], "entities": []}], "datasetContent": [{"text": "When included into the above described translation scenario, the MaxEnt TM outperforms the baseline TM, be it used together with or without TreeLM.", "labels": [], "entities": []}, {"text": "The results are summarized in Table 1.", "labels": [], "entities": []}, {"text": "The improvement is statistically significant according to paired bootstrap resampling test).", "labels": [], "entities": []}, {"text": "In the configuration without TreeLM the improvement is greater (1.33 BLEU) than with TreeLM (0.81 BLEU), which confirms our hypothesis that MaxEnt TM captures some of the contextual dependencies resolved otherwise by language models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.995772659778595}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9945359230041504}]}], "tableCaptions": [{"text": " Table 1: BLEU and NIST evaluation of four con- figurations of our MT system; the WMT 2010 test  set was used.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9952217936515808}, {"text": "NIST", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.8103315234184265}, {"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9625518918037415}, {"text": "WMT 2010 test  set", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.9543605893850327}]}]}