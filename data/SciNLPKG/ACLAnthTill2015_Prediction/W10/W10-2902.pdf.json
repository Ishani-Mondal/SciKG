{"title": [{"text": "Viterbi Training Improves Unsupervised Dependency Parsing", "labels": [], "entities": [{"text": "Viterbi Training Improves Unsupervised Dependency Parsing", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.6692186941703161}]}], "abstractContent": [{"text": "We show that Viterbi (or \"hard\") EM is well-suited to unsupervised grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7127871513366699}]}, {"text": "It is more accurate than standard inside-outside re-estimation (classic EM), significantly faster, and simpler.", "labels": [], "entities": []}, {"text": "Our experiments with Klein and Manning's Dependency Model with Valence (DMV) attain state-of-the-art performance-44.8% accuracy on Section 23 (all sentences) of the Wall Street Journal corpus-without clever initialization; with a good initial-izer, Viterbi training improves to 47.9%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9948607087135315}, {"text": "Wall Street Journal corpus-without", "start_pos": 165, "end_pos": 199, "type": "DATASET", "confidence": 0.8865300416946411}]}, {"text": "This generalizes to the Brown corpus, our held-out set, where accuracy reaches 50.8%-a 7.5% gain over previous best results.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.982170432806015}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9994751811027527}]}, {"text": "We find that classic EM learns better from short sentences but cannot cope with longer ones, where Viterbi thrives.", "labels": [], "entities": [{"text": "EM", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.94190514087677}]}, {"text": "However, we explain that both algorithms optimize the wrong objectives and prove that there are fundamental disconnects between the likelihoods of sentences, best parses, and true parses, beyond the well-established discrepancies between likelihood , accuracy and extrinsic performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 251, "end_pos": 259, "type": "METRIC", "confidence": 0.9897012710571289}]}], "introductionContent": [{"text": "Unsupervised learning is hard, often involving difficult objective functions.", "labels": [], "entities": []}, {"text": "A typical approach is to attempt maximizing the likelihood of unlabeled data, in accordance with a probabilistic model.", "labels": [], "entities": []}, {"text": "Sadly, such functions are riddled with local optima, inter alia), since their number of peaks grows exponentially with instances of hidden variables.", "labels": [], "entities": []}, {"text": "Furthermore, a higher likelihood does not always translate into superior task-specific accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9933237433433533}]}, {"text": "Both complications are real, but we will discuss perhaps more significant shortcomings.", "labels": [], "entities": []}, {"text": "We prove that learning can be error-prone even in cases when likelihood is an appropriate measure of extrinsic performance and where global optimization is feasible.", "labels": [], "entities": []}, {"text": "This is because a key challenge in unsupervised learning is that the desired likelihood is unknown.", "labels": [], "entities": []}, {"text": "Its absence renders tasks like structure discovery inherently underconstrained.", "labels": [], "entities": [{"text": "structure discovery", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7854607403278351}]}, {"text": "Search-based algorithms adopt surrogate metrics, gambling on convergence to the \"right\" regularities in data.", "labels": [], "entities": []}, {"text": "Their wrong objectives create cases in which both efficiency and performance improve when expensive exact learning techniques are replaced by cheap approximations.", "labels": [], "entities": []}, {"text": "We propose using Viterbi training), instead of inside-outside reestimation, to induce hierarchical syntactic structure from natural language text.", "labels": [], "entities": []}, {"text": "Our experiments with Dependency Model with Valence (DMV), a popular state-of-the-art model, beat previous benchmark accuracies by 3.8% (on Section 23 of WSJ) and 7.5% (on parsed Brown).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.8345517516136169}, {"text": "Section 23 of WSJ)", "start_pos": 139, "end_pos": 157, "type": "DATASET", "confidence": 0.8418159246444702}]}, {"text": "Since objective functions used in unsupervised grammar induction are provably wrong, advantages of exact inference may not apply.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7343075275421143}]}, {"text": "It makes sense to try the Viterbi approximation -it is also wrong, only simpler and cheaper than classic EM.", "labels": [], "entities": []}, {"text": "As it turns out, Viterbi EM is not only faster but also more accurate, consistent with hypotheses of and.", "labels": [], "entities": []}, {"text": "We begin by reviewing the model, standard data sets and metrics, and our experimental results.", "labels": [], "entities": []}, {"text": "After relating our contributions to prior work, we delve into proofs by construction, using the DMV.", "labels": [], "entities": [{"text": "DMV", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.938444197177887}]}, {"text": "Figure 2: A dependency structure fora short sentence and its probability, as factored by the DMV, after summing out PORDER ().", "labels": [], "entities": [{"text": "PORDER", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9948063492774963}]}], "datasetContent": [{"text": "The DMV is traditionally trained and tested on customized subsets of Penn English Treebank's Wall Street Journal portion.", "labels": [], "entities": [{"text": "Penn English Treebank's Wall Street Journal portion", "start_pos": 69, "end_pos": 120, "type": "DATASET", "confidence": 0.9589382261037827}]}, {"text": "Following, we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty sub-trees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic \"head-percolation\" rules to convert the rest, as is standard practice.", "labels": [], "entities": []}, {"text": "We experiment with WSJk (sentences with at most k tokens), for 1 \u2264 k \u2264 45, and Section 23 of WSJ \u221e (all sentence lengths).", "labels": [], "entities": [{"text": "WSJk", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.787461519241333}, {"text": "Section 23 of WSJ \u221e", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.6371606469154358}]}, {"text": "We also evaluate on Brown100, similarly derived from the parsed portion of the Brown corpus (, as our held-out set.", "labels": [], "entities": [{"text": "Brown100", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.9243491291999817}, {"text": "Brown corpus", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.8818831741809845}]}, {"text": "shows these corpora's sentence and token counts.", "labels": [], "entities": []}, {"text": "Proposed parse trees are judged on accuracy: a directed score is simply the overall fraction of correctly guessed dependencies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.999245285987854}]}, {"text": "Let S be a set of sentences, with |s| the number of terminals (to- kens) for each s \u2208 S.", "labels": [], "entities": []}, {"text": "Denote by T (s) the set of all dependency parse trees of s, and let ti (s) stand for the parent of token i, For a given model of grammar, parameterized by \u03b8, let\u02c6tlet\u02c6 let\u02c6t \u03b8 (s) \u2208 T (s) be a (not necessarily unique) likeliest (also known as Viterbi) parse of s:  Following, we trained the DMV on data sets WSJ{1, . .", "labels": [], "entities": [{"text": "WSJ", "start_pos": 308, "end_pos": 311, "type": "DATASET", "confidence": 0.9021852016448975}]}, {"text": ", 45} using three initialization strategies: (i) the uninformed uniform prior; (ii) a linguistically-biased initializer, AdHoc * ; 2 and (iii) an oracle -the supervised MLE solution.", "labels": [], "entities": []}, {"text": "Standard training is without smoothing, iterating each run until successive changes in overall per-token cross-entropy drop below 2 \u221220 bits.", "labels": [], "entities": []}, {"text": "We re-trained all models using Viterbi EM instead of inside-outside re-estimation, explored Laplace (add-one) smoothing during training, and experimented with hybrid initialization strategies.", "labels": [], "entities": []}, {"text": "The DMV has no parameters to capture syntactic relationships beyond local trees, e.g., agreement.", "labels": [], "entities": []}, {"text": "suggest that classic EM breaks down as sentences get longer precisely because the model makes unwarranted independence assumptions.", "labels": [], "entities": [{"text": "EM", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9537535905838013}]}, {"text": "They hypothesize that the DMV reserves too much probability mass for what should be unlikely productions.", "labels": [], "entities": []}, {"text": "Since EM faithfully allocates such re-distributions across the possible parse trees, once sentences grow sufficiently long, this process begins to deplete what began as likelier structures.", "labels": [], "entities": []}, {"text": "But medium lengths avoid a flood of exponentially-confusing longer sentences (and the sparseness of unrepresentative shorter ones).", "labels": [], "entities": []}, {"text": "Our experiments corroborate this hypothesis.", "labels": [], "entities": []}, {"text": "First of all, Viterbi manages to hang onto supervised solutions much better than classic EM.", "labels": [], "entities": []}, {"text": "Second, Viterbi does not universally degrade with more (complex) training sets, except with a biased initializer.", "labels": [], "entities": []}, {"text": "And third, Viterbi learns poorly from small data sets of short sentences (WSJk, k < 5).", "labels": [], "entities": [{"text": "WSJk", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.6186736822128296}]}, {"text": "Viterbi maybe better suited to unsupervised grammar induction compared with classic EM, but neither is sufficient, by itself.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.6997980177402496}]}, {"text": "Both algorithms abandon good solutions and make no guarantees with respect to extrinsic performance.", "labels": [], "entities": []}, {"text": "Unfortunately, these two approaches share a deep flaw.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies on Section 23 of WSJ{10, 20, \u221e } and Brown100 for three recent state-of-the-art  systems, our initializer, and smoothed Viterbi-trained runs that employ different initialization strategies.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9885590672492981}, {"text": "WSJ", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.8306307196617126}, {"text": "Brown100", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.6801905632019043}]}]}