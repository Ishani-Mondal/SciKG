{"title": [{"text": "Exploring Deep Belief Network for Chinese Relation Extraction", "labels": [], "entities": [{"text": "Chinese Relation Extraction", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7889533837636312}]}], "abstractContent": [{"text": "Relation extraction is a fundamental task in information extraction that identifies the semantic relationships between two entities in the text.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.947354644536972}, {"text": "information extraction", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7704405188560486}]}, {"text": "In this paper, a novel model based on Deep Belief Network (DBN) is first presented to detect and classify the relations among Chinese entities.", "labels": [], "entities": []}, {"text": "The experiments conducted on the Automatic Content Extraction (ACE) 2004 dataset demonstrate that the proposed approach is effective in handling high dimensional feature space including character N-grams, entity types and the position information.", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE) 2004 dataset", "start_pos": 33, "end_pos": 80, "type": "DATASET", "confidence": 0.7494548782706261}]}, {"text": "It outperforms the state-of-the-art learning models such as SVM or BP neutral network.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information Extraction (IE) is to automatically pullout the structured information required by the users from a large volume of plain text.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8303189814090729}]}, {"text": "It normally includes three sequential tasks, i.e., entity extraction, relation extraction and event extraction.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7942597270011902}, {"text": "relation extraction", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8093122243881226}, {"text": "event extraction", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.703119546175003}]}, {"text": "In this paper, we limit our focus on relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.916559100151062}]}, {"text": "In early time, pattern-based approaches were the main focus of most research studies in relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.9649347960948944}]}, {"text": "Although pattern-based approaches achieved reasonably good results, they have some obvious flaws.", "labels": [], "entities": []}, {"text": "It requires expensive handcraft work to assemble patterns and not all relations can be identified by a set of reliable patterns (Willy).", "labels": [], "entities": []}, {"text": "Also, once the interest of task is transferred to a different domain or a different language, patterns have to be revised or even rewritten.", "labels": [], "entities": []}, {"text": "That is to say, the discovered patterns are heavily dependent on the task in a specific domain or on a particular corpus.", "labels": [], "entities": []}, {"text": "Naturally, avast amount of work was spent on feature-based machine learning approaches in later years.", "labels": [], "entities": []}, {"text": "In this camp, relation extraction is typically cast as a classification problem, where the most important issue is to train a model to scale and measure the similarity of features reflecting relation instances.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.932453840970993}]}, {"text": "The entity semantic information expressing relation was often formulated as the lexical and syntactic features, which are identical to a certain linear vector in high dimensions.", "labels": [], "entities": []}, {"text": "Many learning models are capable of self-training and classifying these vectors according to similarity, such as Support Vector Machine (SVM) and Neural Network (NN).", "labels": [], "entities": []}, {"text": "Recently, kernel-based approaches have been developing rapidly.", "labels": [], "entities": []}, {"text": "These approaches involved kernels of structure representations, like parse tree or dependency tree, in similarity calculation.", "labels": [], "entities": [{"text": "similarity calculation", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.7379387319087982}]}, {"text": "In fact, feature-based approaches can be viewed as the special and simplified kinds of kernel-based approaches.", "labels": [], "entities": []}, {"text": "They used dot-product as the kernel function and did not range over the intricate structure information ).", "labels": [], "entities": []}, {"text": "Relation extraction in Chinese received quite limited attention as compared to English and other western languages.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9611576199531555}]}, {"text": "The main reason is the unique characteristic of Chinese, such as more flexible grammar, lack of boundary information and morphological variations etc).", "labels": [], "entities": []}, {"text": "Especially, the existing Chinese syntactic analysis tools at current stage are not yet reliable to capture the valuable structured information.", "labels": [], "entities": []}, {"text": "It is urgent to develop approaches that are in particular suitable for Chinese relation extraction.", "labels": [], "entities": [{"text": "Chinese relation extraction", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.7504287560780843}]}, {"text": "In this paper, we explore the use of Deep Belief Network (DBN), anew feature-based machine learning model for Chinese relation extraction.", "labels": [], "entities": [{"text": "Chinese relation extraction", "start_pos": 110, "end_pos": 137, "type": "TASK", "confidence": 0.7610471248626709}]}, {"text": "It is a neural network model developed under the deep learning architecture that is claimed by to be able to automatically learn a deep hierarchy of features with increasing levels of abstraction for the complex problems like natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 226, "end_pos": 259, "type": "TASK", "confidence": 0.7414612770080566}]}, {"text": "It avoids assembling patterns that express the semantic relation information and meanwhile it succeeds to produce accurate model that is not confined to the parsing results.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured in the following manner.", "labels": [], "entities": []}, {"text": "Section 2 reviews the previous work on relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.9368502497673035}]}, {"text": "Section 3 presents task definition, briefly introduces the DBN model and the feature construction.", "labels": [], "entities": [{"text": "task definition", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7276474833488464}]}, {"text": "Section 4 provides the experimental results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments are conducted on the ACE 2004 Chinese relation extraction dataset, which consists of 221 documents selected from broadcast news and newswire reports.", "labels": [], "entities": [{"text": "ACE 2004 Chinese relation extraction dataset", "start_pos": 37, "end_pos": 81, "type": "DATASET", "confidence": 0.8775474727153778}]}, {"text": "There are 2620 relation instances and 11800 pairs of entities have no relationship in the dataset.", "labels": [], "entities": []}, {"text": "The size of the feature space is 3017.", "labels": [], "entities": []}, {"text": "We examine the proposed DBN model using 4-fold cross-validation.", "labels": [], "entities": []}, {"text": "The performance is measured by precision, recall, and Fmeasure.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9997745156288147}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9997401833534241}, {"text": "Fmeasure", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9984782338142395}]}, {"text": "2*Precision*Recall -measure= Precision+Recall F In the following experiments, we plan to test the effectiveness of the DBN model in three ways: Detection Only: For each relation candidate, we only recognize whether there is a certain relationship between the two entities, no matter what type of relation they hold.", "labels": [], "entities": [{"text": "Recall -measure", "start_pos": 12, "end_pos": 27, "type": "METRIC", "confidence": 0.887484093507131}, {"text": "Precision+Recall F", "start_pos": 29, "end_pos": 47, "type": "METRIC", "confidence": 0.724047563970089}]}, {"text": "We first evaluate relation detection, where only two output classes are concerned, i.e. NULL (which means no relation recognized) and RELATION.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.8812370002269745}, {"text": "NULL", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9811362624168396}, {"text": "RELATION", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9971689581871033}]}, {"text": "The parameters used in DBN, SVM and NN (BP only) are tuned experimentally and the results with the best parameter settings are presented in.", "labels": [], "entities": []}, {"text": "In each of our experiments, we test many parameters of SVM and chose the best set of that to show below.", "labels": [], "entities": []}, {"text": "Regarding the structure of DBN, we experiment with different combinations of unit numbers in the RBM layers.", "labels": [], "entities": []}, {"text": "Finally we choose DBN with three RBM layers and one BP layer.", "labels": [], "entities": []}, {"text": "And the numbers of units in each RBM layer are 2400, 1800 and 1200 respectively, which is the best size of each layer in our experiment.", "labels": [], "entities": []}, {"text": "Our empirical results showed that the numbers of units in adjoining layers should not decrease the dimension of feature vector too much when casting the vector transformation.", "labels": [], "entities": []}, {"text": "NN has the same structure as DBN.", "labels": [], "entities": [{"text": "NN", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8404607772827148}, {"text": "DBN", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.9539963006973267}]}, {"text": "As for SVM, we choose the linear kernel with the penalty parameter C=0.3, which is the best penalty coefficient, and set the other parameters as default after comparing different kernels and parameter values., with their best parameter settings, DBN performs much better than both SVM and NN (BP only) in terms of F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 314, "end_pos": 323, "type": "METRIC", "confidence": 0.9777740240097046}]}, {"text": "It tells that DBN is quite good in this binary classification task.", "labels": [], "entities": [{"text": "DBN", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.6378480195999146}, {"text": "binary classification task", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7506180206934611}]}, {"text": "Since RBM is a fast approach to approximate global optimum of networks, its advantage over NN (BP only) is clearly demonstrated in their results.", "labels": [], "entities": [{"text": "RBM", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.8933605551719666}]}, {"text": "In the next experiment, we go one step further.", "labels": [], "entities": []}, {"text": "If a relation is detected, we classified it into one of the 5 pre-defined relation types.", "labels": [], "entities": []}, {"text": "For relation type classification, DBN and NN (BP only) have the same structures as they are in the first experiment.", "labels": [], "entities": [{"text": "relation type classification", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.8967995643615723}]}, {"text": "We adopt SVM linear kernel again and set C to 0.09 and other parameters as default.", "labels": [], "entities": []}, {"text": "The overall performance of detection and classification of three models are illustrated in  In the third experiment, we unify relation detection and relation type classification into one classification task.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7651667892932892}, {"text": "relation type classification", "start_pos": 149, "end_pos": 177, "type": "TASK", "confidence": 0.6702568133672079}]}, {"text": "All the candidates are directly classified into one of the 6 classes, including 5 relation types and a NULL class.", "labels": [], "entities": []}, {"text": "Parameter settings of the three models in this experiment are identical to those in the second experiment, except that C in SVM is set to 0.1.", "labels": [], "entities": [{"text": "Parameter", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9876958131790161}]}, {"text": "Comparing the results of the second and the third experiments, SVM perform better (although not quite significantly) when detection and classification are in sequence than in combination.", "labels": [], "entities": [{"text": "SVM", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.955708920955658}]}, {"text": "This finding is consistent with our previous work (to be added later).", "labels": [], "entities": []}, {"text": "It can possibly be that preceding detection helps to deal with the severe unbalance problem, i.e. there are much more relation candidates that don't hold pre-defined relations.", "labels": [], "entities": [{"text": "preceding detection", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7387177050113678}]}, {"text": "However, DBN obtaining the opposite result cause by that the amount of examples we have is not sufficient for DBN to self-train itself well for type classification.", "labels": [], "entities": [{"text": "DBN", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.9418028593063354}, {"text": "DBN", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.9291823506355286}, {"text": "type classification", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.9224881231784821}]}, {"text": "We will further exam this issue in our feature work.", "labels": [], "entities": []}, {"text": "Next, we compare the performance of DBN with different structures by changing the number of RBM layers.", "labels": [], "entities": []}, {"text": "All the candidates are directly classified into 6 types in this experiment.", "labels": [], "entities": []}, {"text": "The results provided in show that the performance can be improved when more RBM layers are incorporated.", "labels": [], "entities": []}, {"text": "Multiple RBM layers enhance representation power.", "labels": [], "entities": []}, {"text": "Since it was reported by that three RBM layer is enough to detect the complex features and more RBM layer are of less help, we do not try to go beyond the three layers in this experiment.", "labels": [], "entities": []}, {"text": "Note that the improvement is more obvious from two layers to three layers than from one layer to two layers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The internal postion structure features  between two named entities", "labels": [], "entities": []}, {"text": " Table 2. Performances of DBN, SVM and NN  models for detection only", "labels": [], "entities": []}, {"text": " Table 3. Performances of DBN and other  classification models for detection and  classification in sequence", "labels": [], "entities": [{"text": "detection and  classification", "start_pos": 67, "end_pos": 96, "type": "TASK", "confidence": 0.632083902756373}]}, {"text": " Table 4. Performances of DBN, SVM and NN  models for detection and classification in  combination", "labels": [], "entities": [{"text": "detection and classification", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.6713602443536123}]}, {"text": " Table 5. Performance RBM with different  layers", "labels": [], "entities": [{"text": "RBM", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.7675374150276184}]}, {"text": " Table 8. \"Near\" and \"Social\" are two  symmetric relation types. Ideally, they should  have better results. But due to quite small  number of training examples, you can see that  they are actually the types with the worst F- measure.", "labels": [], "entities": [{"text": "F- measure", "start_pos": 222, "end_pos": 232, "type": "METRIC", "confidence": 0.9900008837381998}]}, {"text": " Table 6. Performance of DBN for each  relation type", "labels": [], "entities": []}, {"text": " Table 7. Distribution of the identified relations", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.94655841588974}]}]}