{"title": [{"text": "Textual Entailment Recognition using Word Overlap, Mutual Information and Subpath Set", "labels": [], "entities": [{"text": "Textual Entailment Recognition", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7811308900515238}]}], "abstractContent": [{"text": "When two texts have an inclusion relation, the relationship between them is called entailment.", "labels": [], "entities": []}, {"text": "The task of mechanically distinguishing such a relation is called recognising textual entailment (RTE), which is basically a kind of semantic analysis.", "labels": [], "entities": [{"text": "recognising textual entailment (RTE)", "start_pos": 66, "end_pos": 102, "type": "TASK", "confidence": 0.7119150161743164}, {"text": "semantic analysis", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.7231190502643585}]}, {"text": "A variety of methods have been proposed for RTE.", "labels": [], "entities": [{"text": "RTE", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9884940981864929}]}, {"text": "However, when the previous methods were combined, the performances were not clear.", "labels": [], "entities": []}, {"text": "So, we utilized each method as a feature of machine learning, in order to combine methods.", "labels": [], "entities": []}, {"text": "We have dealt with the binary classification problem of two texts exhibiting inclusion, and proposed a method that uses machine learning to judge whether the two texts present the same content.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.7523523271083832}]}, {"text": "We have built a program capable to perform entailment judgment on the basis of word overlap, i.e. the matching rate of the words in the two texts, mutual information, and similarity of the respective syntax trees (Subpath Set).", "labels": [], "entities": [{"text": "entailment judgment", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.9091486632823944}]}, {"text": "Word overlap was calclated by utilizing BiLingual Evaluation Understudy (BLEU).", "labels": [], "entities": [{"text": "Word overlap", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6594123840332031}, {"text": "BiLingual Evaluation Understudy (BLEU)", "start_pos": 40, "end_pos": 78, "type": "METRIC", "confidence": 0.7478953003883362}]}, {"text": "Mutual information is based on co-occurrence frequency, and the Subpath Set was determined by using the Japanise WordNet.", "labels": [], "entities": [{"text": "Japanise WordNet", "start_pos": 104, "end_pos": 120, "type": "DATASET", "confidence": 0.8179650008678436}]}, {"text": "A Confidence-Weighted Score of 68.6% was obtained in the mutual information experiment on RTE.", "labels": [], "entities": [{"text": "Confidence-Weighted Score", "start_pos": 2, "end_pos": 27, "type": "METRIC", "confidence": 0.9806549251079559}, {"text": "RTE", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.6916688680648804}]}, {"text": "Mutual information and the use of three methods of SVM were shown to be effective.", "labels": [], "entities": [{"text": "SVM", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.935904860496521}]}], "introductionContent": [{"text": "This paper can help solve textual entailment problems.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7829780876636505}]}, {"text": "Researchers of natural language processing have recently become interested in the automatic recognition of textual entailment (RTE), which is the task of mechanically distinguishing an inclusion relation.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6772316892941793}, {"text": "automatic recognition of textual entailment (RTE)", "start_pos": 82, "end_pos": 131, "type": "TASK", "confidence": 0.8219202160835266}]}, {"text": "Text implication recognition is the task of taking a text (T) and a hypothesis (H), and judging whether one (the text) can be inferred from the other (hypothesis).", "labels": [], "entities": [{"text": "Text implication recognition", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8600164850552877}]}, {"text": "Here below is an example task.", "labels": [], "entities": []}, {"text": "In case of entailment, we call the relation to be 'true').", "labels": [], "entities": []}, {"text": "Example 1: Textual entailment recognition.", "labels": [], "entities": [{"text": "Textual entailment recognition", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.8863435387611389}]}, {"text": "T: Google files for its long-awaited IPO.", "labels": [], "entities": [{"text": "Google files", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.9681513905525208}]}, {"text": "H: Google goes public.", "labels": [], "entities": []}, {"text": "For such a task, large applications such as question answering, information extraction, summarization and machine translation are involved.", "labels": [], "entities": [{"text": "question answering", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9290494620800018}, {"text": "information extraction", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.8755579888820648}, {"text": "summarization", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.9899008870124817}, {"text": "machine translation", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.8098818063735962}]}, {"text": "A large-scale evaluation workshop has been conducted to stimulate research on recognition of entailment ( ).", "labels": [], "entities": [{"text": "recognition of entailment", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.9076266487439474}]}, {"text": "These authors divided the RTE methods into six methods.", "labels": [], "entities": [{"text": "RTE", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9741991758346558}]}, {"text": "We focused on 3 methods of them.", "labels": [], "entities": []}, {"text": "P\u00e9rez and Alfonseca's method () used Word Overlap.", "labels": [], "entities": []}, {"text": "This method is assumed to have taken place when words or sentences of the text and the hypothesis are similar, hence the relation should be true.", "labels": [], "entities": []}, {"text": "P\u00e9rez and Alfonseca used the BLEU algorithm to calculate the entailment relationship.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9974337220191956}]}, {"text": "Glickman et al's. method was considered as using statistical lexical relations.", "labels": [], "entities": []}, {"text": "These authors assumed that the possibility of entailment were high when the co-occurrence frequency of the word in the source and the target were high.", "labels": [], "entities": []}, {"text": "While this maybe correct, we believe nevertheless that it problematic not to consider the co-occurrence of the hypothesis words.", "labels": [], "entities": []}, {"text": "This being so, we proposed to use mutual information.", "labels": [], "entities": []}, {"text": "Finally, Herrera et al's. method is based on Syntactic matching.", "labels": [], "entities": [{"text": "Syntactic matching", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8778121471405029}]}, {"text": "They calculated the degree of similarity of the syntax tree.", "labels": [], "entities": [{"text": "similarity", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9195266366004944}]}, {"text": "We combined these three methods using machine learning techniques.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the textual entailment evaluation data of for the problem of RTE.", "labels": [], "entities": [{"text": "RTE", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9635935425758362}]}, {"text": "This evaluation data is generally available to the public at the Kyoto University 1 . The evaluation data comprises the inference factor, subclassification, entailment judgment, text and hypothesis.", "labels": [], "entities": [{"text": "Kyoto University 1", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.9842693010965983}]}, {"text": "The inference factor is divided into five categories according to the definition provided by Odani et al.: inclusion, lexicon (indeclinable word), lexicon (declinable word), syntax and inference.", "labels": [], "entities": []}, {"text": "They define the classification viewpoint of each inference factor as follows: Example 2: Classification criteria of inference factors \u2022 Inclusion: The text almost includes the hypothesis.", "labels": [], "entities": [{"text": "Inclusion", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9430794715881348}]}, {"text": "\u2022 Lexicon (Indeclinable Word): Information of the hypothesis is given by the meaning or the behaviour of the noun in the text.", "labels": [], "entities": []}, {"text": "\u2022 Lexicon (Declinable Word): Information of the hypothesis is given by the meaning or the behaviour of the declinable word in the text.", "labels": [], "entities": []}, {"text": "\u2022 Syntax: The text and the hypothesis have a relation of syntactic change.", "labels": [], "entities": []}, {"text": "\u2022Inference: Logical form.", "labels": [], "entities": []}, {"text": "They divided the data into 166 subclasses, according to each inference factor.", "labels": [], "entities": []}, {"text": "The entailment judgment is a reliable answer in the text and the hypothesis.", "labels": [], "entities": []}, {"text": "It is a difficult problem to entailment judgment for the criteria answer.", "labels": [], "entities": []}, {"text": "Therefore, when they reported on the RTE workshop, they assumed the following classification criteria: Example 3: Classification criteria of entailment determination.", "labels": [], "entities": [{"text": "RTE workshop", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.908245712518692}, {"text": "entailment determination", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.7427665889263153}]}, {"text": "\u2022\u25ce(T alw ): When the text is true, the hypothesis is always true.", "labels": [], "entities": [{"text": "T alw )", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.8239508668581644}]}, {"text": "\u2022\u25cb(T alm ): When the text is true, the hypothesis is almost true.", "labels": [], "entities": [{"text": "T alm )", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.9447693427403768}]}, {"text": "\u2022\u25b3(F may ): When the text is true, the hypothesis maybe true.", "labels": [], "entities": [{"text": "F may )", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.9759971300760905}]}, {"text": "\u2022\u00d7(F alw ): When the text is true, the hypothesis is false.", "labels": [], "entities": [{"text": "F alw )", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.9287134011586508}]}, {"text": "In terms of the text and the hypothesis, when we observed the evaluation data, the evaluation data accounted for almost every sentence in both the texts and the hypotheses, and also the hypotheses were shorter than the texts.", "labels": [], "entities": []}, {"text": "There is a bias in the number of problems evaluated by the inference factor and by the subclassification.", "labels": [], "entities": []}, {"text": "Lexus is a luxury car.", "labels": [], "entities": [{"text": "Lexus", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9573344588279724}]}, {"text": "The textual entailment evaluation data of Odani et al., described in Section 3, was used in the experiment.", "labels": [], "entities": []}, {"text": "The entailment judgment of four values is manually given to the textual entailment evaluation data.", "labels": [], "entities": []}, {"text": "In our experiment we considered 'T alw ' and 'T alm ' to be 'true' and ' F may ' and ' F alw ' as ' false ' . The evaluation method used was a ConfidenceWeight Score (CWS, also known as Average Precision), proposed by Dagan et al..", "labels": [], "entities": [{"text": "Average Precision)", "start_pos": 186, "end_pos": 204, "type": "METRIC", "confidence": 0.9270084301630656}]}, {"text": "As for the closed test, the threshold value with the maximum CWS was used.", "labels": [], "entities": [{"text": "CWS", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9828683137893677}]}, {"text": "When the Entailment judgment annotated in evaluation data matches with the Entailment judgment of our method, the answer is true.", "labels": [], "entities": []}, {"text": "The threshold of the Closed test was set beforehand (0\u2266th\u22661).", "labels": [], "entities": [{"text": "Closed", "start_pos": 21, "end_pos": 27, "type": "TASK", "confidence": 0.670280933380127}]}, {"text": "When it was above the threshold, it was judged \"true\".", "labels": [], "entities": []}, {"text": "When it was higher than the threshold, it was judged \"false\".", "labels": [], "entities": []}, {"text": "SVM was used to calculate the value of three methods (word overlap, mutual information and subpath set) as the features for learning data, was experimented.", "labels": [], "entities": [{"text": "SVM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7283350825309753}]}, {"text": "Open test was experimented 10-fold crossvalidations.", "labels": [], "entities": []}, {"text": "9 of the data divided into 10 were utilized as the learning data.", "labels": [], "entities": []}, {"text": "Remaining 1 was used as an evaluation data.", "labels": [], "entities": []}, {"text": "It looked for the threshold that CWS becomes the maximum from among the learning data.", "labels": [], "entities": [{"text": "CWS", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.7365325093269348}]}, {"text": "It experimented on the threshold for which it searched by the learning data to the evaluation data.", "labels": [], "entities": []}, {"text": "It repeats until all data that divides this becomes an evaluation data, averaged out.", "labels": [], "entities": []}, {"text": "(Or we experimented Leave-oneout cross validation.)", "labels": [], "entities": [{"text": "Leave-oneout cross validation", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.4984154502550761}]}, {"text": "Using the SVM, experiments were conducted on the numerical results of Sections 4.1 to 4.3 as the features.", "labels": [], "entities": [{"text": "SVM", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.9549088478088379}]}, {"text": "The textual entailment evaluation data numbered 2472: 'T alw ': 924, 'T alm ': 662, 'F may ': 262 and 'F alw ': 624, and there were 4356 words.", "labels": [], "entities": []}, {"text": "The total number of words was 43421.", "labels": [], "entities": []}, {"text": "show the results of the experiment, which focused respectively on the closed and open tests,.", "labels": [], "entities": []}, {"text": "When the 'true' textual entailment evaluation data 'T alw ' only and 'T alw and T alm ' was used, mutual information achieved the best performance.", "labels": [], "entities": []}, {"text": "When the true data 'T alm ' only was used, SVM achieved the best performance.", "labels": [], "entities": [{"text": "T alm '", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9108802477518717}]}], "tableCaptions": [{"text": " Table 2: Results of the RTE experiments", "labels": [], "entities": [{"text": "RTE", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.7106876969337463}]}]}