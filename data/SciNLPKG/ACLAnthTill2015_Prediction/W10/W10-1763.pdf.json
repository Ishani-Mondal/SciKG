{"title": [{"text": "Decision Trees for Lexical Smoothing in Statistical Machine Translation", "labels": [], "entities": [{"text": "Lexical Smoothing", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.8903149962425232}, {"text": "Statistical Machine Translation", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.7327874700228373}]}], "abstractContent": [{"text": "We present a method for incorporating arbitrary context-informed word attributes into statistical machine translation by clustering attribute-qualied source words, and smoothing their word translation probabilities using binary decision trees.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.6421724955240885}]}, {"text": "We describe two ways in which the decision trees are used in machine translation: by using the attribute-qualied source word clusters directly, or by using attribute-dependent lexical translation probabilities that are obtained from the trees, as a lexical smoothing feature in the de-coder model.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.728640079498291}]}, {"text": "We present experiments using Arabic-to-English newswire data, and using Arabic diacritics and part-of-speech as source word attributes, and show that the proposed method improves on a state-of-the-art translation system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern statistical machine translation (SMT) models, such as phrase-based SMT or hierarchical SMT, implicitly incorporate source language context.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 7, "end_pos": 44, "type": "TASK", "confidence": 0.7824241171280543}, {"text": "phrase-based SMT", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.5819385349750519}, {"text": "hierarchical SMT", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.5514801740646362}]}, {"text": "It has been shown, however, that such systems can still benet from the explicit addition of lexical, syntactic or other kinds of context-informed word features.", "labels": [], "entities": []}, {"text": "But the benet obtained from the addition of attribute information is in general countered by the increase in the model complexity, which in turn results in a sparser translation model when estimated from the same corpus of data.", "labels": [], "entities": []}, {"text": "The increase in model sparsity usually results in a deterioration of translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.9602594971656799}]}, {"text": "In this paper, we present a method for using arbitrary types of source-side context-informed word attributes, using binary decision trees to deal with the sparsity side-eect.", "labels": [], "entities": []}, {"text": "The decision trees cluster attribute-dependent source words by reducing the entropy of the lexical translation probabilities.", "labels": [], "entities": []}, {"text": "We also present another method where, instead of clustering the attribute-dependent source words, the decision trees are used to interpolate attributedependent lexical translation probability models, and use those probabilities to compute a feature in the decoder log-linear model.", "labels": [], "entities": []}, {"text": "The experiments we present in this paper were conducted on the translation of Arabicto-English newswire data using a hierarchical system based on, and using Arabic diacritics (see section 2.3) and part-ofspeech (POS) as source word attributes.", "labels": [], "entities": [{"text": "translation of Arabicto-English newswire", "start_pos": 63, "end_pos": 103, "type": "TASK", "confidence": 0.8544902950525284}]}, {"text": "Previous work that attempts to use Arabic diacritics in machine translation runs against the sparsity problem, and appears to lose most of the useful information contained in the diacritics when using partial diacritization (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7043879628181458}]}, {"text": "Using the methods proposed in this paper, we manage to obtain consistent improvements from diacritics against a strong baseline.", "labels": [], "entities": []}, {"text": "The methods we propose, though, are not restrictive to Arabic-to-English translation.", "labels": [], "entities": [{"text": "Arabic-to-English translation", "start_pos": 55, "end_pos": 84, "type": "TASK", "confidence": 0.6862784326076508}]}, {"text": "The same techniques can also be used with other language pairs and arbitrary word attribute types.", "labels": [], "entities": []}, {"text": "The attributes we use in the described experiments are local; but long distance features can also be used.", "labels": [], "entities": []}, {"text": "In the next section, we review relevant previous work in three areas: Lexical smoothing and lexical disambiguation techniques in machine translation; using decision trees in natural language processing, and especially machine translation; and Arabic diacritics.", "labels": [], "entities": [{"text": "Lexical smoothing", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.8869255185127258}, {"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7421045303344727}, {"text": "machine translation", "start_pos": 218, "end_pos": 237, "type": "TASK", "confidence": 0.7638960182666779}]}, {"text": "We present a brief exposition of Arabic orthogra-phy, and refer to previous work on automatic diacritization of Arabic text.", "labels": [], "entities": []}, {"text": "Section 3 describes the procedure for constructing the decision trees, and the two methods for using them in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7367801368236542}]}, {"text": "In section 4 we describe the experimental setup and present experimental results.", "labels": [], "entities": []}, {"text": "Finally, section 5 concludes the paper and discusses future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned before, the experiments we report on use a string-to-dependency-tree hierarchical translation system based on the model described in: Normalized likelihood of the test set alignments without decision trees, then with decision trees using diacritics and part-of-speech respectively.", "labels": [], "entities": []}, {"text": "backward context-free lexical smoothing are used as decoder features in all the experiments.", "labels": [], "entities": []}, {"text": "Other features such as rule probabilities and dependency tree language model are also used.", "labels": [], "entities": []}, {"text": "We use GIZA++ (Och and Ney, 2003) for word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7420263588428497}]}, {"text": "The decoder model parameters are tuned using Minimum Error Rate training to maximize the IBM BLEU score ().", "labels": [], "entities": [{"text": "Minimum Error Rate training", "start_pos": 45, "end_pos": 72, "type": "METRIC", "confidence": 0.832825168967247}, {"text": "IBM", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.8663987517356873}, {"text": "BLEU score", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.921557605266571}]}, {"text": "For training the alignments, we use 27M words from the Sakhr Arabic-English Parallel Corpus (SSUSAC27).", "labels": [], "entities": [{"text": "Sakhr Arabic-English Parallel Corpus (SSUSAC27)", "start_pos": 55, "end_pos": 102, "type": "DATASET", "confidence": 0.8415789348738534}]}, {"text": "The language model uses 7B words from the English Gigaword and from data collected from the web.", "labels": [], "entities": [{"text": "English Gigaword", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.9151352941989899}]}, {"text": "A 3-gram language model is used during decoding.", "labels": [], "entities": []}, {"text": "The decoder produces an N-best list that is reranked using a 5-gram language model.", "labels": [], "entities": []}, {"text": "We tune and test on two separate data sets consisting of documents from the following collections: the newswire portion of NIST MT04, MT05, MT06, and MT08 evaluation sets, the GALE Phase 1 (P1) and Phase 2 (P2) evaluation sets, and the GALE P2 and P3 development sets.", "labels": [], "entities": [{"text": "NIST", "start_pos": 123, "end_pos": 127, "type": "DATASET", "confidence": 0.9471138119697571}, {"text": "MT04", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.5074676275253296}, {"text": "MT05", "start_pos": 134, "end_pos": 138, "type": "DATASET", "confidence": 0.7901508212089539}, {"text": "MT06", "start_pos": 140, "end_pos": 144, "type": "DATASET", "confidence": 0.7412624955177307}, {"text": "MT08 evaluation sets", "start_pos": 150, "end_pos": 170, "type": "DATASET", "confidence": 0.8335586984952291}, {"text": "GALE P2 and P3 development sets", "start_pos": 236, "end_pos": 267, "type": "DATASET", "confidence": 0.7702400286992391}]}, {"text": "The tuning set contains 1994 sentences and the test set contains 3149 sentences.", "labels": [], "entities": []}, {"text": "The average length of sentences is 36 words.", "labels": [], "entities": []}, {"text": "Most of the documents in the two data sets have 4 reference translations, but some have only one.", "labels": [], "entities": []}, {"text": "The average number of reference translations per sentence is 3.94 for the tuning set and 3.67 for the test set.", "labels": [], "entities": []}, {"text": "In the next section, we report on measurements of the likelihood of test data, and describe the translation experiments in detail.", "labels": [], "entities": [{"text": "translation", "start_pos": 96, "end_pos": 107, "type": "TASK", "confidence": 0.9597370028495789}]}], "tableCaptions": []}