{"title": [{"text": "Transliteration Mining with Phonetic Conflation and Iterative Training", "labels": [], "entities": [{"text": "Transliteration Mining", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9159242510795593}]}], "abstractContent": [{"text": "This paper presents transliteration mining on the ACL 2010 NEWS workshop shared translitera-tion mining task data.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.9346749186515808}, {"text": "ACL 2010 NEWS workshop shared translitera-tion mining task data", "start_pos": 50, "end_pos": 113, "type": "DATASET", "confidence": 0.8430435591273837}]}, {"text": "Transliteration mining was done using a generative transliteration model applied on the source language and whose output was constrained on the words in the target language.", "labels": [], "entities": [{"text": "Transliteration mining", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9346503913402557}]}, {"text": "A total of 30 runs were performed on 5 language pairs, with 6 runs for each language pair.", "labels": [], "entities": []}, {"text": "In the presence of limited resources, the runs explored the use of phonetic conflation and iterative training of the transliteration model to improve recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.9842082858085632}]}, {"text": "Using letter conflation improved recall by as much as 48%, with improvements in recall dwarfing drops in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9993215799331665}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9989690780639648}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9983982443809509}]}, {"text": "Using iterative training improved recall, but often at the cost of significant drops in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9988229870796204}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9988389611244202}]}, {"text": "The best runs typically used both letter conflation and iterative learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transliteration Mining (TM) is the process of finding transliterated word pairs in parallel or comparable corpora.", "labels": [], "entities": [{"text": "Transliteration Mining (TM)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8967686176300049}]}, {"text": "TM has many potential applications such as building training data for training transliterators and improving lexical coverage for machine translation and cross language search via translation resource expansion.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.8287220597267151}, {"text": "cross language search", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.6630596220493317}, {"text": "translation resource expansion", "start_pos": 180, "end_pos": 210, "type": "TASK", "confidence": 0.7010288238525391}]}, {"text": "TM has been gaining some attention of late with a shared task in the ACL 2010 NEWS workshop . In this paper, TM was performed using a transliterator that was used to generate possible transliterations of a word while constraining the output to tokens that exist in a target language word sequence.", "labels": [], "entities": [{"text": "TM", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.850159764289856}, {"text": "ACL 2010 NEWS workshop", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.8614020198583603}, {"text": "TM", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.9529838562011719}]}, {"text": "The paper presents the use of phonetic letter conflation and iterative transliterator training to improve TM when only limited transliteration training data is available.", "labels": [], "entities": [{"text": "phonetic letter conflation", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6068907280762991}, {"text": "TM", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.9875574111938477}]}, {"text": "For phonetic letter conflation, a variant of SOUNDEX was used to improve the coverage of existing training data.", "labels": [], "entities": [{"text": "phonetic letter conflation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6186398267745972}, {"text": "SOUNDEX", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.7503170371055603}]}, {"text": "As for iterative transliterator training, an initial transliterator, which was trained on initial set of transliteration pairs, was used to mine transliterations in parallel text.", "labels": [], "entities": []}, {"text": "Then, the automatically found transliterations pairs were considered correct and were used to retrain the transliterator.", "labels": [], "entities": []}, {"text": "1 http://translit.i2r.a-star.edu.sg/news2010/ The proposed improvements in TM were tested using the ACL 2010 NEWS workshop data for Arabic, English-Chinese, English-Hindi, EnglishRussian, and English-Tamil.", "labels": [], "entities": [{"text": "TM", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9426713585853577}, {"text": "ACL 2010 NEWS workshop data", "start_pos": 100, "end_pos": 127, "type": "DATASET", "confidence": 0.8687087297439575}]}, {"text": "For language pair, abase set of 1,000 transliteration pairs were available for training.", "labels": [], "entities": [{"text": "language pair", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.733569324016571}]}, {"text": "The rest of the paper is organized as follows: Section 2 surveys prior work on transliteration mining; Section 3 describes the TM approach and the proposed improvements; Section 4 describes the experimental setup including the evaluation sets; Section 5 reports on experimental results; and Section 6 concludes the paper.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.8168109059333801}, {"text": "TM", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.9341788291931152}]}], "datasetContent": [{"text": "The experiments were done on the ACL-2010 NEWS Workshop TM shared task datasets.", "labels": [], "entities": [{"text": "ACL-2010 NEWS Workshop TM shared task datasets", "start_pos": 33, "end_pos": 79, "type": "DATASET", "confidence": 0.9111327188355582}]}, {"text": "The datasets cover 5 language pairs.", "labels": [], "entities": []}, {"text": "For each pair, a dataset includes a list of 1,000 transliterated words to train a transliterator, and list of parallel word sequences between both languages.", "labels": [], "entities": []}, {"text": "The parallel sequences were extracted parallel Wikipedia article titles for which cross language links exist between both languages.", "labels": [], "entities": []}, {"text": "lists the language pairs and the number of the parallel word sequences., 4, 5, and 6 report results for Arabic, Chinese, Hindi, Russian and Tamil respectively.", "labels": [], "entities": []}, {"text": "As shown in, the recall for English-Chinese TM was dismal and suggests problems in experimental setup.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9991812109947205}]}, {"text": "This would require further investigation.", "labels": [], "entities": []}, {"text": "For the other 4 languages, the results show that not using S-mod and not using iterative training, as in Run 2, led to the highest precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9989562034606934}]}, {"text": "Using both S-mod and iterative training, as in Run 6, led to the highest recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9993317723274231}]}, {"text": "In comparing Runs 1 and 2, where 1 uses S-mod and 2 does not, using S-mod led to 35.6%, 40.6%, 12.2%, and 48.4% improvement in recall and to 6.8%, 2.8%, 6.3%, and 1.3% decline in precision for Arabic, Chinese, Russian, and Tamil respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9997461438179016}, {"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.9993886947631836}]}, {"text": "Except for Russian, the improvements in recall dwarf decline in precision, leading to overall improvements in F-measure for all 4 languages.", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.998685896396637}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9995532631874084}, {"text": "F-measure", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9981698989868164}]}, {"text": "In comparing runs 2 and 3 where iterative training is used, iterative training had marginal impact on precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9995490908622742}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9976928234100342}]}, {"text": "When using S-mod, comparing run 6 where iterative training was performed over the output from run 1, recall increased by 3.9%, 8.8%, 5.0%, and 22.7% for Arabic, Chinese, Russian, and Tamil respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.999675989151001}]}, {"text": "The drop in precision was 9.1% and 17.2% for Arabic and Russian respectively and marginal for Hindi and Tamil.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9996676445007324}]}, {"text": "Except for Russian, the best runs for all languages included the use of S-mod and iterative training.", "labels": [], "entities": []}, {"text": "The best runs were 4 for Arabic and Hindi and 6 for Tamil.", "labels": [], "entities": []}, {"text": "For Russian, the best runs involved using S-mod only without iterative training.", "labels": [], "entities": []}, {"text": "The drop in Russian could be attributed to the relatively large size of training data compared to the other languages (345,969 parallel word sequences).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Language pairs and no. of parallel sequences", "labels": [], "entities": []}, {"text": " Table 6: English-Tamil mining results", "labels": [], "entities": [{"text": "English-Tamil mining", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.5814190059900284}]}]}