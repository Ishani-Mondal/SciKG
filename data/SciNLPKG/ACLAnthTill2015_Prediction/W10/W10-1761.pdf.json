{"title": [{"text": "Improved Translation with Source Syntax Labels", "labels": [], "entities": [{"text": "Improved Translation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.82503542304039}]}], "abstractContent": [{"text": "We present anew translation model that include undecorated hierarchical-style phrase rules, decorated source-syntax rules, and partially decorated rules.", "labels": [], "entities": []}, {"text": "Results show an increase in translation performance of up to 0.8% BLEU for German-English translation when trained on the news-commentary corpus, using syntactic annotation from a source language parser.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9996609687805176}, {"text": "German-English translation", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.6725207269191742}]}, {"text": "We also experimented with annotation from shallow taggers and found this increased performance by 0.5% BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9993764758110046}]}], "introductionContent": [{"text": "Hierarchical decoding is usually described as a formally syntactic model without linguistic commitments, in contrast with syntactic decoding which constrains rules and production with linguistically motivated labels.", "labels": [], "entities": []}, {"text": "However, the decoding mechanism for both hierarchical and syntactic systems are identical and the rule extraction are similar.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.743137538433075}]}, {"text": "Hierarchical and syntax statistical machine translation have made great progress in the last few years and can claim to represent the state of the art in the field.", "labels": [], "entities": [{"text": "syntax statistical machine translation", "start_pos": 17, "end_pos": 55, "type": "TASK", "confidence": 0.5558983013033867}]}, {"text": "Both use synchronous context free grammar (SCFG) formalism, consisting of rewrite rules which simultaneously parse the input sentence and generate the output sentence.", "labels": [], "entities": []}, {"text": "The most common algorithm for decoding with SCFG is currently CKY+ with cube pruning works for both hierarchical and syntactic systems, as implemented in Hiero (), Joshua (, and Moses ( Rewrite rules in hierarchical systems have general applicability as their non-terminals are undecorated, giving hierarchical system broad coverage.", "labels": [], "entities": []}, {"text": "However, rules maybe used in inappropriate situations without the labeled constraints.", "labels": [], "entities": []}, {"text": "The general applicability of undecorated rules create spurious ambiguity which decreases translation performance by causing the decoder to spend more time sifting through duplicate hypotheses.", "labels": [], "entities": []}, {"text": "Syntactic systems makes use of linguistically motivated information to bias the search space at the expense of limiting model coverage.", "labels": [], "entities": []}, {"text": "This paper presents work on combining hierarchical and syntax translation, utilizing the high coverage of hierarchical decoding and the insights that syntactic information can bring.", "labels": [], "entities": [{"text": "syntax translation", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7482457756996155}]}, {"text": "We seek to balance the generality of using undecorated non-terminals with the specificity of labeled non-terminals.", "labels": [], "entities": []}, {"text": "Specifically, we will use syntactic labels from a source language parser to label non-terminal in production rules.", "labels": [], "entities": []}, {"text": "However, other source span information, such as chunk tags, can also be used.", "labels": [], "entities": []}, {"text": "We investigate two methods for combining the hierarchical and syntactic approach.", "labels": [], "entities": []}, {"text": "In the first method, syntactic translation rules are used concurrently with a hierarchical phrase rules.", "labels": [], "entities": [{"text": "syntactic translation", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.6954581141471863}]}, {"text": "Each ruleset is trained independently and used concurrently to decode sentences.", "labels": [], "entities": []}, {"text": "However, results for this method do not improve.", "labels": [], "entities": []}, {"text": "The second method uses one translation model containing both hierarchical and syntactic rules.", "labels": [], "entities": []}, {"text": "Moreover, an individual rule can contain both decorated syntactic non-terminals, and undecorated hierarchical-style non-terminals (also, the left-hand-side non-terminal may, or may not be decorated).", "labels": [], "entities": []}, {"text": "This results in a 0.8% improvement over the hierarchical baseline and analysis suggest that long-range ordering has been improved.", "labels": [], "entities": []}, {"text": "We then applied the same methods but using linguistic annotation from a chunk tagger instead of a parser and obtained an improvement of 0.5% BLEU over the hierarchical baseline, showing that gains with additional sourceside annotation can be obtained with simpler tools.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9993000030517578}]}], "datasetContent": [{"text": "We trained on the New Commentary 2009 corpus 1 , tuning on a hold-out set.", "labels": [], "entities": [{"text": "New Commentary 2009 corpus 1", "start_pos": 18, "end_pos": 46, "type": "DATASET", "confidence": 0.8857471227645874}]}, {"text": "gives more details on the corpus.", "labels": [], "entities": []}, {"text": "nc test2007 was used for testing.", "labels": [], "entities": [{"text": "nc test2007", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8462046980857849}]}, {"text": "The training corpus was cleaned and filtered using standard methods found in the Moses toolkit () and aligned using GIZA++.", "labels": [], "entities": []}, {"text": "Standard MERT weight tuning was used throughout.", "labels": [], "entities": [{"text": "MERT weight tuning", "start_pos": 9, "end_pos": 27, "type": "METRIC", "confidence": 0.9092039863268534}]}, {"text": "The English half of the training data was also used to create a trigram language model which was used for each experiment.", "labels": [], "entities": []}, {"text": "All experiments use truecase data and results are reported in case-sensitive BLEU scores ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.976495623588562}]}, {"text": "We use the same data as described earlier in this chapter to train, tune and test our approach.", "labels": [], "entities": []}, {"text": "The Treetagger chunker) was used to tag the source (German) side of the corpus.", "labels": [], "entities": []}, {"text": "The chunker successfully processed all sentences in the training and test dataset so no sentences were excluded.", "labels": [], "entities": []}, {"text": "The increase training data, as well as the ability to translate all sentences in the test set, explains the higher hierarchical baseline than the previous experiments with parser data.", "labels": [], "entities": []}, {"text": "We use the noun, verb and prepositional chunks, as well as part-of-speech tags, emitted by the chunker.", "labels": [], "entities": []}, {"text": "Results are shown in, line 5 & 6.", "labels": [], "entities": []}, {"text": "Using chunk tags, we see a modest gain of 0.5% BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9987726807594299}]}, {"text": "The same example sentence in is shown with chunk tags in.", "labels": [], "entities": []}, {"text": "The soft syntax model with chunk tags produced the derivation tree shown in.", "labels": [], "entities": []}, {"text": "The derivation make use of an unlexicalized rule local reordering.", "labels": [], "entities": []}, {"text": "In this example, it uses the same number of glue rule as the hierarchical derivation but the output is grammatically correct.", "labels": [], "entities": []}, {"text": "However, overall, the number of glue rules used shows the same reduction that we saw using soft syntax in the earlier section, as can be seen in.", "labels": [], "entities": []}, {"text": "Again, the soft syntax model, this time using chunk tags, is able to reduce the use of the glue rule with empirically informed rules.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training, tuning, and test conditions", "labels": [], "entities": [{"text": "tuning", "start_pos": 20, "end_pos": 26, "type": "TASK", "confidence": 0.7093735337257385}]}, {"text": " Table 2: German-English results for hierarchical  and syntactic models, in %BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9964494705200195}]}, {"text": " Table 5: Effect on %BLEU of varying number of  non-terminals", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9985563158988953}]}, {"text": " Table 6: English-German results in %BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9743466377258301}]}]}