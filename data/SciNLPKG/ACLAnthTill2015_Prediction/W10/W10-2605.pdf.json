{"title": [{"text": "Using Domain Similarity for Performance Estimation", "labels": [], "entities": [{"text": "Performance Estimation", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7128197848796844}]}], "abstractContent": [{"text": "Many natural language processing (NLP) tools exhibit a decrease in performance when they are applied to data that is linguistically different from the corpus used during development.", "labels": [], "entities": []}, {"text": "This makes it hard to develop NLP tools for domains for which annotated corpora are not available.", "labels": [], "entities": []}, {"text": "This paper explores a number of metrics that attempt to predict the cross-domain performance of an NLP tool through statistical inference.", "labels": [], "entities": []}, {"text": "We apply different similarity metrics to compare different domains and investigate the correlation between similarity and accuracy loss of NLP tool.", "labels": [], "entities": [{"text": "similarity", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.986987829208374}, {"text": "accuracy loss", "start_pos": 122, "end_pos": 135, "type": "METRIC", "confidence": 0.9852686822414398}]}, {"text": "We find that the correlation between the performance of the tool and the similarity metric is linear and that the latter can therefore be used to predict the performance of an NLP tool on out-of-domain data.", "labels": [], "entities": []}, {"text": "The approach also provides away to quantify the difference between domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Domain adaptation has recently turned into abroad field of study).", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7880354523658752}]}, {"text": "Many researchers note that the linguistic variation between training and testing corpora is an important factor in assessing the performance of an NLP tool across domains.", "labels": [], "entities": []}, {"text": "For example, a tool that has been developed to extract predicate-argument structures from abstracts of biomedical research papers, will exhibit a lower performance when applied to legal texts.", "labels": [], "entities": []}, {"text": "However, the notion of domain is mostly arbitrarily used to refer to some kind of semantic area.", "labels": [], "entities": []}, {"text": "There is unfortunately no unambiguous measure to assert a domain shift, except by observing the performance loss of an NLP tool when applied across different domains.", "labels": [], "entities": []}, {"text": "This means that we typically need annotated data to reveal a domain shift.", "labels": [], "entities": []}, {"text": "In this paper we will show how unannotated data can be used to get a clearer view on how datasets differ.", "labels": [], "entities": []}, {"text": "This unsupervised way of looking at data will give us a method to measure the difference between data sets and allows us to predict the performance of an NLP tool on unseen, out-of-domain data.", "labels": [], "entities": []}, {"text": "In Section 2 we will explain our approach in detail.", "labels": [], "entities": []}, {"text": "In Section 3 we deal with a case study involving basic part-of-speech taggers, applied to different domains.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.736168771982193}]}, {"text": "An overview of related work can be found in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes this paper and discusses options for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used data extracted from the British National Corpus (BNC) and consisting of written books and periodicals 1 . The BNC annotators provided 9 domain codes (i.e. wridom), making it possible to divide the text from books and periodicals into 9 subcorpora.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 32, "end_pos": 61, "type": "DATASET", "confidence": 0.972338060537974}, {"text": "BNC annotators", "start_pos": 118, "end_pos": 132, "type": "DATASET", "confidence": 0.9566583037376404}]}, {"text": "These annotated semantic domains are: imaginative (wridom1), natural & pure science (wridom2), applied science (wridom3), social science (wridom4), world affairs (wridom5), commerce & finance (wridom6), arts (wridom7), belief & thought (wridom8), and leisure (wridom9).", "labels": [], "entities": []}, {"text": "The extracted corpus contains sentences in which every token is tagged with a part-of-speech tag as defined by the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.9489509463310242}]}, {"text": "Since the BNC has been tagged automatically, using the CLAWS4 automatic tagger ( and the Template Tagger (, the experiments in this article are artificial in the sense that they do not learn real part-of-speech tags but rather partof-speech tags as they are assigned by the automatic taggers.", "labels": [], "entities": [{"text": "BNC", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.8350293636322021}]}], "tableCaptions": [{"text": " Table 1: Average accuracy and standard deviation on 72 cross-validation experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9399579763412476}, {"text": "standard deviation", "start_pos": 31, "end_pos": 49, "type": "METRIC", "confidence": 0.9516298472881317}]}]}