{"title": [{"text": "Self-Training without Reranking for Parser Domain Adaptation and Its Impact on Semantic Role Labeling", "labels": [], "entities": [{"text": "Parser Domain Adaptation", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7502740820248922}, {"text": "Semantic Role Labeling", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.676350732644399}]}], "abstractContent": [{"text": "We compare self-training with and without reranking for parser domain adaptation , and examine the impact of syntactic parser adaptation on a semantic role labeling system.", "labels": [], "entities": [{"text": "parser domain adaptation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.8658308188120524}, {"text": "syntactic parser adaptation", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.6987605094909668}]}, {"text": "Although self-training without reranking has been found not to improve in-domain accuracy for parsers trained on the WSJ Penn Treebank, we show that it is surprisingly effective for parser domain adaptation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9739760160446167}, {"text": "WSJ Penn Treebank", "start_pos": 117, "end_pos": 134, "type": "DATASET", "confidence": 0.9370355606079102}, {"text": "parser domain adaptation", "start_pos": 182, "end_pos": 206, "type": "TASK", "confidence": 0.8923799395561218}]}, {"text": "We also show that simple self-training of a syntactic parser improves out-of-domain accuracy of a semantic role labeler.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9869359135627747}, {"text": "semantic role labeler", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.6279919942220052}]}], "introductionContent": [{"text": "Improvements in data-driven parsing approaches, coupled with the development of treebanks that serve as training data, have resulted in accurate parsers for several languages.", "labels": [], "entities": []}, {"text": "However, portability across domains remains a challenge: parsers trained using a treebank fora specific domain generally perform comparatively poorly in other domains.", "labels": [], "entities": []}, {"text": "In English, the most widely used training set for parsers comes from the Wall Street Journal portion of the Penn Treebank (, and constituent parsers trained on this set are now capable of labeled bracketing precision and recall of over 90% on WSJ testing sentences.", "labels": [], "entities": [{"text": "Wall Street Journal portion of the Penn Treebank", "start_pos": 73, "end_pos": 121, "type": "DATASET", "confidence": 0.9509312510490417}, {"text": "precision", "start_pos": 207, "end_pos": 216, "type": "METRIC", "confidence": 0.9228786826133728}, {"text": "recall", "start_pos": 221, "end_pos": 227, "type": "METRIC", "confidence": 0.9984632730484009}, {"text": "WSJ testing sentences", "start_pos": 243, "end_pos": 264, "type": "DATASET", "confidence": 0.8186296423276266}]}, {"text": "When applied without adaptation to the Brown portion of the Penn Treebank, however, an absolute drop of over 5% in precision and recall is typically observed ().", "labels": [], "entities": [{"text": "Brown portion of the Penn Treebank", "start_pos": 39, "end_pos": 73, "type": "DATASET", "confidence": 0.8883256415526072}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9996744394302368}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9990139007568359}]}, {"text": "In pipelined NLP applications that include a parser, this drop often results in severely degraded results downstream.", "labels": [], "entities": []}, {"text": "We present experiments with a simple selftraining approach to semi-supervised parser domain adaptation that produce results that contradict the commonly held assumption that improved parser accuracy cannot be obtained by self-training a generative parser without reranking.", "labels": [], "entities": [{"text": "parser domain adaptation", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.773199995358785}, {"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9519899487495422}]}, {"text": "We compare this simple self-training approach to the selftraining with reranking approach proposed by, and show that althoughs approach produces better labeled bracketing precision and recall on out-ofdomain sentences, higher F-score on syntactic parses may not lead to an overall improvement in results obtained in NLP applications that include parsing, contrary to our expectations.", "labels": [], "entities": [{"text": "precision", "start_pos": 171, "end_pos": 180, "type": "METRIC", "confidence": 0.9065495729446411}, {"text": "recall", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.9981449842453003}, {"text": "F-score", "start_pos": 226, "end_pos": 233, "type": "METRIC", "confidence": 0.9972196817398071}, {"text": "parsing", "start_pos": 346, "end_pos": 353, "type": "TASK", "confidence": 0.9618632197380066}]}, {"text": "This is evidenced by results obtained when different adaptation approaches are applied to a parser that serves as a component in a semantic role labeling (SRL) system.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 131, "end_pos": 159, "type": "TASK", "confidence": 0.7856736282507578}]}, {"text": "This is, to our knowledge, the first attempt to quantify the benefits of semisupervised parser domain adaptation in semantic role labeling, a task in which parsing accuracy is crucial.", "labels": [], "entities": [{"text": "parser domain adaptation", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.7471869985262553}, {"text": "semantic role labeling", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.6535499493281046}, {"text": "parsing", "start_pos": 156, "end_pos": 163, "type": "TASK", "confidence": 0.9591783881187439}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.8587270975112915}]}], "datasetContent": [{"text": "In our experiments we use primarily the parser.", "labels": [], "entities": []}, {"text": "Ina few specific experiments we also use the reranker; such cases are noted explicitly and are not central to the paper, serving mostly for comparisons.", "labels": [], "entities": []}, {"text": "We follow the three steps described in section 2.3.", "labels": [], "entities": []}, {"text": "The manually labeled training corpus is the standard WSJ training sections of the Penn Treebank (sections 02 to 21).", "labels": [], "entities": [{"text": "WSJ training sections", "start_pos": 53, "end_pos": 74, "type": "DATASET", "confidence": 0.8026782472928365}, {"text": "Penn Treebank", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.8381077647209167}]}, {"text": "Sections 22 and 23 are used as in-domain development and testing sets, respectively.", "labels": [], "entities": []}, {"text": "The outof-domain material is taken from the Brown portion of the Penn Treebank.", "labels": [], "entities": [{"text": "Brown portion of the Penn Treebank", "start_pos": 44, "end_pos": 78, "type": "DATASET", "confidence": 0.9237543046474457}]}, {"text": "We use the same Brown test set as, every tenth sentence in the corpus.", "labels": [], "entities": []}, {"text": "Another tenth of the corpus is used as a development set, and the rest of the Brown corpus is not used.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.9830724000930786}]}, {"text": "The out-ofdomain text then contains not one but several genres of text.", "labels": [], "entities": []}, {"text": "The larger set of unlabeled data is composed of approximately 5.3 million words (320k sentences) of 20th century novels available from Project Gutenberg 3 , which do not match exactly the target domain, but is closer to it in general than to the source domain (WSJ).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Brown development set F-scores ob- tained with self-trained models with different  relative weights given to the gold-standard WSJ  training data. The last row shows the F-score for  the original model (without adaptation).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.7292700409889221}, {"text": "WSJ  training data", "start_pos": 137, "end_pos": 155, "type": "DATASET", "confidence": 0.8479936321576437}, {"text": "F-score", "start_pos": 180, "end_pos": 187, "type": "METRIC", "confidence": 0.9942356944084167}]}, {"text": " Table 1. Labeled constituent precision, recall and F-score for the WSJ and Brown test sets, ob- tained with the baseline model (trained only on the WSJ training set) and with the self-trained  model. Results on Brown show an absolute improvement of almost 2%, while results on WSJ  show a drop of about 1%. The last row shows the results obtained by", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.953231930732727}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9997155070304871}, {"text": "F-score", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9988106489181519}, {"text": "WSJ and Brown test sets", "start_pos": 68, "end_pos": 91, "type": "DATASET", "confidence": 0.7888598263263702}, {"text": "WSJ training set", "start_pos": 149, "end_pos": 165, "type": "DATASET", "confidence": 0.9552286068598429}, {"text": "WSJ", "start_pos": 278, "end_pos": 281, "type": "DATASET", "confidence": 0.9730088710784912}]}, {"text": " Table 3: Brown development set F-scores  obtained with self-trained models created  with different amounts of unlabeled data.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.5720633268356323}]}]}