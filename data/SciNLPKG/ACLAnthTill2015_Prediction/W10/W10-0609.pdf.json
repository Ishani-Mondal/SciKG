{"title": [{"text": "Using fMRI activation to conceptual stimuli to evaluate methods for extracting conceptual representations from corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a series of methods for deriving conceptual representations from corpora and investigate the usefulness of the fMRI data and machine learning methodology of Mitchell et al.", "labels": [], "entities": [{"text": "fMRI data", "start_pos": 122, "end_pos": 131, "type": "DATASET", "confidence": 0.8933671414852142}]}, {"text": "(2008) as a basis for evaluating the different models.", "labels": [], "entities": []}, {"text": "Within this framework , the quality of a semantic model is quantified by its ability to predict the fMRI activation associated with conceptual stimuli.", "labels": [], "entities": []}, {"text": "Mitchell et al. used a manually-acquired set of verbs as the basis for their semantic model; in this paper, we also consider automatically acquired feature-norm-like semantic representations.", "labels": [], "entities": []}, {"text": "These models make different assumptions about the kinds of information available in corpora that is relevant to representing conceptual knowledge.", "labels": [], "entities": []}, {"text": "Our results indicate that automatically-acquired representations can make equally powerful predictions about the brain activity associated with the stimuli.", "labels": [], "entities": []}], "introductionContent": [{"text": "presented a novel approach for predicting human brain activity associated with conceptual stimuli.", "labels": [], "entities": [{"text": "predicting human brain activity associated with conceptual stimuli", "start_pos": 31, "end_pos": 97, "type": "TASK", "confidence": 0.837875209748745}]}, {"text": "This approach represents a useful development for interdisciplinary researchers interested in lexical semantics, for several reasons.", "labels": [], "entities": []}, {"text": "Most broadly, it is useful in testing the hypothesis that distributional properties of words in corpora can reveal important information about the meanings of words.", "labels": [], "entities": []}, {"text": "A strong version of this hypothesis (i.e. that children in part learn the meaning of concrete concept words from co-occurring words in discourse that they are exposed to) has formed the basis of one class of probabilistic cognitive models of conceptual representation.", "labels": [], "entities": []}, {"text": "Furthermore this approach is useful for testing hypotheses about the kind of co-occurring information that is useful for representing conceptual semantics.", "labels": [], "entities": []}, {"text": "In, for example, they adopt the position that the meaning of concrete concepts is encoded in the brain with information associated with basic sensory and motor activities (such as actions involving changes to spatial relationships and physical actions performed on objects).", "labels": [], "entities": []}, {"text": "At a more technical level, Mitchell et al.'s fMRI activation data give researchers developing featurebased models of conceptual representation an important benchmark for evaluation.", "labels": [], "entities": [{"text": "fMRI activation data", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.6244361102581024}]}, {"text": "For these researchers, a key problem is the lack of a reasonable \"gold standard\" against which the quality of the representations generated by a computational model maybe evaluated.", "labels": [], "entities": []}, {"text": "Previous research has adopted two main approaches to evaluation.", "labels": [], "entities": []}, {"text": "Firstly, some models -especially those aiming to extract representations composed of psychologically meaningful semantic feature units, such as  -have been evaluated against features gathered in large scale property norming studies (e.g.).", "labels": [], "entities": []}, {"text": "By comparing the system output against features elicited by people, this kind of eval-uation aims to test the psychological validity of computational methods.", "labels": [], "entities": []}, {"text": "Furthermore, it allows a finegrained analysis of performance, for example by revealing the classes of features (part-of, taxonomic, etc) which a given model is particularly good at extracting ( . However, property norms come with important caveats.", "labels": [], "entities": []}, {"text": "One problem is that they tend to overrepresent informative or salient information about concepts whilst under-representing other kinds of features.", "labels": [], "entities": []}, {"text": "For example, participants report that camels have humps, but not that camels have hearts, even though all participants are likely to have both pieces of information accessible in their representation of the concept CAMEL.", "labels": [], "entities": []}, {"text": "If a model is successful in extracting these less salient features, there is noway of evaluating their correctness using property norms.", "labels": [], "entities": []}, {"text": "A related issue is that participants can only report verbalizable features, which may not represent the total sum of their conceptual knowledge.", "labels": [], "entities": []}, {"text": "A second problem with using property norms as the basis of evaluation is that there is often no direct lexical match between feature terms appearing in the system output and the norms.", "labels": [], "entities": []}, {"text": "Feature norms are typically normalized such that near-synonymous properties (e.g. is endangered, is an endangered species, is almost extinct, etc., for WHALE) given by different participants are mapped to the same feature label (e.g. is endangered).", "labels": [], "entities": []}, {"text": "As a consequence, a model may correctly extract endangered for WHALE, but other lexical forms of the same feature will not match any feature in the norms.", "labels": [], "entities": [{"text": "WHALE", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.55869460105896}]}, {"text": "One solution to this is to create an expansion set for each feature which includes its synonyms ( . However, this is only a partial solution because lexical variation in features is not limited to synonyms.", "labels": [], "entities": []}, {"text": "A second approach to evaluating semantic models uses classification or similarity data.", "labels": [], "entities": []}, {"text": "For example, evaluated their models by calculating cosine similarity scores between semantic representations and using these similarity scores to predict behavioral data which are contingent on the semantic similarity between pairs of concepts (e.g. lexical substitution errors, semantic priming latencies, word-association norms, etc).", "labels": [], "entities": []}, {"text": "Although this approach is psychologically motivated, it evaluates a set of extracted features more indirectly than comparison with norm data.", "labels": [], "entities": []}, {"text": "In computational linguistics, a similarly indirect evaluation method is to cluster the extracted representations.", "labels": [], "entities": []}, {"text": "This approach avoids the difficulties in evaluating individual features; however it only allows consideration along one dimension of the data, namely the similarity between pairs of concepts.", "labels": [], "entities": []}, {"text": "fMRI data such as the dataset offers an advancement over both of these evaluation techniques.", "labels": [], "entities": [{"text": "fMRI data", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8721430599689484}]}, {"text": "Unlike, for example, property norming data, fMRI data offers direct insight into how the brain is functioning in response to given stimuli.", "labels": [], "entities": []}, {"text": "Its multidimensional nature makes it easier to inspect what aspects of meaning a particular model is performing strongly or weakly on, and allows for better control of experimental variation.", "labels": [], "entities": []}, {"text": "Finally, it avoids the two major issues associated with property norms, which we outlined above.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "In the next section, we briefly describe the models which we used to extract conceptual representations for the 60 concepts in the dataset.", "labels": [], "entities": []}, {"text": "In Section 3, we outline our experimental objectives, and the framework we adopt for testing our semantic models.", "labels": [], "entities": []}, {"text": "In Section 4, we present the results of our evaluation, which indicate above chance performance for each of the models.", "labels": [], "entities": []}, {"text": "Finally, we examine the differences between models by investigating for which concepts prediction of the fMRI activity is poorest, and discuss these differences with respect to the differing assumptions made by the methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned above, we are primarily interested in using the fMRI data to evaluate the quality of the different methods for extracting conceptual representations from corpora (rather than being interested in investigating methods for predicting fMRI activation).", "labels": [], "entities": [{"text": "fMRI data", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.821386992931366}]}, {"text": "We make no attempt to build on the method described by, although there are likely to be many interesting avenues through which that method could be extended.", "labels": [], "entities": []}, {"text": "We therefore followed the Mitchell et al. methodology as closely as possible, using the same multiple regression training and leave-two-out cross-validation paradigms as presented in their paper and supporting online material.", "labels": [], "entities": []}, {"text": "The only parameter that we varied was the extraction method (and corpus) that was used to generate the feature-vectors associated with the 60 concepts that were used during the training phase.", "labels": [], "entities": []}, {"text": "The quality of the predictions generated for the concepts using each semantic model can therefore be adopted as an index of model performance.", "labels": [], "entities": []}, {"text": "The Mitchell et al. method uses co-occurrence with a specific set of 25 manually selected verbs (eat, push, etc) that are the same for each concept.", "labels": [], "entities": []}, {"text": "This results in 25-dimensional feature vectors for input into training.", "labels": [], "entities": []}, {"text": "However, for both the SVD model and our triple extraction models there are no a priori constraints on the number of unique features that can be extracted for the concepts.", "labels": [], "entities": []}, {"text": "For these models, we selected the top 200 features associated with each concept; therefore, across all 60 concepts in the Mitchell et al. dataset, there are thousands of unique features extracted which are used in the concepts' representations.", "labels": [], "entities": []}, {"text": "To ensure that the linear regression model for each method would befitted using the same number of free parameters during training (thereby maximizing the comparability of the different methods), we reduced the dimensionality of the generated feature spaces for the SVD method and the two triple-extraction methods using Principal Components Analysis (PCA).", "labels": [], "entities": []}, {"text": "The concept \u00d7 feature extraction frequency matrices for the three models were submitted to PCA, and the first 25 components (i.e. those components which best charac-5 For example, the method currently makes the simplifying assumption that the activity in neighbouring voxels is independent.", "labels": [], "entities": [{"text": "PCA", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9123982787132263}]}, {"text": "terized the variance of the original features) for each model were selected.", "labels": [], "entities": []}, {"text": "In the case of the SVD model, these 25 dimensions explained 77.7% of the variance in the original 3,061-dimensional vectors.", "labels": [], "entities": []}, {"text": "For our unweighted extraction method, the 25 extracted components explained 63.0% of the original 5,525 dimensions; for the weighted method the components explained 71.5% of the original 6,567 dimensions.", "labels": [], "entities": []}, {"text": "It is interesting to consider the kind of semantic information that is being captured by the resultant PCA components.", "labels": [], "entities": []}, {"text": "In particular, the components appear to capture meaningful distinctions between stimuli.", "labels": [], "entities": []}, {"text": "For example, the first PCA component for our weighted triple extraction method can be interpreted as the concepts' degree of \"animalness\" (animal stimuli have high values on this component).", "labels": [], "entities": []}, {"text": "gives a summary comparison of the different models, in terms of whether or not each uses part of speech (POS) data, syntactic information (i.e. GRs), and semantic filtering (Section 2.3).", "labels": [], "entities": [{"text": "semantic filtering", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.7072015851736069}]}, {"text": "It should be noted that the BNC corpus (used with the SVD model and our triple-extraction method) is 10,000 times smaller than the corpus from which the Mitchell et al. feature vectors are derived.", "labels": [], "entities": [{"text": "BNC corpus", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.891717404127121}]}, {"text": "As such the semantic representations we extract with our method need to make better use of the data available in the corpus if they are to compete with the verb-based features used by Mitchell et al.'s method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracy results for the four semantic models.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9984453320503235}]}, {"text": " Table 4: Leave-out pairs for which each model performs least accurately, across the nine participants. Nr. = the number  of participants for which this leave-out pair was correctly matched.", "labels": [], "entities": []}]}