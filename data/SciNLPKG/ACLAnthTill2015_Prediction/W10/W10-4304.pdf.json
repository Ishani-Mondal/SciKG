{"title": [{"text": "Modeling User Satisfaction Transitions in Dialogues from Overall Ratings", "labels": [], "entities": [{"text": "Modeling User Satisfaction Transitions", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8252606466412544}]}], "abstractContent": [{"text": "This paper proposes a novel approach for predicting user satisfaction transitions during a dialogue only from the ratings given to entire dialogues, with the aim of reducing the cost of creating reference ratings for utterances/dialogue-acts that have been necessary in conventional approaches.", "labels": [], "entities": [{"text": "predicting user satisfaction transitions during a dialogue", "start_pos": 41, "end_pos": 99, "type": "TASK", "confidence": 0.8279700108936855}]}, {"text": "In our approach, we first train hidden Markov models (HMMs) of dialogue-act sequences associated with each overall rating.", "labels": [], "entities": []}, {"text": "Then, we combine such rating-related HMMs into a single HMM to decode a sequence of dialogue-acts into state sequences representing to which overall rating each dialogue-act is most related, which leads to our rating predictions.", "labels": [], "entities": []}, {"text": "Experimental results in two dialogue domains show that our approach can make reasonable predictions; it significantly outperforms a baseline and nears the upper bound of a supervised approach in some evaluation criteria.", "labels": [], "entities": []}, {"text": "We also show that introducing states that represent dialogue-act sequences that occur commonly in all ratings into an HMM significantly improves prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9544080495834351}]}], "introductionContent": [{"text": "In recent years, there has been intensive work on the automatic evaluation of dialogues ().", "labels": [], "entities": [{"text": "automatic evaluation of dialogues", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.6618644893169403}]}, {"text": "Automatic evaluation makes it possible to predict the performance of dialogue systems without the costly process of performing surveys with human subjects, leading to a rapid improvement cycle for dialogue systems.", "labels": [], "entities": []}, {"text": "It is also useful for detecting problematic situations in an ongoing dialogue (.", "labels": [], "entities": []}, {"text": "In these studies, the typical approach is to train a prediction model, such as a regression or classification model, using features representing the whole or apart of a dialogue together with human reference labels (e.g., reference ratings).", "labels": [], "entities": []}, {"text": "However, creating such reference labels by hand can be extremely costly when we want to predict user satisfaction transitions during a dialogue because we need to create reference labels after each utterance/dialogue-act in the training data.", "labels": [], "entities": []}, {"text": "This paper proposes a novel approach for predicting user satisfaction transitions during a dialogue only from the dialogues with overall ratings.", "labels": [], "entities": [{"text": "predicting user satisfaction transitions during a dialogue", "start_pos": 41, "end_pos": 99, "type": "TASK", "confidence": 0.785803062575204}]}, {"text": "The approach makes it possible to avoid creating reference labels for utterances/dialogueacts and only requires a single reference label for each dialogue.", "labels": [], "entities": []}, {"text": "More specifically, we predict the user satisfaction rating after each dialogue-act in a dialogue only by using dialogues with dialoguelevel (overall) user satisfaction ratings as training data.", "labels": [], "entities": []}, {"text": "Our basic approach is to train hidden Markov models (HMMs) of dialogue-act sequences associated with each overall rating and combine such rating-related HMMs into a single HMM.", "labels": [], "entities": []}, {"text": "We use this combined HMM to decode a sequence of dialogue-acts by the Viterbi algorithm into state sequences that indicate from which rating-related HMM each dialogue-act is most likely to have been generated, leading to our rating predictions for the dialogue-acts.", "labels": [], "entities": []}, {"text": "This paper experimentally examines the validity of our approach and explores several model topologies for possible improvement.", "labels": [], "entities": []}, {"text": "In Section 2, we review related work on automatic evaluation of dialogues.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our approach in detail.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the experiment we performed to verify our approach and present the results.", "labels": [], "entities": []}, {"text": "In Section 5, we summarize and mention future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To verify our approach, we first prepared dialogue data.", "labels": [], "entities": []}, {"text": "Then, we trained our HMMs and compared them with a random baseline and an upper bound that uses a supervised approach; that is, an HMM is trained using reference labels on the dialogueact level.", "labels": [], "entities": []}, {"text": "We performed a ten-fold cross validation.", "labels": [], "entities": []}, {"text": "We first separated utterance-level labeled data (i.e., AD-SUB2 or AL-SUB1) into 10 disjoint sets.", "labels": [], "entities": []}, {"text": "Then, for each set S, we used dialogue-level labeled data (i.e., AD-SUB1 or AL-ALL) excluding S for training HMMs.", "labels": [], "entities": []}, {"text": "Here, 'supervised' only used the utterance-level labeled data excluding S for training.", "labels": [], "entities": []}, {"text": "Then, we made the models (i.e., ergodic0, ergodic1, ergodic2, concat1, concat2, random and supervised) output rating sequences for the dialogue-acts in Sand evaluated them with the reference ratings in S.", "labels": [], "entities": []}, {"text": "We repeated this process ten times to evaluate the overall performance.", "labels": [], "entities": []}, {"text": "Since utterance-level ratings are provided only after system/listener utterances, we only evaluated ratings after dialogue-acts corresponding to system/listener utterances.", "labels": [], "entities": []}, {"text": "When a system/listener utterance contained multiple dialogue-acts, the dialogue-acts were assumed to have the same rating as that utterance.", "labels": [], "entities": []}, {"text": "When the output rating sequences contain 0, which can be the case for ergodic1-2 and concat1-2, the 0 is replaced by the most previous non-zero rating.", "labels": [], "entities": []}, {"text": "When 0 is found at the beginning of a dialogue, it remained 0.", "labels": [], "entities": []}, {"text": "Although our reference ratings always started with four (cf. Section 4.1.1), we did not use this information to fill initial zeros because we wanted to evaluate the prediction accuracy when we do not have any prior knowledge.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9134990572929382}]}, {"text": "Since some models may benefit from avoiding evaluating dialogueacts at the beginning because of these zeros, we simply compared the rating sequences where all models produced non-zero values.", "labels": [], "entities": []}, {"text": "For example, when we have three output rating sequences <0,5,6,0,4>, <0,0,1,2,0>, and <1,2,3,4,5> fora given dialogue-act sequence, the zeros that follow non-zero values are first filled with their preceeding values, and thereby we obtain <0,5,6,6,4>, <0,0,1,2,2>, and <1,2,3,4,5>.", "labels": [], "entities": []}, {"text": "Then, by cropping the common non-zero span, we obtain <6,6,4>, <1,2,2>, and <3,4,5>, and use these rating sequences for evaluation.", "labels": [], "entities": []}, {"text": "We used two kinds of evaluation criteria: one for evaluating individual matches and the other for evaluating distributions.", "labels": [], "entities": []}, {"text": "Evaluating Individual Matches: We used the match rate and mean absolute error to evaluate the matching of reference and hypothesis rating sequences.", "labels": [], "entities": [{"text": "match rate", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9102786481380463}, {"text": "mean absolute error", "start_pos": 58, "end_pos": 77, "type": "METRIC", "confidence": 0.807753344376882}]}, {"text": "They are derived by the equations shown below.", "labels": [], "entities": []}, {"text": "In the equations, R (= {R 1 . .", "labels": [], "entities": []}, {"text": "R L }) and H (= {H 1 . .", "labels": [], "entities": []}, {"text": "H L }) denote reference and hypothesis rating sequences fora dialogue, respectively.", "labels": [], "entities": []}, {"text": "L is the length of Rand H (Note that they have the same length).", "labels": [], "entities": []}, {"text": "\u2022 Match Rate (MR) where 'match' returns 1 or 0 depending on whether a rating in R matches that in H.", "labels": [], "entities": [{"text": "Match Rate (MR)", "start_pos": 2, "end_pos": 17, "type": "METRIC", "confidence": 0.9355564832687377}]}, {"text": "\u2022 Mean Absolute Error (MAE) Evaluating Distributions: In generative models, it is important that the output distribution matches that of the reference.", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE", "start_pos": 2, "end_pos": 26, "type": "METRIC", "confidence": 0.9283244013786316}]}, {"text": "Therefore, we additionally use Kullback-Leibler divergence, match rate per rating, and mean absolute error per rating.", "labels": [], "entities": [{"text": "match rate per rating", "start_pos": 60, "end_pos": 81, "type": "METRIC", "confidence": 0.9571369290351868}, {"text": "mean absolute error per rating", "start_pos": 87, "end_pos": 117, "type": "METRIC", "confidence": 0.8483320116996765}]}, {"text": "The Kullback-Leibler divergence evaluates the shape of output distributions.", "labels": [], "entities": []}, {"text": "The match rate per rating and mean absolute error per rating evaluate how accurately each individual rating can be predicted; namely, the accuracy for predicting dialogue-acts with one rating is equally valued with those for other ratings irrespective of the distribution of ratings in the reference.", "labels": [], "entities": [{"text": "match rate", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.955735981464386}, {"text": "mean absolute error", "start_pos": 30, "end_pos": 49, "type": "METRIC", "confidence": 0.8883016109466553}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9992313385009766}]}, {"text": "It is important to use these metrics in the practical as well as information theoretic sense because it is no use predicting only easy-to-guess ratings; we should be able to correctly predict rare but still important cases.", "labels": [], "entities": []}, {"text": "For example, rating 1 in human-human dialogue is quite rare; however, predicting it is very important for detecting problematic situations in a dialogue.", "labels": [], "entities": [{"text": "rating 1", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.901565819978714}]}, {"text": "\u2022 Kullback-Leibler Divergence (KL) where K is the maximum user satisfaction rating (i.e. 7 in this experiment), Rand H denote the sequentially concatenated reference/hypothesis rating sequences of the entire dialogues, and P( * , r) denotes the occurrence probability that a rating r is found in an arbitrary rating sequence.", "labels": [], "entities": [{"text": "Rand H", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9589913785457611}]}, {"text": "\u2022 Match Rate per rating (MR/r) MR/r(R, H) = 1: The match rate (MR), mean absolute error (MAE), Kullback-Leibler divergence (KL), match rate per rating (MR/r) and mean absolute error per rating (MAE/r) for our proposed HMMs, the random baseline, and the upper bound (supervised) for the AD domain.", "labels": [], "entities": [{"text": "Match Rate per rating (MR/r) MR/r", "start_pos": 2, "end_pos": 35, "type": "METRIC", "confidence": 0.93793352941672}, {"text": "match rate (MR)", "start_pos": 51, "end_pos": 66, "type": "METRIC", "confidence": 0.961230993270874}, {"text": "mean absolute error (MAE)", "start_pos": 68, "end_pos": 93, "type": "METRIC", "confidence": 0.8928603529930115}, {"text": "Kullback-Leibler divergence (KL)", "start_pos": 95, "end_pos": 127, "type": "METRIC", "confidence": 0.8244881570339203}, {"text": "match rate per rating (MR/r)", "start_pos": 129, "end_pos": 157, "type": "METRIC", "confidence": 0.9475295477443271}, {"text": "mean absolute error per rating (MAE/r)", "start_pos": 162, "end_pos": 200, "type": "METRIC", "confidence": 0.8675464183092118}]}, {"text": "'e0-e2', 'c1-c2', and 'r' indicate the statistical significance (p<0.01) over ergodic0-2, concat1-2, and random, respectively.", "labels": [], "entities": []}, {"text": "Bold font indicates the best value within each row (except for 'supervised').", "labels": [], "entities": []}, {"text": "where R i and H i denote ratings at i-th positions.", "labels": [], "entities": []}, {"text": "\u2022 Mean Absolute Error per rating (MAE/r) show the evaluation results for the AD and AL domains, respectively.", "labels": [], "entities": [{"text": "Mean Absolute Error per rating (MAE/r)", "start_pos": 2, "end_pos": 40, "type": "METRIC", "confidence": 0.9103497207164765}, {"text": "AD", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9610793590545654}, {"text": "AL", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.7796934247016907}]}, {"text": "The MR and MAE values are averaged overall dialogues.", "labels": [], "entities": [{"text": "MR", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9987370371818542}, {"text": "MAE", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.994178295135498}]}, {"text": "To compare the means of the MR and MAE, we performed a non-parametric multiple comparison test [Steel-Dwass test].", "labels": [], "entities": [{"text": "MR", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9378446936607361}, {"text": "MAE", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9584341645240784}]}, {"text": "We did not perform a statistical test for other criteria because it was difficult to perform sample-wise comparison for distributions.", "labels": [], "entities": []}, {"text": "Naturally, 'supervised' is the best performing model for all criteria in both domains.", "labels": [], "entities": []}, {"text": "Therefore, we focus on how much our proposed models differ from the baseline (random) and the upper bound (supervised).", "labels": [], "entities": []}, {"text": "In the AD domain, we find that ergodic0 and ergodic1 performed rather poorly and concat1 and concat2 performed fairly well, significantly outperforming the random baseline.", "labels": [], "entities": []}, {"text": "However, it is also clear that we still need a great deal of improvement for our models to reach the level of 'supervised'.", "labels": [], "entities": []}, {"text": "A promising sign is that concat2 is not significantly different from 'supervised' in smoothness.", "labels": [], "entities": []}, {"text": "Here, ergodic0 and ergodic1 returned the exact same results.", "labels": [], "entities": []}, {"text": "This means that the state transition paths did not go through the common states at all in ergodic1, suggesting that the common states in ergodic1 have very broad output distributions and the optimal path could not go through the common states, instead preferring other states having sharper distributions.", "labels": [], "entities": []}, {"text": "However, this phenomenon was rightly avoided by introducing more common states as seen in the results for ergodic2; nonetheless, as the results for concat1 and concat2 indicate, the transition probabilities have to be trained appropriately to obtain better results.", "labels": [], "entities": []}, {"text": "In the AL domain, although the tendency of the evaluation results is the same as that for the AD domain, concat2 is clearly the best performing model.", "labels": [], "entities": []}, {"text": "It outperformed other models in almost all cases except for \"Good Listener\" for which concat1 performed better.", "labels": [], "entities": [{"text": "Good Listener\"", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.6587004860242208}]}, {"text": "In fact, the MR/r and MAE/r of concat1 are quite close to those of 'supervised', suggesting the potential of our approach.", "labels": [], "entities": [{"text": "MR/r", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9740680257479349}, {"text": "MAE/r", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9687855442365011}]}, {"text": "Overall, although we still need further improvement in order for our models to be closer to the upper bound, we showed that we can, to some extent, predict user satisfaction transitions in a dialogue only from overall ratings of dialogues using our proposed HMMs.", "labels": [], "entities": []}, {"text": "We also showed that model topologies and learning methods can make significant differences.", "labels": [], "entities": []}, {"text": "Especially, we found the introduction of common states to be crucial in making appropriate models for prediction.", "labels": [], "entities": [{"text": "prediction", "start_pos": 102, "end_pos": 112, "type": "TASK", "confidence": 0.9620770812034607}]}, {"text": "Since our models, especially concat2, significantly outperformed the baseline, we believe that our approach can be one of the viable options for automatically predicting user satisfaction transitions when there exist only overall rating data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation results for the AL domain. See Table 1 for the notations in the table.", "labels": [], "entities": []}]}