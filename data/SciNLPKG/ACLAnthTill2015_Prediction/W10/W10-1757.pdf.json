{"title": [], "abstractContent": [{"text": "We propose anew framework for N-best reranking on sparse feature sets.", "labels": [], "entities": []}, {"text": "The idea is to reformulate the reranking problem as a Multitask Learning problem, where each N-best list corresponds to a distinct task.", "labels": [], "entities": []}, {"text": "This is motivated by the observation that N-best lists often show significant differences in feature distributions.", "labels": [], "entities": []}, {"text": "Training a single reranker directly on this heteroge-nous data can be difficult.", "labels": [], "entities": []}, {"text": "Our proposed meta-algorithm solves this challenge by using multitask learning (such as \u2113 1 /\u2113 2 regularization) to discover common feature representations across N-best lists.", "labels": [], "entities": []}, {"text": "This meta-algorithm is simple to implement, and its modular approach allows one to plug-in different learning algorithms from existing literature.", "labels": [], "entities": []}, {"text": "As a proof of concept, we show statistically significant improvements on a machine translation system involving millions of features.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7283843457698822}]}], "introductionContent": [{"text": "Many natural language processing applications, such as machine translation (MT), parsing, and language modeling, benefit from the N-best reranking framework (.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.8516750395298004}, {"text": "parsing", "start_pos": 81, "end_pos": 88, "type": "TASK", "confidence": 0.7867899537086487}, {"text": "language modeling", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7237712293863297}]}, {"text": "The advantage of N-best reranking is that it abstracts away the complexities of first-pass decoding, allowing the researcher to try new features and learning algorithms with fast experimental turnover.", "labels": [], "entities": []}, {"text": "In the N-best reranking scenario, the training data consists of sets of hypotheses (i.e. N-best lists) generated by a first-pass system, along with their labels.", "labels": [], "entities": []}, {"text": "Given anew N-best list, the goal is to rerank it such that the best hypothesis appears near the top of the list.", "labels": [], "entities": []}, {"text": "Existing research have focused on training a single reranker directly on the entire data.", "labels": [], "entities": []}, {"text": "This approach is reasonable if the data is homogenous, but it fails when features vary significantly across different N-best lists.", "labels": [], "entities": []}, {"text": "In particular, when one employs sparse feature sets, one seldom finds features that are simultaneously active on multiple N-best lists.", "labels": [], "entities": []}, {"text": "In this case, we believe it is more advantageous to view the N-best reranking problem as a multitask learning problem, where each N-best list corresponds to a distinct task.", "labels": [], "entities": []}, {"text": "Multitask learning, a subfield of machine learning, focuses on how to effectively train on a set of different but related datasets (tasks).", "labels": [], "entities": [{"text": "Multitask learning", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8191450238227844}]}, {"text": "Our heterogenous N-best list data fits nicely with this assumption.", "labels": [], "entities": []}, {"text": "The contribution of this work is three-fold: 1.", "labels": [], "entities": []}, {"text": "We introduce the idea of viewing N-best reranking as a multitask learning problem.", "labels": [], "entities": []}, {"text": "This view is particularly apt to any general reranking problem with sparse feature sets.", "labels": [], "entities": []}, {"text": "2. We propose a simple meta-algorithm that first discovers common feature representations across N-bests (via multitask learning) before training a conventional reranker.", "labels": [], "entities": []}, {"text": "Thus it is easily applicable to existing systems.", "labels": [], "entities": []}, {"text": "3. We demonstrate that our proposed method outperforms the conventional reranking approach on a English-Japanese biomedical machine translation task involving millions of features.", "labels": [], "entities": [{"text": "biomedical machine translation task", "start_pos": 113, "end_pos": 148, "type": "TASK", "confidence": 0.7068729028105736}]}, {"text": "The paper is organized as follows: Section 2 describes the feature sparsity problem and Section 3 presents our multitask solution.", "labels": [], "entities": []}, {"text": "The effectiveness of our proposed approach is validated by experiments demonstrated in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Sections 5 and 6 discuss related work and conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "As a proof of concept, we perform experiments on a MT system with millions of features.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9579324722290039}]}, {"text": "We use a hierarchical phrase-based system (Chiang, 2007) to generate N-best lists (N=100).", "labels": [], "entities": []}, {"text": "Sparse features used in reranking are extracted according to.", "labels": [], "entities": [{"text": "Sparse", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9528013467788696}]}, {"text": "Specifically, the majority are lexical features involving joint occurrences of words within the N-best lists and source sentences.", "labels": [], "entities": []}, {"text": "It is worth noting that the fact that the first pass system is a hierarchical system is not essential to the feature extraction step; similar features can be extracted with other systems as first-pass, e.g. a phrase-based system.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7202281057834625}]}, {"text": "That said, the extent of the feature sparsity problem may depend on the performance of the first-pass system.", "labels": [], "entities": []}, {"text": "We experiment with medical domain MT, where large numbers of technical vocabulary cause sparsity challenges.", "labels": [], "entities": [{"text": "medical domain MT", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.4395539164543152}]}, {"text": "Our corpora consists of English abstracts from PubMed 4 with their Japanese translations.", "labels": [], "entities": []}, {"text": "The first-pass system is built on hierarchical phrases extracted from 17k sentence pairs and target (Japanese) language models trained on 800k medical-domain sentences.", "labels": [], "entities": []}, {"text": "For our reranking experiments, we used 500 lists as the training set 5 , 500 lists as held-out, and another 500 for test.", "labels": [], "entities": [{"text": "reranking", "start_pos": 8, "end_pos": 17, "type": "TASK", "confidence": 0.9672474265098572}]}], "tableCaptions": [{"text": " Table 1: Feature growth rate: For N-best list i in  the table, we have (#NewFt = number of new fea- tures introduced since N-best i \u2212 1) ; (#SoFar =  Total number of features defined so far); and (#Ac- tive = number of active features for N-best i). E.g.,  we extracted 7535 new features from N-best 2;", "labels": [], "entities": []}, {"text": " Table 2: Results for different feature sets, with corresponding feature size and train/test BLEU/PER. All  multitask features give statistically significant improvements over the baselines (boldfaced), e.g. Shared  Subspace: 29.1 BLEU vs Baseline: 28.6 BLEU. Combinations of multitask features with high frequency  features also give significant improvements over the high frequency features alone.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9819179773330688}, {"text": "PER", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.7898483276367188}, {"text": "BLEU", "start_pos": 231, "end_pos": 235, "type": "METRIC", "confidence": 0.9946995973587036}, {"text": "BLEU", "start_pos": 254, "end_pos": 258, "type": "METRIC", "confidence": 0.9937015771865845}]}]}