{"title": [{"text": "METEOR-NEXT and the METEOR Paraphrase Tables: Improved Evaluation Support for Five Target Languages", "labels": [], "entities": [{"text": "METEOR-NEXT", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8493970632553101}, {"text": "METEOR Paraphrase Tables", "start_pos": 20, "end_pos": 44, "type": "DATASET", "confidence": 0.6915534635384878}]}], "abstractContent": [{"text": "This paper describes our submission to the WMT10 Shared Evaluation Task and MetricsMATR10.", "labels": [], "entities": [{"text": "WMT10 Shared Evaluation Task", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.5072319805622101}]}, {"text": "We present aversion of the METEOR-NEXT metric with paraphrase tables for five target languages.", "labels": [], "entities": [{"text": "METEOR-NEXT", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.8831192255020142}]}, {"text": "We describe the creation of these paraphrase tables and conduct a tuning experiment that demonstrates consistent improvement across all languages over baseline versions of the metric without paraphrase resources .", "labels": [], "entities": []}], "introductionContent": [{"text": "Workshops such as WMT) and MetricsMATR () focus on the need for accurate automatic metrics for evaluating the quality of machine translation (MT) output.", "labels": [], "entities": [{"text": "WMT", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.6890149712562561}, {"text": "machine translation (MT) output", "start_pos": 121, "end_pos": 152, "type": "TASK", "confidence": 0.8489242196083069}]}, {"text": "While these workshops evaluate metric performance on many target languages, most metrics are limited to English due to the relative lack of lexical resources for other languages.", "labels": [], "entities": []}, {"text": "This paper describes a language-independent method for adding paraphrase support to the METEOR-NEXT metric for all WMT10 target languages.", "labels": [], "entities": [{"text": "METEOR-NEXT", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.7302519083023071}, {"text": "WMT10 target languages", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.605013112227122}]}, {"text": "Taking advantage of the large parallel corpora released for the translation tasks often accompanying evaluation tasks, we automatically construct paraphrase tables using the pivot method ().", "labels": [], "entities": []}, {"text": "We use the WMT09 human evaluation data to tune versions of METEOR-NEXT with and without paraphrases and report significantly better performance for versions with paraphrase support.", "labels": [], "entities": [{"text": "WMT09 human evaluation data", "start_pos": 11, "end_pos": 38, "type": "DATASET", "confidence": 0.8440233916044235}, {"text": "METEOR-NEXT", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.738471269607544}]}], "datasetContent": [{"text": "To evaluate the impact of our paraphrase tables on metric performance, we tune versions of METEOR-NEXT with and without the paraphrase matchers for each language.", "labels": [], "entities": [{"text": "METEOR-NEXT", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.5322131514549255}]}, {"text": "For further comparison, we tune aversion of METEOR-NEXT using the TERp English paraphrase table) used by previous versions of the metric.", "labels": [], "entities": [{"text": "METEOR-NEXT", "start_pos": 44, "end_pos": 55, "type": "METRIC", "confidence": 0.9673557281494141}, {"text": "TERp", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.8297542333602905}]}, {"text": "As shown in, the addition of paraphrases leads to a better tuning point for every target language.", "labels": [], "entities": []}, {"text": "The best scoring subset of paraphrase ta-  Analysis of the phrase matches contributed by the paraphrase matchers reveals an interesting point about the task of paraphrasing for MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 177, "end_pos": 190, "type": "TASK", "confidence": 0.9703762233257294}]}, {"text": "Despite filtering techniques, the final paraphrase tables include some unusual, inaccurate, or highly context-dependent paraphrases.", "labels": [], "entities": []}, {"text": "However, the vast majority of matches identified between actual system output and reference translations correspond to valid paraphrases.", "labels": [], "entities": []}, {"text": "In many cases, the evaluation task itself acts as a final filter; to produce a phrase that can match a spurious paraphrase, not only must a MT system produce incorrect output, but it must produce output that overlaps exactly with an obscure paraphrase of some phrase in the reference translation.", "labels": [], "entities": []}, {"text": "As systems are far more likely to produce phrases with similar words to those in reference translations, far more valid paraphrases exist in typical system output.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sizes of training corpora and phrase ta- bles used for paraphrase extraction", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.8524355888366699}]}, {"text": " Table 2: Sizes of final paraphrase tables", "labels": [], "entities": [{"text": "Sizes", "start_pos": 10, "end_pos": 15, "type": "TASK", "confidence": 0.9631823897361755}]}, {"text": " Table 3: Human ranking judgment data from  WMT09", "labels": [], "entities": [{"text": "Human ranking", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.7382485568523407}, {"text": "WMT09", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.8401121497154236}]}, {"text": " Table 4: Optimal METEOR-NEXT parameters with and without paraphrases for WMT10 target languages", "labels": [], "entities": [{"text": "METEOR-NEXT", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9150264859199524}, {"text": "WMT10 target languages", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.6600064237912496}]}]}