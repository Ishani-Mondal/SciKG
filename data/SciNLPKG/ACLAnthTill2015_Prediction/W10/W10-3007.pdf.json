{"title": [{"text": "Resolving Speculation: MaxEnt Cue Classification and Dependency-Based Scope Rules *", "labels": [], "entities": [{"text": "Resolving Speculation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8722398579120636}, {"text": "MaxEnt Cue Classification", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7021366556485494}]}], "abstractContent": [{"text": "This paper describes a hybrid, two-level approach for resolving hedge cues, the problem of the CoNLL-2010 shared task.", "labels": [], "entities": []}, {"text": "First, a maximum entropy classifier is applied to identify cue words, using both syntactic-and surface-oriented features.", "labels": [], "entities": []}, {"text": "Second, a set of manually crafted rules, operating on dependency representations and the output of the classifier, is applied to resolve the scope of the hedge cues within the sentence.", "labels": [], "entities": []}], "introductionContent": [{"text": "The CoNLL-2010 shared task 1 comprises two sub-tasks.", "labels": [], "entities": [{"text": "CoNLL-2010 shared task 1", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.8205359429121017}]}, {"text": "Task 1 is described as learning to detect sentences containing uncertainty, while the object of Task 2 is learning to resolve the in-sentence scope of hedge cues.", "labels": [], "entities": []}, {"text": "Paralleling this two-fold task definition, the architecture of our system naturally decomposes into two main steps.", "labels": [], "entities": []}, {"text": "First, a maximum entropy (MaxEnt) classifier is applied to automatically detect cue words.", "labels": [], "entities": []}, {"text": "For Task 1, a given sentence is labeled as uncertain if it contains a word classified as a cue.", "labels": [], "entities": []}, {"text": "For Task 2, we then goon to determine the scope of the identified cues using a set of manually crafted rules operating on dependency representations.", "labels": [], "entities": []}, {"text": "For both Task 1 and Task 2, our system participates in the stricter category of 'closed' or 'indomain' systems.", "labels": [], "entities": []}, {"text": "This means that we do not use any additional uncertainty-annotated material beyond the supplied training data, consisting of 14541 sentences from biomedical abstracts and articles (see).", "labels": [], "entities": []}, {"text": "In the official ranking of re-sults, and considering systems in all categories together (closed/open/cross-domain), our system is ranked 4 out of 24 for Task 1 and 3 out of 15 for Task 2, resulting in highest average rank (and F 1 ) overall.", "labels": [], "entities": [{"text": "F 1 )", "start_pos": 227, "end_pos": 232, "type": "METRIC", "confidence": 0.9882521828015646}]}, {"text": "We detail the implementation of the cue classifier and the syntactic rules in Sections 3 and 4, respectively.", "labels": [], "entities": []}, {"text": "Results for the held-out testing are provided in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.8625493049621582}]}, {"text": "First, however, the next section describes the various resources that we used for pre-processing the CoNLL data sets, to prepare the input to our hedge analysis systems.", "labels": [], "entities": [{"text": "CoNLL data sets", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.9305358131726583}]}], "datasetContent": [{"text": "While the training data made available for the shared task consisted of both abstracts and full articles from the BioScope corpus (, the test data were pre-announced to consist of biomedical articles only.", "labels": [], "entities": [{"text": "BioScope corpus", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.9207100570201874}]}, {"text": "In order to make the testing situation during development as similar as possible to what could be expected for the held-out testing, we only tested on sentences taken from the articles part of the training data.", "labels": [], "entities": []}, {"text": "When developing the classifiers we performed 10-fold training and testing over the articles, while always including all sentences from the abstracts in the training set as well.", "labels": [], "entities": []}, {"text": "provides some basic descriptive figures summarizing the training data.", "labels": [], "entities": []}, {"text": "As can be seen in, we will be reporting precision, recall and F-scores for three different levels of evaluation for the cue classifiers: the sentence-level, token-level and cue-level.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9983577132225037}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9986887574195862}, {"text": "F-scores", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9938288331031799}]}, {"text": "The sentence-level scores correspond to Task 1 of the shared task, i.e. correctly identifying sentences as being certain or uncertain.", "labels": [], "entities": []}, {"text": "A sentence is labeled uncertain if it contains at least one token classified as a hedge cue.", "labels": [], "entities": []}, {"text": "The token-level scores indicate how well the classifiers succeed in identifying individual cue words (this score does not take into account the heuristic post-processing rules for finding multi-word cues).", "labels": [], "entities": []}, {"text": "Finally, the cue-level scores are based on the exact-match counts for full hedge cues (possibly spanning multiple tokens).", "labels": [], "entities": []}, {"text": "These latter scores are computed using the official shared task scorer script.", "labels": [], "entities": []}, {"text": "First of all, we may note that the baseline is a strong one: choosing to extend the scope of a cue to the end of the sentence provides an F-score of 45.21.", "labels": [], "entities": [{"text": "F-score", "start_pos": 138, "end_pos": 145, "type": "METRIC", "confidence": 0.9994625449180603}]}, {"text": "Given gold standard cue information, the set of scope rules improves on the baseline by 27 percentage points on the articles section of the data set, giving us an F-score of 72.31.", "labels": [], "entities": [{"text": "F-score", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.9993915557861328}]}, {"text": "Comparing to the evaluation using classified cues (the bottom row of), we find that the use of automatically assigned cues causes a drop in performance of 7.5 percentage points, to a result of 64.77.", "labels": [], "entities": []}, {"text": "presents the final results as obtained on the held-out test data, which constitute the official results for our system in the CoNLL-2010 shared task.", "labels": [], "entities": [{"text": "CoNLL-2010 shared task", "start_pos": 126, "end_pos": 148, "type": "DATASET", "confidence": 0.7752641439437866}]}, {"text": "The held-out test set comprises biomedical articles with a total of 5003 sentences (790 of them hedged).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Enhanced dependency representation of the example sentence The unknown amino acid may  be used by these species with GENIAPoS-tags (P O S), Malt parses (H E A D, D E P R E L) and XLE parses  (X H E A D, X D E P).", "labels": [], "entities": [{"text": "GENIAPoS-tags (P O S)", "start_pos": 127, "end_pos": 148, "type": "METRIC", "confidence": 0.6343045632044474}]}, {"text": " Table 2: Some descriptive figures for the shared task training data. Token-level counts are based on the  tokenization described in Section 2.1.", "labels": [], "entities": []}, {"text": " Table 3: Averaged 10-fold cross-validation results on the articles in the official shared task training data,  always including the abstracts in the training portion. The model listed as final includes features such  as n-grams over surface forms and base forms (both left and right), PoS, subcategorization frames, and  phrase-structural coordination level. The feature types are further described in Section 3.4.", "labels": [], "entities": [{"text": "PoS", "start_pos": 286, "end_pos": 289, "type": "METRIC", "confidence": 0.8228766918182373}]}, {"text": " Table 6: Evaluation results for the official held-out testing.", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9655013680458069}]}, {"text": " Table 5: Evaluation of the scope resolution rules  on the training articles, using both gold standard  cues and predicted cues. For the row labeled De- fault, the scope for each cue is always taken to  span rightward to the end of the sentence. In the  rows labeled Rules, the scopes have been resolved  using the dependency-based rules.", "labels": [], "entities": [{"text": "scope resolution", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7553674578666687}]}]}