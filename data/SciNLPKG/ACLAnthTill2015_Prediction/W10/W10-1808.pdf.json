{"title": [{"text": "To Annotate More Accurately or to Annotate More", "labels": [], "entities": [{"text": "Accurately", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9525662660598755}]}], "abstractContent": [{"text": "The common accepted wisdom is that blind double annotation followed by adju-dication of disagreements is necessary to create training and test corpora that result in the best possible performance.", "labels": [], "entities": []}, {"text": "We provide evidence that this is unlikely to be the case.", "labels": [], "entities": []}, {"text": "Rather, the greatest value for your annotation dollar lies in single annotating more data.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, supervised learning has become the dominant paradigm in Natural Language Processing (NLP), thus making the creation of handannotated corpora a critically important task.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 73, "end_pos": 106, "type": "TASK", "confidence": 0.7258693675200144}]}, {"text": "A corpus where each instance is annotated by a single tagger unavoidably contains errors.", "labels": [], "entities": []}, {"text": "To improve the quality of the data, an annotation project may choose to annotate each instance twice and adjudicate the disagreements, thus producing the (largely) error-free gold standard.", "labels": [], "entities": []}, {"text": "For example,), a large-scale annotation project, chose this option.", "labels": [], "entities": []}, {"text": "However, given a virtually unlimited supply of unlabeled data and limited funding -a typical set of constraints in NLP -an annotation project must always face the realization that for the cost of double annotation, more than twice as much data can be single annotated.", "labels": [], "entities": []}, {"text": "The philosophy behind this alternative says that modern machine learning algorithms can still generalize well in the presence of noise, especially when given larger amounts of training data.", "labels": [], "entities": []}, {"text": "Currently, the commonly accepted wisdom sides with the view that says that blind double annotation followed by adjudication of disagreements is necessary to create annotated corpora that leads to the best possible performance.", "labels": [], "entities": []}, {"text": "We provide empirical evidence that this is unlikely to be the case.", "labels": [], "entities": []}, {"text": "Rather, the greatest value for your annotation dollar lies in single annotating more data.", "labels": [], "entities": []}, {"text": "There may, however, be other considerations that still argue in favor of double annotation.", "labels": [], "entities": []}, {"text": "In this paper, we also consider the arguments of , who suggest that data should be multiply annotated and then filtered to discard all of the examples where the annotators do not have perfect agreement.", "labels": [], "entities": []}, {"text": "We provide evidence that single annotating more data for the same cost is likely to result in better system performance.", "labels": [], "entities": []}, {"text": "This paper proceeds as follows: first, we outline our evaluation framework in Section 2.", "labels": [], "entities": []}, {"text": "Next, we compare the single annotation and adjudication scenarios in Section 3.", "labels": [], "entities": []}, {"text": "Then, we compare the annotation scenario of  with the single annotation scenario in Section 4.", "labels": [], "entities": []}, {"text": "After that, we discuss the results and future work in section 5.", "labels": [], "entities": []}, {"text": "Finally, we draw the conclusion in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of experiment one show that in these circumstances, better performance is achieved by single annotating more data than by deploing resources towards ensuring that the data is annotated more accurately through an adjudication process.", "labels": [], "entities": []}, {"text": "We follow essentially the same experimental design described in section 3.1, using the same state of the art verb WSD system.", "labels": [], "entities": []}, {"text": "We conduct a number of experiments to compare the effect of single annotated versus double annotated data.", "labels": [], "entities": []}, {"text": "We utilized the same training and test sets as the previous experiment and similarly trained an SVM on fractions of the data representing increments of $1.00 investments.", "labels": [], "entities": []}, {"text": "As before, the number of examples designated for single annotation is selected at random from the training data and half of that subset is selected as the training set for the double annotated data.", "labels": [], "entities": []}, {"text": "Again, selecting from the same subset of data results in a more accurate statistical comparison of the models produced.", "labels": [], "entities": []}, {"text": "Classifiers for each annotation scenario are trained on the labels from the first round of annotation, but examples where the second annotator disagreed are thrown out of the double annotated data.", "labels": [], "entities": []}, {"text": "This results in slightly less than half as much data in the double annotation scenario based on the disagreement rate.", "labels": [], "entities": []}, {"text": "Again, the procedure is repeated ten times and the average results are reported.", "labels": [], "entities": []}, {"text": "For a given verb, each classifier created throughout this process is tested on the same double annotated and adjudicated held-out test set.", "labels": [], "entities": []}, {"text": "shows a plot of the accuracy of the classifiers relative to the annotation investment fora typical verb, to call.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9993871450424194}]}, {"text": "As can be seen, the accuracy fora specific investment performing single annotation is always higher than it is for the same investment in double annotated data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9995237588882446}]}, {"text": "shows the average accuracy overall verbs by amount invested.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9995391368865967}]}, {"text": "Again, these accuracy curves are not smooth because the verbs all have a different number of total instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9992376565933228}]}, {"text": "Hence, the accuracy values might jump or drop by a larger amount at the points where a given verb is no longer included in the average.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9991777539253235}]}, {"text": "In this experiment, we consider the arguments of . They suggest that data should beat least double annotated and then filtered to discard all of the examples where there were any annotator disagreements.", "labels": [], "entities": []}, {"text": "The main points of their argument are as follows.", "labels": [], "entities": []}, {"text": "They first consider the data to be dividable into two types, easy (to annotate) cases and hard cases.", "labels": [], "entities": []}, {"text": "Then they correctly note that some annotators could have a systematic bias (i.e., could favor one label over others in certain types of hard cases), which would in turn bias the learning of the classifier.", "labels": [], "entities": []}, {"text": "They show that it is theoretically possible that a band of misclassified hard cases running parallel to the true separating hyperplane could mistakenly shift the decision boundary past up to \u221a N easy cases.", "labels": [], "entities": []}, {"text": "We suggest that it is extremely unlikely that a consequential number of easy cases would exist nearer to the class boundary than the hard cases.", "labels": [], "entities": []}, {"text": "The hard cases are in fact generally considered to define the separating hyperplane.", "labels": [], "entities": []}, {"text": "In this experiment, our goal is to determine how the accuracy of classifiers trained on data labeled according to Beigman and Klebanov's discard disagreements strategy compares empirically to the accuracy resulting from single annotated data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9989468455314636}, {"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.997918426990509}]}, {"text": "As in the previous experiment, this analysis is performed relative to the investment in the annotation effort.", "labels": [], "entities": []}], "tableCaptions": []}