{"title": [{"text": "The Effects of Discourse Connectives Prediction on Implicit Discourse Relation Recognition", "labels": [], "entities": [{"text": "Implicit Discourse Relation Recognition", "start_pos": 51, "end_pos": 90, "type": "TASK", "confidence": 0.7747761011123657}]}], "abstractContent": [{"text": "Implicit discourse relation recognition is difficult due to the absence of explicit discourse connectives between arbitrary spans of text.", "labels": [], "entities": [{"text": "Implicit discourse relation recognition", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6371621415019035}]}, {"text": "In this paper, we use language models to predict the discourse con-nectives between the arguments pair.", "labels": [], "entities": []}, {"text": "We present two methods to apply the predicted connectives to implicit discourse relation recognition.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 61, "end_pos": 100, "type": "TASK", "confidence": 0.5861913114786148}]}, {"text": "One is to use the sense frequency of the specific connec-tives in a supervised framework.", "labels": [], "entities": []}, {"text": "The other is to directly use the presence of the predicted connectives in an unsupervised way.", "labels": [], "entities": []}, {"text": "Results on PDTB2 show that using language model to predict the connectives can achieve comparable F-scores to the previous state-of-art method.", "labels": [], "entities": [{"text": "PDTB2", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.9096233248710632}, {"text": "F-scores", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9986385703086853}]}, {"text": "Our method is quite promising in that not only it has a very small number of features but also once a language model based on other resources is trained it can be more adaptive to other languages and domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse relation analysis involves identifying the discourse relations (e.g., the comparison relation) between arbitrary spans of text, where the discourse connectives (e.g., \"however\", \"because\") mayor may not explicitly exist in the text.", "labels": [], "entities": [{"text": "Discourse relation analysis", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8412014842033386}]}, {"text": "This analysis is one important application both as an end in itself and as an intermediate step in various downstream NLP applications, such as text summarization, question answering etc.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.7822376489639282}, {"text": "question answering", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.9195316433906555}]}, {"text": "This indicates the importance of connectives for discourse relation recognition.", "labels": [], "entities": [{"text": "discourse relation recognition", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.6685858070850372}]}, {"text": "However, with implicit discourse relation recognition, there is no connective between the textual arguments, which results in a very difficult task.", "labels": [], "entities": [{"text": "discourse relation recognition", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.6644080281257629}]}, {"text": "In recent years, a multitude of efforts have been employed to solve this task.", "labels": [], "entities": []}, {"text": "One approach is to exploit various linguistically informed features extracted from human-annotated corpora in a supervised framework) and (.", "labels": [], "entities": []}, {"text": "Another approach is to perform recognition without human-annotated corpora by creating synthetic examples of implicit relations in an unsupervised way (.", "labels": [], "entities": []}, {"text": "Moreover, our initial study on PDTB implicit relation data shows that the averaged F-score for the most general 4 senses can reach 91.8% when we obtain the sense of test examples by mapping each implicit connective to its most frequent sense (i.e., sense recognition using gold-truth implicit connectives).", "labels": [], "entities": [{"text": "PDTB implicit relation data", "start_pos": 31, "end_pos": 58, "type": "DATASET", "confidence": 0.6939119696617126}, {"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9980282187461853}, {"text": "sense recognition", "start_pos": 249, "end_pos": 266, "type": "TASK", "confidence": 0.7185702919960022}]}, {"text": "This high F-score performance again proves that the connectives are very crucial source for implicit relation recognition.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9926921725273132}, {"text": "implicit relation recognition", "start_pos": 92, "end_pos": 121, "type": "TASK", "confidence": 0.664047380288442}]}, {"text": "In this paper, we present anew method to address the problem of recognizing implicit discourse relation.", "labels": [], "entities": []}, {"text": "This method is inspired by the above observations, especially the two gold-truth results, which reveals that discourse connectives are very important signals for discourse relation recognition.", "labels": [], "entities": [{"text": "discourse relation recognition", "start_pos": 162, "end_pos": 192, "type": "TASK", "confidence": 0.6501291791598002}]}, {"text": "Our basic idea is to recover the implicit connectives (not present in real text) between two spans of text with the use of a language model trained on large amount of raw data without any human-annotation.", "labels": [], "entities": []}, {"text": "Then we use these predicted connectives to generate feature vectors in two ways for implicit discourse relation recognition.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 84, "end_pos": 123, "type": "TASK", "confidence": 0.597254678606987}]}, {"text": "One is to use the sense frequency of the specific connectives in a supervised framework.", "labels": [], "entities": []}, {"text": "The other is to directly use the presence of the predicted connectives in an unsupervised way.", "labels": [], "entities": []}, {"text": "We performed evaluation on explicit and implicit relation data sets in the PDTB 2 corpus.", "labels": [], "entities": [{"text": "PDTB 2 corpus", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.9490257302920023}]}, {"text": "Experimental results showed that the two methods achieved comparable F-scores to the state-of-art methods.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9987898468971252}]}, {"text": "It indicates that the method using language model to predict connectives is very useful in solving this task.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work.", "labels": [], "entities": []}, {"text": "Section 3 describes our methods for implicit discourse relation recognition.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 36, "end_pos": 75, "type": "TASK", "confidence": 0.6387260556221008}]}, {"text": "Section 4 presents experiments and results.", "labels": [], "entities": []}, {"text": "Section 5 offers some conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the performance of above systems, we used two widely-used measures, F-score ( i.e., F 1 ) and accuracy.", "labels": [], "entities": [{"text": "F-score", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.998024582862854}, {"text": "F 1 )", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9862543741861979}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9991206526756287}]}, {"text": "In addition, in this work we used the LIBSVM toolkit to construct four linear SVM classifiers for each sense.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of positive and negative instances for each sense in training, development and test sets  of implicit and explicit relation data sets.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of different n-grams in the dif- ferent language models and different corpora.", "labels": [], "entities": []}]}