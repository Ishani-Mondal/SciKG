{"title": [{"text": "Using the Wikipedia Link Structure to Correct the Wikipedia Link Structure", "labels": [], "entities": [{"text": "Wikipedia Link Structure", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.9258020917574564}, {"text": "Wikipedia Link", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9253509342670441}]}], "abstractContent": [{"text": "One of the valuable features of any col-laboratively constructed semantic resource (CSR) is its ability to-as a system-continuously correct itself.", "labels": [], "entities": []}, {"text": "Wikipedia is an excellent example of such a process, with vandalism and misinformation being removed or reverted in astonishing time by a coalition of human editors and machine bots.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9163657426834106}]}, {"text": "However, some errors are harder to spot than others, a problem which can lead to persistent unchecked errors, particularly on more obscure, less viewed article pages.", "labels": [], "entities": []}, {"text": "In this paper we discuss the problems of incorrect link targets in Wikipedia, and propose a method of automatically highlighting and correcting them using only the semantic information found in this encyclopaedia's link structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Wikipedia, despite initial scepticism, is an incredibly robust semantic resource.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9380649328231812}]}, {"text": "Armed with a shared set of standards, legions of volunteers make positive changes to the pages of this vast encyclopaedia everyday.", "labels": [], "entities": []}, {"text": "Some of these editors maybe casual -perhaps noticing an error in a page they were reading and being motivated to correct it -while others actively seek to improve the quality of a wide variety of pages that interest them.", "labels": [], "entities": []}, {"text": "Facilitated by a relatively minimalist set of editing mechanics and incentives, Wikipedia has reached a state in which it is, for the most part, a reliable and stable encyclopaedia.", "labels": [], "entities": []}, {"text": "Just enough regulation to prevent widespread vandalism or inaccuracy (including, on occasion, the temporary locking of particularly controversial pages), and enough editing freedom to maintain accuracy and relevance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9978354573249817}]}, {"text": "There area number of potential approaches to minimizing misinformation and vandalism, falling into two broad categories: adding human incentives, and creating Wiki-crawling bots.", "labels": [], "entities": []}, {"text": "There already exists a wide variety of natural and Wikibased incentives) that have been crucial to the encyclopaedia's success.", "labels": [], "entities": []}, {"text": "By implementing additional incentives, it maybe possible to, for example, increase editor coverage of lessviewed articles.", "labels": [], "entities": []}, {"text": "There are many avenues to explore regarding this, from additional community features such as a reputation system (Adler and, to ideas building upon recent work relating to games with a purpose), providing a form of entertainment that simultaneously aids page maintenance.", "labels": [], "entities": [{"text": "page maintenance", "start_pos": 254, "end_pos": 270, "type": "TASK", "confidence": 0.7292884588241577}]}, {"text": "Wikipedia also benefits from a wide variety of bots and user-assistance tools.", "labels": [], "entities": []}, {"text": "Some make the lives of dedicated editors easier (such as WikiCleaner 1 ), providing an interface that facilitates the detection and correction of errors.", "labels": [], "entities": []}, {"text": "Others carryout repetitive but important tasks, such as ClueBot 2 , an antivandalism bot that reverts various acts of vandalism with surprising speed.", "labels": [], "entities": []}, {"text": "Similar bots have been of great use in not only maintaining existing pages but also in adding new content (such as RamBot , a bot responsible for creating approximately 30,000 U.S city articles).", "labels": [], "entities": [{"text": "RamBot", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.9653513431549072}]}, {"text": "In recent years, researchers have taken an increasing interest in harnessing the semantic data contained in Wikipedia (.", "labels": [], "entities": []}, {"text": "To this end, the encyclopaedia now serves as not only a quick-lookup source for millions of people across the world, but also as an important semantic resource fora wide range of information retrieval, natural language processing and ontology building applications.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 179, "end_pos": 200, "type": "TASK", "confidence": 0.7248522639274597}]}, {"text": "With all this utility, it is increasingly beneficial for Wikipedia to be as accurate and reliable as possible.", "labels": [], "entities": []}, {"text": "In this paper, we will discuss an algorithm that aims to use Wikipedia's inherent link structure to detect and correct errors within that very same structure.", "labels": [], "entities": []}, {"text": "In Section 2 we will explore the nature and causes of this error, outlining the motivations for our algorithm.", "labels": [], "entities": []}, {"text": "Section 3 discusses the inspirations for our approach, as well as our reasons for choosing it.", "labels": [], "entities": []}, {"text": "We will then describe its method in detail, before evaluating its effectiveness and analysing its strengths and weaknesses.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the effectiveness of this algorithm by testing it on a snapshot of Wikipedia from November 2009.", "labels": [], "entities": [{"text": "Wikipedia from November 2009", "start_pos": 80, "end_pos": 108, "type": "DATASET", "confidence": 0.8638192862272263}]}, {"text": "By using old Wikipedia pages we can, inmost cases, easily validate our results against the now-corrected pages of live Wikipedia.", "labels": [], "entities": []}, {"text": "However, finding examples of incorrectly linked articles is no simple task.", "labels": [], "entities": []}, {"text": "Indeed, much of the justification for the algorithm this paper describes stems from the fact that finding these incorrect links is not easy, and actively searching for them is a somewhat tedious task.", "labels": [], "entities": []}, {"text": "While we would like to leave our script crawling across Wikipedia detecting incorrect links by itself, in order to evaluate its performance we need to evaluate how well it performs on a set of pages that are known to contain broken links.", "labels": [], "entities": []}, {"text": "It is impossible to generate such a set automatically, as by their nature these broken links are concerned with the meaning of the text on the pages.", "labels": [], "entities": []}, {"text": "We gauge the performance of our algorithm by looking at how many of the \"best\" suggestions (those with the highest calculated semantic contribution) given fora particular link are, in fact, correct.", "labels": [], "entities": []}], "tableCaptions": []}