{"title": [{"text": "A Proposal fora Configurable Silver Standard", "labels": [], "entities": []}], "abstractContent": [{"text": "Among the many proposals to promote alternatives to costly to create gold standards , just recently the idea of a fully automatically , and thus cheaply, to setup silver standard has been launched.", "labels": [], "entities": []}, {"text": "However, the current construction policy for such a silver standard requires crucial parameters (such as similarity thresholds and agreement cutoffs) to beset a priori, based on extensive testing though, at corpus compile time.", "labels": [], "entities": []}, {"text": "Accordingly, such a corpus is static, once it is released.", "labels": [], "entities": []}, {"text": "We here propose an alternative policy where silver standards can be dynamically optimized and customized on demand (given a specific goal function) using a gold standard as an oracle.", "labels": [], "entities": []}], "introductionContent": [{"text": "Training natural language systems which rely on (semi-)supervised machine learning algorithms, or measuring the systems' performance requires some standardized ground truth from which one can learn or against which one evaluate, respectively.", "labels": [], "entities": []}, {"text": "Usually, a manually crafted gold standard is provided that is generated by human language or domain experts after lots of iterative, guideline-based training rounds.", "labels": [], "entities": []}, {"text": "This procedure is expensive, slow and yields only small, yet highly trustable, amounts of metadata -because human experts are in the loop.", "labels": [], "entities": []}, {"text": "In the CALBC project, an alternative approach is currently under investigation (Rebholz-).", "labels": [], "entities": [{"text": "CALBC project", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.7796013951301575}]}, {"text": "The basic idea is to generate the much needed ground truth automatically.", "labels": [], "entities": []}, {"text": "This is achieved by letting a flock of named entity taggers run on a corpus, without imposing any restriction on the type(s) being annotated.", "labels": [], "entities": []}, {"text": "The (most likely) heterogeneous results are automatically homogenized subsequently, thus yielding a consensus-based, machine-generated ground truth.", "labels": [], "entities": []}, {"text": "Considering the possible benefits (e.g., the positive experience from boosting-style machine learners), but also being aware of the possible drawbacks (varying quality of the different systems, skewed coverage of entity types, different types of guidelines on which they were trained, etc.), the CALBC consortium refers to the outcome of this process as a silver standard (Rebholz-).", "labels": [], "entities": []}, {"text": "This procedure is inexpensive, fast, yields huge amounts of metadata -because computers are in the loopbut after all its applicability and validity has yet to be determined experimentally.", "labels": [], "entities": [{"text": "validity", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9875139594078064}]}, {"text": "The first silver standard corpus (SSC) that came out of the CALBC project was generated by the four main partners' named entity taggers.", "labels": [], "entities": []}, {"text": "The various contributions covered, among others, annotations for genes and proteins, chemicals, diseases, etc (Rebholz-).", "labels": [], "entities": []}, {"text": "After the submission of their runs, the SSC was generated by, first, harmonizing stretches of text in terms of entity mention identification and, second, by mapping these normalized mentions to agreedupon type systems (such as the MESH Semantic Groups as described by for entity type normalization).", "labels": [], "entities": [{"text": "entity mention identification", "start_pos": 111, "end_pos": 140, "type": "TASK", "confidence": 0.6492640574773153}, {"text": "MESH Semantic Groups", "start_pos": 231, "end_pos": 251, "type": "DATASET", "confidence": 0.8216523925463358}, {"text": "entity type normalization", "start_pos": 272, "end_pos": 297, "type": "TASK", "confidence": 0.6378175417582194}]}, {"text": "Basically, the harmonization steps included rules when entity mentions were considered to match or overlap (using a cosine-based similarity criterion) and entity types referred to the same class.", "labels": [], "entities": []}, {"text": "For consensus generation, finally, simple rules for majority votes were established.", "labels": [], "entities": [{"text": "consensus generation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.9657339751720428}]}, {"text": "The CALBC consortium is fully aware of the fact that the value of an SSC can only be assessed by comparing, e.g., systems trained on such a silver standard with systems trained on a gold standard (preferably, though not necessarily, one that is a subset of the document set which makes up the SSC).", "labels": [], "entities": []}, {"text": "In the absence of such a gold standard, the CALBC consortium has spent enormous efforts to find out the most reasonable parameter settings for, e.g., the cosine threshold (setting similar mentions apart from dissimilar ones) or the consensus constraint (where a certain number of entity types equally assigned by different taggers makes one type the consensual silver one and discards all alternative annotations).", "labels": [], "entities": []}, {"text": "Once these criteria are made effective, the SSC is completely fixed.", "labels": [], "entities": [{"text": "SSC", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.6202810406684875}]}, {"text": "As an alternative, we are looking fora more flexible solution.", "labels": [], "entities": []}, {"text": "Our investigation was fuelled by the following observations: \u2022 The idiosyncrasies of guidelines (on which (some) taggers were trained) do not necessarily lead to semantically totally different entities although they differ literally to some degree.", "labels": [], "entities": []}, {"text": "Some guidelines prefer, e.g., \"human IL-7 protein\", others favor \"human IL-7\", and some lean towards \"IL-7\".", "labels": [], "entities": []}, {"text": "As the cosine measure tends to penalize a pair such as \"human IL-7 protein\" and \"IL-7\", we intended to avoid such a prescriptive mode and just look at the type assignment for single tokens as (parts of) entity mentions.", "labels": [], "entities": []}, {"text": "thus avoiding inconclusive mention boundary discussions.", "labels": [], "entities": []}, {"text": "\u2022 While we were counting, for all tokens of the document set, the votes a single token received from different taggers in terms of annotating this token with respect to some type, we generated confidence data for metadata assignments.", "labels": [], "entities": []}, {"text": "Incorporating the distribution of confidence values into the configuration process, this allows us to get rid of a priori fixed majority criteria (e.g., two or three out of five systems must agree on this token) which are hard to justify in an absolute way.", "labels": [], "entities": []}, {"text": "Summarizing, we believe that the nature of diverging tasks to be solved, the levels of entity type specificity to be reached, the sort of guidelines being preferred, etc.", "labels": [], "entities": []}, {"text": "should allow prospective users of a silver standard to customize one on their own and not stick to one that is already prefabricated without concrete application in mind.", "labels": [], "entities": []}, {"text": "3 There maybe tasks where a \"long\" entity such as \"huAs such an enterprise would be quite arbitrary without a reference standard, we even go one step further.", "labels": [], "entities": []}, {"text": "We determine the suitability of, say, different voting scores and varying lexical extensions of mentions by comparison to a gold standard so that the 'optimal' configuration of a silver standard, given a set of goal-derived requirements, can be automatically learned.", "labels": [], "entities": []}, {"text": "In real-world applications, such gold standard annotations would be delivered only fora fraction of the documents contained in the entire corpus being tagged by a flock of taggers.", "labels": [], "entities": []}, {"text": "The gold standard is used to optimize parameters which are subsequently applied to the aggregation of automatically annotated data.", "labels": [], "entities": []}, {"text": "Note that the gold standard is used for optimization only, not for training.", "labels": [], "entities": []}, {"text": "We call such a flexible, dynamically adjustable silver standard a configurable Silver Standard Corpus (conSSC).", "labels": [], "entities": [{"text": "Silver Standard Corpus (conSSC)", "start_pos": 79, "end_pos": 110, "type": "DATASET", "confidence": 0.7672475328048071}]}, {"text": "Ina second step, we split the various conSSCs, re-trained our NER tagger on these data sets and, by comparison with the gold standard, were able to identify the optimal conSSC for this task (which is not the one (SSC I) made available by the CALBC consortium for the first challenge round).", "labels": [], "entities": []}], "datasetContent": [{"text": "The following metrics were used to evaluate how good the silver standard(s) fit(s) the provided gold standard: \u2022 segment-level recall, precision, and F-score values with exact boundaries, the standard way to evaluate NER taggers, \u2022 segment-level recall, precision, and F-score, but with relaxed boundary constraints.", "labels": [], "entities": [{"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9351532459259033}, {"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9959796667098999}, {"text": "F-score", "start_pos": 150, "end_pos": 157, "type": "METRIC", "confidence": 0.9944174289703369}, {"text": "NER taggers", "start_pos": 217, "end_pos": 228, "type": "TASK", "confidence": 0.9389806091785431}, {"text": "recall", "start_pos": 246, "end_pos": 252, "type": "METRIC", "confidence": 0.8668229579925537}, {"text": "precision", "start_pos": 254, "end_pos": 263, "type": "METRIC", "confidence": 0.9960736036300659}, {"text": "F-score", "start_pos": 269, "end_pos": 276, "type": "METRIC", "confidence": 0.9979254007339478}]}, {"text": "This means that two entity mentions are considered to match when they overlap with at least one token and have the same entity type assigned to them, \u2022 accuracy measured on the token level.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9995962977409363}]}, {"text": "These metrics can be considered as optimization criteria.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of single systems (SYS-1 to SYS-5) as evaluated against the gold standard (best  performance scores in bold face). Measurements are taken both for exact as well as overlapping recall  (R), precision (P) and F-score (F", "labels": [], "entities": [{"text": "recall  (R)", "start_pos": 198, "end_pos": 209, "type": "METRIC", "confidence": 0.9553739279508591}, {"text": "precision (P)", "start_pos": 211, "end_pos": 224, "type": "METRIC", "confidence": 0.9582588076591492}, {"text": "F-score", "start_pos": 229, "end_pos": 236, "type": "METRIC", "confidence": 0.9964890480041504}]}, {"text": " Table 3: Merged annotations of the entire crowd of CALBC taggers (best performance scores per param- eter setting in bold face). Parameters: threshold (confidence or cosine) and number of agreeing systems  (agr. systems).", "labels": [], "entities": [{"text": "CALBC taggers", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.6474460959434509}, {"text": "Parameters", "start_pos": 130, "end_pos": 140, "type": "METRIC", "confidence": 0.9813902974128723}]}, {"text": " Table 4: Twin pairs of taggers, contrasting the two best (in bold face) and the two worst performing pairs  obtained by the confidence method.", "labels": [], "entities": []}, {"text": " Table 5: Performance of an NER tagger trained on an SSC, 10-fold cross validation, and all systems.  Parameters: threshold (confidence or cosine) and number of agreeing systems (agr. systems).", "labels": [], "entities": [{"text": "NER tagger", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.9481974542140961}, {"text": "Parameters", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9657980799674988}]}]}