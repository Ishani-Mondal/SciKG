{"title": [{"text": "Application of the Tightness Continuum Measure to Chinese Information Retrieval", "labels": [], "entities": [{"text": "Chinese Information Retrieval", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.635489821434021}]}], "abstractContent": [{"text": "Most word segmentation methods employed in Chinese Information Retrieval systems are based on a static dictionary or a model trained against a manually segmented corpus.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7403406798839569}, {"text": "Chinese Information Retrieval", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.5508343180020651}]}, {"text": "These general seg-mentation approaches may not be optimal because they disregard information within semantic units.", "labels": [], "entities": []}, {"text": "We propose a novel method for improving word-based Chi-nese IR, which performs segmentation according to the tightness of phrases.", "labels": [], "entities": [{"text": "word-based Chi-nese IR", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.5219326118628184}]}, {"text": "In order to evaluate the effectiveness of our method, we employ anew test collection of 203 queries, which include abroad distribution of phrases with different tight-ness values.", "labels": [], "entities": []}, {"text": "The results of our experiments indicate that our method improves IR performance as compared with a general word segmentation approach.", "labels": [], "entities": [{"text": "IR", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9919420480728149}, {"text": "word segmentation", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.7332173585891724}]}, {"text": "The experiments also demonstrate the need for the development of better evaluation corpora .", "labels": [], "entities": []}], "introductionContent": [{"text": "What distinguishes Chinese Information Retrieval from information retrieval (IR) in other languages is the challenge of segmenting the queries and the documents, created by the lack of word delimiters.", "labels": [], "entities": [{"text": "Chinese Information Retrieval", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.5844817658265432}, {"text": "information retrieval (IR)", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.8400977373123169}]}, {"text": "In general, there are two categories of segmenters: character-based methods and word-based methods.", "labels": [], "entities": []}, {"text": "Despite the superior performance of bigram segmenters (), word-based approaches continue to be investigated because of their application in sophisticated IR tasks such as cross language IR, and within techniques such as query expansion ().", "labels": [], "entities": [{"text": "query expansion", "start_pos": 220, "end_pos": 235, "type": "TASK", "confidence": 0.7405711710453033}]}, {"text": "Most word-based segmenters in Chinese IR are either rule-based models, which rely on a lexicon, or statistical-based models, which are trained on manually segmented corpora (.", "labels": [], "entities": [{"text": "IR", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.6312950253486633}]}, {"text": "However, the relationship between the accuracy of Chinese word segmentation and the performance of Chinese IR is non-monotonic.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9987271428108215}, {"text": "Chinese word segmentation", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.5678155918916067}]}, {"text": "reported that segmentation methods achieving segmentation accuracy higher than 90% according to a manual segmentation standard yield no improvement in IR performance.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.9473270773887634}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.8260933756828308}, {"text": "IR", "start_pos": 151, "end_pos": 153, "type": "TASK", "confidence": 0.983430027961731}]}, {"text": "They further argued that IR often benefits from splitting compound words that are annotated as single units by manual segmentation.", "labels": [], "entities": [{"text": "IR", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9936779141426086}]}, {"text": "The essence of the problem is that there is no clear definition of word in Chinese.", "labels": [], "entities": []}, {"text": "Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7399599254131317}]}, {"text": "While units such as \"\" (peanut) and \"\" (match maker) should clearly be considered as a single term in Chinese IR, compounds such as \"\" (machine learning) are more controversial.", "labels": [], "entities": [{"text": "IR", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.7176498174667358}]}, {"text": "1 proposed a \"continuum hypothesis\" that rejects a clean binary classification of Chinese semantic units as either compositional or non-compositional.", "labels": [], "entities": []}, {"text": "Instead, they introduced the notion of a tightness measure, which quantifies the degree of compositionality.", "labels": [], "entities": []}, {"text": "On this tightness continuum, atone extreme are non-compositional semantic units, such as \" \" (match maker), and at the other end are sequences of consecutive words with no dependency relationship, such as \"\" (Shanghai where).", "labels": [], "entities": []}, {"text": "In the middle of the spectrum are compositional compounds such as \" \" (machine learning) and phrases such as \" \" (legitimate income).", "labels": [], "entities": []}, {"text": "In this paper, we propose a method to apply the concept of semantic tightness to Chinese IR, which refines the output of a general Chinese word segmenter using tightness information.", "labels": [], "entities": []}, {"text": "In the first phase, we re-combine multiple units that are considered semantically tight into single terms.", "labels": [], "entities": []}, {"text": "In the second phase, we break single units that are not sufficiently tight.", "labels": [], "entities": []}, {"text": "The experiments involving two different IR systems demonstrate that the new method improves IR performance as compared to the general segmenter.", "labels": [], "entities": [{"text": "IR", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.9692157506942749}]}, {"text": "Most Chinese IR systems are evaluated on the data from the TREC 5 and TREC 6 competitions (.", "labels": [], "entities": [{"text": "IR", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.8913084864616394}, {"text": "TREC 5 and TREC 6 competitions", "start_pos": 59, "end_pos": 89, "type": "DATASET", "confidence": 0.8347154557704926}]}, {"text": "That data contains only 54 queries, which are linked to relevancyjudged documents.", "labels": [], "entities": []}, {"text": "During our experiments, we found the TREC query data is ill-suited for analyzing the effects of compound segmentation on Chinese IR.", "labels": [], "entities": [{"text": "TREC query data", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.7498576045036316}]}, {"text": "For this reason, we created an additional set of queries based on the TREC corpus, which includes a wide variety of semantic compounds.", "labels": [], "entities": [{"text": "TREC corpus", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.9142449796199799}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "After summarizing related work on Chinese IR and word segmentation studies, we introduce the measure of semantic tightness.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.6762521415948868}]}, {"text": "Section 4 describes the integration of the semantic tightness measure into an IR system.", "labels": [], "entities": [{"text": "IR", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.943267822265625}]}, {"text": "Section 5 discusses the available data for Chinese IR evaluation, as well as an approach to acquire new data.", "labels": [], "entities": [{"text": "IR evaluation", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.8468961715698242}]}, {"text": "Section 6 presents the results of our method on word segmentation and IR.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.770770937204361}, {"text": "IR", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.984819769859314}]}, {"text": "A short conclusion wraps up and gives directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted a series of experiments in wordbased Chinese information retrieval, with the aim of establishing which segmenter is best for CIR, while pursuing the best segmentation performance in terms of segmented corpus is not the main crux.", "labels": [], "entities": [{"text": "wordbased Chinese information retrieval", "start_pos": 40, "end_pos": 79, "type": "TASK", "confidence": 0.5309487655758858}]}, {"text": "In this section, we first present the accuracy of different segmentation methods, and then discuss the results of IR systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9989427924156189}, {"text": "segmentation", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.9619777798652649}, {"text": "IR", "start_pos": 114, "end_pos": 116, "type": "TASK", "confidence": 0.9889740943908691}]}, {"text": "We conducted our information retrieval experiments using the Lucene package).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.819086343050003}, {"text": "Lucene package", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.9720439612865448}]}, {"text": "The documents and queries were segmented by our three approaches before indexing and searching process.", "labels": [], "entities": []}, {"text": "In order to analyze the performance of our segmentation methods with different retrieval systems, we employed two score functions: the BM25 function (Peng et al., 2002b) 4 ; and BM25Beta (Function 4), which prefers documents with more query terms.", "labels": [], "entities": [{"text": "BM25 function", "start_pos": 135, "end_pos": 148, "type": "METRIC", "confidence": 0.7678153216838837}, {"text": "BM25Beta", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.580485463142395}]}, {"text": "In the above equation, score(t i , D) is the score of the term ti in the document D.", "labels": [], "entities": [{"text": "score(t i , D)", "start_pos": 23, "end_pos": 37, "type": "METRIC", "confidence": 0.8324578148978097}]}, {"text": "Although we used BM25 as our base score function for score(t i , D), it can be replaced by other score functions, such as tf*idf, or a probability language model.", "labels": [], "entities": [{"text": "BM25", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.5400806069374084}]}, {"text": "\u03b2 is a parameter to control a penalty component for those documents that do not contain all the query terms; T is the number of distinctive query terms in the document; and N is the number of query terms.", "labels": [], "entities": []}, {"text": "The function penalizes documents that do not contain all the query terms, which is an indirect way of incorporating proximity distance 5 . shows the comparison of our two segmenters to ICTCLAS on the IR task.", "labels": [], "entities": [{"text": "IR task", "start_pos": 200, "end_pos": 207, "type": "TASK", "confidence": 0.8949297070503235}]}, {"text": "The performance of IR systems was measured by mean average precision (MAP) of the query set.", "labels": [], "entities": [{"text": "IR", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9815571308135986}, {"text": "mean average precision (MAP)", "start_pos": 46, "end_pos": 74, "type": "METRIC", "confidence": 0.9318046967188517}]}, {"text": "The results show that Tight Combine is better than the ICTCLAS segmentation, especially when using BM25.", "labels": [], "entities": [{"text": "Tight Combine", "start_pos": 22, "end_pos": 35, "type": "METRIC", "confidence": 0.8329479694366455}, {"text": "ICTCLAS segmentation", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.5920856148004532}, {"text": "BM25", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.9566474556922913}]}, {"text": "The relationship between Tight Split and ICTCLAS is not clear.", "labels": [], "entities": [{"text": "Tight Split", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.7070022523403168}, {"text": "ICTCLAS", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.7687354683876038}]}, {"text": "In order to give a more in-depth analysis of the word segmentation methods with respect to the targeted phenomenon of semantic units, we classified the 200 queries into three categories according to their tightness as measured by function 1.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7144109010696411}]}, {"text": "The three classes are queries with tightness in ranges,, and [1, 0), which contain 54, 41, and 108 queries respectively.", "labels": [], "entities": []}, {"text": "Queries in the range [+\u221e, 10) are tight queries, such as \"\" (Virginia).", "labels": [], "entities": []}, {"text": "Queries in the range [1, 0) are loose queries, such as \" \" (advertising company).", "labels": [], "entities": []}, {"text": "Other queries are those compounds which have ambiguous segmentations, such as \"\" (chain reaction).", "labels": [], "entities": []}, {"text": "Because the classification was based on the tightness measure, there are some errors.", "labels": [], "entities": []}, {"text": "For example, \" \" (Renmin University) was classified as a loose query although it should at least be in the middle range.", "labels": [], "entities": [{"text": "Renmin University)", "start_pos": 18, "end_pos": 36, "type": "DATASET", "confidence": 0.9298029939333597}]}, {"text": "The three classes cover the whole tightness continuum, i.e. the whole possible query set.", "labels": [], "entities": []}, {"text": "shows the MAP with respect to these classes for the word segmentation methods.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9537501931190491}, {"text": "word segmentation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7144978195428848}]}, {"text": "For queries with tightness less than 10, the results of ICTCLAS and Tight Combine are approximately equal, which is not surprising since with few ex- We also experimented with replacing \u03b2 with the tightness value, but the results were not substantially different.", "labels": [], "entities": [{"text": "ICTCLAS", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.6194334030151367}]}, {"text": "Results on three query categories.", "labels": [], "entities": []}, {"text": "ceptions they have the same segmentation for both queries and documents.", "labels": [], "entities": []}, {"text": "For the interesting case of segmentation of tight units, i.e. queries in the range, the results show clear superiority for IR systems based on our segmentation methods.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.9775758981704712}, {"text": "IR", "start_pos": 123, "end_pos": 125, "type": "TASK", "confidence": 0.976129412651062}]}, {"text": "When using BM25, MAP is 86.44% for Tight Combine, as compared to 74.48% for standard word segmentation . The advantage of Tight Combine over ICTCLAS is that it combines units such as \" \" (plate glass) as the term is tight, while ICTCLAS segments that unit into \"\" (plate) and \"\" (glass).", "labels": [], "entities": [{"text": "BM25", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.8685178160667419}, {"text": "MAP", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9954195022583008}, {"text": "word segmentation", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7496649324893951}]}, {"text": "This is evidence that word segmentation models based on the tight measure are better than models trained on a human annotated corpus which ignored tightness information.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7672699093818665}]}, {"text": "Interestingly, Tight Split is superior in the range, although the segmentation for these queries is the same as with Tight Combine.", "labels": [], "entities": []}, {"text": "When we analyzed the instances, we found it improved IR results of proper nouns.", "labels": [], "entities": [{"text": "IR", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9004844427108765}]}, {"text": "One possible explanation is that splitting of proper nouns such as \"\" (Virginia state) in documents improved the recall even when the segmentation of the queries remained the same.", "labels": [], "entities": [{"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.99326092004776}]}, {"text": "For example, for query \"\" (Virginia), documents which contain \"\" (Virginia state) should be retrieved.", "labels": [], "entities": []}, {"text": "However, since ICTCLAS treats \" \" as a word, those documents are missed.", "labels": [], "entities": [{"text": "ICTCLAS", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.6945245862007141}]}, {"text": "Instead, Tight Split segments the sequence into \"|,\" which results in the retrieval of those documents.", "labels": [], "entities": []}, {"text": "In the range of [10, 1), the result is mixed.", "labels": [], "entities": []}, {"text": "For some instances, Tight Split is worse than Tight Combine and ICTCLAS, as it segments queries such as \" \" (chain reaction).", "labels": [], "entities": [{"text": "Tight Split", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.4397581070661545}, {"text": "ICTCLAS", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.7028440237045288}]}, {"text": "However, in other instances, it is better than Tight Combine and ICTCLAS since it segments queries such as \"\" (international chess).", "labels": [], "entities": [{"text": "ICTCLAS", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.7363731265068054}]}, {"text": "The result suggests that the setting of the threshold for non-compositional terms should be below 10.", "labels": [], "entities": []}, {"text": "In the range of, the result is also mixed.", "labels": [], "entities": []}, {"text": "One reason for the low performance of Tight Split is that the tightness measure is not precise for those queries, which affects the segmentation.", "labels": [], "entities": [{"text": "Tight Split", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.7005983293056488}]}, {"text": "For example, splitting the queries \"\" (labor movement) and \"\" (Zhongshan University) decreases the IR performance dramatically.", "labels": [], "entities": [{"text": "Zhongshan University)", "start_pos": 63, "end_pos": 84, "type": "DATASET", "confidence": 0.9236675500869751}, {"text": "IR", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9800167083740234}]}, {"text": "In future work, we would like to investigate this problem by segmenting queries manually according to their tightness.", "labels": [], "entities": []}, {"text": "If the manual segmentation is superior, it would provided evidence for the hypothesis that segmentation based on tightness is superior.", "labels": [], "entities": []}, {"text": "The difference between BM25 and BM25 Beta in the range suggests that for Chinese IR, it is better to segment text in a more finegrained way, and combine terms through a score function.", "labels": [], "entities": []}, {"text": "For example, for queries such as \" \" (chain reaction), for which splitting the unit is worse, BM25 Beta decreases the negative effect of splitting dramatically.", "labels": [], "entities": [{"text": "BM25 Beta", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.8874963223934174}]}, {"text": "For the query \"\" (life insurance), when using BM25, Tight Split is worse than ICTCLAS (average precision 0.59 vs. 0.66); but when using BM25 Beta, it is better than ICTCLAS (average precision 0.72 vs. 0.66).", "labels": [], "entities": [{"text": "BM25", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.963630199432373}, {"text": "Tight Split", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.969475507736206}, {"text": "ICTCLAS", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.5705695152282715}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.6834724545478821}, {"text": "BM25", "start_pos": 136, "end_pos": 140, "type": "DATASET", "confidence": 0.9039574861526489}, {"text": "ICTCLAS", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.6165393590927124}]}], "tableCaptions": [{"text": " Table 2. MAP of different IR systems with differ- ent segmenters.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.514784038066864}, {"text": "IR", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9685123562812805}]}, {"text": " Table 3. Results on three query categories.", "labels": [], "entities": []}]}