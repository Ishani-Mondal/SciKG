{"title": [{"text": "An Double Hidden HMM and an CRF for Segmentation Tasks with Pinyin's Finals", "labels": [], "entities": [{"text": "CRF", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.8673365712165833}]}], "abstractContent": [{"text": "We have participated in the open tracks and closed tracks on four corpora of Chi-nese word segmentation tasks in CIPS-SIGHAN-2010 Bake-offs.", "labels": [], "entities": [{"text": "Chi-nese word segmentation tasks", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.7628681808710098}, {"text": "CIPS-SIGHAN-2010 Bake-offs", "start_pos": 113, "end_pos": 139, "type": "DATASET", "confidence": 0.8732264637947083}]}, {"text": "In our experiments , we used the Chinese inner phonology information in all tracks.", "labels": [], "entities": []}, {"text": "For open tracks, we proposed a double hidden lay-ers' HMM (DHHMM) in which Chinese inner phonology information was used as one hidden layer and the BIO tags as another hidden layer.", "labels": [], "entities": []}, {"text": "N-best results were firstly generated by using DHHMM, then the best one was selected by using anew lexical statistic measure.", "labels": [], "entities": [{"text": "DHHMM", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.725931704044342}]}, {"text": "For close tracks, we used CRF model in which the Chinese inner phonology information was used as features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Chinese language has many characteristics not possessed by other languages.", "labels": [], "entities": []}, {"text": "One obvious is that the written Chinese text does not have explicit word boundaries like western languages.", "labels": [], "entities": []}, {"text": "So word segmentation became very significative for Chinese information processing, and is usually considered as the first step of any further processing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7170246541500092}, {"text": "information processing", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.750063568353653}]}, {"text": "Identifying words has been a basic task for many researchers who have devoted themselves on Chinese text processing.", "labels": [], "entities": [{"text": "Identifying words", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8942655324935913}, {"text": "Chinese text processing", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.5967598756154379}]}, {"text": "The biggest characteristic of Chinese language is its trinity of sound, form and meaning).", "labels": [], "entities": []}, {"text": "Hanyu Pinyin is the form of sound for Chinese text and the Chinese phonology information is explicit expressed by Pinyin which is the inner features of Chinese Characters.", "labels": [], "entities": []}, {"text": "And it naturally contributes to the identification of Out-OfVacabulary words (OOV).", "labels": [], "entities": [{"text": "identification of Out-OfVacabulary words (OOV)", "start_pos": 36, "end_pos": 82, "type": "TASK", "confidence": 0.8478874904768807}]}, {"text": "In our work, Chinese phonology information is used as basic features of Chinese characters in all models.", "labels": [], "entities": []}, {"text": "For open tracks, we propose anew double hidden layers HMM in which anew phonology information is builtin as a hidden layer, anew lexical association is proposed to deal with the OOV questions and domains' adaptation questions.", "labels": [], "entities": []}, {"text": "And for closed tracks, CRF model has been used , combined with Chinese inner phonology information.", "labels": [], "entities": [{"text": "CRF", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.8524583578109741}]}, {"text": "We used the CRF++ package Version 0.43 by Taku Kudo 1 . In the rest sections of this paper, we firstly introduce the Chinese phonology in Section 2.", "labels": [], "entities": [{"text": "CRF++ package Version 0.43", "start_pos": 12, "end_pos": 38, "type": "DATASET", "confidence": 0.8855531692504883}]}, {"text": "Then in the Section 3, the models used in our tasks are presented.", "labels": [], "entities": []}, {"text": "And the experiments and results are described in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we give the conclusions and make prospect on future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We build a basic words dictionary for DHHMM and a Pinyin's finals dictionary for both DHHMM and CRF from The Grammatical Knowledge-base of Contemporary Chinese).", "labels": [], "entities": [{"text": "DHHMM", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.9545872211456299}, {"text": "DHHMM and CRF from The Grammatical Knowledge-base of Contemporary Chinese", "start_pos": 86, "end_pos": 159, "type": "DATASET", "confidence": 0.8371929436922073}]}, {"text": "For the finals dictionary, we give each Chinese character a final extracted from its Pinyin.", "labels": [], "entities": []}, {"text": "When it comes to a polyphone, we just combine its all finals simply to one.", "labels": [], "entities": []}, {"text": "For example, \"\u4e2d{ong}\", \"\u5dee{a&ai&i}\".", "labels": [], "entities": []}, {"text": "The training corpus (5,769 KB) we used is the Labeled Corpus provided by the organizer.", "labels": [], "entities": []}, {"text": "We firstly add the Pinyin's finals to each Chinese character of it, then we train the parameters of DHHMM and CRF model on it.", "labels": [], "entities": [{"text": "DHHMM", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.5881428718566895}]}, {"text": "And the test corpus contains four domains: Literature (A), Computer (B), Medicine (C) and Finance(D).", "labels": [], "entities": []}, {"text": "The LLR function's parameters{a, b, c, d} are counted from the current test corpus A, B, C, or D.", "labels": [], "entities": []}, {"text": "It's means that for segmenting A, the LLR parameters are counted from A, so the same for segmenting B, C and D.", "labels": [], "entities": [{"text": "segmenting A", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.8814484179019928}, {"text": "segmenting B", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.8647983074188232}]}], "tableCaptions": [{"text": " Table 3: Results of open tracks using DHHMM:  Literature (A), Computer (B), Medicine (C) and  Finance(D)", "labels": [], "entities": [{"text": "DHHMM", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.935229480266571}]}, {"text": " Table 4: Results of closed tracks using CRF: Lit- erature (A), Computer (B), Medicine (C) and Fi- nance(D)", "labels": [], "entities": []}, {"text": " Table 5: The computation cost in DHHMM and  CRF", "labels": [], "entities": [{"text": "DHHMM", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.7934212684631348}, {"text": "CRF", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.5126747488975525}]}]}