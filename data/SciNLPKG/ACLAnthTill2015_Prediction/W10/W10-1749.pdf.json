{"title": [{"text": "LRscore for Evaluating Lexical and Reordering Quality in MT", "labels": [], "entities": [{"text": "LRscore", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7240096926689148}, {"text": "Evaluating Lexical and Reordering", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.8764331191778183}, {"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.8390291333198547}]}], "abstractContent": [{"text": "The ability to measure the quality of word order in translations is an important goal for research in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7896369993686676}]}, {"text": "Current machine translation metrics do not adequately measure the reordering performance of translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7631707787513733}]}, {"text": "We present a novel metric, the LRscore, which directly measures reordering success.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.6964958906173706}]}, {"text": "The reordering component is balanced by a lexical metric.", "labels": [], "entities": []}, {"text": "Capturing the two most important elements of translation success in a simple combined metric with only one parameter results in an intuitive, shallow, language independent metric.", "labels": [], "entities": [{"text": "translation", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.9622402191162109}]}], "introductionContent": [{"text": "The main purpose of MT evaluation is to determine \"to what extent the makers of a system have succeeded in mimicking the human translator\".", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.9846229553222656}]}, {"text": "But machine translation has no \"ground truth\" as there are many possible correct translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7688334286212921}]}, {"text": "It is impossible to judge whether a translation is incorrect or simply unknown and it is even harder to judge the the degree to which it is incorrect.", "labels": [], "entities": []}, {"text": "Even so, automatic metrics are necessary.", "labels": [], "entities": []}, {"text": "It is nearly impossible to collect enough human judgments for evaluating incremental improvements in research systems, or for tuning statistical machine translation system parameters.", "labels": [], "entities": []}, {"text": "Automatic metrics are also much faster and cheaper than human evaluation and they produce reproducible results.", "labels": [], "entities": []}, {"text": "Machine translation research relies heavily upon automatic metrics to evaluate the performance of models.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.868451863527298}]}, {"text": "However, current metrics rely upon indirect methods for measuring the quality of the word order, and their ability to capture reordering performance has been demonstrated to be poor (.", "labels": [], "entities": []}, {"text": "There are two main approaches to capturing reordering.", "labels": [], "entities": [{"text": "capturing reordering", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.924178808927536}]}, {"text": "The first way to measure the quality of word order is to count the number of matching n-grams between the reference and the hypothesis.", "labels": [], "entities": []}, {"text": "This is the approach taken by the BLEU score ().", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.948585569858551}]}, {"text": "This method discounts any n-gram which is not identical to a reference n-gram, and also does not consider the relative position of the strings.", "labels": [], "entities": []}, {"text": "They can be anywhere in the sentence.", "labels": [], "entities": []}, {"text": "Another common approach is typified by METEOR () and TER ().", "labels": [], "entities": [{"text": "METEOR", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9793053269386292}, {"text": "TER", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9959937334060669}]}, {"text": "They calculate an ordering penalty fora hypothesis based on the minimum number of chunks the translation needs to be broken into in order to align it to the reference.", "labels": [], "entities": []}, {"text": "The disadvantage of the second approach is that aligning sentences with very different words can be inaccurate.", "labels": [], "entities": []}, {"text": "Also there is no notion of how far these blocks are out of order.", "labels": [], "entities": []}, {"text": "More sophisticated metrics, such as the RTE metric, use higher level syntactic or even semantic analysis to determine the quality of the translation.", "labels": [], "entities": [{"text": "RTE", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.6637240052223206}]}, {"text": "These approaches are useful, but can be very slow, require annotation, they are language dependent and their parameters are hard to train.", "labels": [], "entities": []}, {"text": "For most research work shallow metrics are more appropriate.", "labels": [], "entities": []}, {"text": "Apart from failing to capture reordering performance, another common criticism of most current automatic MT metrics is that a particular score value reported does not give insights into quality (.", "labels": [], "entities": [{"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9751930832862854}]}, {"text": "This is because there is no intrinsic significance of a difference in scores.", "labels": [], "entities": []}, {"text": "Ideally, the scores that the metrics report would be meaningful and stand on their own.", "labels": [], "entities": []}, {"text": "However, the most one can say is that higher is better for accuracy metrics and lower is better for error metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9969857335090637}]}, {"text": "We present a novel metric, the LRscore, which explicitly measures the quality of word order in machine translations.", "labels": [], "entities": []}, {"text": "It then combines the reordering metric with a metric measuring lexical success.", "labels": [], "entities": []}, {"text": "This results in a comprehensive met-ric which measures the two most fundamental aspects of translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.9657436609268188}]}, {"text": "We argue that the LRscore is intuitive and meaningful because it is a simple, decomposable metric with only one parameter to train.", "labels": [], "entities": []}, {"text": "The LRscore has many of the properties that are deemed to be desirable in a recent metric evaluation campaign ().", "labels": [], "entities": [{"text": "LRscore", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8709681034088135}]}, {"text": "The LRscore is language independent.", "labels": [], "entities": []}, {"text": "The reordering component relies on abstract alignments and word positions and not on words at all.", "labels": [], "entities": []}, {"text": "The lexical component of the system can be any meaningful metric fora particular target language.", "labels": [], "entities": []}, {"text": "In our experiments we use 1-gram BLEU and 4-gram BLEU, however, if a researcher was interested in morphologically rich languages, a different metric which scores partially correct words might be more appropriate.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9943806529045105}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9907236695289612}]}, {"text": "The LRscore is a shallow metric, which means that it is reasonably fast to run.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.7548874020576477}]}, {"text": "This is important in order to be useful for training of the translation model parameters.", "labels": [], "entities": []}, {"text": "A final advantage is that the LRscore is a sentence level metric.", "labels": [], "entities": []}, {"text": "This means that human judgments can be directly compared to system scores and helps researchers to understand what changes they are seeing between systems.", "labels": [], "entities": []}, {"text": "In this paper we start by describing the reordering metrics and then we present the LRscore.", "labels": [], "entities": [{"text": "LRscore", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9267523288726807}]}, {"text": "Finally we discuss related work and conclude.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}