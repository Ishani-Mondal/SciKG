{"title": [], "abstractContent": [{"text": "We present our work on the identification of opinions and its components: the source, the topic and the message.", "labels": [], "entities": []}, {"text": "We describe a rule-based system for which we achieved a recall of 74% and a precision of 94%.", "labels": [], "entities": [{"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9984425902366638}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9988558292388916}]}, {"text": "Experimentation with machine-learning techniques for the same task is currently underway.", "labels": [], "entities": []}], "introductionContent": [{"text": "For some tasks in language processing such as Information Extraction or Q&A Systems, it is important to know the opinions expressed by different sources and their polarity, positive or negative, with respect to different topics.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7948628067970276}]}, {"text": "There are even commercial applications that provide this kind of service (http://www.jodange.com).", "labels": [], "entities": []}, {"text": "We here present a system for identifying opinions in Spanish texts.", "labels": [], "entities": [{"text": "identifying opinions in Spanish texts", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.8434567928314209}]}, {"text": "We define opinion as the report of someone's statement about any subject ( El investigador de la Polit\u00e9cnica afirma que el principal problema de este sistema es conseguir que sea f\u00e1cil de usar / The researcher at the Polit\u00e9c-nica asserts that the main problem with this system is making it easy to use), or as any mention of discourse participants' beliefs (El PRI acepta participar en el debate / The PRI agrees to participate in the debate).", "labels": [], "entities": []}, {"text": "As a first step, we study the impact of elements that typically introduce such expressions in written text.", "labels": [], "entities": []}, {"text": "These elements are mainly verbs of communication (decir, declarar / say, state) but other verb classes (belief, agreement, appreciation) are also considered.", "labels": [], "entities": []}, {"text": "In other cases, the opinions will be expressed through nouns (opini\u00f3n/opinion, declaraci\u00f3n/statement) or segments introduced by seg\u00fan (according to) or similar expressions.", "labels": [], "entities": []}, {"text": "To complete the opinion, we identify its characteristic arguments: the source, the topic and the message.", "labels": [], "entities": []}, {"text": "In addition to recognizing an opinion, we try to determine its semantic orientation.", "labels": [], "entities": []}, {"text": "To this end, we consider certain subjective elements and operators (reverse, intensifier, enhancing, neutralizing, etc.) which affect them.", "labels": [], "entities": []}, {"text": "In this article, we present only results on the semantic orientation of opinion verbs, opinion nouns and topic introducers (sobre/about, con respecto a/with respect to, etc.).", "labels": [], "entities": []}, {"text": "There are many studies that address these issues:, for instance, discuss in detail various concepts in the area of \"Opinion Mining\" or \"Sentiment Analysis\" and present the main proposals, resources and applications.", "labels": [], "entities": [{"text": "Opinion Mining\"", "start_pos": 116, "end_pos": 131, "type": "TASK", "confidence": 0.7608968714872996}, {"text": "Sentiment Analysis", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.843875914812088}]}, {"text": "For our work, which focuses on the identification of source, topic and message, we have mainly drawn on the following: the scheme for annotating opinions and emotions proposed by; the work on opinion-holder (source) propositional opinion identification presented in); a system for source identification using statistical methods (); a method for opinion-holder and topic extraction from; the study on the identification of source and target presented in; and a work on topic annotation.", "labels": [], "entities": [{"text": "opinion-holder (source) propositional opinion identification", "start_pos": 192, "end_pos": 252, "type": "TASK", "confidence": 0.7179301806858608}, {"text": "source identification", "start_pos": 281, "end_pos": 302, "type": "TASK", "confidence": 0.710367813706398}]}, {"text": "For our semantic orientation study, we have taken some concepts from and analyzed some work on subjectivity operators (.", "labels": [], "entities": [{"text": "semantic orientation", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.7694474756717682}]}, {"text": "In what follows, we briefly present the model that has been defined to represent opinions and two methods for their automatic recognition.", "labels": [], "entities": [{"text": "automatic recognition", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.713748425245285}]}, {"text": "First, we describe a rule-based system that incorporates lexical resources.", "labels": [], "entities": []}, {"text": "This system, whose evaluation is detailed below, achieves a recall of 74% and a precision of 97%.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9996325969696045}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9995507597923279}]}, {"text": "During the evaluation process we produced an annotated corpus of 13,000 words, by manually correcting the system output.", "labels": [], "entities": []}, {"text": "The second system, currently underdevelopment, involves the application of machine-learning techniques to the annotated corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the system we worked with a digital media corpus; the texts were taken from the same publications as those used to create the derivation corpus.", "labels": [], "entities": []}, {"text": "The corpus contains 38 texts with an average of 300 words each, making a total size of approximately 13,000 words.", "labels": [], "entities": []}, {"text": "We applied the system to the entire corpus and performed a manual review of the output in order to evaluate the identification of the defined elements and also the complete opinion identification.", "labels": [], "entities": [{"text": "opinion identification", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.7231110036373138}]}, {"text": "We also made a partial semantic orientation evaluation, taking into account only opinion predicates and topic introducers' values and their effect on the complete opinion value.", "labels": [], "entities": []}, {"text": "In addition to assessing the rules performance, during the review stage the annotated corpus was manually corrected in order to obtain an opinion annotated corpus suitable for machine-learning.", "labels": [], "entities": []}, {"text": "Rows represent: -total: total number of elements in the text, -corr-c: number of completely recognized items, -corr-p: number of partially recognized elements, -non-rec: number of unrecognized elements, -incorr: number of marked segments which do not correspond to the item, -PR: precision, -REC-c: recall calculated using corr-c, -REC-p: recall calculated using corr-p, -F: F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 280, "end_pos": 289, "type": "METRIC", "confidence": 0.9941100478172302}, {"text": "REC-c", "start_pos": 292, "end_pos": 297, "type": "METRIC", "confidence": 0.9862865805625916}, {"text": "recall", "start_pos": 299, "end_pos": 305, "type": "METRIC", "confidence": 0.9925001263618469}, {"text": "REC-p", "start_pos": 332, "end_pos": 337, "type": "METRIC", "confidence": 0.9858607053756714}, {"text": "recall", "start_pos": 339, "end_pos": 345, "type": "METRIC", "confidence": 0.990315854549408}, {"text": "F-measure", "start_pos": 375, "end_pos": 384, "type": "METRIC", "confidence": 0.9288877844810486}]}, {"text": "Most opinion predicates present in the corpus are included in our opinion verbs and nouns list (91%).", "labels": [], "entities": []}, {"text": "Several sources and topics were partially recognized because the rules do not incorporate some complements (prepositional complements or subordinate clause) to the noun phrase.", "labels": [], "entities": []}, {"text": "Message is partially recognized when a pseudodirect discourse is used (Parada agreg\u00f3 que \"la empresa reconoci\u00f3 que hubo un c\u00e1lculo entre horas estimadas y horas reales y eso fue lo que pas\u00f3.", "labels": [], "entities": []}, {"text": "Nosotros, primero empezamos a controlar a nuestro personal ...\").", "labels": [], "entities": []}, {"text": "This style is usually present in journalistic texts).", "labels": [], "entities": []}, {"text": "We recognized 25 non neutral opinion predicates in the corpus: 12 positive verbs and 14 negative verbs.", "labels": [], "entities": []}, {"text": "One verb (especular / speculate) was incorrectly assigned a negative value, its means in this particular context is neutral.", "labels": [], "entities": []}, {"text": "We just found 3 non-neutral topic introducers, the 3 are negative.", "labels": [], "entities": []}, {"text": "The opinion predicates or topic introducers' semantic orientation values were assigned to the opinions containing them.", "labels": [], "entities": [{"text": "topic introducers' semantic orientation", "start_pos": 26, "end_pos": 65, "type": "TASK", "confidence": 0.6953375861048698}]}, {"text": "This method for calculating opinion semantic orientation was correct in all cases (except for the verb especular that was incorrectly analyzed).", "labels": [], "entities": [{"text": "opinion semantic orientation", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6852879524230957}]}], "tableCaptions": [{"text": " Table 1: Number of rules in each module", "labels": [], "entities": []}, {"text": " Table 3: System evaluation results.", "labels": [], "entities": []}]}