{"title": [{"text": "Manifold Learning for the Semi-Supervised Induction of FrameNet Predicates: An Empirical Investigation", "labels": [], "entities": [{"text": "Semi-Supervised Induction of FrameNet Predicates", "start_pos": 26, "end_pos": 74, "type": "TASK", "confidence": 0.5837798774242401}]}], "abstractContent": [{"text": "This work focuses on the empirical investigation of distributional models for the automatic acquisition of frame inspired predicate words.", "labels": [], "entities": [{"text": "automatic acquisition of frame inspired predicate words", "start_pos": 82, "end_pos": 137, "type": "TASK", "confidence": 0.6974125376769474}]}, {"text": "While several semantic spaces, both word-based and syntax-based, are employed, the impact of geometric representation based on dimen-sionality reduction techniques is investigated.", "labels": [], "entities": []}, {"text": "Data statistics are accordingly studied along two orthogonal perspectives: Latent Semantic Analysis exploits global properties while Locality Preserving Projection emphasizes the role of local regularities.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.6830113331476847}, {"text": "Locality Preserving Projection", "start_pos": 133, "end_pos": 163, "type": "TASK", "confidence": 0.7936271528402964}]}, {"text": "This latter is employed by embedding prior FrameNet-derived knowledge in the corresponding non-euclidean transformation.", "labels": [], "entities": []}, {"text": "The empirical investigation here reported sheds some light on the role played by these spaces as complex kernels for supervised (i.e. Support Vector Machine) algorithms: their use configures, as a novel way to semi-supervised lexical learning, a highly appealing research direction for knowledge rich scenarios like FrameNet-based semantic parsing.", "labels": [], "entities": [{"text": "FrameNet-based semantic parsing", "start_pos": 316, "end_pos": 347, "type": "TASK", "confidence": 0.6708435515562693}]}], "introductionContent": [{"text": "Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents ().", "labels": [], "entities": [{"text": "Automatic Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7284477310521262}]}, {"text": "Several NLP applications have exploited this kind of semantic representation ranging from Information Extraction () to Question Answering (, Paraphrase Identification (, and the modeling of Textual Entailment relations ().", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.748967856168747}, {"text": "Question Answering", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.8232789635658264}, {"text": "Paraphrase Identification", "start_pos": 141, "end_pos": 166, "type": "TASK", "confidence": 0.892914742231369}]}, {"text": "Large scale annotated resources have been used by Semantic Role Labeling methods: they are commonly developed using a supervised learning paradigm where a classifier learns to predict role labels based on features extracted from annotated training data.", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7670481204986572}]}, {"text": "One prominent resource has been developed under the Berkeley FrameNet project as a semantic lexicon for the core vocabulary of English, according to the so-called frame semantic model.", "labels": [], "entities": []}, {"text": "Here, a frame is a conceptual structure modeling a prototypical situation, evoked in texts through the occurrence of its lexical units (LU) that linguistically expresses the situation of the frame.", "labels": [], "entities": []}, {"text": "Lexical units of the same frame share semantic arguments.", "labels": [], "entities": []}, {"text": "For example, the frame KILLING has lexical units such as assassin, assassinate, blood-bath, fatal, murderer, kill or suicide that share semantic arguments such as KILLER, INSTRUMENT, CAUSE, VICTIM.", "labels": [], "entities": [{"text": "KILLER", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.976202666759491}, {"text": "INSTRUMENT", "start_pos": 171, "end_pos": 181, "type": "METRIC", "confidence": 0.8372761011123657}, {"text": "CAUSE", "start_pos": 183, "end_pos": 188, "type": "METRIC", "confidence": 0.8443070650100708}]}, {"text": "The current FrameNet release contains about 700 frames and 10,000 LUs.", "labels": [], "entities": [{"text": "FrameNet release", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.8802618682384491}]}, {"text": "A corpus of 150,000 annotated examples sentences, from the British National Corpus (BNC), is also part of FrameNet.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 59, "end_pos": 88, "type": "DATASET", "confidence": 0.9776387413342794}, {"text": "FrameNet", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.9232617020606995}]}, {"text": "Despite the size of this resource, it is underdevelopment and hence incomplete: several frames are not represented by evoking words and the number of annotated sentences is unbalanced across frames.", "labels": [], "entities": []}, {"text": "It is one of the main reason for the performance drop of supervised SRL systems in out-of-domain scenarios ().", "labels": [], "entities": [{"text": "SRL", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9517220854759216}]}, {"text": "The limited coverage of FrameNet corpus is even more noticeable for the LUs dictionary: it only contains 10,000 lexical units, far less than the 210,000 entries in WordNet 3.0.", "labels": [], "entities": [{"text": "FrameNet corpus", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.8886125683784485}]}, {"text": "For example, the lexical unit crown, according to the annotations, evokes the ACCOU-TREMENT frame.", "labels": [], "entities": [{"text": "ACCOU-TREMENT frame", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.819716066122055}]}, {"text": "It refers to a particular sense: according to WordNet, it is \"an ornamental jeweled headdress signifying sovereignty\".", "labels": [], "entities": [{"text": "WordNet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.968090295791626}]}, {"text": "According to the same lexical resource, this LU has 12 lexical senses and the first one (i.e. \"The Crown (or the reigning monarch) as the symbol of the power and authority of a monarchy\") could evoke other frames, like LEADERSHIP.", "labels": [], "entities": [{"text": "LEADERSHIP", "start_pos": 219, "end_pos": 229, "type": "METRIC", "confidence": 0.8344671726226807}]}, {"text": "In ( ) and , the problem of LU automatic induction has been treated in a semi-supervised fashion.", "labels": [], "entities": [{"text": "LU automatic induction", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.699215849240621}]}, {"text": "First, LUs are modeled by exploiting the distributional analysis of an unannotated corpus and the lexical information of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.9674718379974365}]}, {"text": "These representations were used in order to find out frames potentially evoked by novel words in order to extend the FrameNet dictionary limiting the effort of manual annotations.", "labels": [], "entities": [{"text": "FrameNet dictionary", "start_pos": 117, "end_pos": 136, "type": "DATASET", "confidence": 0.8709393441677094}]}, {"text": "In this work the distributional model of LUs is further developed.", "labels": [], "entities": []}, {"text": "As in ( , several word spaces are investigated in order to find the most suitable representation of the properties which characterize a frame.", "labels": [], "entities": []}, {"text": "Two dimensionality reduction techniques are applied herein this context.", "labels": [], "entities": []}, {"text": "Latent Semantic Analysis) uses the Singular Value Decomposition to find the best subspace approximation of the original word space, in the sense of minimizing the global reconstruction error projecting data along the directions of maximal variance.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.661069800456365}]}, {"text": "Locality Preserving Projection) is a linear approximation of the nonlinear Laplacian Eigenmap algorithm: its locality preserving properties allows to add a set of constraints forcing LUs that belong to the same frame to be near in the resulting space after the transformation.", "labels": [], "entities": [{"text": "Locality Preserving Projection", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7982149322827657}]}, {"text": "LSA performs a global analysis of a corpus capturing relations between LUs and removing the noise introduced by spurious directions.", "labels": [], "entities": []}, {"text": "However it risks to ignore lexical senses poorly represented into the corpus.", "labels": [], "entities": []}, {"text": "In ) external knowledge about LUs is provided by their lexical senses from a lexical resource (e.g WordNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.9371848106384277}]}, {"text": "In this work, prior knowledge about the target problem is directly embedded into the space through the LPP transformation, by exploiting locality constraints.", "labels": [], "entities": []}, {"text": "Then a Support Vector Machine is employed to provide a robust acquisition of lexical units combining global information provided by LSA and the local information provided by LPP into a complex kernel function.", "labels": [], "entities": []}, {"text": "In Section 2 related work is presented.", "labels": [], "entities": []}, {"text": "In Sections 3 the investigated distributional model of LUs is presented as well as the dimensionality reduction techniques.", "labels": [], "entities": []}, {"text": "Then, in Section 4 the experimental investigation and comparative evaluations are reported.", "labels": [], "entities": []}, {"text": "Finally, in Section 5 we draw final conclusions and outline future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The adopted gold standard is a subset of the FrameNet database and it consists of the most 100 represented frames in term of annotated examples and LUs.", "labels": [], "entities": [{"text": "FrameNet database", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.9152716398239136}]}, {"text": "As the number of example is extremely unbalanced across frames 1 , the LUs dictionary of each selected frame contains at least 10 LUs.", "labels": [], "entities": []}, {"text": "It is a reasonable amount of information for the SVMs training and it is still a representative data set, being composed of 3,911 LUs, i.e. the 55% of the entire dictionary 2 of 7,230 evoking words.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 49, "end_pos": 53, "type": "TASK", "confidence": 0.9256628155708313}]}, {"text": "All word spaces are derived from the British National Corpus (BNC), which is underlying FrameNet and consisting of about 100 million words for English.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 37, "end_pos": 66, "type": "DATASET", "confidence": 0.9701994359493256}, {"text": "FrameNet", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.9311026334762573}]}, {"text": "Each selected frame is represented into the BNC by at least 362 annotated sentences, as the lack of a reasonable number of examples hardly produces a good distributional model of LUs.", "labels": [], "entities": [{"text": "BNC", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.8965548276901245}]}, {"text": "Each frame's list of LUs is split into train (60%), tuning (20%) and test set (20%) and LUs having Part-ofspeech different from verb, noun or adjective are removed.", "labels": [], "entities": [{"text": "tuning", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9579542279243469}]}, {"text": "In the number of LUs for each set, as well as the maximum and the average number per frame, are summarized.", "labels": [], "entities": []}, {"text": "Four different approaches for the Word Space construction are used.", "labels": [], "entities": [{"text": "Word Space construction", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.6689793070157369}]}, {"text": "The first two correspond to a Word-Based space, the last to a Syntax-Based, as described in section 3.1: Window-n (Wn): contextual features correspond to the set of the 20,000 most frequent lemmatized words in the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 214, "end_pos": 217, "type": "DATASET", "confidence": 0.936625599861145}]}, {"text": "The association measure between LUs and contexts is the Point-wise Mutual Information (PMI).", "labels": [], "entities": [{"text": "Point-wise Mutual Information (PMI)", "start_pos": 56, "end_pos": 91, "type": "METRIC", "confidence": 0.7611969858407974}]}, {"text": "Valid contexts for LUs are fixed to a n-window.", "labels": [], "entities": []}, {"text": "Hereafter two window width values will be investigated: Window5 (W5) and Window10 (W10).", "labels": [], "entities": []}, {"text": "Sentence (Sent): contextual features are the same above, but the valid contexts are extended to the entire sentence length.", "labels": [], "entities": []}, {"text": "SyntaxBased (SyntB): contextual features have been computed according to the \"dependencybased\" vector space discussed 3 in (Pado and Lapata, 2007).", "labels": [], "entities": []}, {"text": "Observable contexts here are made of syntactically-typed co-occurrences within dependency graphs built from the entire set of BNC sentences.", "labels": [], "entities": []}, {"text": "The most frequent 20,000 basic features, i.e. (syntactic relation,lemma) pairs, have been employed as contextual features corresponding to PMI scores.", "labels": [], "entities": [{"text": "PMI", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.9448126554489136}]}, {"text": "Syntactic relations are extracted using the Minipar parser.", "labels": [], "entities": [{"text": "Minipar", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9691248536109924}]}, {"text": "Word space models thus focus on the LUs of the selected 100 frames and the dimensionality have been reduced by applying LSA and LPP at anew size of l = 100.", "labels": [], "entities": []}, {"text": "Any prior knowledge information is provided to the tuning and test sets during the LPP transformation: the construction of the reduced feature space takes in account only LUs from the train set while remaining predicates are represented through the LPP linear projection.", "labels": [], "entities": []}, {"text": "In these experiments the cosine threshold \u03c4 and the maximum number of constraints care estimated over the tuning set and the best parametrizations are shown in.", "labels": [], "entities": []}, {"text": "The adopted implementation of SVM is SVM-Light-TK 4 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of LU examples for each data set  from the 100 frames", "labels": [], "entities": []}]}