{"title": [{"text": "Complete Syntactic Analysis Based on Multi-level Chunking", "labels": [], "entities": [{"text": "Syntactic Analysis", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.9314193725585938}, {"text": "Chunking", "start_pos": 49, "end_pos": 57, "type": "TASK", "confidence": 0.6036720871925354}]}], "abstractContent": [{"text": "This paper describes a complete syntactic analysis system based on multi-level chunking.", "labels": [], "entities": []}, {"text": "On the basis of the correct sequences of Chinese words provided by CLP2010, the system firstly has a Part-of-speech (POS) tagging with Conditional Random Fields (CRFs), and then does the base chunking and complex chunking with Maximum Entropy (ME), and finally generates a complete syntactic analysis tree.", "labels": [], "entities": [{"text": "CLP2010", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.960733950138092}, {"text": "Maximum Entropy (ME)", "start_pos": 227, "end_pos": 247, "type": "METRIC", "confidence": 0.7644306421279907}]}, {"text": "The system took part in the Complete Sentence Parsing Track of the Task 2 Chinese Parsing in CLP2010, achieved the F-1 measure of 63.25% on the overall analysis, ranked the sixth; POS accuracy rate of 89.62%, ranked the third.", "labels": [], "entities": [{"text": "Sentence Parsing Track of the Task 2 Chinese Parsing", "start_pos": 37, "end_pos": 89, "type": "TASK", "confidence": 0.6278079715039995}, {"text": "CLP2010", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.6120880246162415}, {"text": "F-1 measure", "start_pos": 115, "end_pos": 126, "type": "METRIC", "confidence": 0.9899663925170898}, {"text": "POS", "start_pos": 180, "end_pos": 183, "type": "METRIC", "confidence": 0.9956406354904175}, {"text": "accuracy rate", "start_pos": 184, "end_pos": 197, "type": "METRIC", "confidence": 0.9446954429149628}]}], "introductionContent": [{"text": "Chunk is a group of adjacent words which belong to the same s-projection set in a sentence, whose syntactic structure is actually a tree, but apart from the root node, all other nodes are leaf nodes.", "labels": [], "entities": []}, {"text": "Complete syntactic analysis requires a series of analyzing processes, eventually to get a full parsing tree.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7997346818447113}]}, {"text": "Parsing by chunks is proved to be feasible.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9856612682342529}]}, {"text": "The concept of chunking was first proposed by Abney in 1991, who defined chunks in terms of major heads, and parsed by chunks in 1994).", "labels": [], "entities": []}, {"text": "An additional chunk tag set {B, I, O} was added to chunking, which limited dependencies between elements in a chunk, changed chunking into a question of sequenced tags, to promote the development of chunking.", "labels": [], "entities": []}, {"text": "Chunking algorithm was extended to the bottom-up parser, which is trained and tested on the Wall Street Journal (WSJ) part of the Penn Treebank, and achieved a performance of 80.49% F-measure, the results show that it performed better than a standard probabilistic context-free grammar, and can improve performance by adding the information of parent node).", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) part of the Penn Treebank", "start_pos": 92, "end_pos": 143, "type": "DATASET", "confidence": 0.9249364571137861}, {"text": "F-measure", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9991486072540283}]}, {"text": "On Chinese parsing, Maximum Entropy Model was first used to have a POS tagging and chunking, and then a full parsing tree was generated), training and testing in the Penn Chinese Treebank, which achieved 79.56% F-measure.", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 166, "end_pos": 187, "type": "DATASET", "confidence": 0.981109082698822}, {"text": "F-measure", "start_pos": 211, "end_pos": 220, "type": "METRIC", "confidence": 0.9982733726501465}]}, {"text": "The parsing process was divided into POS tagging, base chunking and complex chunking, having a POS tagging and chunking on a given sentence, and then looping the process of complex chunking up to identify the root node (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.966394305229187}, {"text": "POS tagging", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.680431142449379}]}, {"text": "This parsing method is the basis of this paper.", "labels": [], "entities": [{"text": "parsing", "start_pos": 5, "end_pos": 12, "type": "TASK", "confidence": 0.9718344807624817}]}, {"text": "In addition, we have the existing Chinese chunking system in laboratory, which ranked first in Task 2: Chinese Base Chunking of CIPS-ParsEval-2009, so we try to apply chunking to complete syntactic analysis in CLP2010, to achieve better results.", "labels": [], "entities": [{"text": "CIPS-ParsEval-2009", "start_pos": 128, "end_pos": 146, "type": "DATASET", "confidence": 0.49598070979118347}, {"text": "CLP2010", "start_pos": 210, "end_pos": 217, "type": "DATASET", "confidence": 0.9543677568435669}]}, {"text": "We will describe the POS tagging based on CRFs in Section 2, including CRFs, feature template selection and empirical results.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.740665078163147}, {"text": "feature template selection", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.5805945495764414}]}, {"text": "Multi-level chunking based on ME will be expounded in Section 3, including ME, MEMM, base chunking and complex chunking.", "labels": [], "entities": []}, {"text": "Finally, we will summarize our work in Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: tagging results with different features  Model  Explain  Accuracy  CRF  baseline  93.52%  CRF1  add w -1 , pos -1  93.58%  CRF2  add num  93.66%  CRF3  add num, w -1 , pos -1  93.68%  CRF4  add num, rules  93.80%  CRF5 add num, w -1 , pos -1 , rules", "labels": [], "entities": []}, {"text": " Table 4: results with different decoding  Decoding  Accuracy  Recall  Fmeasure  Viterbi  84.87%  84.47%  84.67%  Heuristic  Search", "labels": [], "entities": [{"text": "Accuracy  Recall  Fmeasure  Viterbi  84.87", "start_pos": 53, "end_pos": 95, "type": "METRIC", "confidence": 0.7993988871574402}]}]}