{"title": [{"text": "Opinion Mining of Spanish Customer Comments with Non-Expert Annotations on Mechanical Turk", "labels": [], "entities": [{"text": "Opinion Mining of Spanish Customer Comments", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8175839334726334}]}], "abstractContent": [{"text": "One of the major bottlenecks in the development of data-driven AI Systems is the cost of reliable human annotations.", "labels": [], "entities": []}, {"text": "The recent advent of several crowdsourcing platforms such as Amazon's Mechanical Turk, allowing re-questers the access to affordable and rapid results of a global workforce, greatly facilitates the creation of massive training data.", "labels": [], "entities": []}, {"text": "Most of the available studies on the effectiveness of crowdsourcing report on English data.", "labels": [], "entities": []}, {"text": "We use Mechanical Turk annotations to train an Opinion Mining System to classify Spanish consumer comments.", "labels": [], "entities": []}, {"text": "We design three different Human Intelligence Task (HIT) strategies and report high inter-annotator agreement between non-experts and expert annotators.", "labels": [], "entities": [{"text": "Human Intelligence Task (HIT)", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6282661507527033}]}, {"text": "We evaluate the advantages/drawbacks of each HIT design and show that, in our case, the use of non-expert annotations is a viable and cost-effective alternative to expert annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Obtaining reliable human annotations to train datadriven AI systems is often an arduous and expensive process.", "labels": [], "entities": []}, {"text": "For this reason, crowdsourcing platforms such as Amazon's Mechanical Turk , Crowdflower and others have recently attracted a lot of attention from both companies and academia.", "labels": [], "entities": []}, {"text": "Crowdsourcing enables requesters to tap from a global pool of non-experts to obtain rapid and affordable answers to simple Human Intelligence Tasks (HITs), which can be subsequently used to train data-driven applications.", "labels": [], "entities": []}, {"text": "A number of recent papers on this subject point out that non-expert annotations, if produced in a sufficient quantity, can rival and even surpass the quality of expert annotations, often at a much lower cost (, (.", "labels": [], "entities": []}, {"text": "However, this possible increase in quality depends on the task at hand and on an adequate HIT design (.", "labels": [], "entities": [{"text": "quality", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.972548246383667}]}, {"text": "In this paper, we evaluate the usefulness of MTurk annotations to train an Opinion Mining System to detect opinionated contents (Polarity Detection) in Spanish customer comments on car brands.", "labels": [], "entities": []}, {"text": "Currently, a large majority of MTurk tasks is designed for English speakers.", "labels": [], "entities": [{"text": "MTurk tasks", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9166450500488281}]}, {"text": "One of our reasons for participating in this shared task was to find out how easy it is to obtain annotated data for Spanish.", "labels": [], "entities": []}, {"text": "In addition, we want to find out how useful these data are by comparing them to expert annotations and using them as training data of an Opinion Mining System for polarity detection.", "labels": [], "entities": [{"text": "polarity detection", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.858922004699707}]}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 contains an explanation of the task outline and our goals.", "labels": [], "entities": []}, {"text": "Section 3 contains a description of three different HIT designs that we used in this task.", "labels": [], "entities": []}, {"text": "In Section 4, we provide a detailed analysis of the retrieved HITs and focus on geographical information of the workers, the correlation between the different HIT designs, the quality of the retrieved answers and on the cost-effectiveness of the experiment.", "labels": [], "entities": []}, {"text": "In Section 5, we evaluate the incidence of MTurk-generated annotations on a polarity classification task using two different experimental settings.", "labels": [], "entities": [{"text": "incidence", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9589048624038696}, {"text": "polarity classification task", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.800272524356842}]}, {"text": "Finally, we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "As was mentioned in Section 3, all sentences were extracted from a corpus of user opinions on cars from the automotive section of www.ciao.es (Spanish).", "labels": [], "entities": []}, {"text": "For conducting the experimental evaluation, the following datasets were used: 1.", "labels": [], "entities": []}, {"text": "Baseline: constitutes the dataset used for training the baseline or reference classifiers in Experiment 1.", "labels": [], "entities": []}, {"text": "Automatic annotation for this dataset was obtained by using the following naive approach: those sentences extracted from comments with ratings 5 equal to 5 were assigned to category 'positive', those extracted from comments with ratings equal to 3 were assigned to 'neutral', and those extracted from comments with ratings equal to 1 were assigned to 'negative'.", "labels": [], "entities": []}, {"text": "This dataset contains a total of 5570 sentences, with a vocabulary coverage of 11797 words.", "labels": [], "entities": []}, {"text": "2. MTurk Annotated: constitutes the dataset that was manually annotated by MTurk workers in HIT1.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.8638796806335449}, {"text": "HIT1", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.8759137988090515}]}, {"text": "This dataset is used for training the contrastive classifiers which are to be compared with the baseline system in Experiment 1.", "labels": [], "entities": []}, {"text": "It is also used in various ways in Experiment 2.", "labels": [], "entities": []}, {"text": "The three independent annotations generated by MTurk workers for each sentence within this dataset were consolidated into one unique annotation by majority voting: if the three provided annotations happened to be different 6 , the sentence was assigned to category 'neutral'; otherwise, the sentence was assigned to the category with at least two annotation agreements.", "labels": [], "entities": []}, {"text": "This dataset contains a total of 1000 sentences, with a vocabulary coverage of 3022 words.", "labels": [], "entities": []}, {"text": "3. Expert Annotated: this dataset contains the same sentences as the MTurk Annotated one, but with annotations produced internally by known reliable annotators . Each sentence received one annotation, while the dataset was split between a total of five annotators.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.9296178221702576}]}, {"text": "4. Evaluation: constitutes the gold standard used for evaluating the performance of classifiers.", "labels": [], "entities": []}, {"text": "This dataset was manually annotated by three experts in an independent manner.", "labels": [], "entities": []}, {"text": "The gold standard annotation was consolidated by using the same criterion used in the case of the previous dataset . This dataset contains a total of 500 sentences, with a vocabulary coverage of 2004 words.: Mean accuracy over 20 independent simulations (with standard deviations provided in parenthesis) for each classification subtasks trained with either the baseline or the annotated dataset.", "labels": [], "entities": [{"text": "Mean", "start_pos": 208, "end_pos": 212, "type": "METRIC", "confidence": 0.9905937910079956}, {"text": "accuracy", "start_pos": 213, "end_pos": 221, "type": "METRIC", "confidence": 0.7567889094352722}]}, {"text": "In this sense, twenty independent realizations were actually conducted for each experiment presented and, instead of individual output results, mean values and standard deviations of evaluation metrics are reported.", "labels": [], "entities": []}, {"text": "Each binary classifier realization was trained with a random subsample set of 600 sentences extracted from the training dataset corresponding to the classifier group, i.e. baseline dataset for reference systems, and annotated dataset for contrastive systems.", "labels": [], "entities": []}, {"text": "Training subsample sets were always balanced with respect to the original three categories: 'positive', 'negative' and 'neutral'.", "labels": [], "entities": []}, {"text": "presents the resulting mean values of accuracy for each considered subtask in classifiers trained with either the baseline or the annotated dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9989467263221741}]}, {"text": "As observed in the table, all subtasks benefit from using the annotated dataset for training the classifiers; however, it is important to mention that while similar absolute gains are observed for the 'positive/not positive' and 'neutral/not neutral' subtasks, this is not the case for the subtask 'negative/not negative', which actually gains much less than the other two subtasks.", "labels": [], "entities": []}, {"text": "After considering all evaluation metrics, the benefit provided by human-annotated data availability for categories 'neutral' and 'positive' is evident.", "labels": [], "entities": []}, {"text": "However, in the case of category 'negative', although some gain is also observed, the benefit of humanannotated data does not seem to be as much as for the two other categories.", "labels": [], "entities": []}, {"text": "This, along with the fact that the 'negative/not negative' subtask is actually the best performing one (in terms of accuracy) when baseline training data is used, might suggest that low rating comments contains a better representation of sentences belonging to category 'negative' than medium and high rating comments do with respect to classes 'neutral' and 'positive'.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9988849759101868}]}, {"text": "In any case, this experimental work only verifies the feasibility of constructing training datasets for opinionated content analysis, as well as it provides an approximated idea of costs involved in the generation of this type of resources, by using MTurk.", "labels": [], "entities": [{"text": "opinionated content analysis", "start_pos": 104, "end_pos": 132, "type": "TASK", "confidence": 0.7465226054191589}, {"text": "MTurk", "start_pos": 250, "end_pos": 255, "type": "DATASET", "confidence": 0.8723514080047607}]}, {"text": "In this section, we compare the results of training several polarity classifiers on six different training sets, each of them generated from the MTurk annotations of HIT1.", "labels": [], "entities": [{"text": "MTurk annotations of HIT1", "start_pos": 145, "end_pos": 170, "type": "DATASET", "confidence": 0.8094187378883362}]}, {"text": "We used classifiers as implemented in Mallet) and Weka (), based on a simple bag-of-words representation of the sentences.", "labels": [], "entities": [{"text": "Mallet", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.9720153212547302}, {"text": "Weka", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.788594126701355}]}, {"text": "As the objective was not to obtain optimum performance but only to evaluate the differences between different sets of annotations, all classifiers were used with their default settings.", "labels": [], "entities": []}, {"text": "contains results of four different classifiers (Maxent, C45, Winnow and SVM), trained on these six different datasets and evaluated on the same 500-sentence test set as explained in Section 5.1.", "labels": [], "entities": []}, {"text": "Classification using expert annotations usually outperforms classification using a single batch (one annotation per sentence) of annotations produced using MTurk.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.9376439452171326}]}, {"text": "Using the tree annotations per sentence available from MTurk, all classifiers reach similar or better performance compared to the single set of expert annotations, at a much lower cost (as explained in section 4.4).", "labels": [], "entities": [{"text": "MTurk", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.8968358635902405}]}, {"text": "It is interesting to note that most classifiers benefit from using the full 3000 training examples (1000 sentences with 3 annotations each), which intuitively makes sense as the unanimously labeled examples will have more weight in defining the model of the corresponding class, whereas ambiguous or unclear cases   an important drop in performance when using multiple annotations, but perform well when using the majority vote.", "labels": [], "entities": []}, {"text": "As a first intuition, this maybe due to the fact that SVMs focus on detecting class boundaries (and optimizing the margin between classes) rather than developing a model of each class.", "labels": [], "entities": []}, {"text": "As such, having the same data point appear several times with the same label will not aid in finding appropriate support vectors, whereas having the same data point with conflicting labels may have a negative impact on the margin maximization.", "labels": [], "entities": []}, {"text": "Having only evaluated each classifier (and training set) once on a static test set it is unfortunately not possible to reliably infer the significance of the performance differences (or determine confidence intervals, etc.).", "labels": [], "entities": []}, {"text": "For a more in-depth analysis it might be interesting to use bootstrapping or similar techniques to evaluate the robustness of the results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on MTurk workers for all three HIT  designs: (fictional) worker ID, country code, % of total  number of HITs completed, number of HITs completed  per design and average completion time.", "labels": [], "entities": []}, {"text": " Table 2: Interannotation Agreement as a measure of qual- ity of the annotations in HIT1. \u03ba 1 = Fixed Margin  Kappa. \u03ba 2 = Free Margin Kappa.", "labels": [], "entities": [{"text": "Interannotation", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.947944164276123}, {"text": "qual", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.959298849105835}, {"text": "HIT1", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.9173160195350647}]}, {"text": " Table 5: Accuracy figures of four different classifiers  (Winnow, SVM, C45 and Maxent) trained on six different  datasets (see text for details).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9862986207008362}, {"text": "Winnow", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.8984466791152954}]}]}