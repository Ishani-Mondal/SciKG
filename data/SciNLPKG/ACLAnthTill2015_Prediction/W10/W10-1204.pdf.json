{"title": [{"text": "A Graph-Based Semi-Supervised Learning for Question Semantic Labeling", "labels": [], "entities": [{"text": "Question Semantic Labeling", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.7227737704912821}]}], "abstractContent": [{"text": "We investigate a graph-based semi-supervised learning approach for labeling semantic components of questions such as topic, focus, event, etc., for question understanding task.", "labels": [], "entities": [{"text": "question understanding task", "start_pos": 148, "end_pos": 175, "type": "TASK", "confidence": 0.8116383949915568}]}, {"text": "We focus on graph construction to handle learning with dense/sparse graphs and present Relaxed Linear Neighborhoods method, in which each node is linearly constructed from varying sizes of its neighbors based on the density/sparsity of its surrounding.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7946088016033173}]}, {"text": "With the new graph representation, we show performance improvements on syntactic and real datasets, primarily due to the use of unlabeled data and relaxed graph construction.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the important steps in Question Answering (QA) is question understanding to identify semantic components of questions.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.8962807774543762}, {"text": "question understanding", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.8025760650634766}]}, {"text": "In this paper, we investigate question understanding based on a machine learning approach to discover semantic components.", "labels": [], "entities": [{"text": "question understanding", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.8009902238845825}]}, {"text": "An important issue in information extraction from text is that one often deals with insufficient labeled data and large number of unlabeled data, which have led to improvements in semi-supervised learning (SSL) methods, e.g.,, ().", "labels": [], "entities": [{"text": "information extraction from text", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.8313672691583633}]}, {"text": "Recently, graph based SSL methods have gained interest,).", "labels": [], "entities": [{"text": "SSL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.8667700886726379}]}, {"text": "These methods create graphs whose vertices correspond to labeled and unlabeled data, while the edge weights encode the similarity between each pair of data points.", "labels": [], "entities": []}, {"text": "Classification is performed using these graphs by scoring unlabeled points in such away  that instances connected by large weights are given similar labels.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9578078985214233}]}, {"text": "Such methods can perform well when no parametric information about distribution of data is available and when data is characterized by an underlying manifold structure.", "labels": [], "entities": []}, {"text": "In this paper, we present a semantic component labeling module for our QA system using anew graph-based SSL to benefit from unlabeled questions.", "labels": [], "entities": []}, {"text": "One of the issues affecting the performance of graph-based methods is that there is no reliable approach for model selection when there are too few labeled points ().", "labels": [], "entities": []}, {"text": "Such issues have only recently came into focus ().", "labels": [], "entities": []}, {"text": "This is somewhat surprising because graph construction is a fundamental step.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7381188869476318}]}, {"text": "Rather than proposing yet another learning algorithm, we focus on graph construction for our labeling task, which suffers from insufficient graph sparsification methods.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7869967818260193}]}, {"text": "Such problems are caused by fixed neighborhood assignments in k-nearest neighbor approaches, treating sparse and denser regions of data equally or using improper threshold assumptions in -neighborhood 27 graphs, yielding disconnected components or subgraphs or isolated singleton vertices.", "labels": [], "entities": []}, {"text": "We propose a Relaxed Linear Neighborhood (RLN) method to overcome fixed k or assumptions.", "labels": [], "entities": []}, {"text": "RLN approximates the entire graph by a series of overlapped linear neighborhood patches, where neighborhood N (x i ) of any node xi is captured dynamically based on the density/sparsity of its surrounding.", "labels": [], "entities": []}, {"text": "Moreover, RLN exploits degree of neighborhood during reconstruction method rather than fixed assignments, which does not get affected by outliers, producing a more robust graph, demonstrated in Experiment #1.", "labels": [], "entities": []}, {"text": "We present our question semantic component model in section 3 with the following contributions: (1) anew graph construction method for SSL, which relaxes neighborhood assumptions yielding robust graphs as defined in section 5, (2) anew inference approach to enable learning from unlabeled data as defined in section 6.", "labels": [], "entities": []}, {"text": "The experiments in section 7 yield performance improvement in comparison to other labeling methods on different datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the next, we evaluate the performance of the proposed RLN in comparison to the other methods on syntactic and real datasets.", "labels": [], "entities": []}, {"text": "1. Graph Construction Performance: Here we use a similar syntactic data in shown in.a, which contains two clusters of dissimilar densities and shapes.", "labels": [], "entities": [{"text": "Graph Construction", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8520874977111816}]}, {"text": "We investigate three graph construction methods, linear k-neighborhoods of) in.b, b-matching() in.c and RLN of this work in.d using a dataset of 300 points with binary output values.", "labels": [], "entities": [{"text": "RLN", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9860493540763855}]}, {"text": "b-matching permits a given datum to select k neighboring points but also ensures that exactly k points selects given datum as their neighbor.", "labels": [], "entities": []}, {"text": "In each graph construction method Gaussian kernel distance is used.", "labels": [], "entities": [{"text": "Gaussian kernel distance", "start_pos": 34, "end_pos": 58, "type": "METRIC", "confidence": 0.7673951188723246}]}, {"text": "Experiments are run 50 times whereat each fold only 2 labeled samples from opposite classes are used to predict the rest.", "labels": [], "entities": []}, {"text": "The experiments are repeated for different k, band values.", "labels": [], "entities": []}, {"text": "In, average of trials is shown when k, bare 10 and >0.5.", "labels": [], "entities": []}, {"text": "We also used the N approach but it did not show any improvement over kNN approach.", "labels": [], "entities": []}, {"text": "32 In can separate two classes more efficiently than the rest.", "labels": [], "entities": []}, {"text": "Compared to the bmatching approach, RLN clearly improves the robustness.", "labels": [], "entities": [{"text": "RLN", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.7323192954063416}]}, {"text": "There are more links between clusters in other graph methods than RLN, which shows that RLN can separate two classes much efficiently.", "labels": [], "entities": []}, {"text": "Also since dynamic number of edges are constructed with RLN, unnecessary links are avoided, but for the rest of the graph methods there are edges between faraway nodes (shown with arrows).", "labels": [], "entities": []}, {"text": "In the rest of the experiments, we use b-matching for benchmark as it is the closest approach to the proposed RLN.", "labels": [], "entities": [{"text": "RLN", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.7480804920196533}]}], "tableCaptions": [{"text": " Table 3: Test Data Average Loss on graph construction  with RLN, b-matching, standard SSL with kNN as well  as CRF, CRF with Self Learning (sCRF).", "labels": [], "entities": []}, {"text": " Table 4: Average Loss Results for RLN graph based SSL  as unlabeled tokens is increased.", "labels": [], "entities": [{"text": "Average Loss", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.974150151014328}, {"text": "RLN graph based SSL", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.6700234711170197}]}]}