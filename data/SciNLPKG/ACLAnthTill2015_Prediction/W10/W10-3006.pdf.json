{"title": [{"text": "Memory-Based Resolution of In-Sentence Scopes of Hedge Cues", "labels": [], "entities": [{"text": "Memory-Based Resolution of In-Sentence Scopes of Hedge Cues", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7975767180323601}]}], "abstractContent": [{"text": "In this paper we describe the machine learning systems that we submitted to the CoNLL-2010 Shared Task on Learning to Detect Hedges and Their Scope in Natural Language Text.", "labels": [], "entities": [{"text": "CoNLL-2010 Shared Task on Learning to Detect Hedges and Their Scope in Natural Language Text", "start_pos": 80, "end_pos": 172, "type": "TASK", "confidence": 0.7577531774838765}]}, {"text": "Task 1 on detecting uncertain information was performed by an SVM-based system to process the Wikipedia data and by a memory-based system to process the biological data.", "labels": [], "entities": [{"text": "detecting uncertain information", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.8810075918833414}, {"text": "Wikipedia data", "start_pos": 94, "end_pos": 108, "type": "DATASET", "confidence": 0.966829925775528}]}, {"text": "Task 2 on resolving in-sentence scopes of hedge cues, was performed by a memory-based system that relies on information from syntactic dependencies.", "labels": [], "entities": [{"text": "resolving in-sentence scopes of hedge cues", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.8204835752646128}]}, {"text": "This system scored the highest F1 (57.32) of Task 2.", "labels": [], "entities": [{"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9995793700218201}]}], "introductionContent": [{"text": "In this paper we describe the machine learning systems that CLiPS 1 submitted to the closed track of the CoNLL-2010 Shared Task on Learning to Detect Hedges and Their Scope in Natural Language Text.", "labels": [], "entities": [{"text": "CoNLL-2010 Shared Task on Learning to Detect Hedges and Their Scope in Natural Language Text", "start_pos": 105, "end_pos": 197, "type": "TASK", "confidence": 0.7629567742347717}]}, {"text": "The task consists of two subtasks: detecting whether a sentence contains uncertain information (Task 1), and resolving in-sentence scopes of hedge cues (Task 2).", "labels": [], "entities": [{"text": "detecting whether a sentence contains uncertain information", "start_pos": 35, "end_pos": 94, "type": "TASK", "confidence": 0.8394597768783569}]}, {"text": "To solve Task 1, systems are required to classify sentences into two classes, \"Certain\" or \"Uncertain\", depending on whether the sentence contains factual or uncertain information.", "labels": [], "entities": []}, {"text": "Three annotated training sets are provided: Wikipedia paragraphs (WIKI), biological abstracts (BIO-ABS) and biological full articles (BIO-ART).", "labels": [], "entities": [{"text": "BIO-ABS", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.6438215970993042}]}, {"text": "The two test sets consist of WIKI and BIO-ART data.", "labels": [], "entities": [{"text": "WIKI", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.8102499842643738}, {"text": "BIO-ART data", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.86208775639534}]}, {"text": "Task 2 requires identifying hedge cues and finding their scope in biomedical texts.", "labels": [], "entities": []}, {"text": "Finding the scope of a hedge cue means determining at sentence level which words in the sentence are affected by the hedge cue.", "labels": [], "entities": []}, {"text": "For a sentence like the one in (1) extracted from the BIO-ART training corpus, systems have to identify likely and suggested as hedge cues, and they have to find that likely scopes over the full sentence, and that suggested scopes over by the role of murine MIB in TNF\u03b1 signaling.", "labels": [], "entities": [{"text": "BIO-ART training corpus", "start_pos": 54, "end_pos": 77, "type": "DATASET", "confidence": 0.7803171277046204}]}, {"text": "A scope will be correctly resolved only if both the cue and the scope are correctly identified.", "labels": [], "entities": []}, {"text": "Systems are to be trained on BIO-ABS and BIO-ART and tested on BIO-ART.", "labels": [], "entities": [{"text": "BIO-ABS", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.5470000505447388}, {"text": "BIO-ART", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.5203587412834167}, {"text": "BIO-ART", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.8535748720169067}]}, {"text": "shows that sentences in the BIO-ART dataset can be quite complex because of their length, because of their structure -very often they contain enumerations, and because they contain bibliographic references and references to tables and figures.", "labels": [], "entities": [{"text": "BIO-ART dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9569307863712311}]}, {"text": "Handling these phenomena is necessary to detect scopes correctly in the setting of this task.", "labels": [], "entities": []}, {"text": "Note that the scope of suggested above does not include the bibliographic reference, whereas the scope of likely includes all the bibliographic references, and that the scope of likely does not include the final punctuation mark.", "labels": [], "entities": []}, {"text": "In the case of the BIO data, we approach Task 1 as a prerequisite for Task 2.", "labels": [], "entities": [{"text": "BIO data", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.8372419476509094}]}, {"text": "Therefore we treat them as two consecutive classification tasks: a first one that consists of classifying the tokens of a sentence as being at the beginning of a hedge signal, inside or outside.", "labels": [], "entities": []}, {"text": "This allows the system to find multiword hedge cues.", "labels": [], "entities": []}, {"text": "We tag a sentence as uncertain if at least a hedge cue is found in the sentence.", "labels": [], "entities": []}, {"text": "The second classification task consists of classifying the tokens of a sentence as being the first element of the scope, the last, or neither.", "labels": [], "entities": []}, {"text": "This happens as many times as there are hedge cues in the sentence.", "labels": [], "entities": []}, {"text": "The two classification tasks are implemented using memory-based learners.", "labels": [], "entities": []}, {"text": "Memory-based language processing) is based on the idea that NLP problems can be solved by reuse of solved examples of the problem stored in memory.", "labels": [], "entities": [{"text": "Memory-based language processing", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5775783956050873}]}, {"text": "Given anew problem, the most similar examples are retrieved, and a solution is extrapolated from them.", "labels": [], "entities": []}, {"text": "Section 2 is devoted to related work.", "labels": [], "entities": []}, {"text": "In Section 3 we describe how the data have been preprocessed.", "labels": [], "entities": []}, {"text": "In Section 4 and Section 5 we present the systems that perform Task 1 and Task 2.", "labels": [], "entities": []}, {"text": "Finally, Section 6 puts forward some conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Evaluation of the conversion from xml to  CoNLL format.", "labels": [], "entities": [{"text": "CoNLL format", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.8691465556621552}]}, {"text": " Table 3: Uncertainty detection results (Task 1 - closed track). Post-evaluation results are marked  with *.", "labels": [], "entities": [{"text": "Uncertainty detection", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7448997795581818}]}, {"text": " Table 4. One of the main  sources of errors in detecting the cues are due to  the cue or. Of the 52 occurrences in the test corpus  BIO-ART, the system produces 3 true positives, 8  false positives and 49 false negatives.", "labels": [], "entities": [{"text": "BIO-ART", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.47362905740737915}]}, {"text": " Table 4: Cue matching results (Task 1 -closed  track).", "labels": [], "entities": [{"text": "Cue matching", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.8238272666931152}]}]}