{"title": [{"text": "Taming Structured Perceptrons on Wild Feature Vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "Structured perceptrons are attractive due to their simplicity and speed, and have been used successfully for tuning the weights of binary features in a machine translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.690083235502243}]}, {"text": "In attempting to apply them to tuning the weights of real-valued features with highly skewed distributions, we found that they did notwork well.", "labels": [], "entities": []}, {"text": "This paper describes a modification to the update step and compares the performance of the resulting algorithm to standard minimum error-rate training (MERT).", "labels": [], "entities": [{"text": "minimum error-rate training (MERT)", "start_pos": 123, "end_pos": 157, "type": "METRIC", "confidence": 0.8253275156021118}]}, {"text": "In addition , preliminary results for combining MERT or structured-perceptron tuning of the log-linear feature weights with coordinate ascent of other translation system parameters are presented.", "labels": [], "entities": [{"text": "MERT", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9258841872215271}]}], "introductionContent": [{"text": "Structured perceptrons area relatively recent) update of the classic perceptron algorithm which permit the prediction of vectors of values.", "labels": [], "entities": []}, {"text": "Initially developed for part of speech taggers, they have been applied to tuning the weights of the features in the log-linear models used by statistical machine translation, and found to have performance similar to the Margin-Infused Relaxed Algorithm (MIRA) by) and Minimum-Error Rate Training (MERT) by.", "labels": [], "entities": [{"text": "speech taggers", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7286447733640671}, {"text": "statistical machine translation", "start_pos": 142, "end_pos": 173, "type": "TASK", "confidence": 0.6497213443120321}, {"text": "Minimum-Error Rate Training (MERT)", "start_pos": 268, "end_pos": 302, "type": "METRIC", "confidence": 0.7763131707906723}]}, {"text": "Parameter tuning is an important aspect of current data-driven machine translation systems, as an improper selection of feature weights can dramatically reduce scores on evaluation metrics such as BLEU () or METEOR ().", "labels": [], "entities": [{"text": "Parameter tuning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.806870847940445}, {"text": "machine translation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7269658595323563}, {"text": "BLEU", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.9972736239433289}, {"text": "METEOR", "start_pos": 208, "end_pos": 214, "type": "METRIC", "confidence": 0.8189780712127686}]}, {"text": "When we recently added new features to the CMU-EBMT translation system , in addition to splitting a number of composite features into their components, our previous method of parameter tuning via coordinate ascent 2 became impractical.", "labels": [], "entities": [{"text": "CMU-EBMT translation", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7582559287548065}]}, {"text": "With now more than 50 features partaking in the scoring model, MERT no longer seemed a good choice, as the common wisdom is that it is notable to reliably optimize more than about 20 features (.", "labels": [], "entities": [{"text": "MERT", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.6233596205711365}]}, {"text": "We had been using coordinate ascent because of a need to tune a substantial number of parameters which are not directly part of the log-linear model which can be tuned by MERT or similar methods.", "labels": [], "entities": [{"text": "coordinate ascent", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7778364717960358}]}, {"text": "As a result of the non-model parameters, a full system tuning will involve multiple runs of the tuning algorithm for the feature weights, since the other parameters will affect the optimal weights.", "labels": [], "entities": []}, {"text": "Thus, speed is an important consideration for any method to be used in this setting.", "labels": [], "entities": [{"text": "speed", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.9950839877128601}]}, {"text": "The structured perceptron algorithm is ideally suited due to its speed, provided that it can produce competitive results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present the results of experiments on three data sets in the next section.", "labels": [], "entities": []}, {"text": "The data sets are English-to-Haitian, French-to-English, and Czech-to-English.", "labels": [], "entities": []}, {"text": "The English-to-Haitian system was built using the data released by Carnegie Mellon University.", "labels": [], "entities": []}, {"text": "It consists of a medical phrasebook, a glossary, and a modest amount of newswire text, each available as a set of sentence pairs in English and Haitian Creole.", "labels": [], "entities": []}, {"text": "For training, we used all of the glossary, all but the last 300 phrase pairs of the medical phrasebook (these had previously been used for development and testing of a \"toy\" system), and the first 12,500 sentence pairs of the newswire text.", "labels": [], "entities": []}, {"text": "Tuning was performed using the next 217 sentence pairs of the newswire text, and the test set consisted of the final 800 sentence pairs of the newswire text.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9677488803863525}]}, {"text": "The target language model was built solely from the target half of the training corpus, as we did not have any additional Haitian Creole text.", "labels": [], "entities": []}, {"text": "The French-to-English system was built using the Europarl () version 3 data for French and English.", "labels": [], "entities": [{"text": "Europarl () version 3 data", "start_pos": 49, "end_pos": 75, "type": "DATASET", "confidence": 0.930427634716034}]}, {"text": "As is usual practice, text from the fourth quarter of 2000 was omitted from the training set.", "labels": [], "entities": []}, {"text": "Tuning was performed using 200 sentences from the \"devtest2006\" file and all 2000 sentences of \"test2007\" were used as the final test set.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9617050886154175}]}, {"text": "Two target language models were built and interpolated during decoding; the first was trained on the target half of the bilingal corpus, and the second was built using the Canadian Hansards text released by ISI (Natural Language).", "labels": [], "entities": [{"text": "Canadian Hansards text released by ISI (Natural Language)", "start_pos": 172, "end_pos": 229, "type": "DATASET", "confidence": 0.8610179901123047}]}, {"text": "The Czech-to-English system was built using the parallel data made available for the 2010 Workshop on Statistical Machine Translation (WMT10).", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT10)", "start_pos": 102, "end_pos": 141, "type": "TASK", "confidence": 0.7670441269874573}]}, {"text": "The target language model was built from the target half of the bilingual training corpus.", "labels": [], "entities": []}, {"text": "Tuning was performed on a 200-sentence subset of the \"news-2008-test\" data, and all 2525 sentences of the \"news-2009-test\" data were used as unseen test data.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9582398533821106}, {"text": "news-2008-test\" data", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.8572628696759542}, {"text": "news-2009-test\" data", "start_pos": 107, "end_pos": 127, "type": "DATASET", "confidence": 0.7660993734995524}]}, {"text": "As these experiments were the very first time that the CMU-EBMT system was applied to Czech, there are undoubtedly numerous pre-processing and training improvements which will increase scores above the values presented here.", "labels": [], "entities": []}, {"text": "Parameter tuning was performed using CMERT 0.5, the reimplemented MERT program included with recent releases of the MOSES translation system (specifically, the version included with the 2010-04-01 release), the annealing-based optimizer included with Cunei (, and the Structured Perceptron optimizer.", "labels": [], "entities": [{"text": "MOSES translation", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.6972970962524414}]}, {"text": "Feature weights were initialized to a uniform value of 1.0 for MERT and 10 \u22129 for annealing and Perceptron (since the usual zero causes problems for the decoder).", "labels": [], "entities": [{"text": "MERT", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.6133401393890381}]}, {"text": "Both versions of MERT were permitted to run for 15 iterations or until features weights converged and remained (nearly) unchanged from one iteration to the next, using merged n-best lists from the current and the three most recent prior iterations.", "labels": [], "entities": []}, {"text": "Annealing was run with gamma values from 0.25 to 4.0, skipping the entropy phase.", "labels": [], "entities": []}, {"text": "The Structured Perceptron was allowed to run for 18 iterations and to choose the weights from the iteration which resulted in the highest average Rouge-S score for the top translation in the n-best list.", "labels": [], "entities": []}, {"text": "For French-English, this proved to be the sixth iteration, while for EnglishHaitian it was the twelfth.", "labels": [], "entities": []}, {"text": "We have found that the objective score increases for the first six to eight iterations of SP, after which it fluctuates with no trend up or down (but occasionally setting anew high, which is why we decided to run 18 iterations).", "labels": [], "entities": [{"text": "objective score", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.9295250475406647}, {"text": "SP", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9599902629852295}]}, {"text": "For French-English, we determined the best value of \u03b2 for the Rouge-S scoring to be 1.5, and the best value of the aggressiveness parameter C to be 0.1, using a 40-sentence subset of the French-English tuning set, and then applied those value for the full tuning set.", "labels": [], "entities": [{"text": "French-English tuning set", "start_pos": 187, "end_pos": 212, "type": "DATASET", "confidence": 0.7468692064285278}]}, {"text": "For English-Haitian, we used \u03b2 = 1.2 and C = 0.01 (lower values of C provide more smoothing and overall smaller updates, which is necessary for sparse or noisy data).", "labels": [], "entities": []}, {"text": "Due to limited time prior to submission, the English-Haitian values for \u03b2 and C were re-used for Czech, with no attempt at tuning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: English-to-Haitian tuning performance", "labels": [], "entities": []}, {"text": " Table 2: French-to-English tuning performance", "labels": [], "entities": []}, {"text": " Table 3: Czech-to-English tuning performance", "labels": [], "entities": []}, {"text": " Table 4: English-to-Haitian tuning performance (including coordinate ascent)", "labels": [], "entities": []}]}