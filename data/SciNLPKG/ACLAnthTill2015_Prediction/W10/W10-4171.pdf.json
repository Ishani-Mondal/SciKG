{"title": [{"text": "Soochow University: Description and Analysis of the Chinese Word Sense Induction System for CLP2010", "labels": [], "entities": [{"text": "Soochow University", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9155319333076477}, {"text": "Chinese Word Sense Induction", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.5869826376438141}, {"text": "CLP2010", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.7726248502731323}]}], "abstractContent": [{"text": "Recent studies on word sense induction (WSI) mainly concentrate on European languages, Chinese word sense induction is becoming popular as it presents anew challenge to WSI.", "labels": [], "entities": [{"text": "word sense induction (WSI)", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.8396944999694824}, {"text": "Chinese word sense induction", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.6335230767726898}, {"text": "WSI", "start_pos": 169, "end_pos": 172, "type": "TASK", "confidence": 0.6977213621139526}]}, {"text": "In this paper, we propose a feature-based approach using the spectral clustering algorithm to this problem.", "labels": [], "entities": [{"text": "spectral clustering", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7186214625835419}]}, {"text": "We also compare various clustering algorithms and similarity metrics.", "labels": [], "entities": []}, {"text": "Experimental results show that our system achieves promising performance in F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9878703951835632}]}], "introductionContent": [{"text": "Word sense induction is an open problem of natural language processing (NLP), which governs the process of automatic discovery of the possible senses of a word.", "labels": [], "entities": [{"text": "Word sense induction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7204682230949402}, {"text": "natural language processing (NLP)", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.7979870239893595}]}, {"text": "WSI is similar to word sense disambiguation (WSD) both in methods employed and in problem encountered.", "labels": [], "entities": [{"text": "WSI", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8751893639564514}, {"text": "word sense disambiguation (WSD)", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.7918852219978968}]}, {"text": "In the procedure of WSD, the senses are assumed to be known and the task focuses on choosing the correct one for an ambiguous word in a context.", "labels": [], "entities": [{"text": "WSD", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9822102785110474}]}, {"text": "The main difference between them is that the task of WSD generally requires largescale manually annotated lexical resources while WSI does not.", "labels": [], "entities": [{"text": "WSD", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.8278819918632507}]}, {"text": "As WSI doesn't rely on the manually annotated corpus, it has become one of the most important topics in current NLP research (.", "labels": [], "entities": [{"text": "WSI", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.6757592558860779}]}, {"text": "Typically, the input to a WSI algorithm is a target word to be disambiguated.", "labels": [], "entities": [{"text": "WSI algorithm", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.8565311431884766}]}, {"text": "The task of WSI is to distinguish which target words share the same meaning when they appear in different contexts.", "labels": [], "entities": [{"text": "WSI", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9691770076751709}]}, {"text": "Such result can beat the very least used as empirically grounded suggestions for lexicographers or as input for WSD algorithm.", "labels": [], "entities": [{"text": "WSD", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.7831838726997375}]}, {"text": "Other possible uses include automatic thesaurus or ontology construction, machine translation or information retrieval.", "labels": [], "entities": [{"text": "automatic thesaurus or ontology construction", "start_pos": 28, "end_pos": 72, "type": "TASK", "confidence": 0.5971410512924195}, {"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7832558453083038}, {"text": "information retrieval", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.8065699636936188}]}, {"text": "Compared with European languages, the study of WSI in Chinese is scarce.", "labels": [], "entities": [{"text": "WSI", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9557011723518372}]}, {"text": "Furthermore, as Chinese has its special writing style and Chinese word senses have their own characteristics, the methods that work well in English may not perform effectively in Chinese and the usefulness of WSI in real-world applications has yet to be tested and proved.", "labels": [], "entities": []}, {"text": "The core idea behind word sense induction is that contextual information provides important cues regarding a word's meaning.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7910099824269613}]}, {"text": "The idea dates back to (at least) Firth (1957) (\ud97b\udf59You shall know a word by the company it keeps\ud97b\udf59), and underlies most WSD and lexicon acquisition work to date.", "labels": [], "entities": [{"text": "WSD and lexicon acquisition", "start_pos": 117, "end_pos": 144, "type": "TASK", "confidence": 0.9108127951622009}]}, {"text": "For example, when the adverb phrase occurring prior to the ambiguous word\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59, then the target word is more likely to be a verb and the meaning of which is \"to hold something\"; Otherwise, if an adjective phrase locates in the same position, then it probably means \"confidence\" in English.", "labels": [], "entities": []}, {"text": "Thus, the words surrounds the target word are main contributor to sense induction.", "labels": [], "entities": [{"text": "sense induction", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.8322964608669281}]}, {"text": "The bake off task 4 on WSI in the first CIPS-SIGHAN Joint Conference on Chinese Language Processing (CLP2010) is intended to promote the exchange of ideas among participants and improve the performance of Chinese WSI systems.", "labels": [], "entities": [{"text": "WSI in the first CIPS-SIGHAN Joint Conference on Chinese Language Processing (CLP2010)", "start_pos": 23, "end_pos": 109, "type": "TASK", "confidence": 0.6522636647735324}]}, {"text": "Generally, our WSI system also adopts a clustering algorithm to group the contexts of a target word.", "labels": [], "entities": []}, {"text": "Differently, after generating feature vectors of words, we compute a similarity matrix with each cell denoting the similarity between two contexts.", "labels": [], "entities": []}, {"text": "Furthermore, the set of similarity values of a context with other contexts is viewed as another kind of feature vector, which we refer to as similarity vector.", "labels": [], "entities": []}, {"text": "Both feature vectors and similarity vectors can be separately used as the input to clustering algorithms.", "labels": [], "entities": []}, {"text": "Experimental results show our system achieves good performances on the development dataset as well as on the final test dataset provided by the CLP2010.", "labels": [], "entities": [{"text": "CLP2010", "start_pos": 144, "end_pos": 151, "type": "DATASET", "confidence": 0.9854884743690491}]}], "datasetContent": [{"text": "This section reports the evaluation dataset and system performance for our feature-based Chinese WSI system.", "labels": [], "entities": []}, {"text": "We use the CLP2010 bake off task 4 sample dataset as our development dataset.", "labels": [], "entities": [{"text": "CLP2010 bake off task 4 sample dataset", "start_pos": 11, "end_pos": 49, "type": "DATASET", "confidence": 0.9133020043373108}]}, {"text": "There are 2500 examples containing 50 target words and each word has 50 sentences with different meanings.", "labels": [], "entities": []}, {"text": "The exact meanings of the target words are blind, only the number of the meanings is provided in the data.", "labels": [], "entities": []}, {"text": "We compute the system performance with the sample dataset because it contains the answers of each candidate meaning.", "labels": [], "entities": []}, {"text": "The test dataset provided by the CLP2010 is similar to the sample dataset.", "labels": [], "entities": [{"text": "CLP2010", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9800955057144165}]}, {"text": "It contains 100 target words and 5000 instances in total.", "labels": [], "entities": []}, {"text": "However, it doesn't provide the answers.", "labels": [], "entities": []}, {"text": "The F-score measurement is the same as Zhao and.", "labels": [], "entities": [{"text": "F-score measurement", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9862179756164551}]}, {"text": "Given a particular class r L of size and a particular cluster of size , suppose in the cluster belong to then the value of this class and cluster is defined to be R L S is the recall value and is the precision value.", "labels": [], "entities": [{"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9980732202529907}, {"text": "precision", "start_pos": 200, "end_pos": 209, "type": "METRIC", "confidence": 0.9992430210113525}]}, {"text": "The F-score of class ( , ) the maximum value and F-score value follow: where is the total number of classes and n is the total size.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9916996955871582}, {"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9981542229652405}]}, {"text": "c reports the F-score of our feature-based Chinese WSI for different feature sets with various window sizes using K-means clustering.", "labels": [], "entities": [{"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9990159273147583}]}, {"text": "Since there are different results for each run of K-means clustering algorithm, we perform 20 trials and compute their average as the final results.", "labels": [], "entities": []}, {"text": "The columns denote different window size n, that is, then words before and after the target word are extracted as features.", "labels": [], "entities": []}, {"text": "Particularly, the size of infinity (\u221e) means that all the words in the sentence except the target word are considered.", "labels": [], "entities": []}, {"text": "The rows represent various combinations of feature sets and similarity measurements, currently, four of which are considered as follows: F-All: all the words are considered as features and from them feature vectors are constructed.", "labels": [], "entities": [{"text": "F-All", "start_pos": 137, "end_pos": 142, "type": "METRIC", "confidence": 0.9725216627120972}]}, {"text": "F-Stop: the top 150 most frequently occurring words in the total \"word bags\" of the corpus are regarded as stop words and thus removed from the feature set.", "labels": [], "entities": []}, {"text": "Feature vectors are then formed from these words.", "labels": [], "entities": []}, {"text": "S-All: the feature set and the feature vector are the same as those of F-All, but instead the similarity vector is used for clustering (c.f. Section 2.2).", "labels": [], "entities": []}, {"text": "S-Stop: the feature set and the feature vector are the same as those of F-Stop, but instead the similarity vector is used for clustering.", "labels": [], "entities": []}, {"text": "Experimental results for different feature sets with different window sizes using K-means clustering This table shows that S-Stop achieves the best performance of 0.7320 in F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 173, "end_pos": 180, "type": "METRIC", "confidence": 0.9436696767807007}]}, {"text": "This suggests that for K-means clustering, Chinese WSI can benefit much from removing stop words and adopting similarity vector.", "labels": [], "entities": [{"text": "K-means clustering", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.6781799793243408}, {"text": "Chinese WSI", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.7286240756511688}]}, {"text": "It also shows that: \ud97b\udf59 As the window size increases, the performance is almost consistently enhanced.", "labels": [], "entities": []}, {"text": "This indicates that all the words in the sentence more or less help disambiguate the target word.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 Removing stop words consistently improves the F-score for both similarity metrics.", "labels": [], "entities": [{"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9994798302650452}]}, {"text": "This means some high frequent words do not help discriminate the meaning of the target words, and further work on feature selection is thus encouraged.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.6649271547794342}]}, {"text": "\ud97b\udf59 Similarity vector consistently outperforms feature vector for stop-removed features, but not so for all-words features.", "labels": [], "entities": [{"text": "Similarity", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9412204027175903}]}, {"text": "This maybe due to the fact that, when the window size is limited, the influence of frequently occurring stop words is relatively high, thus the similarity vector misrepresent the context of the target word.", "labels": [], "entities": []}, {"text": "On the contrary, when stop words are removed or the context is wide, the similarity vector can better reflect the target word's context, leading to better performance.", "labels": [], "entities": []}, {"text": "In order to intuitively explain why the similarity vector is more discriminative than the feature vector, we take two sentences containing the Chinese word \"\ud97b\udf59\ud97b\udf59\" (hold, grasp) as an example ().", "labels": [], "entities": []}, {"text": "These two sentences have few common words, so clustering via feature vectors puts them into different classes.", "labels": [], "entities": []}, {"text": "However, since the similarities of these two feature vectors with other feature vectors are much similar, clustering via similarity vectors group them into the same class.", "labels": [], "entities": []}, {"text": "An example from the dataset According to the conclusion of the above experiments, it is better to include all the words except stop words in the sentence as the features in the subsequent experiment.", "labels": [], "entities": []}, {"text": "lists the results using various clustering algorithms with this same experimental setting.", "labels": [], "entities": []}, {"text": "It shows that the spectral clustering algorithm achieves the best performance of 0.7692 in F-score for Chinese WSI using the S-All setup.", "labels": [], "entities": [{"text": "spectral clustering", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.6949032992124557}, {"text": "F-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9959267973899841}, {"text": "Chinese WSI", "start_pos": 103, "end_pos": 114, "type": "DATASET", "confidence": 0.711281955242157}]}, {"text": "Additionally, there are some interesting findings: \ud97b\udf59 Although SC performs best, KM with similarity vectors achieves comparable results of 0.7320 units in F-score, slightly lower than that of SC.", "labels": [], "entities": [{"text": "F-score", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.9983943104743958}]}, {"text": "\ud97b\udf59 HAC performs worst among all clustering algorithms.", "labels": [], "entities": [{"text": "\ud97b\udf59", "start_pos": 0, "end_pos": 1, "type": "DATASET", "confidence": 0.8253071904182434}, {"text": "HAC", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.6442351937294006}]}, {"text": "An observation reveals that this algorithm always groups the instances into highly skewed clusters, i.e., one or two clusters are extremely large while others usually have only one instance in each cluster.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 It is surprising that S-All slightly outperforms F-All by only 0.0006 units in F-score.", "labels": [], "entities": [{"text": "F-All", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.7694339752197266}, {"text": "F-score", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9831215143203735}]}, {"text": "The truth is that, as discussed in the first experiment, KM using F-All doesn't consider instance density while S-All does.", "labels": [], "entities": []}, {"text": "On the contrary, SC identifies the eign-structure in the instance space and thus already considers the density information, therefore S-All will not significantly improve the performance.", "labels": [], "entities": []}, {"text": "Experiments results using different clustering algorithms <lexelt item=\"\ud97b\udf59\ud97b\udf59\" snum=\"4\"> <instance id=\"0012\"> \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59 \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59 <head>\ud97b\udf59\ud97b\udf59</head>\"\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59 \ud97b\udf59\ud97b\udf59 </instance> <instance id=\"0015\"> \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59 \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59<head>\ud97b\udf59 \ud97b\udf59</head>\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59 \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59 </instance> </lexelt>", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Experimental results for differ- ent feature sets with different window sizes us- ing K-means clustering", "labels": [], "entities": [{"text": "clustering", "start_pos": 104, "end_pos": 114, "type": "TASK", "confidence": 0.5088256001472473}]}, {"text": " Table 2  Experiments results using dif- ferent clustering algorithms", "labels": [], "entities": []}]}