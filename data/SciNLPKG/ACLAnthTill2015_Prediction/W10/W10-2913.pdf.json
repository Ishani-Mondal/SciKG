{"title": [{"text": "Computing Optimal Alignments for the IBM-3 Translation Model", "labels": [], "entities": [{"text": "Computing Optimal Alignments", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.619959423939387}, {"text": "IBM-3 Translation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7130624949932098}]}], "abstractContent": [{"text": "Prior work on training the IBM-3 translation model is based on suboptimal methods for computing Viterbi alignments.", "labels": [], "entities": [{"text": "IBM-3 translation", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7199799120426178}]}, {"text": "In this paper, we present the first method guaranteed to produce globally optimal alignments.", "labels": [], "entities": []}, {"text": "This not only results in improved alignments, it also gives us the opportunity to evaluate the quality of standard hillclimbing methods.", "labels": [], "entities": []}, {"text": "Indeed, hill-climbing works reasonably well in practice but still fails to find the global optimum for between 2% and 12% of all sentence pairs and the probabilities can be several tens of orders of magnitude away from the Viterbi alignment.", "labels": [], "entities": []}, {"text": "By reformulating the alignment problem as an Integer Linear Program, we can use standard machinery from global optimization theory to compute the solutions.", "labels": [], "entities": []}, {"text": "We use the well-known branch-and-cut method, but also show how it can be cus-tomized to the specific problem discussed in this paper.", "labels": [], "entities": []}, {"text": "In fact, a large number of alignments can be excluded from the start without losing global optimality.", "labels": [], "entities": []}], "introductionContent": [{"text": "proposed to approach the problem of automatic natural language translation from a statistical viewpoint and introduced five probability models, known as IBM 1-5.", "labels": [], "entities": [{"text": "automatic natural language translation", "start_pos": 36, "end_pos": 74, "type": "TASK", "confidence": 0.6095782145857811}]}, {"text": "Their models were single word based, where each source word could produce at most one target word.", "labels": [], "entities": []}, {"text": "State-of-the-art statistical translation systems follow the phrase based approach, e.g., and hence allow more general alignments.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.7125453650951385}]}, {"text": "Yet, single word based models are still highly relevant: many phrase based systems extract phrases from the alignments found by training the single word based models, and those that train phrases directly usually underperform these systems).", "labels": [], "entities": []}, {"text": "Single word based models can be divided into two classes.", "labels": [], "entities": []}, {"text": "On the one hand, models like IBM-1, IBM-2 and the HMM are computationally easy to handle: both marginals and Viterbi alignments can be computed by dynamic programming or even simpler techniques.", "labels": [], "entities": [{"text": "IBM-1", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.9203553199768066}, {"text": "IBM-2", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.9355964064598083}]}, {"text": "On the other hand there are fertility based models, including IBM 3-5 and Model 6.", "labels": [], "entities": []}, {"text": "These models have been shown to be of higher practical relevance than the members of the first class (Och and Ney, 2003) since they usually produce better alignments.", "labels": [], "entities": []}, {"text": "At the same time, computing Viterbi alignments for these methods has been shown to be NP-hard (), and computing marginals is no easier.", "labels": [], "entities": []}, {"text": "The standard way to handle these models -as implemented in GIZA++ ( -is to use a hillclimbing algorithm.", "labels": [], "entities": []}, {"text": "Recently Udupa and proposed an interesting approximation based on solving sequences of exponentially large subproblems by means of dynamic programming and also addressed the decoding problem.", "labels": [], "entities": []}, {"text": "In both cases there is noway to tell how faraway the result is from the Viterbi alignment.", "labels": [], "entities": [{"text": "Viterbi alignment", "start_pos": 72, "end_pos": 89, "type": "DATASET", "confidence": 0.9635277390480042}]}, {"text": "In this paper we solve the problem of finding IBM-3 Viterbi alignments by means of Integer Linear Programming.", "labels": [], "entities": [{"text": "IBM-3 Viterbi alignments", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.7187144557634989}]}, {"text": "While there is no polynomial run-time guarantee, in practice the applied branch-and-cut framework is fast enough to find optimal solutions even for the large Canadian Hansards task (restricted to sentences with at most 75 words), with a training time of 6 hours on a 2.4 GHz Core 2 Duo (single threaded).", "labels": [], "entities": [{"text": "Canadian Hansards task", "start_pos": 158, "end_pos": 180, "type": "DATASET", "confidence": 0.8232899308204651}]}, {"text": "Integer Linear Programming in the context of machine translation first appeared in the work of, who addressed the translation problem (often called decoding) in terms of a travelings-salesman like formulation.", "labels": [], "entities": [{"text": "Integer Linear Programming", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7947109937667847}, {"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7499570250511169}]}, {"text": "Recently, addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard.", "labels": [], "entities": []}, {"text": "The main difference to our work is that they allow only consecutive words in the phrases.", "labels": [], "entities": []}, {"text": "In their formulation, allowing arbitrary phrases would require an exponential number of variables.", "labels": [], "entities": []}, {"text": "In contrast, our approach handles the classical single word based model where any kind of \"phrases\" in the source sentence are aligned to one-word phrases in the target sentence.", "labels": [], "entities": []}, {"text": "Lacoste- propose an integer linear program fora symmetrized word-level alignment model.", "labels": [], "entities": [{"text": "Lacoste", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9267767667770386}, {"text": "word-level alignment", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.6657047271728516}]}, {"text": "Their approach also allows to take the alignments of neighboring words into account.", "labels": [], "entities": []}, {"text": "In contrast to our work, they only have a very crude fertility model and they are considering a substantially different model.", "labels": [], "entities": []}, {"text": "It should be noted, however, that a subclass of their problems can be solved in polynomial time -the problem is closely related to bipartite graph matching.", "labels": [], "entities": [{"text": "bipartite graph matching", "start_pos": 131, "end_pos": 155, "type": "TASK", "confidence": 0.7129772106806437}]}, {"text": "Less general approaches based on matching have been proposed in () and.", "labels": [], "entities": []}, {"text": "Recently proposed a very innovative cost function for jointly optimizing dictionary entries and alignments, which they minimize using integer linear programming.", "labels": [], "entities": []}, {"text": "They also include a mechanism to derive N-best lists.", "labels": [], "entities": []}, {"text": "However, they mention rather long computation times for rather small corpora.", "labels": [], "entities": []}, {"text": "It is not clear if the large Hansards tasks could be addressed by their method.", "labels": [], "entities": [{"text": "Hansards", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9175137281417847}]}, {"text": "An overview of integer linear programming approaches for natural language processing can be found on http://ilpnlp.wikidot.com/.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.6809873382250468}]}, {"text": "To facilitate further research in this area, the source code will be made publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have tested our method on three different tasks involving a total of three different languages and each in both directions.", "labels": [], "entities": []}, {"text": "The first task is the wellknown Canadian Hansards 2 task (senate debates) for French and English.", "labels": [], "entities": [{"text": "Canadian Hansards 2 task", "start_pos": 32, "end_pos": 56, "type": "DATASET", "confidence": 0.8575224131345749}]}, {"text": "Because of the large dataset we are currently only considering sentence pairs where both sentences have at most 75 words.", "labels": [], "entities": []}, {"text": "Longer sentences are usually not useful to derive model parameters.", "labels": [], "entities": []}, {"text": "The other two datasets are released by the European Corpus Initiative 3 . We choose the Union Bank of Switzerland (UBS) corpus for English and German and the Avalanche Bulletins, originally released by SFISAR, for French and German.", "labels": [], "entities": [{"text": "European Corpus Initiative 3", "start_pos": 43, "end_pos": 71, "type": "DATASET", "confidence": 0.9248570948839188}, {"text": "Union Bank of Switzerland (UBS) corpus", "start_pos": 88, "end_pos": 126, "type": "DATASET", "confidence": 0.6691468618810177}]}, {"text": "For the latter task we have annotated alignments for 150 of the training sentences, where one annotator specified sure and possible alignments.", "labels": [], "entities": []}, {"text": "For details, also on the alignment error rate, see.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 25, "end_pos": 45, "type": "METRIC", "confidence": 0.7835974097251892}]}, {"text": "All corpora have been preprocessed with language-specific rules; their statistics are given in.", "labels": [], "entities": []}, {"text": "We have integrated our method into the standard toolkit GIZA++ 4 and are using the training scheme 1 5 H 5 3 5 4 5 for all tasks.", "labels": [], "entities": [{"text": "GIZA++ 4", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.8962790369987488}]}, {"text": "While we focus on the IBM-3 stage, we also discuss the quality of the resulting IBM-4 parameters and alignments.", "labels": [], "entities": []}, {"text": "Experiments were run on a 2.4 GHz Core 2 Duo with 4 GB memory.", "labels": [], "entities": []}, {"text": "For most sentence pairs, the memory consumption of our method is only marginally more than in standard GIZA++ (600 MB).", "labels": [], "entities": []}, {"text": "In the first iteration on the large Hansards task, however, there area few very difficult sentence pairs where the solver needs up to 90 minutes and 1.5 GB . We observed this in both translation directions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. We have integrated our method into the  standard toolkit GIZA++ 4 and are using the train- ing scheme 1 5 H 5 3 5 4 5 for all tasks. While we fo- cus on the IBM-3 stage, we also discuss the quality  of the resulting IBM-4 parameters and alignments.", "labels": [], "entities": []}, {"text": " Table 1: Corpus statistics for all employed (train- ing) corpora, after preprocessing.", "labels": [], "entities": []}, {"text": " Table 2: Analysis of Hillclimbing on all considered tasks. All numbers are for the IBM-3 translation  model. Iteration 1 is the first iteration after the transfer from HMM, the final iteration is the transfer to  IBM4. The factors are w.r.t. the original formulation, not the negative logarithm of it and are defined as  the maximal ratio between the Viterbi probability and the hillclimbing probability.", "labels": [], "entities": [{"text": "IBM-3 translation", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.6279481202363968}, {"text": "IBM4", "start_pos": 214, "end_pos": 218, "type": "DATASET", "confidence": 0.9781506061553955}]}, {"text": " Table 3: Analysis of the perplexities in training.", "labels": [], "entities": []}, {"text": " Table 4: Alignment error rates on the Avalanche  bulletin task.", "labels": [], "entities": [{"text": "Alignment error rates", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.7204472124576569}, {"text": "Avalanche  bulletin task", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.883074164390564}]}]}