{"title": [], "abstractContent": [{"text": "In this work, we show how active learning in some (target) domain can leverage information from a different but related (source) domain.", "labels": [], "entities": []}, {"text": "We present an algorithm that harnesses the source domain data to learn the best possible initializer hypothesis for doing active learning in the target domain, resulting in improved label complexity.", "labels": [], "entities": []}, {"text": "We also present a variant of this algorithm which additionally uses the domain divergence information to selectively query the most informative points in the target domain, leading to further reductions in label complexity.", "labels": [], "entities": []}, {"text": "Experimental results on a variety of datasets establish the efficacy of the proposed methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Acquiring labeled data to train supervised learning models can be difficult or expensive in many problem domains.", "labels": [], "entities": []}, {"text": "Active Learning tries to circumvent this difficultly by only querying the labels of the most informative examples and, in several cases, has been shown to achieve exponentially lower labelcomplexity (number of queried labels) than supervised learning.", "labels": [], "entities": []}, {"text": "Domain Adaptation), although motivated somewhat differently, attempts to address a seemingly similar problem: lack of labeled data in some target domain.", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6749813705682755}]}, {"text": "Domain Adaptation deals with this problem using labeled data from a different (but related) source domain.", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7341520190238953}]}, {"text": "In this paper, we consider the supervised domain adaptation setting having a large amount of labeled data from a source domain, a large amount of unlabeled data from a target domain, and additionally a small budget for acquiring labels in the target domain.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.7151411175727844}]}, {"text": "We show how, apart from leveraging information in the usual domain adaptation sense, the information from the source domain can be leveraged to intelligently query labels in the target domain.We achieve this by first training the best possible classifier without using target domain labeled data 1 and then using the learned classifier to leverage the inter-domain information when we are additionally provided some fixed budget for acquiring extra labeled target data (i.e., the active learning setting).", "labels": [], "entities": []}, {"text": "There are several ways in which our \"best classifier\" can be utilized.", "labels": [], "entities": []}, {"text": "Our first approach uses this classifier as the initializer while doing (online) active learning in the target domain (Section 3).", "labels": [], "entities": []}, {"text": "Then we present a variant augmenting the first approach using a domain-separator hypothesis which leads to additionally ruling out querying the labels of those target examples that appear \"similar\" to the source domain (Section 4).", "labels": [], "entities": []}, {"text": "shows our basic setup which uses a source (or unsupervised domain-adapted source) classifier v 0 as an initializer for doing active learning in the target domain having some small, fixed budget for querying labels.", "labels": [], "entities": []}, {"text": "Our framework consists of 2 phases: 1) Learning the best possible classi- fier v 0 using source labeled (L S ) and unlabeled data (U S ), and target unlabeled (U T ) data, and 2) Querying labels for target domain examples by leveraging information from the classifier learned in phase-1.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we demonstrate the empirical performance of our algorithms and compare them with a summarizes the methods used with a brief description of each.", "labels": [], "entities": []}, {"text": "Among the first three (ID, SDA, FEDA), FEDA) is a state-of-the-art supervised domain adaptation method but assumes passively acquired labels.", "labels": [], "entities": [{"text": "FEDA", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9833425283432007}, {"text": "FEDA", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9534505009651184}]}, {"text": "The last four, RIAL, ZIAL, SIAL and AODA methods in acquire labels in an active fashion.", "labels": [], "entities": [{"text": "RIAL", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.70179283618927}, {"text": "AODA", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.8994730710983276}]}, {"text": "As the description denotes, RIAL and ZIAL start active learning in target with a randomly initialized and zero initialized base hypothesis, respectively.", "labels": [], "entities": [{"text": "RIAL", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9854230880737305}, {"text": "ZIAL", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.8936137557029724}]}, {"text": "It is also important to distinguish between SIAL and AODA here: SIAL uses an unmodified classifier learned only from source labeled data as the initializer, whereas AODA uses an unsupervised domain-adaptation technique (i.e., without using labeled target data) to learn the initializer.", "labels": [], "entities": []}, {"text": "In our experiments, we use the instance reweighting approach () to perform the unsupervised domain adaptation step.", "labels": [], "entities": []}, {"text": "However, we note that this step can also be performed using any other unsupervised domain adaptation technique such as Structural Correspondence Learning (SCL) ).", "labels": [], "entities": [{"text": "Structural Correspondence Learning (SCL)", "start_pos": 119, "end_pos": 159, "type": "TASK", "confidence": 0.6218825330336889}]}, {"text": "We compare all the approaches based on classification accuracies achieved fora given budget of labeled target examples (Section-5.2), and number of labels requested fora fixed pool of unlabeled target examples and corresponding accuracies  We report our empirical results for the task of sentiment classification using data provided by) which consists of user reviews of eight product types (apparel, books, DVD, electronics, kitchen, music, video, and other) from Amazon.com.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 288, "end_pos": 312, "type": "TASK", "confidence": 0.8515872657299042}]}, {"text": "We also apply PCA to reduce the datadimensionality to 50.", "labels": [], "entities": [{"text": "PCA", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.6922488212585449}]}, {"text": "The sentiment classification task for this dataset is binary classification which corresponds to classifying a review as positive or negative.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9199911653995514}]}, {"text": "The sentiment dataset consists of several domain pairs with varying A-distance (which measures the domain separation), akin to the sense described in).", "labels": [], "entities": [{"text": "A-distance", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9790452122688293}]}, {"text": "presents the domain pairs used in our experiments and their corresponding domain divergences in terms of the A-distance).", "labels": [], "entities": [{"text": "A-distance", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9539775848388672}]}, {"text": "To compute the A-distance from finite samples of source and target domain, we use a surrogate to the true A-distance (the proxy A-distance) in a manner similar to): First, we train a linear classifier to separate the source domain from the target domain using only unlabeled examples from both.", "labels": [], "entities": [{"text": "A-distance", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9506703615188599}]}, {"text": "The average per-instance hinge-loss of this classifier subtracted from 1 serves as our estimate of the proxy A-distance.", "labels": [], "entities": []}, {"text": "A score of 1 means perfectly separable distributions whereas a score of 0 means that the two distributions are essentially the same.", "labels": [], "entities": []}, {"text": "As a general rule, a high score means that the two domains are reasonably far apart.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Classification accuracies for DVD\u2192BOOKS, for fixed target budget.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.667523980140686}, {"text": "BOOKS", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.732802152633667}]}, {"text": " Table 4: Classification accuracies for KITCHEN\u2192APPAREL, for fixed target budget.", "labels": [], "entities": [{"text": "Classification accuracies", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.5765136182308197}, {"text": "KITCHEN", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.7516560554504395}, {"text": "APPAREL", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.5565495491027832}]}, {"text": " Table 5: Accuracy and label complexity of DVD\u2192BOOKS", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980443716049194}, {"text": "BOOKS", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.7130207419395447}]}]}