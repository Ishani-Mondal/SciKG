{"title": [{"text": "A Semi-Supervised Batch-Mode Active Learning Strategy for Improved Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.7323474287986755}]}], "abstractContent": [{"text": "The availability of substantial, in-domain parallel corpora is critical for the development of high-performance statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 112, "end_pos": 149, "type": "TASK", "confidence": 0.785219078262647}]}, {"text": "Such corpora, however, are expensive to produce due to the labor intensive nature of manual translation.", "labels": [], "entities": []}, {"text": "We propose to alleviate this problem with a novel, semi-supervised, batch-mode active learning strategy that attempts to maximize in-domain coverage by selecting sentences, which represent a balance between domain match, translation difficulty, and batch diversity.", "labels": [], "entities": []}, {"text": "Simulation experiments on an English-to-Pashto translation task show that the proposed strategy not only outper-forms the random selection baseline, but also traditional active learning techniques based on dissimilarity to existing training data.", "labels": [], "entities": [{"text": "English-to-Pashto translation task", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.7391438980897268}]}, {"text": "Our approach achieves a relative improvement of 45.9% in BLEU over the seed baseline, while the closest competitor gained only 24.8% with the same number of selected sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9986373782157898}]}], "introductionContent": [{"text": "Rapid development of statistical machine translation (SMT) systems for resource-poor language pairs is a problem of significant interest to the research community in academia, industry, and government.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.7925149152676264}]}, {"text": "Tight turn-around schedules, budget restrictions, and scarcity of human translators preclude the production of large parallel corpora, which form the backbone of SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 162, "end_pos": 165, "type": "TASK", "confidence": 0.9956598877906799}]}, {"text": "Given these constraints, the focus is on making the best possible use of available resources.", "labels": [], "entities": []}, {"text": "This usually involves some form of prioritized data collection.", "labels": [], "entities": []}, {"text": "In other words, one would like to construct the smallest possible parallel training corpus that achieves a desired level of performance on unseen test data.", "labels": [], "entities": []}, {"text": "Within an active learning framework, this can be cast as a data selection problem.", "labels": [], "entities": []}, {"text": "The goal is to choose, for manual translation, the most informative instances from a large pool of source language sentences.", "labels": [], "entities": [{"text": "manual translation", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7089468091726303}]}, {"text": "The resulting sentence pairs, in combination with any existing in-domain seed parallel corpus, are expected to provide a significantly higher performance gain than a na\u00a8\u0131vena\u00a8\u0131ve random selection strategy.", "labels": [], "entities": []}, {"text": "This process is repeated until a certain level of performance is attained.", "labels": [], "entities": []}, {"text": "Previous work on active learning for SMT has focused on unsupervised dissimilarity measures for sentence selection.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9917963743209839}, {"text": "sentence selection", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.724783718585968}]}, {"text": "describe a selection strategy that attempts to maximize coverage by choosing sentences with the highest proportion of previously unseen n-grams.", "labels": [], "entities": []}, {"text": "However, if the pool is not completely in-domain, this strategy may select irrelevant sentences, whose translations are unlikely to improve performance on an in-domain test set.", "labels": [], "entities": []}, {"text": "They also propose a technique, based on TF-IDF, to de-emphasize sentences similar to those that have already been selected.", "labels": [], "entities": []}, {"text": "However, this strategy is bootstrapped by random initial choices that do not necessarily favor sentences that are difficult to translate.", "labels": [], "entities": []}, {"text": "Finally, they work exclusively with the source language and do not use any SMT-derived features to guide selection.", "labels": [], "entities": [{"text": "SMT-derived", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.9581711292266846}]}, {"text": "propose a number of features, such as similarity to the seed corpus, translation probability, relative frequencies of n-grams and \"phrases\" in the seed vs. pool data, etc., for active learning.", "labels": [], "entities": [{"text": "translation", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.9352571964263916}]}, {"text": "While many of their experiments use the above features independently to compare their relative efficacy, one of their experiments attempts to predict a rank, as a linear combination of these features, for each candidate sentence.", "labels": [], "entities": []}, {"text": "The top-ranked sentences are chosen for manual translation.", "labels": [], "entities": [{"text": "manual translation", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7411163747310638}]}, {"text": "The latter strategy is particularly relevant to this paper, because the goal of our active learning strategy is not to compare features, but to learn the trade-off between various characteristics of the candidate sentences that potentially maximizes translation improvement.", "labels": [], "entities": []}, {"text": "The parameters of the linear ranking model proposed by are trained using two held-out development sets D 1 and D 2 -the model attempts to learn the ordering of D 1 that incrementally maximizes translation performance on D 2 . Besides the need for multiple parallel corpora and the computationally intensive nature of incrementally retraining an SMT system, their approach suffers from another major deficiency.", "labels": [], "entities": [{"text": "SMT", "start_pos": 345, "end_pos": 348, "type": "TASK", "confidence": 0.9886227250099182}]}, {"text": "It requires that the pool have the same distributional characteristics as the development sets used to train the ranking model.", "labels": [], "entities": []}, {"text": "Additionally, they select all sentences that constitute a batch in a single operation following the ranking procedure.", "labels": [], "entities": []}, {"text": "Since similar or identical sentences in the pool will typically meet the selection criteria simultaneously, this can have the undesired effect of choosing redundant batches with low diversity.", "labels": [], "entities": []}, {"text": "This results in under-utilization of human translation resources.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel batch-mode active learning strategy that ameliorates the above issues.", "labels": [], "entities": []}, {"text": "Our semi-supervised learning approach combines a parallel ranking strategy with several features, including domain representativeness, translation confidence, and batch diversity.", "labels": [], "entities": [{"text": "translation", "start_pos": 135, "end_pos": 146, "type": "TASK", "confidence": 0.9584325551986694}]}, {"text": "The proposed approach includes a greedy, incremental batch selection strategy, which encourages diversity and reduces redundancy.", "labels": [], "entities": []}, {"text": "The following sections detail our active learning approach, including the experimental setup and simulation results that clearly demonstrate its effectiveness.", "labels": [], "entities": []}], "datasetContent": [{"text": "We demonstrate the effectiveness of the proposed sentence selection algorithm by performing a set of simulation experiments in the context of an English-to-Pashto (E2P) translation task.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.8090568780899048}, {"text": "English-to-Pashto (E2P) translation task", "start_pos": 145, "end_pos": 185, "type": "TASK", "confidence": 0.7124538818995158}]}, {"text": "We simulate a low-resource condition by using a very small number of training sentence pairs, sampled from the collection, to bootstrap a phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 151, "end_pos": 154, "type": "TASK", "confidence": 0.8817726969718933}]}, {"text": "The remainder of this parallel corpus is set aside as the pool.", "labels": [], "entities": []}, {"text": "At each iteration, the selection algorithm picks a fixed-size batch of source sentences from the pool.", "labels": [], "entities": []}, {"text": "The seed training data are augmented with the chosen source sentences and their translations.", "labels": [], "entities": []}, {"text": "A new set of translation models is then estimated and used to decode the test set.", "labels": [], "entities": []}, {"text": "We track SMT performance across several iterations and compare the proposed algorithm to a random selection baseline as well as other common selection strategies.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9954513907432556}]}], "tableCaptions": []}