{"title": [{"text": "The RALI Machine Translation System for WMT 2010", "labels": [], "entities": [{"text": "RALI Machine Translation", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7151676217714945}, {"text": "WMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.8619435429573059}]}], "abstractContent": [{"text": "We describe our system for the translation task of WMT 2010.", "labels": [], "entities": [{"text": "translation task", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.9354552030563354}, {"text": "WMT 2010", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.7035244107246399}]}, {"text": "This system, developed for the English-French and French-English directions, is based on Moses and was trained using only the resources supplied for the workshop.", "labels": [], "entities": []}, {"text": "We report experiments to enhance it with out-of-domain parallel corpora sub-sampling, N-best list post-processing and a French grammatical checker.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents the phrase-based machine translation system developed at RALI in order to participate in both the French-English and English-French translation tasks.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.6128811836242676}, {"text": "RALI", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8359135389328003}, {"text": "English-French translation tasks", "start_pos": 137, "end_pos": 169, "type": "TASK", "confidence": 0.7663644750912985}]}, {"text": "In these two tasks, we used all the corpora supplied for the constraint data condition apart from the LDC Gigaword corpora.", "labels": [], "entities": [{"text": "LDC Gigaword corpora", "start_pos": 102, "end_pos": 122, "type": "DATASET", "confidence": 0.8478176593780518}]}, {"text": "We describe its different components in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 reports our experiments to subsample the available out-of-domain corpora in order to adapt the translation models to the news domain.", "labels": [], "entities": []}, {"text": "Section 4, dedicated to post-processing, presents how N-best lists are reranked and how the French 1-best output is corrected by a grammatical checker.", "labels": [], "entities": []}, {"text": "Section 5 studies how the original source language of news acts upon translation quality.", "labels": [], "entities": []}, {"text": "We conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section reports experiments done on the news-test2009 corpus for testing various configurations.", "labels": [], "entities": [{"text": "news-test2009 corpus", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.9765669107437134}]}, {"text": "In these first experiments, we trained LMs and translation models on the Europarl corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.9943958222866058}]}, {"text": "Case We tested two methods to handle case.", "labels": [], "entities": []}, {"text": "The first one lowercases all training data and documents to translate, while the second one normalizes all training data and documents into their natural case.", "labels": [], "entities": [{"text": "translate", "start_pos": 60, "end_pos": 69, "type": "TASK", "confidence": 0.9665281772613525}]}, {"text": "These two methods require a postprocessing recapitalization but this last step is more basic for the truecase method.", "labels": [], "entities": []}, {"text": "Training models on lowercased material led to a 23.15 % caseinsensitive BLEU and a 21.61 % case-sensitive BLEU; from truecased corpora, we obtained a 23.24 % case-insensitive BLEU and a 22.13 % case-sensitive BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9479120969772339}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9400539398193359}, {"text": "BLEU", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9413878321647644}, {"text": "BLEU", "start_pos": 209, "end_pos": 213, "type": "METRIC", "confidence": 0.9549586772918701}]}, {"text": "As truecasing induces an increase of the two metrics, we built all our models in truecase.", "labels": [], "entities": []}, {"text": "The results shown in the remainder of this paper are reported in terms of caseinsensitive BLEU which showed last year a better correlation with human judgments than casesensitive BLEU for the two languages we consider . Tokenization Two tokenizers were tested: one provided for the workshop and another we developed.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9407536387443542}, {"text": "BLEU", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.8967126607894897}]}, {"text": "They differ mainly in the processing of compound words: our in-house tokenizer splits these words (e.g. percentage-wise is turned into percentage -wise), which improves the lexical coverage of the models trained on the corpus.", "labels": [], "entities": []}, {"text": "This feature does not exist in the WMT tool.", "labels": [], "entities": [{"text": "WMT tool", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.7618440389633179}]}, {"text": "However, using the WMT tokenizer, we measured a 23.24 % BLEU, while our in-house tokenizer yielded a lower BLEU of 22.85 %.", "labels": [], "entities": [{"text": "WMT tokenizer", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.5678019374608994}, {"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9996923208236694}, {"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9995077848434448}]}, {"text": "Follow these results prompted us to use the WMT tokenizer.", "labels": [], "entities": [{"text": "WMT tokenizer", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.616585984826088}]}, {"text": "Serialization In order to test the effect of serialization, i.e. the mapping of all numbers to a special unique token, we measured the BLEU score obtained by a PBM trained on Europarl for English-French, when numbers are left unchanged, line 1) or serialized (line 2).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.9801038205623627}, {"text": "Europarl", "start_pos": 175, "end_pos": 183, "type": "DATASET", "confidence": 0.9778359532356262}]}, {"text": "These results exhibit a slight decrease of BLEU when serialization is performed.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9995892643928528}]}, {"text": "Moreover, if BLEU is computed using a serialized reference (line 3), which is equivalent to ignoring deserialization errors, a minor gain of BLEU is observed, which validates our recovering method.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9977483153343201}, {"text": "BLEU", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9990222454071045}]}, {"text": "Since resorting to serialization/deserialization yields comparable performance to a system not using it, while reducing the model's size, we chose to use it.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: LMs perplexities and BLEU scores mea- sured on news-test2009. Translation models  used here were trained on nc and Europarl.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9994738698005676}, {"text": "mea- sured", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.8784853418668112}, {"text": "Translation", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.9549044966697693}, {"text": "Europarl", "start_pos": 125, "end_pos": 133, "type": "DATASET", "confidence": 0.9798348546028137}]}, {"text": " Table 4: BLEU measured on news-test2009 for  English-French and French-English using transla- tion models trained on nc and a subset of out-of- domain corpora.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9957554340362549}, {"text": "news-test2009", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.9431880712509155}]}, {"text": " Table 5: Official results of our system on news-test2010.", "labels": [], "entities": []}, {"text": " Table 6: BLEU scores measured on parts of  news-test2010 according to the original source  language.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9977774024009705}]}]}