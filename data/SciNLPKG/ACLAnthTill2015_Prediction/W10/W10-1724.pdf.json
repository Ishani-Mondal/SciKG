{"title": [{"text": "Linear Inversion Transduction Grammar Alignments as a Second Translation Path", "labels": [], "entities": [{"text": "Linear Inversion Transduction Grammar Alignments", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.6549936354160308}]}], "abstractContent": [{"text": "We explore the possibility of using Stochastic Bracketing Linear Inversion Transduction Grammars fora full-scale German-English translation task, both on their own and in conjunction with alignments induced with GIZA++.", "labels": [], "entities": [{"text": "Stochastic Bracketing Linear Inversion Transduction Grammars", "start_pos": 36, "end_pos": 96, "type": "TASK", "confidence": 0.6341746399799982}, {"text": "German-English translation task", "start_pos": 113, "end_pos": 144, "type": "TASK", "confidence": 0.6316963036855062}]}, {"text": "The rationale for transduction grammars, the details of the system and some results are presented .", "labels": [], "entities": [{"text": "transduction grammars", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.9044484794139862}]}], "introductionContent": [{"text": "Lately, there has been some interest in using Inversion Transduction Grammars (ITGs) for alignment purposes.", "labels": [], "entities": []}, {"text": "The main problem with ITGs is the time complexity, O(Gn 6 ) doesn't scale well.", "labels": [], "entities": [{"text": "time complexity", "start_pos": 34, "end_pos": 49, "type": "METRIC", "confidence": 0.9500201046466827}, {"text": "O", "start_pos": 51, "end_pos": 52, "type": "METRIC", "confidence": 0.9764704704284668}]}, {"text": "By limiting the grammar to a bracketing ITG (BITG), the grammar constant (G) can be eliminated, but O(n 6 ) is still prohibitive for large data sets.", "labels": [], "entities": [{"text": "BITG", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9243610501289368}, {"text": "grammar constant (G)", "start_pos": 56, "end_pos": 76, "type": "METRIC", "confidence": 0.7665518164634705}, {"text": "O", "start_pos": 100, "end_pos": 101, "type": "METRIC", "confidence": 0.9944173097610474}]}, {"text": "There has been some work on approximate inference of ITGs.", "labels": [], "entities": [{"text": "approximate inference of ITGs", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.6351563259959221}]}, {"text": "present a method for evaluating spans in the sentence pair to determine whether they should be excluded or not.", "labels": [], "entities": []}, {"text": "The algorithm has a best case time complexity of O(n 3 ).", "labels": [], "entities": [{"text": "O", "start_pos": 49, "end_pos": 50, "type": "METRIC", "confidence": 0.9503860473632812}]}, {"text": "Saers, introduce abeam pruning scheme, which reduces time complexity to O(bn 3 ).", "labels": [], "entities": [{"text": "time complexity", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.9191769659519196}, {"text": "O", "start_pos": 72, "end_pos": 73, "type": "METRIC", "confidence": 0.957655131816864}]}, {"text": "They also show that severe pruning is possible without significant deterioration in alignment quality (as measured by downstream translation quality).", "labels": [], "entities": []}, {"text": "use a simpler aligner as guidance for pruning, which reduces the time complexity by two orders of magnitude.", "labels": [], "entities": [{"text": "time complexity", "start_pos": 65, "end_pos": 80, "type": "METRIC", "confidence": 0.9162414371967316}]}, {"text": "Their work also partially implements the phrasal ITGs for translationdriven segmentation introduced in, although they only allow for one-to-many alignments, rather than many-to-many alignments.", "labels": [], "entities": [{"text": "translationdriven segmentation", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.9689925909042358}]}, {"text": "A more extreme approach is taken in Saers,.", "labels": [], "entities": []}, {"text": "Not only is the search severely pruned, but the grammar itself is limited to a linearized form, getting rid of branching within a single parse.", "labels": [], "entities": []}, {"text": "Although a small deterioration in downstream translation quality is noted (compared to harshly pruned SBITGs), the grammar can be induced in linear time.", "labels": [], "entities": []}, {"text": "In this paper we apply SBLITGs to a full size German-English WMT'10 translation task.", "labels": [], "entities": [{"text": "WMT'10 translation task", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.8413239518801371}]}, {"text": "We also use differentiated translation paths to combine SBLITG translation models with a standard GIZA++ translation model.", "labels": [], "entities": [{"text": "SBLITG translation", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7508578896522522}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Corpora available for the German-English translation task after baseline cleaning.", "labels": [], "entities": [{"text": "German-English translation task", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.6103627483050028}]}, {"text": " Table 2: Results for the German-English translation task.", "labels": [], "entities": [{"text": "German-English translation task", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.7359123826026917}]}]}