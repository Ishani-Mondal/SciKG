{"title": [{"text": "Detecting Hedge Cues and their Scopes with Average Perceptron", "labels": [], "entities": [{"text": "Detecting Hedge Cues", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8353912234306335}]}], "abstractContent": [{"text": "In this paper, we proposed a hedge detection method with average perceptron, which was used in the closed challenge in CoNLL-2010 Shared Task.", "labels": [], "entities": [{"text": "hedge detection", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7964452505111694}, {"text": "CoNLL-2010 Shared Task", "start_pos": 119, "end_pos": 141, "type": "DATASET", "confidence": 0.7955205837885538}]}, {"text": "There are two subtasks: (1) detecting uncertain sentences and (2) identifying the in-sentence scopes of hedge cues.", "labels": [], "entities": [{"text": "detecting uncertain sentences", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.8614349563916525}]}, {"text": "We use the unified learning algorithm for both subtasks since that the hedge score of sentence can be decomposed into scores of the words, especially the hedge words.", "labels": [], "entities": []}, {"text": "On the biomedical corpus, our methods achieved F-measure with 77.86% in detecting in-domain uncertain sentences, 77.44% in recognizing hedge cues, and 19.27% in identifying the scopes.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9989245533943176}]}], "introductionContent": [{"text": "Detecting hedged information in biomedical literatures has received considerable interest in the biomedical natural language processing (NLP) community recently.", "labels": [], "entities": [{"text": "Detecting hedged information in biomedical literatures", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.8879386285940806}, {"text": "biomedical natural language processing (NLP)", "start_pos": 97, "end_pos": 141, "type": "TASK", "confidence": 0.7460856097085136}]}, {"text": "Hedge information indicates that authors do not or cannot backup their opinions or statements with facts (, which exists in many natural language texts, such as webpages or blogs, as well as biomedical literatures.", "labels": [], "entities": []}, {"text": "For many NLP applications, such as question answering and information extraction, the information extracted from hedge sentences would be harmful to their final performances.", "labels": [], "entities": [{"text": "question answering", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.9291374981403351}, {"text": "information extraction", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7951226532459259}]}, {"text": "Therefore, the hedge or speculative information should be detected in advance, and dealt with different approaches or discarded directly.", "labels": [], "entities": []}, {"text": "In, there are two different level subtasks: detecting sentences containing uncertainty and identifying the in-sentence scopes of hedge cues.", "labels": [], "entities": []}, {"text": "For example, in the following sentence: These results suggest that the IRE motif in the ALAS mRNA is functional and imply that translation of the mRNA is controlled by cellular iron availability during erythropoiesis.", "labels": [], "entities": [{"text": "IRE", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9382014274597168}, {"text": "ALAS mRNA", "start_pos": 88, "end_pos": 97, "type": "DATASET", "confidence": 0.7822580337524414}]}, {"text": "The words suggest and imply indicate that the statements are not supported with facts.", "labels": [], "entities": []}, {"text": "In the first subtask, the sentence is considered as uncertainty.", "labels": [], "entities": []}, {"text": "In the second subtask, suggest and imply are identified as hedge cues, while the consecutive blocks suggest that the IRE motif in the ALAS mRNA is functional and imply that translation of the mRNA is controlled by cellular iron availability during erythropoiesis are recognized as their corresponding scopes.", "labels": [], "entities": [{"text": "IRE", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9497762322425842}, {"text": "ALAS mRNA", "start_pos": 134, "end_pos": 143, "type": "DATASET", "confidence": 0.7598384022712708}]}, {"text": "In this paper, we proposed a hedge detection method with average perceptron, which was used in the closed challenges in.", "labels": [], "entities": [{"text": "hedge detection", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7804831266403198}]}, {"text": "Our motivation is to use a unified model to detect two level hedge information (word-level and sentence-level) and the model is easily expanded to joint learning of two subtasks.", "labels": [], "entities": []}, {"text": "Since that the hedge score of sentence can be decomposed into scores of the words, especially the hedge words, we chosen linear classifier in our method and used average perceptron as the training algorithm.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, a brief review of related works is presented.", "labels": [], "entities": []}, {"text": "Then, we describe our method in Section 3.", "labels": [], "entities": []}, {"text": "Experiments and results are presented in the section 4.", "labels": [], "entities": []}, {"text": "Finally, the conclusion will be presented in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report our experiments on datasets of CoNLL-2010 shared tasks, including the official results and our experimental results when developing the system.", "labels": [], "entities": []}, {"text": "Our system architecture is shown in, which consists of the following modules.", "labels": [], "entities": []}, {"text": "1. corpus preprocess module, which employs a tokenizer to normalize the corpus; 2.", "labels": [], "entities": []}, {"text": "sentence detection module, which uses a binary sentence-level classifier to determine whether a sentence contains uncertainty information; 3.", "labels": [], "entities": [{"text": "sentence detection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7515923082828522}]}, {"text": "hedge cues detection module, which identifies which words in a sentence are the hedge cues, we train a binary word-level classifier; 4.", "labels": [], "entities": [{"text": "hedge cues detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6796222825845083}]}, {"text": "cue scope recognition module, which recognizes the corresponding scope for each hedge cue by another word-level classifier.", "labels": [], "entities": [{"text": "cue scope recognition", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6451465785503387}]}, {"text": "Our experimental results are obtained on the training datasets by 10-fold cross validation.", "labels": [], "entities": []}, {"text": "The maximum iteration number for training the average perceptron is set to 20.", "labels": [], "entities": []}, {"text": "Our system is implemented with Java: Statistical information on annotated corpus.", "labels": [], "entities": []}, {"text": "In CoNLL-2010 Shared Task, two different datasets are provided to develop the system: (1) biological abstracts and full articles from the BioScope corpus, (2) paragraphs from Wikipedia.", "labels": [], "entities": [{"text": "BioScope corpus", "start_pos": 138, "end_pos": 153, "type": "DATASET", "confidence": 0.8714782893657684}]}, {"text": "Besides manually annotated datasets, three corresponding unlabeled datasets are also allowed for the closed challenges.", "labels": [], "entities": []}, {"text": "But we have not employed any unlabeled datasets in our system.", "labels": [], "entities": []}, {"text": "A preliminary statistics can be found in Table 1.", "labels": [], "entities": []}, {"text": "We make no distinction between sentences from abstracts or full articles in biomedical dataset.", "labels": [], "entities": []}, {"text": "From Table 1, most sentences are certainty while about 18% sentences in biomedical dataset and 22% in Wikipedia dataset are speculative.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 102, "end_pos": 119, "type": "DATASET", "confidence": 0.9826012849807739}]}, {"text": "On the average, there exists nearly 1.29 hedge cues per sentence in biomedical dataset and 1.26 in Wikipedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 99, "end_pos": 108, "type": "DATASET", "confidence": 0.9649975895881653}]}, {"text": "The average length of hedge cues varies in these two corpus.", "labels": [], "entities": []}, {"text": "In biomedical dataset, hedge cues are nearly one word, but more than two words in Wikipedia.", "labels": [], "entities": []}, {"text": "On average, the scope of hedge cue covers 15.42 words.", "labels": [], "entities": [{"text": "hedge cue", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.8099576830863953}]}, {"text": "Precision: Results for in-domain uncertain sentences detection shows the results across domains.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9825703501701355}, {"text": "in-domain uncertain sentences detection", "start_pos": 23, "end_pos": 62, "type": "TASK", "confidence": 0.6744625568389893}]}, {"text": "We split each annotated dataset into 10 folds.", "labels": [], "entities": []}, {"text": "Then training dataset is combined by individually drawing 9 folds out from the split datasets and the rests are used as the test data.", "labels": [], "entities": []}, {"text": "On biomedical dataset, F1-measure gets to 79.24% while 56.16% on Wikipedia dataset.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.999005138874054}, {"text": "Wikipedia dataset", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.9812645018100739}]}, {"text": "Compared with the results within domain, over 5% performance decreases from 84.39% to 79.24% on biomedical, but a slightly increase on Wikipedia.: Results for across-domain uncertain sentences detection", "labels": [], "entities": [{"text": "across-domain uncertain sentences detection", "start_pos": 159, "end_pos": 202, "type": "TASK", "confidence": 0.6411681473255157}]}], "tableCaptions": [{"text": " Table 1: Statistical information on annotated cor- pus.", "labels": [], "entities": []}, {"text": " Table 2: Results for in-domain uncertain sentences  detection", "labels": [], "entities": [{"text": "in-domain uncertain sentences  detection", "start_pos": 22, "end_pos": 62, "type": "TASK", "confidence": 0.6828794777393341}]}, {"text": " Table 3: Results for across-domain uncertain sen- tences detection", "labels": [], "entities": [{"text": "across-domain uncertain sen- tences detection", "start_pos": 22, "end_pos": 67, "type": "TASK", "confidence": 0.6070614258448283}]}, {"text": " Table 6: Results for in-domain hedge cue identifi- cation", "labels": [], "entities": []}, {"text": " Table 4: Top 10 significant words in detecting uncertain sentences", "labels": [], "entities": [{"text": "detecting uncertain sentences", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.9087952176729838}]}, {"text": " Table 5: Top 5 significant POS in detecting uncertain sentences", "labels": [], "entities": [{"text": "POS", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9390852451324463}, {"text": "detecting uncertain sentences", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.9223074316978455}]}, {"text": " Table 8: Results for scopes recognizing with gold  hedge cues (word-level)", "labels": [], "entities": [{"text": "scopes recognizing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.9034433662891388}]}, {"text": " Table 9: Official results for scopes recognizing  (block level)", "labels": [], "entities": [{"text": "scopes recognizing", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9523629546165466}]}]}