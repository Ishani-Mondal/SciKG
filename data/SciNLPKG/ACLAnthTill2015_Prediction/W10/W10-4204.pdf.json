{"title": [{"text": "Hierarchical Reinforcement Learning for Adaptive Text Generation", "labels": [], "entities": [{"text": "Hierarchical Reinforcement Learning", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7881673177083334}, {"text": "Adaptive Text Generation", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.7799503604571024}]}], "abstractContent": [{"text": "We present a novel approach to natural language generation (NLG) that applies hierarchical reinforcement learning to text generation in the wayfinding domain.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.8146633505821228}, {"text": "text generation", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.7165517657995224}]}, {"text": "Our approach aims to optimise the integration of NLG tasks that are inherently different in nature, such as decisions of content selection, text structure , user modelling, referring expression generation (REG), and surface realisation.", "labels": [], "entities": [{"text": "referring expression generation (REG)", "start_pos": 173, "end_pos": 210, "type": "TASK", "confidence": 0.8048068483670553}, {"text": "surface realisation", "start_pos": 216, "end_pos": 235, "type": "TASK", "confidence": 0.7281286418437958}]}, {"text": "It also aims to capture existing interdependen-cies between these areas.", "labels": [], "entities": []}, {"text": "We apply hierarchical reinforcement learning to learn a generation policy that captures these interdepen-dencies, and that can be transferred to other NLG tasks.", "labels": [], "entities": []}, {"text": "Our experimental results-in a simulated environment-show that the learnt wayfinding policy outperforms a baseline policy that takes reasonable actions but without optimization.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic text generation involves a number of subtasks.) list the following as core tasks of a complete NLG system: content selection, discourse planning, sentence planning, sentence aggregation, lexicalisation, referring expression generation and linguistic realisation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7416881918907166}, {"text": "content selection", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.7418392598628998}, {"text": "discourse planning", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7248402535915375}, {"text": "sentence planning", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.7538411021232605}, {"text": "referring expression generation", "start_pos": 213, "end_pos": 244, "type": "TASK", "confidence": 0.7608871261278788}, {"text": "linguistic realisation", "start_pos": 249, "end_pos": 271, "type": "TASK", "confidence": 0.7309978604316711}]}, {"text": "However, decisions made for each of these core tasks are not independent of each other.", "labels": [], "entities": []}, {"text": "The value of one generation task can change the conditions of others, as evidenced by studies in corpus linguistics, and it can therefore be undesirable to treat them all as isolated modules.", "labels": [], "entities": []}, {"text": "In this paper, we focus on interrelated decision making in the areas of content selection, choice of text structure, referring expression and surface form.", "labels": [], "entities": [{"text": "content selection", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.6945288628339767}]}, {"text": "Concretely, we generate route instructions that are tailored specifically towards different user types as well as different environmental features.", "labels": [], "entities": []}, {"text": "In addition, we aim to balance the degree of variation and alignment in texts and produce lexical and syntactic patterns of co-occurrence that resemble those of human texts of the same domain.", "labels": [], "entities": []}, {"text": "Evidence for the importance of this is provided by) who note the way that lexical cohesive ties contribute to text coherence as well as by the theory of interactive alignment.", "labels": [], "entities": [{"text": "interactive alignment", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.6886614263057709}]}, {"text": "According to () we would expect significant traces of lexical and syntactic selfalignment in texts.", "labels": [], "entities": []}, {"text": "Approaches to NLG in the past have been either rule-based) or statistical ().", "labels": [], "entities": []}, {"text": "However, the former relies on a large number of hand-crafted rules, which makes it infeasible for controlling a large number of interrelated variables.", "labels": [], "entities": []}, {"text": "The latter typically requires training on a large corpus of the domain.", "labels": [], "entities": []}, {"text": "While these approaches maybe better suitable for larger domains, for limited domains such as our own, we propose to overcome these drawbacks by applying Reinforcement Learning (RL)-with a hierarchical approach.", "labels": [], "entities": []}, {"text": "Previous work that has used RL for NLG includes who employed it for alignment of referring expressions based on user models.", "labels": [], "entities": []}, {"text": "Also,) used RL for optimising information presentation styles for search results.", "labels": [], "entities": [{"text": "RL", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.696735143661499}]}, {"text": "While both approaches displayed significant effects of adaptation, they focused on a single area of optimisation.", "labels": [], "entities": []}, {"text": "For larger problems, however, such as the one we are aiming to solve, flat RL will not be applicable due to the large state space.", "labels": [], "entities": []}, {"text": "We therefore suggest to divide the problem into a number of subproblems and apply hierarchical reinforcement learning (HRL) ( to solve it.", "labels": [], "entities": []}, {"text": "We describe our problem in more detail in Section 2, our proposed HRL architecture in Sections 3 and 4 and present some results in Section 5.", "labels": [], "entities": []}, {"text": "We show that our learnt policies outperform a baseline that does not adapt to contextual features.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: State and action sets for learning adaptive text generation strategies in the wayfinding domain", "labels": [], "entities": [{"text": "learning adaptive text generation", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.6017537117004395}]}]}