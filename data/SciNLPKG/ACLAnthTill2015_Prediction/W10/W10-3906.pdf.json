{"title": [{"text": "Even Unassociated Features Can Improve Lexical Distributional Similarity", "labels": [], "entities": [{"text": "Lexical Distributional Similarity", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.5795044004917145}]}], "abstractContent": [{"text": "This paper presents anew computation of lexical distributional similarity, which is a corpus-based method for computing similarity of any two words.", "labels": [], "entities": []}, {"text": "Although the conventional method focuses on emphasizing features with which a given word is associated, we propose that even unassociated features of two input words can further improve the performance in total.", "labels": [], "entities": []}, {"text": "We also report in addition that more than 90% of the features has no contribution and thus could be reduced in future.", "labels": [], "entities": []}], "introductionContent": [{"text": "Similarity calculation is one of essential tasks in natural language processing.", "labels": [], "entities": [{"text": "Similarity calculation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9497193396091461}, {"text": "natural language processing", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6435157159964243}]}, {"text": "We look fora semantically similar word to do corpus-driven summarization, machine translation, language generation, recognition of textual entailment and other tasks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7670198380947113}, {"text": "language generation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.747866302728653}, {"text": "recognition of textual entailment", "start_pos": 116, "end_pos": 149, "type": "TASK", "confidence": 0.8927670419216156}]}, {"text": "In task of language modeling and disambiguation we also need to semantically generalize words or cluster words into some groups.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7181310951709747}]}, {"text": "As the amount of text increases more and more in the contemporary world, the importance of similarity calculation also increases concurrently.", "labels": [], "entities": [{"text": "similarity calculation", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.8083512783050537}]}, {"text": "Similarity is computed by roughly two approaches: based on thesaurus and based on corpus.", "labels": [], "entities": []}, {"text": "The former idea uses thesaurus, such as WordNet, that is a knowledge resource of hierarchical word classification.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.957511842250824}, {"text": "word classification", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7314642518758774}]}, {"text": "The latter idea, that is the target of our work, originates from Harris's distributional hypothesis more than four decades ago, stating that semantically similar words tend to appear in similar contexts.", "labels": [], "entities": []}, {"text": "In many cases a context of a word is represented as a feature vector, where each feature is another expression that co-occurs with the given word in the context.", "labels": [], "entities": []}, {"text": "Over along period of its history, in particular in recent years, several works have been done on distributional similarity calculation.", "labels": [], "entities": [{"text": "distributional similarity calculation", "start_pos": 97, "end_pos": 134, "type": "TASK", "confidence": 0.7582012812296549}]}, {"text": "Although the conventional works have attained the fine performance, we attempt to further improve the quality of this measure.", "labels": [], "entities": []}, {"text": "Our motivation of this work simply comes from our observation and analysis of the output by conventional methods; Japanese, our target language here, is written in a mixture of four scripts: Chinese characters, Latin alphabet, and two Japanese-origin characters.", "labels": [], "entities": []}, {"text": "In this writing environment some words which have same meaning and same pronunciation are written in two (or more) different scripts.", "labels": [], "entities": []}, {"text": "This is interesting in terms of similarity calculation since these two words are completely same in semantics so the similarity should be ideally 1.0.", "labels": [], "entities": [{"text": "similarity", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.9750605821609497}]}, {"text": "However, the reality is, as far as we have explored, that the score is far from 1.0 in many same word pairs.", "labels": [], "entities": []}, {"text": "This fact implies that the conventional calculation methods are far enough to the goal and are expected to improve further.", "labels": [], "entities": []}, {"text": "The basic framework for computing distributional similarity is same; for each of two input words a context (i.e., surrounding words) is extracted from a corpus, a vector is made in which an element of the vector is a value or a weight, and two vectors are compared with a formula to compute similarity.", "labels": [], "entities": []}, {"text": "Among these processes we have focused on features, that are elements of the vector, some of which, we think, adversely affect the performance.", "labels": [], "entities": []}, {"text": "That is, traditional approaches such as basically use all of observed words as context, that causes noise in feature vector comparison.", "labels": [], "entities": []}, {"text": "One may agree that the number of the characteristic words to determine the meaning of a word is some, not all, of words around the target word.", "labels": [], "entities": []}, {"text": "Thus our goal is to detect and reduce such noisy features.", "labels": [], "entities": []}, {"text": "Zhitomirsky-Geffet and Dagan (2009) have same motivation with us and introduced a bootstrapping strategy that changes the original features weights.", "labels": [], "entities": []}, {"text": "The general idea here is to promote the weights of features that are common for associated words, since these features are likely to be most characteristic for determining the word's meaning.", "labels": [], "entities": []}, {"text": "In this paper, we propose instead a method to using features that are both unassociated to the two input words, in addition to use of features that are associated to the input.", "labels": [], "entities": []}], "datasetContent": [{"text": "In general it is difficult to answer how similar two given words are.", "labels": [], "entities": []}, {"text": "Human have noway to judge correctness if computed similarity of two words is, for instance, 0.7.", "labels": [], "entities": []}, {"text": "However, given two word pairs, such as (w, w 1 ) and (w, w 2 ), we may answer which of two words, w 1 or w 2 , is more similar tow than the other one.", "labels": [], "entities": []}, {"text": "That is, degree of similarity is defined relatively hence accuracy of similarity measures is evaluated byway of relative comparisons.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.999409556388855}]}, {"text": "In this paper we employ an automatic evaluation method in order to reduce time, human labor, and individual variations.", "labels": [], "entities": []}, {"text": "We first collect four levels of similar word pairs from a thesaurus . Thesaurus is a resource of hierarchical words classification, hence we can collect several levels of similar word pairs according to the depth of common parent nodes that two words have.", "labels": [], "entities": [{"text": "words classification", "start_pos": 110, "end_pos": 130, "type": "TASK", "confidence": 0.7443163990974426}]}, {"text": "Accordingly, we constructed four levels of similarity pairs, Level 0, 1, 2, and 3, where the number increases as the similarity increases.", "labels": [], "entities": []}, {"text": "Each level includes 800 word pairs that are randomly selected.", "labels": [], "entities": []}, {"text": "The following examples are pairs with word Asia in each Level.", "labels": [], "entities": []}, {"text": "Example: Four similarity levels for pair of Asia.", "labels": [], "entities": [{"text": "similarity", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.948814332485199}]}, {"text": "Level 3(high): Asia vs. Europe Level 2: Asia vs. Brazil Level 1: Asia vs. my country Level 0(low): Asia vs. system We then combine word pairs of adjacent similarity Levels, such as Level 0 and 1, that is a test set to see low-level similarity discrimination power.", "labels": [], "entities": []}, {"text": "The performance is calculated in terms of how clearly the measure distinguishes the different levels.", "labels": [], "entities": []}, {"text": "Ina similar fashion, Level 1 and 2, as well as 2 and 3, are combined and tested for middle-level and high-level similarity discrimination, respectively.", "labels": [], "entities": []}, {"text": "The number of pairs in each In this experiment we use Bunrui Goi Hyo also for evaluation.", "labels": [], "entities": [{"text": "Bunrui Goi Hyo", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.6997650762399038}]}, {"text": "Therefore, this experimental setting is a kind of closed test.", "labels": [], "entities": []}, {"text": "However, we see that the advantage to use the same thesaurus in the evaluation seems to be small.", "labels": [], "entities": []}, {"text": "test set is 1,600 as two Levels are combined.", "labels": [], "entities": []}, {"text": "The corpus we use in this experiment is all the articles in The Nihon Keizai Shimbun Database, a Japanese business newspaper corpus covering the years 1990 through 2004.", "labels": [], "entities": [{"text": "The Nihon Keizai Shimbun Database, a Japanese business newspaper corpus covering the years 1990 through 2004", "start_pos": 60, "end_pos": 168, "type": "DATASET", "confidence": 0.8423650895847994}]}, {"text": "As morphological analyzer we use Chasen 2.3.3 with IPA morpheme dictionary.", "labels": [], "entities": [{"text": "Chasen 2.3.3", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.9511785805225372}]}, {"text": "The number of collected triples is 2,584,905, that excludes deleted ones due to onetime appearance and words including some symbols.", "labels": [], "entities": []}, {"text": "In Subsection 2.4 we use Bunrui Goi Hyo, a Japanese thesaurus for synonym collection.", "labels": [], "entities": [{"text": "Bunrui Goi Hyo", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.6795926292737325}, {"text": "synonym collection", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9645119607448578}]}, {"text": "The potential target words are all content words, except words that have less than twenty features.", "labels": [], "entities": []}, {"text": "The number of words after exclusion is 75,530.", "labels": [], "entities": []}, {"text": "Moreover, words that have four or less words in the same category in the thesaurus are regarded as out of target in this paper, due to limitation of Syn(w) in Subsection 2.4.", "labels": [], "entities": []}, {"text": "Also, in order to avoid word sense ambiguity, words that have more than two meanings, i.e., those classified in more than two categories in the thesaurus, also remain to be solved.", "labels": [], "entities": []}, {"text": "shows relation between threshold \u03b1 and the performance of similarity distinction that is drawn in F-measures, for Level 3+2 test set.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.8938655257225037}]}, {"text": "As can be seen, the plots seem to be concave down  and there is a clear peak when \u03b1 is between 2 and 3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance comparison of three meth- ods in each task (in F-measures).  Level  S&K ZG&D proposed  Lvl.3+Lvl.2 0.702 0.791  0.797  Lvl.2+Lvl.1 0.747 0.771  0.773  Lvl.1+Lvl.0 0.838 0.789  0.840", "labels": [], "entities": [{"text": "F-measures", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.942355751991272}]}, {"text": " Table 2: Relation of errors and words with a few  features. In the table, (h) and (l) shows pairs that  are judged higher (lower) by the system. Column  of < 50 (< 20) means number of pairs each of  which has less than 50 (20) features.", "labels": [], "entities": []}]}