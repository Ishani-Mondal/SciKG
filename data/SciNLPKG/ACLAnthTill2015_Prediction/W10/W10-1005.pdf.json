{"title": [{"text": "Search right and thou shalt find ... Using Web Queries for Learner Error Detection", "labels": [], "entities": [{"text": "Learner Error Detection", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.7206076184908549}]}], "abstractContent": [{"text": "We investigate the use of web search queries for detecting errors in non-native writing.", "labels": [], "entities": []}, {"text": "Distinguishing a correct sequence of words from a sequence with a learner error is a baseline task that any error detection and correction system needs to address.", "labels": [], "entities": [{"text": "Distinguishing a correct sequence of words from a sequence", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.8787262505955167}, {"text": "error detection and correction", "start_pos": 108, "end_pos": 138, "type": "TASK", "confidence": 0.7855436056852341}]}, {"text": "Using a large corpus of error-annotated learner data, we investigate whether web search result counts can be used to distinguish correct from incorrect usage.", "labels": [], "entities": []}, {"text": "In this investigation, we compare a variety of query formulation strategies and a number of web resources, including two major search engine APIs and a large web-based n-gram corpus.", "labels": [], "entities": [{"text": "query formulation", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7451876401901245}]}], "introductionContent": [{"text": "Data-driven approaches to the detection and correction of non-native errors in English have been researched actively in the past several years.", "labels": [], "entities": [{"text": "detection and correction of non-native errors in English", "start_pos": 30, "end_pos": 86, "type": "TASK", "confidence": 0.8324944414198399}]}, {"text": "Such errors are particularly amenable to data-driven methods because many prominent learner writing errors involve a relatively small class of phenomena that can be targeted with specific models, in particular article and preposition errors.", "labels": [], "entities": []}, {"text": "Preposition and determiner errors (most of which are article errors) are the second and third most frequent errors in the Cambridge Learner Corpus (after the more intractable problem of content word choice).", "labels": [], "entities": [{"text": "Preposition and determiner errors", "start_pos": 0, "end_pos": 33, "type": "METRIC", "confidence": 0.710274763405323}, {"text": "Cambridge Learner Corpus", "start_pos": 122, "end_pos": 146, "type": "DATASET", "confidence": 0.9640261729558309}]}, {"text": "By targeting the ten most frequent prepositions involved in learner errors, more than 80% of preposition errors in the corpus are covered.", "labels": [], "entities": []}, {"text": "Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (.", "labels": [], "entities": []}, {"text": "Language models are another source of evidence that can be used in error detection.", "labels": [], "entities": [{"text": "error detection", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7022128999233246}]}, {"text": "Using language models for this purpose is not anew approach, it goes back to at least. and use a combination of classification and language modeling.", "labels": [], "entities": []}, {"text": "Once language modeling comes into play, the quantity of the training data comes to the forefront.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7376567721366882}]}, {"text": "It has been well-established that statistical models improve as the size of the training data increases).", "labels": [], "entities": []}, {"text": "This is particularly true for language models: other statistical models such as a classifier, for example, can be targeted towards a specific decision/classification, reducing the appetite for data somewhat, while language models provide probabilities for any sequence of words -a task that requires immense training data resources if the language model is to consider increasingly sparse longer n-grams.", "labels": [], "entities": []}, {"text": "Language models trained on data sources like the Gigaword corpus have become commonplace, but of course there is one corpus that dwarfs any other resource in size: the World Wide Web.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.9210691452026367}]}, {"text": "This has drawn the interest of many researchers in natural language processing over the past decade.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6456835567951202}]}, {"text": "To mention just a few examples, combine trigram counts from the web with an existing language model where the estimates of the existing model are unreliable because of data sparseness.", "labels": [], "entities": []}, {"text": "advocate the use of the web as a corpus to retrieve backoff probabilities for unseen bigrams.", "labels": [], "entities": []}, {"text": "extend this method to a range of additional natural language processing tasks, but also caution that web counts have limitations and add noise.", "labels": [], "entities": []}, {"text": "points out the shortcomings of accessing the web as a corpus through search queries: (a) there is no lemmatization or part-of-speech tagging in search indices, so a linguistically meaningful query can only be approximated, (b) search syntax, as implemented by search engine providers, is limited, (c) there is often a limit on the number of automatic queries that are allowed by search engines, (c) hit count estimates are estimates of retrieved pages, not of retrieved words.", "labels": [], "entities": []}, {"text": "We would like to add to that list that hit count estimates on the web are just that --estimates.", "labels": [], "entities": []}, {"text": "They are computed on the flyby proprietary algorithms, and apparently the algorithms also access different slices of the web index, which causes a fluctuation overtime, as point out.", "labels": [], "entities": []}, {"text": "In 2006, Google made its web-based 5gram language model available through the Linguistic Data Consortium, which opens the possibility of using real n-gram statistics derived from the web directly, instead of using web search as a proxy.", "labels": [], "entities": [{"text": "Linguistic Data Consortium", "start_pos": 78, "end_pos": 104, "type": "DATASET", "confidence": 0.754706452290217}]}, {"text": "In this paper we explore the use of the web as a corpus fora very specific task: distinguishing between a learner error and its correction.", "labels": [], "entities": []}, {"text": "This is obviously not the same as the more ambitious question of whether a system can be built to detect and correct errors on the basis of web counts alone, and this is a distinction worth clarifying.", "labels": [], "entities": []}, {"text": "Any system that successfully detects and corrects an error will need to accomplish three tasks 1 : (1) find apart of the user input that contains an error (error detection).", "labels": [], "entities": [{"text": "error detection", "start_pos": 156, "end_pos": 171, "type": "TASK", "confidence": 0.686284750699997}]}, {"text": "(2) find one or multiple alternative string(s) for the alleged error (candidate generation) and (3) score the alternatives and the original to determine which alternative (if any) is a likely correction (error correction).", "labels": [], "entities": []}, {"text": "Here, we are only concerned with the third task, specifically the comparison between the incorrect and the correct choice.", "labels": [], "entities": []}, {"text": "This is an easily measured task, and is also a minimum requirement for any language model or language model approximation: if the model cannot distinguish an error from a well-formed string, it will not be useful.", "labels": [], "entities": [{"text": "language model approximation", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.6241896649201711}]}, {"text": "We focus on two prominent learner errors in this study: preposition inclusion and choice and article inclusion and choice.", "labels": [], "entities": [{"text": "article inclusion and choice", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.6877958327531815}]}, {"text": "These errors are among the most frequent learner errors (they comprise nearly one third of all errors in the learner corpus used in this study).", "labels": [], "entities": []}, {"text": "In this study, we compare three web data sources: The public Bing API, Google API, and the Google 5-gram language model.", "labels": [], "entities": []}, {"text": "We also pay close attention to strategies of query formulation.", "labels": [], "entities": [{"text": "query formulation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.8829214870929718}]}, {"text": "The questions we address are summarized as follows: Can web data be used to distinguish learner errors from correct phrases?", "labels": [], "entities": []}, {"text": "What is the better resource for web-data: the Bing API, the Google API, or the Google 5-gram data?", "labels": [], "entities": [{"text": "Google 5-gram data", "start_pos": 79, "end_pos": 97, "type": "DATASET", "confidence": 0.7964240312576294}]}, {"text": "What is the best query formulation strategy when using web search results for this task?", "labels": [], "entities": [{"text": "query formulation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.8324841558933258}]}, {"text": "How much context should be included in the query?", "labels": [], "entities": []}, {"text": "use web search hit counts for preposition error detection and correction in French.", "labels": [], "entities": [{"text": "preposition error detection", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6372761428356171}]}, {"text": "They use a set of confusable prepositions to create a candidate set of alternative prepositional choices and generate queries for each of the candidates and the original.", "labels": [], "entities": []}, {"text": "The queries are produced using linguistic analysis to identify both a governing and a governed element as a minimum meaningful context.", "labels": [], "entities": []}, {"text": "On a small test set of 133 sentences, they report accuracy of 69.9% using the Yahoo!", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9995710253715515}, {"text": "Yahoo!", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.9022200703620911}]}, {"text": "target article use and collocation errors with a similar approach.", "labels": [], "entities": []}, {"text": "Their system first analyzes the input sentence using part-of-speech tagging and a chunk parser.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.7197721898555756}]}, {"text": "Based on this analysis, potential error locations for determiners and verbnoun collocation errors are identified.", "labels": [], "entities": []}, {"text": "Query generation is performed at three levels of granularity: the sentence (or clause) level, chunk level and word level.", "labels": [], "entities": [{"text": "Query generation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8972756564617157}]}, {"text": "Queries, in this approach, are not exact string searches but rather a set of strings combined with the chunk containing the potential error through a boolean operator.", "labels": [], "entities": []}, {"text": "An example fora chunk level query for the sentence \"I am learning economics at university\" would be \" AND [at university] AND [learning]\".", "labels": [], "entities": [{"text": "AND", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9589418768882751}]}, {"text": "For article errors the hit count estimates (normalized for query length) are used directly.", "labels": [], "entities": []}, {"text": "If the ratio of the normalized hit count estimate for the alternative article choice to the normalized hit count estimate of the original choice exceeds a manually determined threshold, the alternative is suggested as a correction.", "labels": [], "entities": []}, {"text": "For verb-noun collocations, the situation is more complex since the system does not automatically generate possible alternative choices for noun/verb collocations.", "labels": [], "entities": []}, {"text": "Instead, the snippets (document summaries) that are returned by the initial web search are analyzed and potential alternative collocation candidates are identified.", "labels": [], "entities": []}, {"text": "They then submit a second round of queries to determine whether the suggestions are more frequent than the original collocation.", "labels": [], "entities": []}, {"text": "Results on a 400+ sentence corpus of learner writing show 62% precision and 41% recall for determiners, and 30.7% recall and 37.3% precision for verb-noun collocation errors.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9995846152305603}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9988027811050415}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9988522529602051}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9979921579360962}]}], "datasetContent": [{"text": "Raw accuracy is the ratio of correct predictions to all query pairs: We also calculate accuracy for the subset of query pairs whereat least one of the queries resulted in a successful hit, i.e. a non-zero result.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9977839589118958}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9992485642433167}]}, {"text": "We call this metric Non-Zero-Result-Accurracy (NZRA), it is the ratio of correct predictions to incorrect predictions, ignoring noresults: Finally, retrieval ratio is the ratio of queries that returned non-zero results:", "labels": [], "entities": [{"text": "Non-Zero-Result-Accurracy (NZRA)", "start_pos": 20, "end_pos": 52, "type": "METRIC", "confidence": 0.563142716884613}]}], "tableCaptions": [{"text": " Table 6: Article substitutions (717 query pairs).", "labels": [], "entities": [{"text": "Article substitutions", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7794161140918732}]}]}