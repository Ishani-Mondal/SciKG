{"title": [{"text": "Incremental Decoding for Phrase-based Statistical Machine Translation", "labels": [], "entities": [{"text": "Incremental Decoding", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8961619138717651}, {"text": "Phrase-based Statistical Machine Translation", "start_pos": 25, "end_pos": 69, "type": "TASK", "confidence": 0.8510704189538956}]}], "abstractContent": [{"text": "In this paper we focus on the incremental decoding fora statistical phrase-based machine translation system.", "labels": [], "entities": [{"text": "statistical phrase-based machine translation", "start_pos": 56, "end_pos": 100, "type": "TASK", "confidence": 0.5514567047357559}]}, {"text": "In incremental decoding, translations are generated incre-mentally for every word typed by a user, instead of waiting for the entire sentence as input.", "labels": [], "entities": []}, {"text": "We introduce a novel modification to the beam-search decoding algorithm for phrase-based MT to address this issue, aimed at efficient computation of future costs and avoiding search errors.", "labels": [], "entities": [{"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.7047343850135803}]}, {"text": "Our objective is to do a faster translation during incremental decoding without significant reduction in the translation quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical Machine Translation has matured significantly in the past decade and half, resulting in the proliferation of several web-based and commercial translation services.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7840785384178162}]}, {"text": "Most of these services work on sentence or document level, where a user enters a sentence or chooses a document for translation, which are then translated by the servers.", "labels": [], "entities": []}, {"text": "Translation in such typical scenarios is still offline in the sense that the user input and translation happen sequentially without any interaction between the two phases.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9516832232475281}]}, {"text": "In this paper we study decoding for SMT with the constraint that translations are to be generated incrementally for every word typed in by the user.", "labels": [], "entities": [{"text": "SMT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.990828275680542}]}, {"text": "Such a translation service can be used for language learning, where the user is fluent in the target language and experiments with many different source language sentences interactively, or in real-time translation environments such as speechspeech translation or translation during interactive chats.", "labels": [], "entities": [{"text": "speechspeech translation", "start_pos": 236, "end_pos": 260, "type": "TASK", "confidence": 0.7449374198913574}]}, {"text": "We use a phrase-based decoder similar to Moses ( and propose novel modifications in the decoding algorithm to tackle incremental decoding.", "labels": [], "entities": []}, {"text": "Our system maintains a partial decoder state at every stage and uses it while decoding for each newly added word.", "labels": [], "entities": []}, {"text": "As the decoder has access only to the partial sentence at every stage, the future costs change with every additional word and this has to betaken into account while continuing from an existing partial decoder state.", "labels": [], "entities": []}, {"text": "Another major issue is that as incremental decoding is provided new input one word at at time, some of the entries that were pruned out at an earlier decoder state might later turnout to better candidates resulting in search errors compared to decoding the entire sentence at once.", "labels": [], "entities": []}, {"text": "It is to be noted that, the search error problem is related to the inability to compute full future cost in incremental decoding.", "labels": [], "entities": []}, {"text": "Our proposed modifications address these twin challenges and allow for efficient incremental decoding.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation was performed using our own implementation of the beam-search decoding algorithms.", "labels": [], "entities": []}, {"text": "The architecture of our system is similar to Moses, which we also use for training and for minimum error rate training (MERT) of the loglinear model for translation.", "labels": [], "entities": [{"text": "minimum error rate training (MERT)", "start_pos": 91, "end_pos": 125, "type": "METRIC", "confidence": 0.8496860265731812}, {"text": "translation", "start_pos": 153, "end_pos": 164, "type": "TASK", "confidence": 0.972599446773529}]}, {"text": "Our features include 7 standard phrasebased features: 4 translation model features, i.e. p(f |e), p(e|f ), p lex (f |e) and p lex (e|f ), where e and fare target and source phrases respectively; features for phrase penalty, word penalty and language model, and we do not include the reordering feature.", "labels": [], "entities": [{"text": "phrase penalty", "start_pos": 208, "end_pos": 222, "type": "TASK", "confidence": 0.7714717090129852}, {"text": "word penalty", "start_pos": 224, "end_pos": 236, "type": "TASK", "confidence": 0.6827675998210907}]}, {"text": "We used Giza++ and Moses respectively for aligning the sentences and training the system.", "labels": [], "entities": []}, {"text": "The decoder was written in Java and includes cube pruning and lazier cube pruning functionalities as part of the decoder.", "labels": [], "entities": []}, {"text": "Our decoder supports both regular beam search (similar to Moses) and incremental decoding.", "labels": [], "entities": []}, {"text": "In our experiments we experimented various approaches for storing partial decoder states including memcache and transactional persistence using JDBM but found that the serialization and deserialization of decoder objects directly into and from the memory to work better in terms of speed and memory requirements.", "labels": [], "entities": []}, {"text": "The partial object is retrieved and deserialized from the memory when required by the incremental decoder.", "labels": [], "entities": []}, {"text": "We evaluated the incremental decoder for translations between French and English (in both directions).", "labels": [], "entities": [{"text": "translations between French and English", "start_pos": 41, "end_pos": 80, "type": "TASK", "confidence": 0.8747516989707946}]}, {"text": "We used the Workshop on Machine Translation shared task (WMT07) dataset for training, optimizing and testing.", "labels": [], "entities": [{"text": "Machine Translation shared task", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.7981039583683014}, {"text": "WMT07) dataset", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.8193443616231283}]}, {"text": "The system was trained using Moses and the feature weights were optimized using MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.777771532535553}]}, {"text": "To benchmark our Java decoder, we compare it with Moses by running it in regular beam search mode.", "labels": [], "entities": []}, {"text": "The Moses systems were also optimized separately on the WMT07 devsets.", "labels": [], "entities": [{"text": "WMT07 devsets", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9822014272212982}]}, {"text": "Apart from comparing our decoder with Moses in regular beam search, we also compared the incremental decoding with regular regular beam using our decoder.", "labels": [], "entities": []}, {"text": "To make it comparable with incremental decoding, we used the regular beam search to re-decode the sentence fragments for every additional word in the input sentence.", "labels": [], "entities": []}, {"text": "We measured the following parameters in our empirical analysis: translation quality (as measured by BLEU () and TER ()), search errors and translation speed.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9988051652908325}, {"text": "TER", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9966289401054382}]}, {"text": "Finally, we also measured the effect of different racecourse limits on BLEU and decoding speed for incremental decoding.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9991686344146729}]}], "tableCaptions": [{"text": " Table 1: Regular beam search: Moses v.s. Our decoder", "labels": [], "entities": [{"text": "Regular beam search", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6947389642397562}]}, {"text": " Table 2: BLEU and TER: Re-decoding v.s. Incremental Decoding (ID)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992157220840454}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.997694194316864}, {"text": "Incremental Decoding (ID)", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.5201480925083161}]}, {"text": " Table 3: Search Errors in Incremental Decoding", "labels": [], "entities": []}, {"text": " Table 4: Speed: Re-decoding v.s. Incremental Decoding (ID)", "labels": [], "entities": [{"text": "Speed", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9642854928970337}]}]}