{"title": [{"text": "The First Question Generation Shared Task Evaluation Challenge", "labels": [], "entities": [{"text": "First Question Generation Shared Task Evaluation", "start_pos": 4, "end_pos": 52, "type": "TASK", "confidence": 0.7095942497253418}]}], "abstractContent": [{"text": "The paper briefly describes the First Shared Task Evaluation Challenge on Question Generation that took place in Spring 2010.", "labels": [], "entities": [{"text": "First Shared Task Evaluation Challenge on Question Generation", "start_pos": 32, "end_pos": 93, "type": "TASK", "confidence": 0.5469518266618252}]}, {"text": "The campaign included two tasks: Task A-Question Generation from Paragraphs and Task B-Question Generation from Sentences.", "labels": [], "entities": [{"text": "Task A-Question Generation from Paragraphs", "start_pos": 33, "end_pos": 75, "type": "TASK", "confidence": 0.7073207378387452}]}, {"text": "An overview of each of the tasks is provided.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question Generation is an essential component of learning environments, help systems, information seeking systems, multi-modal conversations between virtual agents, and a myriad of other applications.", "labels": [], "entities": [{"text": "Question Generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7905805706977844}]}, {"text": "Question Generation has been recently defined as the task) of automatically generating questions from some form of input.", "labels": [], "entities": [{"text": "Question Generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6897940635681152}]}, {"text": "The input could vary from information in a database to a deep semantic representation to raw text.", "labels": [], "entities": []}, {"text": "The first Shared Task Evaluation Challenge on Question Generation (QG-STEC) follows along tradition of STECs in Natural Language Processing (see the annual tasks run by the Conference on Natural Language Learning -CoNLL).", "labels": [], "entities": [{"text": "Shared Task Evaluation Challenge on Question Generation (QG-STEC)", "start_pos": 10, "end_pos": 75, "type": "TASK", "confidence": 0.7042775481939316}]}, {"text": "In particular, the idea of a QG-STEC was inspired by the recent activity in the Natural Language Generation (NLG) community to offer shared task evaluation campaigns as a potential avenue to provide a focus for research in NLG and to increase the visibility of NLG in the wider Natural Language Processing (NLP) community.", "labels": [], "entities": []}, {"text": "It should be noted that the QG is currently perceived as a discourse processing task rather than a traditional NLG task.", "labels": [], "entities": [{"text": "discourse processing task", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.7779912650585175}]}, {"text": "Two core aspects of a question are the goal of the question and its importance.", "labels": [], "entities": []}, {"text": "It is difficult to determine whether a particular question is good without knowing the context in which it is posed; ideally one would like to have information about what counts as important and what the goals are in the current context.", "labels": [], "entities": []}, {"text": "This suggests that a STEC on QG should be tied to a particular application, e.g. tutoring systems.", "labels": [], "entities": [{"text": "STEC", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.8597102165222168}]}, {"text": "However, an application-specific STEC would limit the pool of potential participants to those interested in the target application.", "labels": [], "entities": [{"text": "STEC", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.8425989151000977}]}, {"text": "Therefore, the challenge was to find a framework in which the goal and importance are intrinsic to the source of questions and less tied to a particular context/application.", "labels": [], "entities": []}, {"text": "One possibility was to have the general goal of asking questions about salient items in a source of information, e.g. core ideas in a paragraph of text.", "labels": [], "entities": []}, {"text": "Our tasks have been defined with this concept in mind.", "labels": [], "entities": []}, {"text": "Adopting the basic principle of application-independence has the advantage of escaping the problem of a limited pool of participants (to those interested in a particular application had that application been chosen as the target fora QG STEC).", "labels": [], "entities": [{"text": "QG STEC", "start_pos": 234, "end_pos": 241, "type": "DATASET", "confidence": 0.7669249176979065}]}, {"text": "Another decision aimed at attracting as many participants as possible and promoting a more fair comparison environment was the input for the QG tasks.", "labels": [], "entities": [{"text": "QG tasks", "start_pos": 141, "end_pos": 149, "type": "TASK", "confidence": 0.5705535709857941}]}, {"text": "Adopting a specific representation for the input would have favored some participants already familiar with such a representation.", "labels": [], "entities": []}, {"text": "Therefore, we have adopted as a second guiding principle for the first QG-STEC tasks: no representational commitment.", "labels": [], "entities": []}, {"text": "That is, we wanted to have as generic an input as possible.", "labels": [], "entities": []}, {"text": "The input to both task A and B in the first QG STEC is raw text.", "labels": [], "entities": [{"text": "QG STEC", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.6985167860984802}]}, {"text": "The First Workshop on Question Generation (www.questiongeneration.org) has identified four categories of QG tasks: Text-to-Question, Tutorial Dialogue, Assessment, and Query-to-Question.", "labels": [], "entities": [{"text": "Question Generation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.6793513000011444}]}, {"text": "The two tasks in the first QG STEC are part of the Text-to-Question category or part of the Text-to-text Natural Language Generation task categories.", "labels": [], "entities": [{"text": "QG STEC", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.8387612700462341}, {"text": "Text-to-text Natural Language Generation task", "start_pos": 92, "end_pos": 137, "type": "TASK", "confidence": 0.6786863744258881}]}, {"text": "It is important to say that the two tasks offered in the first QG STEC were selected among 5 candidate tasks by the members of the QG community.", "labels": [], "entities": [{"text": "QG STEC", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.8183265328407288}]}, {"text": "A preference poll was conducted and the most preferred tasks, Question Generation from Paragraphs (Task A) and Question Generation from Sentences (Task B), were chosen to be offered in the first QG STEC.", "labels": [], "entities": [{"text": "Question Generation from Paragraphs (Task A)", "start_pos": 62, "end_pos": 106, "type": "TASK", "confidence": 0.8288085609674454}, {"text": "Question Generation from Sentences", "start_pos": 111, "end_pos": 145, "type": "TASK", "confidence": 0.8237603008747101}, {"text": "QG STEC", "start_pos": 195, "end_pos": 202, "type": "DATASET", "confidence": 0.8675698339939117}]}, {"text": "The other three candidate tasks were: Ranking Automatically Generated Questions (Michael Heilman and Noah Smith), Concept Identification and Ordering (Rodney Nielsen and Lee Becker), and Question Type Identification (Vasile Rus and Arthur Graesser).", "labels": [], "entities": [{"text": "Concept Identification and Ordering", "start_pos": 114, "end_pos": 149, "type": "TASK", "confidence": 0.8513732403516769}, {"text": "Question Type Identification", "start_pos": 187, "end_pos": 215, "type": "TASK", "confidence": 0.792642335096995}]}, {"text": "There is overlap between Task A and B.", "labels": [], "entities": []}, {"text": "This was intentional with the aim of encouraging people preferring one task to participate in the other.", "labels": [], "entities": []}, {"text": "The overlap consists of the specific questions in Task A which are more or less similar with the type of questions targeted by Task B. Overall, we had 1 submission for Task A and 4 submissions for Task B. We also had an additional submission on development data for Task A.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation criteria fulfilled two roles.", "labels": [], "entities": []}, {"text": "Firstly, they were provided to the participants as a specification of the kind of questions that their systems should aim to generate.", "labels": [], "entities": []}, {"text": "Secondly, they also played the role of guidelines for the judges of system outputs in the evaluation exercise.", "labels": [], "entities": []}, {"text": "For this task, five criteria were identified: relevance, question type, syntactic correctness and fluency, ambiguity, and variety.", "labels": [], "entities": [{"text": "variety", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9597678184509277}]}, {"text": "All criteria are associated with a scale from 1 to N (where N is 2, 3 or 4), with 1 being the best score and N the worst score.", "labels": [], "entities": []}, {"text": "The procedure for applying these criteria is as follows: \ud97b\udf59 Each of the criteria is applied independently of the other criteria to each of the generated questions (except for the stipulation provided below).", "labels": [], "entities": []}, {"text": "We need some specific stipulations for cases where no question is returned in response to an input.", "labels": [], "entities": []}, {"text": "For each target question type, two questions are expected.", "labels": [], "entities": []}, {"text": "Consequently, we have the following two possibilities regarding missing questions: \ud97b\udf59 No question is returned fora particular target question type: for each of the missing questions, the worst score is recorded for all criteria.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 Only one question is returned: For the missing question, the worst score is assigned on all criteria.", "labels": [], "entities": []}, {"text": "The question that is present is scored following the criteria, with the exception of the VARIETY criterion for which the lowest possible score is assigned.", "labels": [], "entities": [{"text": "VARIETY", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9969484210014343}]}, {"text": "We compute the overall score on a specific criterion.", "labels": [], "entities": []}, {"text": "We can also compute a score which aggregates the overall scores for the criteria.", "labels": [], "entities": []}], "tableCaptions": []}