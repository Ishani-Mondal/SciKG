{"title": [], "abstractContent": [{"text": "The translation model of statistical machine translation systems is trained on parallel data coming from various sources and domains.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9742641448974609}, {"text": "statistical machine translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6314366161823273}]}, {"text": "These corpora are usually con-catenated, word alignments are calculated and phrases are extracted.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.6513018012046814}]}, {"text": "This means that the corpora are not weighted according to their importance to the domain of the translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.9064640998840332}]}, {"text": "This is in contrast to the training of the language model for which well known techniques are used to weight the various sources of texts.", "labels": [], "entities": []}, {"text": "On a smaller granularity, the automatic calculated word alignments differ in quality.", "labels": [], "entities": []}, {"text": "This is usually not considered when extracting phrases either.", "labels": [], "entities": []}, {"text": "In this paper we propose a method to automatically weight the different corpora and alignments.", "labels": [], "entities": []}, {"text": "This is achieved with a resam-pling technique.", "labels": [], "entities": []}, {"text": "We report experimental results fora small (IWSLT) and large (NIST) Arabic/English translation tasks.", "labels": [], "entities": [{"text": "NIST) Arabic/English translation tasks", "start_pos": 61, "end_pos": 99, "type": "TASK", "confidence": 0.7153172833578927}]}, {"text": "In both cases, significant improvements in the BLEU score were observed.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9815304577350616}]}], "introductionContent": [{"text": "Two types of resources are needed to train statistical machine translation (SMT) systems: parallel corpora to train the translation model and monolingual texts in the target language to build the language model.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 43, "end_pos": 80, "type": "TASK", "confidence": 0.7814047286907831}]}, {"text": "The performance of both models depends of course on the quality and quantity of the available resources.", "labels": [], "entities": []}, {"text": "Today, most SMT systems are generic, i.e. the same system is used to translate texts of all kinds.", "labels": [], "entities": [{"text": "SMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9918374419212341}]}, {"text": "Therefore, it is the domain of the training resources that influences the translations that are selected among several choices.", "labels": [], "entities": []}, {"text": "While monolingual texts are in general easily available in many domains, the freely available parallel texts mainly come from international organisations, like the European Union or the United Nations.", "labels": [], "entities": []}, {"text": "These texts, written in particular jargon, are usually much larger than in-domain bitexts.", "labels": [], "entities": []}, {"text": "As an example we can cite the development of an NIST Arabic/English phrase-based translation system.", "labels": [], "entities": [{"text": "NIST Arabic/English phrase-based translation", "start_pos": 48, "end_pos": 92, "type": "TASK", "confidence": 0.7449171642462412}]}, {"text": "The current NIST test sets are composed of a news wire part and a second part of web-style texts.", "labels": [], "entities": [{"text": "NIST test sets", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.9451783696810404}]}, {"text": "For both domains, there is only a small number of in-domain bitexts available, in comparison to almost 200 millions words of out-of-domain UN texts.", "labels": [], "entities": []}, {"text": "The later corpus is therefore likely to dominate the estimation of the probability distributions of the translation model.", "labels": [], "entities": []}, {"text": "It is common practice to use a mixture language model with coefficients that are optimized on the development data, i.e. by these means on the domain of the translation task.", "labels": [], "entities": []}, {"text": "Domain adaptation seems to be more tricky for the translation model and it seems that very little research has been done that seeks to apply similar ideas to the translation model.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7780780494213104}, {"text": "translation", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9801654815673828}]}, {"text": "To the best of our knowledge, there is no commonly accepted method to weight the bitexts coming from different sources so that the translation model is best optimized to the domain of the task.", "labels": [], "entities": []}, {"text": "Mixture models are possible when only two different bitexts are available, but are rarely used for more corpora (see discussion in the next section).", "labels": [], "entities": []}, {"text": "In this work we propose anew method to adapt the translation model of an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9897572994232178}]}, {"text": "We only perform experiments with phrase-based systems, but the method is generic and could be easily applied to an hierarchical or syntax-based system.", "labels": [], "entities": []}, {"text": "We first associate a weighting coefficient to each bitext.", "labels": [], "entities": []}, {"text": "The main idea is to use resampling to produce anew collection of weighted alignment files, followed by the standard procedure to extract the phrases.", "labels": [], "entities": []}, {"text": "Ina second step, we also consider the alignment score of each parallel sentence pair, emphasizing by these means good alignments and down-weighting less reliable ones.", "labels": [], "entities": [{"text": "alignment score", "start_pos": 38, "end_pos": 53, "type": "METRIC", "confidence": 0.9252532422542572}]}, {"text": "All the parameters of our procedure are automatically tuned by optimizing the BLEU score on the development data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9783811867237091}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section describes related work on weighting the corpora and model adaptation.", "labels": [], "entities": [{"text": "weighting", "start_pos": 43, "end_pos": 52, "type": "TASK", "confidence": 0.9619466662406921}, {"text": "model adaptation", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.7092101722955704}]}, {"text": "Section 3 describes the architecture allowing to resample and to weight the bitexts.", "labels": [], "entities": []}, {"text": "Experimental results are presented in section 4 and the paper concludes with a discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "The baseline system is a standard phrase-based SMT system based on the Moses SMT toolkit ).", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8324906826019287}, {"text": "Moses SMT toolkit", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.6860039830207825}]}, {"text": "In our system we used fourteen features functions.", "labels": [], "entities": []}, {"text": "These features functions include phrase and lexical translation probabilities in both directions, seven features for lexicalized distortion model, a word and phrase penalty, and a target language model.", "labels": [], "entities": []}, {"text": "The MERT tool is used to tune the coefficients of these feature functions.", "labels": [], "entities": [{"text": "MERT", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8988293409347534}]}, {"text": "We considered Arabic to English translation.", "labels": [], "entities": [{"text": "Arabic to English translation", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.6893086582422256}]}, {"text": "Tokenization of the Arabic source texts is done by a tool provided by SYSTRAN which also performs a morphological decomposition.", "labels": [], "entities": [{"text": "Tokenization", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9677202105522156}]}, {"text": "We considered two well known official evaluation tasks to evaluate our approach, namely NIST and IWSLT.", "labels": [], "entities": [{"text": "NIST", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8596523404121399}, {"text": "IWSLT", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.5261034369468689}]}, {"text": "For IWSLT, we used the BTEC bitexts (194M words), Dev1, Dev2, Dev3 (60M words each) as training data, Dev6 as development set and Dev7 as test set.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.79733806848526}, {"text": "BTEC bitexts", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.814413458108902}]}, {"text": "From previous experiments, we have evidence that the various development corpora are not equally important and weighting them correctly should improve the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.9950920343399048}]}, {"text": "We analyze the translation quality as measured by the BLEU score for the three methods: equal weights, LM weights and Condor weights and considering onetime resampling.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9467781782150269}, {"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.998221218585968}]}, {"text": "Further experiments were performed using the optimized number of resampling with and without weighting the alignments.", "labels": [], "entities": []}, {"text": "We have realized that it is beneficial to always include the original alignments.", "labels": [], "entities": []}, {"text": "Even if we resample many times there is a chance that some alignments might never be selected but we do not want to loose any information.", "labels": [], "entities": []}, {"text": "By keeping original alignments, all alignments are given a chance to be se-lected at least once.", "labels": [], "entities": []}, {"text": "All these results are summarized in tables 1, 2 and 3.", "labels": [], "entities": []}, {"text": "One time resampling along with equal weights gave worse results than the baseline system while improvements in the BLEU score were observed with LM and Condor weights for the IWSLT task, as shown in table 1.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.98037388920784}, {"text": "IWSLT task", "start_pos": 175, "end_pos": 185, "type": "TASK", "confidence": 0.4360397607088089}]}, {"text": "Resampling many times always gave more stable results, as already shown in and as theoretically expected.", "labels": [], "entities": []}, {"text": "For this task, we resampled 15 times.", "labels": [], "entities": []}, {"text": "The improvements in the BLEU score are shown in table 2.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9543046057224274}]}, {"text": "Furthermore, using the alignment scores resulted in additional improvements in the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9705016314983368}]}, {"text": "For the IWSLT task, we achieved and overall improvement of 1.5 BLEU points on the development set and 1.2 BLEU points on the test set as shown in To validate our approach we further experimented with the NIST evaluation task.", "labels": [], "entities": [{"text": "IWSLT task", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.47795388102531433}, {"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9982689619064331}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9980825185775757}, {"text": "NIST evaluation task", "start_pos": 204, "end_pos": 224, "type": "TASK", "confidence": 0.5650591452916464}]}, {"text": "Most of the training data used in our experiments for the NIST task is made available through the LDC.", "labels": [], "entities": []}, {"text": "The bitexts consist of texts from the GALE project 1 (1.6M words), various news wire translations 2 (8.0M words) on development data from previous years (1.6M words), LDC treebank data (0.4M words) and the ISI extracted bitexts (43.7M words).", "labels": [], "entities": [{"text": "GALE project 1", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.854788581530253}, {"text": "LDC treebank data", "start_pos": 167, "end_pos": 184, "type": "DATASET", "confidence": 0.8327613274256388}, {"text": "ISI extracted bitexts", "start_pos": 206, "end_pos": 227, "type": "DATASET", "confidence": 0.8354207475980123}]}, {"text": "The official NIST06 evaluation data was used as development set and the NIST08 evaluation data was used as test set.", "labels": [], "entities": [{"text": "NIST06 evaluation data", "start_pos": 13, "end_pos": 35, "type": "DATASET", "confidence": 0.928286612033844}, {"text": "NIST08 evaluation data", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.9437565207481384}]}, {"text": "The same procedure was adapted for the NIST task as for the IWSLT task.", "labels": [], "entities": [{"text": "NIST", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.7174127697944641}, {"text": "IWSLT task", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.5970247685909271}]}, {"text": "Results are shown in table 1 by using different weights and onetime resampling.", "labels": [], "entities": []}, {"text": "Further improvements in the results are shown in table 2 with the optimum number of resampling which is 10 for this task.", "labels": [], "entities": []}, {"text": "Finally, results by weighting alignments along with weighting corpora are shown in table 3.", "labels": [], "entities": []}, {"text": "Our final system achieved an improvement of 0.79 BLEU points on the development set and 0.33 BLEU points on the test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9991637468338013}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9987096786499023}]}, {"text": "TER scores are also shown on test set of our final system in table 3.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9941062927246094}]}, {"text": "Note that these results are state-of-the-art when compared to the official results of the 2008 NIST evaluation 3 . The weights of the different corpora are shown in table 4 for the IWSLT and NIST task.", "labels": [], "entities": [{"text": "NIST evaluation", "start_pos": 95, "end_pos": 110, "type": "DATASET", "confidence": 0.7629199326038361}, {"text": "IWSLT", "start_pos": 181, "end_pos": 186, "type": "DATASET", "confidence": 0.7120317816734314}, {"text": "NIST task", "start_pos": 191, "end_pos": 200, "type": "DATASET", "confidence": 0.8991084396839142}]}, {"text": "In both cases, the weights optimized by CONDOR are substantially different form those obtained when creating an interpolated LM on the source side of the bitexts.", "labels": [], "entities": []}, {"text": "In any case, the weights are clearly nonuniform, showing that our algorithm has focused on in-domain data.", "labels": [], "entities": []}, {"text": "This can be nicely seen for the NIST task.", "labels": [], "entities": [{"text": "NIST task", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.551325112581253}]}, {"text": "The Gale texts were explictely created to contain in-domain news wire and WEB texts and actually get a high weight despite their small size, in comparison to the more general news wire collection from LDC.", "labels": [], "entities": [{"text": "Gale texts", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9239881932735443}, {"text": "LDC", "start_pos": 201, "end_pos": 204, "type": "DATASET", "confidence": 0.6911119818687439}]}], "tableCaptions": [{"text": " Table 1: BLEU scores when weighting corpora (one time resampling)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992918968200684}]}, {"text": " Table 2: BLEU scores when weighting corpora (optimum number of resampling)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993950128555298}]}, {"text": " Table 3: BLEU and TER scores when weighting corpora and alignments (optimum number of resam- pling)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992730021476746}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9986990690231323}]}, {"text": " Table 4: Weights of the different bitexts.", "labels": [], "entities": [{"text": "Weights", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9745705127716064}]}]}