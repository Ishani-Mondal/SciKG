{"title": [{"text": "Chinese word segmentation model using bootstrapping", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5752850373586019}]}], "abstractContent": [{"text": "We participate in the CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.5427974164485931}]}, {"text": "Unlike the previous bakeoff series, the purpose of the bakeoff 2010 is to test the cross-domain performance of Chinese seg-mentation model.", "labels": [], "entities": [{"text": "bakeoff 2010", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.8299192190170288}]}, {"text": "This paper summarizes our approach and our bakeoff results.", "labels": [], "entities": []}, {"text": "We mainly propose to use \u03c7 2 statistics to increase the OOV recall and use bootstrapping strategy to increase the overall F score.", "labels": [], "entities": [{"text": "OOV", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9805687665939331}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.8361319303512573}, {"text": "F score", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9908948242664337}]}, {"text": "As the results shows, the approach proposed in the paper does help, both of the OOV recall and the overall F score are improved .", "labels": [], "entities": [{"text": "OOV", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9948375821113586}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.8820449113845825}, {"text": "F score", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.9923037886619568}]}], "introductionContent": [{"text": "After more than twenty years of intensive researches, considerable progress has been made in improving the performance of Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 122, "end_pos": 147, "type": "TASK", "confidence": 0.5846972266832987}]}, {"text": "The bakeoff series hosted by the ACL SIGHAN shows that high F scores can be achieved in the closed test tracks, in which only specified training materials can be used in learning segmentation models.", "labels": [], "entities": [{"text": "ACL SIGHAN", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.8516016602516174}, {"text": "F scores", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9734800159931183}]}, {"text": "Instead of using lexicon-driven approaches, state-of-art Chinese word segmenter now use character tagging model as firstly proposed.", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.7294503450393677}, {"text": "character tagging", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7088580578565598}]}, {"text": "In character tagging model, no predefined Chinese lexicons are required; a tagging model is learned using manually segmented training texts.", "labels": [], "entities": [{"text": "character tagging", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.8241058886051178}]}, {"text": "The model is then used to assign each character a tag indicating the position of this character within word.", "labels": [], "entities": []}, {"text": "Xue's approach has been become the most popular approach to Chinese word segmentation for its high performance and unified way to deal with OOV issues.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.5913205742835999}]}, {"text": "Most of the segmentation works since then follow this approach.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.9866003394126892}]}, {"text": "Major improvements in this line of research including: 1) More sophisticated learning models were introduced instead of the maximum entropy model that Xue used, like conditional random fields (CRFs) model which fit the sequence tagging tasks much better than maximum entropy model).", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 219, "end_pos": 235, "type": "TASK", "confidence": 0.6836807429790497}]}, {"text": "2) More tags were introduced, as shows 6 tags are superior to 4 tags in achieving high performance.", "labels": [], "entities": []}, {"text": "3) New feature templates were added, such as templates used in representing numbers, dates, letters etc.", "labels": [], "entities": []}, {"text": "( Usually, the performance of segmentation model is evaluated on a test set from the same domain as the training set.", "labels": [], "entities": []}, {"text": "Such evaluation does not reveal its ability to deal with domain variation.", "labels": [], "entities": []}, {"text": "It is believed that, when test set is from other domains than the domain where training set is from, the learned model normally underperforms substantially.", "labels": [], "entities": []}, {"text": "The CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation is set to focus on the cross-domain performance of Chinese word segmentation model.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.5698424279689789}, {"text": "Chinese word segmentation", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.6077678998311361}]}, {"text": "We participate in the closed test track for simplified Chinese.", "labels": [], "entities": []}, {"text": "Different with the previous bakeoffs, CIPS-SIGHAN-2010 bake-off provides both label corpus and unlabeled corpora.", "labels": [], "entities": [{"text": "CIPS-SIGHAN-2010 bake-off", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.9126680493354797}]}, {"text": "The labeled corpus is composed of texts from newspaper and has about 1.1 million words in total.", "labels": [], "entities": []}, {"text": "The two unlabeled corpora cover two domains: literature and computer science, and each domain have about 100K characters in size.", "labels": [], "entities": []}, {"text": "The test corpora cover four domains, two of which are literature and computer science, and the other two domains are unknown before releasing.", "labels": [], "entities": []}, {"text": "We build the Chinese word segmenter following the character tagging model.", "labels": [], "entities": [{"text": "Chinese word segmenter", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.591384083032608}]}, {"text": "Instead of using CRF model, we use the hidden Markov support vector machines (, which is also a sequence labeling model like CRF.", "labels": [], "entities": []}, {"text": "We just show it can also be used to model Chinese segmentation tasks as an alternative other than CRF.", "labels": [], "entities": [{"text": "Chinese segmentation tasks", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.7127383550008138}]}, {"text": "To increase the ability of the model to recall OOV words, we propose to use \u03c7 2 statistics and bootstrapping strategy to the overall performance of the model to outof-domain texts.", "labels": [], "entities": []}], "datasetContent": [{"text": "The labeled training texts released by the bakeoff are mainly composed of texts from newspaper.", "labels": [], "entities": []}, {"text": "A peculiarity of the training data is that all Arabic numbers, Latin letters and punctuations in the data are double-byte codes.", "labels": [], "entities": []}, {"text": "As in Chinese texts, there are actually two versions of codes for Arabic numbers, Latin letters and punctuations: one is single-byte codes defined by the western character encoding standard; another is double-byte codes defined by the Chinese character encoding standards.", "labels": [], "entities": []}, {"text": "Chinese normally use both versions without distinguishing them strictly.", "labels": [], "entities": []}, {"text": "The four final test sets released by the bakeoff cover four domains, the statistics of the test sets are shown in table-1.", "labels": [], "entities": []}, {"text": "(the size is measured in characters) We train all models using SVM-HMMs 1 , we set \u03b5 to 0.25.", "labels": [], "entities": []}, {"text": "This is a parameter to control the accuracy of the solution of the optimization problem.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9989420771598816}]}, {"text": "We set C to half of the number of the sentences in the training data.", "labels": [], "entities": [{"text": "C", "start_pos": 7, "end_pos": 8, "type": "METRIC", "confidence": 0.9522897601127625}]}, {"text": "The C parameter is set to trade off the margin size and training error.", "labels": [], "entities": [{"text": "margin size", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.8787706792354584}, {"text": "training error", "start_pos": 56, "end_pos": 70, "type": "METRIC", "confidence": 0.9189940094947815}]}, {"text": "We also set a cutoff frequency to feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.775470495223999}]}, {"text": "Only features are seen more than three times in training data are actually used in the models.", "labels": [], "entities": []}, {"text": "We set K = 3 and run the algorithm shown in section 5.", "labels": [], "entities": []}, {"text": "This gives our final bakeoff results shown in.", "labels": [], "entities": []}, {"text": "To illustrate whether the \u03c7 2 statistics and bootstrapping strategy help or not, we also show two intermediate results using the online scoring system provided by the bakeoff 2 . shows the results of the initial non-\u03c7 2 -based model using feature template (a)-(f), shows results of the initial \u03c7 2 -based model using feature template (a)-(h).", "labels": [], "entities": []}, {"text": "As we see from the table-1, table-3 and table-4, the approach present in this paper does improve both the overall performance and the OOV recalls in all four domains.", "labels": [], "entities": [{"text": "OOV recalls", "start_pos": 134, "end_pos": 145, "type": "METRIC", "confidence": 0.8584083616733551}]}, {"text": "We also do a rapid manual check to the final results; one of the main sources of errors lies in the approach failing to recall numbers encoded by one-byte codes digits.", "labels": [], "entities": []}, {"text": "For the labeled training corpus provided by the bakeoff almost do not use one-byte codes for digits, and the type feature seems do not help too much.", "labels": [], "entities": []}, {"text": "Actually, such numbers can be recalled by simple heuristics using regular expressions.", "labels": [], "entities": []}, {"text": "We do a simple number recognition to the test set of domain D. this will increase the F score from 0.937 to 0.957.", "labels": [], "entities": [{"text": "number recognition", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7480337619781494}, {"text": "F score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9924654960632324}]}], "tableCaptions": []}