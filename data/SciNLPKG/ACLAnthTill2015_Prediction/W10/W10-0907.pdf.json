{"title": [{"text": "Semantic Role Labeling for Open Information Extraction", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7824706236521403}, {"text": "Open Information Extraction", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.6305398941040039}]}], "abstractContent": [{"text": "Open Information Extraction is a recent paradigm for machine reading from arbitrary text.", "labels": [], "entities": [{"text": "Open Information Extraction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6788504620393118}, {"text": "machine reading from arbitrary text", "start_pos": 53, "end_pos": 88, "type": "TASK", "confidence": 0.8411949157714844}]}, {"text": "In contrast to existing techniques, which have used only shallow syntactic features, we investigate the use of semantic features (se-mantic roles) for the task of Open IE.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 163, "end_pos": 170, "type": "TASK", "confidence": 0.5866050720214844}]}, {"text": "We compare TEXTRUNNER (Banko et al., 2007), a state of the art open extractor, with our novel extractor SRL-IE, which is based on UIUC's SRL system (Punyakanok et al., 2008).", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9767112731933594}, {"text": "UIUC's SRL system", "start_pos": 130, "end_pos": 147, "type": "DATASET", "confidence": 0.8994815647602081}]}, {"text": "We find that SRL-IE is robust to noisy heterogeneous Web data and outperforms TEXTRUN-NER on extraction quality.", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 13, "end_pos": 19, "type": "TASK", "confidence": 0.4293633699417114}, {"text": "TEXTRUN-NER", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.8057200312614441}]}, {"text": "On the other hand, TEXTRUNNER performs over 2 orders of magnitude faster and achieves good precision in high locality and high redundancy extractions.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.8944564461708069}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9969310164451599}]}, {"text": "These observations enable the construction of hybrid extractors that output higher quality results than TEXTRUNNER and similar quality as SRL-IE in much less time.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9862815737724304}, {"text": "SRL-IE", "start_pos": 138, "end_pos": 144, "type": "DATASET", "confidence": 0.597831666469574}]}], "introductionContent": [{"text": "The grand challenge of Machine Reading () requires, as a key step, a scalable system for extracting information from large, heterogeneous, unstructured text.", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.813278317451477}]}, {"text": "The traditional approaches to information extraction (e.g.,) do not operate at these scales, since they focus attention on a well-defined small set of relations and require large amounts of training data for each relation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.8633159399032593}]}, {"text": "The recent Open Information Extraction paradigm () attempts to overcome the knowledge acquisition bottleneck with its relation-independent nature and no manually annotated training data.", "labels": [], "entities": [{"text": "Open Information Extraction", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.6397281090418497}, {"text": "knowledge acquisition", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.7712398171424866}]}, {"text": "We are interested in the best possible technique for Open IE.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.7117524147033691}]}, {"text": "The TEXTRUNNER Open IE system () employs only shallow syntactic features in the extraction process.", "labels": [], "entities": [{"text": "TEXTRUNNER Open IE", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.47405145565668744}]}, {"text": "Avoiding the expensive processing of deep syntactic analysis allowed TEXTRUNNER to process at Web scale.", "labels": [], "entities": []}, {"text": "In this paper, we explore the benefits of semantic features and in particular, evaluate the application of semantic role labeling (SRL) to Open IE.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 107, "end_pos": 135, "type": "TASK", "confidence": 0.7812264462312063}, {"text": "Open IE", "start_pos": 139, "end_pos": 146, "type": "TASK", "confidence": 0.5050277411937714}]}, {"text": "SRL is a popular NLP task that has seen significant progress over the last few years.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.89991295337677}]}, {"text": "The advent of hand-constructed semantic resources such as) have resulted in semantic role labelers achieving high in-domain precisions.", "labels": [], "entities": [{"text": "semantic role labelers", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.6692919532457987}, {"text": "precisions", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9481614232063293}]}, {"text": "Our first observation is that semantically labeled arguments in a sentence almost always correspond to the arguments in Open IE extractions.", "labels": [], "entities": [{"text": "Open IE extractions", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.5793811877568563}]}, {"text": "Similarly, the verbs often match up with Open IE relations.", "labels": [], "entities": []}, {"text": "These observations lead us to construct anew Open IE extractor based on SRL.", "labels": [], "entities": [{"text": "Open IE extractor", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.49179256955782574}, {"text": "SRL", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.895264208316803}]}, {"text": "We use UIUC's publicly available SRL system) that is known to be competitive with the state of the art and construct a novel Open IE extractor based on it called SRL-IE.", "labels": [], "entities": [{"text": "UIUC", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.9469197988510132}]}, {"text": "We first need to evaluate SRL-IE's effectiveness in the context of large scale and heterogeneous input data as found on the Web: because SRL uses deeper analysis we expect SRL-IE to be much slower.", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 26, "end_pos": 32, "type": "TASK", "confidence": 0.9870169758796692}, {"text": "SRL", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.937353789806366}]}, {"text": "Second, SRL is trained on news corpora using a resource like Propbank, and so may face recall loss due to out of vocabulary verbs and precision loss due to different writing styles found on the Web.", "labels": [], "entities": [{"text": "SRL", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9692172408103943}, {"text": "Propbank", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.9543677568435669}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9979336261749268}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9984754920005798}]}, {"text": "In this paper we address several empirical ques-52 tions.", "labels": [], "entities": []}, {"text": "Can SRL-IE, our SRL based extractor, achieve adequate precision/recall on the heterogeneous Web text?", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.6821327805519104}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9993416666984558}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9794935584068298}]}, {"text": "What factors influence the relative performance of SRL-IE vs. that of TEXTRUNNER (e.g., n-ary vs. binary extractions, redundancy, locality, sentence length, out of vocabulary verbs, etc.)?", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 51, "end_pos": 57, "type": "TASK", "confidence": 0.6722739934921265}]}, {"text": "In terms of performance, what are the relative tradeoffs between the two?", "labels": [], "entities": []}, {"text": "Finally, is it possible to design a hybrid between the two systems to get the best of both the worlds?", "labels": [], "entities": []}, {"text": "Our results show that: 1.", "labels": [], "entities": []}, {"text": "SRL-IE is surprisingly robust to noisy heterogeneous data and achieves high precision and recall on the Open IE task on Web text.", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.85718834400177}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9992527365684509}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9995033740997314}]}, {"text": "2. SRL-IE outperforms TEXTRUNNER along dimensions such as recall and precision on complex extractions (e.g., n-ary relations).", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.8139990568161011}, {"text": "TEXTRUNNER", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9904329776763916}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9988129138946533}, {"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9976230263710022}]}, {"text": "3. TEXTRUNNER is over 2 orders of magnitude faster, and achieves good precision for extractions with high system confidence or high locality or when the same fact is extracted from multiple sentences.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 3, "end_pos": 13, "type": "METRIC", "confidence": 0.965945303440094}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.999057948589325}]}, {"text": "4. Hybrid extractors that use a combination of SRL-IE and TEXTRUNNER get the best of both worlds.", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.7273162007331848}, {"text": "TEXTRUNNER", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9940963983535767}]}, {"text": "Our hybrid extractors make effective use of available time and achieve a superior balance of precision-recall, better precision compared to TEXTRUNNER, and better recall compared to both TEXTRUNNER and SRL-IE.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 93, "end_pos": 109, "type": "METRIC", "confidence": 0.9995098114013672}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9994496703147888}, {"text": "TEXTRUNNER", "start_pos": 140, "end_pos": 150, "type": "METRIC", "confidence": 0.5856435894966125}, {"text": "recall", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.9993414282798767}, {"text": "SRL-IE", "start_pos": 202, "end_pos": 208, "type": "DATASET", "confidence": 0.8717818260192871}]}], "datasetContent": [{"text": "In our quantitative evaluation we attempt to answer two key questions: what is the relative difference in performance of SRL-IE and TEXTRUNNER on precision, recall and computation time?", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.5990344882011414}, {"text": "TEXTRUNNER", "start_pos": 132, "end_pos": 142, "type": "METRIC", "confidence": 0.9870893359184265}, {"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.998134434223175}, {"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9923943281173706}]}, {"text": "And, what factors influence the relative performance of the two systems?", "labels": [], "entities": []}, {"text": "We explore the first question in Section 5.2 and the second in Section 5.3.", "labels": [], "entities": []}, {"text": "Our goal is to explore the behavior of TEXTRUN-NER and SRL-IE on a large scale dataset containing redundant information, since redundancy has been shown to immensely benefit Web-based Open IE extractors.", "labels": [], "entities": [{"text": "Open IE extractors", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.5943280458450317}]}, {"text": "At the same time, the test set must be a manageable size, due to SRL-IE's relatively slow processing time.", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.6286271810531616}]}, {"text": "We constructed a test set that approximates Web-scale distribution of extractions for five target relations -invent, graduate, study, write, and develop.", "labels": [], "entities": []}, {"text": "We created our test set as follows.", "labels": [], "entities": []}, {"text": "We queried a corpus of 500M Web documents fora sample of sentences with these verbs (or their inflected forms, e.g., invents, invented, etc.).", "labels": [], "entities": []}, {"text": "We then ran TEXTRUNNER and SRL-IE on those sentences to find 200 distinct values of arg0 for each target relation, 100 from each system.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9282028079032898}, {"text": "SRL-IE", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.8005534410476685}, {"text": "arg0", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.986006498336792}]}, {"text": "We searched for at most 100 sentences that contain both the verb-form and arg0.", "labels": [], "entities": [{"text": "arg0", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9735100865364075}]}, {"text": "This resulted in a test set of an average of 6,000 sentences per relation, fora total of 29,842 sentences.", "labels": [], "entities": []}, {"text": "We use this test set for all experiments in this paper.", "labels": [], "entities": []}, {"text": "In order to compute precision and recall on this dataset, we tagged extractions by TEXTRUNNER and by SRL-IE as corrector errors.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9991660118103027}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9989916682243347}, {"text": "TEXTRUNNER", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9921405911445618}, {"text": "SRL-IE", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.5151023864746094}]}, {"text": "A tuple is correct if the arguments have correct boundaries and the relation accurately expresses the relationship between all of the arguments.", "labels": [], "entities": []}, {"text": "Our definition of correct boundaries does not favor either system over the other.", "labels": [], "entities": []}, {"text": "For instance, while TEXTRUNNER extracts <Bunsen, invented, a device> from the sentence \"Bunsen invented a device called the Spectroscope\", and SRL-IE includes the entire phrase \"a device called the Spectroscope\" as the second argument, both extractions would be marked as correct.", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 143, "end_pos": 149, "type": "DATASET", "confidence": 0.8096497058868408}]}, {"text": "Determining the absolute recall in these experiments is precluded by the amount of hand labeling necessary and the ambiguity of such a task.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9759174585342407}, {"text": "hand labeling", "start_pos": 83, "end_pos": 96, "type": "TASK", "confidence": 0.6558686345815659}]}, {"text": "Instead, we compute pseudo-recall by taking the union of correct tuples from both methods as denominator.", "labels": [], "entities": []}, {"text": "shows the performance of TEXTRUNNER and SRL-IE on this dataset.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9654311537742615}, {"text": "SRL-IE", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.4555966258049011}]}, {"text": "Since TEXTRUNNER can output different points on the precision-recall curve based on the confidence of the CRF we choose the point that maximizes F1.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.8964112997055054}, {"text": "precision-recall", "start_pos": 52, "end_pos": 68, "type": "METRIC", "confidence": 0.9947965741157532}, {"text": "F1", "start_pos": 145, "end_pos": 147, "type": "METRIC", "confidence": 0.9992042183876038}]}, {"text": "SRL-IE achieved much higher recall at substantially higher precision.", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9192011952400208}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9996575117111206}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9968396425247192}]}, {"text": "This was, however, at the cost of a much larger processing time.", "labels": [], "entities": []}, {"text": "For our dataset, TEXTRUNNER took 6.3 minutes and SRL- IE took 52.1 hours -roughly 2.5 orders of magnitude longer.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9545621275901794}, {"text": "SRL- IE", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.5834595064322153}]}, {"text": "We ran our experiments on quad-core 2.8GHz processors with 4GB of memory.", "labels": [], "entities": []}, {"text": "Figure 3(a) and(a) report the precision of each system for binary and n-ary extractions measured against available computation time.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9993485808372498}]}, {"text": "PRECHY-BRID starts at slightly higher precision due to our filtering of potentially low quality extractions from TEXTRUNNER.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9991439580917358}, {"text": "TEXTRUNNER", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.5312978625297546}]}, {"text": "For binary this precision is even better than SRL-IE's.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9984614849090576}]}, {"text": "It gradually loses precision until it reaches SRL-IE's level.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9996151924133301}, {"text": "SRL-IE", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.8356442451477051}]}, {"text": "RECALLHYBRID improves on TEXTRUNNER's precision, albeit at a much slower rate and remains worse than SRL-IE and PRECHYBRID throughout.", "labels": [], "entities": [{"text": "RECALLHYBRID", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9772058725357056}, {"text": "TEXTRUNNER", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.7352283000946045}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9785556793212891}, {"text": "SRL-IE", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.6923500895500183}]}, {"text": "The recall for binary and n-ary extractions are shown in(b) and(b), again measured against available time.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9990503191947937}]}, {"text": "While PRECHYBRID significantly improves on TEXTRUNNER's recall, it does lose recall compared to RECALLHYBRID, especially for n-ary extractions.", "labels": [], "entities": [{"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9177187085151672}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9995551705360413}, {"text": "RECALLHYBRID", "start_pos": 96, "end_pos": 108, "type": "METRIC", "confidence": 0.9666205048561096}]}, {"text": "PRECHYBRID also shows a large initial drop in recall due to filtering.", "labels": [], "entities": [{"text": "PRECHYBRID", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7178559303283691}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9929513335227966}]}, {"text": "Lastly, the gains in precision from PRECHYBRID are offset by loss in recall that leaves the F1 measure essentially identical to that of RECALLHYBRID).", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9993540644645691}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9990928173065186}, {"text": "F1 measure", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9832146763801575}, {"text": "RECALLHYBRID", "start_pos": 136, "end_pos": 148, "type": "METRIC", "confidence": 0.765874445438385}]}, {"text": "However, fora fixed time budget both hybrid F-measures are significantly better than TEXTRUNNER and SRL-IE F-measures demonstrating the power of the hybrid extractors.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9615228176116943}, {"text": "TEXTRUNNER", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9898524880409241}, {"text": "SRL-IE F-measures", "start_pos": 100, "end_pos": 117, "type": "METRIC", "confidence": 0.7863856852054596}]}, {"text": "Both methods reach a much higher F1 than TEX-TRUNNER: again of over 0.15 in half SRL-IE's processing time and over 0.3 after the full processing time.", "labels": [], "entities": [{"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9996823072433472}, {"text": "TEX-TRUNNER", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.879532516002655}]}, {"text": "Both hybrids perform better than SRL-IE given equal processing time.", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.5257165431976318}]}, {"text": "We believe that most often constructing a higher quality database of facts with a relatively lower recall is more useful than vice-versa, making PRECHYBRID to be of wider applicability than RE-CALLHYBRID.", "labels": [], "entities": [{"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9989802241325378}, {"text": "RE-CALLHYBRID", "start_pos": 190, "end_pos": 203, "type": "METRIC", "confidence": 0.7043820023536682}]}, {"text": "Still the choice of the actual hybrid extractor could change based on the task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SRL-IE outperforms TEXTRUNNER in both re- call and precision, but has over 2.5 orders of magnitude  longer run time.", "labels": [], "entities": [{"text": "SRL-IE", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.6933510899543762}, {"text": "TEXTRUNNER", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9672305583953857}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.998713493347168}]}]}