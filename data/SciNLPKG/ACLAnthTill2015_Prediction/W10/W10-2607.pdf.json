{"title": [{"text": "Domain Adaptation with Unlabeled Data for Dialog Act Tagging", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7267339080572128}, {"text": "Dialog Act Tagging", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7495625019073486}]}], "abstractContent": [{"text": "We investigate the classification of utterances into high-level dialog act categories using word-based features, under conditions where the train and test data differ by genre and/or language.", "labels": [], "entities": [{"text": "classification of utterances into high-level dialog act categories", "start_pos": 19, "end_pos": 85, "type": "TASK", "confidence": 0.8222585022449493}]}, {"text": "We handle the cross-language cases with machine translation of the test utterances.", "labels": [], "entities": []}, {"text": "We analyze and compare two feature-based approaches to using unlabeled data in adaptation: restriction to a shared feature set, and an implementation of Blitzer et al.'s Structural Correspondence Learning.", "labels": [], "entities": [{"text": "Structural Correspondence Learning", "start_pos": 170, "end_pos": 204, "type": "TASK", "confidence": 0.7808942596117655}]}, {"text": "Both methods lead to increased detection of backchannels in the cross-language cases by utilizing correlations between backchannel words and utterance length.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dialog act (or speech act) tagging aims to label abstract functions of utterances in conversations, such as Request, Floorgrab, or Statement; potential applications include automatic conversation analysis, punctuation transcription, and humancomputer dialog systems.", "labels": [], "entities": [{"text": "Dialog act (or speech act) tagging", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.5592296048998833}, {"text": "automatic conversation analysis", "start_pos": 173, "end_pos": 204, "type": "TASK", "confidence": 0.6410438219706217}, {"text": "punctuation transcription", "start_pos": 206, "end_pos": 231, "type": "TASK", "confidence": 0.727865919470787}]}, {"text": "Although some applications require domain-specific tag sets, it is often useful to label utterances based on generic tags, and several tag sets have been developed for this purpose, e.g. DAMSL).", "labels": [], "entities": []}, {"text": "Many approaches to automatic dialog act (DA) tagging assume hand-labeled training data.", "labels": [], "entities": [{"text": "automatic dialog act (DA) tagging", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.684166669845581}]}, {"text": "However, when building anew system it maybe difficult to find a labeled corpus that matches the target domain, or even the language.", "labels": [], "entities": []}, {"text": "Even within the same language, speech from different domains can differ linguistically, and the same DA categories might be characterized by different cues.", "labels": [], "entities": []}, {"text": "The domain characteristics (face-to-face vs. telephone, two-party vs. multi-party, informal vs. agendadriven, familiar vs. stranger) can influence both the distribution of tags and word choice.", "labels": [], "entities": []}, {"text": "This work attempts to use unlabeled target domain data in order to improve cross-domain training performance, an approach referred to as both unsupervised and semi-supervised domain adaptation in the literature.", "labels": [], "entities": []}, {"text": "We refer to the labeled training domain as the source domain.", "labels": [], "entities": []}, {"text": "We compare two adaptation approaches: a simple one based on forcing the classifier to learn only on \"shared\" features that appear in both domains, and a more complex one based on Structural Correspondence Learning (SCL) from . The shared feature approach has been investigated for adaptation in other tasks, e.g. Aue and for sentiment classification and  for parsing.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 325, "end_pos": 349, "type": "TASK", "confidence": 0.9567603468894958}, {"text": "parsing", "start_pos": 359, "end_pos": 366, "type": "TASK", "confidence": 0.9792068004608154}]}, {"text": "SCL has been used successfully for sentiment classification and part-ofspeech tagging); here we investigate its applicability to the DA classification task, using a multi-view learning implementation as suggested by.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.9734441339969635}, {"text": "part-ofspeech tagging", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.7767250537872314}, {"text": "DA classification task", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.9269536932309469}]}, {"text": "In addition to analyzing these two methods on a novel task, we show an interesting comparison between them: in this setting, both methods turnout to have a similar effect caused by correlating cues fora particular DA class (Backchannel) with length.", "labels": [], "entities": []}, {"text": "We classify pre-segmented utterances based on their transcripts, and we consider only four highlevel classes: Statement, Question, Backchannel, and Incomplete.", "labels": [], "entities": [{"text": "Backchannel", "start_pos": 131, "end_pos": 142, "type": "METRIC", "confidence": 0.9616414308547974}]}, {"text": "Experiments are performed using all train/test pairs among three conversational speech corpora : the Meeting Recorder Dialog Act corpus (MRDA) (), Switchboard DAMSL (Swbd) (, and the Spanish Callhome dialog act corpus (SpCH) ().", "labels": [], "entities": [{"text": "Meeting Recorder Dialog Act corpus (MRDA)", "start_pos": 101, "end_pos": 142, "type": "DATASET", "confidence": 0.7578258439898491}, {"text": "Spanish Callhome dialog act corpus (SpCH)", "start_pos": 183, "end_pos": 224, "type": "DATASET", "confidence": 0.817874550819397}]}, {"text": "The first is multi-party, face-to-face meeting speech; the second is topicprompted telephone speech between strangers; and the third is informal telephone speech between friends and family members.", "labels": [], "entities": []}, {"text": "The first two are in English, while the third is in Spanish.", "labels": [], "entities": []}, {"text": "When the source and target domains differ in language, we apply machine translation to the target domain to convert it to the language of the source domain.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Proportion of utterances in each  DA category (Incomplete, Statement, Question,  Backchannel) in each domain's training set.", "labels": [], "entities": [{"text": "Backchannel", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.9519796967506409}]}, {"text": " Table 2: Overall accuracy and average per-class  recall on each test set, using in-domain, in-domain  translated, and cross-domain training. Starred re- sults under the accuracy column are significantly  different from the corresponding cross-domain  baseline under McNemar's test (p < 0.05). (Sig- nificance is not calculated for the average per-class  recall column.) \"Majority\" classifies everything as  Statement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9994864463806152}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.979924201965332}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9980466365814209}]}]}