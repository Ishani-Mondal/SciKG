{"title": [{"text": "Towards an Empirically Motivated Typology of Follow-Up Questions: The Role of Dialogue Context", "labels": [], "entities": []}], "abstractContent": [{"text": "A central problem in Interactive Question Answering (IQA) is how to answer Follow-Up Questions (FU Qs), possibly by taking advantage of information from the dialogue context.", "labels": [], "entities": [{"text": "Interactive Question Answering (IQA)", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.7492794493834177}, {"text": "answer Follow-Up Questions (FU Qs)", "start_pos": 68, "end_pos": 102, "type": "TASK", "confidence": 0.5313731900283268}]}, {"text": "We assume that FU Qs can be classified into specific types which determine if and how the correct answer relates to the preceding dialogue.", "labels": [], "entities": [{"text": "FU Qs", "start_pos": 15, "end_pos": 20, "type": "TASK", "confidence": 0.5947252362966537}]}, {"text": "The main goal of this paper is to propose an empirically motivated typology of FU Qs, which we then apply in a practical IQA setting.", "labels": [], "entities": [{"text": "FU Qs", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.5908675789833069}]}, {"text": "We adopt a supervised machine learning framework that ranks answer candidates to FU Qs.", "labels": [], "entities": [{"text": "FU Qs", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.5150143504142761}]}, {"text": "Both the answer ranking and the classification of FU Qs is done in this framework, based on a host of measures that include shallow and deep inter-utterance relations, automatically collected dialogue management meta information, and human annotation.", "labels": [], "entities": [{"text": "classification of FU Qs", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.5649522244930267}]}, {"text": "We use Principal Component Analysis (PCA) to integrate these measures.", "labels": [], "entities": []}, {"text": "As a result, we confirm earlier findings about the benefit of distinguishing between topic shift and topic continuation FU Qs.", "labels": [], "entities": [{"text": "topic shift", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.671493411064148}, {"text": "topic continuation FU Qs", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.5150534063577652}]}, {"text": "We then present a typology of FU Qs that is more fine-grained, extracted from the PCA and based on real dialogue data.", "labels": [], "entities": [{"text": "FU Qs", "start_pos": 30, "end_pos": 35, "type": "TASK", "confidence": 0.5319443643093109}, {"text": "PCA", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.9393157362937927}]}, {"text": "Since all our measures are automatically computable, our results are relevant for IQA systems dealing with naturally occurring FU Qs.", "labels": [], "entities": []}], "introductionContent": [{"text": "When real users engage in written conversations with an Interactive Question Answering (IQA) system, they typically do so in a sort of dialogue rather than asking single shot questions.", "labels": [], "entities": [{"text": "Interactive Question Answering (IQA)", "start_pos": 56, "end_pos": 92, "type": "TASK", "confidence": 0.755763590335846}]}, {"text": "The questions' context, i.e., the preceding interactions, should be useful for understanding FollowUp Questions (FU Qs) and helping the system pinpoint the correct answer.", "labels": [], "entities": [{"text": "understanding FollowUp Questions (FU Qs)", "start_pos": 79, "end_pos": 119, "type": "TASK", "confidence": 0.6078109102589744}]}, {"text": "In previous work, we studied how dialogue context should be considered to answer FU Qs.", "labels": [], "entities": [{"text": "FU Qs", "start_pos": 81, "end_pos": 86, "type": "TASK", "confidence": 0.5280449688434601}]}, {"text": "We have used Logistic Regression Models (LRMs), both for learning which aspects of dialogue structure are relevant to answering FU Qs, and for comparing the accuracy with which the resulting IQA systems can correctly answer these questions.", "labels": [], "entities": [{"text": "answering FU Qs", "start_pos": 118, "end_pos": 133, "type": "TASK", "confidence": 0.6449133654435476}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9991693496704102}]}, {"text": "Unlike much of the related research in IQA, which used artificial collections of user questions, our work has been based on real user-system dialogues we collected via a chatbot-inspired help-desk IQA system deployed on the website of our University Library.", "labels": [], "entities": [{"text": "University Library", "start_pos": 239, "end_pos": 257, "type": "DATASET", "confidence": 0.8594011962413788}]}, {"text": "Previously, our experiments used a selection of shallow () and deep ( features, all of which describe specific relations holding between two utterances (i.e., user questions or system answers).", "labels": [], "entities": []}, {"text": "In this paper we present additional features derived from automatically collected dialogue meta-data from our chatbot's dialogue management component.", "labels": [], "entities": []}, {"text": "We use Principal Component Analysis (PCA) to combine the benefits of all these information sources, as opposed to using only certain hand-selected features as in our previous work.", "labels": [], "entities": [{"text": "Principal Component Analysis (PCA)", "start_pos": 7, "end_pos": 41, "type": "TASK", "confidence": 0.7293544312318166}]}, {"text": "The main goal of this paper is to learn from data anew typology of FU Qs; we then compare it to an existing typology based on hand-annotated FU Q types, as proposed in the literature.", "labels": [], "entities": []}, {"text": "We show how this new typology is effective for finding the correct answer to a FU Q.", "labels": [], "entities": [{"text": "FU Q", "start_pos": 79, "end_pos": 83, "type": "TASK", "confidence": 0.42750489711761475}]}, {"text": "We produce this typology by analyzing the main components of the PCA.", "labels": [], "entities": [{"text": "PCA", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.828557014465332}]}, {"text": "This paper presents two main results.", "labels": [], "entities": []}, {"text": "A new, empirically motivated typology of FU Qs confirms earlier results about the practical benefit of distinguishing between topic continuation and topic shift FU Qs, which are typically based on hand annotation.", "labels": [], "entities": [{"text": "FU Qs", "start_pos": 41, "end_pos": 46, "type": "TASK", "confidence": 0.5573941022157669}, {"text": "topic continuation", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7409319281578064}, {"text": "topic shift FU Qs", "start_pos": 149, "end_pos": 166, "type": "TASK", "confidence": 0.542803667485714}]}, {"text": "We then show that we can do without such hand annotations, in that our fully automatic, on-line measures -which include automatically collected dialogue meta-data from our chatbot's dialogue manager -lead to better performance in identifying correct answers to FU Qs.", "labels": [], "entities": [{"text": "identifying correct answers to FU Qs", "start_pos": 230, "end_pos": 266, "type": "TASK", "confidence": 0.5601488053798676}]}, {"text": "In the remainder of this paper, we first review relevant previous work concerning FU Q typologies in IQA.", "labels": [], "entities": [{"text": "FU Q typologies", "start_pos": 82, "end_pos": 97, "type": "METRIC", "confidence": 0.4678430954615275}]}, {"text": "Section 3 then introduces our collection of realistic IQA dialogues which we will use in all our experiments; the section includes descriptions of meta information in the form of dialogue management features and post-hoc human annotations.", "labels": [], "entities": []}, {"text": "In Section 4 we introduce our experimental framework, based on inter-utterance features and LRMs.", "labels": [], "entities": []}, {"text": "Our experimental results are presented in Section 5, which is followed by our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We employ a standard 10-fold cross-validation scheme for splitting training and prediction data.", "labels": [], "entities": [{"text": "splitting training and prediction", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.6274037435650826}]}, {"text": "We assess our LRMs by comparing the ranks that the models assign to the gold-standard correct A 2 candidate (i.e., the single A 2 that our library domain experts had marked as correct for each of the 1,522 FU Qs).", "labels": [], "entities": []}, {"text": "To determine whether differences in A 2 ranking performance are significant, we consult both the paired t-test and the Wilcoxon signed rank test about the difference of the 1,522 ranks.", "labels": [], "entities": [{"text": "Wilcoxon signed rank test", "start_pos": 119, "end_pos": 144, "type": "METRIC", "confidence": 0.5771970897912979}]}], "tableCaptions": [{"text": " Table 1: Improving ranking of correct A 2 (out of 306 answer candidates) with different PCA-based  interaction terms. Significance tests of rank differences wrt. result in preceding row.", "labels": [], "entities": []}, {"text": " Table 2: Strongest loadings for the three PCs retained as interaction terms in Model PCA B , and indication  of each PC's positive/negative influence on lexical similarity-based A 2 selection features", "labels": [], "entities": []}]}