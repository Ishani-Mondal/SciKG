{"title": [{"text": "Eliminating Redundancy by Spectral Relaxation for Multi-Document Summarization", "labels": [], "entities": [{"text": "Eliminating Redundancy", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9060819447040558}, {"text": "Spectral Relaxation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.759573757648468}, {"text": "Summarization", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.7234291434288025}]}], "abstractContent": [{"text": "This paper focuses on redundancy, overlapping information in multi-documents, and presents a method for detecting salient, key sentences from documents that discuss the same event.", "labels": [], "entities": [{"text": "detecting salient, key sentences from documents that discuss the same event", "start_pos": 104, "end_pos": 179, "type": "TASK", "confidence": 0.7009264826774597}]}, {"text": "To eliminate redundancy, we used spectral clustering and classified each sentence into groups, each of which consists of semantically related sentences.", "labels": [], "entities": []}, {"text": "Then, we applied link analysis, the Markov Random Walk (MRW) Model to deciding the importance of a sentence within documents.", "labels": [], "entities": []}, {"text": "The method was tested on the NTCIR evaluation data, and the result shows the effectiveness of the method.", "labels": [], "entities": [{"text": "NTCIR evaluation data", "start_pos": 29, "end_pos": 50, "type": "DATASET", "confidence": 0.971725861231486}]}], "introductionContent": [{"text": "With the exponential growth of information on the Internet, it is becoming increasingly difficult fora user to read and understand all the materials from a series of large-scale document streams that is potentially of interest.", "labels": [], "entities": []}, {"text": "Multi-document summarization is an issue to attack the problem.", "labels": [], "entities": [{"text": "Multi-document summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.751447468996048}]}, {"text": "It differs from single document summarization in that it is important to identify differences and similarities across documents.", "labels": [], "entities": [{"text": "single document summarization", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.6236466566721598}]}, {"text": "Graph-based ranking methods, such as PageRank () and HITS) have recently applied and been successfully used for multi-document summarization ().", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 112, "end_pos": 140, "type": "TASK", "confidence": 0.6862076222896576}]}, {"text": "Given a set of documents, the model constructs graph consisting vertices and edges where vertices are sentences and edges reflect the relationships between sentences.", "labels": [], "entities": []}, {"text": "The model then applies a graph-based ranking method to obtain the rank scores for the sentences.", "labels": [], "entities": []}, {"text": "Finally, the sentences with large rank scores are chosen into the summary.", "labels": [], "entities": []}, {"text": "However, when they are strung together, the resulting summary still contains much overlapping information.", "labels": [], "entities": []}, {"text": "Because all the sentences are ranked based on a sentence as unit of information.", "labels": [], "entities": []}, {"text": "Therefore, for example, semantically related two sentences with \"high recommendation\" are ranked with high score, and thus are regarded as a summary sentence.", "labels": [], "entities": []}, {"text": "To attack the problem, Wan et al. proposed two models, i.e., the Cluster-based conditional Markov Random Walk model and the Cluster-based HITS model, both make use of the theme clusters in the document set (.", "labels": [], "entities": []}, {"text": "Their model first groups documents into theme clusters by using a simple clustering method, k-means.", "labels": [], "entities": []}, {"text": "Next, the model constructs a directed or undirected graph to reflect the relationships between sentences and clusters by using link analysis.", "labels": [], "entities": []}, {"text": "They reported that the results on the DUC2001 and DUC2002 datasets showed the effectiveness of their models.", "labels": [], "entities": [{"text": "DUC2001", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9616863131523132}, {"text": "DUC2002 datasets", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.9166036546230316}]}, {"text": "However, one of the problems using multivariate clustering such as k-means is that it is something of a black art when applied to high-dimensional data.", "labels": [], "entities": []}, {"text": "The available techniques for searching this large space do not offer guarantees of global optimality, thus the resulting summary still contains much overlapping information, especially fora large amount of documents.", "labels": [], "entities": []}, {"text": "This paper focuses extractive summarization, and present a method for detecting key sentences from documents that discuss the same event.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.6431118845939636}]}, {"text": "Like Wan et al.'s approach, we applied link analysis, the Markov Random Walk (MRW) model) to a graph consisting sentences and clusters.", "labels": [], "entities": []}, {"text": "To attack the problem dealing with the high dimensional spaces, we applied spectral clustering technique () to the sentences from a document set.", "labels": [], "entities": []}, {"text": "Spectral clustering is a transformation of the original sentences into a set of orthogonal eigenvectors.", "labels": [], "entities": [{"text": "Spectral clustering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9121441841125488}]}, {"text": "We worked in the space defined by the first few eigenvectors, using standard clustering techniques in the transformed space.", "labels": [], "entities": []}], "datasetContent": [{"text": "We had an experiment by using the NTCIR-3 1 SUMM to evaluate our approach.", "labels": [], "entities": [{"text": "NTCIR-3 1 SUMM", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.8061079780260721}]}, {"text": "NTCIR-3 has two tasks, single, and multi-document summarization.", "labels": [], "entities": [{"text": "NTCIR-3", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.914597749710083}]}, {"text": "The data is collected from two years) Mainichi Japanese Newspaper articles.", "labels": [], "entities": [{"text": "Mainichi Japanese Newspaper articles", "start_pos": 38, "end_pos": 74, "type": "DATASET", "confidence": 0.950802817940712}]}, {"text": "We used multi-document summarization task.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 8, "end_pos": 36, "type": "TASK", "confidence": 0.5817530453205109}]}, {"text": "There are two types of gold standard data provided to human judges, FBFREE DryRun and FormalRun, each of which consists of 30 topics.", "labels": [], "entities": [{"text": "FBFREE DryRun", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.6378511339426041}]}, {"text": "There are two types of correct summary according to the character length, i.e., \"long\" and \"short\".", "labels": [], "entities": []}, {"text": "All documents were tagged by a morphological analysis, ChaSen () and noun words are extracted.", "labels": [], "entities": []}, {"text": "We used FormalRun consisting of 30 topics as a test data.", "labels": [], "entities": []}, {"text": "Similarly, we randomly chose 10 topics from the FBFREE DryRun data to tuning a parameter \u03c3 in Spectral Clustering, and the number of l in the l-dimensional space obtained by the Spectral Clustering.", "labels": [], "entities": [{"text": "FBFREE DryRun data", "start_pos": 48, "end_pos": 66, "type": "DATASET", "confidence": 0.9335205554962158}]}, {"text": "\u03c3 is searched in steps of 0.01 from 1.0 to 5.0.", "labels": [], "entities": []}, {"text": "l in the l-dimensional space is searched in steps 10% from 0 to 80% against the total number of words in the training data.", "labels": [], "entities": []}, {"text": "The size that optimized the average F-score of 10 topics was chosen.", "labels": [], "entities": [{"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9877415895462036}]}, {"text": "Here, F-score is the standard measure used in the clustering algorithm, and it combines recall and precision with an equal weight.", "labels": [], "entities": [{"text": "F-score", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.9961626529693604}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9994258880615234}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9981926083564758}]}, {"text": "Precision is a ratio of the number of correct pair of sentences obtained by the k-means divided by the total number of pairs obtained by the k-means.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9926153421401978}]}, {"text": "Recall indicates a ratio of the number of correct pair of sentences obtained by the k-means divided by the total number of correct pairs.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9879769682884216}]}, {"text": "As a result, \u03c3 and l are set to 4.5 and 80%, respectively.", "labels": [], "entities": []}, {"text": "It is difficult to predict the actual cluster number kin a given input sentences to produce optimal results.", "labels": [], "entities": []}, {"text": "The usual drawback in many clustering algorithms is that they cannot give a valid criterion for measuring class structure.", "labels": [], "entities": []}, {"text": "Therefore, similar to Wan et. al's method (, we typically set the number of k of expected clusters as \u221a N where N is the number of all sentences in the document set.", "labels": [], "entities": []}, {"text": "We used these values of the parameters and evaluated by using test data.", "labels": [], "entities": []}, {"text": "We used two evaluation measures.", "labels": [], "entities": []}, {"text": "One is cosine similarity between the generated summary by the system and the human generated summary.", "labels": [], "entities": []}, {"text": "Another is ROUGE score used in DUC (.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.9828679263591766}, {"text": "DUC", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.7402816414833069}]}, {"text": "We used a word instead of n-gram sequence in formula (6).", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "\"# of doc\" and \"# of sent\" refer to the average number of documents and sentences, respectively.", "labels": [], "entities": []}, {"text": "\"# of sum\" denotes to the average number of summary sentences provided by NTCIR3 SUMM.", "labels": [], "entities": [{"text": "NTCIR3 SUMM", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9049937129020691}]}, {"text": "\"cos\" and \"ROUGE\" refer to the results evaluated by using cosine, and ROUGE score, respectively.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9935759902000427}, {"text": "ROUGE score", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.9820713996887207}]}, {"text": "\"MRW\" indicates the results obtained by directly applying MRW model to the input sentences.", "labels": [], "entities": [{"text": "MRW", "start_pos": 1, "end_pos": 4, "type": "METRIC", "confidence": 0.7437359094619751}]}, {"text": "We can see from that our approach (Spectral) outperforms the baselines, \"MRW\" and \"k-means\", regardless of the types of summary (long/short) and evaluation measures (cosine/ROUGE).", "labels": [], "entities": [{"text": "MRW", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.839409351348877}, {"text": "ROUGE", "start_pos": 173, "end_pos": 178, "type": "METRIC", "confidence": 0.9738757014274597}]}, {"text": "The results obtained by three approaches show that \"short\" was better than \"long\".", "labels": [], "entities": []}, {"text": "This indicates that the rank score of correct sentences within the candidate sentences obtained by the MRW model works well.", "labels": [], "entities": [{"text": "rank score", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9514137506484985}, {"text": "MRW", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.8911492228507996}]}, {"text": "Comparing the results evaluated by \"ROUGE\" were worse than those of \"cos\" at any approaches.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9930574893951416}]}, {"text": "One reason is that the difference of summarization technique, i.e., our work is extractive summarization, while the gold standard data provided by NTCIR-3 SUMM is the abstracts written by human professionals.", "labels": [], "entities": [{"text": "summarization", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9779395461082458}, {"text": "NTCIR-3 SUMM", "start_pos": 147, "end_pos": 159, "type": "DATASET", "confidence": 0.8620737791061401}]}, {"text": "As a result, a large number of words in a candidate summary are extracted by our approaches.", "labels": [], "entities": []}, {"text": "For future work, it is necessary to extend our method to involve paraphrasing for extracted key sentences to reduce the gap between automatically generated summaries and humanwritten abstracts (.", "labels": [], "entities": []}, {"text": "It is interesting to note how our approach affects for the number of sentences as an input.", "labels": [], "entities": []}, {"text": "illustrates the results of summary \"long\" with evaluated ROUGE score.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.9721290469169617}]}, {"text": "We can see from that our approach is more robust than k-means and the MRW model, even fora large number of input data.", "labels": [], "entities": [{"text": "MRW", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8358713984489441}]}, {"text": "We have seen the same observations from other three results, i.e., the results of short and long with evaluated cos and short with evaluated ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.9888483285903931}]}, {"text": "We recall that the cluster number k is set to the square root of the sentence number.", "labels": [], "entities": []}, {"text": "We tested different number of k to see how the cluster number: Long with ROUGE vs. # of sentences affects the summarization performance.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9956615567207336}, {"text": "summarization", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.968730092048645}]}, {"text": "In the experiment, we set k = r * | N | where r is a parameter ranged from 0 to 1 (.", "labels": [], "entities": []}, {"text": "Because of space is limited, we report only the result with summary \"long\" and ROUGE score.", "labels": [], "entities": [{"text": "summary \"long\"", "start_pos": 60, "end_pos": 74, "type": "METRIC", "confidence": 0.7775474935770035}, {"text": "ROUGE score", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9795647859573364}]}, {"text": "The result is shown in.", "labels": [], "entities": []}, {"text": "Overall the results obtained by our approach and k-means outperformed the results obtained by directly applying MRW model, while the results by k-means was worse than the results by MRW model when the ratio of the number of sentences was larger than 0.8.", "labels": [], "entities": [{"text": "MRW", "start_pos": 112, "end_pos": 115, "type": "DATASET", "confidence": 0.9002985954284668}]}, {"text": "This shows that clusterbased summarization is effective reduce redundancy, overlapping information.", "labels": [], "entities": []}, {"text": "also shows that our approach always outperforms, regardless of how many number of sentences were used.", "labels": [], "entities": []}, {"text": "This indicates that the MRW model with spectral clustering is more robust than that with the baseline, k-means, with respect to the different number of clusters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results against 30 topics", "labels": [], "entities": []}]}