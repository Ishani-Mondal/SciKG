{"title": [{"text": "The UMUS system for named entity generation at GREC 2010", "labels": [], "entities": [{"text": "named entity generation", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.656051884094874}, {"text": "GREC", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.845909833908081}]}], "abstractContent": [{"text": "We present the UMUS (Universit\u00e9 du Maine/Universit\u00e4t Stuttgart) submission for the NEG task at GREC'10.", "labels": [], "entities": [{"text": "UMUS (Universit\u00e9 du Maine/Universit\u00e4t Stuttgart) submission", "start_pos": 15, "end_pos": 74, "type": "DATASET", "confidence": 0.8323688924312591}, {"text": "NEG task", "start_pos": 83, "end_pos": 91, "type": "TASK", "confidence": 0.9060133695602417}, {"text": "GREC'10", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.857136607170105}]}, {"text": "We refined and tuned our 2009 system but we still rely on predicting generic labels and then choosing from the list of expressions that match those labels.", "labels": [], "entities": []}, {"text": "We handled recur-sive expressions with care by generating specific labels for all the possible embed-dings.", "labels": [], "entities": []}, {"text": "The resulting system performs at a type accuracy of 0.84 an a string accuracy of 0.81 on the development set.", "labels": [], "entities": [{"text": "type accuracy", "start_pos": 35, "end_pos": 48, "type": "METRIC", "confidence": 0.7194012403488159}, {"text": "string accuracy", "start_pos": 62, "end_pos": 77, "type": "METRIC", "confidence": 0.6941895484924316}]}], "introductionContent": [{"text": "The Named Entity Generation (NEG) task consists in choosing a referential expression (complete name, last name, pronoun, possessive pronoun, elision...) for all person entities in a text.", "labels": [], "entities": [{"text": "Named Entity Generation (NEG)", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.8233514328797659}]}, {"text": "Texts are biographies of chefs, composers and inventors from Wikipedia.", "labels": [], "entities": []}, {"text": "For each reference, a list of expressions is given from which the system has to choose.", "labels": [], "entities": []}, {"text": "This task is challenging because of the following aspects: 1.", "labels": [], "entities": []}, {"text": "The data is imperfect as it is a patchwork of multiple authors' writing.", "labels": [], "entities": []}, {"text": "2. The problem is hard to handle with a classifier because text is predicted, not classes.", "labels": [], "entities": []}, {"text": "3. The problem has a complex graph structure.", "labels": [], "entities": []}, {"text": "4. Some decisions are recursive for embedded references, i.e. \"his father\".", "labels": [], "entities": []}, {"text": "5. Syntactic/semantic features cannot be extracted with a classical parser because the word sequence is latent.", "labels": [], "entities": []}, {"text": "We do not deal with all of these challenges but we try to mitigate their impact.", "labels": [], "entities": []}, {"text": "Our system extends our approach for GREC'09).", "labels": [], "entities": [{"text": "GREC'09", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.5692601799964905}]}, {"text": "We use a sequence classifier to predict generic labels for the possible expressions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. The source code of our systems is made  available to the community at http://code.google  .com/p/icsicrf-grecneg.", "labels": [], "entities": []}, {"text": " Table 1: Results on the dev set comparing our sys- tem from last year (old) to the refined one (new),  according to REG08 TYPE accuracy (T.acc), pre- cision and recall, String accuracy (S.acc), BLEU1  an NIST.", "labels": [], "entities": [{"text": "REG08 TYPE accuracy (T.acc)", "start_pos": 117, "end_pos": 144, "type": "METRIC", "confidence": 0.8117253482341766}, {"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.98857182264328}, {"text": "String accuracy (S.acc)", "start_pos": 170, "end_pos": 193, "type": "METRIC", "confidence": 0.8449187040328979}, {"text": "BLEU1", "start_pos": 195, "end_pos": 200, "type": "METRIC", "confidence": 0.9995193481445312}, {"text": "NIST", "start_pos": 205, "end_pos": 209, "type": "METRIC", "confidence": 0.8383380770683289}]}]}