{"title": [{"text": "Fast Consensus Hypothesis Regeneration for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7568835318088531}]}], "abstractContent": [{"text": "This paper presents a fast consensus hypothesis regeneration approach for machine translation.", "labels": [], "entities": [{"text": "consensus hypothesis regeneration", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.7172230084737142}, {"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.8220609426498413}]}, {"text": "It combines the advantages of feature-based fast consensus decoding and hypothesis regeneration.", "labels": [], "entities": [{"text": "hypothesis regeneration", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.7585810720920563}]}, {"text": "Our approach is more efficient than previous work on hypothesis regeneration, and it explores a wider search space than consensus decoding, resulting in improved performance.", "labels": [], "entities": [{"text": "hypothesis regeneration", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.76753830909729}]}, {"text": "Experimental results show consistent improvements across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-to-English NIST task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9987491369247437}, {"text": "NIST task", "start_pos": 189, "end_pos": 198, "type": "DATASET", "confidence": 0.8048052191734314}]}], "introductionContent": [{"text": "State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.7888136307398478}]}, {"text": "In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest.", "labels": [], "entities": []}, {"text": "Then in the second pass, various re-ranking algorithms are adopted to compute the final translation.", "labels": [], "entities": []}, {"text": "The re-ranking algorithms include rescoring () and Minimum Bayes-Risk (MBR) decoding (.", "labels": [], "entities": [{"text": "Minimum Bayes-Risk (MBR)", "start_pos": 51, "end_pos": 75, "type": "METRIC", "confidence": 0.7543707966804505}]}, {"text": "Rescoring uses more sophisticated additional feature functions to score the hypotheses.", "labels": [], "entities": [{"text": "Rescoring", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.8998653888702393}]}, {"text": "MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance fora specific loss function.", "labels": [], "entities": [{"text": "MT", "start_pos": 143, "end_pos": 145, "type": "TASK", "confidence": 0.9799970388412476}]}, {"text": "In particular, sentence-level BLEU loss function gives gains on BLEU ().", "labels": [], "entities": [{"text": "BLEU loss", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9336561858654022}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9924696087837219}]}, {"text": "The na\u00efve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k 2 ) comparisons.", "labels": [], "entities": []}, {"text": "Therefore, only small number k is applicable.", "labels": [], "entities": []}, {"text": "Very recently, proposed a fast consensus decoding (FCD) algorithm in which the similarity scores are computed based on the feature expectations over the translation N-best list or translation forest.", "labels": [], "entities": []}, {"text": "It is equivalent to MBR decoding when using a linear similarity function, such as unigram precision.", "labels": [], "entities": [{"text": "MBR decoding", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.6302449405193329}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.5274621248245239}]}, {"text": "Re-ranking approaches improve performance on an N-best list whose contents are fixed.", "labels": [], "entities": []}, {"text": "A complementary strategy is to augment the contents of an N-best list in order to broaden the search space.", "labels": [], "entities": []}, {"text": "have proposed a three-pass SMT process, in which a hypothesis regeneration pass is added between the decoding and rescoring passes.", "labels": [], "entities": [{"text": "SMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9781352281570435}]}, {"text": "New hypotheses are generated based on the original N-best hypotheses through n-gram expansion, confusion-network decoding or re-decoding.", "labels": [], "entities": []}, {"text": "All three hypothesis regeneration methods obtained decent and comparable improvements in conjunction with the same rescoring model.", "labels": [], "entities": [{"text": "hypothesis regeneration", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7491465210914612}]}, {"text": "However, since the final translation candidates in this approach are produced from different methods, local feature functions (such as translation models and reordering models) of each hypothesis are not directly comparable and rescoring must exploit rich global feature functions to compensate for the loss of local feature functions.", "labels": [], "entities": []}, {"text": "Thus this approach is dependent on the use of computationally expensive features for rescoring, which makes it inefficient.", "labels": [], "entities": []}, {"text": "In this paper, we propose a fast consensus hypothesis regeneration method that combines the advantages of feature-based fast consensus decoding and hypothesis regeneration.", "labels": [], "entities": [{"text": "consensus hypothesis regeneration", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.6384284098943075}, {"text": "hypothesis regeneration", "start_pos": 148, "end_pos": 171, "type": "TASK", "confidence": 0.7221199423074722}]}, {"text": "That is, we integrate the feature-based similarity/loss function based on evaluation metrics such as BLEU score into the hypothesis regeneration procedure to score the partial hypotheses in the beam search and compute the final translations.", "labels": [], "entities": [{"text": "similarity/loss function", "start_pos": 40, "end_pos": 64, "type": "METRIC", "confidence": 0.8294211328029633}, {"text": "BLEU score", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9821680784225464}]}, {"text": "Thus, our approach is more efficient than the original threepass hypothesis regeneration.", "labels": [], "entities": []}, {"text": "Moreover, our approach explores more search space than consen-sus decoding, giving it an advantage over the latter.", "labels": [], "entities": []}, {"text": "In particular, we extend linear corpus BLEU ( to n-gram expectationbased linear BLEU, then further extend the ngram expectation computed on full-length hypotheses to n-gram expectation computed on fixedlength partial hypotheses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.8767220377922058}]}, {"text": "Finally, we extend the hypothesis regeneration with forward n-gram expansion to bidirectional n-gram expansion including both the forward and backward n-gram expansion.", "labels": [], "entities": [{"text": "hypothesis regeneration", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7209046930074692}]}, {"text": "Experimental results show consistent improvements over the baseline across language pairs, and up to 0.72 BLEU points are obtained from a competitive baseline on the Chinese-toEnglish NIST task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9993879795074463}, {"text": "Chinese-toEnglish NIST task", "start_pos": 166, "end_pos": 193, "type": "DATASET", "confidence": 0.5763052999973297}]}], "datasetContent": [{"text": "We carried out experiments based on translation N-best lists generated by a state-of-the-art phrase-based statistical machine translation system, similar to (.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 93, "end_pos": 137, "type": "TASK", "confidence": 0.6306102126836777}]}, {"text": "In detail, the phrase table is derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning.", "labels": [], "entities": []}, {"text": "The baseline is a log-linear feature combination that includes language models, the distortion components, translation model, phrase and word penalties.", "labels": [], "entities": []}, {"text": "Weights on feature functions are found by lattice MERT ( ).", "labels": [], "entities": [{"text": "MERT", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9683893918991089}]}], "tableCaptions": [{"text": " Table 1: Statistics of training, dev, and test sets for  Chinese-to-English task.", "labels": [], "entities": []}, {"text": " Table 2: Translation performances in BLEU-4(%)  over 1000-best lists for Chinese-to-English task: \"res- coring\" represents the results of rescoring; \"three- pass\", three-pass hypothesis regeneration with for- ward n-gram expansion; \"FCD\", fast consensus de- coding; \"Fwd\", the results of hypothesis regeneration  with forward n-gram expansion; \"Bwd\", backward n- gram expansion; and \"Bid\", bi-directional n-gram  expansion.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9359068274497986}, {"text": "BLEU-4", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.7908554673194885}, {"text": "Fwd", "start_pos": 268, "end_pos": 271, "type": "METRIC", "confidence": 0.9545563459396362}, {"text": "Bwd", "start_pos": 346, "end_pos": 349, "type": "METRIC", "confidence": 0.850753903388977}, {"text": "Bid", "start_pos": 385, "end_pos": 388, "type": "METRIC", "confidence": 0.8647620677947998}]}, {"text": " Table 4. The same trend as  in the first experiment can also be observed in  this experiment. It is worth noticing that enlarg- ing the size of the N-best list from 1000 to  10,000 did not change the performance signifi- cantly. Bi-directional n-gram expansion obtained  improvements of 0.", "labels": [], "entities": []}, {"text": " Table 4: Translation performances in BLEU-4 (%)  over 10K-best lists.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.5176359415054321}, {"text": "BLEU-4", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9915398955345154}]}, {"text": " Table 5: Translation performances in BLEU-4 (%)  over 1000-best lists for Chinese-to-English task:  \"full\" represents expectations over n-gram counts that  are computed on whole hypotheses; \"partial\"  represents expectations over n-gram counts that are  computed on partial hypotheses.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9005833864212036}, {"text": "BLEU-4", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9386891722679138}]}]}