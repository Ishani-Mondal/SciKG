{"title": [{"text": "How to Get the Same News from Different Language News Papers", "labels": [], "entities": [{"text": "Get the Same News from Different Language News Papers", "start_pos": 7, "end_pos": 60, "type": "TASK", "confidence": 0.7744881676303016}]}], "abstractContent": [{"text": "This paper presents an ongoing work on identifying similarity between documents across News papers in different languages.", "labels": [], "entities": []}, {"text": "Our aim is to identify similar documents fora given News or event as a query, across languages and make cross lingual search more accurate and easy.", "labels": [], "entities": []}, {"text": "For example given an event or News in English, all the English news documents related to the query are retrieved as well as in other languages such as Hindi, Bengali, Tamil, Telugu, Malayalam, Spanish.", "labels": [], "entities": []}, {"text": "We use Vector Space Model, a known method for similarity calculation, but the novelty is in identification of terms for VSM calculation.", "labels": [], "entities": [{"text": "similarity calculation", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.6942952871322632}, {"text": "VSM calculation", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.884762704372406}]}, {"text": "Here a robust translation system is not used for translating the documents.", "labels": [], "entities": []}, {"text": "The system is working with good recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9997642636299133}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9990838766098022}]}], "introductionContent": [{"text": "In this paper we present a novel method for identifying similar News documents from various language families such as IndoEuropean, Indo-Aryan and Dravidian.", "labels": [], "entities": []}, {"text": "The languages considered from the above language families are English, Hindi, Bengali, Tamil, Telugu, Malayalam and Spanish.", "labels": [], "entities": []}, {"text": "The News documents in various languages are obtained using a crawler.", "labels": [], "entities": []}, {"text": "The documents are represented as vector of terms.", "labels": [], "entities": []}, {"text": "Given a query in any of the language mentioned above, the documents relevant to the query are retrieved.", "labels": [], "entities": []}, {"text": "The first two document retrieved in the language of the query is taken as base for the identification of similar documents.", "labels": [], "entities": []}, {"text": "The documents are converted into terms and the terms are translated to other languages using bilingual dictionaries.", "labels": [], "entities": []}, {"text": "The terms thus obtained is used for similarity calculation.", "labels": [], "entities": [{"text": "similarity calculation", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.7522992193698883}]}, {"text": "The paper is further organized as follows.", "labels": [], "entities": []}, {"text": "In the following section 2, related work is described.", "labels": [], "entities": []}, {"text": "In section 3, the algorithm is discussed.", "labels": [], "entities": []}, {"text": "Section 4 describes experiments and results.", "labels": [], "entities": []}, {"text": "The paper concludes with section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have performed three experiments with two different data sets.", "labels": [], "entities": []}, {"text": "The first data set was collected by crawling the web fora single day's news articles and obtained 1000 documents from various online news magazines in various languages.", "labels": [], "entities": []}, {"text": "The test set was taken from Times of India, The Hindu for English, BBC, Dinamani, Dinamalar for Tamil, Yahoo for Telugu, Matrubhumi for Malayalam, BBC and Dainik Jagran for Hindi and BBC for Spanish.", "labels": [], "entities": [{"text": "BBC", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9505653381347656}, {"text": "BBC", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.9562027454376221}, {"text": "BBC", "start_pos": 183, "end_pos": 186, "type": "DATASET", "confidence": 0.8890188932418823}]}, {"text": "The distribution of documents in the first set for various languages is as follows: 300 English, 200 Tamil, 150 Telugu, 125 Hindi, 125 Malayalam, 50 Spanish.", "labels": [], "entities": []}, {"text": "The given below shows the language distribution in this first set.", "labels": [], "entities": []}, {"text": "The number of similar documents were 600 in this set.", "labels": [], "entities": []}, {"text": "In the second data set we have taken news documents of one week time duration.", "labels": [], "entities": []}, {"text": "This consisted of 9750 documents.", "labels": [], "entities": []}, {"text": "The language distribution for this data set is shown in.", "labels": [], "entities": []}, {"text": "This second data set consisted of 5350 similar documents.", "labels": [], "entities": []}, {"text": "In the first experiment we took all the unique words (separated by white space) as terms for building the document vector.", "labels": [], "entities": []}, {"text": "In the second experiment the terms taken were same as the first experiment, except that all the stop words were removed.", "labels": [], "entities": []}, {"text": "In the third experiment, the terms taken for document vector creation were four consecutive words.", "labels": [], "entities": [{"text": "document vector creation", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.7327917814254761}]}, {"text": "The results obtained for three experiments for data set 1 is shown in.", "labels": [], "entities": []}, {"text": "And results for data set 2 are shown in. shows the similarity identification for various languages.", "labels": [], "entities": []}, {"text": "Here we take a news story document as a query and perform similarity analysis across all documents in the document collection to identify similarly occurring news stories.", "labels": [], "entities": []}, {"text": "In the first data set in the gold standard there are 600 similar pairs of documents.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9047128260135651}]}, {"text": "And in the second data set there are 5350 similar pairs of documents in the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.8282301425933838}]}, {"text": "It is observed that even though there were more similar documents which could have been identified, but the system could not identify those documents.", "labels": [], "entities": []}, {"text": "The cosine measure for those unidentified documents was found to be lower than 0.8.", "labels": [], "entities": [{"text": "cosine measure", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.949958086013794}]}, {"text": "We have taken 0.8 as the threshold for documents to be considered similar.", "labels": [], "entities": []}, {"text": "In the documents which were not identified by the system, the content described consisted of less number of words.", "labels": [], "entities": []}, {"text": "These were mostly two paragraph documents; hence the similarity score obtained was less than the threshold.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 53, "end_pos": 69, "type": "METRIC", "confidence": 0.9845698475837708}]}, {"text": "In experiment three, we find that the number of false positives is decreased and also the number of documents identified similar is increased.", "labels": [], "entities": []}, {"text": "This is because, in this case the system sees for terms of four words and hence single word matches are reduced.", "labels": [], "entities": []}, {"text": "The other advantage of this is the words get the context, in a sense that the words in each sequence are not independent.", "labels": [], "entities": []}, {"text": "The words get an order and are sensitive to that order.", "labels": [], "entities": []}, {"text": "Hence we find that it is solving the polysemy problem to some extent.", "labels": [], "entities": []}, {"text": "The system can be further improved by creating robust map files between terms in different languages.", "labels": [], "entities": []}, {"text": "The bilingual dictionaries also need to be improved.", "labels": [], "entities": []}, {"text": "In our work, since we are using a sequence of words as terms for document vectors, we do not require proper, sophisticated translation systems.", "labels": [], "entities": []}, {"text": "A word byword translation would suffice to get the desired results.", "labels": [], "entities": [{"text": "word byword translation", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.6537322103977203}]}], "tableCaptions": []}