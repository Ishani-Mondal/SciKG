{"title": [{"text": "Maximum Likelihood Estimation of Feature-based Distributions", "labels": [], "entities": []}], "abstractContent": [{"text": "Motivated by recent work in phonotac-tic learning (Hayes and Wilson 2008, Al-bright 2009), this paper shows how to define feature-based probability distributions whose parameters can be provably efficiently estimated.", "labels": [], "entities": []}, {"text": "The main idea is that these distributions are defined as a product of simpler distributions (cf. Ghahra-mani and Jordan 1997).", "labels": [], "entities": []}, {"text": "One advantage of this framework is it draws attention to what is minimally necessary to describe and learn phonological feature interactions in phonotactic patterns.", "labels": [], "entities": []}, {"text": "The \"bottom-up\" approach adopted here is contrasted with the \"top-down\" approach in Hayes and Wilson (2008), and it is argued that the bottom-up approach is more analytically transparent.", "labels": [], "entities": []}], "introductionContent": [{"text": "The hypothesis that the atomic units of phonology are phonological features, and not segments, is one of the tenets of modern phonology.", "labels": [], "entities": []}, {"text": "According to this hypothesis, segments are essentially epiphenomenal and exist only by virtue of being a shorthand description of a collection of more primitive units-the features.", "labels": [], "entities": []}, {"text": "Incorporating this hypothesis into phonological learning models has been the focus of much influential work (.", "labels": [], "entities": []}, {"text": "This paper makes three contributions.", "labels": [], "entities": []}, {"text": "The first contribution is a framework within which: 1.", "labels": [], "entities": []}, {"text": "researchers can choose which statistical independence assumptions to make regarding phonological features; 2.", "labels": [], "entities": []}, {"text": "feature systems can be fully integrated into strictly local (i.e. n-gram models) and strictly piecewise models () in order to define families of provably wellformed, feature-based probability distributions that are provably efficiently estimable.", "labels": [], "entities": []}, {"text": "The main idea is to define a family of distributions as the normalized product of simpler distributions.", "labels": [], "entities": []}, {"text": "Each simpler distribution can be represented by a Probabilistic Deterministic Finite Acceptor (PDFA), and the product of these PDFAs defines the actual distribution.", "labels": [], "entities": [{"text": "Probabilistic Deterministic Finite Acceptor (PDFA)", "start_pos": 50, "end_pos": 100, "type": "METRIC", "confidence": 0.6870153886931283}]}, {"text": "When a family of distributions F is defined in this way, F may have many fewer parameters than if F is defined over the product PDFA directly.", "labels": [], "entities": []}, {"text": "This is because the parameters of the distributions are defined in terms of the factors which combine in predictable ways via the product.", "labels": [], "entities": []}, {"text": "Fewer parameters means accurate estimation occurs with less data and, relatedly, the family contains fewer distributions.", "labels": [], "entities": []}, {"text": "This idea is not new.", "labels": [], "entities": []}, {"text": "It is explicit in Factorial Hidden Markov Models (FHMMs), and more recently underlies approaches to describing and inferring regular string transductions.", "labels": [], "entities": [{"text": "regular string transductions", "start_pos": 125, "end_pos": 153, "type": "TASK", "confidence": 0.6094655493895212}]}, {"text": "Although HMMs and probabilistic finite-state automata describe the same class of distributions (), this paper presents these ideas informal language-theoretic and automata-theoretic terms because (1) there are no hidden states and is thus simpler than FHMMs, (2) determinstic automata have several desirable properties crucially used here, and (3) PDFAs add probabilities to structure whereas HMMs add structure to probabilities and the authors are more comfortable with the former perspective (for further discussion, see).", "labels": [], "entities": [{"text": "FHMMs", "start_pos": 252, "end_pos": 257, "type": "DATASET", "confidence": 0.8685970306396484}]}, {"text": "The second contribution illustrates the main idea with a feature-based bigram model with a strong statistical independence assumption: no two features interact.", "labels": [], "entities": []}, {"text": "This is shown to capture exactly the intuition that sounds with like features have like distributions.", "labels": [], "entities": []}, {"text": "Also, the assumption of non-interacting features is shown to be too strong because like sounds do not have like distributions in actual phonotactic patterns.", "labels": [], "entities": []}, {"text": "Four kinds of featural interactions are identified and possible solutions are discussed.", "labels": [], "entities": []}, {"text": "Finally, we compare this proposal with.", "labels": [], "entities": []}, {"text": "Essentially, the model here represents a \"bottom-up\" approach whereas theirs is \"top-down.\"", "labels": [], "entities": []}, {"text": "\"Top-down\" models, which consider every set of features as potentially interacting in every allowable context, face the difficult problem of searching avast space and often resort to heuristic-based methods, which are difficult to analyze.", "labels": [], "entities": []}, {"text": "To illustrate, we suggest that the role played by phonological features in the phonotactic learner in is not well-understood.", "labels": [], "entities": []}, {"text": "We demonstrate that classes of all segments but one (i.e. the complement classes of single segments) play a significant role, which diminishes the contribution provided by natural classes themselves (i.e. ones made by phonological features).", "labels": [], "entities": []}, {"text": "In contrast, the proposed model here is analytically transparent.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "\u00a72 reviews some background.", "labels": [], "entities": []}, {"text": "\u00a73 discusses bigram models and \u00a74 defines feature systems and feature-based distributions.", "labels": [], "entities": []}, {"text": "\u00a75 develops a model with a strong independence assumption and \u00a76 discusses featural interaction.", "labels": [], "entities": []}, {"text": "\u00a77 dicusses and \u00a78 concludes.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: ML estimates of parameters of segment- based SL 2 distributions.", "labels": [], "entities": [{"text": "ML", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9089409708976746}]}, {"text": " Table 3: ML estimates of parameters of feature- based SL 2 distributions.", "labels": [], "entities": [{"text": "ML", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9137296080589294}]}, {"text": " Table 4: Correlations of different settings versions  of HW maxent model with Scholes data.", "labels": [], "entities": [{"text": "HW maxent model", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.881525973478953}]}, {"text": " Table 4. Is the \u223c 0.01 gain in r score worth  the additional parameters which refer to phono-logically natural classes? Also, the feature-based  SL 2 model in  \u00a74 only receives an r score of 0.751,  much lower than the results in", "labels": [], "entities": []}]}