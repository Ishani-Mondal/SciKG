{"title": [{"text": "JHU System Combination Scheme for WMT 2010", "labels": [], "entities": [{"text": "JHU System Combination Scheme", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.8985972702503204}, {"text": "WMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.894399881362915}]}], "abstractContent": [{"text": "This paper describes the JHU system combination scheme that was used in the WMT 2010 submission.", "labels": [], "entities": [{"text": "JHU", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.7625875473022461}, {"text": "WMT 2010 submission", "start_pos": 76, "end_pos": 95, "type": "DATASET", "confidence": 0.7172031005223592}]}, {"text": "The in-cremental alignment scheme of (Karakos et.al, 2008) was used for confusion network generation.", "labels": [], "entities": [{"text": "confusion network generation", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.8403635819753011}]}, {"text": "The system order in the alignment of each sentence was learned using SVMs, following the work of (Karakos et.al, 2010).", "labels": [], "entities": []}, {"text": "Additionally, web-scale n-grams from the Google corpus were used to build language models that improved the quality of the combination output.", "labels": [], "entities": [{"text": "Google corpus", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.7675699293613434}]}, {"text": "Experiments in Spanish-English, French-English, German-English and Czech-English language pairs were conducted, and the results show approximately 1 BLEU point and 2 TER points improvement over the best individual system .", "labels": [], "entities": [{"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.999667763710022}, {"text": "TER", "start_pos": 166, "end_pos": 169, "type": "METRIC", "confidence": 0.993846595287323}]}], "introductionContent": [{"text": "System Combination refers to the method of combining output of multiple MT systems, to produce a output better than each individual system.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9488362073898315}]}, {"text": "Currently, there are several approaches to machine translation which can be classified as phrasebased, hierarchical, syntax-based which are equally good in their translation quality even though the underlying frameworks are completely different.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7487583160400391}]}, {"text": "The motivation behind System Combination arises from this diversity in the state-of-art MT systems, which suggests that systems with different paradigms make different errors, and can be made better by combining their strengths.", "labels": [], "entities": [{"text": "System Combination", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7962864935398102}, {"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9660715460777283}]}, {"text": "One approach of combining translations is based on representing translations by confusion network and then aligning these confusion networks using string alignment algorithms, . Another approach generates features for every translation to train algorithms for ranking systems based on their quality and the top ranking output is considered to be a candidate translation,) is an example of ranking based combination.", "labels": [], "entities": []}, {"text": "We use ideas from ranking based approaches to learn order in which systems should be aligned in a confusion network based approach.", "labels": [], "entities": []}, {"text": "Our approach is based on incremental alignment of confusion networks), wherein each system output is represented by a confusion network.", "labels": [], "entities": []}, {"text": "The confusion networks are then aligned in a pre-defined order to generate a combination output.", "labels": [], "entities": []}, {"text": "This paper contributes two enhancements to).", "labels": [], "entities": []}, {"text": "First, use of Support Vector Machines to learn order in which the system outputs should be aligned.", "labels": [], "entities": []}, {"text": "Second, we explore use of Google n-grams for building dynamic language model and interpolate the resulting language model with a large static language model for rescoring of system combination outputs.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 illustrates the idea and pipeline of the baseline combination system; Section 3 gives details of SVM ranking for learning system order for combination; Section 4 explains use of Google n-gram based language models; Results are discussed in Section 5; Concluding remarks are given in Section 6;", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for all Language pairs on development set  es-en  fr-en  cz-en  de-en  Combination  BLEU TER BLEU TER BLEU TER BLEU TER  BEST SYSTEM 29.27 52.38 26.74 56.88 21.56 58.24 26.53 56.87  BASELINE  28.57 51.61 27.65 55.20 21.01 58.79 26.80 54.54  SVM  28.68 51.99 27.53 55.35 21.56 58.24 26.85  54.9  SVM+NGRAM 29.92 50.92 27.86 55.06 21.80 57.78 27.24 54.86", "labels": [], "entities": [{"text": "Combination  BLEU TER BLEU TER BLEU TER BLEU TER  BEST", "start_pos": 89, "end_pos": 143, "type": "METRIC", "confidence": 0.875368595123291}]}]}