{"title": [{"text": "Deep Syntax Language Models and Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.7502887845039368}]}], "abstractContent": [{"text": "Hierarchical Models increase the reordering capabilities of MT systems by introducing non-terminal symbols to phrases that map source language (SL) words/phrases to the correct position in the target language (TL) translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9904202818870544}]}, {"text": "Building translations via discontiguous TL phrases increases the difficulty of language modeling, however, introducing the need for heuristic techniques such as cube pruning (Chiang, 2005), for example.", "labels": [], "entities": []}, {"text": "An additional possibility to aid language modeling in hierarchical systems is to use a language model that models fluency of words not using their local context in the string, as in traditional language models, but instead using the deeper context of a word.", "labels": [], "entities": []}, {"text": "In this paper, we explore the potential of deep syntax language models providing an interesting comparison with the traditional string-based language model.", "labels": [], "entities": []}, {"text": "We include an experimental evaluation that compares the two kinds of models independently of any MT system to investigate the possible potential of integrating a deep syntax language model into Hierarchical SMT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9369240403175354}, {"text": "SMT", "start_pos": 207, "end_pos": 210, "type": "TASK", "confidence": 0.7145312428474426}]}], "introductionContent": [{"text": "In Phrase-Based Models of Machine Translation all phrases consistent with the word alignment are extracted (, with shorter phrases needed for high coverage of unseen data and longer phrases providing improved fluency in target language translations.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7066977024078369}]}, {"text": "Hierarchical Models) build on PhraseBased Models by relaxing the constraint that phrases must be contiguous sequences of words and allow a short phrase (or phrases) nested within a longer phrase to be replaced by a non-terminal symbol forming anew hierarchical phrase.", "labels": [], "entities": []}, {"text": "Traditional language models use the local context of words to estimate the probability of the sentence and introducing hierarchical phrases that generate discontiguous sequences of TL words increases the difficulty of computing language model probabilities during decoding and require sophisticated heuristic language modeling techniques).", "labels": [], "entities": []}, {"text": "Leaving aside heuristic language modeling fora moment, the difficulty of integrating a traditional string-based language model into the decoding process in a hierarchical system, highlights a slight incongruity between the translation model and language model in Hierarchical Models.", "labels": [], "entities": []}, {"text": "According to the translation model, the best way to build a fluent TL translation is via discontiguous phrases, while the language model can only provide information about the fluency of contiguous sequences of words.", "labels": [], "entities": [{"text": "TL translation", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.9128890335559845}]}, {"text": "Intuitively, a language model that models fluency between discontiguous words maybe well-suited to hierarchical models.", "labels": [], "entities": []}, {"text": "Deep syntax language models condition the probability of a word on its deep context, i.e. words linked to it via dependency relations, as opposed to preceding words in the string.", "labels": [], "entities": []}, {"text": "During decoding in Hierarchical Models, words missing a context in the string due to being preceded by a non-terminal, might however be in a dependency relation with a word that is already present in the string and this context could add useful information about the fluency of the hypothesis as its constructed.", "labels": [], "entities": []}, {"text": "In addition, using the deep context of a word provides a deeper notion of fluency than the local context provides on its own and this might be useful to improve such things as lexical choice in SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 194, "end_pos": 197, "type": "TASK", "confidence": 0.9893926382064819}]}, {"text": "Good lexical choice is very important and the deeper context of a word, if available, may provide more meaningful information and result in better lexical choice.", "labels": [], "entities": []}, {"text": "Integrating such a model into a Hierarchical SMT system is not straightforward, however, and we believe before embarking on this its worthwhile to evaluate the model independently of any MT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.7017860412597656}, {"text": "MT", "start_pos": 187, "end_pos": 189, "type": "TASK", "confidence": 0.9534413814544678}]}, {"text": "We therefore provide an experimental evaluation of the model and in order to provide an interesting comparison, we evaluate a traditional string-based language model on the same data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carryout an experimental evaluation to investigate the potential of the deep syntax language model we describe in this paper independently of any machine translation system.", "labels": [], "entities": []}, {"text": "We train a 5-gram deep syntax language model on 7M English f-structures, and evaluate it by computing the perplexity and ngram coverage statistics on a heldout test set of parsed fluent English sentences.", "labels": [], "entities": [{"text": "ngram coverage", "start_pos": 121, "end_pos": 135, "type": "METRIC", "confidence": 0.8280978202819824}]}, {"text": "In order to provide an interesting comparison, we also train a traditional string-based 5-gram language model on the same training data and test it on the same held-out test set of English sentences.", "labels": [], "entities": []}, {"text": "A deep syntax language model comes with the obvious disadvantage that any data it is trained on must be in-coverage of the parser, whereas a string-based language model can be trained on any available data of the appropriate language.", "labels": [], "entities": []}, {"text": "Since parser coverage is not the focus of our work, we eliminate its effects from the evaluation by selecting the training and test data for both the stringbased and deep syntax language models on the basis that they are in fact in-coverage of the parser.", "labels": [], "entities": [{"text": "parser coverage", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.9268557727336884}]}], "tableCaptions": [{"text": " Table 1: Language model statistics for string-based and deep syntax language models, statistics are for  string tokens and LFG lemmas for the same set of 7.29M English sentences", "labels": [], "entities": []}, {"text": " Table 2: Ngram coverage and perplexity (ppl) on held-out test set. Note: DS = deep syntax, SB string- based, eos = end of sentence markers", "labels": [], "entities": [{"text": "coverage", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.7087221741676331}]}, {"text": " Table 3: Most frequent trigrams in test set for deep  syntax model", "labels": [], "entities": []}]}