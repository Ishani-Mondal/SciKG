{"title": [{"text": "Exploring Representation-Learning Approaches to Domain Adaptation", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.6779634952545166}]}], "abstractContent": [{"text": "Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data.", "labels": [], "entities": []}, {"text": "Sequence labeling systems like part-of-speech taggers are typically trained on newswire text, and in tests their error rate on, for example, biomedical data can triple, or worse.", "labels": [], "entities": [{"text": "Sequence labeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8850125372409821}, {"text": "part-of-speech taggers", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7476188540458679}, {"text": "error rate", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9652546346187592}]}, {"text": "We investigate techniques for building open-domain sequence labeling systems that approach the ideal of a system whose accuracy is high and constant across domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9981122016906738}]}, {"text": "In particular, we investigate unsupervised techniques for representation learning that provide new features which are stable across domains, in that they are predictive in both the training and out-of-domain test data.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.9364006519317627}]}, {"text": "In experiments , our novel techniques reduce error by as much as 29% relative to the previous state of the art on out-of-domain text.", "labels": [], "entities": [{"text": "error", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9943177103996277}]}], "introductionContent": [{"text": "Supervised natural language processing (NLP) systems exhibit a significant drop-off in performance when tested on domains that differ from their training domains.", "labels": [], "entities": [{"text": "Supervised natural language processing (NLP)", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7378564136368888}]}, {"text": "Past research in a variety of NLP tasks, like parsing () and chunking), has shown that systems suffer from a drop-off in performance on out-of-domain tests.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9693311452865601}, {"text": "chunking", "start_pos": 61, "end_pos": 69, "type": "TASK", "confidence": 0.8800203204154968}]}, {"text": "Two separate experiments with part-of-speech (POS) taggers trained on Wall Street Journal (WSJ) text show that they can reach accuracies of 97-98% on WSJ test sets, but achieve accuracies of at most 90% on biomedical text (R.).", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) text", "start_pos": 70, "end_pos": 100, "type": "DATASET", "confidence": 0.9416991812842233}, {"text": "accuracies", "start_pos": 126, "end_pos": 136, "type": "METRIC", "confidence": 0.9918021559715271}, {"text": "WSJ test sets", "start_pos": 150, "end_pos": 163, "type": "DATASET", "confidence": 0.9629136125246683}, {"text": "accuracies", "start_pos": 177, "end_pos": 187, "type": "METRIC", "confidence": 0.9899118542671204}]}, {"text": "The major cause for poor performance on outof-domain texts is the traditional representation used by supervised NLP systems.", "labels": [], "entities": []}, {"text": "Most systems depend to varying degrees on lexical features, which tie predictions to the words observed in each example.", "labels": [], "entities": []}, {"text": "While such features have been used in a variety of tasks for better in-domain performance, they are pitfalls for out-of-domain tests for two reasons: first, the vocabulary can differ greatly between domains, so that important words in the test data may never be seen in the training data.", "labels": [], "entities": []}, {"text": "And second, the connection between words and labels may also change across domains.", "labels": [], "entities": []}, {"text": "For instance, \"signaling\" appears only as a present participle (VBG) in WSJ text (as in, \"signaling that ...\"), but predominantly as a noun (as in \"signaling pathway\") in biomedical text.", "labels": [], "entities": []}, {"text": "Representation learning is a promising new approach to discovering useful features that are stable across domains. and our previous work (2009) demonstrate novel, unsupervised representation learning techniques that produce new features for domain adaptation of a POS tagger.", "labels": [], "entities": [{"text": "Representation learning", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9268903136253357}, {"text": "domain adaptation of a POS tagger", "start_pos": 241, "end_pos": 274, "type": "TASK", "confidence": 0.6543226639429728}]}, {"text": "This framework is attractive for several reasons: experimentally, learned features can yield significant improvements over standard supervised models on out-of-domain tests.", "labels": [], "entities": []}, {"text": "Since the representation learning techniques are unsupervised, they can be applied to arbitrary new domains to yield the best set of features for learning on WSJ text and predicting on the new domain.", "labels": [], "entities": []}, {"text": "There is no need to supply additional labeled examples for each new domain.", "labels": [], "entities": []}, {"text": "This reduces the effort for domain adaptation, and makes it possible to apply systems to open-domain text collections like the Web, where it is prohibitively expensive to collect a labeled sample that is truly representative of all domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7440434545278549}]}, {"text": "Here we explore two novel directions in the representation-learning framework for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7454145848751068}]}, {"text": "Specifically, we investigate empirically the effects of representation learning techniques on POS tagging to answer the following: 1.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.8992960751056671}]}, {"text": "Can we produce multi-dimensional representations for domain adaptation?", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7542405724525452}]}, {"text": "Our previous efforts have provided only a single new feature in the learned representations.", "labels": [], "entities": []}, {"text": "We now show how we can perform a multi-dimensional clustering of words such that each dimension of the clustering forms anew feature in our representation; such multi-dimensional representations dramatically reduce the out-of-domain error rate of our POS tagger from 9.5% to 6.7%.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 251, "end_pos": 261, "type": "TASK", "confidence": 0.6196424961090088}]}, {"text": "2. Can maximum-entropy models be used to produce representations for domain adaptation?", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7433931231498718}]}, {"text": "Recent work on contrastive estimation) has shown that maximum-entropybased latent variable models can yield more accurate clusterings for POS tagging than more traditional generative models trained with ExpectationMaximization.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.8712542653083801}]}, {"text": "Our preliminary results show that such models can be used effectively as representations for domain adaptation as well, matching state-of-the-art results while using far less data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7268919199705124}]}, {"text": "The next section provides background information on learning representations for NLP tasks using latent-variable language models.", "labels": [], "entities": []}, {"text": "Section 3 describes our experimental setup.", "labels": [], "entities": []}, {"text": "In Sections 4 and 5, we empirically investigate our two questions with a series of representation-learning methods.", "labels": [], "entities": []}, {"text": "Section 6 analyzes our best learned representation to help explain its effectiveness.", "labels": [], "entities": []}, {"text": "Section 7 presents previous work, and Section 8 concludes and outlines directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the same experimental setup as: the Penn Treebank () Wall Street Journal portion for our labeled training data; 561 MEDLINE sentences (9576 words) from the Penn BioIE project) for our labeled test set; and all of the unlabeled text from the Penn Treebank WSJ portion plus Blitzer et al.'s MEDLINE corpus of 71,306 unlabeled sentences to train our latent variable models.", "labels": [], "entities": [{"text": "Penn Treebank () Wall Street Journal portion", "start_pos": 43, "end_pos": 87, "type": "DATASET", "confidence": 0.9437933479036603}, {"text": "Penn BioIE project", "start_pos": 163, "end_pos": 181, "type": "DATASET", "confidence": 0.8982823689778646}, {"text": "Penn Treebank WSJ portion", "start_pos": 248, "end_pos": 273, "type": "DATASET", "confidence": 0.9745400547981262}, {"text": "MEDLINE corpus", "start_pos": 296, "end_pos": 310, "type": "DATASET", "confidence": 0.7039945870637894}]}, {"text": "The two texts come from two very different domains, making this data a tough test for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7731796205043793}]}, {"text": "23% of the word types in the test text are Out-Of-Vocabulary (OOV), meaning that they are never observed in the training data.", "labels": [], "entities": [{"text": "Out-Of-Vocabulary (OOV)", "start_pos": 43, "end_pos": 66, "type": "METRIC", "confidence": 0.8477645069360733}]}, {"text": "We use a number of unsupervised representation learning techniques to discover features from our unlabeled data, and a supervised classifier to train on the training set annotated with learned features.", "labels": [], "entities": []}, {"text": "We use an open source Conditional Random Field (CRF) () software package 1 designed by Sunita Sajarwal and William W. Cohen to implement our supervised models.", "labels": [], "entities": []}, {"text": "We refer to the baseline system with feature set following our previous work as PLAIN-CRF.", "labels": [], "entities": []}, {"text": "Our learned features will supplement this set.", "labels": [], "entities": []}, {"text": "For comparison, we also report on the performance of Blitzer et al.'s Structural Correspondence Learning (SCL) (2006), our HMM-based model (2009)(HY09), and two other baselines: \u2022 TEST-CRF: Our baseline model, trained and tested on the test data.", "labels": [], "entities": [{"text": "HY09", "start_pos": 146, "end_pos": 150, "type": "DATASET", "confidence": 0.5911153554916382}, {"text": "TEST-CRF", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.876984715461731}]}, {"text": "This is our upper bound.", "labels": [], "entities": []}, {"text": "\u2022 SELF-CRF: Following the self-training paradigm (e.g., (McClosky et al., 2006b; McClosky et al., 2006a)), we train our baseline first on the training set, then apply it to the test set, then retrain it on the training set plus the automatically labeled test set.", "labels": [], "entities": [{"text": "SELF-CRF", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.869573712348938}]}, {"text": "We perform only one iteration of retraining, although in general multiple iterations are possible, usually with diminishing marginal returns.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The configuration of HMM layers and HMM states", "labels": [], "entities": []}, {"text": " Table 3: SEM-CRF reduces error compared with  SCL by 1.1% on all words; I-HMM* closes 33%  of the gap between the state-of-the-art HY09 and  the upper-bound, TEST-CRF.", "labels": [], "entities": [{"text": "error", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9921369552612305}, {"text": "HY09", "start_pos": 132, "end_pos": 136, "type": "DATASET", "confidence": 0.9647321105003357}]}]}