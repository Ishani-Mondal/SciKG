{"title": [{"text": "Expectation Vectors: A Semiotics Inspired Approach to Geometric Lexical-Semantic Representation", "labels": [], "entities": [{"text": "Expectation Vectors", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8559089004993439}, {"text": "Geometric Lexical-Semantic Representation", "start_pos": 54, "end_pos": 95, "type": "TASK", "confidence": 0.6228823761145273}]}], "abstractContent": [{"text": "We introduce anew family of geometric models of meaning, inspired by principles from semiotics and information theory, based on what we call Expectation Vectors.", "labels": [], "entities": []}, {"text": "We present theoretical arguments in support of these representations over traditional context-feature vectors: primarily that they provide a more intuitive representation of meaning, and detach vector representation from the specific context features thereby allowing arbitrarily sophisticated language models to be leveraged.", "labels": [], "entities": []}, {"text": "We present a preliminary evaluation of an expectation vector based word sense disambiguation system using the SemEval-2007 task 2 dataset, with very encouraging results, particularly with respect to ambiguous verbs.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.6342918078104655}, {"text": "SemEval-2007 task 2 dataset", "start_pos": 110, "end_pos": 137, "type": "DATASET", "confidence": 0.7113561779260635}]}], "introductionContent": [{"text": "It is a cornerstone assumption of distributional lexical semantics that the distribution of words in a corpus reflects their meaning.", "labels": [], "entities": []}, {"text": "Common interpretations of this include the Distributional Hypothesis and the Contextual Hypotheses, which state that there is a relationship between a word's meaning, and the context(s) in which it appears.", "labels": [], "entities": []}, {"text": "In recent years this insight has been borne out by correlations between human judgements and distributional models of word similarity, and steady advances in tasks such as word sense disambiguation (Sch\u00fctze, 1998) and information retrieval.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 172, "end_pos": 197, "type": "TASK", "confidence": 0.7052842775980631}, {"text": "information retrieval", "start_pos": 218, "end_pos": 239, "type": "TASK", "confidence": 0.8548993766307831}]}, {"text": "The workhorse of these approaches are wordspace models: vectors built from context features which serve as geometric analogues of meaning.", "labels": [], "entities": []}, {"text": "Despite many advances, substantial problems exist with this approach to modelling meaning.", "labels": [], "entities": [{"text": "modelling meaning", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.8687775731086731}]}, {"text": "Amongst these are the problems of data sparseness and of how to model compositional meaning.", "labels": [], "entities": []}, {"text": "In this short paper, we introduce anew family of wordspace models, based on insights gleaned from semiotics and information theory, called Expectation Vectors.", "labels": [], "entities": []}, {"text": "These retain the convenient vector-based paradigm whilst encouraging the exploitation of advances in language modelling from other areas of NLP.", "labels": [], "entities": []}, {"text": "We finish by outlining some present efforts to evaluate expectation vectors in the area of word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.7410471240679423}]}], "datasetContent": [{"text": "An expectation vector was produced for each training and test instance in the SemEval dataset by matching the headword's context against that of each word position in the British National Corpus using an implementation of the distance based similarity measure outlined in section 3.2.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.7696597278118134}, {"text": "British National Corpus", "start_pos": 171, "end_pos": 194, "type": "DATASET", "confidence": 0.9522872368494669}]}, {"text": "For matters of convenience, independent forwards and backwards expectation vectors were produced from the context preceding the headword and that following it, and their elements were multiplied together to produce the final vector.", "labels": [], "entities": []}, {"text": "No lemmatization or part-of-speech tagging was employed.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.681034728884697}]}, {"text": "Neither was any dimensionality reduction, each vector therefore having ~650,000 elements: one for each word type in the corpus.", "labels": [], "entities": []}, {"text": "Each test sample's vector was compared against all corresponding training sample vectors using both cosine similarity and Euclidean distance 2 . In the MAX setups (see), each test case was assigned the sense of the single nearest training example according to the metric being used.", "labels": [], "entities": []}, {"text": "In the CosOR setup, sense scores were generated by applying a probabilistic OR operation over the squared Cosine similarities of all relevant training examples 3 . The BaseMFS setup is a popular baseline in which the most frequent sense in the training set fora given ambiguous word is attributed to every test case.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Recall on SemEval WSD task, including", "labels": [], "entities": [{"text": "SemEval WSD task", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.6574609478314718}]}, {"text": " Table 3: Recall of best official SemEval WSD systems", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9620643854141235}, {"text": "SemEval WSD", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.7994307279586792}]}]}