{"title": [], "abstractContent": [{"text": "The paper introduced the task designing ideas, data preparation methods, evaluation metrics and results of the second Chinese syntactic parsing evaluation (CIPS-Bakeoff-ParsEval-2010) jointed with SIGHAN Bakeoff tasks.", "labels": [], "entities": [{"text": "Chinese syntactic parsing evaluation", "start_pos": 118, "end_pos": 154, "type": "TASK", "confidence": 0.6669653430581093}]}], "introductionContent": [{"text": "Syntactic parsing is an important technique in the research area of natural language processing.", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9319136440753937}, {"text": "natural language processing", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6554844280083975}]}, {"text": "The evaluation-driven methodology is a good way to spur the its development.", "labels": [], "entities": []}, {"text": "Two main parts of the method area benchmark database and several well-designed evaluation metrics.", "labels": [], "entities": []}, {"text": "Its feasibility has been proven in the English language.", "labels": [], "entities": []}, {"text": "After the release of the Penn Treebank (PTB) () and the PARSEVAL metrics (, some new corpusbased syntactic parsing techniques were explored in the English language.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.9734926342964172}, {"text": "corpusbased syntactic parsing", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.6514405210812887}]}, {"text": "Based on them, many state-of-art English parser were built, including the well-known Collins parser, Charniak parser) and Berkeley parser.", "labels": [], "entities": [{"text": "Collins", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.9661312103271484}]}, {"text": "By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar), many syntactic parser other than the CFG formalism were also developed.", "labels": [], "entities": [{"text": "CFG", "start_pos": 214, "end_pos": 217, "type": "DATASET", "confidence": 0.8966732025146484}]}, {"text": "These include Malt Parser,), Stanford Parser ( and C&C Parser.", "labels": [], "entities": [{"text": "Malt Parser", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.8460798263549805}, {"text": "Stanford Parser", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.9231601357460022}, {"text": "C&C Parser", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.7878872752189636}]}, {"text": "Based on the Penn Chinese Treebank (CTB) () developed on the similar annotation scheme of PTB, these parsing techniques were also transferred to the Chinese language.", "labels": [], "entities": [{"text": "Penn Chinese Treebank (CTB)", "start_pos": 13, "end_pos": 40, "type": "DATASET", "confidence": 0.9661025702953339}, {"text": "PTB", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.958139955997467}]}, {"text": "() explored the feasibility of applying lexicalized PCFG in Chinese.", "labels": [], "entities": []}, {"text": "( proposed a joint syntactic and semantic model for parsing Chinese.", "labels": [], "entities": [{"text": "parsing Chinese", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.9052523970603943}]}, {"text": "But till now, there is not a good Chinese parser whose performance can approach the state-of-art English parser.", "labels": [], "entities": []}, {"text": "It is still an open challenge for parsing Chinese sentences due to some special characteristics of the Chinese language.", "labels": [], "entities": [{"text": "parsing Chinese sentences", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.9048394958178202}]}, {"text": "We need to find a suitable benchmark database and evaluation metrics for the Chinese language.", "labels": [], "entities": []}, {"text": "Last year, we organized the first Chinese syntactic parsing evaluation ---CIPS-ParsEval-2009 (.", "labels": [], "entities": [{"text": "Chinese syntactic parsing evaluation", "start_pos": 34, "end_pos": 70, "type": "TASK", "confidence": 0.655087485909462}, {"text": "CIPS-ParsEval-2009", "start_pos": 74, "end_pos": 92, "type": "DATASET", "confidence": 0.8048970699310303}]}, {"text": "Five Chinese parsing tasks were designed as follows: \ud97b\udf59 Task 1: Part-of-speech (POS) tagging; \ud97b\udf59 Task 2: Base chunk (BC) parsing \ud97b\udf59 Task 3: Functional chunk (FC) parsing \ud97b\udf59 Task 4: Event description clause (EDC) recognition \ud97b\udf59 Task 5: Constituent parsing in EDCs They cover different levels of Chinese syntactic parsing, including POS tagging (Task 1), shallow parsing (, complex sentence splitting (Task 4) and constituent tree parsing (Task 5).", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.6834834098815918}, {"text": "Functional chunk (FC) parsing \ud97b\udf59 Task", "start_pos": 137, "end_pos": 173, "type": "TASK", "confidence": 0.7063072696328163}, {"text": "Event description clause (EDC) recognition \ud97b\udf59 Task", "start_pos": 177, "end_pos": 226, "type": "TASK", "confidence": 0.663402236170239}, {"text": "Constituent parsing", "start_pos": 230, "end_pos": 249, "type": "TASK", "confidence": 0.7579490840435028}, {"text": "Chinese syntactic parsing", "start_pos": 289, "end_pos": 314, "type": "TASK", "confidence": 0.6959873040517172}, {"text": "POS tagging", "start_pos": 326, "end_pos": 337, "type": "TASK", "confidence": 0.8344190120697021}, {"text": "shallow parsing", "start_pos": 348, "end_pos": 363, "type": "TASK", "confidence": 0.7185198962688446}, {"text": "complex sentence splitting", "start_pos": 367, "end_pos": 393, "type": "TASK", "confidence": 0.6765518188476562}, {"text": "constituent tree parsing", "start_pos": 407, "end_pos": 431, "type": "TASK", "confidence": 0.6509528358777364}]}, {"text": "The news and academic articles annotated in the Tsinghua Chinese Treebank (TCT ver1.0) were used to build different goldstandard data for them.", "labels": [], "entities": [{"text": "Tsinghua Chinese Treebank (TCT ver1.0)", "start_pos": 48, "end_pos": 86, "type": "DATASET", "confidence": 0.8827083536556789}]}, {"text": "Some detailed information about CIPS-ParsEval-2009 can be found in (.", "labels": [], "entities": [{"text": "CIPS-ParsEval-2009", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.9548153877258301}]}, {"text": "This evaluation found the following difficult points for Chinese syntactic parsing.", "labels": [], "entities": [{"text": "Chinese syntactic parsing", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.6670765082041422}]}, {"text": "1) There are two difficulties in Chinese POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.7782905101776123}]}, {"text": "One is the nominal verbs.", "labels": [], "entities": []}, {"text": "The POS accuracy of them is about 17% lower than the overall accuracy.", "labels": [], "entities": [{"text": "POS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9895262718200684}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.8888411521911621}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9983795881271362}]}, {"text": "The other is the unknown words.", "labels": [], "entities": []}, {"text": "The POS accuracy of them is about 40-10% lower than the overall accuracy.", "labels": [], "entities": [{"text": "POS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9865846633911133}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.8974851369857788}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9984890222549438}]}, {"text": "2) The chunks with complex internal structures show poor performance in two chunking tasks.", "labels": [], "entities": []}, {"text": "How to recognize them correctly needs more lexical semantic knowledge.", "labels": [], "entities": []}, {"text": "3) The joint recognition of constituent tag and head position show poor performance in the constituent parsing task of EDCs.", "labels": [], "entities": [{"text": "constituent parsing task", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.7793270548184713}]}, {"text": "Therefore, the second Chinese syntactic parsing evaluation (CIPS-Bakeoff-ParsEval-2010) jointed with SIGHAN Bakeoff tasks was proposed to deal with these problems.", "labels": [], "entities": [{"text": "Chinese syntactic parsing evaluation", "start_pos": 22, "end_pos": 58, "type": "TASK", "confidence": 0.6483405977487564}]}, {"text": "Some new designing ideas are as follows: 1) We use the segments sentences as the input of the syntactic parser to test the effects of POS tagging for Chinese parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 134, "end_pos": 145, "type": "TASK", "confidence": 0.7173565924167633}, {"text": "Chinese parsing", "start_pos": 150, "end_pos": 165, "type": "TASK", "confidence": 0.5681444257497787}]}, {"text": "2) We design anew metric to evaluate performance of event construction recognition in a constituent parser of EDCs.", "labels": [], "entities": [{"text": "event construction recognition", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.7614792287349701}]}, {"text": "3) We try to evaluate the performance of event relation recognition in Chinese complex sentence.", "labels": [], "entities": [{"text": "event relation recognition", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.7194312413533529}]}, {"text": "In the following sections, we will introduce the task designing ideas, data preparation methods, evaluation metrics and results of the evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "For Task 2-1, we designed three kinds of evaluation metrics: 1) POS accuracy (POS-A) This metri is used to evaluate the performance of automatic POS tagging.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.7287680506706238}, {"text": "POS tagging", "start_pos": 145, "end_pos": 156, "type": "TASK", "confidence": 0.6995604336261749}]}, {"text": "Its computation formula is as follows: \u2022 POS accuracy = (sum of words with correct POS tags) / (sum of words in goldstandard sentences) * 100% The correctness criteria of POS tagging is as follows: \ud97b\udf59 The automatically assigned POS tag is same with the gold-standard one.", "labels": [], "entities": [{"text": "POS", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.8649406433105469}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.6539831161499023}, {"text": "POS tagging", "start_pos": 171, "end_pos": 182, "type": "TASK", "confidence": 0.815351665019989}]}, {"text": "2) Constituent parsing evaluation We selected three commonly-used metrics to evaluation the performance of constituent parsing: labeled precision, recall, and F1-score.", "labels": [], "entities": [{"text": "Constituent parsing", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.753521591424942}, {"text": "constituent parsing", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7365988790988922}, {"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.8592602610588074}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9973908066749573}, {"text": "F1-score", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9983965754508972}]}, {"text": "Their computation formulas are as follows: \u2022 Precision = (sum of correctly labeled constituents ) / (sum of parsed constituents) * 100% \u2022 Recall = (sum of correctly labeled constituents) / (sum of gold-standard constituents) *100% \u2022 F1-score = 2*P*R / (P+R) Two correctness criteria are used for constituent parsing evaluation: \ud97b\udf59 'B+C' criteria: the boundaries and syntactic tags of the automatically parsed constituents must be same with the goldstandard ones.", "labels": [], "entities": [{"text": "Precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9769540429115295}, {"text": "Recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9969930648803711}, {"text": "F1-score", "start_pos": 233, "end_pos": 241, "type": "METRIC", "confidence": 0.9992185831069946}, {"text": "constituent parsing evaluation", "start_pos": 296, "end_pos": 326, "type": "TASK", "confidence": 0.8017815351486206}]}, {"text": "\ud97b\udf59 'B+C+H' criteria: the boundaries, syntactic tags and head positions of the automatically parsed constituents must be same with the gold-standard ones.", "labels": [], "entities": []}, {"text": "3) Event recognition evaluation We only considered the recognition recall of each event construction annotated in the event bank, due to the current parsing status of Task 2-1 output.", "labels": [], "entities": [{"text": "Event recognition evaluation", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.8898760080337524}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.8404026627540588}]}, {"text": "For each event target verb annotated in the event bank, we computed their Micro and Macro average recognition recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.600472092628479}]}, {"text": "The computation formulas are as follows: \u2022 Micro Recall = (sum of all correctly recognized event constructions) / (sum of all gold standard event constructions) * 100% \u2022 Macro Recall = (sum of Micro-R of each event target verb ) / (sum of event target verbs in gold-standard set ) The correctness criteria of event recognition should consider following two matching conditions: Condition 1: Each event chunk in a goldstandard event construction should have a corresponding constituent in the automatic parse tree.", "labels": [], "entities": [{"text": "Micro Recall", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.9000672101974487}, {"text": "event recognition", "start_pos": 309, "end_pos": 326, "type": "TASK", "confidence": 0.7244070619344711}]}, {"text": "For the single-word chunk, the automatically assigned POS tag should be same with the gold standard one.", "labels": [], "entities": []}, {"text": "For the multiword chunk, the boundary, syntactic tag and head positions of the automatically parsed constituent should be same with the gold-standard ones.", "labels": [], "entities": []}, {"text": "Meanwhile, the corresponding constituents should have the same layout sequences with the gold standard event construction.", "labels": [], "entities": []}, {"text": "Condition 2: All event-chunk-corresponding constituents should have a common ancestor node in the parse tree.", "labels": [], "entities": []}, {"text": "One of the left and right boundaries of the ancestor node should be same with the left and right boundaries of the corresponding event construction.", "labels": [], "entities": []}, {"text": "For Task 2-2, we design two kinds of evaluation metrics: 1) POS accuracy (POS-A) This index is used to evaluate the performance of automatic POS tagging.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.6431408524513245}, {"text": "POS tagging", "start_pos": 141, "end_pos": 152, "type": "TASK", "confidence": 0.6889784932136536}]}, {"text": "Then we computed the labeled precision, recall and F1-socre of these two parts and obtain the arithmetic mean of these two F1-score as the final ranking index.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9303341507911682}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9993816614151001}, {"text": "F1-socre", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9990887641906738}, {"text": "arithmetic mean", "start_pos": 94, "end_pos": 109, "type": "METRIC", "confidence": 0.9479645192623138}, {"text": "F1-score", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9896429181098938}]}, {"text": "(TPI=Take Part In)  The Task 2 of CIPS-Bakeoff-2010 attracted 13 participants.", "labels": [], "entities": [{"text": "TPI", "start_pos": 1, "end_pos": 4, "type": "METRIC", "confidence": 0.9845663905143738}, {"text": "CIPS-Bakeoff-2010", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.9207392930984497}]}, {"text": "Almost all of them took part in the two subtasks: Task 2-1 and 2-2.", "labels": [], "entities": []}, {"text": "Only one participant took part in the Task 2-2 subtask alone.", "labels": [], "entities": []}, {"text": "Among them, 9 participants submitted parsing results.", "labels": [], "entities": []}, {"text": "In Task 2-1, we received 16 parsing results, including 13 close track systems and 3 open track systems.", "labels": [], "entities": []}, {"text": "In Task 2-2, we received 15 parsing results, including 9 close track systems and 6 open track systems.", "labels": [], "entities": []}, {"text": "shows the submission information of all participants of Task 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  shows the basic statistics of all the training and  test sets in Task 2.", "labels": [], "entities": []}, {"text": " Table 3  Result submission data of all participants in Task 2. (TPI=Take Part In)", "labels": [], "entities": [{"text": "TPI", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9802119731903076}]}, {"text": " Table 4 shows the detailed  comparison data.", "labels": [], "entities": []}, {"text": " Table 7 and Table 8 show the evaluation re- sults of event recognition in the close and open  tracks respectively. When we consider the com- plete event constructions contained in a parse  tree, the best Macro-Recall is only about 71%.  There are still lots of room to improve in the fu- ture.", "labels": [], "entities": [{"text": "event recognition", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7547008693218231}]}, {"text": " Table 7 Event recognition evaluation results of Task 2-1 (Close Track), ranked with Macro-R", "labels": [], "entities": [{"text": "Event recognition", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.8471605181694031}]}, {"text": " Table 9 Constituent parsing evaluation results of Task 2-2 (Close Track), ranked with Tot-F1  (S_S=simple sentence, C_S=complex sentence)", "labels": [], "entities": [{"text": "Constituent parsing evaluation", "start_pos": 9, "end_pos": 39, "type": "TASK", "confidence": 0.8013657728830973}]}]}