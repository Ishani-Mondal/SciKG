{"title": [{"text": "New Parameterizations and Features for PSCFG-Based Machine Translation", "labels": [], "entities": [{"text": "PSCFG-Based Machine Translation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.7849088311195374}]}], "abstractContent": [{"text": "We propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006).", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.886639416217804}]}, {"text": "We add a source-span variance model that, for each rule utilized in a prob-abilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training time.", "labels": [], "entities": []}, {"text": "We further propose different methods of combining hierarchical and syntax-based PSCFG models, by merging the grammars as well as by interpolating the translation models.", "labels": [], "entities": []}, {"text": "Finally, we compare syntax-augmented MT, which extracts rules based on target-side syntax, to a corresponding variant based on source-side syntax, and experiment with a model extension that jointly takes source and target syntax into account .", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.7860315442085266}]}], "introductionContent": [{"text": "The Probabilistic Synchronous Context Free Grammar (PSCFG) formalism suggests an intuitive approach to model the long-distance and lexically sensitive reordering phenomena that often occur across language pairs considered for statistical machine translation.", "labels": [], "entities": [{"text": "Probabilistic Synchronous Context Free Grammar (PSCFG) formalism", "start_pos": 4, "end_pos": 68, "type": "TASK", "confidence": 0.7013458179103004}, {"text": "statistical machine translation", "start_pos": 226, "end_pos": 257, "type": "TASK", "confidence": 0.6722013056278229}]}, {"text": "As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations.", "labels": [], "entities": []}, {"text": "Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the statistical translation model.", "labels": [], "entities": []}, {"text": "Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (;).", "labels": [], "entities": []}, {"text": "In this work, we propose several improvements to the hierarchical phrase-based MT model of and its syntax-based extension by.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.8706353306770325}]}, {"text": "We add a source span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training (i.e., rule extraction) time.", "labels": [], "entities": [{"text": "rule extraction)", "start_pos": 349, "end_pos": 365, "type": "TASK", "confidence": 0.8007021049658457}]}, {"text": "We further propose different methods of combining hierarchical and syntax-based PSCFG models, by merging the grammars as well as by interpolating the translation models.", "labels": [], "entities": []}, {"text": "Finally, we compare syntax-augmented MT, which extracts rules based on target-side syntax, to a corresponding variant based on source-side syntax, and experiment with a model extension based on source and target syntax.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.8038389682769775}]}, {"text": "We evaluate the different models on the NIST large resource Chinese-to-English translation task.", "labels": [], "entities": [{"text": "NIST large resource Chinese-to-English translation task", "start_pos": 40, "end_pos": 95, "type": "TASK", "confidence": 0.7712955276171366}]}, {"text": "introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length, by estimating for each possible source span length a Bernoulli distribution p(R|) where R takes value one if reordering takes place and zero otherwise.", "labels": [], "entities": []}, {"text": "Maximumlikelihood estimation of the distribution amounts to simply counting the relative frequency of nonterminal reorderings overall extracted rule instances that incurred a substitution of span length . Ina more fine-grained approach they add a separate binary feature R, for each combination of reordering truth value Rand span length (where all \u2265 10 are merged into a single value), and then tune the feature weights discriminatively on a development set.", "labels": [], "entities": []}, {"text": "Our approach differs from in that we estimate one source span length distribution for each substitution site of each grammar rule, resulting in unique distributions for each rule, estimated from all instances of the rule in the training data.", "labels": [], "entities": []}, {"text": "This enables our model to condition reordering range on the individual rules used in a derivation, and even allows to distinguish between two rules r 1 and r 2 that both reorder arguments with identical mean span lengths , but where the span lengths encountered in extracted instances of r 1 are all close to , whereas span length instances for r 2 vary widely.", "labels": [], "entities": []}, {"text": "propose a hypbrid approach between hierarchical phrase based MT and a rule based MT system, reporting improvement over each individual model on an Englishto-German translation task.", "labels": [], "entities": []}, {"text": "Essentially, the rule based system is converted to a single-nonterminal PSCFG, and hence can be combined with the hierarchical model, another single-nonterminal PSCFG, by taking the union of the rule sets and augmenting the feature vectors, adding zerovalues for rules that only exist in one of the two grammars.", "labels": [], "entities": []}, {"text": "We face the challenge of combining the single-nonterminal hierarchical grammar with a multi-nonterminal syntax-augmented grammar.", "labels": [], "entities": []}, {"text": "Thus one hierarchical rule typically corresponds to many syntax-augmented rules.", "labels": [], "entities": []}, {"text": "The SAMT system used by adds hierarchical rules separately to the syntax-augmented grammar, resulting in a backbone grammar of well-estimated hierarchical rules supporting the sparser syntactic rules.", "labels": [], "entities": []}, {"text": "They allow the model preference between hierarchical and syntax rules to be learned from development data by adding an indicator feature to all rules, which is one for hierarchical rules and zero for syntax rules.", "labels": [], "entities": []}, {"text": "However, no empirical comparison is given between the purely syntax-augmented and the hybrid grammar.", "labels": [], "entities": []}, {"text": "We aim to fill this gap by experimenting with both models, and further refine the hybrid approach by adding interpolated probability models to the syntax rules.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approaches by comparing translation quality according to the IBM-BLEU () metric on the NIST Chineseto-English translation task using MT04 as development set to train the model parameters \u03bb, and MT05, MT06 and MT08 as test sets.", "labels": [], "entities": [{"text": "NIST Chineseto-English translation task", "start_pos": 103, "end_pos": 142, "type": "TASK", "confidence": 0.6725042909383774}, {"text": "MT04", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.8585305213928223}, {"text": "MT05", "start_pos": 210, "end_pos": 214, "type": "DATASET", "confidence": 0.8883615732192993}, {"text": "MT06", "start_pos": 216, "end_pos": 220, "type": "DATASET", "confidence": 0.8727129101753235}, {"text": "MT08", "start_pos": 225, "end_pos": 229, "type": "DATASET", "confidence": 0.8998116254806519}]}, {"text": "We perform PSCFG rule extraction and decoding using the open-source \"SAMT\" system (, using the provided implementations for the hierarchical and syntax-augmented grammars.", "labels": [], "entities": [{"text": "PSCFG rule extraction", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.829811672369639}]}, {"text": "For all systems, we use the bottom-up chart parsing decoder implemented in the SAMT toolkit with a reordering limit of 15 source words, and correspondingly extract rules from initial phrase pairs of maximum source length 15.", "labels": [], "entities": [{"text": "bottom-up chart parsing decoder", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.7373168170452118}, {"text": "SAMT toolkit", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.8920388519763947}]}, {"text": "All rules have at most two nonterminal symbols, which must be non-consecutive on the source side, and rules must contain at least one source-side terminal symbol.", "labels": [], "entities": []}, {"text": "For parameter tuning, we use the L 0 -regularized minimum-error-rate training tool provided by the SAMT toolkit.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.731421634554863}, {"text": "SAMT toolkit", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.9154096841812134}]}, {"text": "The parallel training data comprises of 9.6M sentence pairs (206M Chinese Words, 228M English words).", "labels": [], "entities": []}, {"text": "The source and target language parses for the syntax-augmented grammar were generated by the Stanford parser (.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "The source span models (indicated by +span) achieve small test set improvements of 0.15 BLEU points on average for the hierarchical and 0.26 BLEU points for the syntax-augmented system, but these are not statistically significant.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9990708827972412}, {"text": "BLEU", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9978972673416138}]}, {"text": "Augmenting a syntax-augmented grammar with hierarchical features (\"Syntax+hiermodels\") results in average test set improvements of 0.5 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9993084669113159}]}, {"text": "These improvements are not statistically significant either, but persist across all three test sets.", "labels": [], "entities": []}, {"text": "This demonstrates the benefit of more reliable feature estimation.", "labels": [], "entities": []}, {"text": "Further augmenting the hierarchical rules to the grammar (\"Syntax+hiermodels+hierrules\") does not yield additional improvements.", "labels": [], "entities": []}, {"text": "The use of bilingual syntactic parses ('Syntax/src&tgt') turns out detrimental to translation quality.", "labels": [], "entities": []}, {"text": "We assume this is due to the huge number of nonterminals in these grammars and the great amount of badly-estimated low-occurrence-count rules.", "labels": [], "entities": []}, {"text": "Perhaps merging this grammar with a regular syntax-augmented grammar could yield better results.", "labels": [], "entities": []}, {"text": "We also experimented with a source-parse based model ('Syntax/src').", "labels": [], "entities": []}, {"text": "While not being able to match translation quality of its target-based counterpart, the model still outperforms the hierarchical system on all test sets.", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9392523169517517}]}], "tableCaptions": []}