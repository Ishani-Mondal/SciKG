{"title": [{"text": "MATREX: The DCU MT System for WMT 2010", "labels": [], "entities": [{"text": "MATREX", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.8177849054336548}, {"text": "WMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8352837562561035}]}], "abstractContent": [{"text": "This paper describes the DCU machine translation system in the evaluation campaign of the Joint Fifth Workshop on Statistical Machine Translation and Metrics in ACL-2010.", "labels": [], "entities": [{"text": "DCU machine translation", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7309653759002686}, {"text": "Statistical Machine Translation and Metrics in ACL-2010", "start_pos": 114, "end_pos": 169, "type": "TASK", "confidence": 0.7138578423431942}]}, {"text": "We describe the modular design of our multi-engine machine translation (MT) system with particular focus on the components used in this participation.", "labels": [], "entities": [{"text": "multi-engine machine translation (MT)", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.7828262050946554}]}, {"text": "We participated in the English-Spanish and English-Czech translation tasks, in which we employed our multi-engine architecture to translate.", "labels": [], "entities": [{"text": "English-Czech translation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.5135819166898727}]}, {"text": "We also participated in the system combination task which was carried out by the MBR decoder and confusion network decoder.", "labels": [], "entities": [{"text": "MBR decoder", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.9768862426280975}]}], "introductionContent": [{"text": "In this paper, we present the DCU multi-engine MT system MATREX (Machine Translation using Examples).", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.7366071939468384}, {"text": "Machine Translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7634268701076508}]}, {"text": "This system exploits example-based MT, statistical MT (SMT), and system combination techniques.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9629998803138733}, {"text": "statistical MT (SMT)", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7137336134910583}]}, {"text": "We participated in the English-Spanish (enes) and English-Czech (en-cs) translation tasks.", "labels": [], "entities": []}, {"text": "For these two tasks, we employ several individual MT systems: 1) Baseline: phrasebased SMT ( ); 2) EBMT: Monolingually chunking both source and target sides of the dataset using a marker-based chunker (); 3) Factored translation model ( ); 4) Source-side context-informed (SSCI) systems (; 5) the moses-chart (a Moses implementation of the hierarchical phrase-based (HPB) approach of) and 6) Apertium) rule-based machine translation (RBMT).", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.968221127986908}, {"text": "phrasebased SMT", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.5126824975013733}, {"text": "machine translation (RBMT", "start_pos": 413, "end_pos": 438, "type": "TASK", "confidence": 0.6206982731819153}]}, {"text": "Finally, we use a word-level combination framework ( to combine the multiple translation hypotheses and employ anew rescoring model to generate the final translation.", "labels": [], "entities": []}, {"text": "For the system combination task, we first use the minimum Bayes-risk (MBR) () decoder to select the best hypothesis as the alignment reference for the confusion network (CN) ().", "labels": [], "entities": [{"text": "Bayes-risk (MBR)", "start_pos": 58, "end_pos": 74, "type": "METRIC", "confidence": 0.8345746546983719}]}, {"text": "We then build the CN using the TER metric), and finally search for the best translation.", "labels": [], "entities": [{"text": "TER", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9798531532287598}]}, {"text": "The remainder of this paper is organised as follows: Section 2 details the various components of our system, in particular the multi-engine strategies used for the shared task.", "labels": [], "entities": []}, {"text": "In Section 3, we outline the complete system setup for the shared task and provide evaluation results on the test set.", "labels": [], "entities": []}, {"text": "Section 4 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes our experimental setup for the en-cs and en-es translation tasks.", "labels": [], "entities": [{"text": "en-es translation tasks", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.6786421338717142}]}, {"text": "The CzEng corpus (Bojar and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2009) is a collection of parallel texts from sources of different quality and as such it contains some noise.", "labels": [], "entities": [{"text": "CzEng corpus (Bojar and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2009)", "start_pos": 4, "end_pos": 76, "type": "DATASET", "confidence": 0.9085921496152878}]}, {"text": "As the first step, we discarded those sentence pairs having more than 10% of non-Latin characters.", "labels": [], "entities": []}, {"text": "The CzEng corpus is quite large (8M sentence pairs).", "labels": [], "entities": [{"text": "CzEng corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8422465920448303}]}, {"text": "Although we were able to build a vanilla SMT system on all parallel data available (News-Commentary + CzEng), we also attempted to build additional systems using NewsCommentary data (which we considered indomain) and various in-domain subsets of CzEng hoping to achieve better results on domainspecific data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9678487181663513}, {"text": "NewsCommentary data", "start_pos": 162, "end_pos": 181, "type": "DATASET", "confidence": 0.9367300271987915}]}, {"text": "For our first system, we selected 128,218 sentence pairs from CzEng labeled as news.", "labels": [], "entities": []}, {"text": "For the other two systems, we selected subsets of 2M and 4M sentence pairs identified as most similar to the development sets (as a sample of in-domain data) based on cosine similarity of their representation in a TF-IDF weighted vector space model (cf.).", "labels": [], "entities": []}, {"text": "We also applied the pseudo-relevavance-feedback technique for query expansion to select another subset with 2M sentence pairs.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.7905110120773315}]}, {"text": "We used the output of 15 systems for system combination for the en-cs translation task.", "labels": [], "entities": [{"text": "en-cs translation task", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.6598228017489115}]}, {"text": "Among these, 5 systems were built using Moses and varying the size of the training data (DCUAll, DCU-Ex2M, DCU-4M, DCU-2M and DCUNews); 9 context-informed PB-SMT systems (DCU-SSCI-*) using (combinations of) various context features (word, PoS, supertags and dependency relations) trained only on the News Commentary data (marked with \u2021 in); and one system using the moses-chart decoder, also trained on the news commentary data.", "labels": [], "entities": [{"text": "News Commentary data", "start_pos": 300, "end_pos": 320, "type": "DATASET", "confidence": 0.9290496110916138}, {"text": "news commentary data", "start_pos": 407, "end_pos": 427, "type": "DATASET", "confidence": 0.6538434227307638}]}, {"text": "Three baseline systems using Moses were built, where we varied the amount of training data used: \u2022 epn: This system uses all of the Europarl and News-Commentary parallel data.", "labels": [], "entities": [{"text": "Europarl and News-Commentary parallel data", "start_pos": 132, "end_pos": 174, "type": "DATASET", "confidence": 0.7362768292427063}]}, {"text": "\u2022 UN-half: This system uses the data suplied to \"epn\", plus an additional 2.1M sentences pairs randomly selected from the United Nations corpus.", "labels": [], "entities": [{"text": "UN-half", "start_pos": 2, "end_pos": 9, "type": "DATASET", "confidence": 0.8563356995582581}, {"text": "United Nations corpus", "start_pos": 122, "end_pos": 143, "type": "DATASET", "confidence": 0.8716660340627035}]}, {"text": "\u2022 all: This system uses all of the available parallel data.", "labels": [], "entities": []}, {"text": "For en-es we also obtained output from the factored model (trained only on the news commentary corpus) and the Apertium RBMT system.", "labels": [], "entities": [{"text": "news commentary corpus", "start_pos": 79, "end_pos": 101, "type": "DATASET", "confidence": 0.6374405523141226}, {"text": "Apertium RBMT system", "start_pos": 111, "end_pos": 131, "type": "DATASET", "confidence": 0.7208123703797659}]}, {"text": "We also derived phrase alignments using the), and added those phrase translations in the Moses phrase table.", "labels": [], "entities": [{"text": "phrase alignments", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7095503062009811}]}, {"text": "The systems marked with use a language model built using the Spanish Gigaword corpus, in addition to the one built using the provided monolingual data.", "labels": [], "entities": [{"text": "Spanish Gigaword corpus", "start_pos": 61, "end_pos": 84, "type": "DATASET", "confidence": 0.8218504389127096}]}, {"text": "These 6 sets of system outputs are then used for system combination.", "labels": [], "entities": [{"text": "system combination", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7371045053005219}]}, {"text": "The evaluation results for en-es and en-cs experiments are shown in and", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: en-es experimental results.", "labels": [], "entities": []}, {"text": " Table 4: en-cs experimental results.", "labels": [], "entities": []}]}