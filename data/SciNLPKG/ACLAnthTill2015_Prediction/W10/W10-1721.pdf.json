{"title": [{"text": "The Cunei Machine Translation Platform for WMT '10", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.675718829035759}, {"text": "WMT '", "start_pos": 43, "end_pos": 48, "type": "TASK", "confidence": 0.7596112191677094}]}], "abstractContent": [{"text": "This paper describes the Cunei Machine Translation Platform and how it was used in the WMT '10 German to English and Czech to English translation tasks.", "labels": [], "entities": [{"text": "Cunei Machine Translation", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.7049078543980917}, {"text": "WMT '10 German to English and Czech to English translation tasks", "start_pos": 87, "end_pos": 151, "type": "TASK", "confidence": 0.7289936542510986}]}, {"text": "1 The Cunei Machine Translation Platform The Cunei Machine Translation Platform (Phillips and Brown, 2009) is open-source software and freely available at http://www.cunei.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.633519321680069}, {"text": "Machine Translation Platform (Phillips and Brown, 2009)", "start_pos": 51, "end_pos": 106, "type": "TASK", "confidence": 0.6583521246910096}]}, {"text": "Like Moses (Koehn et al., 2007) and Joshua (Li et al., 2009), Cunei provides a statistical decoder that combines partial translations (ei-ther phase pairs or grammar rules) in order to compose a coherent sentence in the target language.", "labels": [], "entities": []}, {"text": "What makes Cunei unique is that it models the translation task with a non-parametric model that assesses the relevance of each translation instance.", "labels": [], "entities": [{"text": "translation task", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.9188350141048431}]}, {"text": "The process begins by encoding in a lattice all possible contiguous phrases from the input.", "labels": [], "entities": []}, {"text": "1 For each source phrase in the lattice, Cunei locates instances of it in the corpus and then identifies the aligned target phrase(s).", "labels": [], "entities": []}, {"text": "This much is standard to most data-driven MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.95850670337677}]}, {"text": "The typical step at this stage is to model a phrase pair by computing relative frequencies over the collection of translation instances.", "labels": [], "entities": []}, {"text": "This model for the phrase pair will never change and knowledge of the translation instances can subsequently be discarded.", "labels": [], "entities": []}, {"text": "In contrast to using a phrase pair as the basic unit of modeling, Cunei models each translation instance.", "labels": [], "entities": []}, {"text": "A distance function, represented by a log-linear model, scores the relevance of each translation instance.", "labels": [], "entities": []}, {"text": "Our model then sums the scores of translation instances that predict the same target hypothesis.", "labels": [], "entities": []}, {"text": "The advantage of this approach is that it provides a flexible framework for novel sources of 1 Cunei offers limited support for non-contiguous phrases, similar in concept to grammar rules, but this setting was disabled in our experiments. information.", "labels": [], "entities": []}, {"text": "The non-parametric model still uses information gleaned overall translation instances, but it permits us to define a distance function that operates over one translation instance at a time.", "labels": [], "entities": []}, {"text": "This enables us to score a wide-variety of information represented by the translation instance with respect to the input and the target hypothesis under consideration.", "labels": [], "entities": []}, {"text": "For example, we could compute how similar one translation instance's parse tree or morpho-syntactic information is to the input.", "labels": [], "entities": []}, {"text": "Furthermore , this information will vary throughout the corpus with some translation instances exhibiting higher similarity to the input.", "labels": [], "entities": []}, {"text": "Our approach captures that these instances are more relevant and they will have a larger effect on the model.", "labels": [], "entities": []}, {"text": "For the WMT '10 task, we exploited instance-specific context and alignment features which will be discussed in more detail below.", "labels": [], "entities": [{"text": "WMT '10 task", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.6692240089178085}]}, {"text": "1.1 Formalism Cunei's model is a hybrid between the approaches of Statistical MT and Example-Based MT.", "labels": [], "entities": [{"text": "Statistical MT", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.5406180620193481}, {"text": "Example-Based MT", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.5167828947305679}]}, {"text": "A typical SMT model will score a phrase pair with source s, target t, log features \u03c6, and weights \u03bb using a log-linear model, as shown in Equation 1 of Figure 1.", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9953012466430664}]}, {"text": "There is no prototypical model for EBMT, but Equation 2 demonstrates a reasonable framework where evidence for the phrase pair is accumulated overall instances of translation.", "labels": [], "entities": []}, {"text": "Each instance of translation from the corpus has a source sand target t.", "labels": [], "entities": []}, {"text": "In the most limited case s = sand t = t , but typically an EBMT system will have some notion of similarity and use instances of translation that do not exactly match the input.", "labels": [], "entities": []}, {"text": "Cunei's model is defined in such away that we maintain the distance function \u03c6(s, s , t , t) from the EBMT model, but compute it in a much more efficient manner.", "labels": [], "entities": [{"text": "\u03c6", "start_pos": 77, "end_pos": 78, "type": "METRIC", "confidence": 0.7292949557304382}]}, {"text": "In particular, we remove the real-space summation within a logarithm that makes it impractical to tune model weights.", "labels": [], "entities": []}, {"text": "While the inner term initially appears complex, it is simply the expectation of each feature under the distribution of translation instances and can be efficiently computed with an online update.", "labels": [], "entities": []}, {"text": "Last, the introduction of \u03b4, a slack variable, is necessary to additionally ensure that the score of this model is equal to Equation 2.", "labels": [], "entities": [{"text": "Equation", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9958207607269287}]}, {"text": "Specifying the model in this manner ties together the two different mod-eling approaches pursued by SMT and EBMT; the SMT model of Equation 1 is merely a special case of our model when the features for all instances of a translation are constant such that \u03c6 k (s, s , t , t) = \u03c6 k (s, t) \u2200s , t.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9831937551498413}, {"text": "EBMT", "start_pos": 108, "end_pos": 112, "type": "DATASET", "confidence": 0.9010584354400635}, {"text": "SMT", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.9116464853286743}]}, {"text": "Indeed, this distinction illuminates the primary advantage of our model.", "labels": [], "entities": []}, {"text": "Each feature is calculated particular to one translation instance in the corpus and each translation instance is scored individually.", "labels": [], "entities": []}, {"text": "The model is then responsible for ag-gregating knowledge across multiple instances of translation.", "labels": [], "entities": []}, {"text": "Unlike the SMT model, our aggregate model does not maintain feature independence.", "labels": [], "entities": [{"text": "SMT", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9887599945068359}]}, {"text": "Each instance of translation represents a joint set of features.", "labels": [], "entities": []}, {"text": "The higher the score of a translation instance, the more all its features inform the aggregate model.", "labels": [], "entities": []}, {"text": "Thus, our model is biased toward feature values that represent relevant translation instances.", "labels": [], "entities": []}, {"text": "1.2 Context Not all translations found in a corpus are equally useful.", "labels": [], "entities": []}, {"text": "Often, when dealing with data of varying quality, training a SMT system on all of the data degrades performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9908678531646729}]}, {"text": "A common work-around is to perform some sort of sub-sampling that selects a small quantity of novel phrase pairs from the large out-of-domain corpus such that they do not overwhelm the number of phrase pairs extracted from the smaller in-domain corpus.", "labels": [], "entities": []}, {"text": "Instead of building our model from a heuristic sub-sample, we utilize Cunei's modeling approach to explicitly identify the relevance of each translation instance.", "labels": [], "entities": []}, {"text": "We add features to the model that identify when a translation instance occurs within the same context as the input.", "labels": [], "entities": []}, {"text": "This permits us to train on all available data by dynamically weight-ing each instance of a translation.", "labels": [], "entities": []}, {"text": "First, we capture the broader context or genre of a translation instance by comparing the document in the corpus from which it was extracted to the input document.", "labels": [], "entities": []}, {"text": "These documents are modeled as a bag of words, and we use common document-level distance metrics from the field of information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 115, "end_pos": 136, "type": "TASK", "confidence": 0.6956629008054733}]}, {"text": "Specifically, we implement as features document-level precision, recall, cosine distance and Jensen-Shannon distance (Lin, 1991).", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9544384479522705}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9992017149925232}, {"text": "cosine distance", "start_pos": 73, "end_pos": 88, "type": "METRIC", "confidence": 0.8461452424526215}]}, {"text": "In order to capture local, intra-sentential context , we compare the words immediately to the left and right of each translation instance with the input.", "labels": [], "entities": []}, {"text": "We add one feature that counts the total number of adjacent words that match the input and a second feature that penalizes translation instances whose adjacent context only (or mostly) occurs in one direction.", "labels": [], "entities": []}, {"text": "As a variation on the same concept, we also add four binary features that indicate when a unigram or bigram match is present on the left or right hand side.", "labels": [], "entities": []}, {"text": "The corpus in which an instance is located can also substantially alter the style of a translation.", "labels": [], "entities": []}, {"text": "For example, both the German to English and the Czech to English corpora consisted of in-domain News Commenary and out-of-domain Europarl text.", "labels": [], "entities": [{"text": "News Commenary", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.9280460774898529}, {"text": "Europarl text", "start_pos": 129, "end_pos": 142, "type": "DATASET", "confidence": 0.9215480089187622}]}, {"text": "When creating the index, Cunei stores the name of the corpus that is associated with each sentence.", "labels": [], "entities": []}, {"text": "From this information we create a set of binary features for each instance of translation that indicate from which corpus the instance originated.", "labels": [], "entities": []}, {"text": "The weights for these origin features can be 150 conceived as mixture weights specifying the relevance of each corpus.", "labels": [], "entities": []}, {"text": "1.3 Alignment After a match is found on the source-side of the corpus, Cunei must determine the target phrase to which it aligns.", "labels": [], "entities": []}, {"text": "The phrase alignment is treated as a hidden variable and not specified during training.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.6785945296287537}]}, {"text": "Ideally, the full alignment process would be carried out dynamically at run-time.", "labels": [], "entities": []}, {"text": "Unfortunately , even a simple word alignment such as IBM Model-1 is too expensive.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7385113686323166}, {"text": "IBM Model-1", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.8535880446434021}]}, {"text": "Instead, we run a word aligner offline and our on-line phrase alignment computes features over the the word alignments.", "labels": [], "entities": []}, {"text": "The phrase alignment features are then components of the model for each translation instance.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7555266618728638}]}, {"text": "While the calculations are not exactly the same, conceptually this work is modeled after (Vo-gel, 2005).", "labels": [], "entities": [{"text": "Vo-gel, 2005)", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.8136301785707474}]}, {"text": "For each source-side match in the corpus, an alignment matrix is loaded for the complete sentence in which the match resides.", "labels": [], "entities": []}, {"text": "This alignment matrix contains scores for all word correspondences in the sentence pair and can be created using GIZA++ (Och and Ney, 2003) or the Berke-ley aligner (Liang et al., 2006).", "labels": [], "entities": []}, {"text": "Intuitively, when a source phrase is aligned to a target phrase, this implies that the remainder of the source sentence that is not specified by the source phrase is aligned to the remainder of the target sentence not specified by the target phrase.", "labels": [], "entities": []}, {"text": "Separate features compute the probability that the word alignments for tokens within the phrase are concentrated within the phrase boundaries and that the word alignments for tokens outside the phrase are concentrated outside the phrase boundaries.", "labels": [], "entities": []}, {"text": "In addition, words with no alignment links or weak alignments links demonstrate uncertainty in modeling.", "labels": [], "entities": []}, {"text": "To capture this effect, we incorporate two more features that count the number of uncertain alignments present in the source phrase and the target phrase.", "labels": [], "entities": []}, {"text": "The features described above assess the phrase alignment likelihood fora particular translation instance.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7752207517623901}]}, {"text": "Because they operate overall the word alignments present in a sentence, the alignment scores are contextual and usually vary from instance to instance.", "labels": [], "entities": []}, {"text": "As the model weights change, so too will the phrase alignment scores.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7103525698184967}]}, {"text": "Each source phrase is modeled as having some probability of aligning to every possible target phrase within a given sentence.", "labels": [], "entities": []}, {"text": "However, it is not practical to compute all possible phrase alignments, so we extract translation instances using only a few high-scoring phrase alignments for each occurrence of a source phrase in the corpus.", "labels": [], "entities": [{"text": "phrase alignments", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7102151215076447}]}, {"text": "2 As discussed previously, these extracted translation instances form the basic modeling unit in Cunei.", "labels": [], "entities": [{"text": "Cunei", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.9382443428039551}]}], "introductionContent": [], "datasetContent": [{"text": "The newswire evaluation sets from the prior two years were selected as development data.", "labels": [], "entities": [{"text": "newswire evaluation sets", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.8172019521395365}]}, {"text": "636 sentences were sampled from WMT '09 for tuning and all 2,051 sentences from WMT '08 were reserved for testing.", "labels": [], "entities": [{"text": "WMT '09", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9724766214688619}, {"text": "WMT '08", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.9563844005266825}]}, {"text": "Finally, a blind evaluation was also performed with the new WMT '10 test set.", "labels": [], "entities": [{"text": "WMT '10 test set", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.9459494352340698}]}, {"text": "All systems were tuned toward BLEU () and all evaluation metrics were run on lowercased, tokenized text.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9986478686332703}]}, {"text": "The results in show the performance of Cunei 3 against the Moses system we also built with the same data.", "labels": [], "entities": []}, {"text": "The first Cunei system we built included all the alignment features discussed in \u00a71.3.", "labels": [], "entities": []}, {"text": "These per-instance alignment features are essential to Cunei's run-time phrase extraction and cannot be disabled.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.6723020821809769}]}, {"text": "The second, and complete, system added to this all the context features described in \u00a71.2.", "labels": [], "entities": []}, {"text": "Cunei, in general, performs significantly better than Moses in German and is competitive with Moses in Czech.", "labels": [], "entities": []}, {"text": "However, we hoped to see a larger gain from the addition of the context features.", "labels": [], "entities": []}, {"text": "In order to better understand our results and see if there was greater potential for the context features, we selectively added a few of the features at a time to the German system.", "labels": [], "entities": [{"text": "German system", "start_pos": 167, "end_pos": 180, "type": "DATASET", "confidence": 0.9137135744094849}]}, {"text": "These experiments are reported in.", "labels": [], "entities": []}, {"text": "What is interesting here is that most subsets of context features did better than the whole and none degraded the baseline (at least according to BLEU) on the test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9978367686271667}]}, {"text": "We did not expect a fully additive gain from the combination, as many of the context features do represent different ways of capturing the same phenomena.", "labels": [], "entities": []}, {"text": "However, we were still surprised to find an apparently detrimental interaction among the full set of context features.", "labels": [], "entities": []}, {"text": "Theoretically adding new features should only improve a system as a feature can always by ignored by assigning it a weight of zero.", "labels": [], "entities": []}, {"text": "However, new features expand the hypothesis space and provide the model with more degrees of freedom which may make it easier to get stuck in local minima.", "labels": [], "entities": []}, {"text": "While the gradient-based, annealing method for optimization that we use tends work better than MERT, it is still susceptible to these issues.", "labels": [], "entities": [{"text": "MERT", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.7627714276313782}]}, {"text": "Indeed, the variation on the tuning set-while relatively inconsequential-is evidence that this is occurring and that we have not found the global optimum.", "labels": [], "entities": []}, {"text": "Further investigation is necessary into the interaction between the context features and techniques for robust optimization.", "labels": [], "entities": []}], "tableCaptions": []}