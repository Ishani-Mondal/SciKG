{"title": [{"text": "Empirical Studies in Learning to Read", "labels": [], "entities": [{"text": "Learning to Read", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.6279145181179047}]}], "abstractContent": [{"text": "In this paper, we present empirical results on the challenge of learning to read.", "labels": [], "entities": []}, {"text": "That is, given a handful of examples of the concepts and relations in an ontology and a large corpus, the system should learn to map from text to the concepts/relations of the ontology.", "labels": [], "entities": []}, {"text": "In this paper, we report contrastive experiments on the recall, precision, and F-measure (F) of the mapping in the following conditions: (1) employing word-based patterns, employing semantic structure, and combining the two; and (2) fully automatic learning versus allowing minimal questions of a human informant.", "labels": [], "entities": [{"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9990578293800354}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9984000325202942}, {"text": "F-measure (F)", "start_pos": 79, "end_pos": 92, "type": "METRIC", "confidence": 0.9437518417835236}]}], "introductionContent": [{"text": "This paper reports empirical results with an algorithm that \"learns to read\" text and map that text into concepts and relations in an ontology specified by the user.", "labels": [], "entities": []}, {"text": "Our approach uses unsupervised and semi-supervised algorithms to harness the diversity and redundancy of the ways concepts and relations are expressed in document collections.", "labels": [], "entities": []}, {"text": "Diversity can be used to automatically generate patterns and paraphrases for new concepts and relations to boost recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.993697464466095}]}, {"text": "Redundancy can be exploited to automatically check and improve the accuracy of those patterns, allowing for system learning without human supervision.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9982284903526306}]}, {"text": "For example, the system learns how to recognize anew relation (e.g. invent), starting from 5-20 instances (e.g. Thomas Edison + the light bulb).", "labels": [], "entities": []}, {"text": "The system iteratively searches a collection of documents to find sentences where those instances are expressed (e.g. \"Thomas Edison's patent for the light bulb\"), induces patterns over textual features found in those instances (e.g. patent(possessive:A, for:B)), and repeats the cycle by applying the generated patterns to find additional instances followed by inducing more patterns from those instances.", "labels": [], "entities": []}, {"text": "Unsupervised measures of redundancy and coverage are used to estimate the reliability of the induced patterns and learned instances; only the most reliable are added, which minimizes the amount of noise introduced at each step.", "labels": [], "entities": [{"text": "coverage", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9889928698539734}]}, {"text": "There have been two approaches to evaluation of mapping text to concepts and relations: Automatic Content Extraction (ACE) and Knowledge Base Population (KBP) 2 . In ACE, complete manual annotation fora small corpus (~25k words) was possible; thus, both recall and precision could be measured across every instance in the test set.", "labels": [], "entities": [{"text": "recall", "start_pos": 254, "end_pos": 260, "type": "METRIC", "confidence": 0.9981127977371216}, {"text": "precision", "start_pos": 265, "end_pos": 274, "type": "METRIC", "confidence": 0.9977808594703674}]}, {"text": "This evaluation can be termed micro reading in that it evaluates every concept/relation mention in the corpus.", "labels": [], "entities": []}, {"text": "In ACE, learning algorithms had roughly 300k words of training data.", "labels": [], "entities": []}, {"text": "By contrast, in KBP, the corpus of documents in the test set was too large fora complete answer key.", "labels": [], "entities": [{"text": "KBP", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.72206711769104}]}, {"text": "Rather than a complete answer key, relations were extracted fora list of entities; system output was pooled and judged manually.", "labels": [], "entities": []}, {"text": "This type of reading has been termed macro reading , since finding any instance of the relation in the 1.3M document corpus is measured success, rather than finding every instance.", "labels": [], "entities": [{"text": "macro reading", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.7285415530204773}]}, {"text": "Only 118 queries were provided, though several hundred were created and distributed by participants.", "labels": [], "entities": []}, {"text": "In the study in this paper, recall, precision, and F are measured for 11 relations under the following contrastive conditions 1.", "labels": [], "entities": [{"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9994770884513855}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9994305968284607}, {"text": "F", "start_pos": 51, "end_pos": 52, "type": "METRIC", "confidence": 0.9997352957725525}]}, {"text": "Patterns based on words vs. predicateargument structure vs. combining both.", "labels": [], "entities": []}, {"text": "2. Fully automatic vs. a few periodic responses by humans to specific queries.", "labels": [], "entities": []}, {"text": "Though many prior studies have focused on precision, e.g., to find any text justification to answer a question, we focus equally on recall and report recall performance as well as precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9982309937477112}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9986487030982971}, {"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.979510486125946}, {"text": "precision", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9987695813179016}]}, {"text": "This addresses the challenge of finding information on rarely mentioned entities (no matter how challenging the expression).", "labels": [], "entities": []}, {"text": "We believe the effect will be improved technology overall.", "labels": [], "entities": []}, {"text": "We evaluate our system in a micro-reading context on 11 relations.", "labels": [], "entities": []}, {"text": "Ina fully automatic configuration, the system achieves an F of .48).", "labels": [], "entities": [{"text": "F", "start_pos": 58, "end_pos": 59, "type": "METRIC", "confidence": 0.9993158578872681}]}, {"text": "With limited human intervention, F rises to .58 (Recall=.49, Precision=.70).", "labels": [], "entities": [{"text": "F", "start_pos": 33, "end_pos": 34, "type": "METRIC", "confidence": 0.999721348285675}, {"text": "Recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9989196062088013}, {"text": "Precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9980728626251221}]}, {"text": "We see that patterns based on predicate-argument structure (text graphs) outperform patterns based on surface strings with respect to both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9986457228660583}, {"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9883849620819092}]}, {"text": "Section 2 describes our approach; section 3, some challenges; section 4, the implementation; section 5, evaluation; section 6, empirical results on extraction type; section 7, the effect of periodic, limited human feedback; section 8, related work; and section 9, lessons learned and conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate system performance, we ran two sep the expected class of entities for this argument.", "labels": [], "entities": []}, {"text": "Entity types are one of the 7 ACE types (Person, political entity, Location, FacilCurrently the syssals when the types are correct.", "labels": [], "entities": [{"text": "FacilCurrently", "start_pos": 77, "end_pos": 91, "type": "METRIC", "confidence": 0.9525405168533325}]}, {"text": "Potentially, the system could use pattern matches that violate type constraints as an additional type of negative example.", "labels": [], "entities": []}, {"text": "Any implementation would need to account for the fact too general patterns are quite effective when the type constraints are met.", "labels": [], "entities": []}, {"text": "For example, for the relation employed, <ORG>'s <PER> is a fairly precise pattern, despite clearly being overly general without the type constraints.", "labels": [], "entities": [{"text": "PER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9546236991882324}]}, {"text": "only two relations (sibling below includes ACE types/dates are in columns labeled with the first letter of the name of the . We have only included those types that are an argument for some relation.", "labels": [], "entities": []}], "tableCaptions": []}