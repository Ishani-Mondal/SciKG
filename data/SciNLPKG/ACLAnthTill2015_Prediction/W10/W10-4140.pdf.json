{"title": [{"text": "A domain adaption Word Segmenter For Sighan Bakeoff 2010", "labels": [], "entities": [{"text": "domain adaption Word Segmenter", "start_pos": 2, "end_pos": 32, "type": "TASK", "confidence": 0.8243048787117004}, {"text": "Sighan Bakeoff 2010", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.6193519433339437}]}], "abstractContent": [{"text": "We present a Chinese word segmentation system which ran on the closed track of the simplified Chinese Word Segmentation task of CIPS-SIGHAN-CLP 2010 bakeoffs.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.6125849684079488}, {"text": "Chinese Word Segmentation task", "start_pos": 94, "end_pos": 124, "type": "TASK", "confidence": 0.7103631719946861}, {"text": "CIPS-SIGHAN-CLP 2010 bakeoffs", "start_pos": 128, "end_pos": 157, "type": "DATASET", "confidence": 0.8284106850624084}]}, {"text": "Our segmenter was built using a HMM.", "labels": [], "entities": [{"text": "segmenter", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.966143786907196}]}, {"text": "To fulfill the cross-domain segmentation task, we use semi-supervised machine learning method to get the HMM model.", "labels": [], "entities": [{"text": "cross-domain segmentation", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.7176325917243958}]}, {"text": "Finally we get the mean result of four domains: P=0.719, R=0.72", "labels": [], "entities": []}], "introductionContent": [{"text": "The 2010 Sighan Bakeoff included two types of evaluations: (1) Closed training: In the closed training evaluation, participants can only use data provided by organizers to train their systems specifically, the following data resources and software tools are not permitted to be used in the training: 1) Unspecified corpus; 2) Unspecified dictionary, word list or character list: include the dictionaries of named entity, character lists for specific type of Chinese named entities, idiom dictionaries, semantic lexicons, etc.", "labels": [], "entities": [{"text": "Sighan Bakeoff", "start_pos": 9, "end_pos": 23, "type": "DATASET", "confidence": 0.7691225111484528}]}, {"text": "3) Human-encoded rule bases; 4) Unspecified software tools, include word segmenters, part-of-speech taggers, or parsers which are trained using unspecified data resources.", "labels": [], "entities": [{"text": "word segmenters", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.7097954005002975}, {"text": "part-of-speech taggers", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.7225767225027084}]}, {"text": "The character type information to distinguish the following four character types can be used in training: Chinese characters, English letters, digits and punctuations.", "labels": [], "entities": []}, {"text": "We prefer character-based Tagging than dictionary based word segmentation in closed training, for we can only use the provide train corpus and scale of the corpus is not large enough.", "labels": [], "entities": [{"text": "character-based Tagging", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.49153727293014526}, {"text": "dictionary based word segmentation", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.6439061760902405}]}, {"text": "If we select dictionary based method we will encounter the out-of-vocabulary problem.", "labels": [], "entities": []}, {"text": "But in character-based Tagging method we can yield a better performance than the dictionary based method for such problem.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.6974940299987793}]}], "datasetContent": [{"text": "We use HMM to establish the Word Segment prototype system and make use of the Labeled supplied by the Chinese Academy of Sciences to train the HMM and get the model parameters which will be used for the next iterative scaling.", "labels": [], "entities": [{"text": "Word Segment prototype", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7307024598121643}]}, {"text": "After that, we can get a system based on HMM model.", "labels": [], "entities": []}, {"text": "Then, with the help of the gotten system, we process the unlabeled corpus.", "labels": [], "entities": []}, {"text": "Once it is finished, we should add the processed corpus to the labeled corpus and get a larger corpus with which we can retrain the HMM.", "labels": [], "entities": []}, {"text": "All these steps have been done according four test corpuses: literature, computer, medicine, finance.", "labels": [], "entities": []}, {"text": "In the table, R indicates the recall rate, P indicates the precision rate, F1 indicates the macro average, OOV R indicates the out-of-vocabulary (OOV) rate, OOV RR indicates the out-of-vocabulary (OOV) self repair rate, IV RR indicates the out-of-vocabulary (OOV) self repair rate.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.9868587851524353}, {"text": "precision rate", "start_pos": 59, "end_pos": 73, "type": "METRIC", "confidence": 0.9905182421207428}, {"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9755020141601562}, {"text": "OOV R", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.9747112989425659}, {"text": "OOV RR", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9344153702259064}]}, {"text": "In order to more easily view data, we have presented the Graph2.", "labels": [], "entities": []}, {"text": "From the table and graph, we can see that the finance corpus has a better result, the computer corpus don't show a good result for the R, P, F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.991612434387207}]}, {"text": "Generally speaking, this result is a reflection for the difference between the dictionary based Tagging method and character-based Tagging method.", "labels": [], "entities": []}, {"text": "After recheck our corpus, we can find that there are more technical terms in the computer corpus than finance corpus.", "labels": [], "entities": []}, {"text": "The explanation for the result is that if the system encounter a technical terms, the character-based Tagging method will have a bad performance.", "labels": [], "entities": []}, {"text": "In such situation, dictionary based Tagging method may have a better performance.", "labels": [], "entities": []}, {"text": "For the OOV Rand OOV RR, the system has a not bad performance.", "labels": [], "entities": [{"text": "OOV Rand OOV RR", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.8764678239822388}]}], "tableCaptions": []}