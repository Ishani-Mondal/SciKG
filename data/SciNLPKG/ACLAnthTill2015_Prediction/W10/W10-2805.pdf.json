{"title": [{"text": "A Regression Model of Adjective-Noun Compositionality in Distributional Semantics", "labels": [], "entities": [{"text": "Adjective-Noun Compositionality", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.7399590909481049}]}], "abstractContent": [{"text": "In this paper we explore the computational modelling of compositionality in distri-butional models of semantics.", "labels": [], "entities": []}, {"text": "In particular , we model the semantic composition of pairs of adjacent English Adjectives and Nouns from the British National Corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 109, "end_pos": 132, "type": "DATASET", "confidence": 0.9535441795984904}]}, {"text": "We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent AN lemma pairs are treated as single tokens.", "labels": [], "entities": [{"text": "BNC", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.9168283343315125}]}, {"text": "We then extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and a Partial Least Squares Regression (PLSR) model.", "labels": [], "entities": []}, {"text": "We propose two evaluation methods for the implemented models.", "labels": [], "entities": []}, {"text": "Our study leads to the conclusion that regression-based models of composition-ality generally out-perform additive and multiplicative approaches, and also show a number of advantages that make them very promising for future research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word-space vector models or distributional models of semantics (henceforth DSMs), are computational models that build contextual semantic representations for lexical items from corpus data.", "labels": [], "entities": []}, {"text": "DSMs have been successfully used in the recent years fora number of different computational tasks involving semantic relations between words (e.g. synonym identification, computation of semantic similarity, modelling selectional preferences, etc., fora thorough discussion of the field, cf.).", "labels": [], "entities": [{"text": "synonym identification", "start_pos": 147, "end_pos": 169, "type": "TASK", "confidence": 0.8834744095802307}]}, {"text": "The theoretical foundation of DSMs is to be found in the \"distributional hypothesis of meaning\", attributed to Z.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.9510387778282166}]}, {"text": "Harris, which maintains that meaning is susceptible to distributional analysis and, in particular, that differences in meaning between words or morphemes in a language correlate with differences in their distribution.", "labels": [], "entities": []}, {"text": "While the vector-based representation of word meaning has been used fora longtime in computational linguistics, the techniques that are currently used have not seen much development with regards to one of the main aspects of semantics in natural language: compositionality.", "labels": [], "entities": []}, {"text": "To be fair, the study of semantic compositionality in DSMs has seen a slight revival in the recent times, cf.,,,, who propose various DSM approaches to represent argument structure, subject-verb and verb-object co-selection.", "labels": [], "entities": []}, {"text": "Current approaches to compositionality in DSMs are based on the application of a simple geometric operation on the basis of individual vectors (vector addition, pointwisemultiplication of corresponding dimensions, tensor product) which should in principle approximate the composition of any two given vectors.", "labels": [], "entities": []}, {"text": "On the contrary, since the the very nature of compositionality depends on the semantic relation being instantiated in a syntactic structure, we propose that the composition of vector representations must be modelled as a relation-specific phenomenon.", "labels": [], "entities": []}, {"text": "In particular, we propose that the usual procedures from machine learning tasks must be implemented also in the search for semantic compositionality in DSM.", "labels": [], "entities": []}, {"text": "In this paper we present work in progress on the computational modelling of compositionality in a data-set of English Adjective-Noun pairs extracted from the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 158, "end_pos": 161, "type": "DATASET", "confidence": 0.9115970134735107}]}, {"text": "We extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and, finally, a multinomial multiple regression model by Partial Least Squares Regression (PLSR).", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the three models of compositionality that were built, we devised two different procedures based on the Euclidean measure of geometric distance.", "labels": [], "entities": []}, {"text": "The first method draws a direct comparison of the different predicted vectors for each candidate A-N pair by computing the Euclidean distance between the observed vector and the modelled predictions.", "labels": [], "entities": []}, {"text": "We also inspect a general distance matrix for the whole compositionality subspace, i.e. all the observed vectors and all the predicted vectors.", "labels": [], "entities": []}, {"text": "We extract the 10 nearest neighbours for the 380 Adjective-Noun pairs in the test set and look for the intended predicted vectors in each case.", "labels": [], "entities": []}, {"text": "The idea here is that the best models should produce predictions that are as close as possible to the originally observed A-N vector.", "labels": [], "entities": []}, {"text": "Our second evaluation method uses the 10 nearest neighbours of each of the observed A-N pairs in the test set as gold-standard (excluding any modelled predictions), and compares them with the 10 nearest neighbours of each of the corresponding predictions as generated by the models.", "labels": [], "entities": []}, {"text": "The aim is to assess if the predictions made by each model share any top-10 neighbours with their corresponding gold-standard.", "labels": [], "entities": []}, {"text": "We award 1 point for every shared neighbour.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of distance values between the 380  observed A-N pairs and the predictions from each model  (ADD=additive, MUL=multiplicative, PLSR=Partial Least  Squares Regression).", "labels": [], "entities": [{"text": "ADD", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9890854954719543}, {"text": "MUL", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9849104285240173}, {"text": "Partial Least  Squares Regression", "start_pos": 150, "end_pos": 183, "type": "METRIC", "confidence": 0.5382101535797119}]}, {"text": " Table 2. Overall, 223 items in the test set had  at least one predicted vector in the top-10 list; of  these, 219 (98%) were generated by PLSR and the  remaining 4 (1%) by the multiplicative model.", "labels": [], "entities": []}, {"text": " Table 2: Nearest predicted neighbours and their positions in  the top-10 list.", "labels": [], "entities": []}, {"text": " Table 3: Shared neighbours with respect to the gold standard  and shared predicted neighbours.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.8821527361869812}]}]}