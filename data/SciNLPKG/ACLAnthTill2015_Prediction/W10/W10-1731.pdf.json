{"title": [], "abstractContent": [{"text": "This paper describes the system developed in collabaration between UCH and UPV for the 2010 WMT.", "labels": [], "entities": [{"text": "UCH", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9843037724494934}, {"text": "UPV for the 2010 WMT", "start_pos": 75, "end_pos": 95, "type": "DATASET", "confidence": 0.690559184551239}]}, {"text": "For this year's workshop , we present a system for English-Spanish translation.", "labels": [], "entities": [{"text": "English-Spanish translation", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6319376230239868}]}, {"text": "Output N-best lists were rescored via a target Neural Network Language Model, yielding improvements in the final translation quality as measured by BLEU and TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9963107705116272}, {"text": "TER", "start_pos": 157, "end_pos": 160, "type": "METRIC", "confidence": 0.8966095447540283}]}], "introductionContent": [{"text": "In Statistical Machine Translation (SMT), the goal is to translate a sentence f from a given source language into an equivalent sentenc\u00ea e from a certain target language.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8686930437882742}]}, {"text": "Such statement is typically formalised by means of the so-called log-linear models () as follows: where h k (f , e) is a score function representing an important feature for the translation off into e, K is the number of models (or features) and \u03bb k are the weights of the log-linear combination.", "labels": [], "entities": []}, {"text": "Typically, the weights \u03bb k are optimised during the tuning stage with the use of a development set.", "labels": [], "entities": []}, {"text": "Such features typically include the target language model p(e), which is one of the core components of an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9930137991905212}]}, {"text": "In fact, most of the times it is assigned a relatively high weight in the log-linear combination described above.", "labels": [], "entities": []}, {"text": "Traditionally, language modelling techniques have been classified into two main groups, the first one including traditional grammars such as context-free grammars, and the second one comprising more statistical, corpus-based models, such as n-gram models.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7387502789497375}]}, {"text": "In order to assign a probability to a given word, such models rely on the assumption that such probability depends on the previous history, i.e. then \u2212 1 preceding words in the utterance.", "labels": [], "entities": []}, {"text": "Nowadays, n-gram models have become a \"de facto\" standard for language modelling in state-ofthe-art SMT systems.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.709643080830574}, {"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.979496955871582}]}, {"text": "In the present work, we present a system which follows a coherent and natural evolution of probabilistic Language Models.", "labels": [], "entities": []}, {"text": "Specifically, we propose the use of a continuous space language model trained in the form of a Neural Network Language Model (NN LM).", "labels": [], "entities": []}, {"text": "The use of continuous space representation of language has been successfully applied in recent NN approaches to language modelling ().", "labels": [], "entities": [{"text": "language modelling", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.712643027305603}]}, {"text": "However, the use of Neural Network Language Models (NN LMs)) in state-of-theart SMT systems is not so popular.", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9314644932746887}]}, {"text": "The only comprehensive work refers to, where the target LM is presented in the form of a fullyconnected Multilayer Perceptron.", "labels": [], "entities": []}, {"text": "The presented system combines a standard, state-of-the-art SMT system with a NN LM via log-linear combination and N -best output rescoring.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9938690066337585}]}, {"text": "We chose to participate in the EnglishSpanish direction.", "labels": [], "entities": [{"text": "EnglishSpanish direction", "start_pos": 31, "end_pos": 55, "type": "DATASET", "confidence": 0.9076061844825745}]}], "datasetContent": [{"text": "NN LM was trained with the concatenation of the News-shuffled and News-Commentary10 Spanish corpora.", "labels": [], "entities": [{"text": "NN LM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9650860130786896}, {"text": "News-shuffled", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9644300937652588}]}, {"text": "Other language resources were discarded due to the large amount of computational resources that would have been needed for training a NN LM with such material.", "labels": [], "entities": []}, {"text": "shows some statistics of the corpora.", "labels": [], "entities": []}, {"text": "In order to reduce the complexity of the model, the vocabulary was restricted to the 20K more frequent words in the concatenation of news corpora.", "labels": [], "entities": []}, {"text": "Using this restricted vocabulary implies that 6.4% of the running words of the news-test2008 set, and 7.3% of the running words within the official 2010 test set, will be considered as unknown for our system.", "labels": [], "entities": [{"text": "news-test2008 set", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.9297972023487091}, {"text": "official 2010 test set", "start_pos": 139, "end_pos": 161, "type": "DATASET", "confidence": 0.754263736307621}]}, {"text": "In addition, the vocabulary includes a special token for unknown words used for compute probabilities when an unknown word appears, as described in Equation 2.", "labels": [], "entities": []}, {"text": "A 6-gram NN LM was trained for this task, based in previous works ().", "labels": [], "entities": []}, {"text": "The distributed encoding input layer consists of 640 units (128 for each word), the hidden layer has 500 units, and the output layer has 20K units, one for each word in the restricted vocabulary.", "labels": [], "entities": []}, {"text": "The total number of weights in the network was 10 342 003.", "labels": [], "entities": []}, {"text": "The training procedure was conducted by means of the stochastic back-propagation algorithm with weight decay, with a replacement of 300K training samples and 200K validation samples in each training epoch.", "labels": [], "entities": []}, {"text": "The training and validation sets were randomly extracted from the concatenation of news corpora.", "labels": [], "entities": []}, {"text": "The training set consisted of 102M words (3M sentences) and validation set 8M words (300K sentences).", "labels": [], "entities": []}, {"text": "The network needed 129 epochs for achieving convergence, resulting in 38.7M and 25.8M training and validation samples respectively.", "labels": [], "entities": []}, {"text": "For training the NN LM we used the April toolkit, which implements a pattern recognition and neural networks toolkit.", "labels": [], "entities": [{"text": "NN LM", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.8731406629085541}, {"text": "April toolkit", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9465263783931732}, {"text": "pattern recognition", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7769860029220581}]}, {"text": "The perplexity achieved by the 6-gram NN LM in the Spanish news-test08 development set was 116, versus 94 obtained with a standard 6-gram language model with interpolation and Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "Spanish news-test08 development set", "start_pos": 51, "end_pos": 86, "type": "DATASET", "confidence": 0.881251335144043}]}, {"text": "The number of sentences in the N -best list was set to 1 000 unique output sentences.", "labels": [], "entities": []}, {"text": "Results can be seen in.", "labels": [], "entities": []}, {"text": "In order to assess the reliability of such results, we computed pairwise improvement intervals as described in, by means of bootstrapping with 1000 bootstrap iterations and at a 95% confidence level.", "labels": [], "entities": []}, {"text": "Such confidence test reported the improvements to be statistically significant.", "labels": [], "entities": []}, {"text": "Four more experiments have done in order to study the influence of the N -best list size in the   For each N -best list size (200, 400, 600 and 800) the weights of the log-linear interpolation were optimised by means of MERT over the test08 set.", "labels": [], "entities": [{"text": "MERT", "start_pos": 220, "end_pos": 224, "type": "METRIC", "confidence": 0.9348384141921997}]}, {"text": "Table 3 shows the test results for each N -best list size using the correspondent optimised weights.", "labels": [], "entities": []}, {"text": "As it can be seen, the size of the N -best list seems to have an impact on the final translation quality produced.", "labels": [], "entities": []}, {"text": "Although in this case the results are not statistically significant for each size step, the final difference (from 27.5 to 27.8) is already significant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: English-Spanish translation quality for  development and official test set. Results are given  in BLEU/TER.  test08 (dev) test10 (test)  Baseline  24.8/60.0  26.7/55.1  NN LM  25.2/59.6  27.8/54.0", "labels": [], "entities": [{"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9942969679832458}, {"text": "TER", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.5914502739906311}, {"text": "NN LM  25.2", "start_pos": 179, "end_pos": 190, "type": "DATASET", "confidence": 0.9052229921023051}]}, {"text": " Table 3: Test set BLEU/TER performance for each  N -best list size.  N -best list size BLEU TER  200  27.5  54.2  400  27.6  54.2  600  27.7  54.1  800  27.6  54.2  1000  27.8  54.0", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9970703125}, {"text": "TER", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.5168487429618835}, {"text": "N -best list size BLEU TER", "start_pos": 70, "end_pos": 96, "type": "METRIC", "confidence": 0.7829974889755249}]}]}